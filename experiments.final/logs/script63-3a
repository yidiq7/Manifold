+ RUN=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='2 3'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1 --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc3bc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc3bc1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc2f6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc25bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc255950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc166510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc1bcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc189ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc189730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc118b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0d3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0dd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0a1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efbffea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc003b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc02f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5e3c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d8f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d8ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d53268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d0ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5cc5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5cd36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5cd37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5dfc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5dfc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5c83ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5c09510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5c29f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc03f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5b8c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5b8c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5b8c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5bf5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5bd7d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.019547626
test_loss: 0.019671382
train_loss: 0.0090725655
test_loss: 0.00898119
train_loss: 0.006269957
test_loss: 0.0066933804
train_loss: 0.005477133
test_loss: 0.0056914627
train_loss: 0.0053307153
test_loss: 0.005301539
train_loss: 0.005177132
test_loss: 0.0051679346
train_loss: 0.004584152
test_loss: 0.0050352486
train_loss: 0.004709363
test_loss: 0.0047426084
train_loss: 0.0045332955
test_loss: 0.004968934
train_loss: 0.004450911
test_loss: 0.004846005
train_loss: 0.0042535053
test_loss: 0.004670961
train_loss: 0.004462671
test_loss: 0.004772661
train_loss: 0.0041610645
test_loss: 0.0046513537
train_loss: 0.004110224
test_loss: 0.004519015
train_loss: 0.0041226554
test_loss: 0.0048108515
train_loss: 0.0041026324
test_loss: 0.0044956687
train_loss: 0.0039682505
test_loss: 0.004372526
train_loss: 0.004067778
test_loss: 0.004321856
train_loss: 0.0039699683
test_loss: 0.0043749553
train_loss: 0.003813988
test_loss: 0.0041737743
train_loss: 0.0040311324
test_loss: 0.0042153453
train_loss: 0.0039775805
test_loss: 0.0042263716
train_loss: 0.0038991314
test_loss: 0.004353083
train_loss: 0.004083889
test_loss: 0.0041781124
train_loss: 0.0038717086
test_loss: 0.004348058
train_loss: 0.004047476
test_loss: 0.0041527497
train_loss: 0.004040085
test_loss: 0.0044506085
train_loss: 0.0037939711
test_loss: 0.0041256505
train_loss: 0.0038523804
test_loss: 0.0042982325
train_loss: 0.003646867
test_loss: 0.004183979
train_loss: 0.003758031
test_loss: 0.004175936
train_loss: 0.0034912326
test_loss: 0.0041251816
train_loss: 0.0035836175
test_loss: 0.0042388123
train_loss: 0.003477535
test_loss: 0.003987771
train_loss: 0.0038018895
test_loss: 0.003973167
train_loss: 0.0035137008
test_loss: 0.003928891
train_loss: 0.00344377
test_loss: 0.0039027405
train_loss: 0.0034626015
test_loss: 0.0038776102
train_loss: 0.0036062435
test_loss: 0.0039891796
train_loss: 0.0034179867
test_loss: 0.0040519987
train_loss: 0.0034938576
test_loss: 0.003945753
train_loss: 0.0034357607
test_loss: 0.003971588
train_loss: 0.0036368994
test_loss: 0.0040100412
train_loss: 0.0035285421
test_loss: 0.0038912948
train_loss: 0.0034754837
test_loss: 0.004097174
train_loss: 0.0035042984
test_loss: 0.0039125034
train_loss: 0.0034002087
test_loss: 0.0037513967
train_loss: 0.0033820462
test_loss: 0.0039441707
train_loss: 0.0032757826
test_loss: 0.003755735
train_loss: 0.0035601624
test_loss: 0.0037517531
train_loss: 0.003427818
test_loss: 0.0037525482
train_loss: 0.0033806544
test_loss: 0.0038233057
train_loss: 0.003265006
test_loss: 0.0036814178
train_loss: 0.003451409
test_loss: 0.0037147857
train_loss: 0.0033702694
test_loss: 0.0038042273
train_loss: 0.0034005356
test_loss: 0.0040830285
train_loss: 0.0035511795
test_loss: 0.004015981
train_loss: 0.0032240718
test_loss: 0.0035995601
train_loss: 0.0033367996
test_loss: 0.0036661767
train_loss: 0.0032932581
test_loss: 0.0037487417
train_loss: 0.003391197
test_loss: 0.0038559055
train_loss: 0.0032253219
test_loss: 0.0037323972
train_loss: 0.0031579952
test_loss: 0.0035365745
train_loss: 0.0032422296
test_loss: 0.0035806636
train_loss: 0.0032111001
test_loss: 0.0038482659
train_loss: 0.003291798
test_loss: 0.0036947862
train_loss: 0.003109728
test_loss: 0.0036800758
train_loss: 0.0032820161
test_loss: 0.0036755698
train_loss: 0.003481783
test_loss: 0.0036724657
train_loss: 0.0029738753
test_loss: 0.0034515145
train_loss: 0.003355905
test_loss: 0.0037474388
train_loss: 0.003100234
test_loss: 0.0036929282
train_loss: 0.0031402444
test_loss: 0.0036745954
train_loss: 0.003534715
test_loss: 0.0035946153
train_loss: 0.003177234
test_loss: 0.0037799387
train_loss: 0.00330197
test_loss: 0.0037961465
train_loss: 0.002825023
test_loss: 0.0034027852
train_loss: 0.003039434
test_loss: 0.0035345107
train_loss: 0.0030412555
test_loss: 0.0036148957
train_loss: 0.0031319219
test_loss: 0.003370568
train_loss: 0.0031488682
test_loss: 0.003540186
train_loss: 0.0028720784
test_loss: 0.0035075166
train_loss: 0.0030095554
test_loss: 0.0036241552
train_loss: 0.0030054187
test_loss: 0.0036150832
train_loss: 0.0031567193
test_loss: 0.0035198943
train_loss: 0.0031195716
test_loss: 0.0035026195
train_loss: 0.003205
test_loss: 0.0035274802
train_loss: 0.0029213028
test_loss: 0.0035053417
train_loss: 0.0029996864
test_loss: 0.0035015382
train_loss: 0.0031255074
test_loss: 0.0037172693
train_loss: 0.0030857595
test_loss: 0.0036228434
train_loss: 0.0030892007
test_loss: 0.0035819702
train_loss: 0.0030290368
test_loss: 0.0035269032
train_loss: 0.0030759373
test_loss: 0.0035402868
train_loss: 0.0033104937
test_loss: 0.0037094196
train_loss: 0.0030366126
test_loss: 0.0037057057
train_loss: 0.0030122912
test_loss: 0.0036834236
train_loss: 0.0029721374
test_loss: 0.0035096689
train_loss: 0.0032497342
test_loss: 0.0034526417
train_loss: 0.0029798911
test_loss: 0.00347377
train_loss: 0.0027542568
test_loss: 0.003471824
train_loss: 0.0029923492
test_loss: 0.0035209518
train_loss: 0.0031998272
test_loss: 0.00355104
train_loss: 0.0032948111
test_loss: 0.003453693
train_loss: 0.0031483178
test_loss: 0.003475432
train_loss: 0.0033348862
test_loss: 0.0036273901
train_loss: 0.0030804984
test_loss: 0.0036315022
train_loss: 0.0032577193
test_loss: 0.0036114228
train_loss: 0.0031659636
test_loss: 0.003802141
train_loss: 0.0030901646
test_loss: 0.0035083375
train_loss: 0.0029157498
test_loss: 0.0034951034
train_loss: 0.0028182624
test_loss: 0.0033984596
train_loss: 0.0028133206
test_loss: 0.003326088
train_loss: 0.0029177275
test_loss: 0.0032958766
train_loss: 0.0029230635
test_loss: 0.0034473774
train_loss: 0.0030684215
test_loss: 0.0035568478
train_loss: 0.0033718667
test_loss: 0.0035185178
train_loss: 0.0029587739
test_loss: 0.0035073983
train_loss: 0.002879326
test_loss: 0.0034426556
train_loss: 0.002886285
test_loss: 0.003563428
train_loss: 0.0028952968
test_loss: 0.0034910107
train_loss: 0.0027492498
test_loss: 0.0035135318
train_loss: 0.0028979182
test_loss: 0.003454611
train_loss: 0.0030358643
test_loss: 0.0035228077
train_loss: 0.002753323
test_loss: 0.0033736704
train_loss: 0.0030144933
test_loss: 0.0034357475
train_loss: 0.0027485294
test_loss: 0.0035915799
train_loss: 0.0029275483
test_loss: 0.003461708
train_loss: 0.0026932012
test_loss: 0.0035387974
train_loss: 0.0029065844
test_loss: 0.003393748
train_loss: 0.002970633
test_loss: 0.0034050585
train_loss: 0.0031778896
test_loss: 0.0035654195
train_loss: 0.003054358
test_loss: 0.0034266133
train_loss: 0.0028854788
test_loss: 0.003346553
train_loss: 0.0028133374
test_loss: 0.0033250407
train_loss: 0.002825835
test_loss: 0.0033496371
train_loss: 0.002874393
test_loss: 0.0033193908
train_loss: 0.002926884
test_loss: 0.003607652
train_loss: 0.003126409
test_loss: 0.003431352
train_loss: 0.0030203417
test_loss: 0.0033926833
train_loss: 0.0028834217
test_loss: 0.003385762
train_loss: 0.0028411853
test_loss: 0.0033201354
train_loss: 0.0031297058
test_loss: 0.003518323
train_loss: 0.0028077648
test_loss: 0.0034006243
train_loss: 0.003007865
test_loss: 0.0033842642
train_loss: 0.0028573652
test_loss: 0.0034196775
train_loss: 0.002902547
test_loss: 0.0034724858
train_loss: 0.002824613
test_loss: 0.0032689595
train_loss: 0.002866327
test_loss: 0.003336344
train_loss: 0.0026882575
test_loss: 0.0032234036
train_loss: 0.0029545038
test_loss: 0.0033085416
train_loss: 0.0029573021
test_loss: 0.00350564
train_loss: 0.002799062
test_loss: 0.0034960934
train_loss: 0.0027994132
test_loss: 0.003284992
train_loss: 0.002702122
test_loss: 0.0033569399
train_loss: 0.0028348344
test_loss: 0.0033491221
train_loss: 0.0029710871
test_loss: 0.003295566
train_loss: 0.003029543
test_loss: 0.0035612963
train_loss: 0.0029697614
test_loss: 0.0034142302
train_loss: 0.002884876
test_loss: 0.003381833
train_loss: 0.0028450328
test_loss: 0.0033320785
train_loss: 0.0027937444
test_loss: 0.003335728
train_loss: 0.002887777
test_loss: 0.0032338463
train_loss: 0.002654045
test_loss: 0.0032966551
train_loss: 0.0029149055
test_loss: 0.0033503133
train_loss: 0.0026438679
test_loss: 0.003198512
train_loss: 0.0026022391
test_loss: 0.0032339573
train_loss: 0.0029360927
test_loss: 0.0031914106
train_loss: 0.0027526254
test_loss: 0.0033213145
train_loss: 0.0026618934
test_loss: 0.0032308914
train_loss: 0.0027489283
test_loss: 0.0033388557
train_loss: 0.003222722
test_loss: 0.0034210582
train_loss: 0.0034299411
test_loss: 0.0035947682
train_loss: 0.002843319
test_loss: 0.0035041643
train_loss: 0.0027061112
test_loss: 0.0033784953
train_loss: 0.003024421
test_loss: 0.0031818687
train_loss: 0.0030919171
test_loss: 0.0035386032
train_loss: 0.0028586753
test_loss: 0.0032383043
train_loss: 0.0027233376
test_loss: 0.0032418056
train_loss: 0.003011302
test_loss: 0.0033342505
train_loss: 0.0027760505
test_loss: 0.003261263
train_loss: 0.0030223336
test_loss: 0.0032019895
train_loss: 0.0026941323
test_loss: 0.0033408583
train_loss: 0.0028003599
test_loss: 0.0031526724
train_loss: 0.0029007941
test_loss: 0.0034161375
train_loss: 0.002626876
test_loss: 0.0033419884
train_loss: 0.002694023
test_loss: 0.003144593
train_loss: 0.00271307
test_loss: 0.003214994
train_loss: 0.0027537274
test_loss: 0.0031517926
train_loss: 0.0028093643
test_loss: 0.0031971983
train_loss: 0.0028385757
test_loss: 0.0032446086
train_loss: 0.0027733813
test_loss: 0.0034759967
train_loss: 0.0028015967
test_loss: 0.0034257513
train_loss: 0.002635348
test_loss: 0.0031476042
train_loss: 0.0023957689
test_loss: 0.0030443403
train_loss: 0.0026956652
test_loss: 0.0031948178
train_loss: 0.0027950779
test_loss: 0.0031661435
train_loss: 0.002696092
test_loss: 0.0032547778
train_loss: 0.0027798621
test_loss: 0.0032376836
train_loss: 0.002688308
test_loss: 0.003332539
train_loss: 0.0028173276
test_loss: 0.0031972623
train_loss: 0.0028132463
test_loss: 0.0033499089
train_loss: 0.0031623738
test_loss: 0.0033269152
train_loss: 0.002786164
test_loss: 0.0033834744
train_loss: 0.0031135888
test_loss: 0.0033840556
train_loss: 0.0026651553
test_loss: 0.0032382691
train_loss: 0.0026890999
test_loss: 0.003319003
train_loss: 0.002792087
test_loss: 0.0032163612
train_loss: 0.0026116234
test_loss: 0.0033806683
train_loss: 0.0026158623
test_loss: 0.0031239614
train_loss: 0.0026314107
test_loss: 0.0032441325
train_loss: 0.0026469356
test_loss: 0.0030629989
train_loss: 0.0025724557
test_loss: 0.00327408
train_loss: 0.0027236014
test_loss: 0.0032804494
train_loss: 0.0026818425
test_loss: 0.0032046246
train_loss: 0.0027574685
test_loss: 0.0032881144
train_loss: 0.0027022338
test_loss: 0.003108547
train_loss: 0.002743667
test_loss: 0.0033077
train_loss: 0.002777342
test_loss: 0.0033761356
train_loss: 0.002554835
test_loss: 0.0032199097
train_loss: 0.0026825909
test_loss: 0.0032697592
train_loss: 0.0026999621
test_loss: 0.0031754095
train_loss: 0.0024217698
test_loss: 0.0033690229
train_loss: 0.0025917653
test_loss: 0.0031233663
train_loss: 0.0028114829
test_loss: 0.0032382866
train_loss: 0.0027476624
test_loss: 0.003409831
train_loss: 0.0028319359
test_loss: 0.003225152
train_loss: 0.0025178567
test_loss: 0.003197765
train_loss: 0.0027305246
test_loss: 0.0032894956
train_loss: 0.002803234
test_loss: 0.0034056862
train_loss: 0.0028627482
test_loss: 0.0032638193
train_loss: 0.0029985865
test_loss: 0.0033122015
train_loss: 0.0028155248
test_loss: 0.0033387826
train_loss: 0.0027036667
test_loss: 0.0032714938
train_loss: 0.0028630737
test_loss: 0.0032813158
train_loss: 0.0026126604
test_loss: 0.0031988195
train_loss: 0.0026209902
test_loss: 0.0032417884
train_loss: 0.0027234002
test_loss: 0.0033010526
train_loss: 0.0026880282
test_loss: 0.0032584174
train_loss: 0.002762279
test_loss: 0.003310852
train_loss: 0.002726587
test_loss: 0.0032861647
train_loss: 0.0026257313
test_loss: 0.003142028
train_loss: 0.0025778664
test_loss: 0.0030626336
train_loss: 0.0025943327
test_loss: 0.0031601323
train_loss: 0.0026659602
test_loss: 0.0032112047
train_loss: 0.0025574851
test_loss: 0.0032067134
train_loss: 0.00271732
test_loss: 0.0032462399
train_loss: 0.0025556935
test_loss: 0.003166267
train_loss: 0.0025719004
test_loss: 0.0032335941
train_loss: 0.0027727224
test_loss: 0.0033520472
train_loss: 0.0026562877
test_loss: 0.0031408158
train_loss: 0.0026923446
test_loss: 0.003292029
train_loss: 0.0025160247
test_loss: 0.0031453564
train_loss: 0.0024189523
test_loss: 0.003115839
train_loss: 0.0026687554
test_loss: 0.0031823325
train_loss: 0.002682491
test_loss: 0.0031937123
train_loss: 0.0025997362
test_loss: 0.003151958
train_loss: 0.0027012008
test_loss: 0.0031147015
train_loss: 0.0025992217
test_loss: 0.003533864
train_loss: 0.0025204343
test_loss: 0.0032185288
train_loss: 0.0027215588
test_loss: 0.0032751097
train_loss: 0.0025348132
test_loss: 0.0031375815
train_loss: 0.0028095096
test_loss: 0.0032115935
train_loss: 0.0025713623
test_loss: 0.0032993476
train_loss: 0.002621413
test_loss: 0.0031954846
train_loss: 0.0024814971
test_loss: 0.0030397547
train_loss: 0.0026260803
test_loss: 0.0031995226
train_loss: 0.0025979038
test_loss: 0.0031914676
train_loss: 0.0025971734
test_loss: 0.003248692
train_loss: 0.0030938676
test_loss: 0.0034732234
train_loss: 0.0026483994
test_loss: 0.0032684952
train_loss: 0.0027586455
test_loss: 0.0033304596
train_loss: 0.0028772866
test_loss: 0.003260725
train_loss: 0.0027161753
test_loss: 0.0031898678
train_loss: 0.002536872
test_loss: 0.0031462116
train_loss: 0.002630613
test_loss: 0.0031388162
train_loss: 0.0025586563
test_loss: 0.003173081
train_loss: 0.0028531898
test_loss: 0.0031996535
train_loss: 0.0027549951
test_loss: 0.0033838912
train_loss: 0.0025846928
test_loss: 0.003104193
train_loss: 0.0024986614
test_loss: 0.0033008936
train_loss: 0.002775758
test_loss: 0.0032895943
train_loss: 0.002772307
test_loss: 0.0033783403
train_loss: 0.002718167
test_loss: 0.0032204606
train_loss: 0.0027224112
test_loss: 0.0030071212
train_loss: 0.0024644127
test_loss: 0.0031549106
train_loss: 0.002532889
test_loss: 0.0030257497
train_loss: 0.0024496533
test_loss: 0.0031925093
train_loss: 0.0023963158
test_loss: 0.0029779105
train_loss: 0.0024233332
test_loss: 0.0030250254
train_loss: 0.002536714
test_loss: 0.003176634
train_loss: 0.0027666967
test_loss: 0.0032432538
train_loss: 0.0024197388
test_loss: 0.003253465
train_loss: 0.0029169878
test_loss: 0.0034578792
train_loss: 0.0027120921
test_loss: 0.0031953193
train_loss: 0.0025132964
test_loss: 0.0030944238
train_loss: 0.002676434
test_loss: 0.0030667614
train_loss: 0.0023834049
test_loss: 0.00330171
train_loss: 0.0024319836
test_loss: 0.003156773
train_loss: 0.0024584215
test_loss: 0.0030539038
train_loss: 0.0027637458
test_loss: 0.0031473078
train_loss: 0.002597585
test_loss: 0.0032146797
train_loss: 0.0026314098
test_loss: 0.0032535386
train_loss: 0.0028311112
test_loss: 0.0031532736
train_loss: 0.0027307014
test_loss: 0.003292303
train_loss: 0.00267726
test_loss: 0.003286467
train_loss: 0.0026124779
test_loss: 0.0033061751
train_loss: 0.0025667772
test_loss: 0.0031463292
train_loss: 0.0025587752
test_loss: 0.0031405352
train_loss: 0.0024749776
test_loss: 0.0030493222
train_loss: 0.0025178455
test_loss: 0.0031446076
train_loss: 0.0024154168
test_loss: 0.0031210517
train_loss: 0.002609696
test_loss: 0.003080377
train_loss: 0.0026693458
test_loss: 0.0031875062
train_loss: 0.0025300256
test_loss: 0.0031683906
train_loss: 0.0024807556
test_loss: 0.003110037
train_loss: 0.0024542338
test_loss: 0.0030096434
train_loss: 0.002450978
test_loss: 0.0029865035
train_loss: 0.0026240128
test_loss: 0.0030784276
train_loss: 0.0025440054
test_loss: 0.0032461074
train_loss: 0.002582653
test_loss: 0.0030958874
train_loss: 0.002752099
test_loss: 0.0031691738
train_loss: 0.00269428
test_loss: 0.0030995032
train_loss: 0.0025932903
test_loss: 0.003092225
train_loss: 0.002448188
test_loss: 0.00313418
train_loss: 0.0025063078
test_loss: 0.0029509466
train_loss: 0.0024693797
test_loss: 0.0029468816
train_loss: 0.00262089
test_loss: 0.003243929
train_loss: 0.002646273
test_loss: 0.003057043
train_loss: 0.0025041103
test_loss: 0.0030433333
train_loss: 0.0029375395
test_loss: 0.0035068288
train_loss: 0.0024414447
test_loss: 0.0029233298
train_loss: 0.002566438
test_loss: 0.0032200178
train_loss: 0.0025737358
test_loss: 0.0033183282
train_loss: 0.0024729944
test_loss: 0.0032712352
train_loss: 0.0027658953
test_loss: 0.003351998
train_loss: 0.0025987923
test_loss: 0.0031846971
train_loss: 0.0026269965
test_loss: 0.0032000847
train_loss: 0.0024411494/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0032126354
train_loss: 0.002560078
test_loss: 0.003026019
train_loss: 0.002644809
test_loss: 0.003232324
train_loss: 0.0023800246
test_loss: 0.0029141563
train_loss: 0.0025498027
test_loss: 0.0029841291
train_loss: 0.0026895276
test_loss: 0.0031211667
train_loss: 0.0027047307
test_loss: 0.0032030554
train_loss: 0.0024833623
test_loss: 0.0031813611
train_loss: 0.0025495898
test_loss: 0.0030673563
train_loss: 0.0027053724
test_loss: 0.0033673234
train_loss: 0.002633778
test_loss: 0.0030812847
train_loss: 0.0025631562
test_loss: 0.0031251484
train_loss: 0.0024574527
test_loss: 0.003230523
train_loss: 0.002821933
test_loss: 0.003120549
train_loss: 0.0025756587
test_loss: 0.0031189083
train_loss: 0.00249699
test_loss: 0.0031774729
train_loss: 0.0026065325
test_loss: 0.0030327952
train_loss: 0.0025449756
test_loss: 0.0031637473
train_loss: 0.0026330429
test_loss: 0.003210316
train_loss: 0.0025113681
test_loss: 0.0030163305
train_loss: 0.002590535
test_loss: 0.003071573
train_loss: 0.0024417685
test_loss: 0.0031393494
train_loss: 0.0023713158
test_loss: 0.0029934049
train_loss: 0.0025528274
test_loss: 0.0029964452
train_loss: 0.0026006075
test_loss: 0.0032278167
train_loss: 0.0024897803
test_loss: 0.0030757496
train_loss: 0.002413476
test_loss: 0.0031063438
train_loss: 0.0025376377
test_loss: 0.0030348764
train_loss: 0.0024660714
test_loss: 0.0030175198
train_loss: 0.002412226
test_loss: 0.0030282724
train_loss: 0.00232203
test_loss: 0.002991953
train_loss: 0.0024197064
test_loss: 0.0031470598
train_loss: 0.0025783249
test_loss: 0.0031458843
train_loss: 0.0024773595
test_loss: 0.0030919118
train_loss: 0.0024919591
test_loss: 0.003150029
train_loss: 0.0025356773
test_loss: 0.0031160566
train_loss: 0.0025923483
test_loss: 0.0031655896
train_loss: 0.0024821889
test_loss: 0.0031022725
train_loss: 0.002557021
test_loss: 0.0029873028
train_loss: 0.002798224
test_loss: 0.0032589764
train_loss: 0.0027342234
test_loss: 0.003073332
train_loss: 0.0024486268
test_loss: 0.0030973046
train_loss: 0.002436045
test_loss: 0.0030808318
train_loss: 0.0029918118
test_loss: 0.0031587463
train_loss: 0.0025038186
test_loss: 0.00307083
train_loss: 0.0024309482
test_loss: 0.002891278
train_loss: 0.0023055123
test_loss: 0.0028691937
train_loss: 0.0022931718
test_loss: 0.0029486648
train_loss: 0.0024666006
test_loss: 0.0031697948
train_loss: 0.002434828
test_loss: 0.0031721594
train_loss: 0.002684794
test_loss: 0.0031193423
train_loss: 0.0025489626
test_loss: 0.0031713264
train_loss: 0.0026219536
test_loss: 0.0031988425
train_loss: 0.0027065915
test_loss: 0.003382226
train_loss: 0.0026149082
test_loss: 0.0031611882
train_loss: 0.002571876
test_loss: 0.003026249
train_loss: 0.0025392056
test_loss: 0.0030589036
train_loss: 0.0024663433
test_loss: 0.0030268452
train_loss: 0.0025131316
test_loss: 0.0030573246
train_loss: 0.0024927997
test_loss: 0.0030460132
train_loss: 0.0024919226
test_loss: 0.0030590286
train_loss: 0.002466178
test_loss: 0.0032317145
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f301d82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f301d8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f302ad048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f301e0e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30215378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f3016ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30157840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300fd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300fdd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300fd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300c4048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30098620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300c46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30056840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300560d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10094488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10094158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10050400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10012378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10039f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e647cf950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e647cfc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e647a8158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64751268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64751ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646fe6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646cc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646ccd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646f6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646cce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e6464d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64672950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646727b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64635378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e645d32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64579268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.20475843e-05
Iter: 2 loss: 1.07282967e-05
Iter: 3 loss: 9.60075886e-06
Iter: 4 loss: 8.76355e-06
Iter: 5 loss: 8.44306851e-06
Iter: 6 loss: 7.98728706e-06
Iter: 7 loss: 7.4197319e-06
Iter: 8 loss: 9.58292458e-06
Iter: 9 loss: 7.28441273e-06
Iter: 10 loss: 6.83092367e-06
Iter: 11 loss: 1.06186853e-05
Iter: 12 loss: 6.80403264e-06
Iter: 13 loss: 6.48051855e-06
Iter: 14 loss: 6.70939426e-06
Iter: 15 loss: 6.27997815e-06
Iter: 16 loss: 5.95777192e-06
Iter: 17 loss: 6.47154593e-06
Iter: 18 loss: 5.80829874e-06
Iter: 19 loss: 5.47589116e-06
Iter: 20 loss: 8.82174572e-06
Iter: 21 loss: 5.46545198e-06
Iter: 22 loss: 5.24184543e-06
Iter: 23 loss: 4.84430711e-06
Iter: 24 loss: 1.47764376e-05
Iter: 25 loss: 4.84428438e-06
Iter: 26 loss: 4.74110402e-06
Iter: 27 loss: 4.66936945e-06
Iter: 28 loss: 4.50187054e-06
Iter: 29 loss: 4.385377e-06
Iter: 30 loss: 4.32525258e-06
Iter: 31 loss: 4.14454962e-06
Iter: 32 loss: 4.28057683e-06
Iter: 33 loss: 4.03414197e-06
Iter: 34 loss: 3.81890459e-06
Iter: 35 loss: 3.86415877e-06
Iter: 36 loss: 3.65972119e-06
Iter: 37 loss: 3.63894878e-06
Iter: 38 loss: 3.55120392e-06
Iter: 39 loss: 3.43443344e-06
Iter: 40 loss: 3.33964408e-06
Iter: 41 loss: 3.30505873e-06
Iter: 42 loss: 3.15753778e-06
Iter: 43 loss: 3.27017437e-06
Iter: 44 loss: 3.06776656e-06
Iter: 45 loss: 3.03495631e-06
Iter: 46 loss: 2.99806379e-06
Iter: 47 loss: 2.95365726e-06
Iter: 48 loss: 2.87242142e-06
Iter: 49 loss: 4.77785488e-06
Iter: 50 loss: 2.87232206e-06
Iter: 51 loss: 2.78791458e-06
Iter: 52 loss: 3.51804e-06
Iter: 53 loss: 2.78344305e-06
Iter: 54 loss: 2.72244324e-06
Iter: 55 loss: 2.91235574e-06
Iter: 56 loss: 2.70460851e-06
Iter: 57 loss: 2.63651282e-06
Iter: 58 loss: 2.62994104e-06
Iter: 59 loss: 2.57990655e-06
Iter: 60 loss: 2.51113829e-06
Iter: 61 loss: 2.82366591e-06
Iter: 62 loss: 2.49793084e-06
Iter: 63 loss: 2.4140179e-06
Iter: 64 loss: 2.75533148e-06
Iter: 65 loss: 2.39547717e-06
Iter: 66 loss: 2.35556877e-06
Iter: 67 loss: 2.31339232e-06
Iter: 68 loss: 2.30621163e-06
Iter: 69 loss: 2.24544442e-06
Iter: 70 loss: 2.40341865e-06
Iter: 71 loss: 2.22477183e-06
Iter: 72 loss: 2.2070526e-06
Iter: 73 loss: 2.19802564e-06
Iter: 74 loss: 2.16815056e-06
Iter: 75 loss: 2.11784618e-06
Iter: 76 loss: 2.1176902e-06
Iter: 77 loss: 2.07874973e-06
Iter: 78 loss: 2.15191017e-06
Iter: 79 loss: 2.06220489e-06
Iter: 80 loss: 2.02366618e-06
Iter: 81 loss: 2.6221287e-06
Iter: 82 loss: 2.02366255e-06
Iter: 83 loss: 1.99591295e-06
Iter: 84 loss: 1.97023201e-06
Iter: 85 loss: 1.96364158e-06
Iter: 86 loss: 1.93588767e-06
Iter: 87 loss: 2.24007727e-06
Iter: 88 loss: 1.93526239e-06
Iter: 89 loss: 1.91058234e-06
Iter: 90 loss: 1.93829646e-06
Iter: 91 loss: 1.89719231e-06
Iter: 92 loss: 1.87156309e-06
Iter: 93 loss: 1.92643438e-06
Iter: 94 loss: 1.86156149e-06
Iter: 95 loss: 1.83688212e-06
Iter: 96 loss: 1.9255026e-06
Iter: 97 loss: 1.83061388e-06
Iter: 98 loss: 1.80538109e-06
Iter: 99 loss: 1.96951805e-06
Iter: 100 loss: 1.80270717e-06
Iter: 101 loss: 1.78597838e-06
Iter: 102 loss: 1.75176069e-06
Iter: 103 loss: 2.35989933e-06
Iter: 104 loss: 1.75104299e-06
Iter: 105 loss: 1.71490774e-06
Iter: 106 loss: 1.82133545e-06
Iter: 107 loss: 1.70372925e-06
Iter: 108 loss: 1.69595592e-06
Iter: 109 loss: 1.68801103e-06
Iter: 110 loss: 1.67285816e-06
Iter: 111 loss: 1.66232689e-06
Iter: 112 loss: 1.65688994e-06
Iter: 113 loss: 1.64021253e-06
Iter: 114 loss: 1.6363706e-06
Iter: 115 loss: 1.62562787e-06
Iter: 116 loss: 1.61658863e-06
Iter: 117 loss: 1.61360481e-06
Iter: 118 loss: 1.60305103e-06
Iter: 119 loss: 1.58066302e-06
Iter: 120 loss: 1.94111522e-06
Iter: 121 loss: 1.57991894e-06
Iter: 122 loss: 1.56510225e-06
Iter: 123 loss: 1.753883e-06
Iter: 124 loss: 1.56503688e-06
Iter: 125 loss: 1.54793599e-06
Iter: 126 loss: 1.53968915e-06
Iter: 127 loss: 1.53146607e-06
Iter: 128 loss: 1.51436882e-06
Iter: 129 loss: 1.63492257e-06
Iter: 130 loss: 1.5128785e-06
Iter: 131 loss: 1.50066421e-06
Iter: 132 loss: 1.55946077e-06
Iter: 133 loss: 1.49848347e-06
Iter: 134 loss: 1.48522565e-06
Iter: 135 loss: 1.50683718e-06
Iter: 136 loss: 1.47913545e-06
Iter: 137 loss: 1.46716707e-06
Iter: 138 loss: 1.4712864e-06
Iter: 139 loss: 1.45866852e-06
Iter: 140 loss: 1.44668047e-06
Iter: 141 loss: 1.43987006e-06
Iter: 142 loss: 1.43469867e-06
Iter: 143 loss: 1.43766715e-06
Iter: 144 loss: 1.42668432e-06
Iter: 145 loss: 1.42073657e-06
Iter: 146 loss: 1.40547809e-06
Iter: 147 loss: 1.53088683e-06
Iter: 148 loss: 1.40277632e-06
Iter: 149 loss: 1.38673431e-06
Iter: 150 loss: 1.4671441e-06
Iter: 151 loss: 1.38406631e-06
Iter: 152 loss: 1.37535835e-06
Iter: 153 loss: 1.37499774e-06
Iter: 154 loss: 1.36811445e-06
Iter: 155 loss: 1.35532036e-06
Iter: 156 loss: 1.63853383e-06
Iter: 157 loss: 1.3552816e-06
Iter: 158 loss: 1.34745403e-06
Iter: 159 loss: 1.44916498e-06
Iter: 160 loss: 1.34741845e-06
Iter: 161 loss: 1.33922572e-06
Iter: 162 loss: 1.3501882e-06
Iter: 163 loss: 1.33510173e-06
Iter: 164 loss: 1.32906678e-06
Iter: 165 loss: 1.32855871e-06
Iter: 166 loss: 1.32411787e-06
Iter: 167 loss: 1.31238835e-06
Iter: 168 loss: 1.36544054e-06
Iter: 169 loss: 1.31012712e-06
Iter: 170 loss: 1.2997973e-06
Iter: 171 loss: 1.32306889e-06
Iter: 172 loss: 1.2958867e-06
Iter: 173 loss: 1.28872193e-06
Iter: 174 loss: 1.29362309e-06
Iter: 175 loss: 1.28430418e-06
Iter: 176 loss: 1.27556814e-06
Iter: 177 loss: 1.28879969e-06
Iter: 178 loss: 1.27144017e-06
Iter: 179 loss: 1.27102862e-06
Iter: 180 loss: 1.26737245e-06
Iter: 181 loss: 1.26496093e-06
Iter: 182 loss: 1.25816371e-06
Iter: 183 loss: 1.2896171e-06
Iter: 184 loss: 1.25569e-06
Iter: 185 loss: 1.2480632e-06
Iter: 186 loss: 1.28360261e-06
Iter: 187 loss: 1.24657504e-06
Iter: 188 loss: 1.2391564e-06
Iter: 189 loss: 1.33258254e-06
Iter: 190 loss: 1.23910354e-06
Iter: 191 loss: 1.23403618e-06
Iter: 192 loss: 1.22424376e-06
Iter: 193 loss: 1.43131433e-06
Iter: 194 loss: 1.22418305e-06
Iter: 195 loss: 1.21736889e-06
Iter: 196 loss: 1.21738663e-06
Iter: 197 loss: 1.2118885e-06
Iter: 198 loss: 1.2372343e-06
Iter: 199 loss: 1.21080677e-06
Iter: 200 loss: 1.20751577e-06
Iter: 201 loss: 1.20309801e-06
Iter: 202 loss: 1.20285802e-06
Iter: 203 loss: 1.19759238e-06
Iter: 204 loss: 1.19753395e-06
Iter: 205 loss: 1.19457241e-06
Iter: 206 loss: 1.19063975e-06
Iter: 207 loss: 1.19039487e-06
Iter: 208 loss: 1.18433707e-06
Iter: 209 loss: 1.19288939e-06
Iter: 210 loss: 1.18139167e-06
Iter: 211 loss: 1.17410252e-06
Iter: 212 loss: 1.19245806e-06
Iter: 213 loss: 1.17158902e-06
Iter: 214 loss: 1.16682452e-06
Iter: 215 loss: 1.16640706e-06
Iter: 216 loss: 1.16383706e-06
Iter: 217 loss: 1.15817068e-06
Iter: 218 loss: 1.24125063e-06
Iter: 219 loss: 1.15792045e-06
Iter: 220 loss: 1.153021e-06
Iter: 221 loss: 1.18459025e-06
Iter: 222 loss: 1.15247281e-06
Iter: 223 loss: 1.14899831e-06
Iter: 224 loss: 1.14901673e-06
Iter: 225 loss: 1.14680677e-06
Iter: 226 loss: 1.14119121e-06
Iter: 227 loss: 1.18980392e-06
Iter: 228 loss: 1.14029308e-06
Iter: 229 loss: 1.13488841e-06
Iter: 230 loss: 1.20012896e-06
Iter: 231 loss: 1.13481724e-06
Iter: 232 loss: 1.12963914e-06
Iter: 233 loss: 1.14993168e-06
Iter: 234 loss: 1.12850535e-06
Iter: 235 loss: 1.12510497e-06
Iter: 236 loss: 1.12259909e-06
Iter: 237 loss: 1.12143118e-06
Iter: 238 loss: 1.11742122e-06
Iter: 239 loss: 1.11732288e-06
Iter: 240 loss: 1.11504505e-06
Iter: 241 loss: 1.11217935e-06
Iter: 242 loss: 1.11196937e-06
Iter: 243 loss: 1.10903034e-06
Iter: 244 loss: 1.11743475e-06
Iter: 245 loss: 1.10817564e-06
Iter: 246 loss: 1.10476765e-06
Iter: 247 loss: 1.11935583e-06
Iter: 248 loss: 1.10402993e-06
Iter: 249 loss: 1.10120106e-06
Iter: 250 loss: 1.13610395e-06
Iter: 251 loss: 1.10118481e-06
Iter: 252 loss: 1.09937264e-06
Iter: 253 loss: 1.09437758e-06
Iter: 254 loss: 1.12036093e-06
Iter: 255 loss: 1.09275402e-06
Iter: 256 loss: 1.08749214e-06
Iter: 257 loss: 1.15216324e-06
Iter: 258 loss: 1.08741472e-06
Iter: 259 loss: 1.08416725e-06
Iter: 260 loss: 1.13504871e-06
Iter: 261 loss: 1.0841718e-06
Iter: 262 loss: 1.08199515e-06
Iter: 263 loss: 1.07757057e-06
Iter: 264 loss: 1.15942134e-06
Iter: 265 loss: 1.07748133e-06
Iter: 266 loss: 1.07489404e-06
Iter: 267 loss: 1.10307724e-06
Iter: 268 loss: 1.07484777e-06
Iter: 269 loss: 1.07211781e-06
Iter: 270 loss: 1.08216e-06
Iter: 271 loss: 1.07144217e-06
Iter: 272 loss: 1.06949744e-06
Iter: 273 loss: 1.0674446e-06
Iter: 274 loss: 1.06706091e-06
Iter: 275 loss: 1.0640648e-06
Iter: 276 loss: 1.06407674e-06
Iter: 277 loss: 1.06195876e-06
Iter: 278 loss: 1.05840888e-06
Iter: 279 loss: 1.058409e-06
Iter: 280 loss: 1.05409549e-06
Iter: 281 loss: 1.05436379e-06
Iter: 282 loss: 1.05064498e-06
Iter: 283 loss: 1.05076776e-06
Iter: 284 loss: 1.04845958e-06
Iter: 285 loss: 1.04695312e-06
Iter: 286 loss: 1.05032268e-06
Iter: 287 loss: 1.04634546e-06
Iter: 288 loss: 1.04475464e-06
Iter: 289 loss: 1.04200194e-06
Iter: 290 loss: 1.04201672e-06
Iter: 291 loss: 1.03918035e-06
Iter: 292 loss: 1.05110621e-06
Iter: 293 loss: 1.03858952e-06
Iter: 294 loss: 1.036409e-06
Iter: 295 loss: 1.03641401e-06
Iter: 296 loss: 1.03492653e-06
Iter: 297 loss: 1.03164211e-06
Iter: 298 loss: 1.08116103e-06
Iter: 299 loss: 1.03153377e-06
Iter: 300 loss: 1.02828812e-06
Iter: 301 loss: 1.0423056e-06
Iter: 302 loss: 1.02760168e-06
Iter: 303 loss: 1.02464878e-06
Iter: 304 loss: 1.06251821e-06
Iter: 305 loss: 1.024624e-06
Iter: 306 loss: 1.02299077e-06
Iter: 307 loss: 1.02118599e-06
Iter: 308 loss: 1.0209335e-06
Iter: 309 loss: 1.01933176e-06
Iter: 310 loss: 1.01923308e-06
Iter: 311 loss: 1.01785849e-06
Iter: 312 loss: 1.01501041e-06
Iter: 313 loss: 1.06120854e-06
Iter: 314 loss: 1.01494095e-06
Iter: 315 loss: 1.01266767e-06
Iter: 316 loss: 1.02043202e-06
Iter: 317 loss: 1.01208389e-06
Iter: 318 loss: 1.01018236e-06
Iter: 319 loss: 1.03604202e-06
Iter: 320 loss: 1.01018134e-06
Iter: 321 loss: 1.00811917e-06
Iter: 322 loss: 1.00815328e-06
Iter: 323 loss: 1.00646434e-06
Iter: 324 loss: 1.00431521e-06
Iter: 325 loss: 1.00688862e-06
Iter: 326 loss: 1.00315833e-06
Iter: 327 loss: 1.00069826e-06
Iter: 328 loss: 1.0058211e-06
Iter: 329 loss: 9.99724193e-07
Iter: 330 loss: 9.98282644e-07
Iter: 331 loss: 9.98156338e-07
Iter: 332 loss: 9.97059374e-07
Iter: 333 loss: 9.94639322e-07
Iter: 334 loss: 1.02983267e-06
Iter: 335 loss: 9.94525e-07
Iter: 336 loss: 9.9239935e-07
Iter: 337 loss: 1.0041515e-06
Iter: 338 loss: 9.92098e-07
Iter: 339 loss: 9.90394255e-07
Iter: 340 loss: 1.01170406e-06
Iter: 341 loss: 9.9039562e-07
Iter: 342 loss: 9.88985903e-07
Iter: 343 loss: 9.86529585e-07
Iter: 344 loss: 9.86527311e-07
Iter: 345 loss: 9.84743224e-07
Iter: 346 loss: 9.84674671e-07
Iter: 347 loss: 9.82872621e-07
Iter: 348 loss: 9.79462698e-07
Iter: 349 loss: 1.05569461e-06
Iter: 350 loss: 9.79477818e-07
Iter: 351 loss: 9.77143e-07
Iter: 352 loss: 9.84179451e-07
Iter: 353 loss: 9.76425554e-07
Iter: 354 loss: 9.75107e-07
Iter: 355 loss: 9.74920795e-07
Iter: 356 loss: 9.73625674e-07
Iter: 357 loss: 9.72241423e-07
Iter: 358 loss: 9.72034741e-07
Iter: 359 loss: 9.70503379e-07
Iter: 360 loss: 9.72481757e-07
Iter: 361 loss: 9.69742587e-07
Iter: 362 loss: 9.67872438e-07
Iter: 363 loss: 9.76287e-07
Iter: 364 loss: 9.67468509e-07
Iter: 365 loss: 9.66048333e-07
Iter: 366 loss: 9.83036216e-07
Iter: 367 loss: 9.66036851e-07
Iter: 368 loss: 9.65064146e-07
Iter: 369 loss: 9.62922172e-07
Iter: 370 loss: 9.96670678e-07
Iter: 371 loss: 9.62870331e-07
Iter: 372 loss: 9.60935381e-07
Iter: 373 loss: 9.73162e-07
Iter: 374 loss: 9.60731541e-07
Iter: 375 loss: 9.59026806e-07
Iter: 376 loss: 9.76691581e-07
Iter: 377 loss: 9.58983492e-07
Iter: 378 loss: 9.58104238e-07
Iter: 379 loss: 9.57691555e-07
Iter: 380 loss: 9.57237262e-07
Iter: 381 loss: 9.56138138e-07
Iter: 382 loss: 9.69541702e-07
Iter: 383 loss: 9.56112444e-07
Iter: 384 loss: 9.5504015e-07
Iter: 385 loss: 9.53076494e-07
Iter: 386 loss: 9.98296e-07
Iter: 387 loss: 9.5307314e-07
Iter: 388 loss: 9.51441962e-07
Iter: 389 loss: 9.54883717e-07
Iter: 390 loss: 9.50838626e-07
Iter: 391 loss: 9.4944761e-07
Iter: 392 loss: 9.49391733e-07
Iter: 393 loss: 9.48482352e-07
Iter: 394 loss: 9.46378236e-07
Iter: 395 loss: 9.72584075e-07
Iter: 396 loss: 9.46225e-07
Iter: 397 loss: 9.44400767e-07
Iter: 398 loss: 9.58626742e-07
Iter: 399 loss: 9.44276053e-07
Iter: 400 loss: 9.42875886e-07
Iter: 401 loss: 9.5145225e-07
Iter: 402 loss: 9.42692964e-07
Iter: 403 loss: 9.41530857e-07
Iter: 404 loss: 9.45919851e-07
Iter: 405 loss: 9.41229644e-07
Iter: 406 loss: 9.40205382e-07
Iter: 407 loss: 9.3888076e-07
Iter: 408 loss: 9.3883034e-07
Iter: 409 loss: 9.36788922e-07
Iter: 410 loss: 9.40335e-07
Iter: 411 loss: 9.35840831e-07
Iter: 412 loss: 9.34671903e-07
Iter: 413 loss: 9.34489947e-07
Iter: 414 loss: 9.33620413e-07
Iter: 415 loss: 9.31887143e-07
Iter: 416 loss: 9.64431592e-07
Iter: 417 loss: 9.31864065e-07
Iter: 418 loss: 9.30779834e-07
Iter: 419 loss: 9.30615101e-07
Iter: 420 loss: 9.29722262e-07
Iter: 421 loss: 9.28192321e-07
Iter: 422 loss: 9.28185614e-07
Iter: 423 loss: 9.27033113e-07
Iter: 424 loss: 9.3496908e-07
Iter: 425 loss: 9.26901691e-07
Iter: 426 loss: 9.25723612e-07
Iter: 427 loss: 9.34999889e-07
Iter: 428 loss: 9.25642837e-07
Iter: 429 loss: 9.24928713e-07
Iter: 430 loss: 9.23368589e-07
Iter: 431 loss: 9.47222532e-07
Iter: 432 loss: 9.23295602e-07
Iter: 433 loss: 9.21843309e-07
Iter: 434 loss: 9.29808095e-07
Iter: 435 loss: 9.21674427e-07
Iter: 436 loss: 9.20089576e-07
Iter: 437 loss: 9.28043903e-07
Iter: 438 loss: 9.19841398e-07
Iter: 439 loss: 9.18676733e-07
Iter: 440 loss: 9.21139531e-07
Iter: 441 loss: 9.18234264e-07
Iter: 442 loss: 9.17117632e-07
Iter: 443 loss: 9.16168e-07
Iter: 444 loss: 9.15793635e-07
Iter: 445 loss: 9.14305588e-07
Iter: 446 loss: 9.27926578e-07
Iter: 447 loss: 9.1427404e-07
Iter: 448 loss: 9.13285589e-07
Iter: 449 loss: 9.24435085e-07
Iter: 450 loss: 9.13268e-07
Iter: 451 loss: 9.1255805e-07
Iter: 452 loss: 9.10904362e-07
Iter: 453 loss: 9.32707394e-07
Iter: 454 loss: 9.10812503e-07
Iter: 455 loss: 9.09750838e-07
Iter: 456 loss: 9.0956928e-07
Iter: 457 loss: 9.08849131e-07
Iter: 458 loss: 9.07273659e-07
Iter: 459 loss: 9.33617e-07
Iter: 460 loss: 9.07263029e-07
Iter: 461 loss: 9.06135767e-07
Iter: 462 loss: 9.06131334e-07
Iter: 463 loss: 9.04861622e-07
Iter: 464 loss: 9.06246e-07
Iter: 465 loss: 9.04200363e-07
Iter: 466 loss: 9.03314799e-07
Iter: 467 loss: 9.02596867e-07
Iter: 468 loss: 9.02321744e-07
Iter: 469 loss: 9.01238081e-07
Iter: 470 loss: 9.06204605e-07
Iter: 471 loss: 9.01024634e-07
Iter: 472 loss: 8.99725364e-07
Iter: 473 loss: 9.08971799e-07
Iter: 474 loss: 8.99615941e-07
Iter: 475 loss: 8.98804387e-07
Iter: 476 loss: 8.98108397e-07
Iter: 477 loss: 8.97889095e-07
Iter: 478 loss: 8.96440781e-07
Iter: 479 loss: 8.98965141e-07
Iter: 480 loss: 8.95753033e-07
Iter: 481 loss: 8.94562163e-07
Iter: 482 loss: 9.02938041e-07
Iter: 483 loss: 8.94429036e-07
Iter: 484 loss: 8.93242941e-07
Iter: 485 loss: 8.98788244e-07
Iter: 486 loss: 8.93023298e-07
Iter: 487 loss: 8.92223113e-07
Iter: 488 loss: 8.92562923e-07
Iter: 489 loss: 8.91691286e-07
Iter: 490 loss: 8.90903e-07
Iter: 491 loss: 9.02205215e-07
Iter: 492 loss: 8.90884621e-07
Iter: 493 loss: 8.90337105e-07
Iter: 494 loss: 8.89031696e-07
Iter: 495 loss: 9.07025878e-07
Iter: 496 loss: 8.88950694e-07
Iter: 497 loss: 8.88086504e-07
Iter: 498 loss: 8.88044042e-07
Iter: 499 loss: 8.87113515e-07
Iter: 500 loss: 8.88767204e-07
Iter: 501 loss: 8.86705834e-07
Iter: 502 loss: 8.85926966e-07
Iter: 503 loss: 8.83931421e-07
Iter: 504 loss: 8.96882625e-07
Iter: 505 loss: 8.83434723e-07
Iter: 506 loss: 8.82917e-07
Iter: 507 loss: 8.82254824e-07
Iter: 508 loss: 8.81350331e-07
Iter: 509 loss: 8.85073518e-07
Iter: 510 loss: 8.81151664e-07
Iter: 511 loss: 8.80575612e-07
Iter: 512 loss: 8.79671347e-07
Iter: 513 loss: 8.79650543e-07
Iter: 514 loss: 8.78607295e-07
Iter: 515 loss: 8.85073518e-07
Iter: 516 loss: 8.78478772e-07
Iter: 517 loss: 8.77481739e-07
Iter: 518 loss: 8.80662242e-07
Iter: 519 loss: 8.77203547e-07
Iter: 520 loss: 8.76102035e-07
Iter: 521 loss: 8.8191166e-07
Iter: 522 loss: 8.7592025e-07
Iter: 523 loss: 8.75141836e-07
Iter: 524 loss: 8.74489217e-07
Iter: 525 loss: 8.74277305e-07
Iter: 526 loss: 8.72937562e-07
Iter: 527 loss: 8.86430371e-07
Iter: 528 loss: 8.72884812e-07
Iter: 529 loss: 8.72220085e-07
Iter: 530 loss: 8.70939118e-07
Iter: 531 loss: 8.98156259e-07
Iter: 532 loss: 8.70945e-07
Iter: 533 loss: 8.70626309e-07
Iter: 534 loss: 8.70313215e-07
Iter: 535 loss: 8.69672249e-07
Iter: 536 loss: 8.69330904e-07
Iter: 537 loss: 8.69121266e-07
Iter: 538 loss: 8.68396569e-07
Iter: 539 loss: 8.67656581e-07
Iter: 540 loss: 8.67529138e-07
Iter: 541 loss: 8.66751066e-07
Iter: 542 loss: 8.66729863e-07
Iter: 543 loss: 8.66200594e-07
Iter: 544 loss: 8.65175593e-07
Iter: 545 loss: 8.86459418e-07
Iter: 546 loss: 8.65172296e-07
Iter: 547 loss: 8.63836362e-07
Iter: 548 loss: 8.65780407e-07
Iter: 549 loss: 8.63198e-07
Iter: 550 loss: 8.62027548e-07
Iter: 551 loss: 8.65568438e-07
Iter: 552 loss: 8.61632486e-07
Iter: 553 loss: 8.60752607e-07
Iter: 554 loss: 8.73235251e-07
Iter: 555 loss: 8.6075022e-07
Iter: 556 loss: 8.59844704e-07
Iter: 557 loss: 8.61714454e-07
Iter: 558 loss: 8.59445038e-07
Iter: 559 loss: 8.58873591e-07
Iter: 560 loss: 8.60174623e-07
Iter: 561 loss: 8.58654346e-07
Iter: 562 loss: 8.57818634e-07
Iter: 563 loss: 8.59308784e-07
Iter: 564 loss: 8.57440114e-07
Iter: 565 loss: 8.56547103e-07
Iter: 566 loss: 8.56162956e-07
Iter: 567 loss: 8.55708322e-07
Iter: 568 loss: 8.54969471e-07
Iter: 569 loss: 8.54952248e-07
Iter: 570 loss: 8.54389043e-07
Iter: 571 loss: 8.56728093e-07
Iter: 572 loss: 8.54253415e-07
Iter: 573 loss: 8.53773713e-07
Iter: 574 loss: 8.53023323e-07
Iter: 575 loss: 8.53001e-07
Iter: 576 loss: 8.52147082e-07
Iter: 577 loss: 8.53725339e-07
Iter: 578 loss: 8.51801133e-07
Iter: 579 loss: 8.51137088e-07
Iter: 580 loss: 8.51098946e-07
Iter: 581 loss: 8.5058889e-07
Iter: 582 loss: 8.49456512e-07
Iter: 583 loss: 8.62537149e-07
Iter: 584 loss: 8.49349931e-07
Iter: 585 loss: 8.48234e-07
Iter: 586 loss: 8.52005542e-07
Iter: 587 loss: 8.47896672e-07
Iter: 588 loss: 8.46767193e-07
Iter: 589 loss: 8.5059412e-07
Iter: 590 loss: 8.4649696e-07
Iter: 591 loss: 8.45991053e-07
Iter: 592 loss: 8.45890952e-07
Iter: 593 loss: 8.45397608e-07
Iter: 594 loss: 8.44536032e-07
Iter: 595 loss: 8.44548538e-07
Iter: 596 loss: 8.43997611e-07
Iter: 597 loss: 8.43989255e-07
Iter: 598 loss: 8.43429063e-07
Iter: 599 loss: 8.42460736e-07
Iter: 600 loss: 8.42476425e-07
Iter: 601 loss: 8.41549877e-07
Iter: 602 loss: 8.45297564e-07
Iter: 603 loss: 8.41367864e-07
Iter: 604 loss: 8.40289886e-07
Iter: 605 loss: 8.47998422e-07
Iter: 606 loss: 8.40223493e-07
Iter: 607 loss: 8.39517497e-07
Iter: 608 loss: 8.38600158e-07
Iter: 609 loss: 8.38527626e-07
Iter: 610 loss: 8.37586754e-07
Iter: 611 loss: 8.44689851e-07
Iter: 612 loss: 8.37515756e-07
Iter: 613 loss: 8.36798563e-07
Iter: 614 loss: 8.40188534e-07
Iter: 615 loss: 8.36690901e-07
Iter: 616 loss: 8.35836886e-07
Iter: 617 loss: 8.36806862e-07
Iter: 618 loss: 8.35384071e-07
Iter: 619 loss: 8.34814841e-07
Iter: 620 loss: 8.33676381e-07
Iter: 621 loss: 8.55742087e-07
Iter: 622 loss: 8.33667286e-07
Iter: 623 loss: 8.32488809e-07
Iter: 624 loss: 8.39207246e-07
Iter: 625 loss: 8.32328055e-07
Iter: 626 loss: 8.31689647e-07
Iter: 627 loss: 8.31618195e-07
Iter: 628 loss: 8.31007185e-07
Iter: 629 loss: 8.30376166e-07
Iter: 630 loss: 8.30304657e-07
Iter: 631 loss: 8.29509645e-07
Iter: 632 loss: 8.34257833e-07
Iter: 633 loss: 8.2938061e-07
Iter: 634 loss: 8.28540919e-07
Iter: 635 loss: 8.30037436e-07
Iter: 636 loss: 8.28162456e-07
Iter: 637 loss: 8.27589929e-07
Iter: 638 loss: 8.27927352e-07
Iter: 639 loss: 8.27223346e-07
Iter: 640 loss: 8.26513e-07
Iter: 641 loss: 8.26507289e-07
Iter: 642 loss: 8.2614838e-07
Iter: 643 loss: 8.25328414e-07
Iter: 644 loss: 8.35152036e-07
Iter: 645 loss: 8.2526833e-07
Iter: 646 loss: 8.24248787e-07
Iter: 647 loss: 8.28283248e-07
Iter: 648 loss: 8.24051938e-07
Iter: 649 loss: 8.23384767e-07
Iter: 650 loss: 8.23383232e-07
Iter: 651 loss: 8.22875279e-07
Iter: 652 loss: 8.228958e-07
Iter: 653 loss: 8.22484935e-07
Iter: 654 loss: 8.21790593e-07
Iter: 655 loss: 8.21685376e-07
Iter: 656 loss: 8.21185267e-07
Iter: 657 loss: 8.20417483e-07
Iter: 658 loss: 8.22158199e-07
Iter: 659 loss: 8.20100126e-07
Iter: 660 loss: 8.19356273e-07
Iter: 661 loss: 8.21448396e-07
Iter: 662 loss: 8.19138165e-07
Iter: 663 loss: 8.18442743e-07
Iter: 664 loss: 8.18427225e-07
Iter: 665 loss: 8.18000558e-07
Iter: 666 loss: 8.17034106e-07
Iter: 667 loss: 8.29179385e-07
Iter: 668 loss: 8.16960664e-07
Iter: 669 loss: 8.16267743e-07
Iter: 670 loss: 8.16224428e-07
Iter: 671 loss: 8.15675094e-07
Iter: 672 loss: 8.15259398e-07
Iter: 673 loss: 8.15058115e-07
Iter: 674 loss: 8.14615419e-07
Iter: 675 loss: 8.14596945e-07
Iter: 676 loss: 8.14174427e-07
Iter: 677 loss: 8.13272038e-07
Iter: 678 loss: 8.25364054e-07
Iter: 679 loss: 8.13192457e-07
Iter: 680 loss: 8.12569965e-07
Iter: 681 loss: 8.19832394e-07
Iter: 682 loss: 8.12542623e-07
Iter: 683 loss: 8.11963787e-07
Iter: 684 loss: 8.17965656e-07
Iter: 685 loss: 8.11958557e-07
Iter: 686 loss: 8.11520067e-07
Iter: 687 loss: 8.10924121e-07
Iter: 688 loss: 8.10893312e-07
Iter: 689 loss: 8.10035772e-07
Iter: 690 loss: 8.10240408e-07
Iter: 691 loss: 8.09423625e-07
Iter: 692 loss: 8.0837026e-07
Iter: 693 loss: 8.12490043e-07
Iter: 694 loss: 8.08134246e-07
Iter: 695 loss: 8.0732724e-07
Iter: 696 loss: 8.14849727e-07
Iter: 697 loss: 8.07252e-07
Iter: 698 loss: 8.06422861e-07
Iter: 699 loss: 8.10100914e-07
Iter: 700 loss: 8.06249432e-07
Iter: 701 loss: 8.05854029e-07
Iter: 702 loss: 8.0537643e-07
Iter: 703 loss: 8.05324817e-07
Iter: 704 loss: 8.04598358e-07
Iter: 705 loss: 8.13325869e-07
Iter: 706 loss: 8.04587501e-07
Iter: 707 loss: 8.04177e-07
Iter: 708 loss: 8.04485808e-07
Iter: 709 loss: 8.03935166e-07
Iter: 710 loss: 8.03355192e-07
Iter: 711 loss: 8.06749028e-07
Iter: 712 loss: 8.03292835e-07
Iter: 713 loss: 8.02827e-07
Iter: 714 loss: 8.01715828e-07
Iter: 715 loss: 8.14698467e-07
Iter: 716 loss: 8.01623344e-07
Iter: 717 loss: 8.01170813e-07
Iter: 718 loss: 8.01011652e-07
Iter: 719 loss: 8.00429916e-07
Iter: 720 loss: 7.9998108e-07
Iter: 721 loss: 7.99803274e-07
Iter: 722 loss: 7.99060558e-07
Iter: 723 loss: 8.00663713e-07
Iter: 724 loss: 7.98796e-07
Iter: 725 loss: 7.98046187e-07
Iter: 726 loss: 7.99547934e-07
Iter: 727 loss: 7.97765324e-07
Iter: 728 loss: 7.97001064e-07
Iter: 729 loss: 7.9725794e-07
Iter: 730 loss: 7.96481e-07
Iter: 731 loss: 7.96095378e-07
Iter: 732 loss: 7.95972085e-07
Iter: 733 loss: 7.95521657e-07
Iter: 734 loss: 7.94826406e-07
Iter: 735 loss: 7.94827372e-07
Iter: 736 loss: 7.94162361e-07
Iter: 737 loss: 7.98191422e-07
Iter: 738 loss: 7.94069706e-07
Iter: 739 loss: 7.93349727e-07
Iter: 740 loss: 7.95502103e-07
Iter: 741 loss: 7.93126787e-07
Iter: 742 loss: 7.92737069e-07
Iter: 743 loss: 7.96075e-07
Iter: 744 loss: 7.92708533e-07
Iter: 745 loss: 7.92315745e-07
Iter: 746 loss: 7.92277547e-07
Iter: 747 loss: 7.9199981e-07
Iter: 748 loss: 7.91533807e-07
Iter: 749 loss: 7.90948775e-07
Iter: 750 loss: 7.90883291e-07
Iter: 751 loss: 7.90144441e-07
Iter: 752 loss: 7.90136482e-07
Iter: 753 loss: 7.89758076e-07
Iter: 754 loss: 7.89070214e-07
Iter: 755 loss: 7.89073681e-07
Iter: 756 loss: 7.88218586e-07
Iter: 757 loss: 7.89541218e-07
Iter: 758 loss: 7.87832448e-07
Iter: 759 loss: 7.86881515e-07
Iter: 760 loss: 7.9203312e-07
Iter: 761 loss: 7.86749752e-07
Iter: 762 loss: 7.86121745e-07
Iter: 763 loss: 7.87361955e-07
Iter: 764 loss: 7.85843895e-07
Iter: 765 loss: 7.85435816e-07
Iter: 766 loss: 7.85394207e-07
Iter: 767 loss: 7.85099246e-07
Iter: 768 loss: 7.84389044e-07
Iter: 769 loss: 7.93985805e-07
Iter: 770 loss: 7.84310373e-07
Iter: 771 loss: 7.83687e-07
Iter: 772 loss: 7.92321259e-07
Iter: 773 loss: 7.83680207e-07
Iter: 774 loss: 7.83063797e-07
Iter: 775 loss: 7.83704195e-07
Iter: 776 loss: 7.82719894e-07
Iter: 777 loss: 7.82212226e-07
Iter: 778 loss: 7.8576e-07
Iter: 779 loss: 7.82158963e-07
Iter: 780 loss: 7.81534084e-07
Iter: 781 loss: 7.81158064e-07
Iter: 782 loss: 7.80920857e-07
Iter: 783 loss: 7.8036112e-07
Iter: 784 loss: 7.80531366e-07
Iter: 785 loss: 7.7998925e-07
Iter: 786 loss: 7.79565653e-07
Iter: 787 loss: 7.79473794e-07
Iter: 788 loss: 7.79264496e-07
Iter: 789 loss: 7.7867918e-07
Iter: 790 loss: 7.84992e-07
Iter: 791 loss: 7.78621313e-07
Iter: 792 loss: 7.77943569e-07
Iter: 793 loss: 7.78918775e-07
Iter: 794 loss: 7.77616151e-07
Iter: 795 loss: 7.76870309e-07
Iter: 796 loss: 7.81926e-07
Iter: 797 loss: 7.76811703e-07
Iter: 798 loss: 7.76072852e-07
Iter: 799 loss: 7.78379444e-07
Iter: 800 loss: 7.75872195e-07
Iter: 801 loss: 7.75144e-07
Iter: 802 loss: 7.80307232e-07
Iter: 803 loss: 7.75088665e-07
Iter: 804 loss: 7.7464847e-07
Iter: 805 loss: 7.74045e-07
Iter: 806 loss: 7.74019099e-07
Iter: 807 loss: 7.73476074e-07
Iter: 808 loss: 7.73475961e-07
Iter: 809 loss: 7.72928388e-07
Iter: 810 loss: 7.72864041e-07
Iter: 811 loss: 7.72482736e-07
Iter: 812 loss: 7.72105636e-07
Iter: 813 loss: 7.72103249e-07
Iter: 814 loss: 7.71850409e-07
Iter: 815 loss: 7.71184318e-07
Iter: 816 loss: 7.76198306e-07
Iter: 817 loss: 7.71066e-07
Iter: 818 loss: 7.70295458e-07
Iter: 819 loss: 7.72281226e-07
Iter: 820 loss: 7.70042504e-07
Iter: 821 loss: 7.69490498e-07
Iter: 822 loss: 7.69467647e-07
Iter: 823 loss: 7.6904746e-07
Iter: 824 loss: 7.68531493e-07
Iter: 825 loss: 7.68484938e-07
Iter: 826 loss: 7.67812e-07
Iter: 827 loss: 7.67576807e-07
Iter: 828 loss: 7.6721426e-07
Iter: 829 loss: 7.66559083e-07
Iter: 830 loss: 7.75023523e-07
Iter: 831 loss: 7.66534299e-07
Iter: 832 loss: 7.6602953e-07
Iter: 833 loss: 7.70227246e-07
Iter: 834 loss: 7.65986215e-07
Iter: 835 loss: 7.65551704e-07
Iter: 836 loss: 7.66756045e-07
Iter: 837 loss: 7.654005e-07
Iter: 838 loss: 7.65022435e-07
Iter: 839 loss: 7.64584797e-07
Iter: 840 loss: 7.64525339e-07
Iter: 841 loss: 7.63949856e-07
Iter: 842 loss: 7.71859845e-07
Iter: 843 loss: 7.6395105e-07
Iter: 844 loss: 7.63461344e-07
Iter: 845 loss: 7.64461447e-07
Iter: 846 loss: 7.63277399e-07
Iter: 847 loss: 7.62867671e-07
Iter: 848 loss: 7.65323307e-07
Iter: 849 loss: 7.62843399e-07
Iter: 850 loss: 7.62451123e-07
Iter: 851 loss: 7.61768774e-07
Iter: 852 loss: 7.78507456e-07
Iter: 853 loss: 7.61766955e-07
Iter: 854 loss: 7.61249794e-07
Iter: 855 loss: 7.6202582e-07
Iter: 856 loss: 7.61002525e-07
Iter: 857 loss: 7.60291641e-07
Iter: 858 loss: 7.6143408e-07
Iter: 859 loss: 7.60008447e-07
Iter: 860 loss: 7.5966318e-07
Iter: 861 loss: 7.5958792e-07
Iter: 862 loss: 7.59147497e-07
Iter: 863 loss: 7.58480212e-07
Iter: 864 loss: 7.58486863e-07
Iter: 865 loss: 7.57837938e-07
Iter: 866 loss: 7.57466751e-07
Iter: 867 loss: 7.57183784e-07
Iter: 868 loss: 7.5667765e-07
Iter: 869 loss: 7.56629163e-07
Iter: 870 loss: 7.56164582e-07
Iter: 871 loss: 7.58860665e-07
Iter: 872 loss: 7.5609978e-07
Iter: 873 loss: 7.55761562e-07
Iter: 874 loss: 7.55472456e-07
Iter: 875 loss: 7.55368092e-07
Iter: 876 loss: 7.54963935e-07
Iter: 877 loss: 7.58856913e-07
Iter: 878 loss: 7.54916243e-07
Iter: 879 loss: 7.54536757e-07
Iter: 880 loss: 7.55785209e-07
Iter: 881 loss: 7.54407097e-07
Iter: 882 loss: 7.54006635e-07
Iter: 883 loss: 7.54449047e-07
Iter: 884 loss: 7.53811491e-07
Iter: 885 loss: 7.53377549e-07
Iter: 886 loss: 7.5612337e-07
Iter: 887 loss: 7.53354925e-07
Iter: 888 loss: 7.53020572e-07
Iter: 889 loss: 7.52241e-07
Iter: 890 loss: 7.62030709e-07
Iter: 891 loss: 7.52197479e-07
Iter: 892 loss: 7.51506718e-07
Iter: 893 loss: 7.56591248e-07
Iter: 894 loss: 7.51427706e-07
Iter: 895 loss: 7.50965796e-07
Iter: 896 loss: 7.51984317e-07
Iter: 897 loss: 7.50743e-07
Iter: 898 loss: 7.50461879e-07
Iter: 899 loss: 7.50445054e-07
Iter: 900 loss: 7.50158e-07
Iter: 901 loss: 7.49654248e-07
Iter: 902 loss: 7.49649871e-07
Iter: 903 loss: 7.49185915e-07
Iter: 904 loss: 7.48891921e-07
Iter: 905 loss: 7.48747652e-07
Iter: 906 loss: 7.48253115e-07
Iter: 907 loss: 7.48211619e-07
Iter: 908 loss: 7.47757213e-07
Iter: 909 loss: 7.47432068e-07
Iter: 910 loss: 7.47305648e-07
Iter: 911 loss: 7.46811907e-07
Iter: 912 loss: 7.46586693e-07
Iter: 913 loss: 7.46337321e-07
Iter: 914 loss: 7.46074306e-07
Iter: 915 loss: 7.459887e-07
Iter: 916 loss: 7.45643661e-07
Iter: 917 loss: 7.45969487e-07
Iter: 918 loss: 7.4544414e-07
Iter: 919 loss: 7.45081365e-07
Iter: 920 loss: 7.45795091e-07
Iter: 921 loss: 7.44918907e-07
Iter: 922 loss: 7.44442332e-07
Iter: 923 loss: 7.4541822e-07
Iter: 924 loss: 7.44266629e-07
Iter: 925 loss: 7.43760324e-07
Iter: 926 loss: 7.44115823e-07
Iter: 927 loss: 7.43457e-07
Iter: 928 loss: 7.42970315e-07
Iter: 929 loss: 7.42389034e-07
Iter: 930 loss: 7.42316786e-07
Iter: 931 loss: 7.41956569e-07
Iter: 932 loss: 7.41865847e-07
Iter: 933 loss: 7.41445376e-07
Iter: 934 loss: 7.418476e-07
Iter: 935 loss: 7.41178155e-07
Iter: 936 loss: 7.40777182e-07
Iter: 937 loss: 7.40576354e-07
Iter: 938 loss: 7.40391e-07
Iter: 939 loss: 7.39869392e-07
Iter: 940 loss: 7.4142423e-07
Iter: 941 loss: 7.39676182e-07
Iter: 942 loss: 7.39147e-07
Iter: 943 loss: 7.39153904e-07
Iter: 944 loss: 7.38829954e-07
Iter: 945 loss: 7.38036874e-07
Iter: 946 loss: 7.44008446e-07
Iter: 947 loss: 7.37879361e-07
Iter: 948 loss: 7.37254709e-07
Iter: 949 loss: 7.45745069e-07
Iter: 950 loss: 7.37247888e-07
Iter: 951 loss: 7.36894322e-07
Iter: 952 loss: 7.36891707e-07
Iter: 953 loss: 7.36585434e-07
Iter: 954 loss: 7.36085326e-07
Iter: 955 loss: 7.47949457e-07
Iter: 956 loss: 7.36080096e-07
Iter: 957 loss: 7.35736648e-07
Iter: 958 loss: 7.3574256e-07
Iter: 959 loss: 7.35449589e-07
Iter: 960 loss: 7.34837045e-07
Iter: 961 loss: 7.4385531e-07
Iter: 962 loss: 7.34825562e-07
Iter: 963 loss: 7.34183175e-07
Iter: 964 loss: 7.39350696e-07
Iter: 965 loss: 7.3412474e-07
Iter: 966 loss: 7.3362321e-07
Iter: 967 loss: 7.33514923e-07
Iter: 968 loss: 7.33199442e-07
Iter: 969 loss: 7.32732644e-07
Iter: 970 loss: 7.32712351e-07
Iter: 971 loss: 7.32307058e-07
Iter: 972 loss: 7.32154888e-07
Iter: 973 loss: 7.31939053e-07
Iter: 974 loss: 7.31517616e-07
Iter: 975 loss: 7.31394152e-07
Iter: 976 loss: 7.31143189e-07
Iter: 977 loss: 7.30972943e-07
Iter: 978 loss: 7.30862439e-07
Iter: 979 loss: 7.30588852e-07
Iter: 980 loss: 7.29974488e-07
Iter: 981 loss: 7.40363248e-07
Iter: 982 loss: 7.2996761e-07
Iter: 983 loss: 7.29337444e-07
Iter: 984 loss: 7.30294e-07
Iter: 985 loss: 7.29076e-07
Iter: 986 loss: 7.28722057e-07
Iter: 987 loss: 7.28663338e-07
Iter: 988 loss: 7.28269299e-07
Iter: 989 loss: 7.27666759e-07
Iter: 990 loss: 7.2762839e-07
Iter: 991 loss: 7.27215706e-07
Iter: 992 loss: 7.33098545e-07
Iter: 993 loss: 7.27203087e-07
Iter: 994 loss: 7.26755957e-07
Iter: 995 loss: 7.26584801e-07
Iter: 996 loss: 7.26368398e-07
Iter: 997 loss: 7.25988855e-07
Iter: 998 loss: 7.26936776e-07
Iter: 999 loss: 7.25832365e-07
Iter: 1000 loss: 7.25410189e-07
Iter: 1001 loss: 7.26664609e-07
Iter: 1002 loss: 7.25284735e-07
Iter: 1003 loss: 7.24882057e-07
Iter: 1004 loss: 7.27706492e-07
Iter: 1005 loss: 7.24843801e-07
Iter: 1006 loss: 7.2442225e-07
Iter: 1007 loss: 7.24467895e-07
Iter: 1008 loss: 7.24075619e-07
Iter: 1009 loss: 7.23553285e-07
Iter: 1010 loss: 7.23222968e-07
Iter: 1011 loss: 7.23025e-07
Iter: 1012 loss: 7.22614686e-07
Iter: 1013 loss: 7.22596212e-07
Iter: 1014 loss: 7.22134587e-07
Iter: 1015 loss: 7.22082518e-07
Iter: 1016 loss: 7.21727474e-07
Iter: 1017 loss: 7.21257948e-07
Iter: 1018 loss: 7.21025458e-07
Iter: 1019 loss: 7.20768469e-07
Iter: 1020 loss: 7.20203843e-07
Iter: 1021 loss: 7.22166419e-07
Iter: 1022 loss: 7.2007424e-07
Iter: 1023 loss: 7.19663944e-07
Iter: 1024 loss: 7.19637e-07
Iter: 1025 loss: 7.19300488e-07
Iter: 1026 loss: 7.18609272e-07
Iter: 1027 loss: 7.30278941e-07
Iter: 1028 loss: 7.18604156e-07
Iter: 1029 loss: 7.18059198e-07
Iter: 1030 loss: 7.22227071e-07
Iter: 1031 loss: 7.18004e-07
Iter: 1032 loss: 7.17421926e-07
Iter: 1033 loss: 7.18598358e-07
Iter: 1034 loss: 7.17169428e-07
Iter: 1035 loss: 7.16743671e-07
Iter: 1036 loss: 7.16289719e-07
Iter: 1037 loss: 7.16208774e-07
Iter: 1038 loss: 7.1577017e-07
Iter: 1039 loss: 7.20018704e-07
Iter: 1040 loss: 7.1575414e-07
Iter: 1041 loss: 7.15274837e-07
Iter: 1042 loss: 7.182731e-07
Iter: 1043 loss: 7.15235103e-07
Iter: 1044 loss: 7.14948101e-07
Iter: 1045 loss: 7.14699468e-07
Iter: 1046 loss: 7.14647854e-07
Iter: 1047 loss: 7.14061628e-07
Iter: 1048 loss: 7.14459475e-07
Iter: 1049 loss: 7.13710961e-07
Iter: 1050 loss: 7.13046916e-07
Iter: 1051 loss: 7.16053648e-07
Iter: 1052 loss: 7.12932888e-07
Iter: 1053 loss: 7.12480755e-07
Iter: 1054 loss: 7.12491e-07
Iter: 1055 loss: 7.12045392e-07
Iter: 1056 loss: 7.11158577e-07
Iter: 1057 loss: 7.26368967e-07
Iter: 1058 loss: 7.11141809e-07
Iter: 1059 loss: 7.10749305e-07
Iter: 1060 loss: 7.10733e-07
Iter: 1061 loss: 7.10321331e-07
Iter: 1062 loss: 7.11565e-07
Iter: 1063 loss: 7.10205313e-07
Iter: 1064 loss: 7.09934284e-07
Iter: 1065 loss: 7.093671e-07
Iter: 1066 loss: 7.19991931e-07
Iter: 1067 loss: 7.09365054e-07
Iter: 1068 loss: 7.08958169e-07
Iter: 1069 loss: 7.08946118e-07
Iter: 1070 loss: 7.08568223e-07
Iter: 1071 loss: 7.08473749e-07
Iter: 1072 loss: 7.08232051e-07
Iter: 1073 loss: 7.07791969e-07
Iter: 1074 loss: 7.0704084e-07
Iter: 1075 loss: 7.07039931e-07
Iter: 1076 loss: 7.06994342e-07
Iter: 1077 loss: 7.06724904e-07
Iter: 1078 loss: 7.0636861e-07
Iter: 1079 loss: 7.06356445e-07
Iter: 1080 loss: 7.06112701e-07
Iter: 1081 loss: 7.05733044e-07
Iter: 1082 loss: 7.05623506e-07
Iter: 1083 loss: 7.05390505e-07
Iter: 1084 loss: 7.0487414e-07
Iter: 1085 loss: 7.04725494e-07
Iter: 1086 loss: 7.04399156e-07
Iter: 1087 loss: 7.03906835e-07
Iter: 1088 loss: 7.0386136e-07
Iter: 1089 loss: 7.03427304e-07
Iter: 1090 loss: 7.04635909e-07
Iter: 1091 loss: 7.03279284e-07
Iter: 1092 loss: 7.02840168e-07
Iter: 1093 loss: 7.02823968e-07
Iter: 1094 loss: 7.02430043e-07
Iter: 1095 loss: 7.02030093e-07
Iter: 1096 loss: 7.04046386e-07
Iter: 1097 loss: 7.01922943e-07
Iter: 1098 loss: 7.01509748e-07
Iter: 1099 loss: 7.05451725e-07
Iter: 1100 loss: 7.01482463e-07
Iter: 1101 loss: 7.01195063e-07
Iter: 1102 loss: 7.00630778e-07
Iter: 1103 loss: 7.11176654e-07
Iter: 1104 loss: 7.0062e-07
Iter: 1105 loss: 7.00060923e-07
Iter: 1106 loss: 7.01721717e-07
Iter: 1107 loss: 6.99897839e-07
Iter: 1108 loss: 6.99489419e-07
Iter: 1109 loss: 6.99456507e-07
Iter: 1110 loss: 6.99189e-07
Iter: 1111 loss: 6.98514555e-07
Iter: 1112 loss: 7.06496166e-07
Iter: 1113 loss: 6.98473514e-07
Iter: 1114 loss: 6.97750579e-07
Iter: 1115 loss: 7.02357283e-07
Iter: 1116 loss: 6.97680889e-07
Iter: 1117 loss: 6.97377345e-07
Iter: 1118 loss: 6.97330279e-07
Iter: 1119 loss: 6.97086762e-07
Iter: 1120 loss: 6.96548909e-07
Iter: 1121 loss: 7.02655427e-07
Iter: 1122 loss: 6.96481038e-07
Iter: 1123 loss: 6.95881226e-07
Iter: 1124 loss: 6.97032647e-07
Iter: 1125 loss: 6.95619747e-07
Iter: 1126 loss: 6.95016297e-07
Iter: 1127 loss: 6.96834832e-07
Iter: 1128 loss: 6.94825815e-07
Iter: 1129 loss: 6.94472533e-07
Iter: 1130 loss: 6.9440506e-07
Iter: 1131 loss: 6.94162054e-07
Iter: 1132 loss: 6.93588845e-07
Iter: 1133 loss: 6.99761301e-07
Iter: 1134 loss: 6.93507673e-07
Iter: 1135 loss: 6.9317997e-07
Iter: 1136 loss: 6.93141146e-07
Iter: 1137 loss: 6.92848744e-07
Iter: 1138 loss: 6.93476181e-07
Iter: 1139 loss: 6.92697313e-07
Iter: 1140 loss: 6.92387232e-07
Iter: 1141 loss: 6.92138656e-07
Iter: 1142 loss: 6.9204566e-07
Iter: 1143 loss: 6.9159762e-07
Iter: 1144 loss: 6.93548259e-07
Iter: 1145 loss: 6.91497576e-07
Iter: 1146 loss: 6.91143839e-07
Iter: 1147 loss: 6.94212929e-07
Iter: 1148 loss: 6.9111951e-07
Iter: 1149 loss: 6.90767706e-07
Iter: 1150 loss: 6.90106731e-07
Iter: 1151 loss: 7.03649334e-07
Iter: 1152 loss: 6.90103207e-07
Iter: 1153 loss: 6.89617423e-07
Iter: 1154 loss: 6.96551638e-07
Iter: 1155 loss: 6.89611909e-07
Iter: 1156 loss: 6.89101569e-07
Iter: 1157 loss: 6.90541469e-07
Iter: 1158 loss: 6.88933198e-07
Iter: 1159 loss: 6.88631303e-07
Iter: 1160 loss: 6.88047066e-07
Iter: 1161 loss: 7.0153726e-07
Iter: 1162 loss: 6.88050875e-07
Iter: 1163 loss: 6.87382681e-07
Iter: 1164 loss: 6.90725415e-07
Iter: 1165 loss: 6.87247962e-07
Iter: 1166 loss: 6.87014563e-07
Iter: 1167 loss: 6.87006946e-07
Iter: 1168 loss: 6.86677083e-07
Iter: 1169 loss: 6.86144517e-07
Iter: 1170 loss: 6.992218e-07
Iter: 1171 loss: 6.86145427e-07
Iter: 1172 loss: 6.8559757e-07
Iter: 1173 loss: 6.87229715e-07
Iter: 1174 loss: 6.85434429e-07
Iter: 1175 loss: 6.85001851e-07
Iter: 1176 loss: 6.84996166e-07
Iter: 1177 loss: 6.84709221e-07
Iter: 1178 loss: 6.84121e-07
Iter: 1179 loss: 6.95935455e-07
Iter: 1180 loss: 6.84123677e-07
Iter: 1181 loss: 6.8358e-07
Iter: 1182 loss: 6.85133955e-07
Iter: 1183 loss: 6.83405915e-07
Iter: 1184 loss: 6.82838049e-07
Iter: 1185 loss: 6.87179408e-07
Iter: 1186 loss: 6.82794905e-07
Iter: 1187 loss: 6.82434461e-07
Iter: 1188 loss: 6.84801876e-07
Iter: 1189 loss: 6.82382961e-07
Iter: 1190 loss: 6.82109089e-07
Iter: 1191 loss: 6.81638426e-07
Iter: 1192 loss: 6.81623135e-07
Iter: 1193 loss: 6.81233871e-07
Iter: 1194 loss: 6.81238134e-07
Iter: 1195 loss: 6.80802486e-07
Iter: 1196 loss: 6.8035115e-07
Iter: 1197 loss: 6.80262644e-07
Iter: 1198 loss: 6.7972104e-07
Iter: 1199 loss: 6.80632184e-07
Iter: 1200 loss: 6.79461436e-07
Iter: 1201 loss: 6.7904773e-07
Iter: 1202 loss: 6.84436486e-07
Iter: 1203 loss: 6.79060236e-07
Iter: 1204 loss: 6.78616175e-07
Iter: 1205 loss: 6.79163406e-07
Iter: 1206 loss: 6.78394713e-07
Iter: 1207 loss: 6.78118283e-07
Iter: 1208 loss: 6.77931666e-07
Iter: 1209 loss: 6.77837647e-07
Iter: 1210 loss: 6.77442927e-07
Iter: 1211 loss: 6.77438038e-07
Iter: 1212 loss: 6.77163143e-07
Iter: 1213 loss: 6.76579873e-07
Iter: 1214 loss: 6.85532882e-07
Iter: 1215 loss: 6.76534626e-07
Iter: 1216 loss: 6.75922308e-07
Iter: 1217 loss: 6.77098228e-07
Iter: 1218 loss: 6.75671572e-07
Iter: 1219 loss: 6.75049591e-07
Iter: 1220 loss: 6.8063423e-07
Iter: 1221 loss: 6.75037427e-07
Iter: 1222 loss: 6.74537887e-07
Iter: 1223 loss: 6.77359594e-07
Iter: 1224 loss: 6.74499574e-07
Iter: 1225 loss: 6.74151238e-07
Iter: 1226 loss: 6.7493886e-07
Iter: 1227 loss: 6.74052671e-07
Iter: 1228 loss: 6.73740658e-07
Iter: 1229 loss: 6.74417549e-07
Iter: 1230 loss: 6.73637487e-07
Iter: 1231 loss: 6.73240152e-07
Iter: 1232 loss: 6.74383e-07
Iter: 1233 loss: 6.73127204e-07
Iter: 1234 loss: 6.72768465e-07
Iter: 1235 loss: 6.72313945e-07
Iter: 1236 loss: 6.72285523e-07
Iter: 1237 loss: 6.71651094e-07
Iter: 1238 loss: 6.72526141e-07
Iter: 1239 loss: 6.7135187e-07
Iter: 1240 loss: 6.71056341e-07
Iter: 1241 loss: 6.70928785e-07
Iter: 1242 loss: 6.70605573e-07
Iter: 1243 loss: 6.70206191e-07
Iter: 1244 loss: 6.7016208e-07
Iter: 1245 loss: 6.69850579e-07
Iter: 1246 loss: 6.73217869e-07
Iter: 1247 loss: 6.6984262e-07
Iter: 1248 loss: 6.69482574e-07
Iter: 1249 loss: 6.69968472e-07
Iter: 1250 loss: 6.69287e-07
Iter: 1251 loss: 6.6904056e-07
Iter: 1252 loss: 6.68504526e-07
Iter: 1253 loss: 6.75555839e-07
Iter: 1254 loss: 6.68451889e-07
Iter: 1255 loss: 6.67738846e-07
Iter: 1256 loss: 6.71401267e-07
Iter: 1257 loss: 6.67622487e-07
Iter: 1258 loss: 6.67220775e-07
Iter: 1259 loss: 6.67218046e-07
Iter: 1260 loss: 6.66827191e-07
Iter: 1261 loss: 6.66332426e-07
Iter: 1262 loss: 6.66278595e-07
Iter: 1263 loss: 6.6595851e-07
Iter: 1264 loss: 6.65949415e-07
Iter: 1265 loss: 6.65608e-07
Iter: 1266 loss: 6.6537757e-07
Iter: 1267 loss: 6.65256835e-07
Iter: 1268 loss: 6.64914921e-07
Iter: 1269 loss: 6.66377559e-07
Iter: 1270 loss: 6.64836421e-07
Iter: 1271 loss: 6.64467052e-07
Iter: 1272 loss: 6.64105301e-07
Iter: 1273 loss: 6.64048116e-07
Iter: 1274 loss: 6.63717969e-07
Iter: 1275 loss: 6.63693413e-07
Iter: 1276 loss: 6.6333223e-07
Iter: 1277 loss: 6.63025389e-07
Iter: 1278 loss: 6.62963771e-07
Iter: 1279 loss: 6.62483842e-07
Iter: 1280 loss: 6.62727871e-07
Iter: 1281 loss: 6.62180469e-07
Iter: 1282 loss: 6.61928311e-07
Iter: 1283 loss: 6.61870502e-07
Iter: 1284 loss: 6.61594243e-07
Iter: 1285 loss: 6.60987666e-07
Iter: 1286 loss: 6.72918645e-07
Iter: 1287 loss: 6.60999149e-07
Iter: 1288 loss: 6.60544288e-07
Iter: 1289 loss: 6.60914679e-07
Iter: 1290 loss: 6.603006e-07
Iter: 1291 loss: 6.59819193e-07
Iter: 1292 loss: 6.59833177e-07
Iter: 1293 loss: 6.5940327e-07
Iter: 1294 loss: 6.59720058e-07
Iter: 1295 loss: 6.59158786e-07
Iter: 1296 loss: 6.58751333e-07
Iter: 1297 loss: 6.58779754e-07
Iter: 1298 loss: 6.58452336e-07
Iter: 1299 loss: 6.58022827e-07
Iter: 1300 loss: 6.58026e-07
Iter: 1301 loss: 6.57724968e-07
Iter: 1302 loss: 6.5709321e-07
Iter: 1303 loss: 6.64956588e-07
Iter: 1304 loss: 6.57031705e-07
Iter: 1305 loss: 6.56543534e-07
Iter: 1306 loss: 6.62495097e-07
Iter: 1307 loss: 6.56547911e-07
Iter: 1308 loss: 6.56127327e-07
Iter: 1309 loss: 6.5674044e-07
Iter: 1310 loss: 6.55942e-07
Iter: 1311 loss: 6.55619885e-07
Iter: 1312 loss: 6.55616191e-07
Iter: 1313 loss: 6.55340557e-07
Iter: 1314 loss: 6.54904454e-07
Iter: 1315 loss: 6.54904738e-07
Iter: 1316 loss: 6.54428277e-07
Iter: 1317 loss: 6.542287e-07
Iter: 1318 loss: 6.53975235e-07
Iter: 1319 loss: 6.53416805e-07
Iter: 1320 loss: 6.5824355e-07
Iter: 1321 loss: 6.53412087e-07
Iter: 1322 loss: 6.52946255e-07
Iter: 1323 loss: 6.58701822e-07
Iter: 1324 loss: 6.52936365e-07
Iter: 1325 loss: 6.52637823e-07
Iter: 1326 loss: 6.52077858e-07
Iter: 1327 loss: 6.62670175e-07
Iter: 1328 loss: 6.52061317e-07
Iter: 1329 loss: 6.51654318e-07
Iter: 1330 loss: 6.54817086e-07
Iter: 1331 loss: 6.51618279e-07
Iter: 1332 loss: 6.51247092e-07
Iter: 1333 loss: 6.54261441e-07
Iter: 1334 loss: 6.51197e-07
Iter: 1335 loss: 6.50939569e-07
Iter: 1336 loss: 6.50623178e-07
Iter: 1337 loss: 6.50598508e-07
Iter: 1338 loss: 6.50284164e-07
Iter: 1339 loss: 6.50268362e-07
Iter: 1340 loss: 6.50012453e-07
Iter: 1341 loss: 6.49495746e-07
Iter: 1342 loss: 6.59935e-07
Iter: 1343 loss: 6.49459707e-07
Iter: 1344 loss: 6.48910145e-07
Iter: 1345 loss: 6.49251433e-07
Iter: 1346 loss: 6.48550099e-07
Iter: 1347 loss: 6.48363937e-07
Iter: 1348 loss: 6.48231378e-07
Iter: 1349 loss: 6.47972058e-07
Iter: 1350 loss: 6.48233e-07
Iter: 1351 loss: 6.47848765e-07
Iter: 1352 loss: 6.47494289e-07
Iter: 1353 loss: 6.47297384e-07
Iter: 1354 loss: 6.47159766e-07
Iter: 1355 loss: 6.46708941e-07
Iter: 1356 loss: 6.48605749e-07
Iter: 1357 loss: 6.46610715e-07
Iter: 1358 loss: 6.46331614e-07
Iter: 1359 loss: 6.46328203e-07
Iter: 1360 loss: 6.46000501e-07
Iter: 1361 loss: 6.45392561e-07
Iter: 1362 loss: 6.56550128e-07
Iter: 1363 loss: 6.45392504e-07
Iter: 1364 loss: 6.4488097e-07
Iter: 1365 loss: 6.4644e-07
Iter: 1366 loss: 6.44739032e-07
Iter: 1367 loss: 6.44209251e-07
Iter: 1368 loss: 6.49675599e-07
Iter: 1369 loss: 6.44213344e-07
Iter: 1370 loss: 6.43959083e-07
Iter: 1371 loss: 6.43854946e-07
Iter: 1372 loss: 6.4368578e-07
Iter: 1373 loss: 6.43350745e-07
Iter: 1374 loss: 6.47753211e-07
Iter: 1375 loss: 6.43333408e-07
Iter: 1376 loss: 6.43124736e-07
Iter: 1377 loss: 6.42653276e-07
Iter: 1378 loss: 6.51051096e-07
Iter: 1379 loss: 6.42669875e-07
Iter: 1380 loss: 6.42184602e-07
Iter: 1381 loss: 6.44725105e-07
Iter: 1382 loss: 6.42144471e-07
Iter: 1383 loss: 6.41757424e-07
Iter: 1384 loss: 6.4544065e-07
Iter: 1385 loss: 6.41741963e-07
Iter: 1386 loss: 6.41462577e-07
Iter: 1387 loss: 6.41140559e-07
Iter: 1388 loss: 6.41089855e-07
Iter: 1389 loss: 6.40516305e-07
Iter: 1390 loss: 6.41217355e-07
Iter: 1391 loss: 6.40225323e-07
Iter: 1392 loss: 6.39748805e-07
Iter: 1393 loss: 6.42771624e-07
Iter: 1394 loss: 6.39679342e-07
Iter: 1395 loss: 6.39359598e-07
Iter: 1396 loss: 6.39356472e-07
Iter: 1397 loss: 6.39193956e-07
Iter: 1398 loss: 6.38732047e-07
Iter: 1399 loss: 6.44236422e-07
Iter: 1400 loss: 6.38713232e-07
Iter: 1401 loss: 6.38374104e-07
Iter: 1402 loss: 6.42452505e-07
Iter: 1403 loss: 6.38348922e-07
Iter: 1404 loss: 6.3797296e-07
Iter: 1405 loss: 6.38652182e-07
Iter: 1406 loss: 6.37839321e-07
Iter: 1407 loss: 6.37434596e-07
Iter: 1408 loss: 6.37567155e-07
Iter: 1409 loss: 6.37180392e-07
Iter: 1410 loss: 6.36663572e-07
Iter: 1411 loss: 6.41458939e-07
Iter: 1412 loss: 6.36635832e-07
Iter: 1413 loss: 6.36394589e-07
Iter: 1414 loss: 6.35850085e-07
Iter: 1415 loss: 6.44526949e-07
Iter: 1416 loss: 6.35828314e-07
Iter: 1417 loss: 6.35484753e-07
Iter: 1418 loss: 6.35478727e-07
Iter: 1419 loss: 6.35110837e-07
Iter: 1420 loss: 6.35888284e-07
Iter: 1421 loss: 6.34991807e-07
Iter: 1422 loss: 6.3472271e-07
Iter: 1423 loss: 6.34715661e-07
Iter: 1424 loss: 6.34460662e-07
Iter: 1425 loss: 6.34085268e-07
Iter: 1426 loss: 6.35393121e-07
Iter: 1427 loss: 6.33970558e-07
Iter: 1428 loss: 6.33641434e-07
Iter: 1429 loss: 6.36148798e-07
Iter: 1430 loss: 6.33628304e-07
Iter: 1431 loss: 6.33207719e-07
Iter: 1432 loss: 6.33160425e-07
Iter: 1433 loss: 6.32871263e-07
Iter: 1434 loss: 6.32494789e-07
Iter: 1435 loss: 6.32467618e-07
Iter: 1436 loss: 6.32186811e-07
Iter: 1437 loss: 6.31912485e-07
Iter: 1438 loss: 6.31906175e-07
Iter: 1439 loss: 6.31650607e-07
Iter: 1440 loss: 6.31566252e-07
Iter: 1441 loss: 6.31443356e-07
Iter: 1442 loss: 6.31166472e-07
Iter: 1443 loss: 6.33643936e-07
Iter: 1444 loss: 6.31153114e-07
Iter: 1445 loss: 6.30867646e-07
Iter: 1446 loss: 6.30713259e-07
Iter: 1447 loss: 6.30577802e-07
Iter: 1448 loss: 6.3029529e-07
Iter: 1449 loss: 6.30188651e-07
Iter: 1450 loss: 6.30010902e-07
Iter: 1451 loss: 6.29707188e-07
Iter: 1452 loss: 6.29680471e-07
Iter: 1453 loss: 6.29429564e-07
Iter: 1454 loss: 6.2895856e-07
Iter: 1455 loss: 6.28965836e-07
Iter: 1456 loss: 6.28492e-07
Iter: 1457 loss: 6.30171144e-07
Iter: 1458 loss: 6.28380519e-07
Iter: 1459 loss: 6.27981535e-07
Iter: 1460 loss: 6.32183401e-07
Iter: 1461 loss: 6.27980057e-07
Iter: 1462 loss: 6.27664861e-07
Iter: 1463 loss: 6.29244482e-07
Iter: 1464 loss: 6.27595909e-07
Iter: 1465 loss: 6.27339773e-07
Iter: 1466 loss: 6.27008262e-07
Iter: 1467 loss: 6.26995416e-07
Iter: 1468 loss: 6.26693748e-07
Iter: 1469 loss: 6.29912506e-07
Iter: 1470 loss: 6.26675e-07
Iter: 1471 loss: 6.26359e-07
Iter: 1472 loss: 6.26504516e-07
Iter: 1473 loss: 6.26124574e-07
Iter: 1474 loss: 6.25749408e-07
Iter: 1475 loss: 6.28030818e-07
Iter: 1476 loss: 6.25703137e-07
Iter: 1477 loss: 6.25354176e-07
Iter: 1478 loss: 6.25743155e-07
Iter: 1479 loss: 6.25157668e-07
Iter: 1480 loss: 6.24812628e-07
Iter: 1481 loss: 6.24479412e-07
Iter: 1482 loss: 6.2440165e-07
Iter: 1483 loss: 6.24110726e-07
Iter: 1484 loss: 6.24109e-07
Iter: 1485 loss: 6.23798599e-07
Iter: 1486 loss: 6.24142899e-07
Iter: 1487 loss: 6.23616813e-07
Iter: 1488 loss: 6.23375797e-07
Iter: 1489 loss: 6.23012795e-07
Iter: 1490 loss: 6.22982725e-07
Iter: 1491 loss: 6.22505922e-07
Iter: 1492 loss: 6.2605136e-07
Iter: 1493 loss: 6.22462323e-07
Iter: 1494 loss: 6.22126208e-07
Iter: 1495 loss: 6.26258384e-07
Iter: 1496 loss: 6.22143375e-07
Iter: 1497 loss: 6.21770937e-07
Iter: 1498 loss: 6.21385539e-07
Iter: 1499 loss: 6.21355071e-07
Iter: 1500 loss: 6.20887192e-07
Iter: 1501 loss: 6.21139293e-07
Iter: 1502 loss: 6.20602236e-07
Iter: 1503 loss: 6.20175683e-07
Iter: 1504 loss: 6.20160904e-07
Iter: 1505 loss: 6.19893e-07
Iter: 1506 loss: 6.1994092e-07
Iter: 1507 loss: 6.19684556e-07
Iter: 1508 loss: 6.1941364e-07
Iter: 1509 loss: 6.22340053e-07
Iter: 1510 loss: 6.19405512e-07
Iter: 1511 loss: 6.19138177e-07
Iter: 1512 loss: 6.18814909e-07
Iter: 1513 loss: 6.18789272e-07
Iter: 1514 loss: 6.1841871e-07
Iter: 1515 loss: 6.18457e-07
Iter: 1516 loss: 6.18118747e-07
Iter: 1517 loss: 6.17831859e-07
Iter: 1518 loss: 6.17798264e-07
Iter: 1519 loss: 6.17561e-07
Iter: 1520 loss: 6.17150477e-07
Iter: 1521 loss: 6.27151621e-07
Iter: 1522 loss: 6.17153034e-07
Iter: 1523 loss: 6.16654518e-07
Iter: 1524 loss: 6.16588125e-07
Iter: 1525 loss: 6.16222792e-07
Iter: 1526 loss: 6.15869567e-07
Iter: 1527 loss: 6.15844215e-07
Iter: 1528 loss: 6.15504746e-07
Iter: 1529 loss: 6.17095e-07
Iter: 1530 loss: 6.15448812e-07
Iter: 1531 loss: 6.15169483e-07
Iter: 1532 loss: 6.15144074e-07
Iter: 1533 loss: 6.14941428e-07
Iter: 1534 loss: 6.14614123e-07
Iter: 1535 loss: 6.14246801e-07
Iter: 1536 loss: 6.1421639e-07
Iter: 1537 loss: 6.13968382e-07
Iter: 1538 loss: 6.13872146e-07
Iter: 1539 loss: 6.13638804e-07
Iter: 1540 loss: 6.13344241e-07
Iter: 1541 loss: 6.13334805e-07
Iter: 1542 loss: 6.12956967e-07
Iter: 1543 loss: 6.17764726e-07
Iter: 1544 loss: 6.12951567e-07
Iter: 1545 loss: 6.12692531e-07
Iter: 1546 loss: 6.12464191e-07
Iter: 1547 loss: 6.1239291e-07
Iter: 1548 loss: 6.12064071e-07
Iter: 1549 loss: 6.11947883e-07
Iter: 1550 loss: 6.11731934e-07
Iter: 1551 loss: 6.11677e-07
Iter: 1552 loss: 6.11541225e-07
Iter: 1553 loss: 6.11326755e-07
Iter: 1554 loss: 6.1104231e-07
Iter: 1555 loss: 6.1102088e-07
Iter: 1556 loss: 6.10645e-07
Iter: 1557 loss: 6.10853135e-07
Iter: 1558 loss: 6.10382244e-07
Iter: 1559 loss: 6.1017e-07
Iter: 1560 loss: 6.10139068e-07
Iter: 1561 loss: 6.09834558e-07
Iter: 1562 loss: 6.09578763e-07
Iter: 1563 loss: 6.0951777e-07
Iter: 1564 loss: 6.09192625e-07
Iter: 1565 loss: 6.09786582e-07
Iter: 1566 loss: 6.09083827e-07
Iter: 1567 loss: 6.08767664e-07
Iter: 1568 loss: 6.10590291e-07
Iter: 1569 loss: 6.08714117e-07
Iter: 1570 loss: 6.08483731e-07
Iter: 1571 loss: 6.10028735e-07
Iter: 1572 loss: 6.08461278e-07
Iter: 1573 loss: 6.0822191e-07
Iter: 1574 loss: 6.08062e-07
Iter: 1575 loss: 6.07987e-07
Iter: 1576 loss: 6.07655352e-07
Iter: 1577 loss: 6.11013661e-07
Iter: 1578 loss: 6.07672121e-07
Iter: 1579 loss: 6.07411494e-07
Iter: 1580 loss: 6.07219704e-07
Iter: 1581 loss: 6.07152288e-07
Iter: 1582 loss: 6.06800654e-07
Iter: 1583 loss: 6.06514845e-07
Iter: 1584 loss: 6.06401898e-07
Iter: 1585 loss: 6.0586774e-07
Iter: 1586 loss: 6.08041205e-07
Iter: 1587 loss: 6.05746152e-07
Iter: 1588 loss: 6.05505193e-07
Iter: 1589 loss: 6.05496552e-07
Iter: 1590 loss: 6.05227683e-07
Iter: 1591 loss: 6.05557887e-07
Iter: 1592 loss: 6.0508421e-07
Iter: 1593 loss: 6.0487946e-07
Iter: 1594 loss: 6.04786237e-07
Iter: 1595 loss: 6.04684942e-07
Iter: 1596 loss: 6.04278796e-07
Iter: 1597 loss: 6.07159336e-07
Iter: 1598 loss: 6.04228774e-07
Iter: 1599 loss: 6.03999752e-07
Iter: 1600 loss: 6.03660624e-07
Iter: 1601 loss: 6.03669093e-07
Iter: 1602 loss: 6.03218837e-07
Iter: 1603 loss: 6.05277251e-07
Iter: 1604 loss: 6.03136527e-07
Iter: 1605 loss: 6.02787111e-07
Iter: 1606 loss: 6.06294691e-07
Iter: 1607 loss: 6.02791602e-07
Iter: 1608 loss: 6.02564e-07
Iter: 1609 loss: 6.02928367e-07
Iter: 1610 loss: 6.02457703e-07
Iter: 1611 loss: 6.02205432e-07
Iter: 1612 loss: 6.02782848e-07
Iter: 1613 loss: 6.0211471e-07
Iter: 1614 loss: 6.01825263e-07
Iter: 1615 loss: 6.02712873e-07
Iter: 1616 loss: 6.01759e-07
Iter: 1617 loss: 6.01523766e-07
Iter: 1618 loss: 6.01093461e-07
Iter: 1619 loss: 6.10187499e-07
Iter: 1620 loss: 6.01101647e-07
Iter: 1621 loss: 6.00623878e-07
Iter: 1622 loss: 6.0178138e-07
Iter: 1623 loss: 6.00450676e-07
Iter: 1624 loss: 5.99935163e-07
Iter: 1625 loss: 6.02779892e-07
Iter: 1626 loss: 5.99874056e-07
Iter: 1627 loss: 5.9971137e-07
Iter: 1628 loss: 5.99644068e-07
Iter: 1629 loss: 5.99463192e-07
Iter: 1630 loss: 5.99110308e-07
Iter: 1631 loss: 6.06976528e-07
Iter: 1632 loss: 5.99094506e-07
Iter: 1633 loss: 5.98965585e-07
Iter: 1634 loss: 5.98946144e-07
Iter: 1635 loss: 5.98771294e-07
Iter: 1636 loss: 5.98481051e-07
Iter: 1637 loss: 6.04561365e-07
Iter: 1638 loss: 5.98481e-07
Iter: 1639 loss: 5.98177166e-07
Iter: 1640 loss: 5.98568306e-07
Iter: 1641 loss: 5.98031875e-07
Iter: 1642 loss: 5.97730377e-07
Iter: 1643 loss: 6.00379906e-07
Iter: 1644 loss: 5.97693884e-07
Iter: 1645 loss: 5.97404835e-07
Iter: 1646 loss: 5.97903806e-07
Iter: 1647 loss: 5.97257497e-07
Iter: 1648 loss: 5.96949803e-07
Iter: 1649 loss: 5.97534608e-07
Iter: 1650 loss: 5.96830489e-07
Iter: 1651 loss: 5.96542691e-07
Iter: 1652 loss: 5.98772772e-07
Iter: 1653 loss: 5.9652416e-07
Iter: 1654 loss: 5.96302755e-07
Iter: 1655 loss: 5.96348514e-07
Iter: 1656 loss: 5.9616923e-07
Iter: 1657 loss: 5.95929691e-07
Iter: 1658 loss: 5.95722724e-07
Iter: 1659 loss: 5.9563348e-07
Iter: 1660 loss: 5.95260872e-07
Iter: 1661 loss: 5.96413486e-07
Iter: 1662 loss: 5.95141501e-07
Iter: 1663 loss: 5.94763719e-07
Iter: 1664 loss: 5.96446114e-07
Iter: 1665 loss: 5.94714038e-07
Iter: 1666 loss: 5.94232461e-07
Iter: 1667 loss: 5.97326675e-07
Iter: 1668 loss: 5.94189373e-07
Iter: 1669 loss: 5.93988148e-07
Iter: 1670 loss: 5.93995082e-07
Iter: 1671 loss: 5.93829e-07
Iter: 1672 loss: 5.93484629e-07
Iter: 1673 loss: 5.96245854e-07
Iter: 1674 loss: 5.9349793e-07
Iter: 1675 loss: 5.93286813e-07
Iter: 1676 loss: 5.92798131e-07
Iter: 1677 loss: 5.98822567e-07
Iter: 1678 loss: 5.92726053e-07
Iter: 1679 loss: 5.92353e-07
Iter: 1680 loss: 5.94279754e-07
Iter: 1681 loss: 5.92286767e-07
Iter: 1682 loss: 5.91939397e-07
Iter: 1683 loss: 5.93170739e-07
Iter: 1684 loss: 5.91847652e-07
Iter: 1685 loss: 5.91588673e-07
Iter: 1686 loss: 5.95145195e-07
Iter: 1687 loss: 5.91578896e-07
Iter: 1688 loss: 5.91314461e-07
Iter: 1689 loss: 5.91323897e-07
Iter: 1690 loss: 5.91142225e-07
Iter: 1691 loss: 5.90815262e-07
Iter: 1692 loss: 5.92457582e-07
Iter: 1693 loss: 5.90781099e-07
Iter: 1694 loss: 5.90526156e-07
Iter: 1695 loss: 5.91794731e-07
Iter: 1696 loss: 5.90479772e-07
Iter: 1697 loss: 5.90246486e-07
Iter: 1698 loss: 5.90206923e-07
Iter: 1699 loss: 5.90019e-07
Iter: 1700 loss: 5.89792194e-07
Iter: 1701 loss: 5.89829369e-07
Iter: 1702 loss: 5.89572778e-07
Iter: 1703 loss: 5.89289584e-07
Iter: 1704 loss: 5.92712809e-07
Iter: 1705 loss: 5.89286628e-07
Iter: 1706 loss: 5.88966259e-07
Iter: 1707 loss: 5.89928845e-07
Iter: 1708 loss: 5.88852686e-07
Iter: 1709 loss: 5.88628779e-07
Iter: 1710 loss: 5.88448415e-07
Iter: 1711 loss: 5.88397e-07
Iter: 1712 loss: 5.87960699e-07
Iter: 1713 loss: 5.91444518e-07
Iter: 1714 loss: 5.87921193e-07
Iter: 1715 loss: 5.87730767e-07
Iter: 1716 loss: 5.87273519e-07
Iter: 1717 loss: 5.91382388e-07
Iter: 1718 loss: 5.87197178e-07
Iter: 1719 loss: 5.86751923e-07
Iter: 1720 loss: 5.92018523e-07
Iter: 1721 loss: 5.86741123e-07
Iter: 1722 loss: 5.86517046e-07
Iter: 1723 loss: 5.88509806e-07
Iter: 1724 loss: 5.8650852e-07
Iter: 1725 loss: 5.86243857e-07
Iter: 1726 loss: 5.86154158e-07
Iter: 1727 loss: 5.85995508e-07
Iter: 1728 loss: 5.85716464e-07
Iter: 1729 loss: 5.8826646e-07
Iter: 1730 loss: 5.85697592e-07
Iter: 1731 loss: 5.85448106e-07
Iter: 1732 loss: 5.85525754e-07
Iter: 1733 loss: 5.85286102e-07
Iter: 1734 loss: 5.84934128e-07
Iter: 1735 loss: 5.85382168e-07
Iter: 1736 loss: 5.84743589e-07
Iter: 1737 loss: 5.84374447e-07
Iter: 1738 loss: 5.84291968e-07
Iter: 1739 loss: 5.84070676e-07
Iter: 1740 loss: 5.8399803e-07
Iter: 1741 loss: 5.83872747e-07
Iter: 1742 loss: 5.83686187e-07
Iter: 1743 loss: 5.83579379e-07
Iter: 1744 loss: 5.83505312e-07
Iter: 1745 loss: 5.83292831e-07
Iter: 1746 loss: 5.84341e-07
Iter: 1747 loss: 5.83258156e-07
Iter: 1748 loss: 5.83011399e-07
Iter: 1749 loss: 5.83056078e-07
Iter: 1750 loss: 5.82800851e-07
Iter: 1751 loss: 5.82522944e-07
Iter: 1752 loss: 5.82140046e-07
Iter: 1753 loss: 5.82139933e-07
Iter: 1754 loss: 5.81646532e-07
Iter: 1755 loss: 5.83796407e-07
Iter: 1756 loss: 5.81553252e-07
Iter: 1757 loss: 5.81377662e-07
Iter: 1758 loss: 5.81317522e-07
Iter: 1759 loss: 5.81128688e-07
Iter: 1760 loss: 5.80860331e-07
Iter: 1761 loss: 5.80861297e-07
Iter: 1762 loss: 5.8061903e-07
Iter: 1763 loss: 5.84439476e-07
Iter: 1764 loss: 5.80617041e-07
Iter: 1765 loss: 5.80436051e-07
Iter: 1766 loss: 5.80518417e-07
Iter: 1767 loss: 5.80279732e-07
Iter: 1768 loss: 5.80066853e-07
Iter: 1769 loss: 5.79830839e-07
Iter: 1770 loss: 5.79795312e-07
Iter: 1771 loss: 5.79336074e-07
Iter: 1772 loss: 5.81818881e-07
Iter: 1773 loss: 5.79281959e-07
Iter: 1774 loss: 5.78976255e-07
Iter: 1775 loss: 5.80177471e-07
Iter: 1776 loss: 5.78902814e-07
Iter: 1777 loss: 5.78540039e-07
Iter: 1778 loss: 5.80073333e-07
Iter: 1779 loss: 5.78447271e-07
Iter: 1780 loss: 5.78258e-07
Iter: 1781 loss: 5.78004403e-07
Iter: 1782 loss: 5.77976436e-07
Iter: 1783 loss: 5.77724791e-07
Iter: 1784 loss: 5.7770535e-07
Iter: 1785 loss: 5.77592232e-07
Iter: 1786 loss: 5.77320179e-07
Iter: 1787 loss: 5.80191113e-07
Iter: 1788 loss: 5.77273681e-07
Iter: 1789 loss: 5.76929153e-07
Iter: 1790 loss: 5.79390701e-07
Iter: 1791 loss: 5.76896e-07
Iter: 1792 loss: 5.76637035e-07
Iter: 1793 loss: 5.7930913e-07
Iter: 1794 loss: 5.7662362e-07
Iter: 1795 loss: 5.76474577e-07
Iter: 1796 loss: 5.76071272e-07
Iter: 1797 loss: 5.82487e-07
Iter: 1798 loss: 5.76053537e-07
Iter: 1799 loss: 5.7565984e-07
Iter: 1800 loss: 5.75661034e-07
Iter: 1801 loss: 5.75460717e-07
Iter: 1802 loss: 5.75344245e-07
Iter: 1803 loss: 5.7524926e-07
Iter: 1804 loss: 5.74943556e-07
Iter: 1805 loss: 5.75441e-07
Iter: 1806 loss: 5.7482157e-07
Iter: 1807 loss: 5.74544401e-07
Iter: 1808 loss: 5.76221112e-07
Iter: 1809 loss: 5.74504384e-07
Iter: 1810 loss: 5.74342835e-07
Iter: 1811 loss: 5.76678076e-07
Iter: 1812 loss: 5.74335957e-07
Iter: 1813 loss: 5.74127512e-07
Iter: 1814 loss: 5.73797934e-07
Iter: 1815 loss: 5.73784575e-07
Iter: 1816 loss: 5.73500643e-07
Iter: 1817 loss: 5.74474598e-07
Iter: 1818 loss: 5.73429361e-07
Iter: 1819 loss: 5.73073805e-07
Iter: 1820 loss: 5.75139211e-07
Iter: 1821 loss: 5.73028274e-07
Iter: 1822 loss: 5.72803401e-07
Iter: 1823 loss: 5.72388899e-07
Iter: 1824 loss: 5.80952474e-07
Iter: 1825 loss: 5.72393901e-07
Iter: 1826 loss: 5.72062902e-07
Iter: 1827 loss: 5.72067279e-07
Iter: 1828 loss: 5.71806368e-07
Iter: 1829 loss: 5.72723764e-07
Iter: 1830 loss: 5.71730538e-07
Iter: 1831 loss: 5.71548355e-07
Iter: 1832 loss: 5.71591386e-07
Iter: 1833 loss: 5.71438761e-07
Iter: 1834 loss: 5.71170062e-07
Iter: 1835 loss: 5.72653562e-07
Iter: 1836 loss: 5.71127202e-07
Iter: 1837 loss: 5.70963948e-07
Iter: 1838 loss: 5.70637326e-07
Iter: 1839 loss: 5.77107e-07
Iter: 1840 loss: 5.7064824e-07
Iter: 1841 loss: 5.70264888e-07
Iter: 1842 loss: 5.7250088e-07
Iter: 1843 loss: 5.70201166e-07
Iter: 1844 loss: 5.69829922e-07
Iter: 1845 loss: 5.70311386e-07
Iter: 1846 loss: 5.69628355e-07
Iter: 1847 loss: 5.69393478e-07
Iter: 1848 loss: 5.69372673e-07
Iter: 1849 loss: 5.69159965e-07
Iter: 1850 loss: 5.69022e-07
Iter: 1851 loss: 5.68937708e-07
Iter: 1852 loss: 5.68729433e-07
Iter: 1853 loss: 5.69253189e-07
Iter: 1854 loss: 5.68636267e-07
Iter: 1855 loss: 5.68332723e-07
Iter: 1856 loss: 5.69887106e-07
Iter: 1857 loss: 5.68294809e-07
Iter: 1858 loss: 5.68131384e-07
Iter: 1859 loss: 5.67763777e-07
Iter: 1860 loss: 5.73778493e-07
Iter: 1861 loss: 5.67756445e-07
Iter: 1862 loss: 5.67529185e-07
Iter: 1863 loss: 5.6750406e-07
Iter: 1864 loss: 5.67259349e-07
Iter: 1865 loss: 5.67227175e-07
Iter: 1866 loss: 5.67044424e-07
Iter: 1867 loss: 5.66752874e-07
Iter: 1868 loss: 5.66548692e-07
Iter: 1869 loss: 5.66458198e-07
Iter: 1870 loss: 5.66268341e-07
Iter: 1871 loss: 5.66227811e-07
Iter: 1872 loss: 5.66015274e-07
Iter: 1873 loss: 5.65701271e-07
Iter: 1874 loss: 5.65687e-07
Iter: 1875 loss: 5.65382038e-07
Iter: 1876 loss: 5.6587362e-07
Iter: 1877 loss: 5.65215885e-07
Iter: 1878 loss: 5.649066e-07
Iter: 1879 loss: 5.67416578e-07
Iter: 1880 loss: 5.64912227e-07
Iter: 1881 loss: 5.64654101e-07
Iter: 1882 loss: 5.6606109e-07
Iter: 1883 loss: 5.64613913e-07
Iter: 1884 loss: 5.64384322e-07
Iter: 1885 loss: 5.64269271e-07
Iter: 1886 loss: 5.6412506e-07
Iter: 1887 loss: 5.63852268e-07
Iter: 1888 loss: 5.64221409e-07
Iter: 1889 loss: 5.63705271e-07
Iter: 1890 loss: 5.63470735e-07
Iter: 1891 loss: 5.63468461e-07
Iter: 1892 loss: 5.63292645e-07
Iter: 1893 loss: 5.63061803e-07
Iter: 1894 loss: 5.63043955e-07
Iter: 1895 loss: 5.62783612e-07
Iter: 1896 loss: 5.62652815e-07
Iter: 1897 loss: 5.62525e-07
Iter: 1898 loss: 5.6240583e-07
Iter: 1899 loss: 5.62303455e-07
Iter: 1900 loss: 5.62156856e-07
Iter: 1901 loss: 5.61775e-07
Iter: 1902 loss: 5.65298e-07
Iter: 1903 loss: 5.61732634e-07
Iter: 1904 loss: 5.61372417e-07
Iter: 1905 loss: 5.65507662e-07
Iter: 1906 loss: 5.61352806e-07
Iter: 1907 loss: 5.61023569e-07
Iter: 1908 loss: 5.62045898e-07
Iter: 1909 loss: 5.60899252e-07
Iter: 1910 loss: 5.60693763e-07
Iter: 1911 loss: 5.60366e-07
Iter: 1912 loss: 5.60351054e-07
Iter: 1913 loss: 5.6000863e-07
Iter: 1914 loss: 5.61775778e-07
Iter: 1915 loss: 5.59932801e-07
Iter: 1916 loss: 5.59733905e-07
Iter: 1917 loss: 5.59700482e-07
Iter: 1918 loss: 5.5960129e-07
Iter: 1919 loss: 5.593227e-07
Iter: 1920 loss: 5.61924821e-07
Iter: 1921 loss: 5.59285127e-07
Iter: 1922 loss: 5.58887848e-07
Iter: 1923 loss: 5.60824049e-07
Iter: 1924 loss: 5.58857664e-07
Iter: 1925 loss: 5.58558668e-07
Iter: 1926 loss: 5.61896854e-07
Iter: 1927 loss: 5.58550425e-07
Iter: 1928 loss: 5.58348461e-07
Iter: 1929 loss: 5.58134388e-07
Iter: 1930 loss: 5.58081865e-07
Iter: 1931 loss: 5.57696126e-07
Iter: 1932 loss: 5.57789861e-07
Iter: 1933 loss: 5.5741657e-07
Iter: 1934 loss: 5.57062e-07
Iter: 1935 loss: 5.58173156e-07
Iter: 1936 loss: 5.56948919e-07
Iter: 1937 loss: 5.56787882e-07
Iter: 1938 loss: 5.56747295e-07
Iter: 1939 loss: 5.56577561e-07
Iter: 1940 loss: 5.56271743e-07
Iter: 1941 loss: 5.56268787e-07
Iter: 1942 loss: 5.5598656e-07
Iter: 1943 loss: 5.57393207e-07
Iter: 1944 loss: 5.55951715e-07
Iter: 1945 loss: 5.55608153e-07
Iter: 1946 loss: 5.56406235e-07
Iter: 1947 loss: 5.5547514e-07
Iter: 1948 loss: 5.55249528e-07
Iter: 1949 loss: 5.55191491e-07
Iter: 1950 loss: 5.55047905e-07
Iter: 1951 loss: 5.54900794e-07
Iter: 1952 loss: 5.54851511e-07
Iter: 1953 loss: 5.54707242e-07
Iter: 1954 loss: 5.54353733e-07
Iter: 1955 loss: 5.57930434e-07
Iter: 1956 loss: 5.54293479e-07
Iter: 1957 loss: 5.53968334e-07
Iter: 1958 loss: 5.55653855e-07
Iter: 1959 loss: 5.53914617e-07
Iter: 1960 loss: 5.53730331e-07
Iter: 1961 loss: 5.53721236e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6
+ date
Wed Oct 21 11:12:28 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee813df158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81429f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81429268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812f4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812ef598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812ef840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812796a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81215598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81215ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81297598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee8129d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d7c1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d7c1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee811b9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee811b9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d77c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d77c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d6fd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d6c5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d6aed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee811d99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d626ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d656620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d761268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d761a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d5a7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d58a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d59b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d52a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d552ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d60a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4ed6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4ed730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4f0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4da8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d673268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.025194433
test_loss: 0.023110636
train_loss: 0.010310841
test_loss: 0.010788235
train_loss: 0.0063897427
test_loss: 0.0069764024
train_loss: 0.0051481547
test_loss: 0.0055416953
train_loss: 0.004544401
test_loss: 0.004927233
train_loss: 0.0041004224
test_loss: 0.0046658306
train_loss: 0.004025884
test_loss: 0.0046371454
train_loss: 0.0040539065
test_loss: 0.0048913504
train_loss: 0.0037005583
test_loss: 0.004320377
train_loss: 0.0038519832
test_loss: 0.004373812
train_loss: 0.0038969778
test_loss: 0.004351088
train_loss: 0.0034851735
test_loss: 0.0041042506
train_loss: 0.0036536297
test_loss: 0.003953703
train_loss: 0.0032386193
test_loss: 0.004076515
train_loss: 0.0036976095
test_loss: 0.0042083035
train_loss: 0.003200599
test_loss: 0.0038717333
train_loss: 0.003669896
test_loss: 0.0039634537
train_loss: 0.0036293578
test_loss: 0.0041042273
train_loss: 0.0035273824
test_loss: 0.00402706
train_loss: 0.0034000925
test_loss: 0.0039601573
train_loss: 0.0032257442
test_loss: 0.0039587985
train_loss: 0.0031617791
test_loss: 0.0041256854
train_loss: 0.0032776278
test_loss: 0.004033732
train_loss: 0.003489577
test_loss: 0.0039704284
train_loss: 0.0032581529
test_loss: 0.0038812568
train_loss: 0.003302399
test_loss: 0.0038640324
train_loss: 0.0031706307
test_loss: 0.0038777892
train_loss: 0.0032950114
test_loss: 0.003842349
train_loss: 0.0035696058
test_loss: 0.003902768
train_loss: 0.0030589092
test_loss: 0.003805045
train_loss: 0.0032939606
test_loss: 0.003905695
train_loss: 0.0032471388
test_loss: 0.0039141746
train_loss: 0.0032466738
test_loss: 0.003913614
train_loss: 0.0033660373
test_loss: 0.0039042698
train_loss: 0.0031094453
test_loss: 0.003895641
train_loss: 0.0033132443
test_loss: 0.0039754156
train_loss: 0.0030440064
test_loss: 0.0038598785
train_loss: 0.003345397
test_loss: 0.0037141573
train_loss: 0.0031742766
test_loss: 0.0037122394
train_loss: 0.002975363
test_loss: 0.0038592075
train_loss: 0.0033824132
test_loss: 0.0038594436
train_loss: 0.0031651778
test_loss: 0.003784422
train_loss: 0.0029288712
test_loss: 0.0037089777
train_loss: 0.0028629424
test_loss: 0.003744931
train_loss: 0.0030417724
test_loss: 0.0038431708
train_loss: 0.0032339122
test_loss: 0.00375414
train_loss: 0.0032499414
test_loss: 0.0038089429
train_loss: 0.0033793258
test_loss: 0.0040466976
train_loss: 0.0033395006
test_loss: 0.0042521097
train_loss: 0.0033296042
test_loss: 0.003989337
train_loss: 0.0030479324
test_loss: 0.0037267958
train_loss: 0.003040888
test_loss: 0.0038123918
train_loss: 0.002938806
test_loss: 0.00356653
train_loss: 0.0032026
test_loss: 0.0036538597
train_loss: 0.0030390634
test_loss: 0.003685207
train_loss: 0.0032275766
test_loss: 0.0037612417
train_loss: 0.0029279138
test_loss: 0.0037155706
train_loss: 0.0031224648
test_loss: 0.0037791994
train_loss: 0.0031286594
test_loss: 0.0038725052
train_loss: 0.0031681855
test_loss: 0.0037410152
train_loss: 0.0031848645
test_loss: 0.0038372234
train_loss: 0.00307099
test_loss: 0.0036107765
train_loss: 0.002901545
test_loss: 0.003613769
train_loss: 0.0029660857
test_loss: 0.0035885198
train_loss: 0.0028820797
test_loss: 0.0038036003
train_loss: 0.0030736523
test_loss: 0.0035439965
train_loss: 0.0030246992
test_loss: 0.0037498293
train_loss: 0.0032962984
test_loss: 0.004160189
train_loss: 0.003458164
test_loss: 0.0039314656
train_loss: 0.0030575683
test_loss: 0.0037646634
train_loss: 0.003350905
test_loss: 0.0037582116
train_loss: 0.0031277672
test_loss: 0.0036485742
train_loss: 0.002863029
test_loss: 0.0036082736
train_loss: 0.0029652487
test_loss: 0.0036596362
train_loss: 0.0030650645
test_loss: 0.0036598444
train_loss: 0.0029050077
test_loss: 0.0038145753
train_loss: 0.003032151
test_loss: 0.0036894337
train_loss: 0.002973441
test_loss: 0.0036701972
train_loss: 0.002881941
test_loss: 0.0036679185
train_loss: 0.003000666
test_loss: 0.003624764
train_loss: 0.0033266996
test_loss: 0.0036752517
train_loss: 0.0029354435
test_loss: 0.0035241474
train_loss: 0.0028345035
test_loss: 0.0036741
train_loss: 0.0030226128
test_loss: 0.0036759083
train_loss: 0.0028988435
test_loss: 0.0038330383
train_loss: 0.0031913763
test_loss: 0.0036865815
train_loss: 0.003189894
test_loss: 0.003715343
train_loss: 0.0027654222
test_loss: 0.0035954476
train_loss: 0.003150758
test_loss: 0.0036764215
train_loss: 0.0030954543
test_loss: 0.0037503575
train_loss: 0.003275795
test_loss: 0.003607842
train_loss: 0.0035222836
test_loss: 0.0039272304
train_loss: 0.0033486062
test_loss: 0.0039387015
train_loss: 0.0029770704
test_loss: 0.0035907377
train_loss: 0.0030225338
test_loss: 0.0036193426
train_loss: 0.0031252555
test_loss: 0.0037949462
train_loss: 0.002927986
test_loss: 0.003545851
train_loss: 0.0031897714
test_loss: 0.0036758287
train_loss: 0.00304558
test_loss: 0.0035952167
train_loss: 0.0028907005
test_loss: 0.0035703809
train_loss: 0.0028551607
test_loss: 0.003632332
train_loss: 0.0028625177
test_loss: 0.0037777887
train_loss: 0.003197608
test_loss: 0.0036930216
train_loss: 0.003063746
test_loss: 0.0036270597
train_loss: 0.0029527394
test_loss: 0.003598627
train_loss: 0.0030734306
test_loss: 0.0035080684
train_loss: 0.0031148943
test_loss: 0.0036569056
train_loss: 0.0031162426
test_loss: 0.0036965352
train_loss: 0.002793735
test_loss: 0.0035429797
train_loss: 0.0030805566
test_loss: 0.0035965508
train_loss: 0.0029374138
test_loss: 0.0035569181
train_loss: 0.0028767865
test_loss: 0.0035153716
train_loss: 0.0027027573
test_loss: 0.003510368
train_loss: 0.0029938358
test_loss: 0.0036478892
train_loss: 0.002877255
test_loss: 0.0035644085
train_loss: 0.0029315865
test_loss: 0.0035883836
train_loss: 0.003148341
test_loss: 0.0036027245
train_loss: 0.0028622858
test_loss: 0.0037333136
train_loss: 0.002933751
test_loss: 0.003489398
train_loss: 0.0028089767
test_loss: 0.003548242
train_loss: 0.003005623
test_loss: 0.003625554
train_loss: 0.0028554625
test_loss: 0.0035614474
train_loss: 0.0030609185
test_loss: 0.0037922242
train_loss: 0.002963445
test_loss: 0.0034892834
train_loss: 0.0030901483
test_loss: 0.0036154513
train_loss: 0.0031753043
test_loss: 0.0037633495
train_loss: 0.0033824865
test_loss: 0.0036542956
train_loss: 0.0033060913
test_loss: 0.003656802
train_loss: 0.0032626255
test_loss: 0.0036742769
train_loss: 0.002852629
test_loss: 0.0037284396
train_loss: 0.0032796473
test_loss: 0.0036807293
train_loss: 0.00271149
test_loss: 0.003553167
train_loss: 0.0026753661
test_loss: 0.00345967
train_loss: 0.0026685856
test_loss: 0.0035688998
train_loss: 0.0031835968
test_loss: 0.0036233529
train_loss: 0.0029525298
test_loss: 0.0036478597
train_loss: 0.0029213473
test_loss: 0.0037102895
train_loss: 0.002891866
test_loss: 0.003683831
train_loss: 0.0029333204
test_loss: 0.0035722926
train_loss: 0.0029431519
test_loss: 0.0038019489
train_loss: 0.003229297
test_loss: 0.003790273
train_loss: 0.0029963767
test_loss: 0.0035170175
train_loss: 0.0030789382
test_loss: 0.0035909554
train_loss: 0.00312832
test_loss: 0.0034677628
train_loss: 0.0031166554
test_loss: 0.0036411923
train_loss: 0.0029026808
test_loss: 0.0036306987
train_loss: 0.0029749214
test_loss: 0.0036327266
train_loss: 0.0028737045
test_loss: 0.0038808207
train_loss: 0.0028568497
test_loss: 0.0035378635
train_loss: 0.002744075
test_loss: 0.0035089094
train_loss: 0.0029269224
test_loss: 0.0035761988
train_loss: 0.0031937184
test_loss: 0.0036028004
train_loss: 0.003247986
test_loss: 0.0036943448
train_loss: 0.0027181203
test_loss: 0.0036904525
train_loss: 0.0027926848
test_loss: 0.003474862
train_loss: 0.0026403589
test_loss: 0.0035099678
train_loss: 0.0030719098
test_loss: 0.003603512
train_loss: 0.0027578715
test_loss: 0.0035831982
train_loss: 0.0029677262
test_loss: 0.0035835824
train_loss: 0.0028936062
test_loss: 0.0035495725
train_loss: 0.0028784804
test_loss: 0.0036186944
train_loss: 0.002779987
test_loss: 0.0035799758
train_loss: 0.0031392942
test_loss: 0.0036591557
train_loss: 0.0028588283
test_loss: 0.003376408
train_loss: 0.002852135
test_loss: 0.0035133874
train_loss: 0.0027411848
test_loss: 0.003571226
train_loss: 0.0028520087
test_loss: 0.0035053839
train_loss: 0.0029995476
test_loss: 0.0035636427
train_loss: 0.002719915
test_loss: 0.003424328
train_loss: 0.0029842432
test_loss: 0.0036014493
train_loss: 0.0026673824
test_loss: 0.0036272835
train_loss: 0.0029081032
test_loss: 0.0035063066
train_loss: 0.0027544403
test_loss: 0.0034714208
train_loss: 0.0026343542
test_loss: 0.0035003081
train_loss: 0.0028731618
test_loss: 0.0033882926
train_loss: 0.0027693326
test_loss: 0.0035169336
train_loss: 0.0028663522
test_loss: 0.0034112083
train_loss: 0.002747799
test_loss: 0.003552817
train_loss: 0.002837382
test_loss: 0.0034938997
train_loss: 0.0029429472
test_loss: 0.0035976171
train_loss: 0.0027898212
test_loss: 0.0034525385
train_loss: 0.0027208833
test_loss: 0.0034761978
train_loss: 0.002852776
test_loss: 0.003529151
train_loss: 0.002935764
test_loss: 0.0036161134
train_loss: 0.0027843132
test_loss: 0.0034322976
train_loss: 0.0027609617
test_loss: 0.0035088062
train_loss: 0.0026829098
test_loss: 0.0036042384
train_loss: 0.0032645655
test_loss: 0.0035259088
train_loss: 0.0029549436
test_loss: 0.0036564092
train_loss: 0.0029499857
test_loss: 0.003631931
train_loss: 0.002750425
test_loss: 0.003483084
train_loss: 0.0028624614
test_loss: 0.0036002004
train_loss: 0.0030468656
test_loss: 0.0036573964
train_loss: 0.0028833176
test_loss: 0.0034793476
train_loss: 0.0028541868
test_loss: 0.0034817238
train_loss: 0.0028768494
test_loss: 0.0036347879
train_loss: 0.0031572252
test_loss: 0.0037191287
train_loss: 0.002836349
test_loss: 0.0036435435
train_loss: 0.0029057432
test_loss: 0.003509931
train_loss: 0.0029617993
test_loss: 0.0035664958
train_loss: 0.0026869
test_loss: 0.0034349007
train_loss: 0.002738635
test_loss: 0.0035018462
train_loss: 0.002650505
test_loss: 0.003415637
train_loss: 0.002967069
test_loss: 0.0037233087
train_loss: 0.0029728513
test_loss: 0.0036340826
train_loss: 0.0029142955
test_loss: 0.003645205
train_loss: 0.0026007365
test_loss: 0.0035181392
train_loss: 0.0027762263
test_loss: 0.003365362
train_loss: 0.0025828886
test_loss: 0.0034622103
train_loss: 0.0026776171
test_loss: 0.003530883
train_loss: 0.0028911894
test_loss: 0.003568275
train_loss: 0.0031026264
test_loss: 0.0036909545
train_loss: 0.0033109118
test_loss: 0.0036241328
train_loss: 0.0029827282
test_loss: 0.0038153597
train_loss: 0.0027518698
test_loss: 0.0035452298
train_loss: 0.0026707705
test_loss: 0.0034615532
train_loss: 0.0027483148
test_loss: 0.0034717747
train_loss: 0.0029308568
test_loss: 0.003680856
train_loss: 0.003044969
test_loss: 0.0034636213
train_loss: 0.002953954
test_loss: 0.0037542987
train_loss: 0.003003291
test_loss: 0.003823113
train_loss: 0.0031732689
test_loss: 0.003720542
train_loss: 0.003035988
test_loss: 0.0036456601
train_loss: 0.0027841162
test_loss: 0.0038518365
train_loss: 0.0030715931
test_loss: 0.0036400694
train_loss: 0.0032187318
test_loss: 0.0037891697
train_loss: 0.0028598248
test_loss: 0.003537194
train_loss: 0.0027437173
test_loss: 0.0033845971
train_loss: 0.0027679687
test_loss: 0.0036416869
train_loss: 0.003135328
test_loss: 0.0040364508
train_loss: 0.0027697433
test_loss: 0.0035608374
train_loss: 0.0030193096
test_loss: 0.003545773
train_loss: 0.002925136
test_loss: 0.0035090137
train_loss: 0.0028583899
test_loss: 0.003519357
train_loss: 0.0029421297
test_loss: 0.0035916783
train_loss: 0.002747239
test_loss: 0.003560001
train_loss: 0.0027080122
test_loss: 0.0033873178
train_loss: 0.0027677165
test_loss: 0.0034754013
train_loss: 0.0031422921
test_loss: 0.0039772647
train_loss: 0.0028983592
test_loss: 0.0037232114
train_loss: 0.0028942048
test_loss: 0.003769643
train_loss: 0.0027787765
test_loss: 0.0035911496
train_loss: 0.0029230262
test_loss: 0.003793312
train_loss: 0.0027649752
test_loss: 0.0036232912
train_loss: 0.0028759588
test_loss: 0.0035029189
train_loss: 0.0026381866
test_loss: 0.0034675244
train_loss: 0.0026246926
test_loss: 0.0034703454
train_loss: 0.0027601318
test_loss: 0.00341854
train_loss: 0.0030260868
test_loss: 0.0035969706
train_loss: 0.0028003943
test_loss: 0.0035250217
train_loss: 0.0027795385
test_loss: 0.0035082148
train_loss: 0.0027664346
test_loss: 0.0034375447
train_loss: 0.0028067513
test_loss: 0.003605116
train_loss: 0.0029763903
test_loss: 0.003496069
train_loss: 0.0026995775
test_loss: 0.0033905134
train_loss: 0.002756687
test_loss: 0.0033996417
train_loss: 0.0026535066
test_loss: 0.0034755028
train_loss: 0.0029052568
test_loss: 0.0034978641
train_loss: 0.0028094107
test_loss: 0.003555657
train_loss: 0.0030502258
test_loss: 0.00364097
train_loss: 0.0028081965
test_loss: 0.0036562171
train_loss: 0.0028560408
test_loss: 0.0035784517
train_loss: 0.0029341998
test_loss: 0.0034889274
train_loss: 0.0027865516
test_loss: 0.0034714502
train_loss: 0.002664049
test_loss: 0.0033623676
train_loss: 0.0028058197
test_loss: 0.0034668718
train_loss: 0.0028034814
test_loss: 0.003758159
train_loss: 0.00271322
test_loss: 0.0035413338
train_loss: 0.002708406
test_loss: 0.0035605552
train_loss: 0.0028300916
test_loss: 0.0035655801
train_loss: 0.002685506
test_loss: 0.0035234015
train_loss: 0.0030162027
test_loss: 0.003438278
train_loss: 0.0029662475
test_loss: 0.003619555
train_loss: 0.0027148128
test_loss: 0.0035323696
train_loss: 0.0026829836
test_loss: 0.0034789871
train_loss: 0.002907959
test_loss: 0.0038307533
train_loss: 0.0027909388
test_loss: 0.0035472647
train_loss: 0.0028285459
test_loss: 0.0035651668
train_loss: 0.002766843
test_loss: 0.0036169433
train_loss: 0.0028579938
test_loss: 0.003465784
train_loss: 0.0028299459
test_loss: 0.0035696423
train_loss: 0.0030124635
test_loss: 0.0035184496
train_loss: 0.0027219562
test_loss: 0.0033826723
train_loss: 0.002661201
test_loss: 0.0033943895
train_loss: 0.0026849462
test_loss: 0.0036153747
train_loss: 0.002888586
test_loss: 0.0035569558
train_loss: 0.0030185392
test_loss: 0.003604468
train_loss: 0.0027685617
test_loss: 0.0036190783
train_loss: 0.002805593
test_loss: 0.00341782
train_loss: 0.0025422175
test_loss: 0.003394879
train_loss: 0.002932062
test_loss: 0.0036453574
train_loss: 0.0028611978
test_loss: 0.003536034
train_loss: 0.0028123066
test_loss: 0.0034506128
train_loss: 0.00291736
test_loss: 0.0035616788
train_loss: 0.00291675
test_loss: 0.0036218492
train_loss: 0.0028547787
test_loss: 0.0037921644
train_loss: 0.0027452265
test_loss: 0.0035464007
train_loss: 0.0027861726
test_loss: 0.0036519743
train_loss: 0.002749468
test_loss: 0.0034882275
train_loss: 0.0027963126
test_loss: 0.003562525
train_loss: 0.0028435031
test_loss: 0.003572203
train_loss: 0.002677355
test_loss: 0.0035496447
train_loss: 0.0027444197
test_loss: 0.0034572629
train_loss: 0.0027560666
test_loss: 0.003414849
train_loss: 0.0027662767
test_loss: 0.003385235
train_loss: 0.002556981
test_loss: 0.003335892
train_loss: 0.002625689
test_loss: 0.0033425698
train_loss: 0.002791469
test_loss: 0.0034735631
train_loss: 0.0028822634
test_loss: 0.0034684893
train_loss: 0.0026253588
test_loss: 0.0033473806
train_loss: 0.0025504394
test_loss: 0.003384721
train_loss: 0.0028322095
test_loss: 0.0034621053
train_loss: 0.0027670169
test_loss: 0.0034692176
train_loss: 0.0027754628
test_loss: 0.0034996413
train_loss: 0.002820869
test_loss: 0.0035934863
train_loss: 0.0030362855
test_loss: 0.003546401
train_loss: 0.0030920883
test_loss: 0.003591036
train_loss: 0.0027525814
test_loss: 0.0035640462
train_loss: 0.0027084043
test_loss: 0.0034442917
train_loss: 0.002838762
test_loss: 0.0035925875
train_loss: 0.0026209902
test_loss: 0.0035216324
train_loss: 0.0027898934
test_loss: 0.0034359181
train_loss: 0.0029634715
test_loss: 0.003556626
train_loss: 0.0027387897
test_loss: 0.0035732682
train_loss: 0.002908708
test_loss: 0.0036236658
train_loss: 0.0030117137
test_loss: 0.0035683836
train_loss: 0.0025604859
test_loss: 0.003514366
train_loss: 0.002510483
test_loss: 0.0033008154
train_loss: 0.002830075
test_loss: 0.003543705
train_loss: 0.0029278002
test_loss: 0.0036412177
train_loss: 0.0028854022
test_loss: 0.0035969247
train_loss: 0.002672387
test_loss: 0.0035165
train_loss: 0.0026935318
test_loss: 0.0034021817
train_loss: 0.0028068204
test_loss: 0.0034845236
train_loss: 0.0026685237
test_loss: 0.0034821932
train_loss: 0.0028340076
test_loss: 0.0034771326
train_loss: 0.0026680012
test_loss: 0.0033820367
train_loss: 0.0027026166
test_loss: 0.0033927374
train_loss: 0.002621938
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0033718268
train_loss: 0.0028831642
test_loss: 0.0036163565
train_loss: 0.0028631624
test_loss: 0.003777802
train_loss: 0.002583895
test_loss: 0.0033533284
train_loss: 0.0027490614
test_loss: 0.003468036
train_loss: 0.0027345512
test_loss: 0.003370081
train_loss: 0.002703107
test_loss: 0.0034602731
train_loss: 0.0026799769
test_loss: 0.0033333814
train_loss: 0.0025981641
test_loss: 0.0035516394
train_loss: 0.0029105826
test_loss: 0.0035868532
train_loss: 0.002624886
test_loss: 0.003348778
train_loss: 0.0027439469
test_loss: 0.0035794894
train_loss: 0.0029758476
test_loss: 0.0036566532
train_loss: 0.002848931
test_loss: 0.0035978635
train_loss: 0.0026759403
test_loss: 0.0034384218
train_loss: 0.0028853333
test_loss: 0.0035702263
train_loss: 0.0028339908
test_loss: 0.0034181934
train_loss: 0.0029688873
test_loss: 0.0035700086
train_loss: 0.0028755302
test_loss: 0.003391671
train_loss: 0.0026547275
test_loss: 0.0035549547
train_loss: 0.0026906854
test_loss: 0.0034214174
train_loss: 0.0026764479
test_loss: 0.0035095455
train_loss: 0.0027557183
test_loss: 0.0035445194
train_loss: 0.0025431612
test_loss: 0.003316021
train_loss: 0.0025826297
test_loss: 0.0035065247
train_loss: 0.0027827797
test_loss: 0.0034740416
train_loss: 0.0029307697
test_loss: 0.0034419857
train_loss: 0.0026256875
test_loss: 0.0033539638
train_loss: 0.0025865885
test_loss: 0.00334915
train_loss: 0.0026383854
test_loss: 0.0034623016
train_loss: 0.0026779487
test_loss: 0.0033359984
train_loss: 0.0027340276
test_loss: 0.003374681
train_loss: 0.0026981414
test_loss: 0.0034982287
train_loss: 0.0026017786
test_loss: 0.003395537
train_loss: 0.0026805154
test_loss: 0.0036029115
train_loss: 0.0029659495
test_loss: 0.0036738839
train_loss: 0.0029576651
test_loss: 0.0035396868
train_loss: 0.0027964418
test_loss: 0.0034693992
train_loss: 0.0030361924
test_loss: 0.003419796
train_loss: 0.002599189
test_loss: 0.0036259864
train_loss: 0.0026632852
test_loss: 0.003484903
train_loss: 0.0027521858
test_loss: 0.0035516552
train_loss: 0.0030925511
test_loss: 0.0034198042
train_loss: 0.0030966988
test_loss: 0.0037768583
train_loss: 0.0029135533
test_loss: 0.0035740172
train_loss: 0.0029283806
test_loss: 0.0036423337
train_loss: 0.0026666597
test_loss: 0.003600704
train_loss: 0.002763742
test_loss: 0.0034406537
train_loss: 0.0026708941
test_loss: 0.00335256
train_loss: 0.0027775895
test_loss: 0.0033719984
train_loss: 0.0027703005
test_loss: 0.003506965
train_loss: 0.002602586
test_loss: 0.0035044774
train_loss: 0.0025832744
test_loss: 0.003363342
train_loss: 0.0028076593
test_loss: 0.0036327387
train_loss: 0.0028053906
test_loss: 0.0034676034
train_loss: 0.0026996483
test_loss: 0.0035479185
train_loss: 0.0026676415
test_loss: 0.003548929
train_loss: 0.0026622093
test_loss: 0.0034754404
train_loss: 0.0027137618
test_loss: 0.003487335
train_loss: 0.0025696382
test_loss: 0.0034455708
train_loss: 0.00280922
test_loss: 0.003439209
train_loss: 0.0026961837
test_loss: 0.0035228934
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e95f32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e95f3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e965c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e9562b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e94ec1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e95017b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e94b29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e947e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e947e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e947e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e9476950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e978d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e940c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93d4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93d4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93a1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93a0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93359d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92e7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e9310f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92b4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92b4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d48906a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92ba158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92b4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d4859488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d4806598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d482a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d47b7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d47e0d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d477d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d477da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d477dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d47547b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d477d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d46c5048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.32208734e-05
Iter: 2 loss: 1.09611901e-05
Iter: 3 loss: 1.07958094e-05
Iter: 4 loss: 9.96090785e-06
Iter: 5 loss: 1.14697614e-05
Iter: 6 loss: 9.59993395e-06
Iter: 7 loss: 8.78153332e-06
Iter: 8 loss: 1.47096916e-05
Iter: 9 loss: 8.71147586e-06
Iter: 10 loss: 8.30887802e-06
Iter: 11 loss: 8.07907691e-06
Iter: 12 loss: 7.90594277e-06
Iter: 13 loss: 7.35863296e-06
Iter: 14 loss: 8.42571444e-06
Iter: 15 loss: 7.13198961e-06
Iter: 16 loss: 6.63300398e-06
Iter: 17 loss: 1.38318337e-05
Iter: 18 loss: 6.63202536e-06
Iter: 19 loss: 6.37626772e-06
Iter: 20 loss: 6.2019908e-06
Iter: 21 loss: 6.10775623e-06
Iter: 22 loss: 5.68936048e-06
Iter: 23 loss: 6.19517732e-06
Iter: 24 loss: 5.46993488e-06
Iter: 25 loss: 5.36160223e-06
Iter: 26 loss: 5.30999023e-06
Iter: 27 loss: 5.16716545e-06
Iter: 28 loss: 4.89863623e-06
Iter: 29 loss: 1.08347658e-05
Iter: 30 loss: 4.89791455e-06
Iter: 31 loss: 4.61728587e-06
Iter: 32 loss: 5.67663346e-06
Iter: 33 loss: 4.54993415e-06
Iter: 34 loss: 4.32851175e-06
Iter: 35 loss: 4.29438569e-06
Iter: 36 loss: 4.14077158e-06
Iter: 37 loss: 3.84357872e-06
Iter: 38 loss: 5.31209844e-06
Iter: 39 loss: 3.79297626e-06
Iter: 40 loss: 3.57782915e-06
Iter: 41 loss: 5.10667496e-06
Iter: 42 loss: 3.55861289e-06
Iter: 43 loss: 3.39978556e-06
Iter: 44 loss: 5.11470353e-06
Iter: 45 loss: 3.39612e-06
Iter: 46 loss: 3.28133092e-06
Iter: 47 loss: 3.41964369e-06
Iter: 48 loss: 3.22107576e-06
Iter: 49 loss: 3.12973043e-06
Iter: 50 loss: 3.31936553e-06
Iter: 51 loss: 3.093337e-06
Iter: 52 loss: 3.00165675e-06
Iter: 53 loss: 4.24167501e-06
Iter: 54 loss: 3.00118541e-06
Iter: 55 loss: 2.94764322e-06
Iter: 56 loss: 2.96327e-06
Iter: 57 loss: 2.90899561e-06
Iter: 58 loss: 2.82303699e-06
Iter: 59 loss: 3.22491223e-06
Iter: 60 loss: 2.80706445e-06
Iter: 61 loss: 2.75553703e-06
Iter: 62 loss: 2.74355398e-06
Iter: 63 loss: 2.71044064e-06
Iter: 64 loss: 2.6327757e-06
Iter: 65 loss: 2.6175926e-06
Iter: 66 loss: 2.56590556e-06
Iter: 67 loss: 2.51287156e-06
Iter: 68 loss: 2.5035838e-06
Iter: 69 loss: 2.46320838e-06
Iter: 70 loss: 2.4090964e-06
Iter: 71 loss: 2.40621898e-06
Iter: 72 loss: 2.3430855e-06
Iter: 73 loss: 2.47880325e-06
Iter: 74 loss: 2.31860759e-06
Iter: 75 loss: 2.25322765e-06
Iter: 76 loss: 2.26344832e-06
Iter: 77 loss: 2.20386642e-06
Iter: 78 loss: 2.15033378e-06
Iter: 79 loss: 2.70899773e-06
Iter: 80 loss: 2.14893566e-06
Iter: 81 loss: 2.10100779e-06
Iter: 82 loss: 2.27160353e-06
Iter: 83 loss: 2.08869596e-06
Iter: 84 loss: 2.05159427e-06
Iter: 85 loss: 2.55137593e-06
Iter: 86 loss: 2.05139213e-06
Iter: 87 loss: 2.02834235e-06
Iter: 88 loss: 1.99461761e-06
Iter: 89 loss: 1.99366195e-06
Iter: 90 loss: 1.97211716e-06
Iter: 91 loss: 1.9698773e-06
Iter: 92 loss: 1.94647714e-06
Iter: 93 loss: 1.9267577e-06
Iter: 94 loss: 1.9203394e-06
Iter: 95 loss: 1.89493971e-06
Iter: 96 loss: 1.89478521e-06
Iter: 97 loss: 1.8747462e-06
Iter: 98 loss: 1.85834074e-06
Iter: 99 loss: 1.85248359e-06
Iter: 100 loss: 1.82979193e-06
Iter: 101 loss: 1.81854557e-06
Iter: 102 loss: 1.80761344e-06
Iter: 103 loss: 1.77651145e-06
Iter: 104 loss: 2.12115197e-06
Iter: 105 loss: 1.77588072e-06
Iter: 106 loss: 1.76005983e-06
Iter: 107 loss: 1.92702055e-06
Iter: 108 loss: 1.75967295e-06
Iter: 109 loss: 1.74611569e-06
Iter: 110 loss: 1.71861336e-06
Iter: 111 loss: 2.22029303e-06
Iter: 112 loss: 1.71811735e-06
Iter: 113 loss: 1.68880024e-06
Iter: 114 loss: 1.84478517e-06
Iter: 115 loss: 1.68422889e-06
Iter: 116 loss: 1.66238624e-06
Iter: 117 loss: 1.65788867e-06
Iter: 118 loss: 1.64346829e-06
Iter: 119 loss: 1.62506285e-06
Iter: 120 loss: 1.62468314e-06
Iter: 121 loss: 1.606821e-06
Iter: 122 loss: 1.6522331e-06
Iter: 123 loss: 1.60067873e-06
Iter: 124 loss: 1.58914645e-06
Iter: 125 loss: 1.59261378e-06
Iter: 126 loss: 1.58087641e-06
Iter: 127 loss: 1.56063413e-06
Iter: 128 loss: 1.69223404e-06
Iter: 129 loss: 1.55847681e-06
Iter: 130 loss: 1.54799079e-06
Iter: 131 loss: 1.58904299e-06
Iter: 132 loss: 1.54559791e-06
Iter: 133 loss: 1.53250619e-06
Iter: 134 loss: 1.52899702e-06
Iter: 135 loss: 1.52086261e-06
Iter: 136 loss: 1.5078034e-06
Iter: 137 loss: 1.49593029e-06
Iter: 138 loss: 1.49262269e-06
Iter: 139 loss: 1.47207902e-06
Iter: 140 loss: 1.60128684e-06
Iter: 141 loss: 1.46975503e-06
Iter: 142 loss: 1.45778199e-06
Iter: 143 loss: 1.45775709e-06
Iter: 144 loss: 1.448376e-06
Iter: 145 loss: 1.43408602e-06
Iter: 146 loss: 1.43386239e-06
Iter: 147 loss: 1.41958162e-06
Iter: 148 loss: 1.51175141e-06
Iter: 149 loss: 1.41798296e-06
Iter: 150 loss: 1.40533689e-06
Iter: 151 loss: 1.40044506e-06
Iter: 152 loss: 1.39356291e-06
Iter: 153 loss: 1.38081089e-06
Iter: 154 loss: 1.46525917e-06
Iter: 155 loss: 1.37955419e-06
Iter: 156 loss: 1.36930669e-06
Iter: 157 loss: 1.47549326e-06
Iter: 158 loss: 1.36900735e-06
Iter: 159 loss: 1.36108338e-06
Iter: 160 loss: 1.37655479e-06
Iter: 161 loss: 1.35785149e-06
Iter: 162 loss: 1.35268738e-06
Iter: 163 loss: 1.41098349e-06
Iter: 164 loss: 1.35261234e-06
Iter: 165 loss: 1.34695551e-06
Iter: 166 loss: 1.33899789e-06
Iter: 167 loss: 1.33866945e-06
Iter: 168 loss: 1.33185404e-06
Iter: 169 loss: 1.33178696e-06
Iter: 170 loss: 1.32752552e-06
Iter: 171 loss: 1.31566446e-06
Iter: 172 loss: 1.3813858e-06
Iter: 173 loss: 1.31219213e-06
Iter: 174 loss: 1.29876821e-06
Iter: 175 loss: 1.42010686e-06
Iter: 176 loss: 1.29809223e-06
Iter: 177 loss: 1.29049909e-06
Iter: 178 loss: 1.34012078e-06
Iter: 179 loss: 1.28963268e-06
Iter: 180 loss: 1.28088936e-06
Iter: 181 loss: 1.30044839e-06
Iter: 182 loss: 1.27751605e-06
Iter: 183 loss: 1.27233579e-06
Iter: 184 loss: 1.27208978e-06
Iter: 185 loss: 1.26810028e-06
Iter: 186 loss: 1.25966949e-06
Iter: 187 loss: 1.2618566e-06
Iter: 188 loss: 1.2534781e-06
Iter: 189 loss: 1.24698977e-06
Iter: 190 loss: 1.24693111e-06
Iter: 191 loss: 1.24145913e-06
Iter: 192 loss: 1.23995483e-06
Iter: 193 loss: 1.23664631e-06
Iter: 194 loss: 1.22767869e-06
Iter: 195 loss: 1.30580315e-06
Iter: 196 loss: 1.22718484e-06
Iter: 197 loss: 1.22282e-06
Iter: 198 loss: 1.25000247e-06
Iter: 199 loss: 1.22232063e-06
Iter: 200 loss: 1.21747939e-06
Iter: 201 loss: 1.21666335e-06
Iter: 202 loss: 1.21340622e-06
Iter: 203 loss: 1.20841514e-06
Iter: 204 loss: 1.24588951e-06
Iter: 205 loss: 1.20801747e-06
Iter: 206 loss: 1.20335949e-06
Iter: 207 loss: 1.1990852e-06
Iter: 208 loss: 1.1979987e-06
Iter: 209 loss: 1.1923654e-06
Iter: 210 loss: 1.19620336e-06
Iter: 211 loss: 1.18884896e-06
Iter: 212 loss: 1.18207265e-06
Iter: 213 loss: 1.20570348e-06
Iter: 214 loss: 1.18034495e-06
Iter: 215 loss: 1.17336606e-06
Iter: 216 loss: 1.24799487e-06
Iter: 217 loss: 1.17322134e-06
Iter: 218 loss: 1.16907654e-06
Iter: 219 loss: 1.16276942e-06
Iter: 220 loss: 1.16264562e-06
Iter: 221 loss: 1.15549506e-06
Iter: 222 loss: 1.1650875e-06
Iter: 223 loss: 1.15194348e-06
Iter: 224 loss: 1.14316822e-06
Iter: 225 loss: 1.19189076e-06
Iter: 226 loss: 1.14193631e-06
Iter: 227 loss: 1.13706244e-06
Iter: 228 loss: 1.1895014e-06
Iter: 229 loss: 1.13694909e-06
Iter: 230 loss: 1.13208944e-06
Iter: 231 loss: 1.13935516e-06
Iter: 232 loss: 1.12970793e-06
Iter: 233 loss: 1.12590783e-06
Iter: 234 loss: 1.18215007e-06
Iter: 235 loss: 1.12592863e-06
Iter: 236 loss: 1.12344514e-06
Iter: 237 loss: 1.12526072e-06
Iter: 238 loss: 1.12196722e-06
Iter: 239 loss: 1.11884799e-06
Iter: 240 loss: 1.1220784e-06
Iter: 241 loss: 1.11716406e-06
Iter: 242 loss: 1.11322026e-06
Iter: 243 loss: 1.12585178e-06
Iter: 244 loss: 1.11209465e-06
Iter: 245 loss: 1.10858423e-06
Iter: 246 loss: 1.1018459e-06
Iter: 247 loss: 1.24692019e-06
Iter: 248 loss: 1.10187943e-06
Iter: 249 loss: 1.09669088e-06
Iter: 250 loss: 1.14539341e-06
Iter: 251 loss: 1.09647863e-06
Iter: 252 loss: 1.09168468e-06
Iter: 253 loss: 1.12823489e-06
Iter: 254 loss: 1.09135192e-06
Iter: 255 loss: 1.0876297e-06
Iter: 256 loss: 1.08930692e-06
Iter: 257 loss: 1.08505947e-06
Iter: 258 loss: 1.0816043e-06
Iter: 259 loss: 1.07742517e-06
Iter: 260 loss: 1.0769952e-06
Iter: 261 loss: 1.07095866e-06
Iter: 262 loss: 1.10840858e-06
Iter: 263 loss: 1.07027665e-06
Iter: 264 loss: 1.06507264e-06
Iter: 265 loss: 1.08440463e-06
Iter: 266 loss: 1.06381958e-06
Iter: 267 loss: 1.05965228e-06
Iter: 268 loss: 1.05961988e-06
Iter: 269 loss: 1.05742379e-06
Iter: 270 loss: 1.06357106e-06
Iter: 271 loss: 1.05670574e-06
Iter: 272 loss: 1.05389358e-06
Iter: 273 loss: 1.05229947e-06
Iter: 274 loss: 1.05100253e-06
Iter: 275 loss: 1.04803485e-06
Iter: 276 loss: 1.07597077e-06
Iter: 277 loss: 1.04790706e-06
Iter: 278 loss: 1.04565891e-06
Iter: 279 loss: 1.04750688e-06
Iter: 280 loss: 1.04435514e-06
Iter: 281 loss: 1.04153014e-06
Iter: 282 loss: 1.03846901e-06
Iter: 283 loss: 1.03799368e-06
Iter: 284 loss: 1.03440505e-06
Iter: 285 loss: 1.07766937e-06
Iter: 286 loss: 1.03437378e-06
Iter: 287 loss: 1.03182595e-06
Iter: 288 loss: 1.03805746e-06
Iter: 289 loss: 1.0309177e-06
Iter: 290 loss: 1.02739318e-06
Iter: 291 loss: 1.03633352e-06
Iter: 292 loss: 1.02611023e-06
Iter: 293 loss: 1.02344165e-06
Iter: 294 loss: 1.01890851e-06
Iter: 295 loss: 1.01890373e-06
Iter: 296 loss: 1.0143915e-06
Iter: 297 loss: 1.04039293e-06
Iter: 298 loss: 1.01379555e-06
Iter: 299 loss: 1.01028729e-06
Iter: 300 loss: 1.02865579e-06
Iter: 301 loss: 1.00972716e-06
Iter: 302 loss: 1.00741045e-06
Iter: 303 loss: 1.0073785e-06
Iter: 304 loss: 1.00598891e-06
Iter: 305 loss: 1.00757245e-06
Iter: 306 loss: 1.00525699e-06
Iter: 307 loss: 1.00355169e-06
Iter: 308 loss: 1.00707371e-06
Iter: 309 loss: 1.00283103e-06
Iter: 310 loss: 1.00113675e-06
Iter: 311 loss: 1.00027307e-06
Iter: 312 loss: 9.99496e-07
Iter: 313 loss: 9.96480821e-07
Iter: 314 loss: 1.01509022e-06
Iter: 315 loss: 9.96097469e-07
Iter: 316 loss: 9.94271545e-07
Iter: 317 loss: 9.90219633e-07
Iter: 318 loss: 1.04900244e-06
Iter: 319 loss: 9.90051149e-07
Iter: 320 loss: 9.86195118e-07
Iter: 321 loss: 9.86191935e-07
Iter: 322 loss: 9.84414214e-07
Iter: 323 loss: 1.0016912e-06
Iter: 324 loss: 9.84367148e-07
Iter: 325 loss: 9.82682309e-07
Iter: 326 loss: 9.82427764e-07
Iter: 327 loss: 9.81248604e-07
Iter: 328 loss: 9.79077186e-07
Iter: 329 loss: 9.8299131e-07
Iter: 330 loss: 9.78148364e-07
Iter: 331 loss: 9.75793e-07
Iter: 332 loss: 9.75068815e-07
Iter: 333 loss: 9.73688543e-07
Iter: 334 loss: 9.70673113e-07
Iter: 335 loss: 9.79123797e-07
Iter: 336 loss: 9.69719622e-07
Iter: 337 loss: 9.68532277e-07
Iter: 338 loss: 9.67832e-07
Iter: 339 loss: 9.66566176e-07
Iter: 340 loss: 9.65246159e-07
Iter: 341 loss: 9.65027084e-07
Iter: 342 loss: 9.63038929e-07
Iter: 343 loss: 9.76647584e-07
Iter: 344 loss: 9.62873287e-07
Iter: 345 loss: 9.61238243e-07
Iter: 346 loss: 9.5946541e-07
Iter: 347 loss: 9.59227918e-07
Iter: 348 loss: 9.57402e-07
Iter: 349 loss: 9.57396196e-07
Iter: 350 loss: 9.56174745e-07
Iter: 351 loss: 9.54263896e-07
Iter: 352 loss: 9.54200232e-07
Iter: 353 loss: 9.51857487e-07
Iter: 354 loss: 9.57783e-07
Iter: 355 loss: 9.51066227e-07
Iter: 356 loss: 9.48698926e-07
Iter: 357 loss: 9.57716225e-07
Iter: 358 loss: 9.48143452e-07
Iter: 359 loss: 9.45680313e-07
Iter: 360 loss: 9.67553092e-07
Iter: 361 loss: 9.45572708e-07
Iter: 362 loss: 9.44364956e-07
Iter: 363 loss: 9.41646e-07
Iter: 364 loss: 9.79780111e-07
Iter: 365 loss: 9.41510848e-07
Iter: 366 loss: 9.39199424e-07
Iter: 367 loss: 9.71714826e-07
Iter: 368 loss: 9.39192e-07
Iter: 369 loss: 9.3734559e-07
Iter: 370 loss: 9.38332164e-07
Iter: 371 loss: 9.36151594e-07
Iter: 372 loss: 9.35228115e-07
Iter: 373 loss: 9.34934292e-07
Iter: 374 loss: 9.33854494e-07
Iter: 375 loss: 9.33877402e-07
Iter: 376 loss: 9.33003435e-07
Iter: 377 loss: 9.31792215e-07
Iter: 378 loss: 9.32928856e-07
Iter: 379 loss: 9.31069053e-07
Iter: 380 loss: 9.29033e-07
Iter: 381 loss: 9.31335308e-07
Iter: 382 loss: 9.2789378e-07
Iter: 383 loss: 9.2665465e-07
Iter: 384 loss: 9.31342868e-07
Iter: 385 loss: 9.26346274e-07
Iter: 386 loss: 9.24540871e-07
Iter: 387 loss: 9.2320289e-07
Iter: 388 loss: 9.22627123e-07
Iter: 389 loss: 9.20734067e-07
Iter: 390 loss: 9.22189201e-07
Iter: 391 loss: 9.1953973e-07
Iter: 392 loss: 9.17030377e-07
Iter: 393 loss: 9.22659069e-07
Iter: 394 loss: 9.16088084e-07
Iter: 395 loss: 9.13667805e-07
Iter: 396 loss: 9.26709561e-07
Iter: 397 loss: 9.13311794e-07
Iter: 398 loss: 9.11972108e-07
Iter: 399 loss: 9.11949428e-07
Iter: 400 loss: 9.10675681e-07
Iter: 401 loss: 9.08113066e-07
Iter: 402 loss: 9.55786732e-07
Iter: 403 loss: 9.08065658e-07
Iter: 404 loss: 9.05644583e-07
Iter: 405 loss: 9.12351e-07
Iter: 406 loss: 9.04869864e-07
Iter: 407 loss: 9.0272539e-07
Iter: 408 loss: 9.13573e-07
Iter: 409 loss: 9.02372108e-07
Iter: 410 loss: 9.01149178e-07
Iter: 411 loss: 9.01130079e-07
Iter: 412 loss: 8.99770043e-07
Iter: 413 loss: 9.00573923e-07
Iter: 414 loss: 8.98939561e-07
Iter: 415 loss: 8.97969699e-07
Iter: 416 loss: 8.98073949e-07
Iter: 417 loss: 8.97249379e-07
Iter: 418 loss: 8.95679534e-07
Iter: 419 loss: 9.03133127e-07
Iter: 420 loss: 8.95375251e-07
Iter: 421 loss: 8.93920514e-07
Iter: 422 loss: 8.9763995e-07
Iter: 423 loss: 8.93395395e-07
Iter: 424 loss: 8.9248681e-07
Iter: 425 loss: 8.9269696e-07
Iter: 426 loss: 8.91789512e-07
Iter: 427 loss: 8.90278102e-07
Iter: 428 loss: 8.96776385e-07
Iter: 429 loss: 8.89958301e-07
Iter: 430 loss: 8.88402099e-07
Iter: 431 loss: 8.88121122e-07
Iter: 432 loss: 8.87059059e-07
Iter: 433 loss: 8.8533136e-07
Iter: 434 loss: 8.8770787e-07
Iter: 435 loss: 8.84520489e-07
Iter: 436 loss: 8.83e-07
Iter: 437 loss: 8.91826403e-07
Iter: 438 loss: 8.82825418e-07
Iter: 439 loss: 8.81263702e-07
Iter: 440 loss: 8.93621348e-07
Iter: 441 loss: 8.81150697e-07
Iter: 442 loss: 8.80062657e-07
Iter: 443 loss: 8.80345567e-07
Iter: 444 loss: 8.79307606e-07
Iter: 445 loss: 8.78158346e-07
Iter: 446 loss: 8.76202819e-07
Iter: 447 loss: 8.76200033e-07
Iter: 448 loss: 8.76848276e-07
Iter: 449 loss: 8.75380351e-07
Iter: 450 loss: 8.74533725e-07
Iter: 451 loss: 8.74095406e-07
Iter: 452 loss: 8.73672889e-07
Iter: 453 loss: 8.72518513e-07
Iter: 454 loss: 8.70944291e-07
Iter: 455 loss: 8.70850272e-07
Iter: 456 loss: 8.69944301e-07
Iter: 457 loss: 8.69872281e-07
Iter: 458 loss: 8.68772759e-07
Iter: 459 loss: 8.68363202e-07
Iter: 460 loss: 8.67767369e-07
Iter: 461 loss: 8.66674156e-07
Iter: 462 loss: 8.7013791e-07
Iter: 463 loss: 8.66318373e-07
Iter: 464 loss: 8.64949925e-07
Iter: 465 loss: 8.65885852e-07
Iter: 466 loss: 8.64117055e-07
Iter: 467 loss: 8.6259729e-07
Iter: 468 loss: 8.78257424e-07
Iter: 469 loss: 8.62572904e-07
Iter: 470 loss: 8.61732e-07
Iter: 471 loss: 8.60188322e-07
Iter: 472 loss: 8.95568746e-07
Iter: 473 loss: 8.60173429e-07
Iter: 474 loss: 8.58461647e-07
Iter: 475 loss: 8.72134535e-07
Iter: 476 loss: 8.58337103e-07
Iter: 477 loss: 8.56826773e-07
Iter: 478 loss: 8.6970249e-07
Iter: 479 loss: 8.56758675e-07
Iter: 480 loss: 8.55935411e-07
Iter: 481 loss: 8.54855898e-07
Iter: 482 loss: 8.54788254e-07
Iter: 483 loss: 8.53388e-07
Iter: 484 loss: 8.5905657e-07
Iter: 485 loss: 8.53071469e-07
Iter: 486 loss: 8.52603193e-07
Iter: 487 loss: 8.52401911e-07
Iter: 488 loss: 8.51894697e-07
Iter: 489 loss: 8.50503966e-07
Iter: 490 loss: 8.59166789e-07
Iter: 491 loss: 8.50152446e-07
Iter: 492 loss: 8.48729599e-07
Iter: 493 loss: 8.5312729e-07
Iter: 494 loss: 8.48298e-07
Iter: 495 loss: 8.47107117e-07
Iter: 496 loss: 8.47113711e-07
Iter: 497 loss: 8.46293347e-07
Iter: 498 loss: 8.45683928e-07
Iter: 499 loss: 8.45394197e-07
Iter: 500 loss: 8.4437437e-07
Iter: 501 loss: 8.44571787e-07
Iter: 502 loss: 8.43661383e-07
Iter: 503 loss: 8.4215128e-07
Iter: 504 loss: 8.56788915e-07
Iter: 505 loss: 8.42095176e-07
Iter: 506 loss: 8.41205349e-07
Iter: 507 loss: 8.4246318e-07
Iter: 508 loss: 8.40791756e-07
Iter: 509 loss: 8.39801601e-07
Iter: 510 loss: 8.39143866e-07
Iter: 511 loss: 8.38742153e-07
Iter: 512 loss: 8.3727457e-07
Iter: 513 loss: 8.52333301e-07
Iter: 514 loss: 8.37216419e-07
Iter: 515 loss: 8.36211711e-07
Iter: 516 loss: 8.43435828e-07
Iter: 517 loss: 8.36141567e-07
Iter: 518 loss: 8.35307219e-07
Iter: 519 loss: 8.33936e-07
Iter: 520 loss: 8.33962758e-07
Iter: 521 loss: 8.32939577e-07
Iter: 522 loss: 8.47222111e-07
Iter: 523 loss: 8.32934177e-07
Iter: 524 loss: 8.31843863e-07
Iter: 525 loss: 8.36435106e-07
Iter: 526 loss: 8.31616148e-07
Iter: 527 loss: 8.31091029e-07
Iter: 528 loss: 8.30063925e-07
Iter: 529 loss: 8.52870471e-07
Iter: 530 loss: 8.30053295e-07
Iter: 531 loss: 8.28677344e-07
Iter: 532 loss: 8.30738827e-07
Iter: 533 loss: 8.28002044e-07
Iter: 534 loss: 8.27462372e-07
Iter: 535 loss: 8.27179633e-07
Iter: 536 loss: 8.26752967e-07
Iter: 537 loss: 8.2548388e-07
Iter: 538 loss: 8.30683121e-07
Iter: 539 loss: 8.24998892e-07
Iter: 540 loss: 8.23770051e-07
Iter: 541 loss: 8.43333453e-07
Iter: 542 loss: 8.23780567e-07
Iter: 543 loss: 8.22836171e-07
Iter: 544 loss: 8.27873123e-07
Iter: 545 loss: 8.22681784e-07
Iter: 546 loss: 8.21704646e-07
Iter: 547 loss: 8.20933906e-07
Iter: 548 loss: 8.20618425e-07
Iter: 549 loss: 8.19471097e-07
Iter: 550 loss: 8.22944855e-07
Iter: 551 loss: 8.19163233e-07
Iter: 552 loss: 8.1801079e-07
Iter: 553 loss: 8.32037699e-07
Iter: 554 loss: 8.17977707e-07
Iter: 555 loss: 8.17254545e-07
Iter: 556 loss: 8.1733458e-07
Iter: 557 loss: 8.16696456e-07
Iter: 558 loss: 8.15623423e-07
Iter: 559 loss: 8.16272404e-07
Iter: 560 loss: 8.14923112e-07
Iter: 561 loss: 8.14397822e-07
Iter: 562 loss: 8.14187558e-07
Iter: 563 loss: 8.13683471e-07
Iter: 564 loss: 8.12540293e-07
Iter: 565 loss: 8.29744351e-07
Iter: 566 loss: 8.12461963e-07
Iter: 567 loss: 8.11373752e-07
Iter: 568 loss: 8.12024155e-07
Iter: 569 loss: 8.1064286e-07
Iter: 570 loss: 8.09379912e-07
Iter: 571 loss: 8.12744e-07
Iter: 572 loss: 8.08948812e-07
Iter: 573 loss: 8.08348659e-07
Iter: 574 loss: 8.08196546e-07
Iter: 575 loss: 8.07568824e-07
Iter: 576 loss: 8.06934963e-07
Iter: 577 loss: 8.06838784e-07
Iter: 578 loss: 8.05885861e-07
Iter: 579 loss: 8.05634102e-07
Iter: 580 loss: 8.05053901e-07
Iter: 581 loss: 8.04081424e-07
Iter: 582 loss: 8.04064086e-07
Iter: 583 loss: 8.03297667e-07
Iter: 584 loss: 8.04052888e-07
Iter: 585 loss: 8.02844397e-07
Iter: 586 loss: 8.02020168e-07
Iter: 587 loss: 8.01240105e-07
Iter: 588 loss: 8.01085321e-07
Iter: 589 loss: 8.00187e-07
Iter: 590 loss: 8.00073281e-07
Iter: 591 loss: 7.99449e-07
Iter: 592 loss: 7.9871711e-07
Iter: 593 loss: 7.98600126e-07
Iter: 594 loss: 7.97634357e-07
Iter: 595 loss: 8.04420779e-07
Iter: 596 loss: 7.97546e-07
Iter: 597 loss: 7.96567747e-07
Iter: 598 loss: 8.02610202e-07
Iter: 599 loss: 7.96456561e-07
Iter: 600 loss: 7.95903702e-07
Iter: 601 loss: 7.94923494e-07
Iter: 602 loss: 7.94917e-07
Iter: 603 loss: 7.93652646e-07
Iter: 604 loss: 7.96728045e-07
Iter: 605 loss: 7.93155891e-07
Iter: 606 loss: 7.91978948e-07
Iter: 607 loss: 7.92164542e-07
Iter: 608 loss: 7.91035745e-07
Iter: 609 loss: 7.90932745e-07
Iter: 610 loss: 7.90455886e-07
Iter: 611 loss: 7.89866704e-07
Iter: 612 loss: 7.88559532e-07
Iter: 613 loss: 8.08437903e-07
Iter: 614 loss: 7.88508544e-07
Iter: 615 loss: 7.8736349e-07
Iter: 616 loss: 7.94200503e-07
Iter: 617 loss: 7.87206147e-07
Iter: 618 loss: 7.8644922e-07
Iter: 619 loss: 7.91778689e-07
Iter: 620 loss: 7.86334851e-07
Iter: 621 loss: 7.85515567e-07
Iter: 622 loss: 7.85682403e-07
Iter: 623 loss: 7.8489029e-07
Iter: 624 loss: 7.84045369e-07
Iter: 625 loss: 7.8405418e-07
Iter: 626 loss: 7.83365465e-07
Iter: 627 loss: 7.82253437e-07
Iter: 628 loss: 7.822523e-07
Iter: 629 loss: 7.81765152e-07
Iter: 630 loss: 7.8078051e-07
Iter: 631 loss: 8.00715725e-07
Iter: 632 loss: 7.80773462e-07
Iter: 633 loss: 7.8008128e-07
Iter: 634 loss: 7.80026653e-07
Iter: 635 loss: 7.79352376e-07
Iter: 636 loss: 7.79564e-07
Iter: 637 loss: 7.78904791e-07
Iter: 638 loss: 7.78365916e-07
Iter: 639 loss: 7.77271907e-07
Iter: 640 loss: 7.97065e-07
Iter: 641 loss: 7.77258265e-07
Iter: 642 loss: 7.76186482e-07
Iter: 643 loss: 7.91953312e-07
Iter: 644 loss: 7.76195293e-07
Iter: 645 loss: 7.75403805e-07
Iter: 646 loss: 7.78865058e-07
Iter: 647 loss: 7.75255899e-07
Iter: 648 loss: 7.74272735e-07
Iter: 649 loss: 7.74216232e-07
Iter: 650 loss: 7.73503473e-07
Iter: 651 loss: 7.7259233e-07
Iter: 652 loss: 7.73409454e-07
Iter: 653 loss: 7.72047e-07
Iter: 654 loss: 7.71088366e-07
Iter: 655 loss: 7.7732841e-07
Iter: 656 loss: 7.70964959e-07
Iter: 657 loss: 7.7007445e-07
Iter: 658 loss: 7.74981913e-07
Iter: 659 loss: 7.69936662e-07
Iter: 660 loss: 7.69278813e-07
Iter: 661 loss: 7.68757104e-07
Iter: 662 loss: 7.6855514e-07
Iter: 663 loss: 7.67754045e-07
Iter: 664 loss: 7.79093284e-07
Iter: 665 loss: 7.6774711e-07
Iter: 666 loss: 7.6697313e-07
Iter: 667 loss: 7.67938332e-07
Iter: 668 loss: 7.66584776e-07
Iter: 669 loss: 7.6616459e-07
Iter: 670 loss: 7.71202622e-07
Iter: 671 loss: 7.66160952e-07
Iter: 672 loss: 7.65669142e-07
Iter: 673 loss: 7.64588e-07
Iter: 674 loss: 7.8125646e-07
Iter: 675 loss: 7.64550293e-07
Iter: 676 loss: 7.63460093e-07
Iter: 677 loss: 7.6677577e-07
Iter: 678 loss: 7.63190428e-07
Iter: 679 loss: 7.62342779e-07
Iter: 680 loss: 7.63573951e-07
Iter: 681 loss: 7.61943681e-07
Iter: 682 loss: 7.61051183e-07
Iter: 683 loss: 7.75217302e-07
Iter: 684 loss: 7.61058743e-07
Iter: 685 loss: 7.60582566e-07
Iter: 686 loss: 7.6030426e-07
Iter: 687 loss: 7.60091439e-07
Iter: 688 loss: 7.59315583e-07
Iter: 689 loss: 7.59816089e-07
Iter: 690 loss: 7.58846454e-07
Iter: 691 loss: 7.57917178e-07
Iter: 692 loss: 7.6121745e-07
Iter: 693 loss: 7.57703901e-07
Iter: 694 loss: 7.56829706e-07
Iter: 695 loss: 7.63845435e-07
Iter: 696 loss: 7.56749671e-07
Iter: 697 loss: 7.56068857e-07
Iter: 698 loss: 7.56254508e-07
Iter: 699 loss: 7.55590804e-07
Iter: 700 loss: 7.54887196e-07
Iter: 701 loss: 7.56885242e-07
Iter: 702 loss: 7.54667212e-07
Iter: 703 loss: 7.53678478e-07
Iter: 704 loss: 7.56984775e-07
Iter: 705 loss: 7.53421205e-07
Iter: 706 loss: 7.52957703e-07
Iter: 707 loss: 7.59902264e-07
Iter: 708 loss: 7.52958613e-07
Iter: 709 loss: 7.52540473e-07
Iter: 710 loss: 7.51786956e-07
Iter: 711 loss: 7.51782522e-07
Iter: 712 loss: 7.5088667e-07
Iter: 713 loss: 7.50739e-07
Iter: 714 loss: 7.50134859e-07
Iter: 715 loss: 7.4925606e-07
Iter: 716 loss: 7.52716062e-07
Iter: 717 loss: 7.49067908e-07
Iter: 718 loss: 7.48267e-07
Iter: 719 loss: 7.48279945e-07
Iter: 720 loss: 7.4778211e-07
Iter: 721 loss: 7.46902856e-07
Iter: 722 loss: 7.67893255e-07
Iter: 723 loss: 7.46901094e-07
Iter: 724 loss: 7.4595431e-07
Iter: 725 loss: 7.51266214e-07
Iter: 726 loss: 7.45817317e-07
Iter: 727 loss: 7.45077955e-07
Iter: 728 loss: 7.47281206e-07
Iter: 729 loss: 7.44890258e-07
Iter: 730 loss: 7.44135491e-07
Iter: 731 loss: 7.48586899e-07
Iter: 732 loss: 7.44051e-07
Iter: 733 loss: 7.43425858e-07
Iter: 734 loss: 7.44607689e-07
Iter: 735 loss: 7.43210535e-07
Iter: 736 loss: 7.42653128e-07
Iter: 737 loss: 7.44146746e-07
Iter: 738 loss: 7.42467876e-07
Iter: 739 loss: 7.41872668e-07
Iter: 740 loss: 7.46273884e-07
Iter: 741 loss: 7.41804115e-07
Iter: 742 loss: 7.41425424e-07
Iter: 743 loss: 7.42715315e-07
Iter: 744 loss: 7.41323902e-07
Iter: 745 loss: 7.40825953e-07
Iter: 746 loss: 7.40433563e-07
Iter: 747 loss: 7.40257803e-07
Iter: 748 loss: 7.39647419e-07
Iter: 749 loss: 7.3890476e-07
Iter: 750 loss: 7.38827453e-07
Iter: 751 loss: 7.37666653e-07
Iter: 752 loss: 7.42724865e-07
Iter: 753 loss: 7.37456389e-07
Iter: 754 loss: 7.36782169e-07
Iter: 755 loss: 7.36757499e-07
Iter: 756 loss: 7.36202651e-07
Iter: 757 loss: 7.3574239e-07
Iter: 758 loss: 7.35571234e-07
Iter: 759 loss: 7.34949e-07
Iter: 760 loss: 7.3447444e-07
Iter: 761 loss: 7.34254968e-07
Iter: 762 loss: 7.33469051e-07
Iter: 763 loss: 7.46246656e-07
Iter: 764 loss: 7.3347411e-07
Iter: 765 loss: 7.32871626e-07
Iter: 766 loss: 7.34744901e-07
Iter: 767 loss: 7.32687454e-07
Iter: 768 loss: 7.31940531e-07
Iter: 769 loss: 7.32963827e-07
Iter: 770 loss: 7.3158418e-07
Iter: 771 loss: 7.31002331e-07
Iter: 772 loss: 7.3429203e-07
Iter: 773 loss: 7.30941565e-07
Iter: 774 loss: 7.30286615e-07
Iter: 775 loss: 7.32294779e-07
Iter: 776 loss: 7.3012535e-07
Iter: 777 loss: 7.2948751e-07
Iter: 778 loss: 7.31447813e-07
Iter: 779 loss: 7.29304e-07
Iter: 780 loss: 7.28735131e-07
Iter: 781 loss: 7.30021952e-07
Iter: 782 loss: 7.28545501e-07
Iter: 783 loss: 7.28043744e-07
Iter: 784 loss: 7.27054555e-07
Iter: 785 loss: 7.44983822e-07
Iter: 786 loss: 7.27038355e-07
Iter: 787 loss: 7.26161716e-07
Iter: 788 loss: 7.335085e-07
Iter: 789 loss: 7.261159e-07
Iter: 790 loss: 7.25380175e-07
Iter: 791 loss: 7.2867374e-07
Iter: 792 loss: 7.25262169e-07
Iter: 793 loss: 7.24336246e-07
Iter: 794 loss: 7.26569e-07
Iter: 795 loss: 7.23999506e-07
Iter: 796 loss: 7.23479388e-07
Iter: 797 loss: 7.22792663e-07
Iter: 798 loss: 7.22756909e-07
Iter: 799 loss: 7.21910283e-07
Iter: 800 loss: 7.27325528e-07
Iter: 801 loss: 7.21807226e-07
Iter: 802 loss: 7.21257607e-07
Iter: 803 loss: 7.23864844e-07
Iter: 804 loss: 7.21163701e-07
Iter: 805 loss: 7.20402056e-07
Iter: 806 loss: 7.22139134e-07
Iter: 807 loss: 7.201063e-07
Iter: 808 loss: 7.19547245e-07
Iter: 809 loss: 7.20399328e-07
Iter: 810 loss: 7.19305433e-07
Iter: 811 loss: 7.18500928e-07
Iter: 812 loss: 7.22408117e-07
Iter: 813 loss: 7.18359843e-07
Iter: 814 loss: 7.17758269e-07
Iter: 815 loss: 7.19295826e-07
Iter: 816 loss: 7.17536068e-07
Iter: 817 loss: 7.16956663e-07
Iter: 818 loss: 7.1846e-07
Iter: 819 loss: 7.16765612e-07
Iter: 820 loss: 7.1623424e-07
Iter: 821 loss: 7.1558793e-07
Iter: 822 loss: 7.15519377e-07
Iter: 823 loss: 7.1468304e-07
Iter: 824 loss: 7.19302079e-07
Iter: 825 loss: 7.14520183e-07
Iter: 826 loss: 7.13880922e-07
Iter: 827 loss: 7.15679732e-07
Iter: 828 loss: 7.13677082e-07
Iter: 829 loss: 7.13152701e-07
Iter: 830 loss: 7.13140366e-07
Iter: 831 loss: 7.12731946e-07
Iter: 832 loss: 7.1184968e-07
Iter: 833 loss: 7.26830535e-07
Iter: 834 loss: 7.11813868e-07
Iter: 835 loss: 7.10981169e-07
Iter: 836 loss: 7.12761903e-07
Iter: 837 loss: 7.10657673e-07
Iter: 838 loss: 7.0982e-07
Iter: 839 loss: 7.14968394e-07
Iter: 840 loss: 7.09676669e-07
Iter: 841 loss: 7.08921561e-07
Iter: 842 loss: 7.15065255e-07
Iter: 843 loss: 7.08874381e-07
Iter: 844 loss: 7.08308221e-07
Iter: 845 loss: 7.08941116e-07
Iter: 846 loss: 7.08056803e-07
Iter: 847 loss: 7.07674644e-07
Iter: 848 loss: 7.07638492e-07
Iter: 849 loss: 7.07418167e-07
Iter: 850 loss: 7.06902142e-07
Iter: 851 loss: 7.16909312e-07
Iter: 852 loss: 7.06909077e-07
Iter: 853 loss: 7.06222636e-07
Iter: 854 loss: 7.0892429e-07
Iter: 855 loss: 7.06041192e-07
Iter: 856 loss: 7.05606055e-07
Iter: 857 loss: 7.05182401e-07
Iter: 858 loss: 7.05093498e-07
Iter: 859 loss: 7.04276488e-07
Iter: 860 loss: 7.07192839e-07
Iter: 861 loss: 7.04082595e-07
Iter: 862 loss: 7.03378532e-07
Iter: 863 loss: 7.05329512e-07
Iter: 864 loss: 7.03134333e-07
Iter: 865 loss: 7.02379452e-07
Iter: 866 loss: 7.09492269e-07
Iter: 867 loss: 7.02363366e-07
Iter: 868 loss: 7.01900888e-07
Iter: 869 loss: 7.01768101e-07
Iter: 870 loss: 7.01482918e-07
Iter: 871 loss: 7.00803412e-07
Iter: 872 loss: 6.99981115e-07
Iter: 873 loss: 6.99914949e-07
Iter: 874 loss: 6.99439056e-07
Iter: 875 loss: 6.99358452e-07
Iter: 876 loss: 6.98910071e-07
Iter: 877 loss: 7.00948704e-07
Iter: 878 loss: 6.98799056e-07
Iter: 879 loss: 6.98344877e-07
Iter: 880 loss: 6.98572705e-07
Iter: 881 loss: 6.98056567e-07
Iter: 882 loss: 6.97406335e-07
Iter: 883 loss: 7.0213008e-07
Iter: 884 loss: 6.97367341e-07
Iter: 885 loss: 6.97108874e-07
Iter: 886 loss: 6.96991e-07
Iter: 887 loss: 6.96822667e-07
Iter: 888 loss: 6.9619216e-07
Iter: 889 loss: 6.96249344e-07
Iter: 890 loss: 6.95697963e-07
Iter: 891 loss: 6.9492927e-07
Iter: 892 loss: 6.95120093e-07
Iter: 893 loss: 6.9441694e-07
Iter: 894 loss: 6.9356804e-07
Iter: 895 loss: 6.99338045e-07
Iter: 896 loss: 6.93490904e-07
Iter: 897 loss: 6.93022798e-07
Iter: 898 loss: 6.98804456e-07
Iter: 899 loss: 6.93020525e-07
Iter: 900 loss: 6.92647177e-07
Iter: 901 loss: 6.92985395e-07
Iter: 902 loss: 6.92411334e-07
Iter: 903 loss: 6.91893604e-07
Iter: 904 loss: 6.91535263e-07
Iter: 905 loss: 6.91362857e-07
Iter: 906 loss: 6.90599165e-07
Iter: 907 loss: 6.90988941e-07
Iter: 908 loss: 6.90159254e-07
Iter: 909 loss: 6.89496e-07
Iter: 910 loss: 6.99784209e-07
Iter: 911 loss: 6.89495835e-07
Iter: 912 loss: 6.8889949e-07
Iter: 913 loss: 6.91769628e-07
Iter: 914 loss: 6.88788532e-07
Iter: 915 loss: 6.88304397e-07
Iter: 916 loss: 6.90205809e-07
Iter: 917 loss: 6.88197304e-07
Iter: 918 loss: 6.87631029e-07
Iter: 919 loss: 6.88156717e-07
Iter: 920 loss: 6.87303213e-07
Iter: 921 loss: 6.86853e-07
Iter: 922 loss: 6.87546446e-07
Iter: 923 loss: 6.86629164e-07
Iter: 924 loss: 6.85990926e-07
Iter: 925 loss: 6.87594365e-07
Iter: 926 loss: 6.85772591e-07
Iter: 927 loss: 6.8528152e-07
Iter: 928 loss: 6.84614236e-07
Iter: 929 loss: 6.84578538e-07
Iter: 930 loss: 6.83835935e-07
Iter: 931 loss: 6.90713705e-07
Iter: 932 loss: 6.83802455e-07
Iter: 933 loss: 6.83176e-07
Iter: 934 loss: 6.88519e-07
Iter: 935 loss: 6.83143867e-07
Iter: 936 loss: 6.82694917e-07
Iter: 937 loss: 6.82857547e-07
Iter: 938 loss: 6.82388759e-07
Iter: 939 loss: 6.81812253e-07
Iter: 940 loss: 6.816515e-07
Iter: 941 loss: 6.81308e-07
Iter: 942 loss: 6.80507924e-07
Iter: 943 loss: 6.83578833e-07
Iter: 944 loss: 6.8030721e-07
Iter: 945 loss: 6.79705295e-07
Iter: 946 loss: 6.83205201e-07
Iter: 947 loss: 6.79641403e-07
Iter: 948 loss: 6.79020332e-07
Iter: 949 loss: 6.82998291e-07
Iter: 950 loss: 6.78958713e-07
Iter: 951 loss: 6.78597189e-07
Iter: 952 loss: 6.8060524e-07
Iter: 953 loss: 6.78542278e-07
Iter: 954 loss: 6.78128686e-07
Iter: 955 loss: 6.77226637e-07
Iter: 956 loss: 6.89718149e-07
Iter: 957 loss: 6.77185e-07
Iter: 958 loss: 6.76750687e-07
Iter: 959 loss: 6.76711352e-07
Iter: 960 loss: 6.76337663e-07
Iter: 961 loss: 6.75995352e-07
Iter: 962 loss: 6.75910258e-07
Iter: 963 loss: 6.75292029e-07
Iter: 964 loss: 6.75647129e-07
Iter: 965 loss: 6.74874855e-07
Iter: 966 loss: 6.74299883e-07
Iter: 967 loss: 6.78282731e-07
Iter: 968 loss: 6.74213652e-07
Iter: 969 loss: 6.73542161e-07
Iter: 970 loss: 6.76495404e-07
Iter: 971 loss: 6.73415173e-07
Iter: 972 loss: 6.72948772e-07
Iter: 973 loss: 6.730628e-07
Iter: 974 loss: 6.72573492e-07
Iter: 975 loss: 6.72008468e-07
Iter: 976 loss: 6.72658643e-07
Iter: 977 loss: 6.71743805e-07
Iter: 978 loss: 6.71045655e-07
Iter: 979 loss: 6.73518798e-07
Iter: 980 loss: 6.70856195e-07
Iter: 981 loss: 6.70339375e-07
Iter: 982 loss: 6.73894874e-07
Iter: 983 loss: 6.70304075e-07
Iter: 984 loss: 6.6965913e-07
Iter: 985 loss: 6.71068051e-07
Iter: 986 loss: 6.69446763e-07
Iter: 987 loss: 6.69026747e-07
Iter: 988 loss: 6.73890099e-07
Iter: 989 loss: 6.69042208e-07
Iter: 990 loss: 6.68809264e-07
Iter: 991 loss: 6.68395842e-07
Iter: 992 loss: 6.68393909e-07
Iter: 993 loss: 6.67888e-07
Iter: 994 loss: 6.70340967e-07
Iter: 995 loss: 6.67779091e-07
Iter: 996 loss: 6.67183826e-07
Iter: 997 loss: 6.67183144e-07
Iter: 998 loss: 6.66721e-07
Iter: 999 loss: 6.6610346e-07
Iter: 1000 loss: 6.67098561e-07
Iter: 1001 loss: 6.65797643e-07
Iter: 1002 loss: 6.65284347e-07
Iter: 1003 loss: 6.6912844e-07
Iter: 1004 loss: 6.65248876e-07
Iter: 1005 loss: 6.64647473e-07
Iter: 1006 loss: 6.66090955e-07
Iter: 1007 loss: 6.64450226e-07
Iter: 1008 loss: 6.64039646e-07
Iter: 1009 loss: 6.63579954e-07
Iter: 1010 loss: 6.63507763e-07
Iter: 1011 loss: 6.62776756e-07
Iter: 1012 loss: 6.67116922e-07
Iter: 1013 loss: 6.62672619e-07
Iter: 1014 loss: 6.62108732e-07
Iter: 1015 loss: 6.62587e-07
Iter: 1016 loss: 6.61720605e-07
Iter: 1017 loss: 6.61317245e-07
Iter: 1018 loss: 6.61260515e-07
Iter: 1019 loss: 6.60910189e-07
Iter: 1020 loss: 6.61508807e-07
Iter: 1021 loss: 6.60729938e-07
Iter: 1022 loss: 6.60362502e-07
Iter: 1023 loss: 6.61396484e-07
Iter: 1024 loss: 6.60263936e-07
Iter: 1025 loss: 6.59900138e-07
Iter: 1026 loss: 6.59543389e-07
Iter: 1027 loss: 6.59454145e-07
Iter: 1028 loss: 6.59042541e-07
Iter: 1029 loss: 6.63097353e-07
Iter: 1030 loss: 6.59009629e-07
Iter: 1031 loss: 6.58571707e-07
Iter: 1032 loss: 6.58028398e-07
Iter: 1033 loss: 6.57954672e-07
Iter: 1034 loss: 6.57327575e-07
Iter: 1035 loss: 6.59866373e-07
Iter: 1036 loss: 6.57203486e-07
Iter: 1037 loss: 6.56754196e-07
Iter: 1038 loss: 6.62976561e-07
Iter: 1039 loss: 6.56759084e-07
Iter: 1040 loss: 6.56321617e-07
Iter: 1041 loss: 6.56223165e-07
Iter: 1042 loss: 6.55963e-07
Iter: 1043 loss: 6.55476072e-07
Iter: 1044 loss: 6.54805604e-07
Iter: 1045 loss: 6.54742053e-07
Iter: 1046 loss: 6.54197265e-07
Iter: 1047 loss: 6.54217274e-07
Iter: 1048 loss: 6.53730808e-07
Iter: 1049 loss: 6.54011899e-07
Iter: 1050 loss: 6.53410439e-07
Iter: 1051 loss: 6.52868096e-07
Iter: 1052 loss: 6.52881454e-07
Iter: 1053 loss: 6.52549488e-07
Iter: 1054 loss: 6.52698418e-07
Iter: 1055 loss: 6.52304152e-07
Iter: 1056 loss: 6.51883454e-07
Iter: 1057 loss: 6.5274935e-07
Iter: 1058 loss: 6.51675919e-07
Iter: 1059 loss: 6.51223e-07
Iter: 1060 loss: 6.50936101e-07
Iter: 1061 loss: 6.50733114e-07
Iter: 1062 loss: 6.50326115e-07
Iter: 1063 loss: 6.50334187e-07
Iter: 1064 loss: 6.49978801e-07
Iter: 1065 loss: 6.49612275e-07
Iter: 1066 loss: 6.49566971e-07
Iter: 1067 loss: 6.49011895e-07
Iter: 1068 loss: 6.49885578e-07
Iter: 1069 loss: 6.48766161e-07
Iter: 1070 loss: 6.48239279e-07
Iter: 1071 loss: 6.48233936e-07
Iter: 1072 loss: 6.47817103e-07
Iter: 1073 loss: 6.4725441e-07
Iter: 1074 loss: 6.47204388e-07
Iter: 1075 loss: 6.46637e-07
Iter: 1076 loss: 6.46966782e-07
Iter: 1077 loss: 6.46272156e-07
Iter: 1078 loss: 6.4557338e-07
Iter: 1079 loss: 6.50054744e-07
Iter: 1080 loss: 6.45528473e-07
Iter: 1081 loss: 6.45213106e-07
Iter: 1082 loss: 6.45225043e-07
Iter: 1083 loss: 6.44912802e-07
Iter: 1084 loss: 6.45602199e-07
Iter: 1085 loss: 6.44787576e-07
Iter: 1086 loss: 6.44495117e-07
Iter: 1087 loss: 6.44758302e-07
Iter: 1088 loss: 6.44294232e-07
Iter: 1089 loss: 6.43893259e-07
Iter: 1090 loss: 6.44460499e-07
Iter: 1091 loss: 6.43689532e-07
Iter: 1092 loss: 6.43258772e-07
Iter: 1093 loss: 6.43854321e-07
Iter: 1094 loss: 6.43059252e-07
Iter: 1095 loss: 6.42631448e-07
Iter: 1096 loss: 6.44932527e-07
Iter: 1097 loss: 6.42559144e-07
Iter: 1098 loss: 6.42107523e-07
Iter: 1099 loss: 6.42236046e-07
Iter: 1100 loss: 6.41768167e-07
Iter: 1101 loss: 6.4125453e-07
Iter: 1102 loss: 6.40937e-07
Iter: 1103 loss: 6.4074e-07
Iter: 1104 loss: 6.40402391e-07
Iter: 1105 loss: 6.4026608e-07
Iter: 1106 loss: 6.39997779e-07
Iter: 1107 loss: 6.39456971e-07
Iter: 1108 loss: 6.48684534e-07
Iter: 1109 loss: 6.39429174e-07
Iter: 1110 loss: 6.3882834e-07
Iter: 1111 loss: 6.41241513e-07
Iter: 1112 loss: 6.38694758e-07
Iter: 1113 loss: 6.38183508e-07
Iter: 1114 loss: 6.38217557e-07
Iter: 1115 loss: 6.37760536e-07
Iter: 1116 loss: 6.37576136e-07
Iter: 1117 loss: 6.37412e-07
Iter: 1118 loss: 6.37066e-07
Iter: 1119 loss: 6.37178687e-07
Iter: 1120 loss: 6.36841321e-07
Iter: 1121 loss: 6.36410391e-07
Iter: 1122 loss: 6.36574555e-07
Iter: 1123 loss: 6.36121911e-07
Iter: 1124 loss: 6.35572633e-07
Iter: 1125 loss: 6.37630308e-07
Iter: 1126 loss: 6.35436663e-07
Iter: 1127 loss: 6.34994649e-07
Iter: 1128 loss: 6.34852e-07
Iter: 1129 loss: 6.3459089e-07
Iter: 1130 loss: 6.33929631e-07
Iter: 1131 loss: 6.41697511e-07
Iter: 1132 loss: 6.33930426e-07
Iter: 1133 loss: 6.33570153e-07
Iter: 1134 loss: 6.33792922e-07
Iter: 1135 loss: 6.33326067e-07
Iter: 1136 loss: 6.329592e-07
Iter: 1137 loss: 6.33248931e-07
Iter: 1138 loss: 6.32708634e-07
Iter: 1139 loss: 6.32325566e-07
Iter: 1140 loss: 6.32319825e-07
Iter: 1141 loss: 6.32106776e-07
Iter: 1142 loss: 6.31548346e-07
Iter: 1143 loss: 6.35664946e-07
Iter: 1144 loss: 6.3144671e-07
Iter: 1145 loss: 6.30663408e-07
Iter: 1146 loss: 6.32962326e-07
Iter: 1147 loss: 6.30411591e-07
Iter: 1148 loss: 6.29688429e-07
Iter: 1149 loss: 6.32177944e-07
Iter: 1150 loss: 6.29515284e-07
Iter: 1151 loss: 6.29256e-07
Iter: 1152 loss: 6.29126362e-07
Iter: 1153 loss: 6.28908936e-07
Iter: 1154 loss: 6.28571797e-07
Iter: 1155 loss: 6.28582143e-07
Iter: 1156 loss: 6.28128305e-07
Iter: 1157 loss: 6.30034e-07
Iter: 1158 loss: 6.28059411e-07
Iter: 1159 loss: 6.27607278e-07
Iter: 1160 loss: 6.27464772e-07
Iter: 1161 loss: 6.27181407e-07
Iter: 1162 loss: 6.2662042e-07
Iter: 1163 loss: 6.28084194e-07
Iter: 1164 loss: 6.26408337e-07
Iter: 1165 loss: 6.2582933e-07
Iter: 1166 loss: 6.29668136e-07
Iter: 1167 loss: 6.25771747e-07
Iter: 1168 loss: 6.25298071e-07
Iter: 1169 loss: 6.26424e-07
Iter: 1170 loss: 6.25133225e-07
Iter: 1171 loss: 6.24718837e-07
Iter: 1172 loss: 6.24849577e-07
Iter: 1173 loss: 6.24420863e-07
Iter: 1174 loss: 6.23974756e-07
Iter: 1175 loss: 6.3077573e-07
Iter: 1176 loss: 6.23968731e-07
Iter: 1177 loss: 6.23569463e-07
Iter: 1178 loss: 6.23182302e-07
Iter: 1179 loss: 6.23078222e-07
Iter: 1180 loss: 6.22605683e-07
Iter: 1181 loss: 6.23176959e-07
Iter: 1182 loss: 6.22358073e-07
Iter: 1183 loss: 6.21907e-07
Iter: 1184 loss: 6.24765164e-07
Iter: 1185 loss: 6.21839035e-07
Iter: 1186 loss: 6.21554364e-07
Iter: 1187 loss: 6.21539357e-07
Iter: 1188 loss: 6.21316872e-07
Iter: 1189 loss: 6.20819151e-07
Iter: 1190 loss: 6.24223674e-07
Iter: 1191 loss: 6.20695857e-07
Iter: 1192 loss: 6.20302501e-07
Iter: 1193 loss: 6.20260039e-07
Iter: 1194 loss: 6.19987645e-07
Iter: 1195 loss: 6.19926482e-07
Iter: 1196 loss: 6.19729917e-07
Iter: 1197 loss: 6.19288926e-07
Iter: 1198 loss: 6.19151933e-07
Iter: 1199 loss: 6.18899151e-07
Iter: 1200 loss: 6.18450485e-07
Iter: 1201 loss: 6.18437355e-07
Iter: 1202 loss: 6.1815274e-07
Iter: 1203 loss: 6.17889896e-07
Iter: 1204 loss: 6.17826686e-07
Iter: 1205 loss: 6.17278715e-07
Iter: 1206 loss: 6.19232196e-07
Iter: 1207 loss: 6.1718913e-07
Iter: 1208 loss: 6.16734326e-07
Iter: 1209 loss: 6.20890717e-07
Iter: 1210 loss: 6.16724037e-07
Iter: 1211 loss: 6.16320676e-07
Iter: 1212 loss: 6.1594011e-07
Iter: 1213 loss: 6.15894919e-07
Iter: 1214 loss: 6.15372585e-07
Iter: 1215 loss: 6.15137367e-07
Iter: 1216 loss: 6.14858209e-07
Iter: 1217 loss: 6.14409885e-07
Iter: 1218 loss: 6.14360772e-07
Iter: 1219 loss: 6.139623e-07
Iter: 1220 loss: 6.16229727e-07
Iter: 1221 loss: 6.13879081e-07
Iter: 1222 loss: 6.13666884e-07
Iter: 1223 loss: 6.13166321e-07
Iter: 1224 loss: 6.1939329e-07
Iter: 1225 loss: 6.13113457e-07
Iter: 1226 loss: 6.12778194e-07
Iter: 1227 loss: 6.12724e-07
Iter: 1228 loss: 6.12519443e-07
Iter: 1229 loss: 6.12231474e-07
Iter: 1230 loss: 6.12210329e-07
Iter: 1231 loss: 6.11848236e-07
Iter: 1232 loss: 6.14035571e-07
Iter: 1233 loss: 6.11800715e-07
Iter: 1234 loss: 6.11453515e-07
Iter: 1235 loss: 6.12597887e-07
Iter: 1236 loss: 6.11375526e-07
Iter: 1237 loss: 6.11018436e-07
Iter: 1238 loss: 6.10623829e-07
Iter: 1239 loss: 6.10536233e-07
Iter: 1240 loss: 6.10076427e-07
Iter: 1241 loss: 6.15427e-07
Iter: 1242 loss: 6.10071766e-07
Iter: 1243 loss: 6.09675567e-07
Iter: 1244 loss: 6.11361429e-07
Iter: 1245 loss: 6.09607241e-07
Iter: 1246 loss: 6.09237077e-07
Iter: 1247 loss: 6.0880086e-07
Iter: 1248 loss: 6.08750611e-07
Iter: 1249 loss: 6.08267783e-07
Iter: 1250 loss: 6.10069e-07
Iter: 1251 loss: 6.08139544e-07
Iter: 1252 loss: 6.07937864e-07
Iter: 1253 loss: 6.07900688e-07
Iter: 1254 loss: 6.07653e-07
Iter: 1255 loss: 6.07198274e-07
Iter: 1256 loss: 6.16222223e-07
Iter: 1257 loss: 6.07169227e-07
Iter: 1258 loss: 6.06730168e-07
Iter: 1259 loss: 6.08234927e-07
Iter: 1260 loss: 6.06586468e-07
Iter: 1261 loss: 6.06155709e-07
Iter: 1262 loss: 6.10163511e-07
Iter: 1263 loss: 6.06160825e-07
Iter: 1264 loss: 6.05858759e-07
Iter: 1265 loss: 6.05337732e-07
Iter: 1266 loss: 6.05334662e-07
Iter: 1267 loss: 6.04859e-07
Iter: 1268 loss: 6.10540098e-07
Iter: 1269 loss: 6.04867637e-07
Iter: 1270 loss: 6.04433637e-07
Iter: 1271 loss: 6.04858e-07
Iter: 1272 loss: 6.04213142e-07
Iter: 1273 loss: 6.0374856e-07
Iter: 1274 loss: 6.04593481e-07
Iter: 1275 loss: 6.03536307e-07
Iter: 1276 loss: 6.03130616e-07
Iter: 1277 loss: 6.06499043e-07
Iter: 1278 loss: 6.0310316e-07
Iter: 1279 loss: 6.02762725e-07
Iter: 1280 loss: 6.03675687e-07
Iter: 1281 loss: 6.02631076e-07
Iter: 1282 loss: 6.02310195e-07
Iter: 1283 loss: 6.02134264e-07
Iter: 1284 loss: 6.01979878e-07
Iter: 1285 loss: 6.0156367e-07
Iter: 1286 loss: 6.031064e-07
Iter: 1287 loss: 6.01453962e-07
Iter: 1288 loss: 6.01081126e-07
Iter: 1289 loss: 6.01087095e-07
Iter: 1290 loss: 6.0086893e-07
Iter: 1291 loss: 6.00361545e-07
Iter: 1292 loss: 6.05802256e-07
Iter: 1293 loss: 6.00282874e-07
Iter: 1294 loss: 5.99713e-07
Iter: 1295 loss: 6.02746809e-07
Iter: 1296 loss: 5.99644522e-07
Iter: 1297 loss: 5.99212228e-07
Iter: 1298 loss: 6.04732e-07
Iter: 1299 loss: 5.992311e-07
Iter: 1300 loss: 5.98976726e-07
Iter: 1301 loss: 5.98463316e-07
Iter: 1302 loss: 6.06399567e-07
Iter: 1303 loss: 5.98423753e-07
Iter: 1304 loss: 5.98100087e-07
Iter: 1305 loss: 5.98073484e-07
Iter: 1306 loss: 5.97799044e-07
Iter: 1307 loss: 5.97622261e-07
Iter: 1308 loss: 5.97502321e-07
Iter: 1309 loss: 5.97092594e-07
Iter: 1310 loss: 5.98254246e-07
Iter: 1311 loss: 5.96945711e-07
Iter: 1312 loss: 5.96610789e-07
Iter: 1313 loss: 6.00426347e-07
Iter: 1314 loss: 5.9661636e-07
Iter: 1315 loss: 5.963376e-07
Iter: 1316 loss: 5.96184179e-07
Iter: 1317 loss: 5.96047244e-07
Iter: 1318 loss: 5.9564718e-07
Iter: 1319 loss: 5.96280756e-07
Iter: 1320 loss: 5.9544027e-07
Iter: 1321 loss: 5.95058054e-07
Iter: 1322 loss: 5.98325073e-07
Iter: 1323 loss: 5.95052938e-07
Iter: 1324 loss: 5.94661174e-07
Iter: 1325 loss: 5.95810093e-07
Iter: 1326 loss: 5.94556e-07
Iter: 1327 loss: 5.94354e-07
Iter: 1328 loss: 5.93938239e-07
Iter: 1329 loss: 6.01127169e-07
Iter: 1330 loss: 5.93930736e-07
Iter: 1331 loss: 5.93345874e-07
Iter: 1332 loss: 5.96804114e-07
Iter: 1333 loss: 5.9329119e-07
Iter: 1334 loss: 5.92810579e-07
Iter: 1335 loss: 5.96411724e-07
Iter: 1336 loss: 5.92766071e-07
Iter: 1337 loss: 5.92531933e-07
Iter: 1338 loss: 5.92029323e-07
Iter: 1339 loss: 6.02746638e-07
Iter: 1340 loss: 5.92025344e-07
Iter: 1341 loss: 5.91625e-07
Iter: 1342 loss: 5.91604476e-07
Iter: 1343 loss: 5.91260687e-07
Iter: 1344 loss: 5.91020694e-07
Iter: 1345 loss: 5.90910076e-07
Iter: 1346 loss: 5.90505579e-07
Iter: 1347 loss: 5.9386997e-07
Iter: 1348 loss: 5.90494e-07
Iter: 1349 loss: 5.90177592e-07
Iter: 1350 loss: 5.9120913e-07
Iter: 1351 loss: 5.90062314e-07
Iter: 1352 loss: 5.89708463e-07
Iter: 1353 loss: 5.89792194e-07
Iter: 1354 loss: 5.89424189e-07
Iter: 1355 loss: 5.89099841e-07
Iter: 1356 loss: 5.89823287e-07
Iter: 1357 loss: 5.889666e-07
Iter: 1358 loss: 5.88719331e-07
Iter: 1359 loss: 5.88729222e-07
Iter: 1360 loss: 5.88488774e-07
Iter: 1361 loss: 5.88101614e-07
Iter: 1362 loss: 5.88104967e-07
Iter: 1363 loss: 5.8773287e-07
Iter: 1364 loss: 5.87997192e-07
Iter: 1365 loss: 5.87490604e-07
Iter: 1366 loss: 5.87189334e-07
Iter: 1367 loss: 5.8719047e-07
Iter: 1368 loss: 5.86893e-07
Iter: 1369 loss: 5.8660936e-07
Iter: 1370 loss: 5.86562919e-07
Iter: 1371 loss: 5.86080091e-07
Iter: 1372 loss: 5.86815759e-07
Iter: 1373 loss: 5.85859e-07
Iter: 1374 loss: 5.85453677e-07
Iter: 1375 loss: 5.90081868e-07
Iter: 1376 loss: 5.85444582e-07
Iter: 1377 loss: 5.85126827e-07
Iter: 1378 loss: 5.8525211e-07
Iter: 1379 loss: 5.8489826e-07
Iter: 1380 loss: 5.84615464e-07
Iter: 1381 loss: 5.86271426e-07
Iter: 1382 loss: 5.84570728e-07
Iter: 1383 loss: 5.84202894e-07
Iter: 1384 loss: 5.84453801e-07
Iter: 1385 loss: 5.83984558e-07
Iter: 1386 loss: 5.83609449e-07
Iter: 1387 loss: 5.84418e-07
Iter: 1388 loss: 5.83502072e-07
Iter: 1389 loss: 5.83102519e-07
Iter: 1390 loss: 5.82792381e-07
Iter: 1391 loss: 5.82683867e-07
Iter: 1392 loss: 5.82777602e-07
Iter: 1393 loss: 5.8241551e-07
Iter: 1394 loss: 5.82253506e-07
Iter: 1395 loss: 5.81894824e-07
Iter: 1396 loss: 5.87813759e-07
Iter: 1397 loss: 5.81846791e-07
Iter: 1398 loss: 5.814112e-07
Iter: 1399 loss: 5.81478844e-07
Iter: 1400 loss: 5.81108168e-07
Iter: 1401 loss: 5.80703954e-07
Iter: 1402 loss: 5.84189081e-07
Iter: 1403 loss: 5.80668484e-07
Iter: 1404 loss: 5.8020521e-07
Iter: 1405 loss: 5.80997778e-07
Iter: 1406 loss: 5.79981247e-07
Iter: 1407 loss: 5.7971755e-07
Iter: 1408 loss: 5.7977428e-07
Iter: 1409 loss: 5.79531843e-07
Iter: 1410 loss: 5.79180437e-07
Iter: 1411 loss: 5.82317796e-07
Iter: 1412 loss: 5.79152e-07
Iter: 1413 loss: 5.78829599e-07
Iter: 1414 loss: 5.79151845e-07
Iter: 1415 loss: 5.786315e-07
Iter: 1416 loss: 5.78349272e-07
Iter: 1417 loss: 5.79268033e-07
Iter: 1418 loss: 5.78273671e-07
Iter: 1419 loss: 5.77866444e-07
Iter: 1420 loss: 5.78788445e-07
Iter: 1421 loss: 5.77717856e-07
Iter: 1422 loss: 5.77426874e-07
Iter: 1423 loss: 5.77113838e-07
Iter: 1424 loss: 5.77028516e-07
Iter: 1425 loss: 5.7652619e-07
Iter: 1426 loss: 5.79864263e-07
Iter: 1427 loss: 5.76501634e-07
Iter: 1428 loss: 5.76161483e-07
Iter: 1429 loss: 5.76167508e-07
Iter: 1430 loss: 5.75945592e-07
Iter: 1431 loss: 5.75449519e-07
Iter: 1432 loss: 5.82583368e-07
Iter: 1433 loss: 5.75439e-07
Iter: 1434 loss: 5.74868523e-07
Iter: 1435 loss: 5.76048706e-07
Iter: 1436 loss: 5.74611818e-07
Iter: 1437 loss: 5.74171395e-07
Iter: 1438 loss: 5.79095854e-07
Iter: 1439 loss: 5.74143826e-07
Iter: 1440 loss: 5.73749332e-07
Iter: 1441 loss: 5.75234651e-07
Iter: 1442 loss: 5.73648663e-07
Iter: 1443 loss: 5.73391958e-07
Iter: 1444 loss: 5.73122634e-07
Iter: 1445 loss: 5.73059708e-07
Iter: 1446 loss: 5.7262946e-07
Iter: 1447 loss: 5.75008585e-07
Iter: 1448 loss: 5.72556132e-07
Iter: 1449 loss: 5.72129295e-07
Iter: 1450 loss: 5.74622788e-07
Iter: 1451 loss: 5.72084e-07
Iter: 1452 loss: 5.71822397e-07
Iter: 1453 loss: 5.71846215e-07
Iter: 1454 loss: 5.71609576e-07
Iter: 1455 loss: 5.71185865e-07
Iter: 1456 loss: 5.73978753e-07
Iter: 1457 loss: 5.71151475e-07
Iter: 1458 loss: 5.70877887e-07
Iter: 1459 loss: 5.70358168e-07
Iter: 1460 loss: 5.82415055e-07
Iter: 1461 loss: 5.70371469e-07
Iter: 1462 loss: 5.70113968e-07
Iter: 1463 loss: 5.70079123e-07
Iter: 1464 loss: 5.69762562e-07
Iter: 1465 loss: 5.69778194e-07
Iter: 1466 loss: 5.69474537e-07
Iter: 1467 loss: 5.69195493e-07
Iter: 1468 loss: 5.69775466e-07
Iter: 1469 loss: 5.69083e-07
Iter: 1470 loss: 5.68799805e-07
Iter: 1471 loss: 5.68634334e-07
Iter: 1472 loss: 5.6851917e-07
Iter: 1473 loss: 5.68135533e-07
Iter: 1474 loss: 5.68156338e-07
Iter: 1475 loss: 5.67870416e-07
Iter: 1476 loss: 5.67707559e-07
Iter: 1477 loss: 5.67585914e-07
Iter: 1478 loss: 5.67156917e-07
Iter: 1479 loss: 5.67209781e-07
Iter: 1480 loss: 5.66837912e-07
Iter: 1481 loss: 5.66479457e-07
Iter: 1482 loss: 5.71177736e-07
Iter: 1483 loss: 5.66484232e-07
Iter: 1484 loss: 5.66103949e-07
Iter: 1485 loss: 5.67055622e-07
Iter: 1486 loss: 5.65970424e-07
Iter: 1487 loss: 5.65749815e-07
Iter: 1488 loss: 5.66857352e-07
Iter: 1489 loss: 5.65706387e-07
Iter: 1490 loss: 5.65451046e-07
Iter: 1491 loss: 5.65600544e-07
Iter: 1492 loss: 5.65276423e-07
Iter: 1493 loss: 5.64938091e-07
Iter: 1494 loss: 5.64553034e-07
Iter: 1495 loss: 5.64499942e-07
Iter: 1496 loss: 5.64481752e-07
Iter: 1497 loss: 5.64303491e-07
Iter: 1498 loss: 5.6408868e-07
Iter: 1499 loss: 5.63602157e-07
Iter: 1500 loss: 5.71240548e-07
Iter: 1501 loss: 5.635859e-07
Iter: 1502 loss: 5.63125809e-07
Iter: 1503 loss: 5.63913375e-07
Iter: 1504 loss: 5.62936634e-07
Iter: 1505 loss: 5.62487116e-07
Iter: 1506 loss: 5.65409721e-07
Iter: 1507 loss: 5.62448122e-07
Iter: 1508 loss: 5.62188291e-07
Iter: 1509 loss: 5.64394e-07
Iter: 1510 loss: 5.62153218e-07
Iter: 1511 loss: 5.61840864e-07
Iter: 1512 loss: 5.61736272e-07
Iter: 1513 loss: 5.61570971e-07
Iter: 1514 loss: 5.6119535e-07
Iter: 1515 loss: 5.61589104e-07
Iter: 1516 loss: 5.60977128e-07
Iter: 1517 loss: 5.60573881e-07
Iter: 1518 loss: 5.61557044e-07
Iter: 1519 loss: 5.60417561e-07
Iter: 1520 loss: 5.60037108e-07
Iter: 1521 loss: 5.65625271e-07
Iter: 1522 loss: 5.6003455e-07
Iter: 1523 loss: 5.59777391e-07
Iter: 1524 loss: 5.59649493e-07
Iter: 1525 loss: 5.5951341e-07
Iter: 1526 loss: 5.59131081e-07
Iter: 1527 loss: 5.62546234e-07
Iter: 1528 loss: 5.59117154e-07
Iter: 1529 loss: 5.58816964e-07
Iter: 1530 loss: 5.58310262e-07
Iter: 1531 loss: 5.58306283e-07
Iter: 1532 loss: 5.5800848e-07
Iter: 1533 loss: 5.58001489e-07
Iter: 1534 loss: 5.57703174e-07
Iter: 1535 loss: 5.59060652e-07
Iter: 1536 loss: 5.5763843e-07
Iter: 1537 loss: 5.57487965e-07
Iter: 1538 loss: 5.57101373e-07
Iter: 1539 loss: 5.59675698e-07
Iter: 1540 loss: 5.56965631e-07
Iter: 1541 loss: 5.56463306e-07
Iter: 1542 loss: 5.58012289e-07
Iter: 1543 loss: 5.56317104e-07
Iter: 1544 loss: 5.56105533e-07
Iter: 1545 loss: 5.560251e-07
Iter: 1546 loss: 5.55781639e-07
Iter: 1547 loss: 5.55712404e-07
Iter: 1548 loss: 5.5557939e-07
Iter: 1549 loss: 5.55203769e-07
Iter: 1550 loss: 5.55309498e-07
Iter: 1551 loss: 5.54906705e-07
Iter: 1552 loss: 5.54525286e-07
Iter: 1553 loss: 5.56854729e-07
Iter: 1554 loss: 5.54478e-07
Iter: 1555 loss: 5.541267e-07
Iter: 1556 loss: 5.55560291e-07
Iter: 1557 loss: 5.54039161e-07
Iter: 1558 loss: 5.53715722e-07
Iter: 1559 loss: 5.55173926e-07
Iter: 1560 loss: 5.53639e-07
Iter: 1561 loss: 5.53446853e-07
Iter: 1562 loss: 5.54107032e-07
Iter: 1563 loss: 5.53393875e-07
Iter: 1564 loss: 5.53128132e-07
Iter: 1565 loss: 5.52983352e-07
Iter: 1566 loss: 5.52873e-07
Iter: 1567 loss: 5.52697486e-07
Iter: 1568 loss: 5.52681627e-07
Iter: 1569 loss: 5.52477616e-07
Iter: 1570 loss: 5.52106485e-07
Iter: 1571 loss: 5.52117228e-07
Iter: 1572 loss: 5.51743938e-07
Iter: 1573 loss: 5.51710968e-07
Iter: 1574 loss: 5.51451478e-07
Iter: 1575 loss: 5.50894356e-07
Iter: 1576 loss: 5.52028439e-07
Iter: 1577 loss: 5.50719733e-07
Iter: 1578 loss: 5.5028454e-07
Iter: 1579 loss: 5.53048e-07
Iter: 1580 loss: 5.5027192e-07
Iter: 1581 loss: 5.49954166e-07
Iter: 1582 loss: 5.49941319e-07
Iter: 1583 loss: 5.49777042e-07
Iter: 1584 loss: 5.49423874e-07
Iter: 1585 loss: 5.55991051e-07
Iter: 1586 loss: 5.49412391e-07
Iter: 1587 loss: 5.4899e-07
Iter: 1588 loss: 5.50139134e-07
Iter: 1589 loss: 5.48856235e-07
Iter: 1590 loss: 5.48547177e-07
Iter: 1591 loss: 5.48543653e-07
Iter: 1592 loss: 5.48251705e-07
Iter: 1593 loss: 5.48497212e-07
Iter: 1594 loss: 5.4807623e-07
Iter: 1595 loss: 5.47818331e-07
Iter: 1596 loss: 5.4875818e-07
Iter: 1597 loss: 5.47742161e-07
Iter: 1598 loss: 5.47437e-07
Iter: 1599 loss: 5.4832924e-07
Iter: 1600 loss: 5.47329705e-07
Iter: 1601 loss: 5.47104207e-07
Iter: 1602 loss: 5.48894377e-07
Iter: 1603 loss: 5.47080901e-07
Iter: 1604 loss: 5.46845513e-07
Iter: 1605 loss: 5.47035199e-07
Iter: 1606 loss: 5.46726255e-07
Iter: 1607 loss: 5.46485239e-07
Iter: 1608 loss: 5.46244451e-07
Iter: 1609 loss: 5.46195906e-07
Iter: 1610 loss: 5.4577481e-07
Iter: 1611 loss: 5.46460342e-07
Iter: 1612 loss: 5.45600244e-07
Iter: 1613 loss: 5.45135379e-07
Iter: 1614 loss: 5.45299258e-07
Iter: 1615 loss: 5.44802845e-07
Iter: 1616 loss: 5.44213265e-07
Iter: 1617 loss: 5.458744e-07
Iter: 1618 loss: 5.44031309e-07
Iter: 1619 loss: 5.43650117e-07
Iter: 1620 loss: 5.43646e-07
Iter: 1621 loss: 5.43418196e-07
Iter: 1622 loss: 5.45393618e-07
Iter: 1623 loss: 5.43387841e-07
Iter: 1624 loss: 5.43166266e-07
Iter: 1625 loss: 5.42706402e-07
Iter: 1626 loss: 5.50681193e-07
Iter: 1627 loss: 5.42674343e-07
Iter: 1628 loss: 5.42300029e-07
Iter: 1629 loss: 5.44011527e-07
Iter: 1630 loss: 5.42230282e-07
Iter: 1631 loss: 5.41955615e-07
Iter: 1632 loss: 5.4195317e-07
Iter: 1633 loss: 5.41699876e-07
Iter: 1634 loss: 5.41506324e-07
Iter: 1635 loss: 5.41429074e-07
Iter: 1636 loss: 5.41183738e-07
Iter: 1637 loss: 5.41185784e-07
Iter: 1638 loss: 5.40960855e-07
Iter: 1639 loss: 5.40702217e-07
Iter: 1640 loss: 5.40684709e-07
Iter: 1641 loss: 5.40399355e-07
Iter: 1642 loss: 5.41884845e-07
Iter: 1643 loss: 5.40359e-07
Iter: 1644 loss: 5.40070914e-07
Iter: 1645 loss: 5.40166127e-07
Iter: 1646 loss: 5.39883445e-07
Iter: 1647 loss: 5.39598886e-07
Iter: 1648 loss: 5.40761675e-07
Iter: 1649 loss: 5.39527264e-07
Iter: 1650 loss: 5.3922372e-07
Iter: 1651 loss: 5.38999871e-07
Iter: 1652 loss: 5.38915174e-07
Iter: 1653 loss: 5.38530571e-07
Iter: 1654 loss: 5.41319537e-07
Iter: 1655 loss: 5.38511927e-07
Iter: 1656 loss: 5.38164045e-07
Iter: 1657 loss: 5.40689371e-07
Iter: 1658 loss: 5.38146537e-07
Iter: 1659 loss: 5.37933715e-07
Iter: 1660 loss: 5.37744256e-07
Iter: 1661 loss: 5.37669052e-07
Iter: 1662 loss: 5.37327196e-07
Iter: 1663 loss: 5.37517622e-07
Iter: 1664 loss: 5.37124095e-07
Iter: 1665 loss: 5.36852212e-07
Iter: 1666 loss: 5.36847892e-07
Iter: 1667 loss: 5.3656936e-07
Iter: 1668 loss: 5.36648031e-07
Iter: 1669 loss: 5.36377797e-07
Iter: 1670 loss: 5.36199252e-07
Iter: 1671 loss: 5.36190612e-07
Iter: 1672 loss: 5.36017296e-07
Iter: 1673 loss: 5.3567544e-07
Iter: 1674 loss: 5.42974362e-07
Iter: 1675 loss: 5.35644119e-07
Iter: 1676 loss: 5.35366212e-07
Iter: 1677 loss: 5.37419623e-07
Iter: 1678 loss: 5.35338415e-07
Iter: 1679 loss: 5.35083132e-07
Iter: 1680 loss: 5.35835397e-07
Iter: 1681 loss: 5.35004119e-07
Iter: 1682 loss: 5.34659762e-07
Iter: 1683 loss: 5.34315461e-07
Iter: 1684 loss: 5.34248898e-07
Iter: 1685 loss: 5.33884531e-07
Iter: 1686 loss: 5.3593817e-07
Iter: 1687 loss: 5.33833941e-07
Iter: 1688 loss: 5.33466789e-07
Iter: 1689 loss: 5.35669471e-07
Iter: 1690 loss: 5.33424782e-07
Iter: 1691 loss: 5.33174898e-07
Iter: 1692 loss: 5.35199945e-07
Iter: 1693 loss: 5.33150342e-07
Iter: 1694 loss: 5.32953322e-07
Iter: 1695 loss: 5.32644208e-07
Iter: 1696 loss: 5.3263193e-07
Iter: 1697 loss: 5.32301954e-07
Iter: 1698 loss: 5.33601337e-07
Iter: 1699 loss: 5.3222584e-07
Iter: 1700 loss: 5.31919341e-07
Iter: 1701 loss: 5.33476282e-07
Iter: 1702 loss: 5.31868864e-07
Iter: 1703 loss: 5.31479373e-07
Iter: 1704 loss: 5.32323838e-07
Iter: 1705 loss: 5.31340788e-07
Iter: 1706 loss: 5.31151954e-07
Iter: 1707 loss: 5.34001344e-07
Iter: 1708 loss: 5.31151e-07
Iter: 1709 loss: 5.30991485e-07
Iter: 1710 loss: 5.30619502e-07
Iter: 1711 loss: 5.35526453e-07
Iter: 1712 loss: 5.30601369e-07
Iter: 1713 loss: 5.30228e-07
Iter: 1714 loss: 5.31127284e-07
Iter: 1715 loss: 5.30087505e-07
Iter: 1716 loss: 5.29745137e-07
Iter: 1717 loss: 5.33658294e-07
Iter: 1718 loss: 5.29732347e-07
Iter: 1719 loss: 5.29458475e-07
Iter: 1720 loss: 5.2960462e-07
Iter: 1721 loss: 5.29274303e-07
Iter: 1722 loss: 5.28972294e-07
Iter: 1723 loss: 5.28585929e-07
Iter: 1724 loss: 5.28546593e-07
Iter: 1725 loss: 5.28337182e-07
Iter: 1726 loss: 5.28304838e-07
Iter: 1727 loss: 5.28023634e-07
Iter: 1728 loss: 5.28186661e-07
Iter: 1729 loss: 5.27837358e-07
Iter: 1730 loss: 5.27471968e-07
Iter: 1731 loss: 5.27378688e-07
Iter: 1732 loss: 5.27160694e-07
Iter: 1733 loss: 5.26773e-07
Iter: 1734 loss: 5.2776727e-07
Iter: 1735 loss: 5.2662125e-07
Iter: 1736 loss: 5.26388249e-07
Iter: 1737 loss: 5.26350163e-07
Iter: 1738 loss: 5.26184408e-07
Iter: 1739 loss: 5.26307417e-07
Iter: 1740 loss: 5.26077883e-07
Iter: 1741 loss: 5.25812538e-07
Iter: 1742 loss: 5.2662773e-07
Iter: 1743 loss: 5.25735e-07
Iter: 1744 loss: 5.25481596e-07
Iter: 1745 loss: 5.25479436e-07
Iter: 1746 loss: 5.2526957e-07
Iter: 1747 loss: 5.24988195e-07
Iter: 1748 loss: 5.25022074e-07
Iter: 1749 loss: 5.24779807e-07
Iter: 1750 loss: 5.24508096e-07
Iter: 1751 loss: 5.27625673e-07
Iter: 1752 loss: 5.24493146e-07
Iter: 1753 loss: 5.24225868e-07
Iter: 1754 loss: 5.24614052e-07
Iter: 1755 loss: 5.24086e-07
Iter: 1756 loss: 5.2381472e-07
Iter: 1757 loss: 5.23674316e-07
Iter: 1758 loss: 5.23552615e-07
Iter: 1759 loss: 5.23230085e-07
Iter: 1760 loss: 5.25724886e-07
Iter: 1761 loss: 5.23168467e-07
Iter: 1762 loss: 5.22886353e-07
Iter: 1763 loss: 5.25650535e-07
Iter: 1764 loss: 5.22875553e-07
Iter: 1765 loss: 5.22699224e-07
Iter: 1766 loss: 5.22412961e-07
Iter: 1767 loss: 5.22399091e-07
Iter: 1768 loss: 5.22043251e-07
Iter: 1769 loss: 5.22796199e-07
Iter: 1770 loss: 5.21920697e-07
Iter: 1771 loss: 5.21729362e-07
Iter: 1772 loss: 5.21689458e-07
Iter: 1773 loss: 5.21559173e-07
Iter: 1774 loss: 5.2137716e-07
Iter: 1775 loss: 5.21359425e-07
Iter: 1776 loss: 5.21020866e-07
Iter: 1777 loss: 5.22413416e-07
Iter: 1778 loss: 5.20955155e-07
Iter: 1779 loss: 5.20722892e-07
Iter: 1780 loss: 5.2035449e-07
Iter: 1781 loss: 5.29818294e-07
Iter: 1782 loss: 5.20342269e-07
Iter: 1783 loss: 5.20001777e-07
Iter: 1784 loss: 5.24035613e-07
Iter: 1785 loss: 5.19994e-07
Iter: 1786 loss: 5.19753542e-07
Iter: 1787 loss: 5.20445042e-07
Iter: 1788 loss: 5.19664297e-07
Iter: 1789 loss: 5.19393552e-07
Iter: 1790 loss: 5.20202491e-07
Iter: 1791 loss: 5.19332048e-07
Iter: 1792 loss: 5.19072671e-07
Iter: 1793 loss: 5.18950799e-07
Iter: 1794 loss: 5.1880113e-07
Iter: 1795 loss: 5.18451543e-07
Iter: 1796 loss: 5.19784351e-07
Iter: 1797 loss: 5.18369632e-07
Iter: 1798 loss: 5.18125205e-07
Iter: 1799 loss: 5.18125148e-07
Iter: 1800 loss: 5.17947228e-07
Iter: 1801 loss: 5.17499416e-07
Iter: 1802 loss: 5.23594167e-07
Iter: 1803 loss: 5.17453657e-07
Iter: 1804 loss: 5.17104752e-07
Iter: 1805 loss: 5.20553442e-07
Iter: 1806 loss: 5.17096964e-07
Iter: 1807 loss: 5.16892669e-07
Iter: 1808 loss: 5.16867487e-07
Iter: 1809 loss: 5.16707701e-07
Iter: 1810 loss: 5.16507953e-07
Iter: 1811 loss: 5.16491355e-07
Iter: 1812 loss: 5.16278362e-07
Iter: 1813 loss: 5.19214268e-07
Iter: 1814 loss: 5.16260627e-07
Iter: 1815 loss: 5.16109708e-07
Iter: 1816 loss: 5.15717147e-07
Iter: 1817 loss: 5.19587331e-07
Iter: 1818 loss: 5.15659792e-07
Iter: 1819 loss: 5.15221359e-07
Iter: 1820 loss: 5.15563158e-07
Iter: 1821 loss: 5.1497e-07
Iter: 1822 loss: 5.14713804e-07
Iter: 1823 loss: 5.14662815e-07
Iter: 1824 loss: 5.14396561e-07
Iter: 1825 loss: 5.14454314e-07
Iter: 1826 loss: 5.14188514e-07
Iter: 1827 loss: 5.13877239e-07
Iter: 1828 loss: 5.14199769e-07
Iter: 1829 loss: 5.13714781e-07
Iter: 1830 loss: 5.13388102e-07
Iter: 1831 loss: 5.15243073e-07
Iter: 1832 loss: 5.1333825e-07
Iter: 1833 loss: 5.13019e-07
Iter: 1834 loss: 5.14816293e-07
Iter: 1835 loss: 5.12964618e-07
Iter: 1836 loss: 5.12763336e-07
Iter: 1837 loss: 5.12892655e-07
Iter: 1838 loss: 5.1263396e-07
Iter: 1839 loss: 5.12353381e-07
Iter: 1840 loss: 5.12329734e-07
Iter: 1841 loss: 5.12111797e-07
Iter: 1842 loss: 5.12005101e-07
Iter: 1843 loss: 5.11906251e-07
Iter: 1844 loss: 5.11773e-07
Iter: 1845 loss: 5.1149118e-07
Iter: 1846 loss: 5.17353726e-07
Iter: 1847 loss: 5.11493681e-07
Iter: 1848 loss: 5.11225721e-07
Iter: 1849 loss: 5.14122064e-07
Iter: 1850 loss: 5.11215717e-07
Iter: 1851 loss: 5.11001815e-07
Iter: 1852 loss: 5.10994e-07
Iter: 1853 loss: 5.10804625e-07
Iter: 1854 loss: 5.10568839e-07
Iter: 1855 loss: 5.1027763e-07
Iter: 1856 loss: 5.10249833e-07
Iter: 1857 loss: 5.09853692e-07
Iter: 1858 loss: 5.11447809e-07
Iter: 1859 loss: 5.09768142e-07
Iter: 1860 loss: 5.09391e-07
Iter: 1861 loss: 5.11027338e-07
Iter: 1862 loss: 5.09317715e-07
Iter: 1863 loss: 5.09056292e-07
Iter: 1864 loss: 5.12682e-07
Iter: 1865 loss: 5.0904714e-07
Iter: 1866 loss: 5.08811e-07
Iter: 1867 loss: 5.08598305e-07
Iter: 1868 loss: 5.08533788e-07
Iter: 1869 loss: 5.08196877e-07
Iter: 1870 loss: 5.09189135e-07
Iter: 1871 loss: 5.08069e-07
Iter: 1872 loss: 5.07745483e-07
Iter: 1873 loss: 5.11767439e-07
Iter: 1874 loss: 5.07723712e-07
Iter: 1875 loss: 5.07505206e-07
Iter: 1876 loss: 5.07273e-07
Iter: 1877 loss: 5.07234745e-07
Iter: 1878 loss: 5.06951721e-07
Iter: 1879 loss: 5.06946378e-07
Iter: 1880 loss: 5.066629e-07
Iter: 1881 loss: 5.07114123e-07
Iter: 1882 loss: 5.06576896e-07
Iter: 1883 loss: 5.06351682e-07
Iter: 1884 loss: 5.06368906e-07
Iter: 1885 loss: 5.06198774e-07
Iter: 1886 loss: 5.05972878e-07
Iter: 1887 loss: 5.07136122e-07
Iter: 1888 loss: 5.05932917e-07
Iter: 1889 loss: 5.05661035e-07
Iter: 1890 loss: 5.05993455e-07
Iter: 1891 loss: 5.05544449e-07
Iter: 1892 loss: 5.05312187e-07
Iter: 1893 loss: 5.0499284e-07
Iter: 1894 loss: 5.04986588e-07
Iter: 1895 loss: 5.04550485e-07
Iter: 1896 loss: 5.05107153e-07
Iter: 1897 loss: 5.04313164e-07
Iter: 1898 loss: 5.0393794e-07
Iter: 1899 loss: 5.09197946e-07
Iter: 1900 loss: 5.03909632e-07
Iter: 1901 loss: 5.03630417e-07
Iter: 1902 loss: 5.04040145e-07
Iter: 1903 loss: 5.03505248e-07
Iter: 1904 loss: 5.03197327e-07
Iter: 1905 loss: 5.06782214e-07
Iter: 1906 loss: 5.0320125e-07
Iter: 1907 loss: 5.02970806e-07
Iter: 1908 loss: 5.02549e-07
Iter: 1909 loss: 5.11893575e-07
Iter: 1910 loss: 5.02554826e-07
Iter: 1911 loss: 5.02501848e-07
Iter: 1912 loss: 5.02352918e-07
Iter: 1913 loss: 5.02218e-07
Iter: 1914 loss: 5.0219154e-07
Iter: 1915 loss: 5.02088e-07
Iter: 1916 loss: 5.01867078e-07
Iter: 1917 loss: 5.02946136e-07
Iter: 1918 loss: 5.01835757e-07
Iter: 1919 loss: 5.016459e-07
Iter: 1920 loss: 5.01510954e-07
Iter: 1921 loss: 5.01431373e-07
Iter: 1922 loss: 5.01157558e-07
Iter: 1923 loss: 5.01232876e-07
Iter: 1924 loss: 5.00979695e-07
Iter: 1925 loss: 5.00580086e-07
Iter: 1926 loss: 5.02008788e-07
Iter: 1927 loss: 5.00478563e-07
Iter: 1928 loss: 5.00337364e-07
Iter: 1929 loss: 5.00309e-07
Iter: 1930 loss: 5.00178e-07
Iter: 1931 loss: 4.99844873e-07
Iter: 1932 loss: 5.03605861e-07
Iter: 1933 loss: 4.99831458e-07
Iter: 1934 loss: 4.99468911e-07
Iter: 1935 loss: 4.99968451e-07
Iter: 1936 loss: 4.99290195e-07
Iter: 1937 loss: 4.99023031e-07
Iter: 1938 loss: 4.99028602e-07
Iter: 1939 loss: 4.98785084e-07
Iter: 1940 loss: 4.99575322e-07
Iter: 1941 loss: 4.98735687e-07
Iter: 1942 loss: 4.98491488e-07
Iter: 1943 loss: 4.98629106e-07
Iter: 1944 loss: 4.98355121e-07
Iter: 1945 loss: 4.98216536e-07
Iter: 1946 loss: 4.98222334e-07
Iter: 1947 loss: 4.98061127e-07
Iter: 1948 loss: 4.9786604e-07
Iter: 1949 loss: 4.97864562e-07
Iter: 1950 loss: 4.97630083e-07
Iter: 1951 loss: 5.00328213e-07
Iter: 1952 loss: 4.97618e-07
Iter: 1953 loss: 4.97481153e-07
Iter: 1954 loss: 4.9712105e-07
Iter: 1955 loss: 4.99620796e-07
Iter: 1956 loss: 4.97053065e-07
Iter: 1957 loss: 4.96673124e-07
Iter: 1958 loss: 5.00051101e-07
Iter: 1959 loss: 4.96645271e-07
Iter: 1960 loss: 4.96387258e-07
Iter: 1961 loss: 4.99196517e-07
Iter: 1962 loss: 4.96387145e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2
+ date
Wed Oct 21 12:50:13 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1 --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c1c9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c0d6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c1fe9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c1fec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c11b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c07e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bff0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bfc2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bfc2488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c03be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c051840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf8f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf8fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf1e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c0a3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c0a3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf8fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e169fdbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3beed950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf04f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e16a2f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e16a2f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e169578c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e169b1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e16986488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e16986d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd00a87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd00b0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd00b0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df0055950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd0017ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d487bc510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d487bc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d487e8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd0080a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e169180d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.04727656
test_loss: 0.04593319
train_loss: 0.020359008
test_loss: 0.020845871
train_loss: 0.012422589
test_loss: 0.012919019
train_loss: 0.008681728
test_loss: 0.010014267
train_loss: 0.007508901
test_loss: 0.008326084
train_loss: 0.0070775202
test_loss: 0.007591439
train_loss: 0.0068660257
test_loss: 0.007374335
train_loss: 0.006080945
test_loss: 0.007047101
train_loss: 0.0059288573
test_loss: 0.006896944
train_loss: 0.0056730565
test_loss: 0.0066436795
train_loss: 0.0054848674
test_loss: 0.0066064075
train_loss: 0.0055770627
test_loss: 0.006447815
train_loss: 0.005030862
test_loss: 0.0062142513
train_loss: 0.0053278534
test_loss: 0.006308455
train_loss: 0.005508627
test_loss: 0.006254092
train_loss: 0.0052335486
test_loss: 0.0059769065
train_loss: 0.0052238847
test_loss: 0.0058520995
train_loss: 0.0052503035
test_loss: 0.0060572126
train_loss: 0.0049848347
test_loss: 0.0058310544
train_loss: 0.0050358595
test_loss: 0.005838017
train_loss: 0.0048251357
test_loss: 0.0057813325
train_loss: 0.0045970464
test_loss: 0.005680734
train_loss: 0.005254079
test_loss: 0.005992147
train_loss: 0.0051401136
test_loss: 0.005638298
train_loss: 0.0049516526
test_loss: 0.006040196
train_loss: 0.0049243188
test_loss: 0.0058531957
train_loss: 0.0049594105
test_loss: 0.0056235217
train_loss: 0.0045231264
test_loss: 0.00546072
train_loss: 0.0043745926
test_loss: 0.005543978
train_loss: 0.004616551
test_loss: 0.005645988
train_loss: 0.004455081
test_loss: 0.0053735687
train_loss: 0.004432856
test_loss: 0.005245917
train_loss: 0.0042045265
test_loss: 0.005195771
train_loss: 0.00458317
test_loss: 0.005483595
train_loss: 0.0048983945
test_loss: 0.0055129845
train_loss: 0.004615143
test_loss: 0.0053911014
train_loss: 0.004250705
test_loss: 0.005238079
train_loss: 0.0045403917
test_loss: 0.0054373527
train_loss: 0.004365779
test_loss: 0.0054899617
train_loss: 0.0039407113
test_loss: 0.0051720697
train_loss: 0.0043285526
test_loss: 0.0054733674
train_loss: 0.0043695047
test_loss: 0.00541137
train_loss: 0.0043933936
test_loss: 0.005496816
train_loss: 0.0043144957
test_loss: 0.0054034204
train_loss: 0.004173503
test_loss: 0.0053329626
train_loss: 0.0044706417
test_loss: 0.005421031
train_loss: 0.004827565
test_loss: 0.005529195
train_loss: 0.0044879937
test_loss: 0.0054503772
train_loss: 0.004344031
test_loss: 0.0052826703
train_loss: 0.003995328
test_loss: 0.005146161
train_loss: 0.0039687185
test_loss: 0.0050902455
train_loss: 0.004396828
test_loss: 0.005498315
train_loss: 0.0043527707
test_loss: 0.005422782
train_loss: 0.004341125
test_loss: 0.005199777
train_loss: 0.0044846903
test_loss: 0.005458369
train_loss: 0.004154704
test_loss: 0.0050623333
train_loss: 0.004050733
test_loss: 0.0050403574
train_loss: 0.004188638
test_loss: 0.0052478765
train_loss: 0.004090078
test_loss: 0.0050378595
train_loss: 0.0040287785
test_loss: 0.005103703
train_loss: 0.004470502
test_loss: 0.005316942
train_loss: 0.004007516
test_loss: 0.0049864785
train_loss: 0.0040696487
test_loss: 0.005383338
train_loss: 0.0042900415
test_loss: 0.0051232157
train_loss: 0.004276801
test_loss: 0.005227621
train_loss: 0.004370775
test_loss: 0.005151008
train_loss: 0.004278271
test_loss: 0.0049110292
train_loss: 0.0041910363
test_loss: 0.0052007353
train_loss: 0.0040061213
test_loss: 0.0050654765
train_loss: 0.004230417
test_loss: 0.0052400604
train_loss: 0.0040801372
test_loss: 0.005054953
train_loss: 0.0041322256
test_loss: 0.0054430296
train_loss: 0.0039695287
test_loss: 0.0054028886
train_loss: 0.004188831
test_loss: 0.005048675
train_loss: 0.004211356
test_loss: 0.005226396
train_loss: 0.0040747295
test_loss: 0.0050390055
train_loss: 0.0040901573
test_loss: 0.0050086533
train_loss: 0.004527921
test_loss: 0.0051036985
train_loss: 0.0042344187
test_loss: 0.0053009433
train_loss: 0.0041879914
test_loss: 0.005729495
train_loss: 0.003962545
test_loss: 0.0051319753
train_loss: 0.00396231
test_loss: 0.0053949202
train_loss: 0.00406551
test_loss: 0.0050234813
train_loss: 0.004281554
test_loss: 0.0049675265
train_loss: 0.003840463
test_loss: 0.005004796
train_loss: 0.003861918
test_loss: 0.0048124064
train_loss: 0.004077878
test_loss: 0.0050758366
train_loss: 0.004348949
test_loss: 0.005189467
train_loss: 0.0042964127
test_loss: 0.005293627
train_loss: 0.004622271
test_loss: 0.0050062817
train_loss: 0.0036814963
test_loss: 0.004964522
train_loss: 0.0040515554
test_loss: 0.0048893
train_loss: 0.004003311
test_loss: 0.0050535263
train_loss: 0.0038656262
test_loss: 0.0052211843
train_loss: 0.004233581
test_loss: 0.0052128984
train_loss: 0.004244972
test_loss: 0.0050733113
train_loss: 0.003734096
test_loss: 0.0049749585
train_loss: 0.0039005298
test_loss: 0.0049818265
train_loss: 0.003852158
test_loss: 0.004963888
train_loss: 0.003809529
test_loss: 0.00485149
train_loss: 0.0038083717
test_loss: 0.0047299247
train_loss: 0.0038670884
test_loss: 0.004858309
train_loss: 0.004141531
test_loss: 0.0051075784
train_loss: 0.003854332
test_loss: 0.0051533887
train_loss: 0.003903283
test_loss: 0.0049368357
train_loss: 0.0035809367
test_loss: 0.004748202
train_loss: 0.0038458654
test_loss: 0.0049080527
train_loss: 0.0038158007
test_loss: 0.00491799
train_loss: 0.004049514
test_loss: 0.0050756894
train_loss: 0.003860538
test_loss: 0.0051745637
train_loss: 0.003849937
test_loss: 0.00490927
train_loss: 0.004208629
test_loss: 0.0052628107
train_loss: 0.004120264
test_loss: 0.005456044
train_loss: 0.003965639
test_loss: 0.004940358
train_loss: 0.004514213
test_loss: 0.0059202267
train_loss: 0.004546942
test_loss: 0.0053944564
train_loss: 0.0037996555
test_loss: 0.0050111846
train_loss: 0.0038276783
test_loss: 0.0052018655
train_loss: 0.0039272117
test_loss: 0.005350185
train_loss: 0.0044833925
test_loss: 0.0051778895
train_loss: 0.0037072478
test_loss: 0.004636128
train_loss: 0.004448153
test_loss: 0.0050402265
train_loss: 0.003690899
test_loss: 0.0049431333
train_loss: 0.0039519984
test_loss: 0.0050689736
train_loss: 0.0038204936
test_loss: 0.0049312045
train_loss: 0.003975883
test_loss: 0.0050026695
train_loss: 0.0039585894
test_loss: 0.0048604137
train_loss: 0.0039747055
test_loss: 0.0050108917
train_loss: 0.004116197
test_loss: 0.004873778
train_loss: 0.003840178
test_loss: 0.0050073643
train_loss: 0.0041151163
test_loss: 0.0051851054
train_loss: 0.0040529515
test_loss: 0.005340728
train_loss: 0.0042377613
test_loss: 0.0048807976
train_loss: 0.003853942
test_loss: 0.0047380463
train_loss: 0.0038662502
test_loss: 0.0050563496
train_loss: 0.003977609
test_loss: 0.0052877567
train_loss: 0.004229832
test_loss: 0.0052416623
train_loss: 0.004124572
test_loss: 0.004931567
train_loss: 0.0038881025
test_loss: 0.005167265
train_loss: 0.0041002464
test_loss: 0.005011109
train_loss: 0.003926753
test_loss: 0.0049422313
train_loss: 0.0038399552
test_loss: 0.004860274
train_loss: 0.0039294674
test_loss: 0.004936772
train_loss: 0.0043076114
test_loss: 0.005151269
train_loss: 0.0038964455
test_loss: 0.0051749563
train_loss: 0.0042674905
test_loss: 0.0055120415
train_loss: 0.004371381
test_loss: 0.0052845865
train_loss: 0.003917507
test_loss: 0.0053507416
train_loss: 0.0040365397
test_loss: 0.004937497
train_loss: 0.004112854
test_loss: 0.0051582004
train_loss: 0.00378283
test_loss: 0.005273264
train_loss: 0.0040811463
test_loss: 0.004974869
train_loss: 0.0037222137
test_loss: 0.0050213193
train_loss: 0.0037600438
test_loss: 0.0051019955
train_loss: 0.0040761176
test_loss: 0.005103604
train_loss: 0.0039141383
test_loss: 0.0049815676
train_loss: 0.003943599
test_loss: 0.004933741
train_loss: 0.0036051688
test_loss: 0.004821793
train_loss: 0.003996802
test_loss: 0.0049254415
train_loss: 0.0041669896
test_loss: 0.0051585496
train_loss: 0.003814748
test_loss: 0.0047874707
train_loss: 0.0036339108
test_loss: 0.004842961
train_loss: 0.0032806636
test_loss: 0.004777748
train_loss: 0.0036772594
test_loss: 0.0049174116
train_loss: 0.0035595447
test_loss: 0.0049722474
train_loss: 0.0039821295
test_loss: 0.0055728722
train_loss: 0.003976546
test_loss: 0.005032206
train_loss: 0.0037200756
test_loss: 0.005048874
train_loss: 0.003965466
test_loss: 0.00508684
train_loss: 0.0041211266
test_loss: 0.004866873
train_loss: 0.0040455298
test_loss: 0.0049285213
train_loss: 0.0037072406
test_loss: 0.0050595365
train_loss: 0.0037576396
test_loss: 0.0051021506
train_loss: 0.00394328
test_loss: 0.0050316104
train_loss: 0.004056524
test_loss: 0.0046314932
train_loss: 0.003911481
test_loss: 0.0051122336
train_loss: 0.00390288
test_loss: 0.00489292
train_loss: 0.0037824865
test_loss: 0.004989925
train_loss: 0.0035473034
test_loss: 0.004863419
train_loss: 0.0037664764
test_loss: 0.004689192
train_loss: 0.0037540945
test_loss: 0.0047975522
train_loss: 0.003950988
test_loss: 0.004949582
train_loss: 0.003888343
test_loss: 0.0051081227
train_loss: 0.0039047392
test_loss: 0.0051911385
train_loss: 0.00377404
test_loss: 0.004968389
train_loss: 0.0039344695
test_loss: 0.005116324
train_loss: 0.0035391035
test_loss: 0.0048437733
train_loss: 0.0037415815
test_loss: 0.004794464
train_loss: 0.0039209407
test_loss: 0.004901974
train_loss: 0.0037607695
test_loss: 0.0050636707
train_loss: 0.0038862037
test_loss: 0.0052259755
train_loss: 0.0042873253
test_loss: 0.0049492144
train_loss: 0.0038079913
test_loss: 0.004918185
train_loss: 0.0037005213
test_loss: 0.005089249
train_loss: 0.003431982
test_loss: 0.004877839
train_loss: 0.0036086128
test_loss: 0.0047208937
train_loss: 0.0037006603
test_loss: 0.004921338
train_loss: 0.0036298716
test_loss: 0.0049652276
train_loss: 0.0037296498
test_loss: 0.0048173307
train_loss: 0.0036027643
test_loss: 0.004906307
train_loss: 0.0035160019
test_loss: 0.0047667264
train_loss: 0.0038320138
test_loss: 0.0047839954
train_loss: 0.0037321148
test_loss: 0.004803929
train_loss: 0.0036232378
test_loss: 0.0049608443
train_loss: 0.0036300302
test_loss: 0.0047470443
train_loss: 0.0037102774
test_loss: 0.0049362793
train_loss: 0.0040042573
test_loss: 0.0052024634
train_loss: 0.0041158707
test_loss: 0.0049546733
train_loss: 0.0041469866
test_loss: 0.004946996
train_loss: 0.0035444396
test_loss: 0.0049022315
train_loss: 0.0037661796
test_loss: 0.004641427
train_loss: 0.0038128847
test_loss: 0.004902811
train_loss: 0.0038745464
test_loss: 0.0049332287
train_loss: 0.0041110376
test_loss: 0.0050099217
train_loss: 0.0039720386
test_loss: 0.0051286533
train_loss: 0.0037678874
test_loss: 0.004886622
train_loss: 0.004087669
test_loss: 0.0051859133
train_loss: 0.0038763434
test_loss: 0.005125674
train_loss: 0.0035955287
test_loss: 0.004824592
train_loss: 0.0034081298
test_loss: 0.0047683967
train_loss: 0.0037911674
test_loss: 0.0048964852
train_loss: 0.0037684948
test_loss: 0.0047038835
train_loss: 0.0041774055
test_loss: 0.005015044
train_loss: 0.004058008
test_loss: 0.0048378007
train_loss: 0.0044563645
test_loss: 0.0054318244
train_loss: 0.0038768742
test_loss: 0.004977419
train_loss: 0.00397171
test_loss: 0.0048505454
train_loss: 0.0035615708
test_loss: 0.004926521
train_loss: 0.0037269536
test_loss: 0.0049402616
train_loss: 0.0039808885
test_loss: 0.0048241336
train_loss: 0.0036824988
test_loss: 0.005066928
train_loss: 0.0040922295
test_loss: 0.004890088
train_loss: 0.0037516146
test_loss: 0.004964741
train_loss: 0.00399173
test_loss: 0.0049610077
train_loss: 0.0038581018
test_loss: 0.0048372685
train_loss: 0.0038447117
test_loss: 0.0046807523
train_loss: 0.003529645
test_loss: 0.0048054433
train_loss: 0.003696106
test_loss: 0.0048628133
train_loss: 0.0037786337
test_loss: 0.0048463847
train_loss: 0.00389648
test_loss: 0.0049441196
train_loss: 0.003771523
test_loss: 0.004878116
train_loss: 0.003561622
test_loss: 0.0049103065
train_loss: 0.003283627
test_loss: 0.0046903347
train_loss: 0.0038562922
test_loss: 0.004814605
train_loss: 0.0037946096
test_loss: 0.004786944
train_loss: 0.003511081
test_loss: 0.0047318027
train_loss: 0.0039523984
test_loss: 0.004821574
train_loss: 0.0034490344
test_loss: 0.00475605
train_loss: 0.0037097821
test_loss: 0.0049733724
train_loss: 0.0038622105
test_loss: 0.004852749
train_loss: 0.0033625509
test_loss: 0.0047495333
train_loss: 0.0039992076
test_loss: 0.004892646
train_loss: 0.0037324708
test_loss: 0.0051407684
train_loss: 0.003599293
test_loss: 0.0049426183
train_loss: 0.0036131798
test_loss: 0.0048286193
train_loss: 0.0035765797
test_loss: 0.004919613
train_loss: 0.003663085
test_loss: 0.004808716
train_loss: 0.0039158044
test_loss: 0.005048465
train_loss: 0.003322386
test_loss: 0.0046977303
train_loss: 0.0035637286
test_loss: 0.0048091314
train_loss: 0.003702079
test_loss: 0.0050267107
train_loss: 0.003567373
test_loss: 0.0048376545
train_loss: 0.0037503275
test_loss: 0.0049219183
train_loss: 0.0035771704
test_loss: 0.0048175137
train_loss: 0.0038435033
test_loss: 0.0047257356
train_loss: 0.0036536953
test_loss: 0.0049859425
train_loss: 0.0039665895
test_loss: 0.004996082
train_loss: 0.0035752114
test_loss: 0.0050314125
train_loss: 0.003708268
test_loss: 0.004878426
train_loss: 0.003476856
test_loss: 0.0046565323
train_loss: 0.0038073263
test_loss: 0.0052709184
train_loss: 0.0037491038
test_loss: 0.005157239
train_loss: 0.003892885
test_loss: 0.004988088
train_loss: 0.003675954
test_loss: 0.0048428318
train_loss: 0.0036040666
test_loss: 0.0047763838
train_loss: 0.0035724523
test_loss: 0.004797819
train_loss: 0.003851192
test_loss: 0.004779588
train_loss: 0.0036379024
test_loss: 0.0051172874
train_loss: 0.0036419386
test_loss: 0.0048529683
train_loss: 0.0036641068
test_loss: 0.0050730407
train_loss: 0.0033207324
test_loss: 0.004816549
train_loss: 0.0036593867
test_loss: 0.004765007
train_loss: 0.003601563
test_loss: 0.0048208255
train_loss: 0.0040498935
test_loss: 0.0052548354
train_loss: 0.004216535
test_loss: 0.005115986
train_loss: 0.0034445403
test_loss: 0.004891754
train_loss: 0.0039063683
test_loss: 0.0050516496
train_loss: 0.003740353
test_loss: 0.0048036575
train_loss: 0.00455743
test_loss: 0.0049278084
train_loss: 0.003604249
test_loss: 0.00480437
train_loss: 0.0036901748
test_loss: 0.004742387
train_loss: 0.0038011344
test_loss: 0.0048483755
train_loss: 0.0039541656
test_loss: 0.004903208
train_loss: 0.0037479568
test_loss: 0.004778453
train_loss: 0.0037362783
test_loss: 0.00487643
train_loss: 0.003569607
test_loss: 0.0048808493
train_loss: 0.0035378523
test_loss: 0.004611264
train_loss: 0.0032953059
test_loss: 0.0048598917
train_loss: 0.0034989503
test_loss: 0.0048073838
train_loss: 0.0038382118
test_loss: 0.004934508
train_loss: 0.0036582276
test_loss: 0.0048824293
train_loss: 0.00396282
test_loss: 0.004711601
train_loss: 0.0037907823
test_loss: 0.004908465
train_loss: 0.0037003588
test_loss: 0.004931736
train_loss: 0.003879197
test_loss: 0.0048509496
train_loss: 0.003970066
test_loss: 0.005178622
train_loss: 0.0040620547
test_loss: 0.004807851
train_loss: 0.0034926604
test_loss: 0.0045962287
train_loss: 0.003913221
test_loss: 0.00489393
train_loss: 0.0036599962
test_loss: 0.004917236
train_loss: 0.0039206184
test_loss: 0.0048973374
train_loss: 0.0037983404
test_loss: 0.0048753195
train_loss: 0.0037766013
test_loss: 0.004811995
train_loss: 0.0035362511
test_loss: 0.004782578
train_loss: 0.004312228
test_loss: 0.0049737548
train_loss: 0.0037195615
test_loss: 0.0048246905
train_loss: 0.0035572296
test_loss: 0.00490188
train_loss: 0.0036038803
test_loss: 0.0049216617
train_loss: 0.0037860763
test_loss: 0.0051057674
train_loss: 0.0038883863
test_loss: 0.0049812035
train_loss: 0.0036427223
test_loss: 0.0047516758
train_loss: 0.003776729
test_loss: 0.004851938
train_loss: 0.0039195246
test_loss: 0.0048897346
train_loss: 0.0038902499
test_loss: 0.004900598
train_loss: 0.0037145508
test_loss: 0.004710713
train_loss: 0.003312184
test_loss: 0.0046347575
train_loss: 0.0033938938
test_loss: 0.0048259455
train_loss: 0.003648201
test_loss: 0.0047019273
train_loss: 0.0037612522
test_loss: 0.00520391
train_loss: 0.003672793
test_loss: 0.004749397
train_loss: 0.0038904839
test_loss: 0.004914551
train_loss: 0.0035522443
test_loss: 0.005242437
train_loss: 0.0043468107
test_loss: 0.0049796416
train_loss: 0.0038637673
test_loss: 0.0049981177
train_loss: 0.004089058
test_loss: 0.004828428
train_loss: 0.0033134588
test_loss: 0.004659542
train_loss: 0.0036833468
test_loss: 0.004897387
train_loss: 0.003526947
test_loss: 0.0046550254
train_loss: 0.0036846504
test_loss: 0.0048910007
train_loss: 0.0032947916
test_loss: 0.004656339
train_loss: 0.0037518921
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.00491748
train_loss: 0.0037773207
test_loss: 0.0047932677
train_loss: 0.0036939448
test_loss: 0.0049175336
train_loss: 0.003745668
test_loss: 0.004673335
train_loss: 0.0037015053
test_loss: 0.004611061
train_loss: 0.003472522
test_loss: 0.004939607
train_loss: 0.0033002102
test_loss: 0.0046812017
train_loss: 0.003697203
test_loss: 0.0046533695
train_loss: 0.004150767
test_loss: 0.0050054826
train_loss: 0.0038694139
test_loss: 0.0049577495
train_loss: 0.0034809562
test_loss: 0.0048969663
train_loss: 0.0031691806
test_loss: 0.0048483075
train_loss: 0.003703667
test_loss: 0.0046880506
train_loss: 0.0034918059
test_loss: 0.0049812114
train_loss: 0.0038345787
test_loss: 0.0048520383
train_loss: 0.0038087736
test_loss: 0.0050681983
train_loss: 0.0036893548
test_loss: 0.004852346
train_loss: 0.0034846244
test_loss: 0.004621957
train_loss: 0.0034227418
test_loss: 0.004827002
train_loss: 0.0035287263
test_loss: 0.0049856976
train_loss: 0.0032755858
test_loss: 0.0047136103
train_loss: 0.0033541848
test_loss: 0.00471906
train_loss: 0.0038389787
test_loss: 0.0046294057
train_loss: 0.0036608658
test_loss: 0.0048499075
train_loss: 0.0037107347
test_loss: 0.0049177846
train_loss: 0.0037006314
test_loss: 0.004704923
train_loss: 0.0035241367
test_loss: 0.005058254
train_loss: 0.0033925944
test_loss: 0.004629879
train_loss: 0.003570295
test_loss: 0.004769428
train_loss: 0.003619008
test_loss: 0.0047128852
train_loss: 0.0033712944
test_loss: 0.004765036
train_loss: 0.0033661881
test_loss: 0.0046601538
train_loss: 0.0036110342
test_loss: 0.0049228705
train_loss: 0.0036711479
test_loss: 0.0048413556
train_loss: 0.0039656498
test_loss: 0.0050903563
train_loss: 0.0037513014
test_loss: 0.004807814
train_loss: 0.00345937
test_loss: 0.0047422014
train_loss: 0.003302209
test_loss: 0.004665612
train_loss: 0.00325434
test_loss: 0.0046447716
train_loss: 0.003593314
test_loss: 0.0048221
train_loss: 0.003548547
test_loss: 0.004798765
train_loss: 0.0036046212
test_loss: 0.0048847585
train_loss: 0.0037756732
test_loss: 0.0049653156
train_loss: 0.003723559
test_loss: 0.0048192535
train_loss: 0.0036715092
test_loss: 0.004856458
train_loss: 0.0034368187
test_loss: 0.005138211
train_loss: 0.003440099
test_loss: 0.0049758987
train_loss: 0.003595437
test_loss: 0.004681973
train_loss: 0.0033854526
test_loss: 0.0046673454
train_loss: 0.003344398
test_loss: 0.004659061
train_loss: 0.0035955035
test_loss: 0.004625402
train_loss: 0.003473172
test_loss: 0.0048095877
train_loss: 0.0036737993
test_loss: 0.004776191
train_loss: 0.0033062717
test_loss: 0.0047915187
train_loss: 0.0036162413
test_loss: 0.0048447866
train_loss: 0.0037277567
test_loss: 0.0046075084
train_loss: 0.003240052
test_loss: 0.004502551
train_loss: 0.003366332
test_loss: 0.00475155
train_loss: 0.0031512477
test_loss: 0.0047788117
train_loss: 0.0033541385
test_loss: 0.0046235244
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d9f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7deec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d9ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d29bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d43510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d43950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7c90bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d430d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7c3e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7c3ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7c10b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7bbfd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7bb5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b56840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b27840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b27bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b452f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b0ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7ac89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a66950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a668c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a20598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a55730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a00620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d79e4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1c15ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1be2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1bfc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1bfc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1bc5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1b66950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1b8a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1b8ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1b34620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1af2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1af28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.37979984e-05
Iter: 2 loss: 2.00734066e-05
Iter: 3 loss: 1.98604357e-05
Iter: 4 loss: 1.85861591e-05
Iter: 5 loss: 2.95694463e-05
Iter: 6 loss: 1.85166373e-05
Iter: 7 loss: 1.75345758e-05
Iter: 8 loss: 1.85106328e-05
Iter: 9 loss: 1.69821069e-05
Iter: 10 loss: 1.56597744e-05
Iter: 11 loss: 1.50086435e-05
Iter: 12 loss: 1.43753141e-05
Iter: 13 loss: 1.30340341e-05
Iter: 14 loss: 2.10407288e-05
Iter: 15 loss: 1.2864487e-05
Iter: 16 loss: 1.18328735e-05
Iter: 17 loss: 1.29643668e-05
Iter: 18 loss: 1.12712914e-05
Iter: 19 loss: 1.04086903e-05
Iter: 20 loss: 1.47030551e-05
Iter: 21 loss: 1.02621125e-05
Iter: 22 loss: 9.6544145e-06
Iter: 23 loss: 1.1926596e-05
Iter: 24 loss: 9.50703179e-06
Iter: 25 loss: 9.01402382e-06
Iter: 26 loss: 1.34773563e-05
Iter: 27 loss: 8.99142e-06
Iter: 28 loss: 8.68823099e-06
Iter: 29 loss: 8.29024793e-06
Iter: 30 loss: 8.26540054e-06
Iter: 31 loss: 7.66986159e-06
Iter: 32 loss: 1.02300583e-05
Iter: 33 loss: 7.5471562e-06
Iter: 34 loss: 7.0924707e-06
Iter: 35 loss: 7.60932335e-06
Iter: 36 loss: 6.84757651e-06
Iter: 37 loss: 6.44236843e-06
Iter: 38 loss: 9.5400892e-06
Iter: 39 loss: 6.41196266e-06
Iter: 40 loss: 6.33998707e-06
Iter: 41 loss: 6.27226427e-06
Iter: 42 loss: 6.13927523e-06
Iter: 43 loss: 5.97515555e-06
Iter: 44 loss: 5.96080281e-06
Iter: 45 loss: 5.7410607e-06
Iter: 46 loss: 7.1040331e-06
Iter: 47 loss: 5.71521332e-06
Iter: 48 loss: 5.58852753e-06
Iter: 49 loss: 5.52369602e-06
Iter: 50 loss: 5.46462661e-06
Iter: 51 loss: 5.26378744e-06
Iter: 52 loss: 5.86902752e-06
Iter: 53 loss: 5.203417e-06
Iter: 54 loss: 5.00308306e-06
Iter: 55 loss: 5.54604503e-06
Iter: 56 loss: 4.93731795e-06
Iter: 57 loss: 4.79714708e-06
Iter: 58 loss: 5.2442515e-06
Iter: 59 loss: 4.75701108e-06
Iter: 60 loss: 4.61417949e-06
Iter: 61 loss: 5.0772087e-06
Iter: 62 loss: 4.57410624e-06
Iter: 63 loss: 4.41454631e-06
Iter: 64 loss: 4.86618546e-06
Iter: 65 loss: 4.36421942e-06
Iter: 66 loss: 4.24622476e-06
Iter: 67 loss: 4.30293858e-06
Iter: 68 loss: 4.16674447e-06
Iter: 69 loss: 4.01927673e-06
Iter: 70 loss: 4.68120379e-06
Iter: 71 loss: 3.99051305e-06
Iter: 72 loss: 3.88383842e-06
Iter: 73 loss: 3.89412025e-06
Iter: 74 loss: 3.80156825e-06
Iter: 75 loss: 3.7089e-06
Iter: 76 loss: 3.70554471e-06
Iter: 77 loss: 3.63468962e-06
Iter: 78 loss: 4.45414207e-06
Iter: 79 loss: 3.63375761e-06
Iter: 80 loss: 3.59953106e-06
Iter: 81 loss: 3.52800407e-06
Iter: 82 loss: 4.741054e-06
Iter: 83 loss: 3.5263015e-06
Iter: 84 loss: 3.44726163e-06
Iter: 85 loss: 4.33360492e-06
Iter: 86 loss: 3.44572982e-06
Iter: 87 loss: 3.40523752e-06
Iter: 88 loss: 3.3436902e-06
Iter: 89 loss: 3.34276092e-06
Iter: 90 loss: 3.26602776e-06
Iter: 91 loss: 3.83028191e-06
Iter: 92 loss: 3.25964311e-06
Iter: 93 loss: 3.20700497e-06
Iter: 94 loss: 3.3958936e-06
Iter: 95 loss: 3.19345327e-06
Iter: 96 loss: 3.1326415e-06
Iter: 97 loss: 3.07977461e-06
Iter: 98 loss: 3.06346465e-06
Iter: 99 loss: 3.02397029e-06
Iter: 100 loss: 3.01965883e-06
Iter: 101 loss: 2.97814518e-06
Iter: 102 loss: 2.91228457e-06
Iter: 103 loss: 2.91168817e-06
Iter: 104 loss: 2.85222086e-06
Iter: 105 loss: 3.38087625e-06
Iter: 106 loss: 2.84926182e-06
Iter: 107 loss: 2.79919504e-06
Iter: 108 loss: 2.75318621e-06
Iter: 109 loss: 2.74100967e-06
Iter: 110 loss: 2.70164674e-06
Iter: 111 loss: 2.69908833e-06
Iter: 112 loss: 2.67061796e-06
Iter: 113 loss: 3.10302494e-06
Iter: 114 loss: 2.67066707e-06
Iter: 115 loss: 2.65094627e-06
Iter: 116 loss: 2.61486434e-06
Iter: 117 loss: 3.4523764e-06
Iter: 118 loss: 2.61485911e-06
Iter: 119 loss: 2.58815498e-06
Iter: 120 loss: 2.82357928e-06
Iter: 121 loss: 2.58684349e-06
Iter: 122 loss: 2.5590125e-06
Iter: 123 loss: 2.55460759e-06
Iter: 124 loss: 2.53534449e-06
Iter: 125 loss: 2.49855611e-06
Iter: 126 loss: 2.48258652e-06
Iter: 127 loss: 2.46367949e-06
Iter: 128 loss: 2.42752435e-06
Iter: 129 loss: 2.97072484e-06
Iter: 130 loss: 2.42749297e-06
Iter: 131 loss: 2.39622295e-06
Iter: 132 loss: 2.42907163e-06
Iter: 133 loss: 2.37885888e-06
Iter: 134 loss: 2.34845902e-06
Iter: 135 loss: 2.4816004e-06
Iter: 136 loss: 2.34227582e-06
Iter: 137 loss: 2.32174693e-06
Iter: 138 loss: 2.4337437e-06
Iter: 139 loss: 2.31865624e-06
Iter: 140 loss: 2.2934621e-06
Iter: 141 loss: 2.26942188e-06
Iter: 142 loss: 2.26368775e-06
Iter: 143 loss: 2.23628217e-06
Iter: 144 loss: 2.29141e-06
Iter: 145 loss: 2.22513063e-06
Iter: 146 loss: 2.1907922e-06
Iter: 147 loss: 2.35683819e-06
Iter: 148 loss: 2.1847186e-06
Iter: 149 loss: 2.18257856e-06
Iter: 150 loss: 2.17609886e-06
Iter: 151 loss: 2.16651961e-06
Iter: 152 loss: 2.14154784e-06
Iter: 153 loss: 2.33164383e-06
Iter: 154 loss: 2.13662179e-06
Iter: 155 loss: 2.10879034e-06
Iter: 156 loss: 2.24620635e-06
Iter: 157 loss: 2.10406051e-06
Iter: 158 loss: 2.08938536e-06
Iter: 159 loss: 2.31699664e-06
Iter: 160 loss: 2.08941287e-06
Iter: 161 loss: 2.07863582e-06
Iter: 162 loss: 2.06056757e-06
Iter: 163 loss: 2.06059849e-06
Iter: 164 loss: 2.03603736e-06
Iter: 165 loss: 2.09811878e-06
Iter: 166 loss: 2.02742012e-06
Iter: 167 loss: 2.00914815e-06
Iter: 168 loss: 2.09004611e-06
Iter: 169 loss: 2.00552404e-06
Iter: 170 loss: 1.98710677e-06
Iter: 171 loss: 2.08887445e-06
Iter: 172 loss: 1.98444332e-06
Iter: 173 loss: 1.97053214e-06
Iter: 174 loss: 1.95895427e-06
Iter: 175 loss: 1.95498546e-06
Iter: 176 loss: 1.93637743e-06
Iter: 177 loss: 1.93645792e-06
Iter: 178 loss: 1.92511197e-06
Iter: 179 loss: 1.93043229e-06
Iter: 180 loss: 1.91750314e-06
Iter: 181 loss: 1.90388835e-06
Iter: 182 loss: 1.90020887e-06
Iter: 183 loss: 1.89184493e-06
Iter: 184 loss: 1.87406852e-06
Iter: 185 loss: 1.95358939e-06
Iter: 186 loss: 1.87069236e-06
Iter: 187 loss: 1.86319278e-06
Iter: 188 loss: 1.86018929e-06
Iter: 189 loss: 1.85395243e-06
Iter: 190 loss: 1.8435785e-06
Iter: 191 loss: 1.84345822e-06
Iter: 192 loss: 1.83271322e-06
Iter: 193 loss: 1.83118118e-06
Iter: 194 loss: 1.82361555e-06
Iter: 195 loss: 1.8096398e-06
Iter: 196 loss: 2.00871864e-06
Iter: 197 loss: 1.80962866e-06
Iter: 198 loss: 1.8005876e-06
Iter: 199 loss: 1.8011101e-06
Iter: 200 loss: 1.79351434e-06
Iter: 201 loss: 1.78378195e-06
Iter: 202 loss: 1.79597635e-06
Iter: 203 loss: 1.77872653e-06
Iter: 204 loss: 1.76509434e-06
Iter: 205 loss: 1.7826992e-06
Iter: 206 loss: 1.75813454e-06
Iter: 207 loss: 1.7487223e-06
Iter: 208 loss: 1.74858008e-06
Iter: 209 loss: 1.74140541e-06
Iter: 210 loss: 1.72845773e-06
Iter: 211 loss: 2.04459593e-06
Iter: 212 loss: 1.72850173e-06
Iter: 213 loss: 1.71826639e-06
Iter: 214 loss: 1.71798251e-06
Iter: 215 loss: 1.71107081e-06
Iter: 216 loss: 1.71465945e-06
Iter: 217 loss: 1.70647138e-06
Iter: 218 loss: 1.69792474e-06
Iter: 219 loss: 1.71066858e-06
Iter: 220 loss: 1.69376767e-06
Iter: 221 loss: 1.6895068e-06
Iter: 222 loss: 1.68883832e-06
Iter: 223 loss: 1.68290399e-06
Iter: 224 loss: 1.66937707e-06
Iter: 225 loss: 1.85794727e-06
Iter: 226 loss: 1.66875816e-06
Iter: 227 loss: 1.66072869e-06
Iter: 228 loss: 1.69329735e-06
Iter: 229 loss: 1.65892482e-06
Iter: 230 loss: 1.65091024e-06
Iter: 231 loss: 1.67881558e-06
Iter: 232 loss: 1.64888684e-06
Iter: 233 loss: 1.63940422e-06
Iter: 234 loss: 1.66836628e-06
Iter: 235 loss: 1.636663e-06
Iter: 236 loss: 1.63025743e-06
Iter: 237 loss: 1.62546689e-06
Iter: 238 loss: 1.62344736e-06
Iter: 239 loss: 1.61341563e-06
Iter: 240 loss: 1.67690018e-06
Iter: 241 loss: 1.61229536e-06
Iter: 242 loss: 1.60530772e-06
Iter: 243 loss: 1.61361925e-06
Iter: 244 loss: 1.60169589e-06
Iter: 245 loss: 1.59155525e-06
Iter: 246 loss: 1.64066728e-06
Iter: 247 loss: 1.58982857e-06
Iter: 248 loss: 1.5840377e-06
Iter: 249 loss: 1.6030159e-06
Iter: 250 loss: 1.58239584e-06
Iter: 251 loss: 1.57606894e-06
Iter: 252 loss: 1.58158173e-06
Iter: 253 loss: 1.5724205e-06
Iter: 254 loss: 1.56270698e-06
Iter: 255 loss: 1.58252851e-06
Iter: 256 loss: 1.55883686e-06
Iter: 257 loss: 1.55463908e-06
Iter: 258 loss: 1.55466864e-06
Iter: 259 loss: 1.55040948e-06
Iter: 260 loss: 1.5612203e-06
Iter: 261 loss: 1.54900727e-06
Iter: 262 loss: 1.54548025e-06
Iter: 263 loss: 1.53664837e-06
Iter: 264 loss: 1.61109733e-06
Iter: 265 loss: 1.53514486e-06
Iter: 266 loss: 1.52678092e-06
Iter: 267 loss: 1.58492662e-06
Iter: 268 loss: 1.52602945e-06
Iter: 269 loss: 1.52030645e-06
Iter: 270 loss: 1.60345974e-06
Iter: 271 loss: 1.52026871e-06
Iter: 272 loss: 1.51529105e-06
Iter: 273 loss: 1.51127915e-06
Iter: 274 loss: 1.50975939e-06
Iter: 275 loss: 1.50332e-06
Iter: 276 loss: 1.51848542e-06
Iter: 277 loss: 1.50093229e-06
Iter: 278 loss: 1.49428524e-06
Iter: 279 loss: 1.51142603e-06
Iter: 280 loss: 1.49198559e-06
Iter: 281 loss: 1.48635331e-06
Iter: 282 loss: 1.51792358e-06
Iter: 283 loss: 1.48556819e-06
Iter: 284 loss: 1.47928449e-06
Iter: 285 loss: 1.48634024e-06
Iter: 286 loss: 1.47589321e-06
Iter: 287 loss: 1.47008802e-06
Iter: 288 loss: 1.50990843e-06
Iter: 289 loss: 1.46948696e-06
Iter: 290 loss: 1.46543312e-06
Iter: 291 loss: 1.4695097e-06
Iter: 292 loss: 1.4631014e-06
Iter: 293 loss: 1.45664899e-06
Iter: 294 loss: 1.46882041e-06
Iter: 295 loss: 1.45387116e-06
Iter: 296 loss: 1.45324225e-06
Iter: 297 loss: 1.45134504e-06
Iter: 298 loss: 1.44987166e-06
Iter: 299 loss: 1.44670912e-06
Iter: 300 loss: 1.50218898e-06
Iter: 301 loss: 1.44663829e-06
Iter: 302 loss: 1.44265482e-06
Iter: 303 loss: 1.43730563e-06
Iter: 304 loss: 1.43700447e-06
Iter: 305 loss: 1.43150351e-06
Iter: 306 loss: 1.46281263e-06
Iter: 307 loss: 1.43076704e-06
Iter: 308 loss: 1.42451393e-06
Iter: 309 loss: 1.46732214e-06
Iter: 310 loss: 1.42384738e-06
Iter: 311 loss: 1.42092824e-06
Iter: 312 loss: 1.41545797e-06
Iter: 313 loss: 1.5415759e-06
Iter: 314 loss: 1.41544501e-06
Iter: 315 loss: 1.40801046e-06
Iter: 316 loss: 1.45114655e-06
Iter: 317 loss: 1.40713e-06
Iter: 318 loss: 1.40235943e-06
Iter: 319 loss: 1.41501482e-06
Iter: 320 loss: 1.40079362e-06
Iter: 321 loss: 1.39560711e-06
Iter: 322 loss: 1.42624901e-06
Iter: 323 loss: 1.39497502e-06
Iter: 324 loss: 1.39055987e-06
Iter: 325 loss: 1.39394456e-06
Iter: 326 loss: 1.38780706e-06
Iter: 327 loss: 1.38298162e-06
Iter: 328 loss: 1.4095981e-06
Iter: 329 loss: 1.3821857e-06
Iter: 330 loss: 1.37860707e-06
Iter: 331 loss: 1.39425015e-06
Iter: 332 loss: 1.37786083e-06
Iter: 333 loss: 1.3747765e-06
Iter: 334 loss: 1.4036259e-06
Iter: 335 loss: 1.37461598e-06
Iter: 336 loss: 1.37141251e-06
Iter: 337 loss: 1.3679462e-06
Iter: 338 loss: 1.36745018e-06
Iter: 339 loss: 1.36339315e-06
Iter: 340 loss: 1.3647666e-06
Iter: 341 loss: 1.36047424e-06
Iter: 342 loss: 1.3565591e-06
Iter: 343 loss: 1.3685285e-06
Iter: 344 loss: 1.35538505e-06
Iter: 345 loss: 1.35118921e-06
Iter: 346 loss: 1.37292079e-06
Iter: 347 loss: 1.35047537e-06
Iter: 348 loss: 1.34579068e-06
Iter: 349 loss: 1.35928951e-06
Iter: 350 loss: 1.34423203e-06
Iter: 351 loss: 1.34189213e-06
Iter: 352 loss: 1.33714718e-06
Iter: 353 loss: 1.4304793e-06
Iter: 354 loss: 1.33707715e-06
Iter: 355 loss: 1.33158551e-06
Iter: 356 loss: 1.3925503e-06
Iter: 357 loss: 1.33147046e-06
Iter: 358 loss: 1.32819264e-06
Iter: 359 loss: 1.33952676e-06
Iter: 360 loss: 1.32728951e-06
Iter: 361 loss: 1.32336447e-06
Iter: 362 loss: 1.33437106e-06
Iter: 363 loss: 1.32214268e-06
Iter: 364 loss: 1.31889215e-06
Iter: 365 loss: 1.32351443e-06
Iter: 366 loss: 1.31732202e-06
Iter: 367 loss: 1.31349043e-06
Iter: 368 loss: 1.33715218e-06
Iter: 369 loss: 1.31311742e-06
Iter: 370 loss: 1.31070124e-06
Iter: 371 loss: 1.34851211e-06
Iter: 372 loss: 1.31074341e-06
Iter: 373 loss: 1.30888884e-06
Iter: 374 loss: 1.30648186e-06
Iter: 375 loss: 1.30639194e-06
Iter: 376 loss: 1.30261219e-06
Iter: 377 loss: 1.30057833e-06
Iter: 378 loss: 1.29888497e-06
Iter: 379 loss: 1.2952836e-06
Iter: 380 loss: 1.31140109e-06
Iter: 381 loss: 1.29460216e-06
Iter: 382 loss: 1.29106684e-06
Iter: 383 loss: 1.3053849e-06
Iter: 384 loss: 1.29027444e-06
Iter: 385 loss: 1.28683371e-06
Iter: 386 loss: 1.31085415e-06
Iter: 387 loss: 1.28647912e-06
Iter: 388 loss: 1.28427678e-06
Iter: 389 loss: 1.2808199e-06
Iter: 390 loss: 1.28084832e-06
Iter: 391 loss: 1.27706778e-06
Iter: 392 loss: 1.30461444e-06
Iter: 393 loss: 1.27672911e-06
Iter: 394 loss: 1.27374699e-06
Iter: 395 loss: 1.27363217e-06
Iter: 396 loss: 1.27140697e-06
Iter: 397 loss: 1.26777741e-06
Iter: 398 loss: 1.26775808e-06
Iter: 399 loss: 1.2657373e-06
Iter: 400 loss: 1.26895861e-06
Iter: 401 loss: 1.26480063e-06
Iter: 402 loss: 1.26256987e-06
Iter: 403 loss: 1.26851205e-06
Iter: 404 loss: 1.2618309e-06
Iter: 405 loss: 1.25880717e-06
Iter: 406 loss: 1.27452176e-06
Iter: 407 loss: 1.25840199e-06
Iter: 408 loss: 1.25633392e-06
Iter: 409 loss: 1.25894621e-06
Iter: 410 loss: 1.2552473e-06
Iter: 411 loss: 1.25350732e-06
Iter: 412 loss: 1.2517653e-06
Iter: 413 loss: 1.25140537e-06
Iter: 414 loss: 1.24800897e-06
Iter: 415 loss: 1.25483916e-06
Iter: 416 loss: 1.2466603e-06
Iter: 417 loss: 1.24352482e-06
Iter: 418 loss: 1.26050134e-06
Iter: 419 loss: 1.24301221e-06
Iter: 420 loss: 1.24084659e-06
Iter: 421 loss: 1.26413556e-06
Iter: 422 loss: 1.2407861e-06
Iter: 423 loss: 1.23870552e-06
Iter: 424 loss: 1.23473819e-06
Iter: 425 loss: 1.31865772e-06
Iter: 426 loss: 1.23473205e-06
Iter: 427 loss: 1.23122038e-06
Iter: 428 loss: 1.24270809e-06
Iter: 429 loss: 1.23030895e-06
Iter: 430 loss: 1.22652887e-06
Iter: 431 loss: 1.24442477e-06
Iter: 432 loss: 1.2258896e-06
Iter: 433 loss: 1.22390202e-06
Iter: 434 loss: 1.24162693e-06
Iter: 435 loss: 1.2238072e-06
Iter: 436 loss: 1.22157826e-06
Iter: 437 loss: 1.22062795e-06
Iter: 438 loss: 1.21945993e-06
Iter: 439 loss: 1.21849621e-06
Iter: 440 loss: 1.21817891e-06
Iter: 441 loss: 1.21693608e-06
Iter: 442 loss: 1.21672872e-06
Iter: 443 loss: 1.2158647e-06
Iter: 444 loss: 1.21401456e-06
Iter: 445 loss: 1.21284245e-06
Iter: 446 loss: 1.21207938e-06
Iter: 447 loss: 1.20928405e-06
Iter: 448 loss: 1.21848575e-06
Iter: 449 loss: 1.20847551e-06
Iter: 450 loss: 1.20601248e-06
Iter: 451 loss: 1.20475238e-06
Iter: 452 loss: 1.20364052e-06
Iter: 453 loss: 1.20092932e-06
Iter: 454 loss: 1.24107942e-06
Iter: 455 loss: 1.20094955e-06
Iter: 456 loss: 1.19902347e-06
Iter: 457 loss: 1.2065866e-06
Iter: 458 loss: 1.19863989e-06
Iter: 459 loss: 1.19638821e-06
Iter: 460 loss: 1.19720812e-06
Iter: 461 loss: 1.19476965e-06
Iter: 462 loss: 1.19305514e-06
Iter: 463 loss: 1.19207391e-06
Iter: 464 loss: 1.19127196e-06
Iter: 465 loss: 1.18831258e-06
Iter: 466 loss: 1.20560821e-06
Iter: 467 loss: 1.18790126e-06
Iter: 468 loss: 1.18557216e-06
Iter: 469 loss: 1.19123638e-06
Iter: 470 loss: 1.1847319e-06
Iter: 471 loss: 1.18202206e-06
Iter: 472 loss: 1.20247648e-06
Iter: 473 loss: 1.18182663e-06
Iter: 474 loss: 1.18070807e-06
Iter: 475 loss: 1.19422282e-06
Iter: 476 loss: 1.18064747e-06
Iter: 477 loss: 1.17940476e-06
Iter: 478 loss: 1.17825425e-06
Iter: 479 loss: 1.17795173e-06
Iter: 480 loss: 1.17619561e-06
Iter: 481 loss: 1.17741331e-06
Iter: 482 loss: 1.17514548e-06
Iter: 483 loss: 1.17296133e-06
Iter: 484 loss: 1.17949503e-06
Iter: 485 loss: 1.17225363e-06
Iter: 486 loss: 1.17015793e-06
Iter: 487 loss: 1.16984438e-06
Iter: 488 loss: 1.16834974e-06
Iter: 489 loss: 1.16554327e-06
Iter: 490 loss: 1.17953857e-06
Iter: 491 loss: 1.16508954e-06
Iter: 492 loss: 1.16350395e-06
Iter: 493 loss: 1.1634919e-06
Iter: 494 loss: 1.16225976e-06
Iter: 495 loss: 1.16117099e-06
Iter: 496 loss: 1.16080651e-06
Iter: 497 loss: 1.15872115e-06
Iter: 498 loss: 1.16198044e-06
Iter: 499 loss: 1.15769183e-06
Iter: 500 loss: 1.15525256e-06
Iter: 501 loss: 1.15527337e-06
Iter: 502 loss: 1.1532934e-06
Iter: 503 loss: 1.15121679e-06
Iter: 504 loss: 1.15119246e-06
Iter: 505 loss: 1.14993111e-06
Iter: 506 loss: 1.15845887e-06
Iter: 507 loss: 1.14979707e-06
Iter: 508 loss: 1.14860609e-06
Iter: 509 loss: 1.15192984e-06
Iter: 510 loss: 1.14816362e-06
Iter: 511 loss: 1.14635316e-06
Iter: 512 loss: 1.14786235e-06
Iter: 513 loss: 1.14527575e-06
Iter: 514 loss: 1.14375212e-06
Iter: 515 loss: 1.14237616e-06
Iter: 516 loss: 1.14198383e-06
Iter: 517 loss: 1.13968986e-06
Iter: 518 loss: 1.15437842e-06
Iter: 519 loss: 1.13945816e-06
Iter: 520 loss: 1.13750866e-06
Iter: 521 loss: 1.14106945e-06
Iter: 522 loss: 1.13669864e-06
Iter: 523 loss: 1.13477086e-06
Iter: 524 loss: 1.13523311e-06
Iter: 525 loss: 1.13336694e-06
Iter: 526 loss: 1.13174917e-06
Iter: 527 loss: 1.13176225e-06
Iter: 528 loss: 1.13013209e-06
Iter: 529 loss: 1.13046565e-06
Iter: 530 loss: 1.12893986e-06
Iter: 531 loss: 1.12670591e-06
Iter: 532 loss: 1.12982491e-06
Iter: 533 loss: 1.12558348e-06
Iter: 534 loss: 1.12373766e-06
Iter: 535 loss: 1.12344105e-06
Iter: 536 loss: 1.12223029e-06
Iter: 537 loss: 1.11992631e-06
Iter: 538 loss: 1.14793772e-06
Iter: 539 loss: 1.11983422e-06
Iter: 540 loss: 1.11837255e-06
Iter: 541 loss: 1.12038811e-06
Iter: 542 loss: 1.11764757e-06
Iter: 543 loss: 1.11576753e-06
Iter: 544 loss: 1.13891906e-06
Iter: 545 loss: 1.11574468e-06
Iter: 546 loss: 1.11472036e-06
Iter: 547 loss: 1.11845839e-06
Iter: 548 loss: 1.11445752e-06
Iter: 549 loss: 1.11376812e-06
Iter: 550 loss: 1.11221652e-06
Iter: 551 loss: 1.13543672e-06
Iter: 552 loss: 1.11216582e-06
Iter: 553 loss: 1.11004772e-06
Iter: 554 loss: 1.11583086e-06
Iter: 555 loss: 1.1094063e-06
Iter: 556 loss: 1.10772737e-06
Iter: 557 loss: 1.12446241e-06
Iter: 558 loss: 1.10766121e-06
Iter: 559 loss: 1.10633709e-06
Iter: 560 loss: 1.10403e-06
Iter: 561 loss: 1.1040031e-06
Iter: 562 loss: 1.10149745e-06
Iter: 563 loss: 1.12389762e-06
Iter: 564 loss: 1.10135966e-06
Iter: 565 loss: 1.09981374e-06
Iter: 566 loss: 1.1231366e-06
Iter: 567 loss: 1.09978077e-06
Iter: 568 loss: 1.09875987e-06
Iter: 569 loss: 1.09679331e-06
Iter: 570 loss: 1.14012403e-06
Iter: 571 loss: 1.09681548e-06
Iter: 572 loss: 1.09451378e-06
Iter: 573 loss: 1.10607425e-06
Iter: 574 loss: 1.09413384e-06
Iter: 575 loss: 1.09259872e-06
Iter: 576 loss: 1.09321456e-06
Iter: 577 loss: 1.09150767e-06
Iter: 578 loss: 1.08979839e-06
Iter: 579 loss: 1.11294219e-06
Iter: 580 loss: 1.0897752e-06
Iter: 581 loss: 1.08875702e-06
Iter: 582 loss: 1.08878157e-06
Iter: 583 loss: 1.08791687e-06
Iter: 584 loss: 1.08647464e-06
Iter: 585 loss: 1.08645656e-06
Iter: 586 loss: 1.08464758e-06
Iter: 587 loss: 1.09172925e-06
Iter: 588 loss: 1.08416646e-06
Iter: 589 loss: 1.08302538e-06
Iter: 590 loss: 1.08187328e-06
Iter: 591 loss: 1.081645e-06
Iter: 592 loss: 1.07943583e-06
Iter: 593 loss: 1.09265955e-06
Iter: 594 loss: 1.07911865e-06
Iter: 595 loss: 1.07752544e-06
Iter: 596 loss: 1.08401457e-06
Iter: 597 loss: 1.07716357e-06
Iter: 598 loss: 1.07583162e-06
Iter: 599 loss: 1.07665073e-06
Iter: 600 loss: 1.07500023e-06
Iter: 601 loss: 1.07395181e-06
Iter: 602 loss: 1.07395272e-06
Iter: 603 loss: 1.07297012e-06
Iter: 604 loss: 1.07101243e-06
Iter: 605 loss: 1.10794826e-06
Iter: 606 loss: 1.07101391e-06
Iter: 607 loss: 1.06915434e-06
Iter: 608 loss: 1.08391691e-06
Iter: 609 loss: 1.0689663e-06
Iter: 610 loss: 1.06770904e-06
Iter: 611 loss: 1.06651589e-06
Iter: 612 loss: 1.06620882e-06
Iter: 613 loss: 1.06405878e-06
Iter: 614 loss: 1.08557151e-06
Iter: 615 loss: 1.06397977e-06
Iter: 616 loss: 1.06353991e-06
Iter: 617 loss: 1.06318851e-06
Iter: 618 loss: 1.06269613e-06
Iter: 619 loss: 1.06162247e-06
Iter: 620 loss: 1.08025358e-06
Iter: 621 loss: 1.06156835e-06
Iter: 622 loss: 1.0601201e-06
Iter: 623 loss: 1.0623038e-06
Iter: 624 loss: 1.05942649e-06
Iter: 625 loss: 1.05797744e-06
Iter: 626 loss: 1.06335858e-06
Iter: 627 loss: 1.05761512e-06
Iter: 628 loss: 1.0564288e-06
Iter: 629 loss: 1.05701235e-06
Iter: 630 loss: 1.05568529e-06
Iter: 631 loss: 1.05417416e-06
Iter: 632 loss: 1.06560788e-06
Iter: 633 loss: 1.05406116e-06
Iter: 634 loss: 1.05291076e-06
Iter: 635 loss: 1.05307527e-06
Iter: 636 loss: 1.05202139e-06
Iter: 637 loss: 1.05079437e-06
Iter: 638 loss: 1.06537107e-06
Iter: 639 loss: 1.05078323e-06
Iter: 640 loss: 1.04965102e-06
Iter: 641 loss: 1.04929404e-06
Iter: 642 loss: 1.04865717e-06
Iter: 643 loss: 1.04729077e-06
Iter: 644 loss: 1.04887681e-06
Iter: 645 loss: 1.04656488e-06
Iter: 646 loss: 1.04498019e-06
Iter: 647 loss: 1.04707965e-06
Iter: 648 loss: 1.04405319e-06
Iter: 649 loss: 1.04264086e-06
Iter: 650 loss: 1.05535605e-06
Iter: 651 loss: 1.04259789e-06
Iter: 652 loss: 1.04122807e-06
Iter: 653 loss: 1.0550234e-06
Iter: 654 loss: 1.04120068e-06
Iter: 655 loss: 1.04041828e-06
Iter: 656 loss: 1.03956791e-06
Iter: 657 loss: 1.03946434e-06
Iter: 658 loss: 1.03836862e-06
Iter: 659 loss: 1.04329047e-06
Iter: 660 loss: 1.03811556e-06
Iter: 661 loss: 1.03713091e-06
Iter: 662 loss: 1.03831076e-06
Iter: 663 loss: 1.03656907e-06
Iter: 664 loss: 1.0355252e-06
Iter: 665 loss: 1.03557545e-06
Iter: 666 loss: 1.03469233e-06
Iter: 667 loss: 1.03266302e-06
Iter: 668 loss: 1.03755076e-06
Iter: 669 loss: 1.03194657e-06
Iter: 670 loss: 1.03049081e-06
Iter: 671 loss: 1.0463699e-06
Iter: 672 loss: 1.03046602e-06
Iter: 673 loss: 1.0296053e-06
Iter: 674 loss: 1.02976651e-06
Iter: 675 loss: 1.02890988e-06
Iter: 676 loss: 1.02724425e-06
Iter: 677 loss: 1.03112313e-06
Iter: 678 loss: 1.02661818e-06
Iter: 679 loss: 1.02566332e-06
Iter: 680 loss: 1.02619856e-06
Iter: 681 loss: 1.0250119e-06
Iter: 682 loss: 1.02342744e-06
Iter: 683 loss: 1.02178967e-06
Iter: 684 loss: 1.02149352e-06
Iter: 685 loss: 1.02110312e-06
Iter: 686 loss: 1.02059403e-06
Iter: 687 loss: 1.01961348e-06
Iter: 688 loss: 1.02158674e-06
Iter: 689 loss: 1.01921466e-06
Iter: 690 loss: 1.01851083e-06
Iter: 691 loss: 1.01812248e-06
Iter: 692 loss: 1.01783871e-06
Iter: 693 loss: 1.01676733e-06
Iter: 694 loss: 1.02040417e-06
Iter: 695 loss: 1.01646719e-06
Iter: 696 loss: 1.01538285e-06
Iter: 697 loss: 1.01679097e-06
Iter: 698 loss: 1.01476962e-06
Iter: 699 loss: 1.01370631e-06
Iter: 700 loss: 1.01604985e-06
Iter: 701 loss: 1.01326827e-06
Iter: 702 loss: 1.01199066e-06
Iter: 703 loss: 1.01431e-06
Iter: 704 loss: 1.01139381e-06
Iter: 705 loss: 1.00983357e-06
Iter: 706 loss: 1.01657372e-06
Iter: 707 loss: 1.00947523e-06
Iter: 708 loss: 1.00832244e-06
Iter: 709 loss: 1.01304829e-06
Iter: 710 loss: 1.00810621e-06
Iter: 711 loss: 1.00711657e-06
Iter: 712 loss: 1.01233297e-06
Iter: 713 loss: 1.00691159e-06
Iter: 714 loss: 1.00610339e-06
Iter: 715 loss: 1.00513796e-06
Iter: 716 loss: 1.00502473e-06
Iter: 717 loss: 1.00350485e-06
Iter: 718 loss: 1.00474188e-06
Iter: 719 loss: 1.00253931e-06
Iter: 720 loss: 1.00103136e-06
Iter: 721 loss: 1.02280444e-06
Iter: 722 loss: 1.00103432e-06
Iter: 723 loss: 1.00005104e-06
Iter: 724 loss: 1.00005161e-06
Iter: 725 loss: 9.99652229e-07
Iter: 726 loss: 9.98604492e-07
Iter: 727 loss: 1.00524812e-06
Iter: 728 loss: 9.9831891e-07
Iter: 729 loss: 9.97117354e-07
Iter: 730 loss: 1.00572242e-06
Iter: 731 loss: 9.96984681e-07
Iter: 732 loss: 9.96027325e-07
Iter: 733 loss: 1.0020583e-06
Iter: 734 loss: 9.95846449e-07
Iter: 735 loss: 9.95064852e-07
Iter: 736 loss: 9.93863523e-07
Iter: 737 loss: 9.93840558e-07
Iter: 738 loss: 9.9254e-07
Iter: 739 loss: 1.00677573e-06
Iter: 740 loss: 9.92536343e-07
Iter: 741 loss: 9.91444495e-07
Iter: 742 loss: 9.92467676e-07
Iter: 743 loss: 9.90787385e-07
Iter: 744 loss: 9.89893579e-07
Iter: 745 loss: 1.00299417e-06
Iter: 746 loss: 9.89896193e-07
Iter: 747 loss: 9.89378918e-07
Iter: 748 loss: 9.89797286e-07
Iter: 749 loss: 9.89060482e-07
Iter: 750 loss: 9.88051397e-07
Iter: 751 loss: 9.86086661e-07
Iter: 752 loss: 1.02772526e-06
Iter: 753 loss: 9.8608848e-07
Iter: 754 loss: 9.84190365e-07
Iter: 755 loss: 9.99396207e-07
Iter: 756 loss: 9.84078156e-07
Iter: 757 loss: 9.82987217e-07
Iter: 758 loss: 9.92278274e-07
Iter: 759 loss: 9.8290036e-07
Iter: 760 loss: 9.81946528e-07
Iter: 761 loss: 9.90124818e-07
Iter: 762 loss: 9.81903895e-07
Iter: 763 loss: 9.81522589e-07
Iter: 764 loss: 9.80479854e-07
Iter: 765 loss: 9.85855081e-07
Iter: 766 loss: 9.80161076e-07
Iter: 767 loss: 9.78585831e-07
Iter: 768 loss: 9.87139856e-07
Iter: 769 loss: 9.78336629e-07
Iter: 770 loss: 9.77212e-07
Iter: 771 loss: 9.88053785e-07
Iter: 772 loss: 9.77225909e-07
Iter: 773 loss: 9.76321417e-07
Iter: 774 loss: 9.75418288e-07
Iter: 775 loss: 9.75200237e-07
Iter: 776 loss: 9.74172e-07
Iter: 777 loss: 9.81425728e-07
Iter: 778 loss: 9.74065415e-07
Iter: 779 loss: 9.73057e-07
Iter: 780 loss: 9.75733428e-07
Iter: 781 loss: 9.72703674e-07
Iter: 782 loss: 9.71798841e-07
Iter: 783 loss: 9.76345746e-07
Iter: 784 loss: 9.71597842e-07
Iter: 785 loss: 9.70675501e-07
Iter: 786 loss: 9.71179702e-07
Iter: 787 loss: 9.70089104e-07
Iter: 788 loss: 9.68800123e-07
Iter: 789 loss: 9.7358668e-07
Iter: 790 loss: 9.68524e-07
Iter: 791 loss: 9.67770234e-07
Iter: 792 loss: 9.68671429e-07
Iter: 793 loss: 9.67357664e-07
Iter: 794 loss: 9.66739321e-07
Iter: 795 loss: 9.66761e-07
Iter: 796 loss: 9.66062089e-07
Iter: 797 loss: 9.65333356e-07
Iter: 798 loss: 9.65177378e-07
Iter: 799 loss: 9.64350306e-07
Iter: 800 loss: 9.63729576e-07
Iter: 801 loss: 9.63454454e-07
Iter: 802 loss: 9.61962428e-07
Iter: 803 loss: 9.6591566e-07
Iter: 804 loss: 9.61439582e-07
Iter: 805 loss: 9.60087846e-07
Iter: 806 loss: 9.76364618e-07
Iter: 807 loss: 9.60027137e-07
Iter: 808 loss: 9.59294539e-07
Iter: 809 loss: 9.59544195e-07
Iter: 810 loss: 9.58772716e-07
Iter: 811 loss: 9.5778887e-07
Iter: 812 loss: 9.58527835e-07
Iter: 813 loss: 9.57167572e-07
Iter: 814 loss: 9.56175427e-07
Iter: 815 loss: 9.70472684e-07
Iter: 816 loss: 9.56205099e-07
Iter: 817 loss: 9.55456e-07
Iter: 818 loss: 9.55749442e-07
Iter: 819 loss: 9.54946131e-07
Iter: 820 loss: 9.53925564e-07
Iter: 821 loss: 9.5861742e-07
Iter: 822 loss: 9.53783456e-07
Iter: 823 loss: 9.53028291e-07
Iter: 824 loss: 9.54321536e-07
Iter: 825 loss: 9.52748792e-07
Iter: 826 loss: 9.51911431e-07
Iter: 827 loss: 9.51543313e-07
Iter: 828 loss: 9.51167522e-07
Iter: 829 loss: 9.50883759e-07
Iter: 830 loss: 9.50509616e-07
Iter: 831 loss: 9.49992341e-07
Iter: 832 loss: 9.49495757e-07
Iter: 833 loss: 9.49391733e-07
Iter: 834 loss: 9.48704098e-07
Iter: 835 loss: 9.46843556e-07
Iter: 836 loss: 9.59018507e-07
Iter: 837 loss: 9.46423256e-07
Iter: 838 loss: 9.46211e-07
Iter: 839 loss: 9.45571514e-07
Iter: 840 loss: 9.44876547e-07
Iter: 841 loss: 9.4545328e-07
Iter: 842 loss: 9.44477392e-07
Iter: 843 loss: 9.43412488e-07
Iter: 844 loss: 9.43901114e-07
Iter: 845 loss: 9.42711836e-07
Iter: 846 loss: 9.41465e-07
Iter: 847 loss: 9.42471672e-07
Iter: 848 loss: 9.40744656e-07
Iter: 849 loss: 9.39845904e-07
Iter: 850 loss: 9.39808842e-07
Iter: 851 loss: 9.39204199e-07
Iter: 852 loss: 9.41525343e-07
Iter: 853 loss: 9.38975859e-07
Iter: 854 loss: 9.38263781e-07
Iter: 855 loss: 9.38092228e-07
Iter: 856 loss: 9.37574896e-07
Iter: 857 loss: 9.3653432e-07
Iter: 858 loss: 9.42004476e-07
Iter: 859 loss: 9.36353e-07
Iter: 860 loss: 9.35564231e-07
Iter: 861 loss: 9.35028197e-07
Iter: 862 loss: 9.34733123e-07
Iter: 863 loss: 9.34211073e-07
Iter: 864 loss: 9.34058221e-07
Iter: 865 loss: 9.33456477e-07
Iter: 866 loss: 9.33768433e-07
Iter: 867 loss: 9.33034073e-07
Iter: 868 loss: 9.32418402e-07
Iter: 869 loss: 9.31246177e-07
Iter: 870 loss: 9.52927451e-07
Iter: 871 loss: 9.3122037e-07
Iter: 872 loss: 9.29961118e-07
Iter: 873 loss: 9.35605158e-07
Iter: 874 loss: 9.29715213e-07
Iter: 875 loss: 9.2870107e-07
Iter: 876 loss: 9.32773617e-07
Iter: 877 loss: 9.28497172e-07
Iter: 878 loss: 9.27456256e-07
Iter: 879 loss: 9.34409172e-07
Iter: 880 loss: 9.27347912e-07
Iter: 881 loss: 9.26655503e-07
Iter: 882 loss: 9.27406063e-07
Iter: 883 loss: 9.26286532e-07
Iter: 884 loss: 9.25531822e-07
Iter: 885 loss: 9.25670633e-07
Iter: 886 loss: 9.24982601e-07
Iter: 887 loss: 9.24086351e-07
Iter: 888 loss: 9.35194521e-07
Iter: 889 loss: 9.24053e-07
Iter: 890 loss: 9.23389e-07
Iter: 891 loss: 9.25462132e-07
Iter: 892 loss: 9.23227844e-07
Iter: 893 loss: 9.22593927e-07
Iter: 894 loss: 9.2308062e-07
Iter: 895 loss: 9.22213e-07
Iter: 896 loss: 9.21304547e-07
Iter: 897 loss: 9.22134404e-07
Iter: 898 loss: 9.20759419e-07
Iter: 899 loss: 9.19937236e-07
Iter: 900 loss: 9.2945254e-07
Iter: 901 loss: 9.19917738e-07
Iter: 902 loss: 9.19049967e-07
Iter: 903 loss: 9.21144363e-07
Iter: 904 loss: 9.1874756e-07
Iter: 905 loss: 9.18211811e-07
Iter: 906 loss: 9.17568e-07
Iter: 907 loss: 9.17450052e-07
Iter: 908 loss: 9.16558e-07
Iter: 909 loss: 9.16548515e-07
Iter: 910 loss: 9.15857299e-07
Iter: 911 loss: 9.14607313e-07
Iter: 912 loss: 9.22222966e-07
Iter: 913 loss: 9.1445e-07
Iter: 914 loss: 9.13411839e-07
Iter: 915 loss: 9.1556609e-07
Iter: 916 loss: 9.12947e-07
Iter: 917 loss: 9.11788788e-07
Iter: 918 loss: 9.21969615e-07
Iter: 919 loss: 9.11708185e-07
Iter: 920 loss: 9.1111076e-07
Iter: 921 loss: 9.10478832e-07
Iter: 922 loss: 9.10420567e-07
Iter: 923 loss: 9.09451273e-07
Iter: 924 loss: 9.16924137e-07
Iter: 925 loss: 9.09370556e-07
Iter: 926 loss: 9.08853394e-07
Iter: 927 loss: 9.15114924e-07
Iter: 928 loss: 9.08849643e-07
Iter: 929 loss: 9.08392735e-07
Iter: 930 loss: 9.07526e-07
Iter: 931 loss: 9.26125608e-07
Iter: 932 loss: 9.07545882e-07
Iter: 933 loss: 9.06583864e-07
Iter: 934 loss: 9.18086357e-07
Iter: 935 loss: 9.0657943e-07
Iter: 936 loss: 9.06008495e-07
Iter: 937 loss: 9.07474885e-07
Iter: 938 loss: 9.05827676e-07
Iter: 939 loss: 9.05083425e-07
Iter: 940 loss: 9.07471872e-07
Iter: 941 loss: 9.04868443e-07
Iter: 942 loss: 9.04355602e-07
Iter: 943 loss: 9.03810587e-07
Iter: 944 loss: 9.03727653e-07
Iter: 945 loss: 9.02970555e-07
Iter: 946 loss: 9.03747434e-07
Iter: 947 loss: 9.02505576e-07
Iter: 948 loss: 9.01483304e-07
Iter: 949 loss: 9.02316572e-07
Iter: 950 loss: 9.00935675e-07
Iter: 951 loss: 8.99919257e-07
Iter: 952 loss: 9.11051643e-07
Iter: 953 loss: 8.99894189e-07
Iter: 954 loss: 8.9909787e-07
Iter: 955 loss: 9.01695898e-07
Iter: 956 loss: 8.98893518e-07
Iter: 957 loss: 8.98065423e-07
Iter: 958 loss: 8.99098666e-07
Iter: 959 loss: 8.97593452e-07
Iter: 960 loss: 8.96935717e-07
Iter: 961 loss: 8.97405812e-07
Iter: 962 loss: 8.96525933e-07
Iter: 963 loss: 8.95435278e-07
Iter: 964 loss: 9.01752799e-07
Iter: 965 loss: 8.95336029e-07
Iter: 966 loss: 8.94509469e-07
Iter: 967 loss: 8.96854203e-07
Iter: 968 loss: 8.94227469e-07
Iter: 969 loss: 8.93670403e-07
Iter: 970 loss: 8.94842628e-07
Iter: 971 loss: 8.93439619e-07
Iter: 972 loss: 8.92719186e-07
Iter: 973 loss: 8.96838912e-07
Iter: 974 loss: 8.92631931e-07
Iter: 975 loss: 8.91952595e-07
Iter: 976 loss: 8.95450967e-07
Iter: 977 loss: 8.9180827e-07
Iter: 978 loss: 8.9143623e-07
Iter: 979 loss: 8.90444539e-07
Iter: 980 loss: 8.97908365e-07
Iter: 981 loss: 8.90203523e-07
Iter: 982 loss: 8.89092291e-07
Iter: 983 loss: 9.00409304e-07
Iter: 984 loss: 8.89071259e-07
Iter: 985 loss: 8.88319221e-07
Iter: 986 loss: 8.88256693e-07
Iter: 987 loss: 8.87746296e-07
Iter: 988 loss: 8.86893758e-07
Iter: 989 loss: 8.94321886e-07
Iter: 990 loss: 8.86830946e-07
Iter: 991 loss: 8.8616531e-07
Iter: 992 loss: 8.89634464e-07
Iter: 993 loss: 8.86003e-07
Iter: 994 loss: 8.85236432e-07
Iter: 995 loss: 8.85319821e-07
Iter: 996 loss: 8.84702672e-07
Iter: 997 loss: 8.83718315e-07
Iter: 998 loss: 8.84605e-07
Iter: 999 loss: 8.8312504e-07
Iter: 1000 loss: 8.8234475e-07
Iter: 1001 loss: 8.82333552e-07
Iter: 1002 loss: 8.81720666e-07
Iter: 1003 loss: 8.83665166e-07
Iter: 1004 loss: 8.81494771e-07
Iter: 1005 loss: 8.81040705e-07
Iter: 1006 loss: 8.80565551e-07
Iter: 1007 loss: 8.8041287e-07
Iter: 1008 loss: 8.79821641e-07
Iter: 1009 loss: 8.79773722e-07
Iter: 1010 loss: 8.79289132e-07
Iter: 1011 loss: 8.80311291e-07
Iter: 1012 loss: 8.79096604e-07
Iter: 1013 loss: 8.78687842e-07
Iter: 1014 loss: 8.7804483e-07
Iter: 1015 loss: 8.78032324e-07
Iter: 1016 loss: 8.77127491e-07
Iter: 1017 loss: 8.77483615e-07
Iter: 1018 loss: 8.7649687e-07
Iter: 1019 loss: 8.75476e-07
Iter: 1020 loss: 8.82067752e-07
Iter: 1021 loss: 8.75350622e-07
Iter: 1022 loss: 8.74536624e-07
Iter: 1023 loss: 8.75670764e-07
Iter: 1024 loss: 8.74157081e-07
Iter: 1025 loss: 8.73262593e-07
Iter: 1026 loss: 8.78424032e-07
Iter: 1027 loss: 8.73109457e-07
Iter: 1028 loss: 8.72349915e-07
Iter: 1029 loss: 8.76735896e-07
Iter: 1030 loss: 8.72271357e-07
Iter: 1031 loss: 8.71595375e-07
Iter: 1032 loss: 8.71397447e-07
Iter: 1033 loss: 8.71034217e-07
Iter: 1034 loss: 8.70421331e-07
Iter: 1035 loss: 8.74783609e-07
Iter: 1036 loss: 8.70383246e-07
Iter: 1037 loss: 8.69654968e-07
Iter: 1038 loss: 8.70863573e-07
Iter: 1039 loss: 8.69337327e-07
Iter: 1040 loss: 8.68571078e-07
Iter: 1041 loss: 8.70260692e-07
Iter: 1042 loss: 8.68266909e-07
Iter: 1043 loss: 8.67939036e-07
Iter: 1044 loss: 8.67958249e-07
Iter: 1045 loss: 8.67574272e-07
Iter: 1046 loss: 8.66825758e-07
Iter: 1047 loss: 8.81243068e-07
Iter: 1048 loss: 8.66782671e-07
Iter: 1049 loss: 8.65928e-07
Iter: 1050 loss: 8.70291444e-07
Iter: 1051 loss: 8.65838274e-07
Iter: 1052 loss: 8.65274615e-07
Iter: 1053 loss: 8.64616482e-07
Iter: 1054 loss: 8.6454e-07
Iter: 1055 loss: 8.63713694e-07
Iter: 1056 loss: 8.68736038e-07
Iter: 1057 loss: 8.63634114e-07
Iter: 1058 loss: 8.6279249e-07
Iter: 1059 loss: 8.62635318e-07
Iter: 1060 loss: 8.62087745e-07
Iter: 1061 loss: 8.61221793e-07
Iter: 1062 loss: 8.72409544e-07
Iter: 1063 loss: 8.61193769e-07
Iter: 1064 loss: 8.60586738e-07
Iter: 1065 loss: 8.61840476e-07
Iter: 1066 loss: 8.60338616e-07
Iter: 1067 loss: 8.59456861e-07
Iter: 1068 loss: 8.61301601e-07
Iter: 1069 loss: 8.59082661e-07
Iter: 1070 loss: 8.58392241e-07
Iter: 1071 loss: 8.58936403e-07
Iter: 1072 loss: 8.57979899e-07
Iter: 1073 loss: 8.57285e-07
Iter: 1074 loss: 8.64072604e-07
Iter: 1075 loss: 8.57274642e-07
Iter: 1076 loss: 8.56696715e-07
Iter: 1077 loss: 8.58718522e-07
Iter: 1078 loss: 8.56511e-07
Iter: 1079 loss: 8.56082352e-07
Iter: 1080 loss: 8.57442046e-07
Iter: 1081 loss: 8.55935241e-07
Iter: 1082 loss: 8.55432859e-07
Iter: 1083 loss: 8.58673275e-07
Iter: 1084 loss: 8.55367603e-07
Iter: 1085 loss: 8.55027338e-07
Iter: 1086 loss: 8.54377618e-07
Iter: 1087 loss: 8.69920598e-07
Iter: 1088 loss: 8.54395466e-07
Iter: 1089 loss: 8.53747736e-07
Iter: 1090 loss: 8.58255476e-07
Iter: 1091 loss: 8.53654342e-07
Iter: 1092 loss: 8.53148606e-07
Iter: 1093 loss: 8.52229846e-07
Iter: 1094 loss: 8.5221609e-07
Iter: 1095 loss: 8.51225252e-07
Iter: 1096 loss: 8.58102567e-07
Iter: 1097 loss: 8.51148343e-07
Iter: 1098 loss: 8.5039e-07
Iter: 1099 loss: 8.50879928e-07
Iter: 1100 loss: 8.49853564e-07
Iter: 1101 loss: 8.48933e-07
Iter: 1102 loss: 8.59969532e-07
Iter: 1103 loss: 8.48956347e-07
Iter: 1104 loss: 8.48412583e-07
Iter: 1105 loss: 8.50755384e-07
Iter: 1106 loss: 8.48330671e-07
Iter: 1107 loss: 8.4779839e-07
Iter: 1108 loss: 8.47231888e-07
Iter: 1109 loss: 8.471294e-07
Iter: 1110 loss: 8.4638657e-07
Iter: 1111 loss: 8.54515e-07
Iter: 1112 loss: 8.46360479e-07
Iter: 1113 loss: 8.45805687e-07
Iter: 1114 loss: 8.48724824e-07
Iter: 1115 loss: 8.45692625e-07
Iter: 1116 loss: 8.4516887e-07
Iter: 1117 loss: 8.4642727e-07
Iter: 1118 loss: 8.44992371e-07
Iter: 1119 loss: 8.44450426e-07
Iter: 1120 loss: 8.48331183e-07
Iter: 1121 loss: 8.44407623e-07
Iter: 1122 loss: 8.44105841e-07
Iter: 1123 loss: 8.43428722e-07
Iter: 1124 loss: 8.52023618e-07
Iter: 1125 loss: 8.43405815e-07
Iter: 1126 loss: 8.42675377e-07
Iter: 1127 loss: 8.4918463e-07
Iter: 1128 loss: 8.42633483e-07
Iter: 1129 loss: 8.42041914e-07
Iter: 1130 loss: 8.41892586e-07
Iter: 1131 loss: 8.41559654e-07
Iter: 1132 loss: 8.40748612e-07
Iter: 1133 loss: 8.43601299e-07
Iter: 1134 loss: 8.40532607e-07
Iter: 1135 loss: 8.39845086e-07
Iter: 1136 loss: 8.39635391e-07
Iter: 1137 loss: 8.39246354e-07
Iter: 1138 loss: 8.38446226e-07
Iter: 1139 loss: 8.38444635e-07
Iter: 1140 loss: 8.37889161e-07
Iter: 1141 loss: 8.40203938e-07
Iter: 1142 loss: 8.3780526e-07
Iter: 1143 loss: 8.37243078e-07
Iter: 1144 loss: 8.36913159e-07
Iter: 1145 loss: 8.36702725e-07
Iter: 1146 loss: 8.35907144e-07
Iter: 1147 loss: 8.38955259e-07
Iter: 1148 loss: 8.35763103e-07
Iter: 1149 loss: 8.3522383e-07
Iter: 1150 loss: 8.35222522e-07
Iter: 1151 loss: 8.34852244e-07
Iter: 1152 loss: 8.35679145e-07
Iter: 1153 loss: 8.34718776e-07
Iter: 1154 loss: 8.34244872e-07
Iter: 1155 loss: 8.3507507e-07
Iter: 1156 loss: 8.34007039e-07
Iter: 1157 loss: 8.33537797e-07
Iter: 1158 loss: 8.33062927e-07
Iter: 1159 loss: 8.32945148e-07
Iter: 1160 loss: 8.32217438e-07
Iter: 1161 loss: 8.33127672e-07
Iter: 1162 loss: 8.31861655e-07
Iter: 1163 loss: 8.30988142e-07
Iter: 1164 loss: 8.38678204e-07
Iter: 1165 loss: 8.30969213e-07
Iter: 1166 loss: 8.30501563e-07
Iter: 1167 loss: 8.29892258e-07
Iter: 1168 loss: 8.29858209e-07
Iter: 1169 loss: 8.28935299e-07
Iter: 1170 loss: 8.33655236e-07
Iter: 1171 loss: 8.28738848e-07
Iter: 1172 loss: 8.28071336e-07
Iter: 1173 loss: 8.29499129e-07
Iter: 1174 loss: 8.2782509e-07
Iter: 1175 loss: 8.26953283e-07
Iter: 1176 loss: 8.32878186e-07
Iter: 1177 loss: 8.2690525e-07
Iter: 1178 loss: 8.26441124e-07
Iter: 1179 loss: 8.27380632e-07
Iter: 1180 loss: 8.26197379e-07
Iter: 1181 loss: 8.25741211e-07
Iter: 1182 loss: 8.26199539e-07
Iter: 1183 loss: 8.25439201e-07
Iter: 1184 loss: 8.249865e-07
Iter: 1185 loss: 8.24967458e-07
Iter: 1186 loss: 8.24628273e-07
Iter: 1187 loss: 8.25519066e-07
Iter: 1188 loss: 8.24503957e-07
Iter: 1189 loss: 8.24041308e-07
Iter: 1190 loss: 8.24435176e-07
Iter: 1191 loss: 8.23751634e-07
Iter: 1192 loss: 8.23363962e-07
Iter: 1193 loss: 8.23087703e-07
Iter: 1194 loss: 8.22928087e-07
Iter: 1195 loss: 8.22248523e-07
Iter: 1196 loss: 8.22575146e-07
Iter: 1197 loss: 8.21782578e-07
Iter: 1198 loss: 8.20895821e-07
Iter: 1199 loss: 8.2857639e-07
Iter: 1200 loss: 8.20860407e-07
Iter: 1201 loss: 8.20232913e-07
Iter: 1202 loss: 8.20954313e-07
Iter: 1203 loss: 8.19867182e-07
Iter: 1204 loss: 8.19319268e-07
Iter: 1205 loss: 8.1986e-07
Iter: 1206 loss: 8.18951776e-07
Iter: 1207 loss: 8.1817393e-07
Iter: 1208 loss: 8.19178581e-07
Iter: 1209 loss: 8.17789328e-07
Iter: 1210 loss: 8.16992042e-07
Iter: 1211 loss: 8.26021505e-07
Iter: 1212 loss: 8.16984141e-07
Iter: 1213 loss: 8.1637711e-07
Iter: 1214 loss: 8.18538865e-07
Iter: 1215 loss: 8.1620226e-07
Iter: 1216 loss: 8.15729436e-07
Iter: 1217 loss: 8.15686235e-07
Iter: 1218 loss: 8.15231033e-07
Iter: 1219 loss: 8.14637701e-07
Iter: 1220 loss: 8.21607728e-07
Iter: 1221 loss: 8.14587338e-07
Iter: 1222 loss: 8.14062673e-07
Iter: 1223 loss: 8.17092939e-07
Iter: 1224 loss: 8.13979739e-07
Iter: 1225 loss: 8.13607471e-07
Iter: 1226 loss: 8.14955683e-07
Iter: 1227 loss: 8.13503e-07
Iter: 1228 loss: 8.13134704e-07
Iter: 1229 loss: 8.12497547e-07
Iter: 1230 loss: 8.25655491e-07
Iter: 1231 loss: 8.12503345e-07
Iter: 1232 loss: 8.11718905e-07
Iter: 1233 loss: 8.14447105e-07
Iter: 1234 loss: 8.11476752e-07
Iter: 1235 loss: 8.10842778e-07
Iter: 1236 loss: 8.1188665e-07
Iter: 1237 loss: 8.10522408e-07
Iter: 1238 loss: 8.09831363e-07
Iter: 1239 loss: 8.157308e-07
Iter: 1240 loss: 8.09817607e-07
Iter: 1241 loss: 8.09355925e-07
Iter: 1242 loss: 8.09374114e-07
Iter: 1243 loss: 8.09078074e-07
Iter: 1244 loss: 8.08335e-07
Iter: 1245 loss: 8.08933e-07
Iter: 1246 loss: 8.07890387e-07
Iter: 1247 loss: 8.07296772e-07
Iter: 1248 loss: 8.11403879e-07
Iter: 1249 loss: 8.07220886e-07
Iter: 1250 loss: 8.06543767e-07
Iter: 1251 loss: 8.08372306e-07
Iter: 1252 loss: 8.06300818e-07
Iter: 1253 loss: 8.05724483e-07
Iter: 1254 loss: 8.06674336e-07
Iter: 1255 loss: 8.05462946e-07
Iter: 1256 loss: 8.04962099e-07
Iter: 1257 loss: 8.11520465e-07
Iter: 1258 loss: 8.04947e-07
Iter: 1259 loss: 8.04507351e-07
Iter: 1260 loss: 8.05708964e-07
Iter: 1261 loss: 8.04407e-07
Iter: 1262 loss: 8.04010142e-07
Iter: 1263 loss: 8.04683737e-07
Iter: 1264 loss: 8.0385712e-07
Iter: 1265 loss: 8.03400269e-07
Iter: 1266 loss: 8.02915167e-07
Iter: 1267 loss: 8.02807449e-07
Iter: 1268 loss: 8.02217698e-07
Iter: 1269 loss: 8.03423291e-07
Iter: 1270 loss: 8.01983219e-07
Iter: 1271 loss: 8.01280237e-07
Iter: 1272 loss: 8.03210241e-07
Iter: 1273 loss: 8.01033252e-07
Iter: 1274 loss: 8.00482894e-07
Iter: 1275 loss: 8.04053855e-07
Iter: 1276 loss: 8.00441285e-07
Iter: 1277 loss: 7.99889676e-07
Iter: 1278 loss: 8.00854764e-07
Iter: 1279 loss: 7.99624161e-07
Iter: 1280 loss: 7.99103532e-07
Iter: 1281 loss: 7.98702274e-07
Iter: 1282 loss: 7.98504743e-07
Iter: 1283 loss: 7.97609289e-07
Iter: 1284 loss: 8.0215375e-07
Iter: 1285 loss: 7.97467578e-07
Iter: 1286 loss: 7.9681331e-07
Iter: 1287 loss: 8.01009321e-07
Iter: 1288 loss: 7.96748054e-07
Iter: 1289 loss: 7.96193149e-07
Iter: 1290 loss: 7.99827e-07
Iter: 1291 loss: 7.96140512e-07
Iter: 1292 loss: 7.95761309e-07
Iter: 1293 loss: 7.96006134e-07
Iter: 1294 loss: 7.9560823e-07
Iter: 1295 loss: 7.95145695e-07
Iter: 1296 loss: 8.00713963e-07
Iter: 1297 loss: 7.95151664e-07
Iter: 1298 loss: 7.94746938e-07
Iter: 1299 loss: 7.94254447e-07
Iter: 1300 loss: 7.94199764e-07
Iter: 1301 loss: 7.93510935e-07
Iter: 1302 loss: 7.98085807e-07
Iter: 1303 loss: 7.93462107e-07
Iter: 1304 loss: 7.93051754e-07
Iter: 1305 loss: 7.9259064e-07
Iter: 1306 loss: 7.92518222e-07
Iter: 1307 loss: 7.91845764e-07
Iter: 1308 loss: 7.94605171e-07
Iter: 1309 loss: 7.91739808e-07
Iter: 1310 loss: 7.91152956e-07
Iter: 1311 loss: 7.92137257e-07
Iter: 1312 loss: 7.90897843e-07
Iter: 1313 loss: 7.90253353e-07
Iter: 1314 loss: 7.93460345e-07
Iter: 1315 loss: 7.90162858e-07
Iter: 1316 loss: 7.8960079e-07
Iter: 1317 loss: 7.91113337e-07
Iter: 1318 loss: 7.8941423e-07
Iter: 1319 loss: 7.88813736e-07
Iter: 1320 loss: 7.89202659e-07
Iter: 1321 loss: 7.88438342e-07
Iter: 1322 loss: 7.87834097e-07
Iter: 1323 loss: 7.88959937e-07
Iter: 1324 loss: 7.87520321e-07
Iter: 1325 loss: 7.87077738e-07
Iter: 1326 loss: 7.870334e-07
Iter: 1327 loss: 7.86535452e-07
Iter: 1328 loss: 7.86462408e-07
Iter: 1329 loss: 7.86150736e-07
Iter: 1330 loss: 7.85825591e-07
Iter: 1331 loss: 7.85780571e-07
Iter: 1332 loss: 7.85470434e-07
Iter: 1333 loss: 7.85113116e-07
Iter: 1334 loss: 7.85107488e-07
Iter: 1335 loss: 7.84641713e-07
Iter: 1336 loss: 7.86704561e-07
Iter: 1337 loss: 7.84579e-07
Iter: 1338 loss: 7.8418077e-07
Iter: 1339 loss: 7.840938e-07
Iter: 1340 loss: 7.83850453e-07
Iter: 1341 loss: 7.83306291e-07
Iter: 1342 loss: 7.8284927e-07
Iter: 1343 loss: 7.82708071e-07
Iter: 1344 loss: 7.82105417e-07
Iter: 1345 loss: 7.91235607e-07
Iter: 1346 loss: 7.82062784e-07
Iter: 1347 loss: 7.81585868e-07
Iter: 1348 loss: 7.81446261e-07
Iter: 1349 loss: 7.81145673e-07
Iter: 1350 loss: 7.80403e-07
Iter: 1351 loss: 7.86978148e-07
Iter: 1352 loss: 7.80386245e-07
Iter: 1353 loss: 7.79883521e-07
Iter: 1354 loss: 7.80534037e-07
Iter: 1355 loss: 7.79637503e-07
Iter: 1356 loss: 7.7902024e-07
Iter: 1357 loss: 7.79048833e-07
Iter: 1358 loss: 7.78542e-07
Iter: 1359 loss: 7.77796515e-07
Iter: 1360 loss: 7.81716608e-07
Iter: 1361 loss: 7.77633829e-07
Iter: 1362 loss: 7.77095806e-07
Iter: 1363 loss: 7.84275699e-07
Iter: 1364 loss: 7.77086314e-07
Iter: 1365 loss: 7.76729507e-07
Iter: 1366 loss: 7.78452886e-07
Iter: 1367 loss: 7.76662603e-07
Iter: 1368 loss: 7.76342404e-07
Iter: 1369 loss: 7.77790092e-07
Iter: 1370 loss: 7.76276693e-07
Iter: 1371 loss: 7.76052843e-07
Iter: 1372 loss: 7.75715307e-07
Iter: 1373 loss: 7.75680405e-07
Iter: 1374 loss: 7.75216e-07
Iter: 1375 loss: 7.78651781e-07
Iter: 1376 loss: 7.75190301e-07
Iter: 1377 loss: 7.74838554e-07
Iter: 1378 loss: 7.74288651e-07
Iter: 1379 loss: 7.74286775e-07
Iter: 1380 loss: 7.73571855e-07
Iter: 1381 loss: 7.74871921e-07
Iter: 1382 loss: 7.73272291e-07
Iter: 1383 loss: 7.72462897e-07
Iter: 1384 loss: 7.75460649e-07
Iter: 1385 loss: 7.72274859e-07
Iter: 1386 loss: 7.71689884e-07
Iter: 1387 loss: 7.77772811e-07
Iter: 1388 loss: 7.71672831e-07
Iter: 1389 loss: 7.71238206e-07
Iter: 1390 loss: 7.71525322e-07
Iter: 1391 loss: 7.70968654e-07
Iter: 1392 loss: 7.70383963e-07
Iter: 1393 loss: 7.71820623e-07
Iter: 1394 loss: 7.70226848e-07
Iter: 1395 loss: 7.69655628e-07
Iter: 1396 loss: 7.70572342e-07
Iter: 1397 loss: 7.69424275e-07
Iter: 1398 loss: 7.68921382e-07
Iter: 1399 loss: 7.71586201e-07
Iter: 1400 loss: 7.6884146e-07
Iter: 1401 loss: 7.68294399e-07
Iter: 1402 loss: 7.71391569e-07
Iter: 1403 loss: 7.68246537e-07
Iter: 1404 loss: 7.67828055e-07
Iter: 1405 loss: 7.71469445e-07
Iter: 1406 loss: 7.67840049e-07
Iter: 1407 loss: 7.67603069e-07
Iter: 1408 loss: 7.67277356e-07
Iter: 1409 loss: 7.67258143e-07
Iter: 1410 loss: 7.66812605e-07
Iter: 1411 loss: 7.67668212e-07
Iter: 1412 loss: 7.66639801e-07
Iter: 1413 loss: 7.66060339e-07
Iter: 1414 loss: 7.68384439e-07
Iter: 1415 loss: 7.65932043e-07
Iter: 1416 loss: 7.65542609e-07
Iter: 1417 loss: 7.64962124e-07
Iter: 1418 loss: 7.64925289e-07
Iter: 1419 loss: 7.64258857e-07
Iter: 1420 loss: 7.68460097e-07
Iter: 1421 loss: 7.64171e-07
Iter: 1422 loss: 7.63617209e-07
Iter: 1423 loss: 7.64917218e-07
Iter: 1424 loss: 7.63452192e-07
Iter: 1425 loss: 7.628459e-07
Iter: 1426 loss: 7.67626886e-07
Iter: 1427 loss: 7.62848686e-07
Iter: 1428 loss: 7.62431114e-07
Iter: 1429 loss: 7.6287472e-07
Iter: 1430 loss: 7.62240745e-07
Iter: 1431 loss: 7.61722845e-07
Iter: 1432 loss: 7.62143145e-07
Iter: 1433 loss: 7.61417084e-07
Iter: 1434 loss: 7.60704665e-07
Iter: 1435 loss: 7.61959029e-07
Iter: 1436 loss: 7.60365424e-07
Iter: 1437 loss: 7.60034254e-07
Iter: 1438 loss: 7.5995e-07
Iter: 1439 loss: 7.59609634e-07
Iter: 1440 loss: 7.6011986e-07
Iter: 1441 loss: 7.59428701e-07
Iter: 1442 loss: 7.58958095e-07
Iter: 1443 loss: 7.59115665e-07
Iter: 1444 loss: 7.58673252e-07
Iter: 1445 loss: 7.58233739e-07
Iter: 1446 loss: 7.59124191e-07
Iter: 1447 loss: 7.58054171e-07
Iter: 1448 loss: 7.57752389e-07
Iter: 1449 loss: 7.59784257e-07
Iter: 1450 loss: 7.57691396e-07
Iter: 1451 loss: 7.57336e-07
Iter: 1452 loss: 7.57250689e-07
Iter: 1453 loss: 7.57013368e-07
Iter: 1454 loss: 7.56500242e-07
Iter: 1455 loss: 7.56236204e-07
Iter: 1456 loss: 7.5600849e-07
Iter: 1457 loss: 7.55338363e-07
Iter: 1458 loss: 7.59327747e-07
Iter: 1459 loss: 7.55242e-07
Iter: 1460 loss: 7.54675398e-07
Iter: 1461 loss: 7.56114503e-07
Iter: 1462 loss: 7.54468374e-07
Iter: 1463 loss: 7.53994073e-07
Iter: 1464 loss: 7.58961278e-07
Iter: 1465 loss: 7.53981681e-07
Iter: 1466 loss: 7.53544498e-07
Iter: 1467 loss: 7.53007157e-07
Iter: 1468 loss: 7.52982146e-07
Iter: 1469 loss: 7.5218793e-07
Iter: 1470 loss: 7.57336124e-07
Iter: 1471 loss: 7.52117217e-07
Iter: 1472 loss: 7.51737957e-07
Iter: 1473 loss: 7.52436847e-07
Iter: 1474 loss: 7.51550829e-07
Iter: 1475 loss: 7.5130896e-07
Iter: 1476 loss: 7.51260586e-07
Iter: 1477 loss: 7.50997742e-07
Iter: 1478 loss: 7.51075959e-07
Iter: 1479 loss: 7.50813456e-07
Iter: 1480 loss: 7.50528841e-07
Iter: 1481 loss: 7.50837216e-07
Iter: 1482 loss: 7.50358822e-07
Iter: 1483 loss: 7.49932042e-07
Iter: 1484 loss: 7.49816195e-07
Iter: 1485 loss: 7.49576316e-07
Iter: 1486 loss: 7.49028e-07
Iter: 1487 loss: 7.52072424e-07
Iter: 1488 loss: 7.48977072e-07
Iter: 1489 loss: 7.48448088e-07
Iter: 1490 loss: 7.49468711e-07
Iter: 1491 loss: 7.48251e-07
Iter: 1492 loss: 7.47820707e-07
Iter: 1493 loss: 7.47717934e-07
Iter: 1494 loss: 7.47416e-07
Iter: 1495 loss: 7.46868579e-07
Iter: 1496 loss: 7.49227809e-07
Iter: 1497 loss: 7.46724481e-07
Iter: 1498 loss: 7.46221872e-07
Iter: 1499 loss: 7.46516889e-07
Iter: 1500 loss: 7.45839941e-07
Iter: 1501 loss: 7.45214095e-07
Iter: 1502 loss: 7.5145806e-07
Iter: 1503 loss: 7.45201e-07
Iter: 1504 loss: 7.44763e-07
Iter: 1505 loss: 7.4584068e-07
Iter: 1506 loss: 7.44559713e-07
Iter: 1507 loss: 7.44056763e-07
Iter: 1508 loss: 7.44171473e-07
Iter: 1509 loss: 7.43674264e-07
Iter: 1510 loss: 7.43208375e-07
Iter: 1511 loss: 7.46041e-07
Iter: 1512 loss: 7.43106057e-07
Iter: 1513 loss: 7.42742543e-07
Iter: 1514 loss: 7.46054411e-07
Iter: 1515 loss: 7.42728218e-07
Iter: 1516 loss: 7.4232878e-07
Iter: 1517 loss: 7.44320289e-07
Iter: 1518 loss: 7.42267105e-07
Iter: 1519 loss: 7.4195583e-07
Iter: 1520 loss: 7.41714871e-07
Iter: 1521 loss: 7.41575832e-07
Iter: 1522 loss: 7.40986138e-07
Iter: 1523 loss: 7.42011878e-07
Iter: 1524 loss: 7.4069203e-07
Iter: 1525 loss: 7.40331586e-07
Iter: 1526 loss: 7.42168652e-07
Iter: 1527 loss: 7.40286907e-07
Iter: 1528 loss: 7.39885763e-07
Iter: 1529 loss: 7.39946756e-07
Iter: 1530 loss: 7.39550728e-07
Iter: 1531 loss: 7.39008726e-07
Iter: 1532 loss: 7.3970773e-07
Iter: 1533 loss: 7.38731728e-07
Iter: 1534 loss: 7.38144706e-07
Iter: 1535 loss: 7.38446886e-07
Iter: 1536 loss: 7.37776077e-07
Iter: 1537 loss: 7.37113737e-07
Iter: 1538 loss: 7.40912697e-07
Iter: 1539 loss: 7.36998231e-07
Iter: 1540 loss: 7.36468678e-07
Iter: 1541 loss: 7.39496841e-07
Iter: 1542 loss: 7.36392792e-07
Iter: 1543 loss: 7.35978801e-07
Iter: 1544 loss: 7.38032554e-07
Iter: 1545 loss: 7.35904e-07
Iter: 1546 loss: 7.3559147e-07
Iter: 1547 loss: 7.35423214e-07
Iter: 1548 loss: 7.35229662e-07
Iter: 1549 loss: 7.34642867e-07
Iter: 1550 loss: 7.35721528e-07
Iter: 1551 loss: 7.34432888e-07
Iter: 1552 loss: 7.34176069e-07
Iter: 1553 loss: 7.34065907e-07
Iter: 1554 loss: 7.33815625e-07
Iter: 1555 loss: 7.33798572e-07
Iter: 1556 loss: 7.33610818e-07
Iter: 1557 loss: 7.33287e-07
Iter: 1558 loss: 7.338042e-07
Iter: 1559 loss: 7.33141633e-07
Iter: 1560 loss: 7.327871e-07
Iter: 1561 loss: 7.33022e-07
Iter: 1562 loss: 7.32548756e-07
Iter: 1563 loss: 7.32189392e-07
Iter: 1564 loss: 7.34083926e-07
Iter: 1565 loss: 7.32109e-07
Iter: 1566 loss: 7.31697583e-07
Iter: 1567 loss: 7.32139597e-07
Iter: 1568 loss: 7.31459238e-07
Iter: 1569 loss: 7.3099136e-07
Iter: 1570 loss: 7.31062187e-07
Iter: 1571 loss: 7.30654961e-07
Iter: 1572 loss: 7.30070781e-07
Iter: 1573 loss: 7.30843908e-07
Iter: 1574 loss: 7.29744897e-07
Iter: 1575 loss: 7.29166e-07
Iter: 1576 loss: 7.33337401e-07
Iter: 1577 loss: 7.2908864e-07
Iter: 1578 loss: 7.28625309e-07
Iter: 1579 loss: 7.29832323e-07
Iter: 1580 loss: 7.28484906e-07
Iter: 1581 loss: 7.27983718e-07
Iter: 1582 loss: 7.30834699e-07
Iter: 1583 loss: 7.27953534e-07
Iter: 1584 loss: 7.27570125e-07
Iter: 1585 loss: 7.27186148e-07
Iter: 1586 loss: 7.27117936e-07
Iter: 1587 loss: 7.27173244e-07
Iter: 1588 loss: 7.26844178e-07
Iter: 1589 loss: 7.26645e-07
Iter: 1590 loss: 7.26609358e-07
Iter: 1591 loss: 7.26486e-07
Iter: 1592 loss: 7.26251699e-07
Iter: 1593 loss: 7.26440703e-07
Iter: 1594 loss: 7.26097142e-07
Iter: 1595 loss: 7.25700602e-07
Iter: 1596 loss: 7.25842142e-07
Iter: 1597 loss: 7.25432699e-07
Iter: 1598 loss: 7.24993129e-07
Iter: 1599 loss: 7.26181668e-07
Iter: 1600 loss: 7.24813049e-07
Iter: 1601 loss: 7.24336871e-07
Iter: 1602 loss: 7.26857e-07
Iter: 1603 loss: 7.24251208e-07
Iter: 1604 loss: 7.23838752e-07
Iter: 1605 loss: 7.23520145e-07
Iter: 1606 loss: 7.23380595e-07
Iter: 1607 loss: 7.22757079e-07
Iter: 1608 loss: 7.25213795e-07
Iter: 1609 loss: 7.22617642e-07
Iter: 1610 loss: 7.22163293e-07
Iter: 1611 loss: 7.22805339e-07
Iter: 1612 loss: 7.21941433e-07
Iter: 1613 loss: 7.2134867e-07
Iter: 1614 loss: 7.22828304e-07
Iter: 1615 loss: 7.21149092e-07
Iter: 1616 loss: 7.20601918e-07
Iter: 1617 loss: 7.23760195e-07
Iter: 1618 loss: 7.20522905e-07
Iter: 1619 loss: 7.20036326e-07
Iter: 1620 loss: 7.22516518e-07
Iter: 1621 loss: 7.19924458e-07
Iter: 1622 loss: 7.1962279e-07
Iter: 1623 loss: 7.21505444e-07
Iter: 1624 loss: 7.19596926e-07
Iter: 1625 loss: 7.19178047e-07
Iter: 1626 loss: 7.19988293e-07
Iter: 1627 loss: 7.19039917e-07
Iter: 1628 loss: 7.18741035e-07
Iter: 1629 loss: 7.18598926e-07
Iter: 1630 loss: 7.18449655e-07
Iter: 1631 loss: 7.17942726e-07
Iter: 1632 loss: 7.19415084e-07
Iter: 1633 loss: 7.17735588e-07
Iter: 1634 loss: 7.17224566e-07
Iter: 1635 loss: 7.18000138e-07
Iter: 1636 loss: 7.1697923e-07
Iter: 1637 loss: 7.16577802e-07
Iter: 1638 loss: 7.20528192e-07
Iter: 1639 loss: 7.16549e-07
Iter: 1640 loss: 7.16178533e-07
Iter: 1641 loss: 7.15868168e-07
Iter: 1642 loss: 7.15784267e-07
Iter: 1643 loss: 7.15276883e-07
Iter: 1644 loss: 7.17541411e-07
Iter: 1645 loss: 7.15169676e-07
Iter: 1646 loss: 7.1469367e-07
Iter: 1647 loss: 7.14488692e-07
Iter: 1648 loss: 7.1427e-07
Iter: 1649 loss: 7.13614e-07
Iter: 1650 loss: 7.1901087e-07
Iter: 1651 loss: 7.1358852e-07
Iter: 1652 loss: 7.13156282e-07
Iter: 1653 loss: 7.13448344e-07
Iter: 1654 loss: 7.12947553e-07
Iter: 1655 loss: 7.1244915e-07
Iter: 1656 loss: 7.16758734e-07
Iter: 1657 loss: 7.12415044e-07
Iter: 1658 loss: 7.12030442e-07
Iter: 1659 loss: 7.13850454e-07
Iter: 1660 loss: 7.1193665e-07
Iter: 1661 loss: 7.1161071e-07
Iter: 1662 loss: 7.15535407e-07
Iter: 1663 loss: 7.11611619e-07
Iter: 1664 loss: 7.11432222e-07
Iter: 1665 loss: 7.1097395e-07
Iter: 1666 loss: 7.15148076e-07
Iter: 1667 loss: 7.10916368e-07
Iter: 1668 loss: 7.10414611e-07
Iter: 1669 loss: 7.13566692e-07
Iter: 1670 loss: 7.10379879e-07
Iter: 1671 loss: 7.09926098e-07
Iter: 1672 loss: 7.12190683e-07
Iter: 1673 loss: 7.09851747e-07
Iter: 1674 loss: 7.09513188e-07
Iter: 1675 loss: 7.09334813e-07
Iter: 1676 loss: 7.09136543e-07
Iter: 1677 loss: 7.08741368e-07
Iter: 1678 loss: 7.1330112e-07
Iter: 1679 loss: 7.08755806e-07
Iter: 1680 loss: 7.08404457e-07
Iter: 1681 loss: 7.08375183e-07
Iter: 1682 loss: 7.08112225e-07
Iter: 1683 loss: 7.07587276e-07
Iter: 1684 loss: 7.08143375e-07
Iter: 1685 loss: 7.07310164e-07
Iter: 1686 loss: 7.06882133e-07
Iter: 1687 loss: 7.08457151e-07
Iter: 1688 loss: 7.06767082e-07
Iter: 1689 loss: 7.06370088e-07
Iter: 1690 loss: 7.07504228e-07
Iter: 1691 loss: 7.06242872e-07
Iter: 1692 loss: 7.05759362e-07
Iter: 1693 loss: 7.05617e-07
Iter: 1694 loss: 7.05324851e-07
Iter: 1695 loss: 7.05019033e-07
Iter: 1696 loss: 7.04954e-07
Iter: 1697 loss: 7.04687295e-07
Iter: 1698 loss: 7.07085348e-07
Iter: 1699 loss: 7.04685704e-07
Iter: 1700 loss: 7.04485672e-07
Iter: 1701 loss: 7.04217712e-07
Iter: 1702 loss: 7.04189461e-07
Iter: 1703 loss: 7.03850787e-07
Iter: 1704 loss: 7.04370109e-07
Iter: 1705 loss: 7.03650812e-07
Iter: 1706 loss: 7.03245746e-07
Iter: 1707 loss: 7.02858301e-07
Iter: 1708 loss: 7.02753e-07
Iter: 1709 loss: 7.02097282e-07
Iter: 1710 loss: 7.07181471e-07
Iter: 1711 loss: 7.02018269e-07
Iter: 1712 loss: 7.01582621e-07
Iter: 1713 loss: 7.05891921e-07
Iter: 1714 loss: 7.01549652e-07
Iter: 1715 loss: 7.01174713e-07
Iter: 1716 loss: 7.01557383e-07
Iter: 1717 loss: 7.0099e-07
Iter: 1718 loss: 7.00598e-07
Iter: 1719 loss: 7.00752651e-07
Iter: 1720 loss: 7.00370379e-07
Iter: 1721 loss: 6.99968382e-07
Iter: 1722 loss: 7.04487e-07
Iter: 1723 loss: 6.99942575e-07
Iter: 1724 loss: 6.99530801e-07
Iter: 1725 loss: 6.99553709e-07
Iter: 1726 loss: 6.99243742e-07
Iter: 1727 loss: 6.98773249e-07
Iter: 1728 loss: 7.00226678e-07
Iter: 1729 loss: 6.98670362e-07
Iter: 1730 loss: 6.98265922e-07
Iter: 1731 loss: 6.98307304e-07
Iter: 1732 loss: 6.97958114e-07
Iter: 1733 loss: 6.98104714e-07
Iter: 1734 loss: 6.97744895e-07
Iter: 1735 loss: 6.97600285e-07
Iter: 1736 loss: 6.97349321e-07
Iter: 1737 loss: 7.02991656e-07
Iter: 1738 loss: 6.97335508e-07
Iter: 1739 loss: 6.9697478e-07
Iter: 1740 loss: 6.96840743e-07
Iter: 1741 loss: 6.9665964e-07
Iter: 1742 loss: 6.96092229e-07
Iter: 1743 loss: 6.99150291e-07
Iter: 1744 loss: 6.96017594e-07
Iter: 1745 loss: 6.95560573e-07
Iter: 1746 loss: 6.96130485e-07
Iter: 1747 loss: 6.95285166e-07
Iter: 1748 loss: 6.9489397e-07
Iter: 1749 loss: 6.9695534e-07
Iter: 1750 loss: 6.9481024e-07
Iter: 1751 loss: 6.94449e-07
Iter: 1752 loss: 6.96491497e-07
Iter: 1753 loss: 6.94405344e-07
Iter: 1754 loss: 6.94057121e-07
Iter: 1755 loss: 6.94021821e-07
Iter: 1756 loss: 6.93823267e-07
Iter: 1757 loss: 6.9338904e-07
Iter: 1758 loss: 6.93906372e-07
Iter: 1759 loss: 6.93195773e-07
Iter: 1760 loss: 6.92797e-07
Iter: 1761 loss: 6.92801109e-07
Iter: 1762 loss: 6.92517801e-07
Iter: 1763 loss: 6.92564868e-07
Iter: 1764 loss: 6.92256606e-07
Iter: 1765 loss: 6.91871264e-07
Iter: 1766 loss: 6.92001208e-07
Iter: 1767 loss: 6.91614787e-07
Iter: 1768 loss: 6.91104049e-07
Iter: 1769 loss: 6.9268242e-07
Iter: 1770 loss: 6.90965066e-07
Iter: 1771 loss: 6.90872525e-07
Iter: 1772 loss: 6.90706315e-07
Iter: 1773 loss: 6.9053408e-07
Iter: 1774 loss: 6.90238721e-07
Iter: 1775 loss: 6.97560324e-07
Iter: 1776 loss: 6.90230252e-07
Iter: 1777 loss: 6.89881176e-07
Iter: 1778 loss: 6.89929834e-07
Iter: 1779 loss: 6.89637091e-07
Iter: 1780 loss: 6.89190301e-07
Iter: 1781 loss: 6.92743811e-07
Iter: 1782 loss: 6.89171031e-07
Iter: 1783 loss: 6.88836735e-07
Iter: 1784 loss: 6.88748742e-07
Iter: 1785 loss: 6.88589353e-07
Iter: 1786 loss: 6.88078103e-07
Iter: 1787 loss: 6.9002823e-07
Iter: 1788 loss: 6.87979423e-07
Iter: 1789 loss: 6.87596582e-07
Iter: 1790 loss: 6.91350692e-07
Iter: 1791 loss: 6.87564579e-07
Iter: 1792 loss: 6.87267516e-07
Iter: 1793 loss: 6.87042302e-07
Iter: 1794 loss: 6.86952148e-07
Iter: 1795 loss: 6.86550493e-07
Iter: 1796 loss: 6.87208967e-07
Iter: 1797 loss: 6.86389967e-07
Iter: 1798 loss: 6.85962902e-07
Iter: 1799 loss: 6.9118289e-07
Iter: 1800 loss: 6.85951761e-07
Iter: 1801 loss: 6.85595296e-07
Iter: 1802 loss: 6.85809198e-07
Iter: 1803 loss: 6.85359794e-07
Iter: 1804 loss: 6.85028e-07
Iter: 1805 loss: 6.84993495e-07
Iter: 1806 loss: 6.84713427e-07
Iter: 1807 loss: 6.84301938e-07
Iter: 1808 loss: 6.86706869e-07
Iter: 1809 loss: 6.84251688e-07
Iter: 1810 loss: 6.83914664e-07
Iter: 1811 loss: 6.8713689e-07
Iter: 1812 loss: 6.83916483e-07
Iter: 1813 loss: 6.83623341e-07
Iter: 1814 loss: 6.84255554e-07
Iter: 1815 loss: 6.83451219e-07
Iter: 1816 loss: 6.83253688e-07
Iter: 1817 loss: 6.83033477e-07
Iter: 1818 loss: 6.82998461e-07
Iter: 1819 loss: 6.82611869e-07
Iter: 1820 loss: 6.83622034e-07
Iter: 1821 loss: 6.82512166e-07
Iter: 1822 loss: 6.82104314e-07
Iter: 1823 loss: 6.82875452e-07
Iter: 1824 loss: 6.81933273e-07
Iter: 1825 loss: 6.81557538e-07
Iter: 1826 loss: 6.82058328e-07
Iter: 1827 loss: 6.81350969e-07
Iter: 1828 loss: 6.80878202e-07
Iter: 1829 loss: 6.80901621e-07
Iter: 1830 loss: 6.80495646e-07
Iter: 1831 loss: 6.80260769e-07
Iter: 1832 loss: 6.80154e-07
Iter: 1833 loss: 6.79902087e-07
Iter: 1834 loss: 6.79781465e-07
Iter: 1835 loss: 6.79650725e-07
Iter: 1836 loss: 6.79287155e-07
Iter: 1837 loss: 6.79918571e-07
Iter: 1838 loss: 6.79150276e-07
Iter: 1839 loss: 6.78788126e-07
Iter: 1840 loss: 6.8120687e-07
Iter: 1841 loss: 6.78754816e-07
Iter: 1842 loss: 6.78426204e-07
Iter: 1843 loss: 6.79091215e-07
Iter: 1844 loss: 6.78260051e-07
Iter: 1845 loss: 6.77955427e-07
Iter: 1846 loss: 6.78107085e-07
Iter: 1847 loss: 6.77738342e-07
Iter: 1848 loss: 6.77561388e-07
Iter: 1849 loss: 6.77528703e-07
Iter: 1850 loss: 6.77284447e-07
Iter: 1851 loss: 6.76812761e-07
Iter: 1852 loss: 6.81352162e-07
Iter: 1853 loss: 6.76743866e-07
Iter: 1854 loss: 6.76289e-07
Iter: 1855 loss: 6.78761126e-07
Iter: 1856 loss: 6.76230343e-07
Iter: 1857 loss: 6.758616e-07
Iter: 1858 loss: 6.76458342e-07
Iter: 1859 loss: 6.75691297e-07
Iter: 1860 loss: 6.75257638e-07
Iter: 1861 loss: 6.76916557e-07
Iter: 1862 loss: 6.75162767e-07
Iter: 1863 loss: 6.74753039e-07
Iter: 1864 loss: 6.74690341e-07
Iter: 1865 loss: 6.74407033e-07
Iter: 1866 loss: 6.73965872e-07
Iter: 1867 loss: 6.75983358e-07
Iter: 1868 loss: 6.73904424e-07
Iter: 1869 loss: 6.73453826e-07
Iter: 1870 loss: 6.75994215e-07
Iter: 1871 loss: 6.73382317e-07
Iter: 1872 loss: 6.73015336e-07
Iter: 1873 loss: 6.73528632e-07
Iter: 1874 loss: 6.72848557e-07
Iter: 1875 loss: 6.72546207e-07
Iter: 1876 loss: 6.72857368e-07
Iter: 1877 loss: 6.72356691e-07
Iter: 1878 loss: 6.71954695e-07
Iter: 1879 loss: 6.7474889e-07
Iter: 1880 loss: 6.71900352e-07
Iter: 1881 loss: 6.71477778e-07
Iter: 1882 loss: 6.71422526e-07
Iter: 1883 loss: 6.71123e-07
Iter: 1884 loss: 6.71156158e-07
Iter: 1885 loss: 6.70957377e-07
Iter: 1886 loss: 6.70818054e-07
Iter: 1887 loss: 6.70549241e-07
Iter: 1888 loss: 6.75013098e-07
Iter: 1889 loss: 6.70544921e-07
Iter: 1890 loss: 6.70240752e-07
Iter: 1891 loss: 6.7003981e-07
Iter: 1892 loss: 6.69934e-07
Iter: 1893 loss: 6.69519522e-07
Iter: 1894 loss: 6.73777777e-07
Iter: 1895 loss: 6.69519295e-07
Iter: 1896 loss: 6.69209385e-07
Iter: 1897 loss: 6.69267592e-07
Iter: 1898 loss: 6.68949156e-07
Iter: 1899 loss: 6.68522318e-07
Iter: 1900 loss: 6.71283601e-07
Iter: 1901 loss: 6.68434495e-07
Iter: 1902 loss: 6.68128848e-07
Iter: 1903 loss: 6.68065866e-07
Iter: 1904 loss: 6.67852873e-07
Iter: 1905 loss: 6.67540519e-07
Iter: 1906 loss: 6.72802912e-07
Iter: 1907 loss: 6.67539098e-07
Iter: 1908 loss: 6.67290863e-07
Iter: 1909 loss: 6.67344636e-07
Iter: 1910 loss: 6.67119707e-07
Iter: 1911 loss: 6.66736639e-07
Iter: 1912 loss: 6.66666779e-07
Iter: 1913 loss: 6.66453843e-07
Iter: 1914 loss: 6.66122787e-07
Iter: 1915 loss: 6.66096184e-07
Iter: 1916 loss: 6.6585136e-07
Iter: 1917 loss: 6.65831635e-07
Iter: 1918 loss: 6.65650873e-07
Iter: 1919 loss: 6.65438847e-07
Iter: 1920 loss: 6.65441405e-07
Iter: 1921 loss: 6.65205789e-07
Iter: 1922 loss: 6.65086304e-07
Iter: 1923 loss: 6.64975516e-07
Iter: 1924 loss: 6.64711251e-07
Iter: 1925 loss: 6.64197159e-07
Iter: 1926 loss: 6.76713398e-07
Iter: 1927 loss: 6.64203526e-07
Iter: 1928 loss: 6.63800165e-07
Iter: 1929 loss: 6.67926e-07
Iter: 1930 loss: 6.63803405e-07
Iter: 1931 loss: 6.63416358e-07
Iter: 1932 loss: 6.64204833e-07
Iter: 1933 loss: 6.63251399e-07
Iter: 1934 loss: 6.62877369e-07
Iter: 1935 loss: 6.63499918e-07
Iter: 1936 loss: 6.62734124e-07
Iter: 1937 loss: 6.62280456e-07
Iter: 1938 loss: 6.63816593e-07
Iter: 1939 loss: 6.62153298e-07
Iter: 1940 loss: 6.61762556e-07
Iter: 1941 loss: 6.62376294e-07
Iter: 1942 loss: 6.61597937e-07
Iter: 1943 loss: 6.61267222e-07
Iter: 1944 loss: 6.65465905e-07
Iter: 1945 loss: 6.61256e-07
Iter: 1946 loss: 6.60991361e-07
Iter: 1947 loss: 6.60885178e-07
Iter: 1948 loss: 6.60728688e-07
Iter: 1949 loss: 6.60354829e-07
Iter: 1950 loss: 6.61852937e-07
Iter: 1951 loss: 6.60252e-07
Iter: 1952 loss: 6.59990803e-07
Iter: 1953 loss: 6.63954324e-07
Iter: 1954 loss: 6.59984607e-07
Iter: 1955 loss: 6.59828459e-07
Iter: 1956 loss: 6.59848183e-07
Iter: 1957 loss: 6.59692e-07
Iter: 1958 loss: 6.59361831e-07
Iter: 1959 loss: 6.60058106e-07
Iter: 1960 loss: 6.59220063e-07
Iter: 1961 loss: 6.59004741e-07
Iter: 1962 loss: 6.58736155e-07
Iter: 1963 loss: 6.58688e-07
Iter: 1964 loss: 6.58307727e-07
Iter: 1965 loss: 6.58488489e-07
Iter: 1966 loss: 6.58014528e-07
Iter: 1967 loss: 6.57581722e-07
Iter: 1968 loss: 6.59608872e-07
Iter: 1969 loss: 6.57508053e-07
Iter: 1970 loss: 6.57063481e-07
Iter: 1971 loss: 6.6039388e-07
Iter: 1972 loss: 6.5701704e-07
Iter: 1973 loss: 6.56788245e-07
Iter: 1974 loss: 6.56982365e-07
Iter: 1975 loss: 6.56636757e-07
Iter: 1976 loss: 6.56229929e-07
Iter: 1977 loss: 6.56291661e-07
Iter: 1978 loss: 6.55928261e-07
Iter: 1979 loss: 6.55529675e-07
Iter: 1980 loss: 6.59278328e-07
Iter: 1981 loss: 6.55484314e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4
+ date
Wed Oct 21 14:28:00 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/300_300_300_1 --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444be4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444be4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444ce3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444c24bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444c2dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444bac400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444b66e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444baf840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444b3e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a51d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a7f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a21f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a0d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444af5e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a44d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444ac5ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444abf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04214b9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042148eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444abf6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042155a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042155ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4b7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042155a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4b2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4b20d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc5248c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc524840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4fb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4fbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4799d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc3c38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc3c3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc3c32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc3a39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc428620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.11113784
test_loss: 0.103638336
train_loss: 0.07230967
test_loss: 0.087605536
train_loss: 0.055875108
test_loss: 0.05916124
train_loss: 0.033990867
test_loss: 0.045454264
train_loss: 0.034245312
test_loss: 0.037927687
train_loss: 0.028211817
test_loss: 0.033453804
train_loss: 0.020020653
test_loss: 0.029648867
train_loss: 0.021903735
test_loss: 0.024972934
train_loss: 0.015644966
test_loss: 0.023035327
train_loss: 0.015281787
test_loss: 0.023028275
train_loss: 0.014986439
test_loss: 0.020076837
train_loss: 0.017617837
test_loss: 0.01905676
train_loss: 0.013957813
test_loss: 0.019608047
train_loss: 0.012739767
test_loss: 0.019935224
train_loss: 0.012277609
test_loss: 0.018281301
train_loss: 0.012426719
test_loss: 0.017655222
train_loss: 0.012218218
test_loss: 0.018900506
train_loss: 0.010872927
test_loss: 0.017211232
train_loss: 0.010666766
test_loss: 0.016713234
train_loss: 0.010587971
test_loss: 0.015734408
train_loss: 0.01073887
test_loss: 0.01660362
train_loss: 0.011589283
test_loss: 0.016152646
train_loss: 0.010863395
test_loss: 0.016274631
train_loss: 0.009089401
test_loss: 0.014487277
train_loss: 0.009151677
test_loss: 0.0149034485
train_loss: 0.013936858
test_loss: 0.0153338965
train_loss: 0.009662943
test_loss: 0.014818741
train_loss: 0.009562081
test_loss: 0.0147844795
train_loss: 0.009524833
test_loss: 0.014340233
train_loss: 0.01004879
test_loss: 0.014767204
train_loss: 0.009057126
test_loss: 0.013984456
train_loss: 0.009295777
test_loss: 0.01467546
train_loss: 0.009304381
test_loss: 0.014929706
train_loss: 0.009730693
test_loss: 0.015640752
train_loss: 0.010277199
test_loss: 0.01424824
train_loss: 0.009352272
test_loss: 0.014177121
train_loss: 0.010442993
test_loss: 0.015318344
train_loss: 0.010008495
test_loss: 0.014251357
train_loss: 0.010299167
test_loss: 0.013897427
train_loss: 0.009029772
test_loss: 0.013301242
train_loss: 0.009336826
test_loss: 0.0140931355
train_loss: 0.009710772
test_loss: 0.01466286
train_loss: 0.0092507955
test_loss: 0.014096435
train_loss: 0.007893528
test_loss: 0.012439001
train_loss: 0.008061612
test_loss: 0.013491639
train_loss: 0.007933897
test_loss: 0.013669418
train_loss: 0.009876013
test_loss: 0.013231433
train_loss: 0.0073834513
test_loss: 0.013647941
train_loss: 0.010490731
test_loss: 0.014002483
train_loss: 0.009496098
test_loss: 0.0128258085
train_loss: 0.008245522
test_loss: 0.013547526
train_loss: 0.010042472
test_loss: 0.013929698
train_loss: 0.009106504
test_loss: 0.0126455715
train_loss: 0.008342129
test_loss: 0.01315236
train_loss: 0.008964603
test_loss: 0.01335498
train_loss: 0.0090192
test_loss: 0.014540104
train_loss: 0.009047487
test_loss: 0.01261593
train_loss: 0.0092380345
test_loss: 0.012477213
train_loss: 0.008537151
test_loss: 0.014212523
train_loss: 0.009581199
test_loss: 0.013242235
train_loss: 0.00801196
test_loss: 0.013008784
train_loss: 0.0077539035
test_loss: 0.012945708
train_loss: 0.008448419
test_loss: 0.013843241
train_loss: 0.008826428
test_loss: 0.01312128
train_loss: 0.0080624735
test_loss: 0.013308975
train_loss: 0.009088159
test_loss: 0.013018409
train_loss: 0.0081472285
test_loss: 0.01235486
train_loss: 0.008678279
test_loss: 0.013541954
train_loss: 0.010880858
test_loss: 0.014619007
train_loss: 0.011449229
test_loss: 0.01612206
train_loss: 0.008346899
test_loss: 0.01456535
train_loss: 0.008398408
test_loss: 0.013056037
train_loss: 0.008888191
test_loss: 0.012920258
train_loss: 0.008508631
test_loss: 0.01236491
train_loss: 0.008895217
test_loss: 0.012993458
train_loss: 0.00797858
test_loss: 0.012204603
train_loss: 0.007493509
test_loss: 0.012038458
train_loss: 0.0074542146
test_loss: 0.013387355
train_loss: 0.008191848
test_loss: 0.014124221
train_loss: 0.008518616
test_loss: 0.0128146885
train_loss: 0.008041704
test_loss: 0.012378849
train_loss: 0.00762422
test_loss: 0.012253884
train_loss: 0.0087187905
test_loss: 0.011819782
train_loss: 0.007866611
test_loss: 0.012763295
train_loss: 0.0071104295
test_loss: 0.01186149
train_loss: 0.007460857
test_loss: 0.0129096005
train_loss: 0.0074849697
test_loss: 0.011795393
train_loss: 0.009183018
test_loss: 0.014240481
train_loss: 0.008318318
test_loss: 0.012407776
train_loss: 0.008697419
test_loss: 0.013812985
train_loss: 0.0077437866
test_loss: 0.012437413
train_loss: 0.00781022
test_loss: 0.012654387
train_loss: 0.0080870055
test_loss: 0.012843042
train_loss: 0.008933743
test_loss: 0.012331719
train_loss: 0.008856814
test_loss: 0.012044261
train_loss: 0.009133594
test_loss: 0.013156319
train_loss: 0.0080766305
test_loss: 0.011900438
train_loss: 0.0075666145
test_loss: 0.011268117
train_loss: 0.008146757
test_loss: 0.01240407
train_loss: 0.007066901
test_loss: 0.012148964
train_loss: 0.0090720495
test_loss: 0.012014954
train_loss: 0.007706686
test_loss: 0.011703523
train_loss: 0.009041301
test_loss: 0.01212128
train_loss: 0.007850224
test_loss: 0.012227464
train_loss: 0.007044385
test_loss: 0.0116727585
train_loss: 0.008479083
test_loss: 0.012852064
train_loss: 0.009269818
test_loss: 0.012765535
train_loss: 0.0076407576
test_loss: 0.013560963
train_loss: 0.006742816
test_loss: 0.011819012
train_loss: 0.007524715
test_loss: 0.012193843
train_loss: 0.0070588086
test_loss: 0.011780601
train_loss: 0.0073194536
test_loss: 0.01211751
train_loss: 0.008232205
test_loss: 0.013506749
train_loss: 0.008030159
test_loss: 0.012693264
train_loss: 0.0069533503
test_loss: 0.011953475
train_loss: 0.007604783
test_loss: 0.011696368
train_loss: 0.007187416
test_loss: 0.0124677615
train_loss: 0.0067012785
test_loss: 0.011778693
train_loss: 0.008835766
test_loss: 0.013045997
train_loss: 0.008402674
test_loss: 0.0116656255
train_loss: 0.0084988475
test_loss: 0.011934688
train_loss: 0.0074336864
test_loss: 0.011854712
train_loss: 0.0071640555
test_loss: 0.011861397
train_loss: 0.0075532724
test_loss: 0.01235826
train_loss: 0.0074509387
test_loss: 0.011981814
train_loss: 0.007373654
test_loss: 0.0124358935
train_loss: 0.0072272494
test_loss: 0.011362778
train_loss: 0.006940626
test_loss: 0.012087195
train_loss: 0.0071149124
test_loss: 0.011599782
train_loss: 0.006613852
test_loss: 0.011341634
train_loss: 0.007773948
test_loss: 0.012556628
train_loss: 0.009011768
test_loss: 0.011846264
train_loss: 0.008197102
test_loss: 0.012516154
train_loss: 0.007901074
test_loss: 0.013263455
train_loss: 0.007046383
test_loss: 0.011935712
train_loss: 0.0076784156
test_loss: 0.0121815745
train_loss: 0.0065208673
test_loss: 0.011805964
train_loss: 0.0071834554
test_loss: 0.013186012
train_loss: 0.006890911
test_loss: 0.011918389
train_loss: 0.007677941
test_loss: 0.012234249
train_loss: 0.008280936
test_loss: 0.012160951
train_loss: 0.008526599
test_loss: 0.013016127
train_loss: 0.0072933394
test_loss: 0.0114831505
train_loss: 0.0079726195
test_loss: 0.0125616705
train_loss: 0.0074658203
test_loss: 0.011904309
train_loss: 0.0074294843
test_loss: 0.0121396715
train_loss: 0.008140609
test_loss: 0.012938964
train_loss: 0.0077526122
test_loss: 0.012738179
train_loss: 0.007026072
test_loss: 0.012331123
train_loss: 0.009655562
test_loss: 0.012765007
train_loss: 0.008899383
test_loss: 0.0121462075
train_loss: 0.00806083
test_loss: 0.011592795
train_loss: 0.007233194
test_loss: 0.011746033
train_loss: 0.0064441795
test_loss: 0.0116754435
train_loss: 0.006375052
test_loss: 0.011469252
train_loss: 0.0070438203
test_loss: 0.0122223245
train_loss: 0.008041285
test_loss: 0.012782239
train_loss: 0.007294457
test_loss: 0.012997358
train_loss: 0.0068082893
test_loss: 0.011109294
train_loss: 0.007060161
test_loss: 0.011824531
train_loss: 0.006106961
test_loss: 0.0113886045
train_loss: 0.007118917
test_loss: 0.011641891
train_loss: 0.0077253487
test_loss: 0.012314122
train_loss: 0.00838735
test_loss: 0.012854867
train_loss: 0.007239521
test_loss: 0.012864782
train_loss: 0.0068179015
test_loss: 0.011481206
train_loss: 0.0071054976
test_loss: 0.012303413
train_loss: 0.0065974845
test_loss: 0.011687044
train_loss: 0.0067062546
test_loss: 0.011927767
train_loss: 0.00696594
test_loss: 0.012097321
train_loss: 0.007019705
test_loss: 0.011975246
train_loss: 0.007517174
test_loss: 0.012091786
train_loss: 0.00836535
test_loss: 0.013007318
train_loss: 0.008148783
test_loss: 0.011619035
train_loss: 0.0067519927
test_loss: 0.011676259
train_loss: 0.006219092
test_loss: 0.011481558
train_loss: 0.0070779724
test_loss: 0.011646644
train_loss: 0.0065612243
test_loss: 0.011209171
train_loss: 0.007672151
test_loss: 0.011952798
train_loss: 0.007078927
test_loss: 0.01155157
train_loss: 0.0067306985
test_loss: 0.011542093
train_loss: 0.006769175
test_loss: 0.011929148
train_loss: 0.0069565442
test_loss: 0.011985602
train_loss: 0.00705646
test_loss: 0.011467213
train_loss: 0.008232845
test_loss: 0.012087136
train_loss: 0.007146701
test_loss: 0.0121255
train_loss: 0.0069865426
test_loss: 0.011684236
train_loss: 0.0068764193
test_loss: 0.013741434
train_loss: 0.0068520172
test_loss: 0.011761556
train_loss: 0.008714597
test_loss: 0.012276991
train_loss: 0.006609738
test_loss: 0.0118170595
train_loss: 0.008550934
test_loss: 0.012206402
train_loss: 0.007559036
test_loss: 0.012435419
train_loss: 0.0076397387
test_loss: 0.012864642
train_loss: 0.007860791
test_loss: 0.012003658
train_loss: 0.00734424
test_loss: 0.01196982
train_loss: 0.0069618393
test_loss: 0.012854531
train_loss: 0.007301023
test_loss: 0.012293067
train_loss: 0.0087563945
test_loss: 0.011867478
train_loss: 0.00730275
test_loss: 0.01167395
train_loss: 0.007221381
test_loss: 0.012015861
train_loss: 0.0071989372
test_loss: 0.011814751
train_loss: 0.0077687628
test_loss: 0.012129623
train_loss: 0.0066811317
test_loss: 0.012031204
train_loss: 0.007431814
test_loss: 0.011408521
train_loss: 0.0068314997
test_loss: 0.012304478
train_loss: 0.007002011
test_loss: 0.011752457
train_loss: 0.006850267
test_loss: 0.011361064
train_loss: 0.0068231244
test_loss: 0.0125976
train_loss: 0.006596503
test_loss: 0.012390772
train_loss: 0.007642767
test_loss: 0.012587707
train_loss: 0.007134497
test_loss: 0.012755715
train_loss: 0.007516031
test_loss: 0.012137574
train_loss: 0.00797217
test_loss: 0.012304358
train_loss: 0.006942114
test_loss: 0.012299455
train_loss: 0.007469321
test_loss: 0.011472173
train_loss: 0.0063615516
test_loss: 0.011924215
train_loss: 0.0073097367
test_loss: 0.012601351
train_loss: 0.0072373506
test_loss: 0.012192585
train_loss: 0.0076510217
test_loss: 0.012665928
train_loss: 0.006827084
test_loss: 0.011610232
train_loss: 0.0076911743
test_loss: 0.012606866
train_loss: 0.007513673
test_loss: 0.012105102
train_loss: 0.0068700146
test_loss: 0.012780606
train_loss: 0.0076996344
test_loss: 0.011855867
train_loss: 0.0074824356
test_loss: 0.0118277
train_loss: 0.0071917027
test_loss: 0.011880176
train_loss: 0.0061696135
test_loss: 0.011831065
train_loss: 0.0070052035
test_loss: 0.011769097
train_loss: 0.0068733282
test_loss: 0.012296228
train_loss: 0.007008271
test_loss: 0.012144437
train_loss: 0.007906732
test_loss: 0.012725474
train_loss: 0.007314492
test_loss: 0.013285478
train_loss: 0.0067521683
test_loss: 0.012790947
train_loss: 0.0071068164
test_loss: 0.011840837
train_loss: 0.0074063176
test_loss: 0.012228715
train_loss: 0.007338493
test_loss: 0.012587089
train_loss: 0.006325949
test_loss: 0.011655393
train_loss: 0.0062143
test_loss: 0.011601353
train_loss: 0.007291948
test_loss: 0.012440736
train_loss: 0.007045515
test_loss: 0.012400051
train_loss: 0.007230856
test_loss: 0.0119189015
train_loss: 0.007688265
test_loss: 0.012086204
train_loss: 0.007664659
test_loss: 0.012397972
train_loss: 0.007195555
test_loss: 0.012396136
train_loss: 0.0071185115
test_loss: 0.011870886
train_loss: 0.0069290507
test_loss: 0.011494341
train_loss: 0.006984034
test_loss: 0.012138963
train_loss: 0.007571757
test_loss: 0.011978834
train_loss: 0.008223339
test_loss: 0.01276746
train_loss: 0.0067467615
test_loss: 0.011639198
train_loss: 0.0059401137
test_loss: 0.011463799
train_loss: 0.006958916
test_loss: 0.011743519
train_loss: 0.0074111284
test_loss: 0.011370205
train_loss: 0.006646172
test_loss: 0.0124907065
train_loss: 0.006070793
test_loss: 0.011395711
train_loss: 0.007172153
test_loss: 0.012332265
train_loss: 0.0069429614
test_loss: 0.012282536
train_loss: 0.007115784
test_loss: 0.011777147
train_loss: 0.006795982
test_loss: 0.011992016
train_loss: 0.006949027
test_loss: 0.0119715175
train_loss: 0.0066857142
test_loss: 0.01209059
train_loss: 0.008252418
test_loss: 0.012079333
train_loss: 0.006371157
test_loss: 0.011716984
train_loss: 0.007199418
test_loss: 0.012455573
train_loss: 0.0067637144
test_loss: 0.011918201
train_loss: 0.0070340396
test_loss: 0.012383398
train_loss: 0.0069195013
test_loss: 0.012243793
train_loss: 0.006372948
test_loss: 0.01179681
train_loss: 0.006867039
test_loss: 0.011853752
train_loss: 0.00711067
test_loss: 0.011725993
train_loss: 0.0075156055
test_loss: 0.012460393
train_loss: 0.007588121
test_loss: 0.012173159
train_loss: 0.007049574
test_loss: 0.0118193645
train_loss: 0.0073911976
test_loss: 0.011832864
train_loss: 0.0060540987
test_loss: 0.011197827
train_loss: 0.0073978896
test_loss: 0.0124430265
train_loss: 0.006536959
test_loss: 0.011709809
train_loss: 0.0073302025
test_loss: 0.012802052
train_loss: 0.006328063
test_loss: 0.0118053835
train_loss: 0.007492874
test_loss: 0.011667315
train_loss: 0.0067237145
test_loss: 0.011730334
train_loss: 0.008143945
test_loss: 0.011963925
train_loss: 0.007487258
test_loss: 0.012125534
train_loss: 0.007222213
test_loss: 0.012519864
train_loss: 0.0066368133
test_loss: 0.011777181
train_loss: 0.006521892
test_loss: 0.012115439
train_loss: 0.0055979574
test_loss: 0.011690299
train_loss: 0.0058798296
test_loss: 0.011458555
train_loss: 0.006503231
test_loss: 0.012103755
train_loss: 0.006456512
test_loss: 0.012304951
train_loss: 0.0077591324
test_loss: 0.01253742
train_loss: 0.0067826915
test_loss: 0.012485942
train_loss: 0.007079927
test_loss: 0.012065922
train_loss: 0.0063555455
test_loss: 0.011749436
train_loss: 0.0066778436
test_loss: 0.011534521
train_loss: 0.005742103
test_loss: 0.0115233585
train_loss: 0.0072059194
test_loss: 0.011863872
train_loss: 0.006213668
test_loss: 0.011434635
train_loss: 0.006002443
test_loss: 0.012008475
train_loss: 0.0074804644
test_loss: 0.012476137
train_loss: 0.007050577
test_loss: 0.012064957
train_loss: 0.007406099
test_loss: 0.012713896
train_loss: 0.007223595
test_loss: 0.012339581
train_loss: 0.006591539
test_loss: 0.011688868
train_loss: 0.0066388855
test_loss: 0.012016128
train_loss: 0.006105776
test_loss: 0.011456051
train_loss: 0.0067384667
test_loss: 0.011454032
train_loss: 0.0061117094
test_loss: 0.012066204
train_loss: 0.006230657
test_loss: 0.012063027
train_loss: 0.0069474326
test_loss: 0.011923617
train_loss: 0.0062675467
test_loss: 0.011394215
train_loss: 0.007877992
test_loss: 0.01263831
train_loss: 0.0058398843
test_loss: 0.011556663
train_loss: 0.0065064644
test_loss: 0.011409453
train_loss: 0.0065907105
test_loss: 0.012211656
train_loss: 0.0061308616
test_loss: 0.01157009
train_loss: 0.0062906872
test_loss: 0.012100474
train_loss: 0.007133349
test_loss: 0.012065432
train_loss: 0.00646172
test_loss: 0.012154466
train_loss: 0.0075322716
test_loss: 0.011974037
train_loss: 0.0062289466
test_loss: 0.01165154
train_loss: 0.007999493
test_loss: 0.0123769455
train_loss: 0.006786533
test_loss: 0.011639104
train_loss: 0.006264723
test_loss: 0.011898067
train_loss: 0.0060292413
test_loss: 0.011685984
train_loss: 0.00789725
test_loss: 0.011749778
train_loss: 0.006064072
test_loss: 0.012237842
train_loss: 0.0070782905
test_loss: 0.012009722
train_loss: 0.0067697624
test_loss: 0.012083216
train_loss: 0.007086354
test_loss: 0.012834379
train_loss: 0.006460142
test_loss: 0.012208375
train_loss: 0.0061704027
test_loss: 0.011254534
train_loss: 0.0075112325
test_loss: 0.013358099
train_loss: 0.0065769185
test_loss: 0.011833102
train_loss: 0.006424084
test_loss: 0.012152093
train_loss: 0.00613736
test_loss: 0.011512001
train_loss: 0.008544241
test_loss: 0.011488005
train_loss: 0.0066527147
test_loss: 0.012931211
train_loss: 0.005971137
test_loss: 0.011758545
train_loss: 0.006749497
test_loss: 0.013029736
train_loss: 0.0062631876
test_loss: 0.011540729
train_loss: 0.0070996745
test_loss: 0.011909948
train_loss: 0.005606084
test_loss: 0.011669644
train_loss: 0.0060518077
test_loss: 0.01170999
train_loss: 0.0071170647
test_loss: 0.011824885
train_loss: 0.006969415
test_loss: /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
0.012686802
train_loss: 0.006722316
test_loss: 0.011766705
train_loss: 0.008104581
test_loss: 0.012341494
train_loss: 0.007256885
test_loss: 0.011698318
train_loss: 0.0066968044
test_loss: 0.012053144
train_loss: 0.0068671945
test_loss: 0.012180887
train_loss: 0.0063430974
test_loss: 0.012465587
train_loss: 0.007104963
test_loss: 0.01231241
train_loss: 0.007155138
test_loss: 0.012715694
train_loss: 0.0066502234
test_loss: 0.012167729
train_loss: 0.005733418
test_loss: 0.012066694
train_loss: 0.0064328033
test_loss: 0.012180459
train_loss: 0.006474511
test_loss: 0.012827962
train_loss: 0.0073289084
test_loss: 0.012168319
train_loss: 0.006137084
test_loss: 0.012145183
train_loss: 0.0073936777
test_loss: 0.012054421
train_loss: 0.0063869487
test_loss: 0.011819007
train_loss: 0.0059592063
test_loss: 0.011636849
train_loss: 0.0077902447
test_loss: 0.012176597
train_loss: 0.006313584
test_loss: 0.011611727
train_loss: 0.006589255
test_loss: 0.011995992
train_loss: 0.00558519
test_loss: 0.011261238
train_loss: 0.0070633627
test_loss: 0.012207257
train_loss: 0.006429538
test_loss: 0.011790194
train_loss: 0.0075265593
test_loss: 0.012242163
train_loss: 0.005647189
test_loss: 0.011713784
train_loss: 0.006918406
test_loss: 0.012332584
train_loss: 0.0062617
test_loss: 0.012413187
train_loss: 0.006871048
test_loss: 0.012367814
train_loss: 0.006228323
test_loss: 0.0119432
train_loss: 0.0055848076
test_loss: 0.012128112
train_loss: 0.0064797327
test_loss: 0.012653506
train_loss: 0.006722371
test_loss: 0.012590708
train_loss: 0.0065328213
test_loss: 0.012040938
train_loss: 0.0061625517
test_loss: 0.011850463
train_loss: 0.0068777245
test_loss: 0.012236824
train_loss: 0.006223116
test_loss: 0.012082402
train_loss: 0.0060113077
test_loss: 0.012730959
train_loss: 0.0075473906
test_loss: 0.012501798
train_loss: 0.0082870545
test_loss: 0.014334853
train_loss: 0.007129807
test_loss: 0.012789
train_loss: 0.006152208
test_loss: 0.012347659
train_loss: 0.007758045
test_loss: 0.012670262
train_loss: 0.007717414
test_loss: 0.012825706
train_loss: 0.006256313
test_loss: 0.01283226
train_loss: 0.007969083
test_loss: 0.01316736
train_loss: 0.0059919134
test_loss: 0.012004356
train_loss: 0.0058204997
test_loss: 0.011993728
train_loss: 0.0067571662
test_loss: 0.011538887
train_loss: 0.006419266
test_loss: 0.012577695
train_loss: 0.005481103
test_loss: 0.011763854
train_loss: 0.0067166286
test_loss: 0.012128471
train_loss: 0.006948938
test_loss: 0.012362763
train_loss: 0.00600762
test_loss: 0.0116375545
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45d0a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45de7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45de7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45d26e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45d4c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45d4c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45c918c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45c46840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45c392f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45c39488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45baa510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45bb0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45b5ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45b93bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7ea1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7e8d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7e8dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7e79598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7e346a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7e34c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7dbf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7dbf1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad04e4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad04ff950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad04ff1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad04b8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad04738c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad04a2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad042f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad044f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad03f46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad03bd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad03b7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad03d7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad03d50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad0324620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 8.68095813e-05
Iter: 2 loss: 0.000832954596
Iter: 3 loss: 6.76405689e-05
Iter: 4 loss: 6.42755331e-05
Iter: 5 loss: 6.0441911e-05
Iter: 6 loss: 5.99476953e-05
Iter: 7 loss: 5.55966835e-05
Iter: 8 loss: 6.18189661e-05
Iter: 9 loss: 5.34945793e-05
Iter: 10 loss: 5.02388357e-05
Iter: 11 loss: 4.34169342e-05
Iter: 12 loss: 0.000160143754
Iter: 13 loss: 4.32443412e-05
Iter: 14 loss: 3.83055813e-05
Iter: 15 loss: 8.30485806e-05
Iter: 16 loss: 3.80867423e-05
Iter: 17 loss: 3.57912104e-05
Iter: 18 loss: 5.36156331e-05
Iter: 19 loss: 3.56277851e-05
Iter: 20 loss: 3.34763754e-05
Iter: 21 loss: 3.40738407e-05
Iter: 22 loss: 3.19216451e-05
Iter: 23 loss: 2.95159662e-05
Iter: 24 loss: 3.06021393e-05
Iter: 25 loss: 2.78831485e-05
Iter: 26 loss: 2.58139462e-05
Iter: 27 loss: 3.50383416e-05
Iter: 28 loss: 2.5405323e-05
Iter: 29 loss: 2.38159846e-05
Iter: 30 loss: 2.80384884e-05
Iter: 31 loss: 2.32817947e-05
Iter: 32 loss: 2.19405192e-05
Iter: 33 loss: 2.44402672e-05
Iter: 34 loss: 2.13698222e-05
Iter: 35 loss: 1.98098114e-05
Iter: 36 loss: 2.80555087e-05
Iter: 37 loss: 1.95696593e-05
Iter: 38 loss: 1.85897188e-05
Iter: 39 loss: 1.97414101e-05
Iter: 40 loss: 1.80719035e-05
Iter: 41 loss: 1.87965888e-05
Iter: 42 loss: 1.75821533e-05
Iter: 43 loss: 1.73116568e-05
Iter: 44 loss: 1.66276313e-05
Iter: 45 loss: 2.2865439e-05
Iter: 46 loss: 1.65253114e-05
Iter: 47 loss: 1.58530147e-05
Iter: 48 loss: 2.05255637e-05
Iter: 49 loss: 1.57909981e-05
Iter: 50 loss: 1.53897672e-05
Iter: 51 loss: 1.94979129e-05
Iter: 52 loss: 1.53783567e-05
Iter: 53 loss: 1.50014585e-05
Iter: 54 loss: 1.55027446e-05
Iter: 55 loss: 1.48111421e-05
Iter: 56 loss: 1.43059224e-05
Iter: 57 loss: 1.42140489e-05
Iter: 58 loss: 1.38733849e-05
Iter: 59 loss: 1.33736758e-05
Iter: 60 loss: 1.51043387e-05
Iter: 61 loss: 1.32420864e-05
Iter: 62 loss: 1.28143947e-05
Iter: 63 loss: 1.31515944e-05
Iter: 64 loss: 1.25563092e-05
Iter: 65 loss: 1.20224531e-05
Iter: 66 loss: 1.28763013e-05
Iter: 67 loss: 1.17756308e-05
Iter: 68 loss: 1.13618607e-05
Iter: 69 loss: 1.60096024e-05
Iter: 70 loss: 1.13535662e-05
Iter: 71 loss: 1.10173851e-05
Iter: 72 loss: 1.11001427e-05
Iter: 73 loss: 1.07722217e-05
Iter: 74 loss: 1.0654e-05
Iter: 75 loss: 1.0594993e-05
Iter: 76 loss: 1.03844268e-05
Iter: 77 loss: 1.09732691e-05
Iter: 78 loss: 1.03170923e-05
Iter: 79 loss: 1.02182321e-05
Iter: 80 loss: 9.92875e-06
Iter: 81 loss: 1.10332767e-05
Iter: 82 loss: 9.80667937e-06
Iter: 83 loss: 9.47010813e-06
Iter: 84 loss: 1.44492442e-05
Iter: 85 loss: 9.46962791e-06
Iter: 86 loss: 9.25309087e-06
Iter: 87 loss: 1.03049742e-05
Iter: 88 loss: 9.21467836e-06
Iter: 89 loss: 9.05929301e-06
Iter: 90 loss: 1.08635486e-05
Iter: 91 loss: 9.05728848e-06
Iter: 92 loss: 8.92304342e-06
Iter: 93 loss: 8.92046774e-06
Iter: 94 loss: 8.8154311e-06
Iter: 95 loss: 8.63246078e-06
Iter: 96 loss: 8.61532499e-06
Iter: 97 loss: 8.48050786e-06
Iter: 98 loss: 8.24738436e-06
Iter: 99 loss: 9.91236902e-06
Iter: 100 loss: 8.22632865e-06
Iter: 101 loss: 8.06595926e-06
Iter: 102 loss: 8.35815354e-06
Iter: 103 loss: 7.9968413e-06
Iter: 104 loss: 7.81665e-06
Iter: 105 loss: 7.99121426e-06
Iter: 106 loss: 7.71484e-06
Iter: 107 loss: 7.57534144e-06
Iter: 108 loss: 8.97419523e-06
Iter: 109 loss: 7.57113457e-06
Iter: 110 loss: 7.46090109e-06
Iter: 111 loss: 7.67330857e-06
Iter: 112 loss: 7.41479153e-06
Iter: 113 loss: 7.23304629e-06
Iter: 114 loss: 8.48551463e-06
Iter: 115 loss: 7.21544438e-06
Iter: 116 loss: 7.1681643e-06
Iter: 117 loss: 7.06773289e-06
Iter: 118 loss: 8.67104427e-06
Iter: 119 loss: 7.06428227e-06
Iter: 120 loss: 6.96294137e-06
Iter: 121 loss: 7.08166135e-06
Iter: 122 loss: 6.90911838e-06
Iter: 123 loss: 6.85880696e-06
Iter: 124 loss: 6.84848237e-06
Iter: 125 loss: 6.80281619e-06
Iter: 126 loss: 6.81971869e-06
Iter: 127 loss: 6.7708329e-06
Iter: 128 loss: 6.69311794e-06
Iter: 129 loss: 6.78348852e-06
Iter: 130 loss: 6.65162e-06
Iter: 131 loss: 6.58347562e-06
Iter: 132 loss: 6.67652421e-06
Iter: 133 loss: 6.54974747e-06
Iter: 134 loss: 6.46988656e-06
Iter: 135 loss: 6.38576421e-06
Iter: 136 loss: 6.37132325e-06
Iter: 137 loss: 6.27858572e-06
Iter: 138 loss: 7.37333767e-06
Iter: 139 loss: 6.27721738e-06
Iter: 140 loss: 6.19469483e-06
Iter: 141 loss: 6.15514909e-06
Iter: 142 loss: 6.11467658e-06
Iter: 143 loss: 6.02820455e-06
Iter: 144 loss: 6.02818909e-06
Iter: 145 loss: 5.97451344e-06
Iter: 146 loss: 6.39563495e-06
Iter: 147 loss: 5.97068629e-06
Iter: 148 loss: 5.91225125e-06
Iter: 149 loss: 6.38763368e-06
Iter: 150 loss: 5.90807485e-06
Iter: 151 loss: 5.88981038e-06
Iter: 152 loss: 5.84642248e-06
Iter: 153 loss: 6.34499e-06
Iter: 154 loss: 5.84234976e-06
Iter: 155 loss: 5.79271682e-06
Iter: 156 loss: 5.76483762e-06
Iter: 157 loss: 5.74282512e-06
Iter: 158 loss: 5.72822046e-06
Iter: 159 loss: 5.70223392e-06
Iter: 160 loss: 5.67459e-06
Iter: 161 loss: 5.71611963e-06
Iter: 162 loss: 5.66169956e-06
Iter: 163 loss: 5.62864898e-06
Iter: 164 loss: 5.64763286e-06
Iter: 165 loss: 5.60767603e-06
Iter: 166 loss: 5.56067153e-06
Iter: 167 loss: 5.57536487e-06
Iter: 168 loss: 5.52683e-06
Iter: 169 loss: 5.47656373e-06
Iter: 170 loss: 5.69184431e-06
Iter: 171 loss: 5.46614319e-06
Iter: 172 loss: 5.42202224e-06
Iter: 173 loss: 5.38237418e-06
Iter: 174 loss: 5.37117785e-06
Iter: 175 loss: 5.31126898e-06
Iter: 176 loss: 5.94110497e-06
Iter: 177 loss: 5.30959278e-06
Iter: 178 loss: 5.25616542e-06
Iter: 179 loss: 5.22591472e-06
Iter: 180 loss: 5.20288131e-06
Iter: 181 loss: 5.50027153e-06
Iter: 182 loss: 5.19280457e-06
Iter: 183 loss: 5.18650904e-06
Iter: 184 loss: 5.16611863e-06
Iter: 185 loss: 5.18675824e-06
Iter: 186 loss: 5.14978274e-06
Iter: 187 loss: 5.10766495e-06
Iter: 188 loss: 5.08691664e-06
Iter: 189 loss: 5.06641663e-06
Iter: 190 loss: 5.01908698e-06
Iter: 191 loss: 5.39387656e-06
Iter: 192 loss: 5.01601426e-06
Iter: 193 loss: 4.98713052e-06
Iter: 194 loss: 5.24409415e-06
Iter: 195 loss: 4.98581039e-06
Iter: 196 loss: 4.9551445e-06
Iter: 197 loss: 5.01051818e-06
Iter: 198 loss: 4.94161941e-06
Iter: 199 loss: 4.91827223e-06
Iter: 200 loss: 4.99049565e-06
Iter: 201 loss: 4.91127776e-06
Iter: 202 loss: 4.88490605e-06
Iter: 203 loss: 4.85145347e-06
Iter: 204 loss: 4.84888642e-06
Iter: 205 loss: 4.80035305e-06
Iter: 206 loss: 4.99468842e-06
Iter: 207 loss: 4.78940183e-06
Iter: 208 loss: 4.75327397e-06
Iter: 209 loss: 4.81510597e-06
Iter: 210 loss: 4.73733326e-06
Iter: 211 loss: 4.70378291e-06
Iter: 212 loss: 4.97071778e-06
Iter: 213 loss: 4.7013109e-06
Iter: 214 loss: 4.67754307e-06
Iter: 215 loss: 4.68404e-06
Iter: 216 loss: 4.66024585e-06
Iter: 217 loss: 4.68804228e-06
Iter: 218 loss: 4.64820596e-06
Iter: 219 loss: 4.64251116e-06
Iter: 220 loss: 4.62502112e-06
Iter: 221 loss: 4.67626705e-06
Iter: 222 loss: 4.61575564e-06
Iter: 223 loss: 4.59497232e-06
Iter: 224 loss: 4.57138685e-06
Iter: 225 loss: 4.56814178e-06
Iter: 226 loss: 4.51855158e-06
Iter: 227 loss: 4.75332945e-06
Iter: 228 loss: 4.50940342e-06
Iter: 229 loss: 4.48860101e-06
Iter: 230 loss: 4.48831042e-06
Iter: 231 loss: 4.466674e-06
Iter: 232 loss: 4.53913799e-06
Iter: 233 loss: 4.46071408e-06
Iter: 234 loss: 4.44063e-06
Iter: 235 loss: 4.45325e-06
Iter: 236 loss: 4.42754754e-06
Iter: 237 loss: 4.40935491e-06
Iter: 238 loss: 4.50898597e-06
Iter: 239 loss: 4.40657686e-06
Iter: 240 loss: 4.3889404e-06
Iter: 241 loss: 4.37959e-06
Iter: 242 loss: 4.37154904e-06
Iter: 243 loss: 4.34391723e-06
Iter: 244 loss: 4.48365063e-06
Iter: 245 loss: 4.33926834e-06
Iter: 246 loss: 4.32175966e-06
Iter: 247 loss: 4.28615294e-06
Iter: 248 loss: 4.94268716e-06
Iter: 249 loss: 4.28551357e-06
Iter: 250 loss: 4.27549867e-06
Iter: 251 loss: 4.26631232e-06
Iter: 252 loss: 4.25426242e-06
Iter: 253 loss: 4.25393864e-06
Iter: 254 loss: 4.24984228e-06
Iter: 255 loss: 4.23773099e-06
Iter: 256 loss: 4.26469069e-06
Iter: 257 loss: 4.23036363e-06
Iter: 258 loss: 4.21088907e-06
Iter: 259 loss: 4.21989535e-06
Iter: 260 loss: 4.19748631e-06
Iter: 261 loss: 4.17225147e-06
Iter: 262 loss: 4.25441795e-06
Iter: 263 loss: 4.16516377e-06
Iter: 264 loss: 4.14101851e-06
Iter: 265 loss: 4.25206645e-06
Iter: 266 loss: 4.13609087e-06
Iter: 267 loss: 4.12343479e-06
Iter: 268 loss: 4.12303325e-06
Iter: 269 loss: 4.10918074e-06
Iter: 270 loss: 4.09811582e-06
Iter: 271 loss: 4.09419135e-06
Iter: 272 loss: 4.07708558e-06
Iter: 273 loss: 4.08369124e-06
Iter: 274 loss: 4.06530307e-06
Iter: 275 loss: 4.04717684e-06
Iter: 276 loss: 4.11949895e-06
Iter: 277 loss: 4.04316961e-06
Iter: 278 loss: 4.02271507e-06
Iter: 279 loss: 4.12983627e-06
Iter: 280 loss: 4.01968373e-06
Iter: 281 loss: 4.00821818e-06
Iter: 282 loss: 4.02666501e-06
Iter: 283 loss: 4.00296358e-06
Iter: 284 loss: 3.98871543e-06
Iter: 285 loss: 4.02578735e-06
Iter: 286 loss: 3.98407383e-06
Iter: 287 loss: 3.97136637e-06
Iter: 288 loss: 3.97093e-06
Iter: 289 loss: 3.96694759e-06
Iter: 290 loss: 3.95587722e-06
Iter: 291 loss: 4.02402748e-06
Iter: 292 loss: 3.952935e-06
Iter: 293 loss: 3.93784239e-06
Iter: 294 loss: 3.91446838e-06
Iter: 295 loss: 3.91400545e-06
Iter: 296 loss: 3.89377601e-06
Iter: 297 loss: 4.0160221e-06
Iter: 298 loss: 3.89126399e-06
Iter: 299 loss: 3.87094497e-06
Iter: 300 loss: 3.9571969e-06
Iter: 301 loss: 3.866734e-06
Iter: 302 loss: 3.85471e-06
Iter: 303 loss: 3.97915846e-06
Iter: 304 loss: 3.85444264e-06
Iter: 305 loss: 3.84093028e-06
Iter: 306 loss: 3.88128319e-06
Iter: 307 loss: 3.83684e-06
Iter: 308 loss: 3.82687722e-06
Iter: 309 loss: 3.82591679e-06
Iter: 310 loss: 3.81845257e-06
Iter: 311 loss: 3.80679148e-06
Iter: 312 loss: 3.8043795e-06
Iter: 313 loss: 3.79663743e-06
Iter: 314 loss: 3.77630067e-06
Iter: 315 loss: 3.8631506e-06
Iter: 316 loss: 3.77220454e-06
Iter: 317 loss: 3.7619252e-06
Iter: 318 loss: 3.89146226e-06
Iter: 319 loss: 3.76204525e-06
Iter: 320 loss: 3.75166474e-06
Iter: 321 loss: 3.73917192e-06
Iter: 322 loss: 3.73811713e-06
Iter: 323 loss: 3.76646972e-06
Iter: 324 loss: 3.73368425e-06
Iter: 325 loss: 3.73131707e-06
Iter: 326 loss: 3.72409204e-06
Iter: 327 loss: 3.74704359e-06
Iter: 328 loss: 3.72063641e-06
Iter: 329 loss: 3.71082615e-06
Iter: 330 loss: 3.69460622e-06
Iter: 331 loss: 3.69463623e-06
Iter: 332 loss: 3.67970301e-06
Iter: 333 loss: 3.88414e-06
Iter: 334 loss: 3.67951975e-06
Iter: 335 loss: 3.66999916e-06
Iter: 336 loss: 3.70070393e-06
Iter: 337 loss: 3.66739414e-06
Iter: 338 loss: 3.65659662e-06
Iter: 339 loss: 3.69453346e-06
Iter: 340 loss: 3.65356755e-06
Iter: 341 loss: 3.6418578e-06
Iter: 342 loss: 3.72448744e-06
Iter: 343 loss: 3.64078301e-06
Iter: 344 loss: 3.63582535e-06
Iter: 345 loss: 3.62777791e-06
Iter: 346 loss: 3.62773017e-06
Iter: 347 loss: 3.61544471e-06
Iter: 348 loss: 3.62940727e-06
Iter: 349 loss: 3.60888225e-06
Iter: 350 loss: 3.59753449e-06
Iter: 351 loss: 3.62382e-06
Iter: 352 loss: 3.59354908e-06
Iter: 353 loss: 3.58303237e-06
Iter: 354 loss: 3.73954549e-06
Iter: 355 loss: 3.58304715e-06
Iter: 356 loss: 3.57696626e-06
Iter: 357 loss: 3.61934735e-06
Iter: 358 loss: 3.57643967e-06
Iter: 359 loss: 3.56958503e-06
Iter: 360 loss: 3.60525382e-06
Iter: 361 loss: 3.56862301e-06
Iter: 362 loss: 3.56487817e-06
Iter: 363 loss: 3.55392876e-06
Iter: 364 loss: 3.60458534e-06
Iter: 365 loss: 3.5497817e-06
Iter: 366 loss: 3.54026497e-06
Iter: 367 loss: 3.61471257e-06
Iter: 368 loss: 3.53962241e-06
Iter: 369 loss: 3.5318883e-06
Iter: 370 loss: 3.522508e-06
Iter: 371 loss: 3.52188431e-06
Iter: 372 loss: 3.51245694e-06
Iter: 373 loss: 3.64922607e-06
Iter: 374 loss: 3.51241533e-06
Iter: 375 loss: 3.50545906e-06
Iter: 376 loss: 3.55265388e-06
Iter: 377 loss: 3.5047542e-06
Iter: 378 loss: 3.49782636e-06
Iter: 379 loss: 3.50668961e-06
Iter: 380 loss: 3.49424158e-06
Iter: 381 loss: 3.48781577e-06
Iter: 382 loss: 3.48591129e-06
Iter: 383 loss: 3.48228014e-06
Iter: 384 loss: 3.4715e-06
Iter: 385 loss: 3.48498224e-06
Iter: 386 loss: 3.46593833e-06
Iter: 387 loss: 3.45696458e-06
Iter: 388 loss: 3.46667321e-06
Iter: 389 loss: 3.45202102e-06
Iter: 390 loss: 3.44076057e-06
Iter: 391 loss: 3.52767165e-06
Iter: 392 loss: 3.43994884e-06
Iter: 393 loss: 3.44250748e-06
Iter: 394 loss: 3.43647616e-06
Iter: 395 loss: 3.43388638e-06
Iter: 396 loss: 3.42990097e-06
Iter: 397 loss: 3.42989165e-06
Iter: 398 loss: 3.42578e-06
Iter: 399 loss: 3.41657824e-06
Iter: 400 loss: 3.54171675e-06
Iter: 401 loss: 3.416043e-06
Iter: 402 loss: 3.40274096e-06
Iter: 403 loss: 3.42536669e-06
Iter: 404 loss: 3.39697658e-06
Iter: 405 loss: 3.38830205e-06
Iter: 406 loss: 3.41276336e-06
Iter: 407 loss: 3.3855722e-06
Iter: 408 loss: 3.37450751e-06
Iter: 409 loss: 3.41216401e-06
Iter: 410 loss: 3.37151869e-06
Iter: 411 loss: 3.36871062e-06
Iter: 412 loss: 3.36761877e-06
Iter: 413 loss: 3.36329e-06
Iter: 414 loss: 3.35536379e-06
Iter: 415 loss: 3.53851328e-06
Iter: 416 loss: 3.35541449e-06
Iter: 417 loss: 3.34731567e-06
Iter: 418 loss: 3.38082691e-06
Iter: 419 loss: 3.34567721e-06
Iter: 420 loss: 3.33865728e-06
Iter: 421 loss: 3.33771936e-06
Iter: 422 loss: 3.33257071e-06
Iter: 423 loss: 3.32341779e-06
Iter: 424 loss: 3.39608391e-06
Iter: 425 loss: 3.32273567e-06
Iter: 426 loss: 3.32421496e-06
Iter: 427 loss: 3.32051832e-06
Iter: 428 loss: 3.31798401e-06
Iter: 429 loss: 3.31610772e-06
Iter: 430 loss: 3.31544379e-06
Iter: 431 loss: 3.31172328e-06
Iter: 432 loss: 3.30443117e-06
Iter: 433 loss: 3.45409376e-06
Iter: 434 loss: 3.30441344e-06
Iter: 435 loss: 3.29641e-06
Iter: 436 loss: 3.33130311e-06
Iter: 437 loss: 3.29485306e-06
Iter: 438 loss: 3.28850274e-06
Iter: 439 loss: 3.27973521e-06
Iter: 440 loss: 3.27943667e-06
Iter: 441 loss: 3.26943473e-06
Iter: 442 loss: 3.38848213e-06
Iter: 443 loss: 3.26920872e-06
Iter: 444 loss: 3.26258714e-06
Iter: 445 loss: 3.26113718e-06
Iter: 446 loss: 3.25685073e-06
Iter: 447 loss: 3.24969096e-06
Iter: 448 loss: 3.33450271e-06
Iter: 449 loss: 3.24958592e-06
Iter: 450 loss: 3.24435e-06
Iter: 451 loss: 3.31429283e-06
Iter: 452 loss: 3.2443254e-06
Iter: 453 loss: 3.24044299e-06
Iter: 454 loss: 3.23248969e-06
Iter: 455 loss: 3.38867471e-06
Iter: 456 loss: 3.2324e-06
Iter: 457 loss: 3.22578649e-06
Iter: 458 loss: 3.26024292e-06
Iter: 459 loss: 3.22483038e-06
Iter: 460 loss: 3.21872471e-06
Iter: 461 loss: 3.22390542e-06
Iter: 462 loss: 3.21513176e-06
Iter: 463 loss: 3.21844755e-06
Iter: 464 loss: 3.21284756e-06
Iter: 465 loss: 3.21024208e-06
Iter: 466 loss: 3.20568597e-06
Iter: 467 loss: 3.20568279e-06
Iter: 468 loss: 3.20245e-06
Iter: 469 loss: 3.19541664e-06
Iter: 470 loss: 3.2988803e-06
Iter: 471 loss: 3.19512901e-06
Iter: 472 loss: 3.18526054e-06
Iter: 473 loss: 3.24815642e-06
Iter: 474 loss: 3.1841173e-06
Iter: 475 loss: 3.1789582e-06
Iter: 476 loss: 3.23730069e-06
Iter: 477 loss: 3.17891522e-06
Iter: 478 loss: 3.175252e-06
Iter: 479 loss: 3.17084641e-06
Iter: 480 loss: 3.17038575e-06
Iter: 481 loss: 3.16336559e-06
Iter: 482 loss: 3.19374112e-06
Iter: 483 loss: 3.16178102e-06
Iter: 484 loss: 3.15680654e-06
Iter: 485 loss: 3.17127433e-06
Iter: 486 loss: 3.15530792e-06
Iter: 487 loss: 3.15032207e-06
Iter: 488 loss: 3.20518802e-06
Iter: 489 loss: 3.15003695e-06
Iter: 490 loss: 3.14614545e-06
Iter: 491 loss: 3.14088493e-06
Iter: 492 loss: 3.14048702e-06
Iter: 493 loss: 3.13478677e-06
Iter: 494 loss: 3.1314371e-06
Iter: 495 loss: 3.12886891e-06
Iter: 496 loss: 3.11945132e-06
Iter: 497 loss: 3.19968285e-06
Iter: 498 loss: 3.11891722e-06
Iter: 499 loss: 3.11433519e-06
Iter: 500 loss: 3.11974691e-06
Iter: 501 loss: 3.11210033e-06
Iter: 502 loss: 3.11522854e-06
Iter: 503 loss: 3.11002941e-06
Iter: 504 loss: 3.10835094e-06
Iter: 505 loss: 3.10389464e-06
Iter: 506 loss: 3.13634064e-06
Iter: 507 loss: 3.10313499e-06
Iter: 508 loss: 3.09961365e-06
Iter: 509 loss: 3.09734469e-06
Iter: 510 loss: 3.09614961e-06
Iter: 511 loss: 3.09023108e-06
Iter: 512 loss: 3.11630674e-06
Iter: 513 loss: 3.0890169e-06
Iter: 514 loss: 3.08382459e-06
Iter: 515 loss: 3.10742917e-06
Iter: 516 loss: 3.08279573e-06
Iter: 517 loss: 3.0790543e-06
Iter: 518 loss: 3.07656569e-06
Iter: 519 loss: 3.07515597e-06
Iter: 520 loss: 3.06803349e-06
Iter: 521 loss: 3.09289362e-06
Iter: 522 loss: 3.06629727e-06
Iter: 523 loss: 3.0638339e-06
Iter: 524 loss: 3.06330662e-06
Iter: 525 loss: 3.0605911e-06
Iter: 526 loss: 3.05406638e-06
Iter: 527 loss: 3.12167322e-06
Iter: 528 loss: 3.05327762e-06
Iter: 529 loss: 3.04638206e-06
Iter: 530 loss: 3.09536063e-06
Iter: 531 loss: 3.04575246e-06
Iter: 532 loss: 3.04129367e-06
Iter: 533 loss: 3.03368824e-06
Iter: 534 loss: 3.03360116e-06
Iter: 535 loss: 3.02997387e-06
Iter: 536 loss: 3.02891317e-06
Iter: 537 loss: 3.02440708e-06
Iter: 538 loss: 3.0733579e-06
Iter: 539 loss: 3.02435501e-06
Iter: 540 loss: 3.02284957e-06
Iter: 541 loss: 3.018602e-06
Iter: 542 loss: 3.042858e-06
Iter: 543 loss: 3.01742239e-06
Iter: 544 loss: 3.01290856e-06
Iter: 545 loss: 3.03075331e-06
Iter: 546 loss: 3.01185582e-06
Iter: 547 loss: 3.0069009e-06
Iter: 548 loss: 3.00943657e-06
Iter: 549 loss: 3.00379952e-06
Iter: 550 loss: 2.99894964e-06
Iter: 551 loss: 3.06202674e-06
Iter: 552 loss: 2.99891508e-06
Iter: 553 loss: 2.99562475e-06
Iter: 554 loss: 2.99476869e-06
Iter: 555 loss: 2.99256499e-06
Iter: 556 loss: 2.98764e-06
Iter: 557 loss: 3.0017211e-06
Iter: 558 loss: 2.98593977e-06
Iter: 559 loss: 2.98198484e-06
Iter: 560 loss: 2.98203258e-06
Iter: 561 loss: 2.97984843e-06
Iter: 562 loss: 2.9772425e-06
Iter: 563 loss: 2.97692304e-06
Iter: 564 loss: 2.97255247e-06
Iter: 565 loss: 2.96865437e-06
Iter: 566 loss: 2.9676055e-06
Iter: 567 loss: 2.9617845e-06
Iter: 568 loss: 3.01190562e-06
Iter: 569 loss: 2.96154531e-06
Iter: 570 loss: 2.95856944e-06
Iter: 571 loss: 2.99070666e-06
Iter: 572 loss: 2.95855671e-06
Iter: 573 loss: 2.95472182e-06
Iter: 574 loss: 2.96211647e-06
Iter: 575 loss: 2.95303016e-06
Iter: 576 loss: 2.95126e-06
Iter: 577 loss: 2.94771735e-06
Iter: 578 loss: 3.01044201e-06
Iter: 579 loss: 2.94768961e-06
Iter: 580 loss: 2.94405481e-06
Iter: 581 loss: 2.9397529e-06
Iter: 582 loss: 2.93923244e-06
Iter: 583 loss: 2.93472908e-06
Iter: 584 loss: 2.9345606e-06
Iter: 585 loss: 2.93126118e-06
Iter: 586 loss: 2.93555149e-06
Iter: 587 loss: 2.92941058e-06
Iter: 588 loss: 2.924814e-06
Iter: 589 loss: 2.93411176e-06
Iter: 590 loss: 2.92283494e-06
Iter: 591 loss: 2.92017694e-06
Iter: 592 loss: 2.92024356e-06
Iter: 593 loss: 2.91798915e-06
Iter: 594 loss: 2.92138679e-06
Iter: 595 loss: 2.91688548e-06
Iter: 596 loss: 2.91405263e-06
Iter: 597 loss: 2.90997195e-06
Iter: 598 loss: 2.90971047e-06
Iter: 599 loss: 2.90622029e-06
Iter: 600 loss: 2.91428933e-06
Iter: 601 loss: 2.90485787e-06
Iter: 602 loss: 2.89997934e-06
Iter: 603 loss: 2.90790149e-06
Iter: 604 loss: 2.89756349e-06
Iter: 605 loss: 2.90043727e-06
Iter: 606 loss: 2.89646414e-06
Iter: 607 loss: 2.89492527e-06
Iter: 608 loss: 2.89295713e-06
Iter: 609 loss: 2.89275567e-06
Iter: 610 loss: 2.89127388e-06
Iter: 611 loss: 2.88712363e-06
Iter: 612 loss: 2.92458935e-06
Iter: 613 loss: 2.88642286e-06
Iter: 614 loss: 2.88168667e-06
Iter: 615 loss: 2.91121864e-06
Iter: 616 loss: 2.88126125e-06
Iter: 617 loss: 2.87723196e-06
Iter: 618 loss: 2.89569743e-06
Iter: 619 loss: 2.87649982e-06
Iter: 620 loss: 2.87230705e-06
Iter: 621 loss: 2.89089576e-06
Iter: 622 loss: 2.87151533e-06
Iter: 623 loss: 2.86835711e-06
Iter: 624 loss: 2.87520152e-06
Iter: 625 loss: 2.86702107e-06
Iter: 626 loss: 2.86376417e-06
Iter: 627 loss: 2.88922774e-06
Iter: 628 loss: 2.86353134e-06
Iter: 629 loss: 2.86032355e-06
Iter: 630 loss: 2.86500244e-06
Iter: 631 loss: 2.8586287e-06
Iter: 632 loss: 2.85590704e-06
Iter: 633 loss: 2.85526744e-06
Iter: 634 loss: 2.85352235e-06
Iter: 635 loss: 2.84958378e-06
Iter: 636 loss: 2.85557735e-06
Iter: 637 loss: 2.84762154e-06
Iter: 638 loss: 2.84412454e-06
Iter: 639 loss: 2.8653385e-06
Iter: 640 loss: 2.84382168e-06
Iter: 641 loss: 2.84377074e-06
Iter: 642 loss: 2.84241059e-06
Iter: 643 loss: 2.84138287e-06
Iter: 644 loss: 2.83816144e-06
Iter: 645 loss: 2.83793543e-06
Iter: 646 loss: 2.83471695e-06
Iter: 647 loss: 2.83023542e-06
Iter: 648 loss: 2.84297562e-06
Iter: 649 loss: 2.82878591e-06
Iter: 650 loss: 2.82373617e-06
Iter: 651 loss: 2.8370448e-06
Iter: 652 loss: 2.82211386e-06
Iter: 653 loss: 2.81851226e-06
Iter: 654 loss: 2.84855423e-06
Iter: 655 loss: 2.81834787e-06
Iter: 656 loss: 2.81511711e-06
Iter: 657 loss: 2.81900975e-06
Iter: 658 loss: 2.81347093e-06
Iter: 659 loss: 2.80947e-06
Iter: 660 loss: 2.8285865e-06
Iter: 661 loss: 2.80883387e-06
Iter: 662 loss: 2.80691711e-06
Iter: 663 loss: 2.80688073e-06
Iter: 664 loss: 2.80528457e-06
Iter: 665 loss: 2.80183326e-06
Iter: 666 loss: 2.86157092e-06
Iter: 667 loss: 2.80177142e-06
Iter: 668 loss: 2.79770347e-06
Iter: 669 loss: 2.81578627e-06
Iter: 670 loss: 2.79690857e-06
Iter: 671 loss: 2.79400456e-06
Iter: 672 loss: 2.79578444e-06
Iter: 673 loss: 2.79203823e-06
Iter: 674 loss: 2.78823427e-06
Iter: 675 loss: 2.80125255e-06
Iter: 676 loss: 2.78720881e-06
Iter: 677 loss: 2.7916642e-06
Iter: 678 loss: 2.78619382e-06
Iter: 679 loss: 2.78570405e-06
Iter: 680 loss: 2.78391644e-06
Iter: 681 loss: 2.78286075e-06
Iter: 682 loss: 2.78188463e-06
Iter: 683 loss: 2.7781316e-06
Iter: 684 loss: 2.78254606e-06
Iter: 685 loss: 2.7761746e-06
Iter: 686 loss: 2.77181857e-06
Iter: 687 loss: 2.79230244e-06
Iter: 688 loss: 2.7710621e-06
Iter: 689 loss: 2.7672595e-06
Iter: 690 loss: 2.76721903e-06
Iter: 691 loss: 2.76423407e-06
Iter: 692 loss: 2.76058313e-06
Iter: 693 loss: 2.80555309e-06
Iter: 694 loss: 2.76061974e-06
Iter: 695 loss: 2.75747379e-06
Iter: 696 loss: 2.76517039e-06
Iter: 697 loss: 2.75635239e-06
Iter: 698 loss: 2.75276534e-06
Iter: 699 loss: 2.76472929e-06
Iter: 700 loss: 2.75170328e-06
Iter: 701 loss: 2.74965851e-06
Iter: 702 loss: 2.7770152e-06
Iter: 703 loss: 2.74960598e-06
Iter: 704 loss: 2.74755303e-06
Iter: 705 loss: 2.74517379e-06
Iter: 706 loss: 2.74500371e-06
Iter: 707 loss: 2.74181366e-06
Iter: 708 loss: 2.75071648e-06
Iter: 709 loss: 2.74073659e-06
Iter: 710 loss: 2.73791761e-06
Iter: 711 loss: 2.73982823e-06
Iter: 712 loss: 2.73598653e-06
Iter: 713 loss: 2.73602814e-06
Iter: 714 loss: 2.7340318e-06
Iter: 715 loss: 2.73299611e-06
Iter: 716 loss: 2.73021169e-06
Iter: 717 loss: 2.73917794e-06
Iter: 718 loss: 2.72878083e-06
Iter: 719 loss: 2.72548596e-06
Iter: 720 loss: 2.72681177e-06
Iter: 721 loss: 2.723227e-06
Iter: 722 loss: 2.71908152e-06
Iter: 723 loss: 2.74742388e-06
Iter: 724 loss: 2.71857243e-06
Iter: 725 loss: 2.71617455e-06
Iter: 726 loss: 2.71758427e-06
Iter: 727 loss: 2.71469298e-06
Iter: 728 loss: 2.71090812e-06
Iter: 729 loss: 2.72312673e-06
Iter: 730 loss: 2.70983219e-06
Iter: 731 loss: 2.70702094e-06
Iter: 732 loss: 2.72228181e-06
Iter: 733 loss: 2.70657529e-06
Iter: 734 loss: 2.70324517e-06
Iter: 735 loss: 2.70561281e-06
Iter: 736 loss: 2.70101873e-06
Iter: 737 loss: 2.69858583e-06
Iter: 738 loss: 2.69857264e-06
Iter: 739 loss: 2.69664815e-06
Iter: 740 loss: 2.69913949e-06
Iter: 741 loss: 2.695695e-06
Iter: 742 loss: 2.69383872e-06
Iter: 743 loss: 2.69094244e-06
Iter: 744 loss: 2.69082102e-06
Iter: 745 loss: 2.69091947e-06
Iter: 746 loss: 2.68911708e-06
Iter: 747 loss: 2.68717304e-06
Iter: 748 loss: 2.68750955e-06
Iter: 749 loss: 2.68575377e-06
Iter: 750 loss: 2.68441136e-06
Iter: 751 loss: 2.68081567e-06
Iter: 752 loss: 2.71586578e-06
Iter: 753 loss: 2.68035274e-06
Iter: 754 loss: 2.67598034e-06
Iter: 755 loss: 2.68640792e-06
Iter: 756 loss: 2.67441101e-06
Iter: 757 loss: 2.67076484e-06
Iter: 758 loss: 2.69413e-06
Iter: 759 loss: 2.67038854e-06
Iter: 760 loss: 2.66698726e-06
Iter: 761 loss: 2.67209703e-06
Iter: 762 loss: 2.66556481e-06
Iter: 763 loss: 2.66233656e-06
Iter: 764 loss: 2.6811008e-06
Iter: 765 loss: 2.66186157e-06
Iter: 766 loss: 2.65897916e-06
Iter: 767 loss: 2.67067276e-06
Iter: 768 loss: 2.65856397e-06
Iter: 769 loss: 2.65596464e-06
Iter: 770 loss: 2.66786878e-06
Iter: 771 loss: 2.65550034e-06
Iter: 772 loss: 2.65336348e-06
Iter: 773 loss: 2.66425604e-06
Iter: 774 loss: 2.65296853e-06
Iter: 775 loss: 2.65078734e-06
Iter: 776 loss: 2.64632604e-06
Iter: 777 loss: 2.73950491e-06
Iter: 778 loss: 2.64635037e-06
Iter: 779 loss: 2.64411437e-06
Iter: 780 loss: 2.64404935e-06
Iter: 781 loss: 2.64352343e-06
Iter: 782 loss: 2.64313803e-06
Iter: 783 loss: 2.64243363e-06
Iter: 784 loss: 2.64042137e-06
Iter: 785 loss: 2.64211053e-06
Iter: 786 loss: 2.63875836e-06
Iter: 787 loss: 2.63601396e-06
Iter: 788 loss: 2.64607024e-06
Iter: 789 loss: 2.63540051e-06
Iter: 790 loss: 2.63284801e-06
Iter: 791 loss: 2.63215e-06
Iter: 792 loss: 2.63047195e-06
Iter: 793 loss: 2.62764706e-06
Iter: 794 loss: 2.64835171e-06
Iter: 795 loss: 2.6274588e-06
Iter: 796 loss: 2.62476397e-06
Iter: 797 loss: 2.62598951e-06
Iter: 798 loss: 2.62296589e-06
Iter: 799 loss: 2.62032154e-06
Iter: 800 loss: 2.65082667e-06
Iter: 801 loss: 2.62027e-06
Iter: 802 loss: 2.61817672e-06
Iter: 803 loss: 2.62216122e-06
Iter: 804 loss: 2.61732475e-06
Iter: 805 loss: 2.61477635e-06
Iter: 806 loss: 2.62381036e-06
Iter: 807 loss: 2.614e-06
Iter: 808 loss: 2.61125524e-06
Iter: 809 loss: 2.62779713e-06
Iter: 810 loss: 2.61096511e-06
Iter: 811 loss: 2.60920956e-06
Iter: 812 loss: 2.6086791e-06
Iter: 813 loss: 2.60777779e-06
Iter: 814 loss: 2.60618481e-06
Iter: 815 loss: 2.62631193e-06
Iter: 816 loss: 2.6061366e-06
Iter: 817 loss: 2.60449815e-06
Iter: 818 loss: 2.61103582e-06
Iter: 819 loss: 2.60401384e-06
Iter: 820 loss: 2.60342813e-06
Iter: 821 loss: 2.60172237e-06
Iter: 822 loss: 2.61263835e-06
Iter: 823 loss: 2.60132902e-06
Iter: 824 loss: 2.5987506e-06
Iter: 825 loss: 2.60308479e-06
Iter: 826 loss: 2.59757121e-06
Iter: 827 loss: 2.59486887e-06
Iter: 828 loss: 2.60406023e-06
Iter: 829 loss: 2.59415674e-06
Iter: 830 loss: 2.59157764e-06
Iter: 831 loss: 2.59329681e-06
Iter: 832 loss: 2.59010767e-06
Iter: 833 loss: 2.58756245e-06
Iter: 834 loss: 2.60324941e-06
Iter: 835 loss: 2.58737145e-06
Iter: 836 loss: 2.58487535e-06
Iter: 837 loss: 2.5858385e-06
Iter: 838 loss: 2.58325599e-06
Iter: 839 loss: 2.58175442e-06
Iter: 840 loss: 2.58154159e-06
Iter: 841 loss: 2.58019645e-06
Iter: 842 loss: 2.58259433e-06
Iter: 843 loss: 2.57965803e-06
Iter: 844 loss: 2.57775946e-06
Iter: 845 loss: 2.57902525e-06
Iter: 846 loss: 2.57643478e-06
Iter: 847 loss: 2.57441025e-06
Iter: 848 loss: 2.57727061e-06
Iter: 849 loss: 2.57336137e-06
Iter: 850 loss: 2.57294732e-06
Iter: 851 loss: 2.57262354e-06
Iter: 852 loss: 2.57151373e-06
Iter: 853 loss: 2.56947578e-06
Iter: 854 loss: 2.60414618e-06
Iter: 855 loss: 2.56951762e-06
Iter: 856 loss: 2.56795602e-06
Iter: 857 loss: 2.56729572e-06
Iter: 858 loss: 2.56656494e-06
Iter: 859 loss: 2.56472231e-06
Iter: 860 loss: 2.56592102e-06
Iter: 861 loss: 2.56364615e-06
Iter: 862 loss: 2.56065e-06
Iter: 863 loss: 2.56705653e-06
Iter: 864 loss: 2.55963369e-06
Iter: 865 loss: 2.55721034e-06
Iter: 866 loss: 2.57434181e-06
Iter: 867 loss: 2.55704936e-06
Iter: 868 loss: 2.55521854e-06
Iter: 869 loss: 2.55781652e-06
Iter: 870 loss: 2.55414966e-06
Iter: 871 loss: 2.55193663e-06
Iter: 872 loss: 2.55339455e-06
Iter: 873 loss: 2.55053374e-06
Iter: 874 loss: 2.54784118e-06
Iter: 875 loss: 2.56008025e-06
Iter: 876 loss: 2.54739916e-06
Iter: 877 loss: 2.54557904e-06
Iter: 878 loss: 2.54559473e-06
Iter: 879 loss: 2.54410315e-06
Iter: 880 loss: 2.54665611e-06
Iter: 881 loss: 2.54331053e-06
Iter: 882 loss: 2.54174938e-06
Iter: 883 loss: 2.54372321e-06
Iter: 884 loss: 2.54083398e-06
Iter: 885 loss: 2.53978101e-06
Iter: 886 loss: 2.53977032e-06
Iter: 887 loss: 2.53844655e-06
Iter: 888 loss: 2.53764438e-06
Iter: 889 loss: 2.53701819e-06
Iter: 890 loss: 2.53622511e-06
Iter: 891 loss: 2.5341069e-06
Iter: 892 loss: 2.55987561e-06
Iter: 893 loss: 2.53379631e-06
Iter: 894 loss: 2.53130634e-06
Iter: 895 loss: 2.54662882e-06
Iter: 896 loss: 2.53115468e-06
Iter: 897 loss: 2.52932182e-06
Iter: 898 loss: 2.52974496e-06
Iter: 899 loss: 2.52798054e-06
Iter: 900 loss: 2.52548534e-06
Iter: 901 loss: 2.53933649e-06
Iter: 902 loss: 2.52521249e-06
Iter: 903 loss: 2.52350765e-06
Iter: 904 loss: 2.52261634e-06
Iter: 905 loss: 2.52175414e-06
Iter: 906 loss: 2.51864572e-06
Iter: 907 loss: 2.53436565e-06
Iter: 908 loss: 2.51809388e-06
Iter: 909 loss: 2.51591723e-06
Iter: 910 loss: 2.53264739e-06
Iter: 911 loss: 2.51576125e-06
Iter: 912 loss: 2.51435108e-06
Iter: 913 loss: 2.5279669e-06
Iter: 914 loss: 2.51435495e-06
Iter: 915 loss: 2.51285019e-06
Iter: 916 loss: 2.51228539e-06
Iter: 917 loss: 2.51144297e-06
Iter: 918 loss: 2.50966355e-06
Iter: 919 loss: 2.51972688e-06
Iter: 920 loss: 2.50937728e-06
Iter: 921 loss: 2.50820062e-06
Iter: 922 loss: 2.50816e-06
Iter: 923 loss: 2.50758944e-06
Iter: 924 loss: 2.5057634e-06
Iter: 925 loss: 2.51109896e-06
Iter: 926 loss: 2.50503126e-06
Iter: 927 loss: 2.5026452e-06
Iter: 928 loss: 2.50713811e-06
Iter: 929 loss: 2.50174685e-06
Iter: 930 loss: 2.49955906e-06
Iter: 931 loss: 2.50564153e-06
Iter: 932 loss: 2.4988185e-06
Iter: 933 loss: 2.49672348e-06
Iter: 934 loss: 2.49982509e-06
Iter: 935 loss: 2.49558684e-06
Iter: 936 loss: 2.49309915e-06
Iter: 937 loss: 2.5016825e-06
Iter: 938 loss: 2.49241498e-06
Iter: 939 loss: 2.49003165e-06
Iter: 940 loss: 2.4977835e-06
Iter: 941 loss: 2.48928632e-06
Iter: 942 loss: 2.48715241e-06
Iter: 943 loss: 2.48878405e-06
Iter: 944 loss: 2.48575611e-06
Iter: 945 loss: 2.48283732e-06
Iter: 946 loss: 2.50108701e-06
Iter: 947 loss: 2.48243691e-06
Iter: 948 loss: 2.48094102e-06
Iter: 949 loss: 2.50185917e-06
Iter: 950 loss: 2.48095603e-06
Iter: 951 loss: 2.47963681e-06
Iter: 952 loss: 2.48114293e-06
Iter: 953 loss: 2.47882713e-06
Iter: 954 loss: 2.47739513e-06
Iter: 955 loss: 2.48104516e-06
Iter: 956 loss: 2.4767528e-06
Iter: 957 loss: 2.47493699e-06
Iter: 958 loss: 2.49679715e-06
Iter: 959 loss: 2.47490107e-06
Iter: 960 loss: 2.47432058e-06
Iter: 961 loss: 2.47335856e-06
Iter: 962 loss: 2.47337698e-06
Iter: 963 loss: 2.47210528e-06
Iter: 964 loss: 2.46947502e-06
Iter: 965 loss: 2.5173058e-06
Iter: 966 loss: 2.46946092e-06
Iter: 967 loss: 2.46678951e-06
Iter: 968 loss: 2.48425158e-06
Iter: 969 loss: 2.46649734e-06
Iter: 970 loss: 2.46399941e-06
Iter: 971 loss: 2.46709556e-06
Iter: 972 loss: 2.46272702e-06
Iter: 973 loss: 2.46045238e-06
Iter: 974 loss: 2.4734361e-06
Iter: 975 loss: 2.46015929e-06
Iter: 976 loss: 2.45823844e-06
Iter: 977 loss: 2.45903266e-06
Iter: 978 loss: 2.45695628e-06
Iter: 979 loss: 2.4540368e-06
Iter: 980 loss: 2.46619652e-06
Iter: 981 loss: 2.45361775e-06
Iter: 982 loss: 2.45142201e-06
Iter: 983 loss: 2.46599188e-06
Iter: 984 loss: 2.45132287e-06
Iter: 985 loss: 2.44961598e-06
Iter: 986 loss: 2.46427635e-06
Iter: 987 loss: 2.44962098e-06
Iter: 988 loss: 2.44832086e-06
Iter: 989 loss: 2.44942612e-06
Iter: 990 loss: 2.44761304e-06
Iter: 991 loss: 2.44696207e-06
Iter: 992 loss: 2.4467472e-06
Iter: 993 loss: 2.44614603e-06
Iter: 994 loss: 2.4447354e-06
Iter: 995 loss: 2.45634465e-06
Iter: 996 loss: 2.4444862e-06
Iter: 997 loss: 2.44307148e-06
Iter: 998 loss: 2.44642365e-06
Iter: 999 loss: 2.4426231e-06
Iter: 1000 loss: 2.44115381e-06
Iter: 1001 loss: 2.44217881e-06
Iter: 1002 loss: 2.44016451e-06
Iter: 1003 loss: 2.43832164e-06
Iter: 1004 loss: 2.4383703e-06
Iter: 1005 loss: 2.43691602e-06
Iter: 1006 loss: 2.4350602e-06
Iter: 1007 loss: 2.45621959e-06
Iter: 1008 loss: 2.43495833e-06
Iter: 1009 loss: 2.43335171e-06
Iter: 1010 loss: 2.43136265e-06
Iter: 1011 loss: 2.43122031e-06
Iter: 1012 loss: 2.42904343e-06
Iter: 1013 loss: 2.42897659e-06
Iter: 1014 loss: 2.42779561e-06
Iter: 1015 loss: 2.42799615e-06
Iter: 1016 loss: 2.42689475e-06
Iter: 1017 loss: 2.42526721e-06
Iter: 1018 loss: 2.44551961e-06
Iter: 1019 loss: 2.42523902e-06
Iter: 1020 loss: 2.42397391e-06
Iter: 1021 loss: 2.42973283e-06
Iter: 1022 loss: 2.42367037e-06
Iter: 1023 loss: 2.42299507e-06
Iter: 1024 loss: 2.43275372e-06
Iter: 1025 loss: 2.42302713e-06
Iter: 1026 loss: 2.42217902e-06
Iter: 1027 loss: 2.42159285e-06
Iter: 1028 loss: 2.42132592e-06
Iter: 1029 loss: 2.42054284e-06
Iter: 1030 loss: 2.41961152e-06
Iter: 1031 loss: 2.41953308e-06
Iter: 1032 loss: 2.41804923e-06
Iter: 1033 loss: 2.41982752e-06
Iter: 1034 loss: 2.41731823e-06
Iter: 1035 loss: 2.41572252e-06
Iter: 1036 loss: 2.42031228e-06
Iter: 1037 loss: 2.4152373e-06
Iter: 1038 loss: 2.41384396e-06
Iter: 1039 loss: 2.41532166e-06
Iter: 1040 loss: 2.41314e-06
Iter: 1041 loss: 2.41134012e-06
Iter: 1042 loss: 2.4149233e-06
Iter: 1043 loss: 2.41058024e-06
Iter: 1044 loss: 2.40875374e-06
Iter: 1045 loss: 2.41407633e-06
Iter: 1046 loss: 2.40813802e-06
Iter: 1047 loss: 2.40656482e-06
Iter: 1048 loss: 2.41426756e-06
Iter: 1049 loss: 2.4062897e-06
Iter: 1050 loss: 2.40471695e-06
Iter: 1051 loss: 2.40674808e-06
Iter: 1052 loss: 2.40384111e-06
Iter: 1053 loss: 2.40241661e-06
Iter: 1054 loss: 2.40241206e-06
Iter: 1055 loss: 2.40093277e-06
Iter: 1056 loss: 2.40151394e-06
Iter: 1057 loss: 2.39996075e-06
Iter: 1058 loss: 2.39946348e-06
Iter: 1059 loss: 2.39908377e-06
Iter: 1060 loss: 2.39860947e-06
Iter: 1061 loss: 2.39745759e-06
Iter: 1062 loss: 2.40756344e-06
Iter: 1063 loss: 2.39716564e-06
Iter: 1064 loss: 2.39585142e-06
Iter: 1065 loss: 2.39609744e-06
Iter: 1066 loss: 2.39487645e-06
Iter: 1067 loss: 2.39280143e-06
Iter: 1068 loss: 2.39773522e-06
Iter: 1069 loss: 2.39218616e-06
Iter: 1070 loss: 2.39068186e-06
Iter: 1071 loss: 2.39472683e-06
Iter: 1072 loss: 2.39011706e-06
Iter: 1073 loss: 2.38852317e-06
Iter: 1074 loss: 2.38758707e-06
Iter: 1075 loss: 2.38680332e-06
Iter: 1076 loss: 2.38478538e-06
Iter: 1077 loss: 2.4029539e-06
Iter: 1078 loss: 2.38459347e-06
Iter: 1079 loss: 2.38290477e-06
Iter: 1080 loss: 2.38339317e-06
Iter: 1081 loss: 2.38174357e-06
Iter: 1082 loss: 2.38008e-06
Iter: 1083 loss: 2.39090537e-06
Iter: 1084 loss: 2.37984455e-06
Iter: 1085 loss: 2.37810082e-06
Iter: 1086 loss: 2.38076291e-06
Iter: 1087 loss: 2.37734935e-06
Iter: 1088 loss: 2.37617542e-06
Iter: 1089 loss: 2.37621953e-06
Iter: 1090 loss: 2.3750913e-06
Iter: 1091 loss: 2.37691802e-06
Iter: 1092 loss: 2.37469567e-06
Iter: 1093 loss: 2.37413701e-06
Iter: 1094 loss: 2.37403037e-06
Iter: 1095 loss: 2.37339327e-06
Iter: 1096 loss: 2.37189965e-06
Iter: 1097 loss: 2.38883786e-06
Iter: 1098 loss: 2.37177937e-06
Iter: 1099 loss: 2.37061977e-06
Iter: 1100 loss: 2.37149084e-06
Iter: 1101 loss: 2.36991946e-06
Iter: 1102 loss: 2.3681564e-06
Iter: 1103 loss: 2.36941833e-06
Iter: 1104 loss: 2.3670118e-06
Iter: 1105 loss: 2.36539404e-06
Iter: 1106 loss: 2.37273616e-06
Iter: 1107 loss: 2.36502319e-06
Iter: 1108 loss: 2.36336155e-06
Iter: 1109 loss: 2.36382857e-06
Iter: 1110 loss: 2.36213782e-06
Iter: 1111 loss: 2.36006417e-06
Iter: 1112 loss: 2.37005247e-06
Iter: 1113 loss: 2.35977313e-06
Iter: 1114 loss: 2.35797052e-06
Iter: 1115 loss: 2.35769949e-06
Iter: 1116 loss: 2.35646849e-06
Iter: 1117 loss: 2.35366e-06
Iter: 1118 loss: 2.36501637e-06
Iter: 1119 loss: 2.35314269e-06
Iter: 1120 loss: 2.35103766e-06
Iter: 1121 loss: 2.35853531e-06
Iter: 1122 loss: 2.35046627e-06
Iter: 1123 loss: 2.34902109e-06
Iter: 1124 loss: 2.34899653e-06
Iter: 1125 loss: 2.34765912e-06
Iter: 1126 loss: 2.35073958e-06
Iter: 1127 loss: 2.34718073e-06
Iter: 1128 loss: 2.34613367e-06
Iter: 1129 loss: 2.34614117e-06
Iter: 1130 loss: 2.34561139e-06
Iter: 1131 loss: 2.34421e-06
Iter: 1132 loss: 2.35738548e-06
Iter: 1133 loss: 2.34401341e-06
Iter: 1134 loss: 2.34248137e-06
Iter: 1135 loss: 2.3423122e-06
Iter: 1136 loss: 2.3411144e-06
Iter: 1137 loss: 2.33909327e-06
Iter: 1138 loss: 2.35762468e-06
Iter: 1139 loss: 2.33901983e-06
Iter: 1140 loss: 2.33752303e-06
Iter: 1141 loss: 2.33696892e-06
Iter: 1142 loss: 2.33605169e-06
Iter: 1143 loss: 2.33389346e-06
Iter: 1144 loss: 2.3397688e-06
Iter: 1145 loss: 2.33315382e-06
Iter: 1146 loss: 2.33095125e-06
Iter: 1147 loss: 2.33296214e-06
Iter: 1148 loss: 2.32970956e-06
Iter: 1149 loss: 2.3274124e-06
Iter: 1150 loss: 2.3460309e-06
Iter: 1151 loss: 2.32723505e-06
Iter: 1152 loss: 2.32551565e-06
Iter: 1153 loss: 2.32313369e-06
Iter: 1154 loss: 2.3229818e-06
Iter: 1155 loss: 2.32113985e-06
Iter: 1156 loss: 2.32113416e-06
Iter: 1157 loss: 2.32015918e-06
Iter: 1158 loss: 2.32000366e-06
Iter: 1159 loss: 2.31937724e-06
Iter: 1160 loss: 2.32079583e-06
Iter: 1161 loss: 2.31905e-06
Iter: 1162 loss: 2.31793183e-06
Iter: 1163 loss: 2.31655804e-06
Iter: 1164 loss: 2.316478e-06
Iter: 1165 loss: 2.31514741e-06
Iter: 1166 loss: 2.31493232e-06
Iter: 1167 loss: 2.31407648e-06
Iter: 1168 loss: 2.31212061e-06
Iter: 1169 loss: 2.31543072e-06
Iter: 1170 loss: 2.31136619e-06
Iter: 1171 loss: 2.3095331e-06
Iter: 1172 loss: 2.31943e-06
Iter: 1173 loss: 2.30918795e-06
Iter: 1174 loss: 2.30782052e-06
Iter: 1175 loss: 2.31046988e-06
Iter: 1176 loss: 2.30718229e-06
Iter: 1177 loss: 2.30582054e-06
Iter: 1178 loss: 2.30510796e-06
Iter: 1179 loss: 2.30440105e-06
Iter: 1180 loss: 2.30217e-06
Iter: 1181 loss: 2.31371519e-06
Iter: 1182 loss: 2.30187516e-06
Iter: 1183 loss: 2.29995612e-06
Iter: 1184 loss: 2.30501178e-06
Iter: 1185 loss: 2.2993263e-06
Iter: 1186 loss: 2.29751822e-06
Iter: 1187 loss: 2.30089245e-06
Iter: 1188 loss: 2.29663237e-06
Iter: 1189 loss: 2.29534476e-06
Iter: 1190 loss: 2.29535362e-06
Iter: 1191 loss: 2.29407078e-06
Iter: 1192 loss: 2.30500154e-06
Iter: 1193 loss: 2.29399188e-06
Iter: 1194 loss: 2.29304601e-06
Iter: 1195 loss: 2.29653961e-06
Iter: 1196 loss: 2.29285365e-06
Iter: 1197 loss: 2.29223883e-06
Iter: 1198 loss: 2.29076431e-06
Iter: 1199 loss: 2.31115109e-06
Iter: 1200 loss: 2.29072202e-06
Iter: 1201 loss: 2.28910517e-06
Iter: 1202 loss: 2.29203079e-06
Iter: 1203 loss: 2.2885074e-06
Iter: 1204 loss: 2.28661247e-06
Iter: 1205 loss: 2.29539182e-06
Iter: 1206 loss: 2.28638305e-06
Iter: 1207 loss: 2.28516319e-06
Iter: 1208 loss: 2.28736189e-06
Iter: 1209 loss: 2.28460976e-06
Iter: 1210 loss: 2.28285671e-06
Iter: 1211 loss: 2.28168528e-06
Iter: 1212 loss: 2.28097747e-06
Iter: 1213 loss: 2.27899727e-06
Iter: 1214 loss: 2.28647059e-06
Iter: 1215 loss: 2.2784684e-06
Iter: 1216 loss: 2.27656983e-06
Iter: 1217 loss: 2.28587032e-06
Iter: 1218 loss: 2.27627197e-06
Iter: 1219 loss: 2.27499208e-06
Iter: 1220 loss: 2.27928717e-06
Iter: 1221 loss: 2.27466444e-06
Iter: 1222 loss: 2.27339478e-06
Iter: 1223 loss: 2.27802843e-06
Iter: 1224 loss: 2.27307123e-06
Iter: 1225 loss: 2.2727088e-06
Iter: 1226 loss: 2.27241458e-06
Iter: 1227 loss: 2.27200462e-06
Iter: 1228 loss: 2.27294959e-06
Iter: 1229 loss: 2.27179544e-06
Iter: 1230 loss: 2.2711838e-06
Iter: 1231 loss: 2.26988891e-06
Iter: 1232 loss: 2.28746489e-06
Iter: 1233 loss: 2.26973089e-06
Iter: 1234 loss: 2.26867e-06
Iter: 1235 loss: 2.26987549e-06
Iter: 1236 loss: 2.26799807e-06
Iter: 1237 loss: 2.26644693e-06
Iter: 1238 loss: 2.27247824e-06
Iter: 1239 loss: 2.26607517e-06
Iter: 1240 loss: 2.26463908e-06
Iter: 1241 loss: 2.26720704e-06
Iter: 1242 loss: 2.26414159e-06
Iter: 1243 loss: 2.26282691e-06
Iter: 1244 loss: 2.26791167e-06
Iter: 1245 loss: 2.26259522e-06
Iter: 1246 loss: 2.26125962e-06
Iter: 1247 loss: 2.26064e-06
Iter: 1248 loss: 2.25999338e-06
Iter: 1249 loss: 2.25828944e-06
Iter: 1250 loss: 2.26686461e-06
Iter: 1251 loss: 2.25785652e-06
Iter: 1252 loss: 2.2564775e-06
Iter: 1253 loss: 2.25770918e-06
Iter: 1254 loss: 2.25558119e-06
Iter: 1255 loss: 2.25362965e-06
Iter: 1256 loss: 2.26068892e-06
Iter: 1257 loss: 2.25305894e-06
Iter: 1258 loss: 2.25275676e-06
Iter: 1259 loss: 2.25233043e-06
Iter: 1260 loss: 2.25174e-06
Iter: 1261 loss: 2.254837e-06
Iter: 1262 loss: 2.25163058e-06
Iter: 1263 loss: 2.25118538e-06
Iter: 1264 loss: 2.25113e-06
Iter: 1265 loss: 2.25076701e-06
Iter: 1266 loss: 2.25017129e-06
Iter: 1267 loss: 2.24902078e-06
Iter: 1268 loss: 2.2489935e-06
Iter: 1269 loss: 2.24810128e-06
Iter: 1270 loss: 2.25245094e-06
Iter: 1271 loss: 2.24802579e-06
Iter: 1272 loss: 2.24685164e-06
Iter: 1273 loss: 2.24841483e-06
Iter: 1274 loss: 2.24629343e-06
Iter: 1275 loss: 2.2451618e-06
Iter: 1276 loss: 2.24837663e-06
Iter: 1277 loss: 2.24491441e-06
Iter: 1278 loss: 2.24354562e-06
Iter: 1279 loss: 2.24437167e-06
Iter: 1280 loss: 2.24280166e-06
Iter: 1281 loss: 2.2414622e-06
Iter: 1282 loss: 2.24940982e-06
Iter: 1283 loss: 2.24120549e-06
Iter: 1284 loss: 2.24018504e-06
Iter: 1285 loss: 2.23982443e-06
Iter: 1286 loss: 2.23916572e-06
Iter: 1287 loss: 2.23787765e-06
Iter: 1288 loss: 2.24181667e-06
Iter: 1289 loss: 2.23754819e-06
Iter: 1290 loss: 2.23637358e-06
Iter: 1291 loss: 2.24738096e-06
Iter: 1292 loss: 2.23626898e-06
Iter: 1293 loss: 2.23578877e-06
Iter: 1294 loss: 2.23570942e-06
Iter: 1295 loss: 2.23532334e-06
Iter: 1296 loss: 2.23511415e-06
Iter: 1297 loss: 2.23494612e-06
Iter: 1298 loss: 2.23438155e-06
Iter: 1299 loss: 2.23357824e-06
Iter: 1300 loss: 2.23359439e-06
Iter: 1301 loss: 2.23233792e-06
Iter: 1302 loss: 2.23157485e-06
Iter: 1303 loss: 2.23109646e-06
Iter: 1304 loss: 2.22973699e-06
Iter: 1305 loss: 2.24868654e-06
Iter: 1306 loss: 2.22976792e-06
Iter: 1307 loss: 2.22884455e-06
Iter: 1308 loss: 2.22969061e-06
Iter: 1309 loss: 2.22820699e-06
Iter: 1310 loss: 2.22690505e-06
Iter: 1311 loss: 2.22819722e-06
Iter: 1312 loss: 2.22625727e-06
Iter: 1313 loss: 2.22489598e-06
Iter: 1314 loss: 2.23030793e-06
Iter: 1315 loss: 2.22457879e-06
Iter: 1316 loss: 2.22331414e-06
Iter: 1317 loss: 2.22516837e-06
Iter: 1318 loss: 2.22278322e-06
Iter: 1319 loss: 2.22150038e-06
Iter: 1320 loss: 2.22489734e-06
Iter: 1321 loss: 2.22112544e-06
Iter: 1322 loss: 2.21994151e-06
Iter: 1323 loss: 2.22049766e-06
Iter: 1324 loss: 2.21913024e-06
Iter: 1325 loss: 2.22123845e-06
Iter: 1326 loss: 2.2187437e-06
Iter: 1327 loss: 2.21832852e-06
Iter: 1328 loss: 2.2178458e-06
Iter: 1329 loss: 2.21779419e-06
Iter: 1330 loss: 2.21703044e-06
Iter: 1331 loss: 2.21730215e-06
Iter: 1332 loss: 2.2164736e-06
Iter: 1333 loss: 2.21555092e-06
Iter: 1334 loss: 2.21526352e-06
Iter: 1335 loss: 2.21473852e-06
Iter: 1336 loss: 2.21378332e-06
Iter: 1337 loss: 2.21626397e-06
Iter: 1338 loss: 2.21352e-06
Iter: 1339 loss: 2.21241453e-06
Iter: 1340 loss: 2.21590926e-06
Iter: 1341 loss: 2.21210303e-06
Iter: 1342 loss: 2.21104051e-06
Iter: 1343 loss: 2.21426967e-06
Iter: 1344 loss: 2.21080063e-06
Iter: 1345 loss: 2.20982929e-06
Iter: 1346 loss: 2.20944639e-06
Iter: 1347 loss: 2.20883339e-06
Iter: 1348 loss: 2.2072029e-06
Iter: 1349 loss: 2.2141935e-06
Iter: 1350 loss: 2.20684888e-06
Iter: 1351 loss: 2.20560173e-06
Iter: 1352 loss: 2.2081681e-06
Iter: 1353 loss: 2.20511265e-06
Iter: 1354 loss: 2.20381298e-06
Iter: 1355 loss: 2.20588936e-06
Iter: 1356 loss: 2.20320726e-06
Iter: 1357 loss: 2.20268907e-06
Iter: 1358 loss: 2.20257448e-06
Iter: 1359 loss: 2.20174184e-06
Iter: 1360 loss: 2.20233892e-06
Iter: 1361 loss: 2.20127185e-06
Iter: 1362 loss: 2.20058701e-06
Iter: 1363 loss: 2.20239122e-06
Iter: 1364 loss: 2.20036054e-06
Iter: 1365 loss: 2.19967251e-06
Iter: 1366 loss: 2.19868934e-06
Iter: 1367 loss: 2.19854292e-06
Iter: 1368 loss: 2.19730509e-06
Iter: 1369 loss: 2.19783578e-06
Iter: 1370 loss: 2.19653134e-06
Iter: 1371 loss: 2.1948606e-06
Iter: 1372 loss: 2.20235461e-06
Iter: 1373 loss: 2.19459457e-06
Iter: 1374 loss: 2.19303911e-06
Iter: 1375 loss: 2.19788626e-06
Iter: 1376 loss: 2.19257936e-06
Iter: 1377 loss: 2.19117783e-06
Iter: 1378 loss: 2.19360618e-06
Iter: 1379 loss: 2.19052345e-06
Iter: 1380 loss: 2.18925243e-06
Iter: 1381 loss: 2.19217509e-06
Iter: 1382 loss: 2.18876244e-06
Iter: 1383 loss: 2.1872429e-06
Iter: 1384 loss: 2.18960258e-06
Iter: 1385 loss: 2.18641799e-06
Iter: 1386 loss: 2.18475361e-06
Iter: 1387 loss: 2.19144067e-06
Iter: 1388 loss: 2.18433206e-06
Iter: 1389 loss: 2.18293962e-06
Iter: 1390 loss: 2.18151945e-06
Iter: 1391 loss: 2.18131186e-06
Iter: 1392 loss: 2.18533864e-06
Iter: 1393 loss: 2.18060904e-06
Iter: 1394 loss: 2.18003515e-06
Iter: 1395 loss: 2.17893785e-06
Iter: 1396 loss: 2.19439562e-06
Iter: 1397 loss: 2.17887646e-06
Iter: 1398 loss: 2.17794e-06
Iter: 1399 loss: 2.18668401e-06
Iter: 1400 loss: 2.17785646e-06
Iter: 1401 loss: 2.17716683e-06
Iter: 1402 loss: 2.17532624e-06
Iter: 1403 loss: 2.1944611e-06
Iter: 1404 loss: 2.17509341e-06
Iter: 1405 loss: 2.17290199e-06
Iter: 1406 loss: 2.18033665e-06
Iter: 1407 loss: 2.17229672e-06
Iter: 1408 loss: 2.17081879e-06
Iter: 1409 loss: 2.18941705e-06
Iter: 1410 loss: 2.17082766e-06
Iter: 1411 loss: 2.16947092e-06
Iter: 1412 loss: 2.16849958e-06
Iter: 1413 loss: 2.16800368e-06
Iter: 1414 loss: 2.16618514e-06
Iter: 1415 loss: 2.16888975e-06
Iter: 1416 loss: 2.16530088e-06
Iter: 1417 loss: 2.16312492e-06
Iter: 1418 loss: 2.16791159e-06
Iter: 1419 loss: 2.16223953e-06
Iter: 1420 loss: 2.16049557e-06
Iter: 1421 loss: 2.16871422e-06
Iter: 1422 loss: 2.16008561e-06
Iter: 1423 loss: 2.15799855e-06
Iter: 1424 loss: 2.16077819e-06
Iter: 1425 loss: 2.15697719e-06
Iter: 1426 loss: 2.15535192e-06
Iter: 1427 loss: 2.16390936e-06
Iter: 1428 loss: 2.1552205e-06
Iter: 1429 loss: 2.15507589e-06
Iter: 1430 loss: 2.15447199e-06
Iter: 1431 loss: 2.15407226e-06
Iter: 1432 loss: 2.15295586e-06
Iter: 1433 loss: 2.15722321e-06
Iter: 1434 loss: 2.15237355e-06
Iter: 1435 loss: 2.15141108e-06
Iter: 1436 loss: 2.15144132e-06
Iter: 1437 loss: 2.15064983e-06
Iter: 1438 loss: 2.14882198e-06
Iter: 1439 loss: 2.17868865e-06
Iter: 1440 loss: 2.14867896e-06
Iter: 1441 loss: 2.14720558e-06
Iter: 1442 loss: 2.15136879e-06
Iter: 1443 loss: 2.14670399e-06
Iter: 1444 loss: 2.14532224e-06
Iter: 1445 loss: 2.15062119e-06
Iter: 1446 loss: 2.14496345e-06
Iter: 1447 loss: 2.14328338e-06
Iter: 1448 loss: 2.14425427e-06
Iter: 1449 loss: 2.14221473e-06
Iter: 1450 loss: 2.14044576e-06
Iter: 1451 loss: 2.15060982e-06
Iter: 1452 loss: 2.14028591e-06
Iter: 1453 loss: 2.13873773e-06
Iter: 1454 loss: 2.13839667e-06
Iter: 1455 loss: 2.13754e-06
Iter: 1456 loss: 2.13568501e-06
Iter: 1457 loss: 2.14020838e-06
Iter: 1458 loss: 2.13499061e-06
Iter: 1459 loss: 2.13339399e-06
Iter: 1460 loss: 2.13590943e-06
Iter: 1461 loss: 2.1326905e-06
Iter: 1462 loss: 2.13134149e-06
Iter: 1463 loss: 2.15411274e-06
Iter: 1464 loss: 2.13127987e-06
Iter: 1465 loss: 2.12969553e-06
Iter: 1466 loss: 2.1364242e-06
Iter: 1467 loss: 2.12938289e-06
Iter: 1468 loss: 2.12886357e-06
Iter: 1469 loss: 2.12751138e-06
Iter: 1470 loss: 2.1407543e-06
Iter: 1471 loss: 2.12741315e-06
Iter: 1472 loss: 2.12594068e-06
Iter: 1473 loss: 2.14593388e-06
Iter: 1474 loss: 2.12593159e-06
Iter: 1475 loss: 2.12484429e-06
Iter: 1476 loss: 2.12391797e-06
Iter: 1477 loss: 2.1235021e-06
Iter: 1478 loss: 2.12217083e-06
Iter: 1479 loss: 2.12098257e-06
Iter: 1480 loss: 2.12047462e-06
Iter: 1481 loss: 2.11879365e-06
Iter: 1482 loss: 2.14099191e-06
Iter: 1483 loss: 2.11882366e-06
Iter: 1484 loss: 2.11720135e-06
Iter: 1485 loss: 2.12302075e-06
Iter: 1486 loss: 2.11681254e-06
Iter: 1487 loss: 2.11519227e-06
Iter: 1488 loss: 2.11582983e-06
Iter: 1489 loss: 2.11406632e-06
Iter: 1490 loss: 2.11262227e-06
Iter: 1491 loss: 2.12076975e-06
Iter: 1492 loss: 2.11244537e-06
Iter: 1493 loss: 2.11119777e-06
Iter: 1494 loss: 2.10979374e-06
Iter: 1495 loss: 2.10961252e-06
Iter: 1496 loss: 2.10782969e-06
Iter: 1497 loss: 2.13354269e-06
Iter: 1498 loss: 2.10784e-06
Iter: 1499 loss: 2.1082426e-06
Iter: 1500 loss: 2.1074768e-06
Iter: 1501 loss: 2.10716394e-06
Iter: 1502 loss: 2.1060398e-06
Iter: 1503 loss: 2.10789267e-06
Iter: 1504 loss: 2.10534904e-06
Iter: 1505 loss: 2.10387884e-06
Iter: 1506 loss: 2.11474412e-06
Iter: 1507 loss: 2.1038054e-06
Iter: 1508 loss: 2.10266671e-06
Iter: 1509 loss: 2.11303e-06
Iter: 1510 loss: 2.10264125e-06
Iter: 1511 loss: 2.10202415e-06
Iter: 1512 loss: 2.10083e-06
Iter: 1513 loss: 2.1228243e-06
Iter: 1514 loss: 2.10075768e-06
Iter: 1515 loss: 2.09905238e-06
Iter: 1516 loss: 2.10294024e-06
Iter: 1517 loss: 2.09859695e-06
Iter: 1518 loss: 2.0969328e-06
Iter: 1519 loss: 2.10248209e-06
Iter: 1520 loss: 2.09661675e-06
Iter: 1521 loss: 2.09481141e-06
Iter: 1522 loss: 2.10128042e-06
Iter: 1523 loss: 2.0943146e-06
Iter: 1524 loss: 2.09275458e-06
Iter: 1525 loss: 2.09375162e-06
Iter: 1526 loss: 2.09160225e-06
Iter: 1527 loss: 2.08996812e-06
Iter: 1528 loss: 2.09032964e-06
Iter: 1529 loss: 2.08877e-06
Iter: 1530 loss: 2.0869461e-06
Iter: 1531 loss: 2.10155986e-06
Iter: 1532 loss: 2.08683127e-06
Iter: 1533 loss: 2.08546453e-06
Iter: 1534 loss: 2.08788e-06
Iter: 1535 loss: 2.08480651e-06
Iter: 1536 loss: 2.08491474e-06
Iter: 1537 loss: 2.08400525e-06
Iter: 1538 loss: 2.08356823e-06
Iter: 1539 loss: 2.08259371e-06
Iter: 1540 loss: 2.08497795e-06
Iter: 1541 loss: 2.08201686e-06
Iter: 1542 loss: 2.08042479e-06
Iter: 1543 loss: 2.08239703e-06
Iter: 1544 loss: 2.07961398e-06
Iter: 1545 loss: 2.07914786e-06
Iter: 1546 loss: 2.07877883e-06
Iter: 1547 loss: 2.0780858e-06
Iter: 1548 loss: 2.07744279e-06
Iter: 1549 loss: 2.07734752e-06
Iter: 1550 loss: 2.07616722e-06
Iter: 1551 loss: 2.07555513e-06
Iter: 1552 loss: 2.07505718e-06
Iter: 1553 loss: 2.07364678e-06
Iter: 1554 loss: 2.08091524e-06
Iter: 1555 loss: 2.07335643e-06
Iter: 1556 loss: 2.07183393e-06
Iter: 1557 loss: 2.07111771e-06
Iter: 1558 loss: 2.07042558e-06
Iter: 1559 loss: 2.06859795e-06
Iter: 1560 loss: 2.08161555e-06
Iter: 1561 loss: 2.06841469e-06
Iter: 1562 loss: 2.06672803e-06
Iter: 1563 loss: 2.06777304e-06
Iter: 1564 loss: 2.06553364e-06
Iter: 1565 loss: 2.06406639e-06
Iter: 1566 loss: 2.08750203e-06
Iter: 1567 loss: 2.06408322e-06
Iter: 1568 loss: 2.0630805e-06
Iter: 1569 loss: 2.06725781e-06
Iter: 1570 loss: 2.06289747e-06
Iter: 1571 loss: 2.06205777e-06
Iter: 1572 loss: 2.0620505e-06
Iter: 1573 loss: 2.06156528e-06
Iter: 1574 loss: 2.06003665e-06
Iter: 1575 loss: 2.06005961e-06
Iter: 1576 loss: 2.05827337e-06
Iter: 1577 loss: 2.05596643e-06
Iter: 1578 loss: 2.06128675e-06
Iter: 1579 loss: 2.05517654e-06
Iter: 1580 loss: 2.05343667e-06
Iter: 1581 loss: 2.07989956e-06
Iter: 1582 loss: 2.05342803e-06
Iter: 1583 loss: 2.05150855e-06
Iter: 1584 loss: 2.0579937e-06
Iter: 1585 loss: 2.05101037e-06
Iter: 1586 loss: 2.0499433e-06
Iter: 1587 loss: 2.04922935e-06
Iter: 1588 loss: 2.04881076e-06
Iter: 1589 loss: 2.04686239e-06
Iter: 1590 loss: 2.04854359e-06
Iter: 1591 loss: 2.04571256e-06
Iter: 1592 loss: 2.04363459e-06
Iter: 1593 loss: 2.05354036e-06
Iter: 1594 loss: 2.04324124e-06
Iter: 1595 loss: 2.04131493e-06
Iter: 1596 loss: 2.04269031e-06
Iter: 1597 loss: 2.04018033e-06
Iter: 1598 loss: 2.03816262e-06
Iter: 1599 loss: 2.04491971e-06
Iter: 1600 loss: 2.03763761e-06
Iter: 1601 loss: 2.03566333e-06
Iter: 1602 loss: 2.04547496e-06
Iter: 1603 loss: 2.03530794e-06
Iter: 1604 loss: 2.03458e-06
Iter: 1605 loss: 2.03441095e-06
Iter: 1606 loss: 2.03354853e-06
Iter: 1607 loss: 2.0376242e-06
Iter: 1608 loss: 2.03344621e-06
Iter: 1609 loss: 2.03295781e-06
Iter: 1610 loss: 2.0313073e-06
Iter: 1611 loss: 2.03716718e-06
Iter: 1612 loss: 2.03047694e-06
Iter: 1613 loss: 2.02830188e-06
Iter: 1614 loss: 2.04134426e-06
Iter: 1615 loss: 2.02805109e-06
Iter: 1616 loss: 2.02668616e-06
Iter: 1617 loss: 2.02509818e-06
Iter: 1618 loss: 2.02502474e-06
Iter: 1619 loss: 2.02433489e-06
Iter: 1620 loss: 2.02375622e-06
Iter: 1621 loss: 2.02269325e-06
Iter: 1622 loss: 2.02354522e-06
Iter: 1623 loss: 2.02206934e-06
Iter: 1624 loss: 2.02084493e-06
Iter: 1625 loss: 2.01895909e-06
Iter: 1626 loss: 2.01898638e-06
Iter: 1627 loss: 2.01748458e-06
Iter: 1628 loss: 2.02790397e-06
Iter: 1629 loss: 2.01740249e-06
Iter: 1630 loss: 2.01576927e-06
Iter: 1631 loss: 2.0152811e-06
Iter: 1632 loss: 2.01423154e-06
Iter: 1633 loss: 2.01191529e-06
Iter: 1634 loss: 2.03015452e-06
Iter: 1635 loss: 2.01178341e-06
Iter: 1636 loss: 2.01018156e-06
Iter: 1637 loss: 2.00926934e-06
Iter: 1638 loss: 2.00858972e-06
Iter: 1639 loss: 2.00721433e-06
Iter: 1640 loss: 2.00719751e-06
Iter: 1641 loss: 2.00582804e-06
Iter: 1642 loss: 2.02569299e-06
Iter: 1643 loss: 2.00588806e-06
Iter: 1644 loss: 2.00566478e-06
Iter: 1645 loss: 2.00471186e-06
Iter: 1646 loss: 2.0068228e-06
Iter: 1647 loss: 2.00407567e-06
Iter: 1648 loss: 2.00241402e-06
Iter: 1649 loss: 2.0111886e-06
Iter: 1650 loss: 2.00214117e-06
Iter: 1651 loss: 2.00112117e-06
Iter: 1652 loss: 2.00150339e-06
Iter: 1653 loss: 2.00026329e-06
Iter: 1654 loss: 1.99950182e-06
Iter: 1655 loss: 1.99948659e-06
Iter: 1656 loss: 1.99866076e-06
Iter: 1657 loss: 1.99783449e-06
Iter: 1658 loss: 1.99786473e-06
Iter: 1659 loss: 1.99647548e-06
Iter: 1660 loss: 1.99725764e-06
Iter: 1661 loss: 1.99580063e-06
Iter: 1662 loss: 1.99422584e-06
Iter: 1663 loss: 1.99666215e-06
Iter: 1664 loss: 1.99342776e-06
Iter: 1665 loss: 1.99189935e-06
Iter: 1666 loss: 1.99590886e-06
Iter: 1667 loss: 1.99128317e-06
Iter: 1668 loss: 1.98946645e-06
Iter: 1669 loss: 1.99647275e-06
Iter: 1670 loss: 1.98900398e-06
Iter: 1671 loss: 1.98805856e-06
Iter: 1672 loss: 1.98800149e-06
Iter: 1673 loss: 1.98752468e-06
Iter: 1674 loss: 1.98752809e-06
Iter: 1675 loss: 1.98716066e-06
Iter: 1676 loss: 1.98613202e-06
Iter: 1677 loss: 1.98537782e-06
Iter: 1678 loss: 1.98476846e-06
Iter: 1679 loss: 1.98349949e-06
Iter: 1680 loss: 1.98346856e-06
Iter: 1681 loss: 1.98268731e-06
Iter: 1682 loss: 1.9822412e-06
Iter: 1683 loss: 1.98189628e-06
Iter: 1684 loss: 1.98054204e-06
Iter: 1685 loss: 1.98254361e-06
Iter: 1686 loss: 1.97982035e-06
Iter: 1687 loss: 1.97881582e-06
Iter: 1688 loss: 1.97865734e-06
Iter: 1689 loss: 1.97817621e-06
Iter: 1690 loss: 1.97688973e-06
Iter: 1691 loss: 1.99173905e-06
Iter: 1692 loss: 1.97676877e-06
Iter: 1693 loss: 1.97530198e-06
Iter: 1694 loss: 1.97915642e-06
Iter: 1695 loss: 1.97485792e-06
Iter: 1696 loss: 1.9734257e-06
Iter: 1697 loss: 1.97682107e-06
Iter: 1698 loss: 1.97293934e-06
Iter: 1699 loss: 1.97148847e-06
Iter: 1700 loss: 1.975141e-06
Iter: 1701 loss: 1.97112558e-06
Iter: 1702 loss: 1.96974383e-06
Iter: 1703 loss: 1.97974805e-06
Iter: 1704 loss: 1.9697186e-06
Iter: 1705 loss: 1.96958399e-06
Iter: 1706 loss: 1.9691829e-06
Iter: 1707 loss: 1.96886458e-06
Iter: 1708 loss: 1.96767064e-06
Iter: 1709 loss: 1.97509962e-06
Iter: 1710 loss: 1.96742894e-06
Iter: 1711 loss: 1.9664144e-06
Iter: 1712 loss: 1.96930728e-06
Iter: 1713 loss: 1.96619862e-06
Iter: 1714 loss: 1.96510496e-06
Iter: 1715 loss: 1.96727524e-06
Iter: 1716 loss: 1.96456085e-06
Iter: 1717 loss: 1.96331439e-06
Iter: 1718 loss: 1.96595443e-06
Iter: 1719 loss: 1.96271458e-06
Iter: 1720 loss: 1.96193832e-06
Iter: 1721 loss: 1.96194287e-06
Iter: 1722 loss: 1.96104338e-06
Iter: 1723 loss: 1.96005203e-06
Iter: 1724 loss: 1.95998359e-06
Iter: 1725 loss: 1.95866733e-06
Iter: 1726 loss: 1.96085784e-06
Iter: 1727 loss: 1.95806933e-06
Iter: 1728 loss: 1.95664484e-06
Iter: 1729 loss: 1.95710618e-06
Iter: 1730 loss: 1.9556046e-06
Iter: 1731 loss: 1.95384655e-06
Iter: 1732 loss: 1.96399742e-06
Iter: 1733 loss: 1.95353414e-06
Iter: 1734 loss: 1.95225721e-06
Iter: 1735 loss: 1.95278972e-06
Iter: 1736 loss: 1.95139182e-06
Iter: 1737 loss: 1.95244183e-06
Iter: 1738 loss: 1.95071334e-06
Iter: 1739 loss: 1.95009852e-06
Iter: 1740 loss: 1.94914742e-06
Iter: 1741 loss: 1.94910126e-06
Iter: 1742 loss: 1.94843324e-06
Iter: 1743 loss: 1.94804397e-06
Iter: 1744 loss: 1.94773565e-06
Iter: 1745 loss: 1.94644053e-06
Iter: 1746 loss: 1.94766926e-06
Iter: 1747 loss: 1.94560835e-06
Iter: 1748 loss: 1.94397126e-06
Iter: 1749 loss: 1.95970733e-06
Iter: 1750 loss: 1.94396e-06
Iter: 1751 loss: 1.94286554e-06
Iter: 1752 loss: 1.94511927e-06
Iter: 1753 loss: 1.94239828e-06
Iter: 1754 loss: 1.94126096e-06
Iter: 1755 loss: 1.95061875e-06
Iter: 1756 loss: 1.94120958e-06
Iter: 1757 loss: 1.94017935e-06
Iter: 1758 loss: 1.93907772e-06
Iter: 1759 loss: 1.93891174e-06
Iter: 1760 loss: 1.93759206e-06
Iter: 1761 loss: 1.93908181e-06
Iter: 1762 loss: 1.93694314e-06
Iter: 1763 loss: 1.93566302e-06
Iter: 1764 loss: 1.94183076e-06
Iter: 1765 loss: 1.93547044e-06
Iter: 1766 loss: 1.93409528e-06
Iter: 1767 loss: 1.93480923e-06
Iter: 1768 loss: 1.93317283e-06
Iter: 1769 loss: 1.93241e-06
Iter: 1770 loss: 1.9323561e-06
Iter: 1771 loss: 1.93149936e-06
Iter: 1772 loss: 1.93756614e-06
Iter: 1773 loss: 1.93142387e-06
Iter: 1774 loss: 1.93098936e-06
Iter: 1775 loss: 1.92994844e-06
Iter: 1776 loss: 1.93217147e-06
Iter: 1777 loss: 1.92919606e-06
Iter: 1778 loss: 1.92781499e-06
Iter: 1779 loss: 1.93927622e-06
Iter: 1780 loss: 1.92776542e-06
Iter: 1781 loss: 1.92674361e-06
Iter: 1782 loss: 1.93122742e-06
Iter: 1783 loss: 1.92645075e-06
Iter: 1784 loss: 1.92536618e-06
Iter: 1785 loss: 1.92732477e-06
Iter: 1786 loss: 1.92483867e-06
Iter: 1787 loss: 1.92370317e-06
Iter: 1788 loss: 1.93460323e-06
Iter: 1789 loss: 1.92369157e-06
Iter: 1790 loss: 1.92294533e-06
Iter: 1791 loss: 1.92504081e-06
Iter: 1792 loss: 1.92267225e-06
Iter: 1793 loss: 1.9219558e-06
Iter: 1794 loss: 1.92072525e-06
Iter: 1795 loss: 1.94949735e-06
Iter: 1796 loss: 1.92076573e-06
Iter: 1797 loss: 1.91934805e-06
Iter: 1798 loss: 1.92200741e-06
Iter: 1799 loss: 1.91879349e-06
Iter: 1800 loss: 1.91711274e-06
Iter: 1801 loss: 1.91900108e-06
Iter: 1802 loss: 1.91624304e-06
Iter: 1803 loss: 1.91527624e-06
Iter: 1804 loss: 1.91523395e-06
Iter: 1805 loss: 1.91470622e-06
Iter: 1806 loss: 1.91472782e-06
Iter: 1807 loss: 1.91430422e-06
Iter: 1808 loss: 1.91309482e-06
Iter: 1809 loss: 1.91525714e-06
Iter: 1810 loss: 1.91227127e-06
Iter: 1811 loss: 1.91067329e-06
Iter: 1812 loss: 1.91617755e-06
Iter: 1813 loss: 1.91024446e-06
Iter: 1814 loss: 1.90887158e-06
Iter: 1815 loss: 1.91293634e-06
Iter: 1816 loss: 1.90843866e-06
Iter: 1817 loss: 1.90700302e-06
Iter: 1818 loss: 1.91667141e-06
Iter: 1819 loss: 1.90683431e-06
Iter: 1820 loss: 1.9056821e-06
Iter: 1821 loss: 1.90824699e-06
Iter: 1822 loss: 1.90530102e-06
Iter: 1823 loss: 1.90401011e-06
Iter: 1824 loss: 1.91198069e-06
Iter: 1825 loss: 1.90380831e-06
Iter: 1826 loss: 1.90283777e-06
Iter: 1827 loss: 1.90267065e-06
Iter: 1828 loss: 1.90200103e-06
Iter: 1829 loss: 1.90079675e-06
Iter: 1830 loss: 1.90109313e-06
Iter: 1831 loss: 1.8997406e-06
Iter: 1832 loss: 1.89796106e-06
Iter: 1833 loss: 1.90148421e-06
Iter: 1834 loss: 1.89718776e-06
Iter: 1835 loss: 1.89568777e-06
Iter: 1836 loss: 1.90166247e-06
Iter: 1837 loss: 1.89529862e-06
Iter: 1838 loss: 1.89626905e-06
Iter: 1839 loss: 1.89479715e-06
Iter: 1840 loss: 1.89439913e-06
Iter: 1841 loss: 1.8931172e-06
Iter: 1842 loss: 1.89936316e-06
Iter: 1843 loss: 1.8927085e-06
Iter: 1844 loss: 1.89175262e-06
Iter: 1845 loss: 1.89331217e-06
Iter: 1846 loss: 1.89131299e-06
Iter: 1847 loss: 1.89022069e-06
Iter: 1848 loss: 1.8905148e-06
Iter: 1849 loss: 1.88939271e-06
Iter: 1850 loss: 1.88827016e-06
Iter: 1851 loss: 1.90408605e-06
Iter: 1852 loss: 1.88829563e-06
Iter: 1853 loss: 1.88734e-06
Iter: 1854 loss: 1.89048319e-06
Iter: 1855 loss: 1.88712193e-06
Iter: 1856 loss: 1.88635568e-06
Iter: 1857 loss: 1.88993818e-06
Iter: 1858 loss: 1.88628235e-06
Iter: 1859 loss: 1.885458e-06
Iter: 1860 loss: 1.88521324e-06
Iter: 1861 loss: 1.88464833e-06
Iter: 1862 loss: 1.88355739e-06
Iter: 1863 loss: 1.88441959e-06
Iter: 1864 loss: 1.88290494e-06
Iter: 1865 loss: 1.88157787e-06
Iter: 1866 loss: 1.88477327e-06
Iter: 1867 loss: 1.88106515e-06
Iter: 1868 loss: 1.87975206e-06
Iter: 1869 loss: 1.88093156e-06
Iter: 1870 loss: 1.87897547e-06
Iter: 1871 loss: 1.87966737e-06
Iter: 1872 loss: 1.87851e-06
Iter: 1873 loss: 1.87787623e-06
Iter: 1874 loss: 1.87703961e-06
Iter: 1875 loss: 1.87696196e-06
Iter: 1876 loss: 1.87634828e-06
Iter: 1877 loss: 1.87491901e-06
Iter: 1878 loss: 1.89083607e-06
Iter: 1879 loss: 1.87474302e-06
Iter: 1880 loss: 1.87303795e-06
Iter: 1881 loss: 1.88824708e-06
Iter: 1882 loss: 1.87301907e-06
Iter: 1883 loss: 1.87181638e-06
Iter: 1884 loss: 1.87450735e-06
Iter: 1885 loss: 1.87145929e-06
Iter: 1886 loss: 1.87018725e-06
Iter: 1887 loss: 1.87898581e-06
Iter: 1888 loss: 1.87005878e-06
Iter: 1889 loss: 1.86902059e-06
Iter: 1890 loss: 1.87064961e-06
Iter: 1891 loss: 1.86863861e-06
Iter: 1892 loss: 1.86742056e-06
Iter: 1893 loss: 1.87282012e-06
Iter: 1894 loss: 1.86719967e-06
Iter: 1895 loss: 1.86642e-06
Iter: 1896 loss: 1.86511807e-06
Iter: 1897 loss: 1.86510363e-06
Iter: 1898 loss: 1.86353441e-06
Iter: 1899 loss: 1.87190381e-06
Iter: 1900 loss: 1.86326088e-06
Iter: 1901 loss: 1.86222371e-06
Iter: 1902 loss: 1.86678187e-06
Iter: 1903 loss: 1.86197792e-06
Iter: 1904 loss: 1.8611164e-06
Iter: 1905 loss: 1.8647645e-06
Iter: 1906 loss: 1.86085094e-06
Iter: 1907 loss: 1.86063608e-06
Iter: 1908 loss: 1.86041393e-06
Iter: 1909 loss: 1.86027444e-06
Iter: 1910 loss: 1.85945555e-06
Iter: 1911 loss: 1.85959095e-06
Iter: 1912 loss: 1.8587325e-06
Iter: 1913 loss: 1.8576352e-06
Iter: 1914 loss: 1.86185957e-06
Iter: 1915 loss: 1.85733529e-06
Iter: 1916 loss: 1.85630802e-06
Iter: 1917 loss: 1.85765111e-06
Iter: 1918 loss: 1.85567399e-06
Iter: 1919 loss: 1.85478552e-06
Iter: 1920 loss: 1.86783143e-06
Iter: 1921 loss: 1.85475938e-06
Iter: 1922 loss: 1.85400131e-06
Iter: 1923 loss: 1.85761508e-06
Iter: 1924 loss: 1.85386546e-06
Iter: 1925 loss: 1.85311615e-06
Iter: 1926 loss: 1.85452882e-06
Iter: 1927 loss: 1.85283943e-06
Iter: 1928 loss: 1.85198e-06
Iter: 1929 loss: 1.85244983e-06
Iter: 1930 loss: 1.85137208e-06
Iter: 1931 loss: 1.85035481e-06
Iter: 1932 loss: 1.84979444e-06
Iter: 1933 loss: 1.84935914e-06
Iter: 1934 loss: 1.84775092e-06
Iter: 1935 loss: 1.85380259e-06
Iter: 1936 loss: 1.8473238e-06
Iter: 1937 loss: 1.84599526e-06
Iter: 1938 loss: 1.85045485e-06
Iter: 1939 loss: 1.8456401e-06
Iter: 1940 loss: 1.84585224e-06
Iter: 1941 loss: 1.84501755e-06
Iter: 1942 loss: 1.84452858e-06
Iter: 1943 loss: 1.84339808e-06
Iter: 1944 loss: 1.86097213e-06
Iter: 1945 loss: 1.84337478e-06
Iter: 1946 loss: 1.8424364e-06
Iter: 1947 loss: 1.84186911e-06
Iter: 1948 loss: 1.8415484e-06
Iter: 1949 loss: 1.84047667e-06
Iter: 1950 loss: 1.84229555e-06
Iter: 1951 loss: 1.83996735e-06
Iter: 1952 loss: 1.83863563e-06
Iter: 1953 loss: 1.84250371e-06
Iter: 1954 loss: 1.83829434e-06
Iter: 1955 loss: 1.83697796e-06
Iter: 1956 loss: 1.84346345e-06
Iter: 1957 loss: 1.83681391e-06
Iter: 1958 loss: 1.83592988e-06
Iter: 1959 loss: 1.84713213e-06
Iter: 1960 loss: 1.83590646e-06
Iter: 1961 loss: 1.83521684e-06
Iter: 1962 loss: 1.83538134e-06
Iter: 1963 loss: 1.8347572e-06
Iter: 1964 loss: 1.8337214e-06
Iter: 1965 loss: 1.83438203e-06
Iter: 1966 loss: 1.83312557e-06
Iter: 1967 loss: 1.83223278e-06
Iter: 1968 loss: 1.83272152e-06
Iter: 1969 loss: 1.83161455e-06
Iter: 1970 loss: 1.83054226e-06
Iter: 1971 loss: 1.83327552e-06
Iter: 1972 loss: 1.83002498e-06
Iter: 1973 loss: 1.82900465e-06
Iter: 1974 loss: 1.83480824e-06
Iter: 1975 loss: 1.82887027e-06
Iter: 1976 loss: 1.82873612e-06
Iter: 1977 loss: 1.82838744e-06
Iter: 1978 loss: 1.82806639e-06
Iter: 1979 loss: 1.82720044e-06
Iter: 1980 loss: 1.83052975e-06
Iter: 1981 loss: 1.82685335e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.8
+ date
Wed Oct 21 16:06:35 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/300_300_300_1 --function f1 --psi 2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2c86158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2d78378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2d66730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2da4e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2ce8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2c5a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2bff8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2bd6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2bd6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2bff158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2b96598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2b40c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2b40a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2ae47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2a978c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2a7ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2a7b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2af8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2a7ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5981526e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59815d6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59815d6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5981598158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59814b12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59814b1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59814f6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c542840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c53f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c566378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c573ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c48f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c4ae0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c4ae1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c4bd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c4ae598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c4e7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.09092356
test_loss: 0.093454376
train_loss: 0.063582286
test_loss: 0.06573653
train_loss: 0.049567237
test_loss: 0.045977637
train_loss: 0.032646436
test_loss: 0.035093687
train_loss: 0.027755015
test_loss: 0.028708005
train_loss: 0.029307928
test_loss: 0.026419034
train_loss: 0.021827437
test_loss: 0.02399983
train_loss: 0.01828951
test_loss: 0.022249559
train_loss: 0.019717518
test_loss: 0.021101546
train_loss: 0.016770722
test_loss: 0.0206972
train_loss: 0.017292341
test_loss: 0.021182323
train_loss: 0.018520253
test_loss: 0.019608982
train_loss: 0.015831517
test_loss: 0.01936955
train_loss: 0.01634121
test_loss: 0.018879412
train_loss: 0.014525766
test_loss: 0.017846595
train_loss: 0.013541677
test_loss: 0.017446717
train_loss: 0.013299365
test_loss: 0.017016765
train_loss: 0.012909753
test_loss: 0.017211232
train_loss: 0.013219612
test_loss: 0.016837694
train_loss: 0.0129676685
test_loss: 0.01706086
train_loss: 0.015150642
test_loss: 0.01767478
train_loss: 0.011580296
test_loss: 0.015754232
train_loss: 0.013323015
test_loss: 0.016564295
train_loss: 0.0130374525
test_loss: 0.016904302
train_loss: 0.011601906
test_loss: 0.015180744
train_loss: 0.010394508
test_loss: 0.016132576
train_loss: 0.011660874
test_loss: 0.015380561
train_loss: 0.012430532
test_loss: 0.015002301
train_loss: 0.011789921
test_loss: 0.016147036
train_loss: 0.011256478
test_loss: 0.015035651
train_loss: 0.012605592
test_loss: 0.015668433
train_loss: 0.011701867
test_loss: 0.015001551
train_loss: 0.01152214
test_loss: 0.014812396
train_loss: 0.011127097
test_loss: 0.014796646
train_loss: 0.013735676
test_loss: 0.015070031
train_loss: 0.011129
test_loss: 0.014284627
train_loss: 0.010059381
test_loss: 0.01434255
train_loss: 0.010408834
test_loss: 0.014234488
train_loss: 0.010621607
test_loss: 0.013924876
train_loss: 0.010960684
test_loss: 0.014198928
train_loss: 0.0104145985
test_loss: 0.0141846435
train_loss: 0.010473836
test_loss: 0.0141779715
train_loss: 0.010346815
test_loss: 0.013934827
train_loss: 0.010887913
test_loss: 0.014163618
train_loss: 0.011122669
test_loss: 0.014100659
train_loss: 0.010336155
test_loss: 0.013463661
train_loss: 0.011422181
test_loss: 0.01408596
train_loss: 0.010435779
test_loss: 0.01352826
train_loss: 0.010082757
test_loss: 0.014325692
train_loss: 0.010183363
test_loss: 0.013709809
train_loss: 0.010069266
test_loss: 0.013966057
train_loss: 0.010680454
test_loss: 0.014241786
train_loss: 0.010969671
test_loss: 0.013834231
train_loss: 0.0094631715
test_loss: 0.013829251
train_loss: 0.009733919
test_loss: 0.013361334
train_loss: 0.009516133
test_loss: 0.013707744
train_loss: 0.010127551
test_loss: 0.013906035
train_loss: 0.008808827
test_loss: 0.013569708
train_loss: 0.010343334
test_loss: 0.013939133
train_loss: 0.0088216495
test_loss: 0.013416298
train_loss: 0.009185628
test_loss: 0.013399385
train_loss: 0.009956274
test_loss: 0.013123985
train_loss: 0.0098444205
test_loss: 0.013470769
train_loss: 0.009400796
test_loss: 0.0133425165
train_loss: 0.009887355
test_loss: 0.01333425
train_loss: 0.010662034
test_loss: 0.0129581075
train_loss: 0.009084679
test_loss: 0.013039146
train_loss: 0.008811977
test_loss: 0.012944155
train_loss: 0.009067698
test_loss: 0.013296364
train_loss: 0.009025534
test_loss: 0.013237674
train_loss: 0.009733262
test_loss: 0.012919666
train_loss: 0.009988732
test_loss: 0.0139548555
train_loss: 0.008150784
test_loss: 0.012823888
train_loss: 0.0086752
test_loss: 0.01337588
train_loss: 0.009173257
test_loss: 0.013409728
train_loss: 0.0083666295
test_loss: 0.013255952
train_loss: 0.008321119
test_loss: 0.012907137
train_loss: 0.008889096
test_loss: 0.013265247
train_loss: 0.0093857795
test_loss: 0.013003782
train_loss: 0.009076161
test_loss: 0.012953694
train_loss: 0.00894598
test_loss: 0.012972529
train_loss: 0.009105023
test_loss: 0.012898393
train_loss: 0.0086817555
test_loss: 0.01270861
train_loss: 0.009561336
test_loss: 0.01306009
train_loss: 0.008862052
test_loss: 0.012409333
train_loss: 0.010003216
test_loss: 0.01344894
train_loss: 0.010178027
test_loss: 0.013018241
train_loss: 0.008482389
test_loss: 0.012651794
train_loss: 0.008314181
test_loss: 0.012547083
train_loss: 0.008997155
test_loss: 0.013041084
train_loss: 0.008878665
test_loss: 0.012410593
train_loss: 0.009005962
test_loss: 0.012437348
train_loss: 0.008944687
test_loss: 0.013060775
train_loss: 0.008836942
test_loss: 0.012614601
train_loss: 0.008285083
test_loss: 0.012903976
train_loss: 0.008143988
test_loss: 0.011982421
train_loss: 0.009263277
test_loss: 0.012309852
train_loss: 0.008030586
test_loss: 0.0118539855
train_loss: 0.008737094
test_loss: 0.012988765
train_loss: 0.008419467
test_loss: 0.012414382
train_loss: 0.009193732
test_loss: 0.012337706
train_loss: 0.008457613
test_loss: 0.012056292
train_loss: 0.0098695215
test_loss: 0.012399586
train_loss: 0.007610406
test_loss: 0.011829827
train_loss: 0.008393269
test_loss: 0.012062403
train_loss: 0.007986017
test_loss: 0.01239631
train_loss: 0.008112793
test_loss: 0.0122600645
train_loss: 0.008216977
test_loss: 0.011925038
train_loss: 0.009233394
test_loss: 0.01232722
train_loss: 0.009024682
test_loss: 0.013134179
train_loss: 0.01080247
test_loss: 0.012631973
train_loss: 0.0077900616
test_loss: 0.012385494
train_loss: 0.008719879
test_loss: 0.01230253
train_loss: 0.008729195
test_loss: 0.012264114
train_loss: 0.010005612
test_loss: 0.01233464
train_loss: 0.007981571
test_loss: 0.012251405
train_loss: 0.008809094
test_loss: 0.011744581
train_loss: 0.008340593
test_loss: 0.012163451
train_loss: 0.0077817906
test_loss: 0.011768749
train_loss: 0.007767717
test_loss: 0.011866685
train_loss: 0.008665965
test_loss: 0.0120541835
train_loss: 0.008130801
test_loss: 0.011931129
train_loss: 0.008367823
test_loss: 0.012192765
train_loss: 0.0077992287
test_loss: 0.012149716
train_loss: 0.0068999166
test_loss: 0.01190872
train_loss: 0.008321501
test_loss: 0.01175522
train_loss: 0.007965517
test_loss: 0.012348374
train_loss: 0.0070124054
test_loss: 0.011686973
train_loss: 0.008665639
test_loss: 0.01235987
train_loss: 0.009007136
test_loss: 0.012001896
train_loss: 0.007544595
test_loss: 0.011784122
train_loss: 0.007424775
test_loss: 0.011265713
train_loss: 0.007095781
test_loss: 0.011627948
train_loss: 0.00649348
test_loss: 0.011835934
train_loss: 0.007871323
test_loss: 0.01184322
train_loss: 0.00793656
test_loss: 0.012034307
train_loss: 0.0072282543
test_loss: 0.011913502
train_loss: 0.00817767
test_loss: 0.011450565
train_loss: 0.006893929
test_loss: 0.011419978
train_loss: 0.009021314
test_loss: 0.012329729
train_loss: 0.007443689
test_loss: 0.011744961
train_loss: 0.007726733
test_loss: 0.011951441
train_loss: 0.0071946946
test_loss: 0.011249165
train_loss: 0.007179492
test_loss: 0.011883246
train_loss: 0.0065671923
test_loss: 0.011179425
train_loss: 0.008370989
test_loss: 0.011773726
train_loss: 0.008251419
test_loss: 0.01206473
train_loss: 0.008304054
test_loss: 0.012319978
train_loss: 0.007999456
test_loss: 0.011835239
train_loss: 0.008437026
test_loss: 0.01151287
train_loss: 0.0073685125
test_loss: 0.011318159
train_loss: 0.0085616335
test_loss: 0.012110171
train_loss: 0.009659786
test_loss: 0.012733443
train_loss: 0.007559644
test_loss: 0.011885969
train_loss: 0.0077833794
test_loss: 0.011647118
train_loss: 0.007471552
test_loss: 0.011595999
train_loss: 0.0076341573
test_loss: 0.011946326
train_loss: 0.0073206956
test_loss: 0.011753995
train_loss: 0.007306543
test_loss: 0.011718452
train_loss: 0.007885166
test_loss: 0.011614352
train_loss: 0.008002389
test_loss: 0.011745146
train_loss: 0.00722096
test_loss: 0.011634107
train_loss: 0.007536947
test_loss: 0.011834632
train_loss: 0.0076384144
test_loss: 0.01167603
train_loss: 0.0069464315
test_loss: 0.011199682
train_loss: 0.0075090337
test_loss: 0.011460356
train_loss: 0.0071875523
test_loss: 0.011438577
train_loss: 0.008406922
test_loss: 0.011802932
train_loss: 0.0066355504
test_loss: 0.0113767525
train_loss: 0.0075696525
test_loss: 0.011428648
train_loss: 0.007023352
test_loss: 0.011460504
train_loss: 0.0073824897
test_loss: 0.011273061
train_loss: 0.0073625795
test_loss: 0.011602665
train_loss: 0.0068531176
test_loss: 0.011677752
train_loss: 0.008346093
test_loss: 0.011216542
train_loss: 0.0063836942
test_loss: 0.010965246
train_loss: 0.0067257513
test_loss: 0.011061311
train_loss: 0.006645728
test_loss: 0.011675402
train_loss: 0.007865876
test_loss: 0.012148995
train_loss: 0.0072003887
test_loss: 0.011671491
train_loss: 0.0077742413
test_loss: 0.011166373
train_loss: 0.00822118
test_loss: 0.011716839
train_loss: 0.0074110436
test_loss: 0.011629145
train_loss: 0.0085148895
test_loss: 0.011654203
train_loss: 0.007831533
test_loss: 0.011569737
train_loss: 0.007604019
test_loss: 0.011393892
train_loss: 0.0065489956
test_loss: 0.011601794
train_loss: 0.0068263584
test_loss: 0.0113093
train_loss: 0.007567262
test_loss: 0.011729621
train_loss: 0.007887483
test_loss: 0.01133576
train_loss: 0.0072610015
test_loss: 0.011393277
train_loss: 0.008598965
test_loss: 0.011217457
train_loss: 0.008048711
test_loss: 0.011287369
train_loss: 0.008071899
test_loss: 0.011263902
train_loss: 0.0074265106
test_loss: 0.011462653
train_loss: 0.007084706
test_loss: 0.011480952
train_loss: 0.0075770654
test_loss: 0.011355075
train_loss: 0.007449846
test_loss: 0.011345672
train_loss: 0.006998594
test_loss: 0.011262804
train_loss: 0.0068090367
test_loss: 0.011289879
train_loss: 0.006836644
test_loss: 0.01118241
train_loss: 0.0071726614
test_loss: 0.011178742
train_loss: 0.007242892
test_loss: 0.011135779
train_loss: 0.007227771
test_loss: 0.011468687
train_loss: 0.006675857
test_loss: 0.011349629
train_loss: 0.0069897114
test_loss: 0.011324209
train_loss: 0.007868795
test_loss: 0.011605011
train_loss: 0.0066918996
test_loss: 0.010737421
train_loss: 0.00722605
test_loss: 0.011326125
train_loss: 0.006654208
test_loss: 0.011076044
train_loss: 0.0063342857
test_loss: 0.010657781
train_loss: 0.0065298574
test_loss: 0.0112727145
train_loss: 0.006338176
test_loss: 0.011383278
train_loss: 0.007809355
test_loss: 0.01134164
train_loss: 0.0067404853
test_loss: 0.011396349
train_loss: 0.005991209
test_loss: 0.01086583
train_loss: 0.0071901944
test_loss: 0.010949481
train_loss: 0.0065817633
test_loss: 0.011000753
train_loss: 0.0069036153
test_loss: 0.011073476
train_loss: 0.007148966
test_loss: 0.011198227
train_loss: 0.006571419
test_loss: 0.01094868
train_loss: 0.007009751
test_loss: 0.011473866
train_loss: 0.0068734176
test_loss: 0.01143864
train_loss: 0.007286758
test_loss: 0.010889631
train_loss: 0.0068447357
test_loss: 0.011073458
train_loss: 0.0075722504
test_loss: 0.011038786
train_loss: 0.0070660533
test_loss: 0.010760162
train_loss: 0.0063919527
test_loss: 0.01109986
train_loss: 0.0063955886
test_loss: 0.011222467
train_loss: 0.0062821712
test_loss: 0.010725094
train_loss: 0.0069327536
test_loss: 0.010935944
train_loss: 0.005925286
test_loss: 0.010625907
train_loss: 0.0067725363
test_loss: 0.010431871
train_loss: 0.0063504972
test_loss: 0.010599273
train_loss: 0.0068373447
test_loss: 0.0106523
train_loss: 0.0067005013
test_loss: 0.011133787
train_loss: 0.0065829647
test_loss: 0.010743076
train_loss: 0.006883383
test_loss: 0.011115104
train_loss: 0.006140154
test_loss: 0.010487059
train_loss: 0.008021463
test_loss: 0.010952483
train_loss: 0.00669361
test_loss: 0.010569129
train_loss: 0.006960901
test_loss: 0.010974043
train_loss: 0.0072850175
test_loss: 0.011082293
train_loss: 0.006745803
test_loss: 0.010937441
train_loss: 0.005940982
test_loss: 0.01052437
train_loss: 0.0060610836
test_loss: 0.010289578
train_loss: 0.0061546476
test_loss: 0.0107629625
train_loss: 0.006280426
test_loss: 0.010877664
train_loss: 0.0065978062
test_loss: 0.011301071
train_loss: 0.006683457
test_loss: 0.010780499
train_loss: 0.006028262
test_loss: 0.010832798
train_loss: 0.0070960233
test_loss: 0.01101247
train_loss: 0.006203418
test_loss: 0.010570513
train_loss: 0.006518551
test_loss: 0.010589034
train_loss: 0.0075596976
test_loss: 0.0105519015
train_loss: 0.006261404
test_loss: 0.010539939
train_loss: 0.0072911945
test_loss: 0.01104164
train_loss: 0.006566045
test_loss: 0.010802383
train_loss: 0.0060276603
test_loss: 0.01108029
train_loss: 0.006326678
test_loss: 0.0107113905
train_loss: 0.0071152174
test_loss: 0.011674377
train_loss: 0.0062887063
test_loss: 0.010738501
train_loss: 0.0064786198
test_loss: 0.010518308
train_loss: 0.00697889
test_loss: 0.011002879
train_loss: 0.006425295
test_loss: 0.010878599
train_loss: 0.006786711
test_loss: 0.011017246
train_loss: 0.007124025
test_loss: 0.011198936
train_loss: 0.0065764505
test_loss: 0.010958921
train_loss: 0.0059404527
test_loss: 0.011172883
train_loss: 0.006395628
test_loss: 0.011013303
train_loss: 0.008218344
test_loss: 0.011511992
train_loss: 0.007115945
test_loss: 0.010629048
train_loss: 0.0067157373
test_loss: 0.010527385
train_loss: 0.0070291026
test_loss: 0.011194505
train_loss: 0.0068922504
test_loss: 0.010710853
train_loss: 0.0065281764
test_loss: 0.010634798
train_loss: 0.007064191
test_loss: 0.010644262
train_loss: 0.006582196
test_loss: 0.010847642
train_loss: 0.006213859
test_loss: 0.01092863
train_loss: 0.006011315
test_loss: 0.010611455
train_loss: 0.0061543193
test_loss: 0.010511891
train_loss: 0.0068562524
test_loss: 0.01072309
train_loss: 0.006763919
test_loss: 0.011004465
train_loss: 0.00597153
test_loss: 0.011021348
train_loss: 0.006302661
test_loss: 0.010703854
train_loss: 0.0070099114
test_loss: 0.010702732
train_loss: 0.0073460448
test_loss: 0.011027928
train_loss: 0.0068121403
test_loss: 0.011053091
train_loss: 0.006322912
test_loss: 0.010946967
train_loss: 0.006810845
test_loss: 0.011158605
train_loss: 0.007066181
test_loss: 0.011019562
train_loss: 0.007910524
test_loss: 0.011109933
train_loss: 0.0062059276
test_loss: 0.010726262
train_loss: 0.006277057
test_loss: 0.010577975
train_loss: 0.0065497044
test_loss: 0.010875226
train_loss: 0.006641336
test_loss: 0.010463711
train_loss: 0.0065695294
test_loss: 0.010884876
train_loss: 0.006226492
test_loss: 0.010614706
train_loss: 0.006370808
test_loss: 0.010864226
train_loss: 0.006335345
test_loss: 0.010610559
train_loss: 0.006536944
test_loss: 0.010463309
train_loss: 0.0070213107
test_loss: 0.010665048
train_loss: 0.006123744
test_loss: 0.010630251
train_loss: 0.006857565
test_loss: 0.011099223
train_loss: 0.00691262
test_loss: 0.010683168
train_loss: 0.0064481352
test_loss: 0.0104413275
train_loss: 0.00741922
test_loss: 0.011402684
train_loss: 0.0067461287
test_loss: 0.010897547
train_loss: 0.005689393
test_loss: 0.010450847
train_loss: 0.0061150454
test_loss: 0.01069793
train_loss: 0.0065072095
test_loss: 0.011072586
train_loss: 0.006292442
test_loss: 0.010482133
train_loss: 0.0060922774
test_loss: 0.010478146
train_loss: 0.0070412364
test_loss: 0.010525952
train_loss: 0.006568567
test_loss: 0.011051298
train_loss: 0.006381816
test_loss: 0.011002667
train_loss: 0.006444135
test_loss: 0.010383432
train_loss: 0.0063489014
test_loss: 0.010474703
train_loss: 0.006365806
test_loss: 0.010863928
train_loss: 0.00834308
test_loss: 0.01055235
train_loss: 0.006195953
test_loss: 0.010610223
train_loss: 0.0070354426
test_loss: 0.010794059
train_loss: 0.0065486757
test_loss: 0.010395575
train_loss: 0.005682331
test_loss: 0.010283403
train_loss: 0.0061903736
test_loss: 0.010523902
train_loss: 0.0065746177
test_loss: 0.010824573
train_loss: 0.005900937
test_loss: 0.010357276
train_loss: 0.0056519425
test_loss: 0.010254869
train_loss: 0.0053804647
test_loss: 0.010129017
train_loss: 0.006330888
test_loss: 0.010214895
train_loss: 0.0073921904
test_loss: 0.010731114
train_loss: 0.0063223317
test_loss: 0.01030883
train_loss: 0.006241261
test_loss: 0.010238705
train_loss: 0.005952987
test_loss: 0.010281826
train_loss: 0.006366486
test_loss: 0.010343859
train_loss: 0.006207431
test_loss: 0.010707925
train_loss: 0.006738295
test_loss: 0.010915948
train_loss: 0.007198682
test_loss: 0.010687892
train_loss: 0.0065731583
test_loss: 0.01054337
train_loss: 0.006608625
test_loss: 0.010427387
train_loss: 0.0067513697
test_loss: 0.010801446
train_loss: 0.005866646
test_loss: 0.010320118
train_loss: 0.006054567
test_loss: 0.010480536
train_loss: 0.0058812737
test_loss: 0.010308076
train_loss: 0.0058261803
test_loss: 0.010253789
train_loss: 0.005754114
test_loss: 0.010444746
train_loss: 0.0055925185
test_loss: 0.010190431
train_loss: 0.006120245
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.010148579
train_loss: 0.007223737
test_loss: 0.010141746
train_loss: 0.00575356
test_loss: 0.009876452
train_loss: 0.005953461
test_loss: 0.011167877
train_loss: 0.0067347623
test_loss: 0.01061089
train_loss: 0.0062945
test_loss: 0.010520637
train_loss: 0.0063122977
test_loss: 0.010665147
train_loss: 0.0061161746
test_loss: 0.010436243
train_loss: 0.0054979557
test_loss: 0.01024602
train_loss: 0.0063263923
test_loss: 0.01023224
train_loss: 0.0066572977
test_loss: 0.011126994
train_loss: 0.0058746072
test_loss: 0.010170097
train_loss: 0.006191185
test_loss: 0.010934445
train_loss: 0.006098976
test_loss: 0.010173987
train_loss: 0.0060433205
test_loss: 0.010064332
train_loss: 0.0063370317
test_loss: 0.010270166
train_loss: 0.0061515365
test_loss: 0.010177994
train_loss: 0.0059174635
test_loss: 0.010526502
train_loss: 0.0065654074
test_loss: 0.010458704
train_loss: 0.0082671065
test_loss: 0.010519263
train_loss: 0.0055116564
test_loss: 0.010236942
train_loss: 0.0069421916
test_loss: 0.010859049
train_loss: 0.006991161
test_loss: 0.010464154
train_loss: 0.0063085645
test_loss: 0.010153057
train_loss: 0.006215699
test_loss: 0.010266688
train_loss: 0.006120411
test_loss: 0.010193638
train_loss: 0.006714035
test_loss: 0.010533477
train_loss: 0.008237298
test_loss: 0.010566659
train_loss: 0.0061553307
test_loss: 0.010219166
train_loss: 0.0063338554
test_loss: 0.010125051
train_loss: 0.0061531747
test_loss: 0.010381152
train_loss: 0.007625049
test_loss: 0.010438532
train_loss: 0.006038606
test_loss: 0.010294823
train_loss: 0.0063397125
test_loss: 0.010643704
train_loss: 0.0063884654
test_loss: 0.010727141
train_loss: 0.0062610595
test_loss: 0.010632383
train_loss: 0.0057867244
test_loss: 0.010409277
train_loss: 0.006577989
test_loss: 0.010672992
train_loss: 0.0061982973
test_loss: 0.010275437
train_loss: 0.0063595623
test_loss: 0.010324455
train_loss: 0.005659163
test_loss: 0.010144051
train_loss: 0.006561636
test_loss: 0.010532215
train_loss: 0.0062885853
test_loss: 0.010227119
train_loss: 0.006153988
test_loss: 0.010807553
train_loss: 0.005706517
test_loss: 0.010292222
train_loss: 0.005923121
test_loss: 0.01022154
train_loss: 0.005570853
test_loss: 0.010503344
train_loss: 0.005489488
test_loss: 0.01017107
train_loss: 0.0062556816
test_loss: 0.010052299
train_loss: 0.0053590136
test_loss: 0.00999194
train_loss: 0.005862766
test_loss: 0.0101810945
train_loss: 0.0060591623
test_loss: 0.010203209
train_loss: 0.0057805157
test_loss: 0.010367822
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98913aeea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891371048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891371a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891343ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f989122cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f989122cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98911ddd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98911e9048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98911b8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98911b8158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891119d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891169510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98911b89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98910e2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98910ae950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98910acc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891052620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98910686a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891018c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891045488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891045510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9890fe0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98542ad950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98542b1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98542b1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9854285510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98542418c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9854264840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98541f0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f985421a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98541c6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f982c7a62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f982c7a61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f982c7b1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f982c78a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f982c791d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.30077154e-05
Iter: 2 loss: 6.58879726e-05
Iter: 3 loss: 6.244627e-05
Iter: 4 loss: 5.61704e-05
Iter: 5 loss: 7.97096727e-05
Iter: 6 loss: 5.46433257e-05
Iter: 7 loss: 5.11497565e-05
Iter: 8 loss: 4.48394348e-05
Iter: 9 loss: 0.000195867833
Iter: 10 loss: 4.48376268e-05
Iter: 11 loss: 3.90025562e-05
Iter: 12 loss: 8.25577372e-05
Iter: 13 loss: 3.85459716e-05
Iter: 14 loss: 3.49927e-05
Iter: 15 loss: 3.50890623e-05
Iter: 16 loss: 3.21791122e-05
Iter: 17 loss: 2.85234091e-05
Iter: 18 loss: 7.58972383e-05
Iter: 19 loss: 2.84928337e-05
Iter: 20 loss: 2.6806254e-05
Iter: 21 loss: 2.89441014e-05
Iter: 22 loss: 2.59401e-05
Iter: 23 loss: 2.37129043e-05
Iter: 24 loss: 3.30207113e-05
Iter: 25 loss: 2.32369257e-05
Iter: 26 loss: 2.19714493e-05
Iter: 27 loss: 2.69519151e-05
Iter: 28 loss: 2.16822918e-05
Iter: 29 loss: 2.05472788e-05
Iter: 30 loss: 2.09579157e-05
Iter: 31 loss: 1.97488334e-05
Iter: 32 loss: 1.86149628e-05
Iter: 33 loss: 2.19665199e-05
Iter: 34 loss: 1.82669392e-05
Iter: 35 loss: 1.72087803e-05
Iter: 36 loss: 2.49608238e-05
Iter: 37 loss: 1.71207212e-05
Iter: 38 loss: 1.64441699e-05
Iter: 39 loss: 1.68118586e-05
Iter: 40 loss: 1.59997435e-05
Iter: 41 loss: 1.56634778e-05
Iter: 42 loss: 1.55164125e-05
Iter: 43 loss: 1.50341293e-05
Iter: 44 loss: 1.57103677e-05
Iter: 45 loss: 1.4796442e-05
Iter: 46 loss: 1.45072609e-05
Iter: 47 loss: 1.39617787e-05
Iter: 48 loss: 2.58508589e-05
Iter: 49 loss: 1.39595832e-05
Iter: 50 loss: 1.33928661e-05
Iter: 51 loss: 1.92472016e-05
Iter: 52 loss: 1.33763569e-05
Iter: 53 loss: 1.30832786e-05
Iter: 54 loss: 1.38364785e-05
Iter: 55 loss: 1.29826058e-05
Iter: 56 loss: 1.25865363e-05
Iter: 57 loss: 1.24120052e-05
Iter: 58 loss: 1.22110123e-05
Iter: 59 loss: 1.1815092e-05
Iter: 60 loss: 1.60243599e-05
Iter: 61 loss: 1.1805022e-05
Iter: 62 loss: 1.15209914e-05
Iter: 63 loss: 1.17826112e-05
Iter: 64 loss: 1.13569595e-05
Iter: 65 loss: 1.0981259e-05
Iter: 66 loss: 1.3125612e-05
Iter: 67 loss: 1.09296871e-05
Iter: 68 loss: 1.07315464e-05
Iter: 69 loss: 1.0639741e-05
Iter: 70 loss: 1.05417603e-05
Iter: 71 loss: 1.02801541e-05
Iter: 72 loss: 1.2608918e-05
Iter: 73 loss: 1.02669765e-05
Iter: 74 loss: 1.00846037e-05
Iter: 75 loss: 9.9441877e-06
Iter: 76 loss: 9.88570355e-06
Iter: 77 loss: 1.00441957e-05
Iter: 78 loss: 9.78638e-06
Iter: 79 loss: 9.70120072e-06
Iter: 80 loss: 9.68029781e-06
Iter: 81 loss: 9.62594913e-06
Iter: 82 loss: 9.52412e-06
Iter: 83 loss: 9.30124952e-06
Iter: 84 loss: 1.25843471e-05
Iter: 85 loss: 9.29152884e-06
Iter: 86 loss: 9.11140887e-06
Iter: 87 loss: 1.16877891e-05
Iter: 88 loss: 9.11114694e-06
Iter: 89 loss: 8.98533744e-06
Iter: 90 loss: 9.05547131e-06
Iter: 91 loss: 8.90308911e-06
Iter: 92 loss: 8.73703721e-06
Iter: 93 loss: 9.99559688e-06
Iter: 94 loss: 8.72443252e-06
Iter: 95 loss: 8.63725109e-06
Iter: 96 loss: 8.66598e-06
Iter: 97 loss: 8.57592386e-06
Iter: 98 loss: 8.44613169e-06
Iter: 99 loss: 8.95153789e-06
Iter: 100 loss: 8.41610745e-06
Iter: 101 loss: 8.315169e-06
Iter: 102 loss: 9.10151903e-06
Iter: 103 loss: 8.30798308e-06
Iter: 104 loss: 8.232244e-06
Iter: 105 loss: 8.11284553e-06
Iter: 106 loss: 8.11149766e-06
Iter: 107 loss: 7.99824647e-06
Iter: 108 loss: 9.40882546e-06
Iter: 109 loss: 7.99728605e-06
Iter: 110 loss: 7.91568527e-06
Iter: 111 loss: 8.03300463e-06
Iter: 112 loss: 7.87585304e-06
Iter: 113 loss: 7.87825e-06
Iter: 114 loss: 7.83717e-06
Iter: 115 loss: 7.80664323e-06
Iter: 116 loss: 7.75250646e-06
Iter: 117 loss: 7.75262197e-06
Iter: 118 loss: 7.69316375e-06
Iter: 119 loss: 7.59761588e-06
Iter: 120 loss: 7.59674367e-06
Iter: 121 loss: 7.49556511e-06
Iter: 122 loss: 7.95738652e-06
Iter: 123 loss: 7.47608919e-06
Iter: 124 loss: 7.37288519e-06
Iter: 125 loss: 7.97729263e-06
Iter: 126 loss: 7.35944514e-06
Iter: 127 loss: 7.3047795e-06
Iter: 128 loss: 7.88439411e-06
Iter: 129 loss: 7.30340889e-06
Iter: 130 loss: 7.26489498e-06
Iter: 131 loss: 7.18067713e-06
Iter: 132 loss: 8.44745591e-06
Iter: 133 loss: 7.17715875e-06
Iter: 134 loss: 7.12691053e-06
Iter: 135 loss: 7.12143037e-06
Iter: 136 loss: 7.07672916e-06
Iter: 137 loss: 7.0500314e-06
Iter: 138 loss: 7.03171645e-06
Iter: 139 loss: 6.95345261e-06
Iter: 140 loss: 7.27776842e-06
Iter: 141 loss: 6.93648326e-06
Iter: 142 loss: 6.89553872e-06
Iter: 143 loss: 7.01732824e-06
Iter: 144 loss: 6.88327145e-06
Iter: 145 loss: 6.82837208e-06
Iter: 146 loss: 6.83382e-06
Iter: 147 loss: 6.78582501e-06
Iter: 148 loss: 6.8465597e-06
Iter: 149 loss: 6.76693162e-06
Iter: 150 loss: 6.75369074e-06
Iter: 151 loss: 6.71576117e-06
Iter: 152 loss: 6.88334421e-06
Iter: 153 loss: 6.70141435e-06
Iter: 154 loss: 6.65177049e-06
Iter: 155 loss: 6.83197e-06
Iter: 156 loss: 6.63919218e-06
Iter: 157 loss: 6.602535e-06
Iter: 158 loss: 6.5873669e-06
Iter: 159 loss: 6.56837165e-06
Iter: 160 loss: 6.5186714e-06
Iter: 161 loss: 7.15092756e-06
Iter: 162 loss: 6.51830578e-06
Iter: 163 loss: 6.48757577e-06
Iter: 164 loss: 6.47259412e-06
Iter: 165 loss: 6.45772707e-06
Iter: 166 loss: 6.41191946e-06
Iter: 167 loss: 6.85088889e-06
Iter: 168 loss: 6.40999133e-06
Iter: 169 loss: 6.38055053e-06
Iter: 170 loss: 6.48521655e-06
Iter: 171 loss: 6.37299399e-06
Iter: 172 loss: 6.33959644e-06
Iter: 173 loss: 6.32271667e-06
Iter: 174 loss: 6.30693103e-06
Iter: 175 loss: 6.27531426e-06
Iter: 176 loss: 6.5655e-06
Iter: 177 loss: 6.27412e-06
Iter: 178 loss: 6.24455424e-06
Iter: 179 loss: 6.2538179e-06
Iter: 180 loss: 6.22351672e-06
Iter: 181 loss: 6.1845285e-06
Iter: 182 loss: 6.47617344e-06
Iter: 183 loss: 6.18158856e-06
Iter: 184 loss: 6.1765727e-06
Iter: 185 loss: 6.17091337e-06
Iter: 186 loss: 6.16119e-06
Iter: 187 loss: 6.13222801e-06
Iter: 188 loss: 6.23639517e-06
Iter: 189 loss: 6.11952782e-06
Iter: 190 loss: 6.08182518e-06
Iter: 191 loss: 6.19094862e-06
Iter: 192 loss: 6.06998856e-06
Iter: 193 loss: 6.03352237e-06
Iter: 194 loss: 6.0072407e-06
Iter: 195 loss: 5.99492205e-06
Iter: 196 loss: 5.95122583e-06
Iter: 197 loss: 6.1877472e-06
Iter: 198 loss: 5.94450512e-06
Iter: 199 loss: 5.9105264e-06
Iter: 200 loss: 6.31529565e-06
Iter: 201 loss: 5.91002754e-06
Iter: 202 loss: 5.88443754e-06
Iter: 203 loss: 5.89906085e-06
Iter: 204 loss: 5.86731721e-06
Iter: 205 loss: 5.83356905e-06
Iter: 206 loss: 6.07044512e-06
Iter: 207 loss: 5.83051042e-06
Iter: 208 loss: 5.8107189e-06
Iter: 209 loss: 5.86693113e-06
Iter: 210 loss: 5.80433971e-06
Iter: 211 loss: 5.78041181e-06
Iter: 212 loss: 5.76083767e-06
Iter: 213 loss: 5.75354125e-06
Iter: 214 loss: 5.72923182e-06
Iter: 215 loss: 6.04195611e-06
Iter: 216 loss: 5.72894214e-06
Iter: 217 loss: 5.70762222e-06
Iter: 218 loss: 5.75032391e-06
Iter: 219 loss: 5.69870554e-06
Iter: 220 loss: 5.67781626e-06
Iter: 221 loss: 5.67790266e-06
Iter: 222 loss: 5.66689505e-06
Iter: 223 loss: 5.65744813e-06
Iter: 224 loss: 5.65436676e-06
Iter: 225 loss: 5.64068068e-06
Iter: 226 loss: 5.60876879e-06
Iter: 227 loss: 5.99764189e-06
Iter: 228 loss: 5.60623448e-06
Iter: 229 loss: 5.58678857e-06
Iter: 230 loss: 5.58509419e-06
Iter: 231 loss: 5.57054955e-06
Iter: 232 loss: 5.54143571e-06
Iter: 233 loss: 6.09188919e-06
Iter: 234 loss: 5.54107055e-06
Iter: 235 loss: 5.5193068e-06
Iter: 236 loss: 5.51845733e-06
Iter: 237 loss: 5.49945435e-06
Iter: 238 loss: 5.48806474e-06
Iter: 239 loss: 5.48012122e-06
Iter: 240 loss: 5.46295814e-06
Iter: 241 loss: 5.46243427e-06
Iter: 242 loss: 5.447685e-06
Iter: 243 loss: 5.44294835e-06
Iter: 244 loss: 5.43475653e-06
Iter: 245 loss: 5.41071e-06
Iter: 246 loss: 5.49329525e-06
Iter: 247 loss: 5.40472047e-06
Iter: 248 loss: 5.38796849e-06
Iter: 249 loss: 5.46188539e-06
Iter: 250 loss: 5.38468748e-06
Iter: 251 loss: 5.36998141e-06
Iter: 252 loss: 5.46522142e-06
Iter: 253 loss: 5.36807602e-06
Iter: 254 loss: 5.35175695e-06
Iter: 255 loss: 5.43750184e-06
Iter: 256 loss: 5.34905394e-06
Iter: 257 loss: 5.34262836e-06
Iter: 258 loss: 5.3352187e-06
Iter: 259 loss: 5.33419961e-06
Iter: 260 loss: 5.31972e-06
Iter: 261 loss: 5.3102458e-06
Iter: 262 loss: 5.30473608e-06
Iter: 263 loss: 5.28758119e-06
Iter: 264 loss: 5.35364916e-06
Iter: 265 loss: 5.2835e-06
Iter: 266 loss: 5.2636733e-06
Iter: 267 loss: 5.2726773e-06
Iter: 268 loss: 5.25050382e-06
Iter: 269 loss: 5.23223298e-06
Iter: 270 loss: 5.28742294e-06
Iter: 271 loss: 5.22654591e-06
Iter: 272 loss: 5.20572303e-06
Iter: 273 loss: 5.30817442e-06
Iter: 274 loss: 5.20211779e-06
Iter: 275 loss: 5.1868119e-06
Iter: 276 loss: 5.19840387e-06
Iter: 277 loss: 5.17751641e-06
Iter: 278 loss: 5.15856209e-06
Iter: 279 loss: 5.37387086e-06
Iter: 280 loss: 5.15809597e-06
Iter: 281 loss: 5.14812655e-06
Iter: 282 loss: 5.16554883e-06
Iter: 283 loss: 5.14349222e-06
Iter: 284 loss: 5.13066061e-06
Iter: 285 loss: 5.13807572e-06
Iter: 286 loss: 5.12223596e-06
Iter: 287 loss: 5.11838334e-06
Iter: 288 loss: 5.11475537e-06
Iter: 289 loss: 5.1073016e-06
Iter: 290 loss: 5.09714755e-06
Iter: 291 loss: 5.09653728e-06
Iter: 292 loss: 5.08415451e-06
Iter: 293 loss: 5.07762115e-06
Iter: 294 loss: 5.07177811e-06
Iter: 295 loss: 5.05933167e-06
Iter: 296 loss: 5.19470632e-06
Iter: 297 loss: 5.0589997e-06
Iter: 298 loss: 5.04811305e-06
Iter: 299 loss: 5.03394631e-06
Iter: 300 loss: 5.03305e-06
Iter: 301 loss: 5.01896557e-06
Iter: 302 loss: 5.19498553e-06
Iter: 303 loss: 5.01871455e-06
Iter: 304 loss: 5.00773513e-06
Iter: 305 loss: 4.99144153e-06
Iter: 306 loss: 4.99102771e-06
Iter: 307 loss: 4.97516885e-06
Iter: 308 loss: 5.17051103e-06
Iter: 309 loss: 4.9750779e-06
Iter: 310 loss: 4.95970335e-06
Iter: 311 loss: 4.95624863e-06
Iter: 312 loss: 4.94636424e-06
Iter: 313 loss: 4.93449534e-06
Iter: 314 loss: 4.93435073e-06
Iter: 315 loss: 4.92429626e-06
Iter: 316 loss: 4.91441824e-06
Iter: 317 loss: 4.91227729e-06
Iter: 318 loss: 4.89590639e-06
Iter: 319 loss: 5.06759443e-06
Iter: 320 loss: 4.89537615e-06
Iter: 321 loss: 4.89312788e-06
Iter: 322 loss: 4.89118975e-06
Iter: 323 loss: 4.88766591e-06
Iter: 324 loss: 4.87722355e-06
Iter: 325 loss: 4.91758146e-06
Iter: 326 loss: 4.87286e-06
Iter: 327 loss: 4.8591055e-06
Iter: 328 loss: 4.90892398e-06
Iter: 329 loss: 4.8552929e-06
Iter: 330 loss: 4.84549037e-06
Iter: 331 loss: 4.86778936e-06
Iter: 332 loss: 4.84163957e-06
Iter: 333 loss: 4.82670885e-06
Iter: 334 loss: 4.84419661e-06
Iter: 335 loss: 4.81887673e-06
Iter: 336 loss: 4.80810922e-06
Iter: 337 loss: 4.85648616e-06
Iter: 338 loss: 4.80588915e-06
Iter: 339 loss: 4.79455866e-06
Iter: 340 loss: 4.78328684e-06
Iter: 341 loss: 4.78084621e-06
Iter: 342 loss: 4.76862306e-06
Iter: 343 loss: 4.92103482e-06
Iter: 344 loss: 4.76866444e-06
Iter: 345 loss: 4.75684919e-06
Iter: 346 loss: 4.7674157e-06
Iter: 347 loss: 4.7501776e-06
Iter: 348 loss: 4.73997261e-06
Iter: 349 loss: 4.82819405e-06
Iter: 350 loss: 4.73932278e-06
Iter: 351 loss: 4.72997135e-06
Iter: 352 loss: 4.74313765e-06
Iter: 353 loss: 4.72528609e-06
Iter: 354 loss: 4.71752446e-06
Iter: 355 loss: 4.82855239e-06
Iter: 356 loss: 4.71752e-06
Iter: 357 loss: 4.71131762e-06
Iter: 358 loss: 4.75912e-06
Iter: 359 loss: 4.71087833e-06
Iter: 360 loss: 4.70670557e-06
Iter: 361 loss: 4.69550605e-06
Iter: 362 loss: 4.76074274e-06
Iter: 363 loss: 4.69227052e-06
Iter: 364 loss: 4.67837253e-06
Iter: 365 loss: 4.75974184e-06
Iter: 366 loss: 4.6765922e-06
Iter: 367 loss: 4.66657093e-06
Iter: 368 loss: 4.66538495e-06
Iter: 369 loss: 4.65809717e-06
Iter: 370 loss: 4.64724735e-06
Iter: 371 loss: 4.64722143e-06
Iter: 372 loss: 4.6399814e-06
Iter: 373 loss: 4.62839807e-06
Iter: 374 loss: 4.62836397e-06
Iter: 375 loss: 4.61635182e-06
Iter: 376 loss: 4.74859735e-06
Iter: 377 loss: 4.61612e-06
Iter: 378 loss: 4.60773344e-06
Iter: 379 loss: 4.60079355e-06
Iter: 380 loss: 4.59834064e-06
Iter: 381 loss: 4.58441355e-06
Iter: 382 loss: 4.73159434e-06
Iter: 383 loss: 4.58415479e-06
Iter: 384 loss: 4.57594206e-06
Iter: 385 loss: 4.57018632e-06
Iter: 386 loss: 4.56733233e-06
Iter: 387 loss: 4.5621473e-06
Iter: 388 loss: 4.56094e-06
Iter: 389 loss: 4.55607733e-06
Iter: 390 loss: 4.58164777e-06
Iter: 391 loss: 4.55520785e-06
Iter: 392 loss: 4.54904784e-06
Iter: 393 loss: 4.54505698e-06
Iter: 394 loss: 4.54260226e-06
Iter: 395 loss: 4.53490975e-06
Iter: 396 loss: 4.53680377e-06
Iter: 397 loss: 4.52920631e-06
Iter: 398 loss: 4.52098038e-06
Iter: 399 loss: 4.52006771e-06
Iter: 400 loss: 4.51421829e-06
Iter: 401 loss: 4.50232938e-06
Iter: 402 loss: 4.58578143e-06
Iter: 403 loss: 4.50121024e-06
Iter: 404 loss: 4.49362733e-06
Iter: 405 loss: 4.51080632e-06
Iter: 406 loss: 4.49104027e-06
Iter: 407 loss: 4.47960429e-06
Iter: 408 loss: 4.49066647e-06
Iter: 409 loss: 4.47336788e-06
Iter: 410 loss: 4.4656872e-06
Iter: 411 loss: 4.48981609e-06
Iter: 412 loss: 4.46351805e-06
Iter: 413 loss: 4.4547869e-06
Iter: 414 loss: 4.47421962e-06
Iter: 415 loss: 4.45163096e-06
Iter: 416 loss: 4.44397847e-06
Iter: 417 loss: 4.46070499e-06
Iter: 418 loss: 4.44122907e-06
Iter: 419 loss: 4.43060162e-06
Iter: 420 loss: 4.44200577e-06
Iter: 421 loss: 4.42466308e-06
Iter: 422 loss: 4.41788643e-06
Iter: 423 loss: 4.41788e-06
Iter: 424 loss: 4.41121847e-06
Iter: 425 loss: 4.44925217e-06
Iter: 426 loss: 4.41033762e-06
Iter: 427 loss: 4.40558733e-06
Iter: 428 loss: 4.40461554e-06
Iter: 429 loss: 4.40161784e-06
Iter: 430 loss: 4.39675659e-06
Iter: 431 loss: 4.39660198e-06
Iter: 432 loss: 4.39267069e-06
Iter: 433 loss: 4.3841269e-06
Iter: 434 loss: 4.38111601e-06
Iter: 435 loss: 4.37628023e-06
Iter: 436 loss: 4.36655318e-06
Iter: 437 loss: 4.39978794e-06
Iter: 438 loss: 4.36383561e-06
Iter: 439 loss: 4.35376705e-06
Iter: 440 loss: 4.42213104e-06
Iter: 441 loss: 4.35281436e-06
Iter: 442 loss: 4.34591448e-06
Iter: 443 loss: 4.37647168e-06
Iter: 444 loss: 4.34470803e-06
Iter: 445 loss: 4.33787864e-06
Iter: 446 loss: 4.33561354e-06
Iter: 447 loss: 4.33180048e-06
Iter: 448 loss: 4.32379875e-06
Iter: 449 loss: 4.39400264e-06
Iter: 450 loss: 4.32329398e-06
Iter: 451 loss: 4.31660374e-06
Iter: 452 loss: 4.30994442e-06
Iter: 453 loss: 4.30874934e-06
Iter: 454 loss: 4.30009368e-06
Iter: 455 loss: 4.41497286e-06
Iter: 456 loss: 4.30015689e-06
Iter: 457 loss: 4.29484498e-06
Iter: 458 loss: 4.32038269e-06
Iter: 459 loss: 4.29402462e-06
Iter: 460 loss: 4.28644398e-06
Iter: 461 loss: 4.30234559e-06
Iter: 462 loss: 4.28346084e-06
Iter: 463 loss: 4.27936584e-06
Iter: 464 loss: 4.28165686e-06
Iter: 465 loss: 4.27682653e-06
Iter: 466 loss: 4.27129407e-06
Iter: 467 loss: 4.26729639e-06
Iter: 468 loss: 4.2654442e-06
Iter: 469 loss: 4.2555962e-06
Iter: 470 loss: 4.27850318e-06
Iter: 471 loss: 4.25173675e-06
Iter: 472 loss: 4.24457039e-06
Iter: 473 loss: 4.2550987e-06
Iter: 474 loss: 4.2409165e-06
Iter: 475 loss: 4.23307711e-06
Iter: 476 loss: 4.29270904e-06
Iter: 477 loss: 4.23240726e-06
Iter: 478 loss: 4.22695393e-06
Iter: 479 loss: 4.23971278e-06
Iter: 480 loss: 4.22484845e-06
Iter: 481 loss: 4.21699087e-06
Iter: 482 loss: 4.22349649e-06
Iter: 483 loss: 4.21236246e-06
Iter: 484 loss: 4.20613105e-06
Iter: 485 loss: 4.23415668e-06
Iter: 486 loss: 4.20478818e-06
Iter: 487 loss: 4.19745584e-06
Iter: 488 loss: 4.2011261e-06
Iter: 489 loss: 4.1926296e-06
Iter: 490 loss: 4.18487616e-06
Iter: 491 loss: 4.20600145e-06
Iter: 492 loss: 4.18240097e-06
Iter: 493 loss: 4.17781848e-06
Iter: 494 loss: 4.17730098e-06
Iter: 495 loss: 4.17178762e-06
Iter: 496 loss: 4.17585352e-06
Iter: 497 loss: 4.16836156e-06
Iter: 498 loss: 4.16513922e-06
Iter: 499 loss: 4.15902196e-06
Iter: 500 loss: 4.29225474e-06
Iter: 501 loss: 4.1589683e-06
Iter: 502 loss: 4.15148224e-06
Iter: 503 loss: 4.22174526e-06
Iter: 504 loss: 4.15095928e-06
Iter: 505 loss: 4.14662372e-06
Iter: 506 loss: 4.14012538e-06
Iter: 507 loss: 4.13982116e-06
Iter: 508 loss: 4.12998907e-06
Iter: 509 loss: 4.18326636e-06
Iter: 510 loss: 4.12860027e-06
Iter: 511 loss: 4.1224157e-06
Iter: 512 loss: 4.13151793e-06
Iter: 513 loss: 4.11943211e-06
Iter: 514 loss: 4.11169322e-06
Iter: 515 loss: 4.1543658e-06
Iter: 516 loss: 4.11065184e-06
Iter: 517 loss: 4.10537086e-06
Iter: 518 loss: 4.13318867e-06
Iter: 519 loss: 4.10444409e-06
Iter: 520 loss: 4.09897393e-06
Iter: 521 loss: 4.09291852e-06
Iter: 522 loss: 4.09217773e-06
Iter: 523 loss: 4.0852924e-06
Iter: 524 loss: 4.18789159e-06
Iter: 525 loss: 4.08523374e-06
Iter: 526 loss: 4.08007782e-06
Iter: 527 loss: 4.07316293e-06
Iter: 528 loss: 4.07287371e-06
Iter: 529 loss: 4.0773848e-06
Iter: 530 loss: 4.06918116e-06
Iter: 531 loss: 4.06668596e-06
Iter: 532 loss: 4.0623022e-06
Iter: 533 loss: 4.16358e-06
Iter: 534 loss: 4.06220124e-06
Iter: 535 loss: 4.05796573e-06
Iter: 536 loss: 4.05218179e-06
Iter: 537 loss: 4.05197898e-06
Iter: 538 loss: 4.04623188e-06
Iter: 539 loss: 4.04596494e-06
Iter: 540 loss: 4.04209368e-06
Iter: 541 loss: 4.03388e-06
Iter: 542 loss: 4.17885803e-06
Iter: 543 loss: 4.03361719e-06
Iter: 544 loss: 4.02585829e-06
Iter: 545 loss: 4.11148721e-06
Iter: 546 loss: 4.02566684e-06
Iter: 547 loss: 4.01928082e-06
Iter: 548 loss: 4.01855596e-06
Iter: 549 loss: 4.01399075e-06
Iter: 550 loss: 4.00690215e-06
Iter: 551 loss: 4.06512027e-06
Iter: 552 loss: 4.00637964e-06
Iter: 553 loss: 4.00015097e-06
Iter: 554 loss: 4.00932049e-06
Iter: 555 loss: 3.9971269e-06
Iter: 556 loss: 3.99025748e-06
Iter: 557 loss: 4.03877357e-06
Iter: 558 loss: 3.98970042e-06
Iter: 559 loss: 3.98395e-06
Iter: 560 loss: 3.98374914e-06
Iter: 561 loss: 3.97937401e-06
Iter: 562 loss: 3.97430267e-06
Iter: 563 loss: 4.04707316e-06
Iter: 564 loss: 3.97433541e-06
Iter: 565 loss: 3.97226222e-06
Iter: 566 loss: 3.97227632e-06
Iter: 567 loss: 3.97000622e-06
Iter: 568 loss: 3.96397036e-06
Iter: 569 loss: 3.98871452e-06
Iter: 570 loss: 3.96144378e-06
Iter: 571 loss: 3.95537791e-06
Iter: 572 loss: 3.98323937e-06
Iter: 573 loss: 3.954151e-06
Iter: 574 loss: 3.9479537e-06
Iter: 575 loss: 3.94847029e-06
Iter: 576 loss: 3.9432889e-06
Iter: 577 loss: 3.93815526e-06
Iter: 578 loss: 4.00113277e-06
Iter: 579 loss: 3.93806113e-06
Iter: 580 loss: 3.93343362e-06
Iter: 581 loss: 3.93652e-06
Iter: 582 loss: 3.93061146e-06
Iter: 583 loss: 3.92359107e-06
Iter: 584 loss: 3.95670213e-06
Iter: 585 loss: 3.9223105e-06
Iter: 586 loss: 3.91850244e-06
Iter: 587 loss: 3.91637514e-06
Iter: 588 loss: 3.91450476e-06
Iter: 589 loss: 3.90685182e-06
Iter: 590 loss: 3.9365782e-06
Iter: 591 loss: 3.9049728e-06
Iter: 592 loss: 3.89963225e-06
Iter: 593 loss: 3.90992091e-06
Iter: 594 loss: 3.89723e-06
Iter: 595 loss: 3.89086108e-06
Iter: 596 loss: 3.92001311e-06
Iter: 597 loss: 3.8895887e-06
Iter: 598 loss: 3.88461331e-06
Iter: 599 loss: 3.90314472e-06
Iter: 600 loss: 3.88334229e-06
Iter: 601 loss: 3.87974023e-06
Iter: 602 loss: 3.87970704e-06
Iter: 603 loss: 3.87606451e-06
Iter: 604 loss: 3.87532418e-06
Iter: 605 loss: 3.87315367e-06
Iter: 606 loss: 3.86990496e-06
Iter: 607 loss: 3.86384227e-06
Iter: 608 loss: 3.9973379e-06
Iter: 609 loss: 3.86382635e-06
Iter: 610 loss: 3.8571784e-06
Iter: 611 loss: 3.89915294e-06
Iter: 612 loss: 3.85624116e-06
Iter: 613 loss: 3.85151816e-06
Iter: 614 loss: 3.85213e-06
Iter: 615 loss: 3.8478147e-06
Iter: 616 loss: 3.84128316e-06
Iter: 617 loss: 3.8880471e-06
Iter: 618 loss: 3.84078248e-06
Iter: 619 loss: 3.83592078e-06
Iter: 620 loss: 3.84080158e-06
Iter: 621 loss: 3.83301176e-06
Iter: 622 loss: 3.82632288e-06
Iter: 623 loss: 3.86118245e-06
Iter: 624 loss: 3.82526832e-06
Iter: 625 loss: 3.82111148e-06
Iter: 626 loss: 3.84711e-06
Iter: 627 loss: 3.82069811e-06
Iter: 628 loss: 3.81671589e-06
Iter: 629 loss: 3.81117206e-06
Iter: 630 loss: 3.81070868e-06
Iter: 631 loss: 3.8046644e-06
Iter: 632 loss: 3.87180171e-06
Iter: 633 loss: 3.804671e-06
Iter: 634 loss: 3.80054848e-06
Iter: 635 loss: 3.80090364e-06
Iter: 636 loss: 3.79738526e-06
Iter: 637 loss: 3.79659787e-06
Iter: 638 loss: 3.79425842e-06
Iter: 639 loss: 3.79290736e-06
Iter: 640 loss: 3.78896129e-06
Iter: 641 loss: 3.79031894e-06
Iter: 642 loss: 3.78514096e-06
Iter: 643 loss: 3.7797422e-06
Iter: 644 loss: 3.86298143e-06
Iter: 645 loss: 3.77967717e-06
Iter: 646 loss: 3.77565425e-06
Iter: 647 loss: 3.78123241e-06
Iter: 648 loss: 3.77364586e-06
Iter: 649 loss: 3.768532e-06
Iter: 650 loss: 3.77523293e-06
Iter: 651 loss: 3.76592288e-06
Iter: 652 loss: 3.76113394e-06
Iter: 653 loss: 3.78328605e-06
Iter: 654 loss: 3.76027219e-06
Iter: 655 loss: 3.75554509e-06
Iter: 656 loss: 3.76373237e-06
Iter: 657 loss: 3.75349055e-06
Iter: 658 loss: 3.74854926e-06
Iter: 659 loss: 3.74725687e-06
Iter: 660 loss: 3.74428987e-06
Iter: 661 loss: 3.73771104e-06
Iter: 662 loss: 3.80379743e-06
Iter: 663 loss: 3.73751595e-06
Iter: 664 loss: 3.73386524e-06
Iter: 665 loss: 3.7658715e-06
Iter: 666 loss: 3.73368175e-06
Iter: 667 loss: 3.73037687e-06
Iter: 668 loss: 3.7262248e-06
Iter: 669 loss: 3.72597106e-06
Iter: 670 loss: 3.72283853e-06
Iter: 671 loss: 3.72266823e-06
Iter: 672 loss: 3.71920555e-06
Iter: 673 loss: 3.73159605e-06
Iter: 674 loss: 3.71839496e-06
Iter: 675 loss: 3.71606052e-06
Iter: 676 loss: 3.70952057e-06
Iter: 677 loss: 3.74173533e-06
Iter: 678 loss: 3.70737507e-06
Iter: 679 loss: 3.7011871e-06
Iter: 680 loss: 3.75325158e-06
Iter: 681 loss: 3.70096495e-06
Iter: 682 loss: 3.69555846e-06
Iter: 683 loss: 3.7051027e-06
Iter: 684 loss: 3.6932795e-06
Iter: 685 loss: 3.68837959e-06
Iter: 686 loss: 3.72520822e-06
Iter: 687 loss: 3.68796668e-06
Iter: 688 loss: 3.6831334e-06
Iter: 689 loss: 3.68531892e-06
Iter: 690 loss: 3.67978282e-06
Iter: 691 loss: 3.67542498e-06
Iter: 692 loss: 3.70577618e-06
Iter: 693 loss: 3.67489497e-06
Iter: 694 loss: 3.67029384e-06
Iter: 695 loss: 3.67025e-06
Iter: 696 loss: 3.66651238e-06
Iter: 697 loss: 3.66197037e-06
Iter: 698 loss: 3.67199868e-06
Iter: 699 loss: 3.66020618e-06
Iter: 700 loss: 3.65462097e-06
Iter: 701 loss: 3.6802669e-06
Iter: 702 loss: 3.65346136e-06
Iter: 703 loss: 3.6489439e-06
Iter: 704 loss: 3.67994517e-06
Iter: 705 loss: 3.64846778e-06
Iter: 706 loss: 3.64525363e-06
Iter: 707 loss: 3.6925212e-06
Iter: 708 loss: 3.64532684e-06
Iter: 709 loss: 3.64242123e-06
Iter: 710 loss: 3.63930371e-06
Iter: 711 loss: 3.63860045e-06
Iter: 712 loss: 3.63578033e-06
Iter: 713 loss: 3.63756794e-06
Iter: 714 loss: 3.63398726e-06
Iter: 715 loss: 3.63014306e-06
Iter: 716 loss: 3.62410083e-06
Iter: 717 loss: 3.62405672e-06
Iter: 718 loss: 3.61837101e-06
Iter: 719 loss: 3.70915086e-06
Iter: 720 loss: 3.61840057e-06
Iter: 721 loss: 3.61422303e-06
Iter: 722 loss: 3.61966704e-06
Iter: 723 loss: 3.61208095e-06
Iter: 724 loss: 3.60804688e-06
Iter: 725 loss: 3.63496e-06
Iter: 726 loss: 3.6076292e-06
Iter: 727 loss: 3.60371587e-06
Iter: 728 loss: 3.60053173e-06
Iter: 729 loss: 3.59932778e-06
Iter: 730 loss: 3.59518845e-06
Iter: 731 loss: 3.59522096e-06
Iter: 732 loss: 3.59170781e-06
Iter: 733 loss: 3.58828083e-06
Iter: 734 loss: 3.58757461e-06
Iter: 735 loss: 3.58245825e-06
Iter: 736 loss: 3.61409866e-06
Iter: 737 loss: 3.58185343e-06
Iter: 738 loss: 3.57781255e-06
Iter: 739 loss: 3.58511261e-06
Iter: 740 loss: 3.5760761e-06
Iter: 741 loss: 3.57511749e-06
Iter: 742 loss: 3.57359977e-06
Iter: 743 loss: 3.57259864e-06
Iter: 744 loss: 3.5693929e-06
Iter: 745 loss: 3.57933368e-06
Iter: 746 loss: 3.56787359e-06
Iter: 747 loss: 3.56298688e-06
Iter: 748 loss: 3.58270154e-06
Iter: 749 loss: 3.56184364e-06
Iter: 750 loss: 3.55782163e-06
Iter: 751 loss: 3.58560851e-06
Iter: 752 loss: 3.55728594e-06
Iter: 753 loss: 3.55413977e-06
Iter: 754 loss: 3.55183897e-06
Iter: 755 loss: 3.5508092e-06
Iter: 756 loss: 3.54571648e-06
Iter: 757 loss: 3.57969247e-06
Iter: 758 loss: 3.54515464e-06
Iter: 759 loss: 3.54136e-06
Iter: 760 loss: 3.53949736e-06
Iter: 761 loss: 3.53773862e-06
Iter: 762 loss: 3.53225187e-06
Iter: 763 loss: 3.581378e-06
Iter: 764 loss: 3.5318219e-06
Iter: 765 loss: 3.52847405e-06
Iter: 766 loss: 3.52628876e-06
Iter: 767 loss: 3.52505435e-06
Iter: 768 loss: 3.52024722e-06
Iter: 769 loss: 3.57739896e-06
Iter: 770 loss: 3.5203027e-06
Iter: 771 loss: 3.5166936e-06
Iter: 772 loss: 3.5246776e-06
Iter: 773 loss: 3.5155922e-06
Iter: 774 loss: 3.51238327e-06
Iter: 775 loss: 3.54511985e-06
Iter: 776 loss: 3.51227891e-06
Iter: 777 loss: 3.50963364e-06
Iter: 778 loss: 3.52890765e-06
Iter: 779 loss: 3.50938444e-06
Iter: 780 loss: 3.5078092e-06
Iter: 781 loss: 3.5034077e-06
Iter: 782 loss: 3.52243205e-06
Iter: 783 loss: 3.50184337e-06
Iter: 784 loss: 3.49674383e-06
Iter: 785 loss: 3.54514668e-06
Iter: 786 loss: 3.49656716e-06
Iter: 787 loss: 3.49283891e-06
Iter: 788 loss: 3.49290576e-06
Iter: 789 loss: 3.48985509e-06
Iter: 790 loss: 3.48469621e-06
Iter: 791 loss: 3.52154893e-06
Iter: 792 loss: 3.48425988e-06
Iter: 793 loss: 3.48081153e-06
Iter: 794 loss: 3.49119159e-06
Iter: 795 loss: 3.47984019e-06
Iter: 796 loss: 3.4758832e-06
Iter: 797 loss: 3.48214985e-06
Iter: 798 loss: 3.47386663e-06
Iter: 799 loss: 3.47049672e-06
Iter: 800 loss: 3.47850619e-06
Iter: 801 loss: 3.46931211e-06
Iter: 802 loss: 3.46548541e-06
Iter: 803 loss: 3.46682782e-06
Iter: 804 loss: 3.46287402e-06
Iter: 805 loss: 3.45853095e-06
Iter: 806 loss: 3.46736465e-06
Iter: 807 loss: 3.45674903e-06
Iter: 808 loss: 3.45093667e-06
Iter: 809 loss: 3.46773254e-06
Iter: 810 loss: 3.44907221e-06
Iter: 811 loss: 3.44865657e-06
Iter: 812 loss: 3.44698651e-06
Iter: 813 loss: 3.44506816e-06
Iter: 814 loss: 3.44666523e-06
Iter: 815 loss: 3.4438724e-06
Iter: 816 loss: 3.44169553e-06
Iter: 817 loss: 3.43914598e-06
Iter: 818 loss: 3.4388263e-06
Iter: 819 loss: 3.43559623e-06
Iter: 820 loss: 3.44212913e-06
Iter: 821 loss: 3.43450097e-06
Iter: 822 loss: 3.43013699e-06
Iter: 823 loss: 3.43005468e-06
Iter: 824 loss: 3.42662793e-06
Iter: 825 loss: 3.42236103e-06
Iter: 826 loss: 3.45089938e-06
Iter: 827 loss: 3.42181147e-06
Iter: 828 loss: 3.417578e-06
Iter: 829 loss: 3.42446515e-06
Iter: 830 loss: 3.41563327e-06
Iter: 831 loss: 3.41199802e-06
Iter: 832 loss: 3.4211007e-06
Iter: 833 loss: 3.41045848e-06
Iter: 834 loss: 3.40611291e-06
Iter: 835 loss: 3.43484226e-06
Iter: 836 loss: 3.40570659e-06
Iter: 837 loss: 3.40266729e-06
Iter: 838 loss: 3.40825932e-06
Iter: 839 loss: 3.40139354e-06
Iter: 840 loss: 3.39803501e-06
Iter: 841 loss: 3.39514918e-06
Iter: 842 loss: 3.39429516e-06
Iter: 843 loss: 3.39032908e-06
Iter: 844 loss: 3.43022293e-06
Iter: 845 loss: 3.39025291e-06
Iter: 846 loss: 3.38675409e-06
Iter: 847 loss: 3.38815062e-06
Iter: 848 loss: 3.38433756e-06
Iter: 849 loss: 3.38281802e-06
Iter: 850 loss: 3.38222435e-06
Iter: 851 loss: 3.38008635e-06
Iter: 852 loss: 3.38597965e-06
Iter: 853 loss: 3.37947745e-06
Iter: 854 loss: 3.37744814e-06
Iter: 855 loss: 3.37344341e-06
Iter: 856 loss: 3.4418772e-06
Iter: 857 loss: 3.37333904e-06
Iter: 858 loss: 3.36905805e-06
Iter: 859 loss: 3.39372423e-06
Iter: 860 loss: 3.36847052e-06
Iter: 861 loss: 3.36548783e-06
Iter: 862 loss: 3.36613311e-06
Iter: 863 loss: 3.36341577e-06
Iter: 864 loss: 3.35939785e-06
Iter: 865 loss: 3.37970278e-06
Iter: 866 loss: 3.3587271e-06
Iter: 867 loss: 3.35528648e-06
Iter: 868 loss: 3.3536719e-06
Iter: 869 loss: 3.35184814e-06
Iter: 870 loss: 3.34841343e-06
Iter: 871 loss: 3.34841729e-06
Iter: 872 loss: 3.34601373e-06
Iter: 873 loss: 3.35033201e-06
Iter: 874 loss: 3.34492347e-06
Iter: 875 loss: 3.34146466e-06
Iter: 876 loss: 3.34191941e-06
Iter: 877 loss: 3.33894241e-06
Iter: 878 loss: 3.33531034e-06
Iter: 879 loss: 3.3414849e-06
Iter: 880 loss: 3.33375237e-06
Iter: 881 loss: 3.32972672e-06
Iter: 882 loss: 3.35047275e-06
Iter: 883 loss: 3.32913055e-06
Iter: 884 loss: 3.32593254e-06
Iter: 885 loss: 3.33700655e-06
Iter: 886 loss: 3.32520654e-06
Iter: 887 loss: 3.32171e-06
Iter: 888 loss: 3.36671246e-06
Iter: 889 loss: 3.32165905e-06
Iter: 890 loss: 3.31985734e-06
Iter: 891 loss: 3.3172214e-06
Iter: 892 loss: 3.31717683e-06
Iter: 893 loss: 3.31431397e-06
Iter: 894 loss: 3.31265778e-06
Iter: 895 loss: 3.3114809e-06
Iter: 896 loss: 3.30678722e-06
Iter: 897 loss: 3.32847412e-06
Iter: 898 loss: 3.30584453e-06
Iter: 899 loss: 3.30264879e-06
Iter: 900 loss: 3.31457659e-06
Iter: 901 loss: 3.30184889e-06
Iter: 902 loss: 3.29814225e-06
Iter: 903 loss: 3.31075853e-06
Iter: 904 loss: 3.29723639e-06
Iter: 905 loss: 3.29433055e-06
Iter: 906 loss: 3.29693239e-06
Iter: 907 loss: 3.2926016e-06
Iter: 908 loss: 3.28842384e-06
Iter: 909 loss: 3.29354884e-06
Iter: 910 loss: 3.28632905e-06
Iter: 911 loss: 3.28250508e-06
Iter: 912 loss: 3.29636441e-06
Iter: 913 loss: 3.2815467e-06
Iter: 914 loss: 3.27803173e-06
Iter: 915 loss: 3.31143906e-06
Iter: 916 loss: 3.2779808e-06
Iter: 917 loss: 3.2756675e-06
Iter: 918 loss: 3.27847852e-06
Iter: 919 loss: 3.27440557e-06
Iter: 920 loss: 3.27161524e-06
Iter: 921 loss: 3.277215e-06
Iter: 922 loss: 3.27043335e-06
Iter: 923 loss: 3.2687135e-06
Iter: 924 loss: 3.26825534e-06
Iter: 925 loss: 3.26717895e-06
Iter: 926 loss: 3.2647456e-06
Iter: 927 loss: 3.29501336e-06
Iter: 928 loss: 3.26459576e-06
Iter: 929 loss: 3.26109034e-06
Iter: 930 loss: 3.25776887e-06
Iter: 931 loss: 3.25715973e-06
Iter: 932 loss: 3.25292262e-06
Iter: 933 loss: 3.27857242e-06
Iter: 934 loss: 3.25232054e-06
Iter: 935 loss: 3.24857115e-06
Iter: 936 loss: 3.25967721e-06
Iter: 937 loss: 3.24728944e-06
Iter: 938 loss: 3.24391749e-06
Iter: 939 loss: 3.25628048e-06
Iter: 940 loss: 3.24311486e-06
Iter: 941 loss: 3.23937115e-06
Iter: 942 loss: 3.24518919e-06
Iter: 943 loss: 3.23751692e-06
Iter: 944 loss: 3.23439485e-06
Iter: 945 loss: 3.27183216e-06
Iter: 946 loss: 3.23434915e-06
Iter: 947 loss: 3.23227e-06
Iter: 948 loss: 3.23023141e-06
Iter: 949 loss: 3.22973301e-06
Iter: 950 loss: 3.22600431e-06
Iter: 951 loss: 3.2491721e-06
Iter: 952 loss: 3.22562232e-06
Iter: 953 loss: 3.22254596e-06
Iter: 954 loss: 3.22066217e-06
Iter: 955 loss: 3.21938523e-06
Iter: 956 loss: 3.21699326e-06
Iter: 957 loss: 3.21679454e-06
Iter: 958 loss: 3.21386324e-06
Iter: 959 loss: 3.22259916e-06
Iter: 960 loss: 3.21300513e-06
Iter: 961 loss: 3.21116931e-06
Iter: 962 loss: 3.20738468e-06
Iter: 963 loss: 3.26345685e-06
Iter: 964 loss: 3.20728691e-06
Iter: 965 loss: 3.20445542e-06
Iter: 966 loss: 3.24797384e-06
Iter: 967 loss: 3.20441518e-06
Iter: 968 loss: 3.20191248e-06
Iter: 969 loss: 3.19903529e-06
Iter: 970 loss: 3.19853211e-06
Iter: 971 loss: 3.19516698e-06
Iter: 972 loss: 3.21161338e-06
Iter: 973 loss: 3.19472883e-06
Iter: 974 loss: 3.19101719e-06
Iter: 975 loss: 3.19256446e-06
Iter: 976 loss: 3.18862976e-06
Iter: 977 loss: 3.18484558e-06
Iter: 978 loss: 3.2180958e-06
Iter: 979 loss: 3.18465618e-06
Iter: 980 loss: 3.18213483e-06
Iter: 981 loss: 3.18476032e-06
Iter: 982 loss: 3.18083812e-06
Iter: 983 loss: 3.17750801e-06
Iter: 984 loss: 3.19702167e-06
Iter: 985 loss: 3.17709032e-06
Iter: 986 loss: 3.17480385e-06
Iter: 987 loss: 3.17688182e-06
Iter: 988 loss: 3.17355079e-06
Iter: 989 loss: 3.17006766e-06
Iter: 990 loss: 3.17677632e-06
Iter: 991 loss: 3.1686136e-06
Iter: 992 loss: 3.16827982e-06
Iter: 993 loss: 3.16754358e-06
Iter: 994 loss: 3.16626938e-06
Iter: 995 loss: 3.16293335e-06
Iter: 996 loss: 3.19154469e-06
Iter: 997 loss: 3.16251908e-06
Iter: 998 loss: 3.15952502e-06
Iter: 999 loss: 3.16997148e-06
Iter: 1000 loss: 3.15880925e-06
Iter: 1001 loss: 3.15604507e-06
Iter: 1002 loss: 3.15644365e-06
Iter: 1003 loss: 3.15387479e-06
Iter: 1004 loss: 3.15085981e-06
Iter: 1005 loss: 3.18728689e-06
Iter: 1006 loss: 3.15076477e-06
Iter: 1007 loss: 3.14846147e-06
Iter: 1008 loss: 3.14863769e-06
Iter: 1009 loss: 3.14669433e-06
Iter: 1010 loss: 3.14316753e-06
Iter: 1011 loss: 3.15892794e-06
Iter: 1012 loss: 3.14241925e-06
Iter: 1013 loss: 3.13995724e-06
Iter: 1014 loss: 3.13869259e-06
Iter: 1015 loss: 3.13744704e-06
Iter: 1016 loss: 3.13364444e-06
Iter: 1017 loss: 3.15461239e-06
Iter: 1018 loss: 3.1331017e-06
Iter: 1019 loss: 3.12999e-06
Iter: 1020 loss: 3.1344789e-06
Iter: 1021 loss: 3.12844918e-06
Iter: 1022 loss: 3.1246127e-06
Iter: 1023 loss: 3.14189401e-06
Iter: 1024 loss: 3.12372458e-06
Iter: 1025 loss: 3.12116026e-06
Iter: 1026 loss: 3.12645898e-06
Iter: 1027 loss: 3.12011525e-06
Iter: 1028 loss: 3.11738722e-06
Iter: 1029 loss: 3.14569752e-06
Iter: 1030 loss: 3.11728263e-06
Iter: 1031 loss: 3.11457393e-06
Iter: 1032 loss: 3.12703128e-06
Iter: 1033 loss: 3.11415033e-06
Iter: 1034 loss: 3.11261556e-06
Iter: 1035 loss: 3.11104941e-06
Iter: 1036 loss: 3.11077156e-06
Iter: 1037 loss: 3.10822043e-06
Iter: 1038 loss: 3.10538417e-06
Iter: 1039 loss: 3.10493965e-06
Iter: 1040 loss: 3.10181395e-06
Iter: 1041 loss: 3.13565101e-06
Iter: 1042 loss: 3.10169094e-06
Iter: 1043 loss: 3.09888242e-06
Iter: 1044 loss: 3.09857364e-06
Iter: 1045 loss: 3.09646907e-06
Iter: 1046 loss: 3.09310917e-06
Iter: 1047 loss: 3.10948144e-06
Iter: 1048 loss: 3.09241341e-06
Iter: 1049 loss: 3.08898234e-06
Iter: 1050 loss: 3.10683299e-06
Iter: 1051 loss: 3.08852941e-06
Iter: 1052 loss: 3.0865333e-06
Iter: 1053 loss: 3.09176153e-06
Iter: 1054 loss: 3.0858705e-06
Iter: 1055 loss: 3.08285757e-06
Iter: 1056 loss: 3.08439985e-06
Iter: 1057 loss: 3.08098242e-06
Iter: 1058 loss: 3.07793562e-06
Iter: 1059 loss: 3.09039706e-06
Iter: 1060 loss: 3.07743517e-06
Iter: 1061 loss: 3.07441019e-06
Iter: 1062 loss: 3.07406322e-06
Iter: 1063 loss: 3.07197479e-06
Iter: 1064 loss: 3.07009486e-06
Iter: 1065 loss: 3.06987363e-06
Iter: 1066 loss: 3.06795482e-06
Iter: 1067 loss: 3.08001654e-06
Iter: 1068 loss: 3.06784568e-06
Iter: 1069 loss: 3.06639163e-06
Iter: 1070 loss: 3.0633646e-06
Iter: 1071 loss: 3.1147456e-06
Iter: 1072 loss: 3.0633405e-06
Iter: 1073 loss: 3.06076663e-06
Iter: 1074 loss: 3.07645723e-06
Iter: 1075 loss: 3.06039601e-06
Iter: 1076 loss: 3.05798449e-06
Iter: 1077 loss: 3.05624053e-06
Iter: 1078 loss: 3.05554136e-06
Iter: 1079 loss: 3.05187746e-06
Iter: 1080 loss: 3.07972232e-06
Iter: 1081 loss: 3.05156618e-06
Iter: 1082 loss: 3.0491592e-06
Iter: 1083 loss: 3.04998e-06
Iter: 1084 loss: 3.04762034e-06
Iter: 1085 loss: 3.04400578e-06
Iter: 1086 loss: 3.05360049e-06
Iter: 1087 loss: 3.04271452e-06
Iter: 1088 loss: 3.03968682e-06
Iter: 1089 loss: 3.04340438e-06
Iter: 1090 loss: 3.03821776e-06
Iter: 1091 loss: 3.03582169e-06
Iter: 1092 loss: 3.03573802e-06
Iter: 1093 loss: 3.03371053e-06
Iter: 1094 loss: 3.03304705e-06
Iter: 1095 loss: 3.03206889e-06
Iter: 1096 loss: 3.02922217e-06
Iter: 1097 loss: 3.04517789e-06
Iter: 1098 loss: 3.02884337e-06
Iter: 1099 loss: 3.02702324e-06
Iter: 1100 loss: 3.03125967e-06
Iter: 1101 loss: 3.02626313e-06
Iter: 1102 loss: 3.02383e-06
Iter: 1103 loss: 3.05023059e-06
Iter: 1104 loss: 3.02378498e-06
Iter: 1105 loss: 3.02246053e-06
Iter: 1106 loss: 3.01983528e-06
Iter: 1107 loss: 3.07313576e-06
Iter: 1108 loss: 3.01979026e-06
Iter: 1109 loss: 3.01763748e-06
Iter: 1110 loss: 3.01674527e-06
Iter: 1111 loss: 3.01562841e-06
Iter: 1112 loss: 3.01270325e-06
Iter: 1113 loss: 3.04746618e-06
Iter: 1114 loss: 3.01257637e-06
Iter: 1115 loss: 3.01053137e-06
Iter: 1116 loss: 3.01288901e-06
Iter: 1117 loss: 3.00935153e-06
Iter: 1118 loss: 3.00617739e-06
Iter: 1119 loss: 3.01058662e-06
Iter: 1120 loss: 3.00452621e-06
Iter: 1121 loss: 3.00195643e-06
Iter: 1122 loss: 3.00262786e-06
Iter: 1123 loss: 3.00015199e-06
Iter: 1124 loss: 2.99664543e-06
Iter: 1125 loss: 3.02956096e-06
Iter: 1126 loss: 2.99655585e-06
Iter: 1127 loss: 2.99425369e-06
Iter: 1128 loss: 2.99315025e-06
Iter: 1129 loss: 2.99190469e-06
Iter: 1130 loss: 2.98854184e-06
Iter: 1131 loss: 3.02685839e-06
Iter: 1132 loss: 2.9885673e-06
Iter: 1133 loss: 2.98648138e-06
Iter: 1134 loss: 2.988711e-06
Iter: 1135 loss: 2.98548139e-06
Iter: 1136 loss: 2.98288796e-06
Iter: 1137 loss: 3.00254123e-06
Iter: 1138 loss: 2.98273267e-06
Iter: 1139 loss: 2.981433e-06
Iter: 1140 loss: 2.98140139e-06
Iter: 1141 loss: 2.98071427e-06
Iter: 1142 loss: 2.97855468e-06
Iter: 1143 loss: 2.98174928e-06
Iter: 1144 loss: 2.97717133e-06
Iter: 1145 loss: 2.97413658e-06
Iter: 1146 loss: 2.99598241e-06
Iter: 1147 loss: 2.97386487e-06
Iter: 1148 loss: 2.97135557e-06
Iter: 1149 loss: 2.97278848e-06
Iter: 1150 loss: 2.96973849e-06
Iter: 1151 loss: 2.96659209e-06
Iter: 1152 loss: 2.98783948e-06
Iter: 1153 loss: 2.96621488e-06
Iter: 1154 loss: 2.96384314e-06
Iter: 1155 loss: 2.96595249e-06
Iter: 1156 loss: 2.96252347e-06
Iter: 1157 loss: 2.95905375e-06
Iter: 1158 loss: 2.9741484e-06
Iter: 1159 loss: 2.95845666e-06
Iter: 1160 loss: 2.95649579e-06
Iter: 1161 loss: 2.95819291e-06
Iter: 1162 loss: 2.95522614e-06
Iter: 1163 loss: 2.95219115e-06
Iter: 1164 loss: 2.95459e-06
Iter: 1165 loss: 2.95015798e-06
Iter: 1166 loss: 2.94742063e-06
Iter: 1167 loss: 2.96727421e-06
Iter: 1168 loss: 2.94726647e-06
Iter: 1169 loss: 2.94488268e-06
Iter: 1170 loss: 2.94702932e-06
Iter: 1171 loss: 2.94346273e-06
Iter: 1172 loss: 2.9440514e-06
Iter: 1173 loss: 2.94248593e-06
Iter: 1174 loss: 2.94160532e-06
Iter: 1175 loss: 2.93990865e-06
Iter: 1176 loss: 2.93997027e-06
Iter: 1177 loss: 2.93763696e-06
Iter: 1178 loss: 2.93716448e-06
Iter: 1179 loss: 2.93566245e-06
Iter: 1180 loss: 2.93340327e-06
Iter: 1181 loss: 2.93926905e-06
Iter: 1182 loss: 2.9327116e-06
Iter: 1183 loss: 2.93004337e-06
Iter: 1184 loss: 2.93393987e-06
Iter: 1185 loss: 2.92882282e-06
Iter: 1186 loss: 2.92628465e-06
Iter: 1187 loss: 2.9332316e-06
Iter: 1188 loss: 2.92537698e-06
Iter: 1189 loss: 2.92247205e-06
Iter: 1190 loss: 2.92410596e-06
Iter: 1191 loss: 2.9204914e-06
Iter: 1192 loss: 2.91782635e-06
Iter: 1193 loss: 2.95094e-06
Iter: 1194 loss: 2.91775405e-06
Iter: 1195 loss: 2.91505603e-06
Iter: 1196 loss: 2.91446759e-06
Iter: 1197 loss: 2.91268702e-06
Iter: 1198 loss: 2.91016204e-06
Iter: 1199 loss: 2.93002654e-06
Iter: 1200 loss: 2.91002448e-06
Iter: 1201 loss: 2.90756066e-06
Iter: 1202 loss: 2.90615435e-06
Iter: 1203 loss: 2.905271e-06
Iter: 1204 loss: 2.90275966e-06
Iter: 1205 loss: 2.92690856e-06
Iter: 1206 loss: 2.90258504e-06
Iter: 1207 loss: 2.90032972e-06
Iter: 1208 loss: 2.90994876e-06
Iter: 1209 loss: 2.89987292e-06
Iter: 1210 loss: 2.89737523e-06
Iter: 1211 loss: 2.91920151e-06
Iter: 1212 loss: 2.89717445e-06
Iter: 1213 loss: 2.89655054e-06
Iter: 1214 loss: 2.89459194e-06
Iter: 1215 loss: 2.90361368e-06
Iter: 1216 loss: 2.89376453e-06
Iter: 1217 loss: 2.89063905e-06
Iter: 1218 loss: 2.90586468e-06
Iter: 1219 loss: 2.89006766e-06
Iter: 1220 loss: 2.8875811e-06
Iter: 1221 loss: 2.89194077e-06
Iter: 1222 loss: 2.88649926e-06
Iter: 1223 loss: 2.88330921e-06
Iter: 1224 loss: 2.90128901e-06
Iter: 1225 loss: 2.88297383e-06
Iter: 1226 loss: 2.88095e-06
Iter: 1227 loss: 2.88666547e-06
Iter: 1228 loss: 2.88028878e-06
Iter: 1229 loss: 2.87798184e-06
Iter: 1230 loss: 2.87648663e-06
Iter: 1231 loss: 2.87570128e-06
Iter: 1232 loss: 2.87315788e-06
Iter: 1233 loss: 2.90689559e-06
Iter: 1234 loss: 2.87314197e-06
Iter: 1235 loss: 2.87101284e-06
Iter: 1236 loss: 2.8695963e-06
Iter: 1237 loss: 2.86882414e-06
Iter: 1238 loss: 2.86566365e-06
Iter: 1239 loss: 2.89235527e-06
Iter: 1240 loss: 2.86541e-06
Iter: 1241 loss: 2.86335489e-06
Iter: 1242 loss: 2.8632503e-06
Iter: 1243 loss: 2.86170643e-06
Iter: 1244 loss: 2.85957685e-06
Iter: 1245 loss: 2.85963642e-06
Iter: 1246 loss: 2.85778515e-06
Iter: 1247 loss: 2.87138528e-06
Iter: 1248 loss: 2.8576419e-06
Iter: 1249 loss: 2.8567797e-06
Iter: 1250 loss: 2.85445731e-06
Iter: 1251 loss: 2.8659083e-06
Iter: 1252 loss: 2.85351e-06
Iter: 1253 loss: 2.8507452e-06
Iter: 1254 loss: 2.87280886e-06
Iter: 1255 loss: 2.85057013e-06
Iter: 1256 loss: 2.84832277e-06
Iter: 1257 loss: 2.84831276e-06
Iter: 1258 loss: 2.84646103e-06
Iter: 1259 loss: 2.84395333e-06
Iter: 1260 loss: 2.8719578e-06
Iter: 1261 loss: 2.84391035e-06
Iter: 1262 loss: 2.84201269e-06
Iter: 1263 loss: 2.83968279e-06
Iter: 1264 loss: 2.83950976e-06
Iter: 1265 loss: 2.83660893e-06
Iter: 1266 loss: 2.83672784e-06
Iter: 1267 loss: 2.83467398e-06
Iter: 1268 loss: 2.83468125e-06
Iter: 1269 loss: 2.8332322e-06
Iter: 1270 loss: 2.83008649e-06
Iter: 1271 loss: 2.83353802e-06
Iter: 1272 loss: 2.82848464e-06
Iter: 1273 loss: 2.82616588e-06
Iter: 1274 loss: 2.83578061e-06
Iter: 1275 loss: 2.82565838e-06
Iter: 1276 loss: 2.8229133e-06
Iter: 1277 loss: 2.82438941e-06
Iter: 1278 loss: 2.82106544e-06
Iter: 1279 loss: 2.82070346e-06
Iter: 1280 loss: 2.82003111e-06
Iter: 1281 loss: 2.81866642e-06
Iter: 1282 loss: 2.81674488e-06
Iter: 1283 loss: 2.81661414e-06
Iter: 1284 loss: 2.8143113e-06
Iter: 1285 loss: 2.81636949e-06
Iter: 1286 loss: 2.81278494e-06
Iter: 1287 loss: 2.81097414e-06
Iter: 1288 loss: 2.81877146e-06
Iter: 1289 loss: 2.81058396e-06
Iter: 1290 loss: 2.80845597e-06
Iter: 1291 loss: 2.80713311e-06
Iter: 1292 loss: 2.80626318e-06
Iter: 1293 loss: 2.80415543e-06
Iter: 1294 loss: 2.82390465e-06
Iter: 1295 loss: 2.8040281e-06
Iter: 1296 loss: 2.80183e-06
Iter: 1297 loss: 2.79869232e-06
Iter: 1298 loss: 2.79854385e-06
Iter: 1299 loss: 2.7958904e-06
Iter: 1300 loss: 2.79592632e-06
Iter: 1301 loss: 2.79376354e-06
Iter: 1302 loss: 2.79421374e-06
Iter: 1303 loss: 2.79219103e-06
Iter: 1304 loss: 2.79011238e-06
Iter: 1305 loss: 2.80927907e-06
Iter: 1306 loss: 2.79004303e-06
Iter: 1307 loss: 2.78806101e-06
Iter: 1308 loss: 2.78798393e-06
Iter: 1309 loss: 2.78637413e-06
Iter: 1310 loss: 2.78412131e-06
Iter: 1311 loss: 2.79736469e-06
Iter: 1312 loss: 2.78370749e-06
Iter: 1313 loss: 2.78269908e-06
Iter: 1314 loss: 2.78259313e-06
Iter: 1315 loss: 2.78128073e-06
Iter: 1316 loss: 2.77862773e-06
Iter: 1317 loss: 2.82261544e-06
Iter: 1318 loss: 2.77846289e-06
Iter: 1319 loss: 2.7763715e-06
Iter: 1320 loss: 2.78184098e-06
Iter: 1321 loss: 2.77560798e-06
Iter: 1322 loss: 2.77337699e-06
Iter: 1323 loss: 2.77614777e-06
Iter: 1324 loss: 2.77220192e-06
Iter: 1325 loss: 2.76951869e-06
Iter: 1326 loss: 2.7824035e-06
Iter: 1327 loss: 2.76898754e-06
Iter: 1328 loss: 2.76728019e-06
Iter: 1329 loss: 2.76932678e-06
Iter: 1330 loss: 2.76638775e-06
Iter: 1331 loss: 2.76402784e-06
Iter: 1332 loss: 2.7681e-06
Iter: 1333 loss: 2.76295395e-06
Iter: 1334 loss: 2.76067112e-06
Iter: 1335 loss: 2.76276683e-06
Iter: 1336 loss: 2.75936668e-06
Iter: 1337 loss: 2.75628713e-06
Iter: 1338 loss: 2.77676622e-06
Iter: 1339 loss: 2.75598177e-06
Iter: 1340 loss: 2.75401067e-06
Iter: 1341 loss: 2.75616367e-06
Iter: 1342 loss: 2.75307821e-06
Iter: 1343 loss: 2.75022444e-06
Iter: 1344 loss: 2.76322521e-06
Iter: 1345 loss: 2.74970193e-06
Iter: 1346 loss: 2.74803233e-06
Iter: 1347 loss: 2.75551565e-06
Iter: 1348 loss: 2.7475719e-06
Iter: 1349 loss: 2.74624472e-06
Iter: 1350 loss: 2.74623744e-06
Iter: 1351 loss: 2.74505487e-06
Iter: 1352 loss: 2.74201375e-06
Iter: 1353 loss: 2.7648598e-06
Iter: 1354 loss: 2.74130343e-06
Iter: 1355 loss: 2.73906335e-06
Iter: 1356 loss: 2.75265324e-06
Iter: 1357 loss: 2.73881824e-06
Iter: 1358 loss: 2.73643e-06
Iter: 1359 loss: 2.73617843e-06
Iter: 1360 loss: 2.73439673e-06
Iter: 1361 loss: 2.73186583e-06
Iter: 1362 loss: 2.77029608e-06
Iter: 1363 loss: 2.73189698e-06
Iter: 1364 loss: 2.73030173e-06
Iter: 1365 loss: 2.72841726e-06
Iter: 1366 loss: 2.72826287e-06
Iter: 1367 loss: 2.72527882e-06
Iter: 1368 loss: 2.74565082e-06
Iter: 1369 loss: 2.72497186e-06
Iter: 1370 loss: 2.72268562e-06
Iter: 1371 loss: 2.72073294e-06
Iter: 1372 loss: 2.72007765e-06
Iter: 1373 loss: 2.71716408e-06
Iter: 1374 loss: 2.75390676e-06
Iter: 1375 loss: 2.71716e-06
Iter: 1376 loss: 2.71488125e-06
Iter: 1377 loss: 2.71604426e-06
Iter: 1378 loss: 2.71342151e-06
Iter: 1379 loss: 2.71099361e-06
Iter: 1380 loss: 2.73567457e-06
Iter: 1381 loss: 2.71089857e-06
Iter: 1382 loss: 2.70923329e-06
Iter: 1383 loss: 2.71855674e-06
Iter: 1384 loss: 2.70899181e-06
Iter: 1385 loss: 2.70710143e-06
Iter: 1386 loss: 2.715964e-06
Iter: 1387 loss: 2.70668033e-06
Iter: 1388 loss: 2.70524743e-06
Iter: 1389 loss: 2.704395e-06
Iter: 1390 loss: 2.70390228e-06
Iter: 1391 loss: 2.70215378e-06
Iter: 1392 loss: 2.69963402e-06
Iter: 1393 loss: 2.69955308e-06
Iter: 1394 loss: 2.6962191e-06
Iter: 1395 loss: 2.72371312e-06
Iter: 1396 loss: 2.6959865e-06
Iter: 1397 loss: 2.69385509e-06
Iter: 1398 loss: 2.69696284e-06
Iter: 1399 loss: 2.69286238e-06
Iter: 1400 loss: 2.69008342e-06
Iter: 1401 loss: 2.70026612e-06
Iter: 1402 loss: 2.68937288e-06
Iter: 1403 loss: 2.68731446e-06
Iter: 1404 loss: 2.69684051e-06
Iter: 1405 loss: 2.68680401e-06
Iter: 1406 loss: 2.68460872e-06
Iter: 1407 loss: 2.6838386e-06
Iter: 1408 loss: 2.6825353e-06
Iter: 1409 loss: 2.68039889e-06
Iter: 1410 loss: 2.69126895e-06
Iter: 1411 loss: 2.67995438e-06
Iter: 1412 loss: 2.6771886e-06
Iter: 1413 loss: 2.67830842e-06
Iter: 1414 loss: 2.67535984e-06
Iter: 1415 loss: 2.67292262e-06
Iter: 1416 loss: 2.6905559e-06
Iter: 1417 loss: 2.67279574e-06
Iter: 1418 loss: 2.67157884e-06
Iter: 1419 loss: 2.67138603e-06
Iter: 1420 loss: 2.67005271e-06
Iter: 1421 loss: 2.66788675e-06
Iter: 1422 loss: 2.66794723e-06
Iter: 1423 loss: 2.66623351e-06
Iter: 1424 loss: 2.67125506e-06
Iter: 1425 loss: 2.66574716e-06
Iter: 1426 loss: 2.66370148e-06
Iter: 1427 loss: 2.6620728e-06
Iter: 1428 loss: 2.66152983e-06
Iter: 1429 loss: 2.65890458e-06
Iter: 1430 loss: 2.67171185e-06
Iter: 1431 loss: 2.65849599e-06
Iter: 1432 loss: 2.65594122e-06
Iter: 1433 loss: 2.65957533e-06
Iter: 1434 loss: 2.65472909e-06
Iter: 1435 loss: 2.65250083e-06
Iter: 1436 loss: 2.6642e-06
Iter: 1437 loss: 2.65210065e-06
Iter: 1438 loss: 2.6497753e-06
Iter: 1439 loss: 2.65045446e-06
Iter: 1440 loss: 2.64806567e-06
Iter: 1441 loss: 2.64592791e-06
Iter: 1442 loss: 2.6636626e-06
Iter: 1443 loss: 2.64580694e-06
Iter: 1444 loss: 2.64350092e-06
Iter: 1445 loss: 2.64396613e-06
Iter: 1446 loss: 2.64182836e-06
Iter: 1447 loss: 2.63944753e-06
Iter: 1448 loss: 2.65919516e-06
Iter: 1449 loss: 2.63929928e-06
Iter: 1450 loss: 2.63764264e-06
Iter: 1451 loss: 2.6364819e-06
Iter: 1452 loss: 2.63586253e-06
Iter: 1453 loss: 2.63624042e-06
Iter: 1454 loss: 2.6347102e-06
Iter: 1455 loss: 2.63388051e-06
Iter: 1456 loss: 2.63208813e-06
Iter: 1457 loss: 2.66480561e-06
Iter: 1458 loss: 2.63209881e-06
Iter: 1459 loss: 2.63022093e-06
Iter: 1460 loss: 2.62745834e-06
Iter: 1461 loss: 2.62739559e-06
Iter: 1462 loss: 2.62562025e-06
Iter: 1463 loss: 2.62544518e-06
Iter: 1464 loss: 2.62416779e-06
Iter: 1465 loss: 2.62364392e-06
Iter: 1466 loss: 2.62309186e-06
Iter: 1467 loss: 2.62067351e-06
Iter: 1468 loss: 2.62474987e-06
Iter: 1469 loss: 2.61967193e-06
Iter: 1470 loss: 2.61755667e-06
Iter: 1471 loss: 2.61964874e-06
Iter: 1472 loss: 2.61619653e-06
Iter: 1473 loss: 2.61317e-06
Iter: 1474 loss: 2.62178742e-06
Iter: 1475 loss: 2.61223749e-06
Iter: 1476 loss: 2.6096659e-06
Iter: 1477 loss: 2.62170033e-06
Iter: 1478 loss: 2.60912111e-06
Iter: 1479 loss: 2.60674187e-06
Iter: 1480 loss: 2.60962452e-06
Iter: 1481 loss: 2.60544289e-06
Iter: 1482 loss: 2.60321167e-06
Iter: 1483 loss: 2.61264358e-06
Iter: 1484 loss: 2.60261436e-06
Iter: 1485 loss: 2.60048478e-06
Iter: 1486 loss: 2.61860782e-06
Iter: 1487 loss: 2.60041497e-06
Iter: 1488 loss: 2.59864373e-06
Iter: 1489 loss: 2.61980949e-06
Iter: 1490 loss: 2.59871422e-06
Iter: 1491 loss: 2.59787112e-06
Iter: 1492 loss: 2.59644412e-06
Iter: 1493 loss: 2.62713911e-06
Iter: 1494 loss: 2.59643639e-06
Iter: 1495 loss: 2.59437593e-06
Iter: 1496 loss: 2.59509852e-06
Iter: 1497 loss: 2.59294984e-06
Iter: 1498 loss: 2.59111357e-06
Iter: 1499 loss: 2.59608623e-06
Iter: 1500 loss: 2.59044486e-06
Iter: 1501 loss: 2.58798673e-06
Iter: 1502 loss: 2.59188619e-06
Iter: 1503 loss: 2.5867871e-06
Iter: 1504 loss: 2.58501e-06
Iter: 1505 loss: 2.60111756e-06
Iter: 1506 loss: 2.58491514e-06
Iter: 1507 loss: 2.58313548e-06
Iter: 1508 loss: 2.58289924e-06
Iter: 1509 loss: 2.58158934e-06
Iter: 1510 loss: 2.57944407e-06
Iter: 1511 loss: 2.59452145e-06
Iter: 1512 loss: 2.57919532e-06
Iter: 1513 loss: 2.57742181e-06
Iter: 1514 loss: 2.57536249e-06
Iter: 1515 loss: 2.57511033e-06
Iter: 1516 loss: 2.57276542e-06
Iter: 1517 loss: 2.60378965e-06
Iter: 1518 loss: 2.57284591e-06
Iter: 1519 loss: 2.57087618e-06
Iter: 1520 loss: 2.56788326e-06
Iter: 1521 loss: 2.56784188e-06
Iter: 1522 loss: 2.57057627e-06
Iter: 1523 loss: 2.56677e-06
Iter: 1524 loss: 2.56572821e-06
Iter: 1525 loss: 2.56451563e-06
Iter: 1526 loss: 2.56437306e-06
Iter: 1527 loss: 2.56249541e-06
Iter: 1528 loss: 2.56061071e-06
Iter: 1529 loss: 2.56030216e-06
Iter: 1530 loss: 2.5581985e-06
Iter: 1531 loss: 2.57898773e-06
Iter: 1532 loss: 2.55816235e-06
Iter: 1533 loss: 2.55625628e-06
Iter: 1534 loss: 2.55589407e-06
Iter: 1535 loss: 2.55471014e-06
Iter: 1536 loss: 2.5522902e-06
Iter: 1537 loss: 2.57249258e-06
Iter: 1538 loss: 2.55222358e-06
Iter: 1539 loss: 2.55082841e-06
Iter: 1540 loss: 2.54953352e-06
Iter: 1541 loss: 2.54916381e-06
Iter: 1542 loss: 2.54674774e-06
Iter: 1543 loss: 2.56126259e-06
Iter: 1544 loss: 2.54627867e-06
Iter: 1545 loss: 2.54427459e-06
Iter: 1546 loss: 2.55122222e-06
Iter: 1547 loss: 2.54370957e-06
Iter: 1548 loss: 2.54157044e-06
Iter: 1549 loss: 2.54757515e-06
Iter: 1550 loss: 2.5408192e-06
Iter: 1551 loss: 2.53917233e-06
Iter: 1552 loss: 2.53927465e-06
Iter: 1553 loss: 2.53785811e-06
Iter: 1554 loss: 2.53538974e-06
Iter: 1555 loss: 2.55050622e-06
Iter: 1556 loss: 2.53511189e-06
Iter: 1557 loss: 2.53483404e-06
Iter: 1558 loss: 2.53422604e-06
Iter: 1559 loss: 2.53355779e-06
Iter: 1560 loss: 2.53195458e-06
Iter: 1561 loss: 2.54890165e-06
Iter: 1562 loss: 2.53180065e-06
Iter: 1563 loss: 2.52963855e-06
Iter: 1564 loss: 2.52899395e-06
Iter: 1565 loss: 2.52774475e-06
Iter: 1566 loss: 2.52560949e-06
Iter: 1567 loss: 2.54638735e-06
Iter: 1568 loss: 2.52557311e-06
Iter: 1569 loss: 2.52360087e-06
Iter: 1570 loss: 2.5229624e-06
Iter: 1571 loss: 2.52187556e-06
Iter: 1572 loss: 2.51974734e-06
Iter: 1573 loss: 2.54950305e-06
Iter: 1574 loss: 2.51973779e-06
Iter: 1575 loss: 2.51836195e-06
Iter: 1576 loss: 2.51639494e-06
Iter: 1577 loss: 2.51633264e-06
Iter: 1578 loss: 2.51378833e-06
Iter: 1579 loss: 2.5373472e-06
Iter: 1580 loss: 2.51371262e-06
Iter: 1581 loss: 2.51193615e-06
Iter: 1582 loss: 2.51409597e-06
Iter: 1583 loss: 2.5109548e-06
Iter: 1584 loss: 2.50883159e-06
Iter: 1585 loss: 2.51980941e-06
Iter: 1586 loss: 2.50842e-06
Iter: 1587 loss: 2.50696485e-06
Iter: 1588 loss: 2.50830431e-06
Iter: 1589 loss: 2.50605945e-06
Iter: 1590 loss: 2.50430594e-06
Iter: 1591 loss: 2.5169204e-06
Iter: 1592 loss: 2.50414155e-06
Iter: 1593 loss: 2.50253447e-06
Iter: 1594 loss: 2.51641177e-06
Iter: 1595 loss: 2.50242329e-06
Iter: 1596 loss: 2.5017298e-06
Iter: 1597 loss: 2.50002654e-06
Iter: 1598 loss: 2.51970209e-06
Iter: 1599 loss: 2.49992513e-06
Iter: 1600 loss: 2.49754248e-06
Iter: 1601 loss: 2.50192033e-06
Iter: 1602 loss: 2.49649497e-06
Iter: 1603 loss: 2.4947135e-06
Iter: 1604 loss: 2.50136077e-06
Iter: 1605 loss: 2.49425375e-06
Iter: 1606 loss: 2.4924118e-06
Iter: 1607 loss: 2.49940194e-06
Iter: 1608 loss: 2.49199502e-06
Iter: 1609 loss: 2.49015352e-06
Iter: 1610 loss: 2.4934393e-06
Iter: 1611 loss: 2.48943888e-06
Iter: 1612 loss: 2.48727542e-06
Iter: 1613 loss: 2.48851029e-06
Iter: 1614 loss: 2.4859778e-06
Iter: 1615 loss: 2.48412289e-06
Iter: 1616 loss: 2.504642e-06
Iter: 1617 loss: 2.48402807e-06
Iter: 1618 loss: 2.48260494e-06
Iter: 1619 loss: 2.48234414e-06
Iter: 1620 loss: 2.48132574e-06
Iter: 1621 loss: 2.47895105e-06
Iter: 1622 loss: 2.49293703e-06
Iter: 1623 loss: 2.47863272e-06
Iter: 1624 loss: 2.47703724e-06
Iter: 1625 loss: 2.47972184e-06
Iter: 1626 loss: 2.47642788e-06
Iter: 1627 loss: 2.47521689e-06
Iter: 1628 loss: 2.47521848e-06
Iter: 1629 loss: 2.47403273e-06
Iter: 1630 loss: 2.47259914e-06
Iter: 1631 loss: 2.47250705e-06
Iter: 1632 loss: 2.47136268e-06
Iter: 1633 loss: 2.47047228e-06
Iter: 1634 loss: 2.47011349e-06
Iter: 1635 loss: 2.46786271e-06
Iter: 1636 loss: 2.47303819e-06
Iter: 1637 loss: 2.46687864e-06
Iter: 1638 loss: 2.4651813e-06
Iter: 1639 loss: 2.47609887e-06
Iter: 1640 loss: 2.46496893e-06
Iter: 1641 loss: 2.46327795e-06
Iter: 1642 loss: 2.46368973e-06
Iter: 1643 loss: 2.46205173e-06
Iter: 1644 loss: 2.46015679e-06
Iter: 1645 loss: 2.47586013e-06
Iter: 1646 loss: 2.46009699e-06
Iter: 1647 loss: 2.45858087e-06
Iter: 1648 loss: 2.45788897e-06
Iter: 1649 loss: 2.45722822e-06
Iter: 1650 loss: 2.45522915e-06
Iter: 1651 loss: 2.46828586e-06
Iter: 1652 loss: 2.45508363e-06
Iter: 1653 loss: 2.45341016e-06
Iter: 1654 loss: 2.4556175e-06
Iter: 1655 loss: 2.45253568e-06
Iter: 1656 loss: 2.45037836e-06
Iter: 1657 loss: 2.45849924e-06
Iter: 1658 loss: 2.44979969e-06
Iter: 1659 loss: 2.44830039e-06
Iter: 1660 loss: 2.45601268e-06
Iter: 1661 loss: 2.44805346e-06
Iter: 1662 loss: 2.44639796e-06
Iter: 1663 loss: 2.46013769e-06
Iter: 1664 loss: 2.44626949e-06
Iter: 1665 loss: 2.4453152e-06
Iter: 1666 loss: 2.44391913e-06
Iter: 1667 loss: 2.4439164e-06
Iter: 1668 loss: 2.4423166e-06
Iter: 1669 loss: 2.4416895e-06
Iter: 1670 loss: 2.44083867e-06
Iter: 1671 loss: 2.43871909e-06
Iter: 1672 loss: 2.45543856e-06
Iter: 1673 loss: 2.43858358e-06
Iter: 1674 loss: 2.43726663e-06
Iter: 1675 loss: 2.44000557e-06
Iter: 1676 loss: 2.4366409e-06
Iter: 1677 loss: 2.43468389e-06
Iter: 1678 loss: 2.43624845e-06
Iter: 1679 loss: 2.43347131e-06
Iter: 1680 loss: 2.43158047e-06
Iter: 1681 loss: 2.44731086e-06
Iter: 1682 loss: 2.43144723e-06
Iter: 1683 loss: 2.4299834e-06
Iter: 1684 loss: 2.42840861e-06
Iter: 1685 loss: 2.42813303e-06
Iter: 1686 loss: 2.42596616e-06
Iter: 1687 loss: 2.44950115e-06
Iter: 1688 loss: 2.42586248e-06
Iter: 1689 loss: 2.42445094e-06
Iter: 1690 loss: 2.42660099e-06
Iter: 1691 loss: 2.42385568e-06
Iter: 1692 loss: 2.42208375e-06
Iter: 1693 loss: 2.42882493e-06
Iter: 1694 loss: 2.42149235e-06
Iter: 1695 loss: 2.42050146e-06
Iter: 1696 loss: 2.43385193e-06
Iter: 1697 loss: 2.42046235e-06
Iter: 1698 loss: 2.419152e-06
Iter: 1699 loss: 2.41944599e-06
Iter: 1700 loss: 2.41819816e-06
Iter: 1701 loss: 2.416808e-06
Iter: 1702 loss: 2.41586895e-06
Iter: 1703 loss: 2.41541147e-06
Iter: 1704 loss: 2.41363659e-06
Iter: 1705 loss: 2.415469e-06
Iter: 1706 loss: 2.41272642e-06
Iter: 1707 loss: 2.41038583e-06
Iter: 1708 loss: 2.42035458e-06
Iter: 1709 loss: 2.40998179e-06
Iter: 1710 loss: 2.40837676e-06
Iter: 1711 loss: 2.40930149e-06
Iter: 1712 loss: 2.40738382e-06
Iter: 1713 loss: 2.40516852e-06
Iter: 1714 loss: 2.42277929e-06
Iter: 1715 loss: 2.40497457e-06
Iter: 1716 loss: 2.40358236e-06
Iter: 1717 loss: 2.40430813e-06
Iter: 1718 loss: 2.40262671e-06
Iter: 1719 loss: 2.40051054e-06
Iter: 1720 loss: 2.40441204e-06
Iter: 1721 loss: 2.39956648e-06
Iter: 1722 loss: 2.39774727e-06
Iter: 1723 loss: 2.4065007e-06
Iter: 1724 loss: 2.39745441e-06
Iter: 1725 loss: 2.39575365e-06
Iter: 1726 loss: 2.39938709e-06
Iter: 1727 loss: 2.39521228e-06
Iter: 1728 loss: 2.39389192e-06
Iter: 1729 loss: 2.40592726e-06
Iter: 1730 loss: 2.39382416e-06
Iter: 1731 loss: 2.39286419e-06
Iter: 1732 loss: 2.39878136e-06
Iter: 1733 loss: 2.3928028e-06
Iter: 1734 loss: 2.39155861e-06
Iter: 1735 loss: 2.39151677e-06
Iter: 1736 loss: 2.39056158e-06
Iter: 1737 loss: 2.38941811e-06
Iter: 1738 loss: 2.38899702e-06
Iter: 1739 loss: 2.38837811e-06
Iter: 1740 loss: 2.38689427e-06
Iter: 1741 loss: 2.38592793e-06
Iter: 1742 loss: 2.38535222e-06
Iter: 1743 loss: 2.38318853e-06
Iter: 1744 loss: 2.40763438e-06
Iter: 1745 loss: 2.38309212e-06
Iter: 1746 loss: 2.38170742e-06
Iter: 1747 loss: 2.38072585e-06
Iter: 1748 loss: 2.38015218e-06
Iter: 1749 loss: 2.37816448e-06
Iter: 1750 loss: 2.40250552e-06
Iter: 1751 loss: 2.37804579e-06
Iter: 1752 loss: 2.37665517e-06
Iter: 1753 loss: 2.37792256e-06
Iter: 1754 loss: 2.37591212e-06
Iter: 1755 loss: 2.37396193e-06
Iter: 1756 loss: 2.37653558e-06
Iter: 1757 loss: 2.37297718e-06
Iter: 1758 loss: 2.37122072e-06
Iter: 1759 loss: 2.37772247e-06
Iter: 1760 loss: 2.370831e-06
Iter: 1761 loss: 2.36892674e-06
Iter: 1762 loss: 2.37384529e-06
Iter: 1763 loss: 2.3683142e-06
Iter: 1764 loss: 2.3666471e-06
Iter: 1765 loss: 2.37318886e-06
Iter: 1766 loss: 2.36632604e-06
Iter: 1767 loss: 2.36488654e-06
Iter: 1768 loss: 2.37820859e-06
Iter: 1769 loss: 2.36486494e-06
Iter: 1770 loss: 2.36363326e-06
Iter: 1771 loss: 2.3711284e-06
Iter: 1772 loss: 2.36347341e-06
Iter: 1773 loss: 2.36289202e-06
Iter: 1774 loss: 2.36191863e-06
Iter: 1775 loss: 2.36189271e-06
Iter: 1776 loss: 2.3605337e-06
Iter: 1777 loss: 2.35898415e-06
Iter: 1778 loss: 2.35874859e-06
Iter: 1779 loss: 2.35700099e-06
Iter: 1780 loss: 2.37606469e-06
Iter: 1781 loss: 2.357e-06
Iter: 1782 loss: 2.35531979e-06
Iter: 1783 loss: 2.3555267e-06
Iter: 1784 loss: 2.35413154e-06
Iter: 1785 loss: 2.35282823e-06
Iter: 1786 loss: 2.37129029e-06
Iter: 1787 loss: 2.35291554e-06
Iter: 1788 loss: 2.3517207e-06
Iter: 1789 loss: 2.35055722e-06
Iter: 1790 loss: 2.35038556e-06
Iter: 1791 loss: 2.34837034e-06
Iter: 1792 loss: 2.36479855e-06
Iter: 1793 loss: 2.34822619e-06
Iter: 1794 loss: 2.34675554e-06
Iter: 1795 loss: 2.34643e-06
Iter: 1796 loss: 2.34554545e-06
Iter: 1797 loss: 2.34369077e-06
Iter: 1798 loss: 2.3524774e-06
Iter: 1799 loss: 2.34328718e-06
Iter: 1800 loss: 2.34176809e-06
Iter: 1801 loss: 2.3482844e-06
Iter: 1802 loss: 2.34152503e-06
Iter: 1803 loss: 2.34008621e-06
Iter: 1804 loss: 2.34259505e-06
Iter: 1805 loss: 2.3394216e-06
Iter: 1806 loss: 2.33806e-06
Iter: 1807 loss: 2.34996901e-06
Iter: 1808 loss: 2.3380303e-06
Iter: 1809 loss: 2.33670085e-06
Iter: 1810 loss: 2.34431104e-06
Iter: 1811 loss: 2.33649985e-06
Iter: 1812 loss: 2.33558058e-06
Iter: 1813 loss: 2.33441142e-06
Iter: 1814 loss: 2.33432502e-06
Iter: 1815 loss: 2.33307719e-06
Iter: 1816 loss: 2.3330781e-06
Iter: 1817 loss: 2.33212359e-06
Iter: 1818 loss: 2.33025139e-06
Iter: 1819 loss: 2.3370103e-06
Iter: 1820 loss: 2.3297855e-06
Iter: 1821 loss: 2.32841421e-06
Iter: 1822 loss: 2.3305106e-06
Iter: 1823 loss: 2.32756724e-06
Iter: 1824 loss: 2.32585126e-06
Iter: 1825 loss: 2.33665446e-06
Iter: 1826 loss: 2.32565685e-06
Iter: 1827 loss: 2.32440607e-06
Iter: 1828 loss: 2.32638217e-06
Iter: 1829 loss: 2.32389539e-06
Iter: 1830 loss: 2.3221462e-06
Iter: 1831 loss: 2.32584443e-06
Iter: 1832 loss: 2.3214925e-06
Iter: 1833 loss: 2.32020534e-06
Iter: 1834 loss: 2.32490356e-06
Iter: 1835 loss: 2.31987451e-06
Iter: 1836 loss: 2.31844137e-06
Iter: 1837 loss: 2.31839249e-06
Iter: 1838 loss: 2.3172272e-06
Iter: 1839 loss: 2.3156997e-06
Iter: 1840 loss: 2.33281662e-06
Iter: 1841 loss: 2.31559079e-06
Iter: 1842 loss: 2.31432568e-06
Iter: 1843 loss: 2.31579043e-06
Iter: 1844 loss: 2.31358263e-06
Iter: 1845 loss: 2.3125649e-06
Iter: 1846 loss: 2.31242348e-06
Iter: 1847 loss: 2.31174226e-06
Iter: 1848 loss: 2.31076865e-06
Iter: 1849 loss: 2.31069339e-06
Iter: 1850 loss: 2.30968408e-06
Iter: 1851 loss: 2.30919113e-06
Iter: 1852 loss: 2.30859314e-06
Iter: 1853 loss: 2.30700789e-06
Iter: 1854 loss: 2.31385957e-06
Iter: 1855 loss: 2.30672094e-06
Iter: 1856 loss: 2.30520732e-06
Iter: 1857 loss: 2.30448904e-06
Iter: 1858 loss: 2.3038283e-06
Iter: 1859 loss: 2.30218347e-06
Iter: 1860 loss: 2.32327602e-06
Iter: 1861 loss: 2.30209275e-06
Iter: 1862 loss: 2.3005623e-06
Iter: 1863 loss: 2.30203591e-06
Iter: 1864 loss: 2.29964644e-06
Iter: 1865 loss: 2.29801708e-06
Iter: 1866 loss: 2.30995693e-06
Iter: 1867 loss: 2.2979234e-06
Iter: 1868 loss: 2.29675652e-06
Iter: 1869 loss: 2.29666944e-06
Iter: 1870 loss: 2.29581428e-06
Iter: 1871 loss: 2.29403804e-06
Iter: 1872 loss: 2.29904208e-06
Iter: 1873 loss: 2.2933109e-06
Iter: 1874 loss: 2.29188072e-06
Iter: 1875 loss: 2.29739408e-06
Iter: 1876 loss: 2.29151919e-06
Iter: 1877 loss: 2.28981071e-06
Iter: 1878 loss: 2.2935003e-06
Iter: 1879 loss: 2.28925819e-06
Iter: 1880 loss: 2.2886602e-06
Iter: 1881 loss: 2.28836257e-06
Iter: 1882 loss: 2.28773388e-06
Iter: 1883 loss: 2.28724707e-06
Iter: 1884 loss: 2.28692807e-06
Iter: 1885 loss: 2.2859922e-06
Iter: 1886 loss: 2.28444378e-06
Iter: 1887 loss: 2.28446788e-06
Iter: 1888 loss: 2.2829372e-06
Iter: 1889 loss: 2.29670604e-06
Iter: 1890 loss: 2.28284739e-06
Iter: 1891 loss: 2.28162435e-06
Iter: 1892 loss: 2.28109297e-06
Iter: 1893 loss: 2.28047656e-06
Iter: 1894 loss: 2.2788663e-06
Iter: 1895 loss: 2.28792669e-06
Iter: 1896 loss: 2.27862301e-06
Iter: 1897 loss: 2.27694272e-06
Iter: 1898 loss: 2.28036515e-06
Iter: 1899 loss: 2.27632609e-06
Iter: 1900 loss: 2.27481928e-06
Iter: 1901 loss: 2.28492e-06
Iter: 1902 loss: 2.27474516e-06
Iter: 1903 loss: 2.27348301e-06
Iter: 1904 loss: 2.27317832e-06
Iter: 1905 loss: 2.27240048e-06
Iter: 1906 loss: 2.27075634e-06
Iter: 1907 loss: 2.2834015e-06
Iter: 1908 loss: 2.27064947e-06
Iter: 1909 loss: 2.26950942e-06
Iter: 1910 loss: 2.268554e-06
Iter: 1911 loss: 2.26813563e-06
Iter: 1912 loss: 2.26714837e-06
Iter: 1913 loss: 2.26708971e-06
Iter: 1914 loss: 2.26619704e-06
Iter: 1915 loss: 2.27758846e-06
Iter: 1916 loss: 2.26619795e-06
Iter: 1917 loss: 2.2658e-06
Iter: 1918 loss: 2.26436487e-06
Iter: 1919 loss: 2.26980274e-06
Iter: 1920 loss: 2.2638269e-06
Iter: 1921 loss: 2.26239808e-06
Iter: 1922 loss: 2.28073236e-06
Iter: 1923 loss: 2.26242241e-06
Iter: 1924 loss: 2.26135921e-06
Iter: 1925 loss: 2.26145835e-06
Iter: 1926 loss: 2.26061366e-06
Iter: 1927 loss: 2.25906388e-06
Iter: 1928 loss: 2.26578732e-06
Iter: 1929 loss: 2.25879876e-06
Iter: 1930 loss: 2.25753365e-06
Iter: 1931 loss: 2.25745453e-06
Iter: 1932 loss: 2.25649501e-06
Iter: 1933 loss: 2.25473104e-06
Iter: 1934 loss: 2.26503926e-06
Iter: 1935 loss: 2.25447047e-06
Iter: 1936 loss: 2.25319241e-06
Iter: 1937 loss: 2.25433223e-06
Iter: 1938 loss: 2.25247231e-06
Iter: 1939 loss: 2.25090139e-06
Iter: 1940 loss: 2.25943086e-06
Iter: 1941 loss: 2.25080453e-06
Iter: 1942 loss: 2.2495019e-06
Iter: 1943 loss: 2.25197596e-06
Iter: 1944 loss: 2.24899759e-06
Iter: 1945 loss: 2.24735231e-06
Iter: 1946 loss: 2.25052736e-06
Iter: 1947 loss: 2.24671203e-06
Iter: 1948 loss: 2.2452607e-06
Iter: 1949 loss: 2.2524041e-06
Iter: 1950 loss: 2.24504561e-06
Iter: 1951 loss: 2.2442207e-06
Iter: 1952 loss: 2.24421365e-06
Iter: 1953 loss: 2.24334372e-06
Iter: 1954 loss: 2.24205246e-06
Iter: 1955 loss: 2.24195583e-06
Iter: 1956 loss: 2.24100859e-06
Iter: 1957 loss: 2.24124233e-06
Iter: 1958 loss: 2.24037285e-06
Iter: 1959 loss: 2.23877828e-06
Iter: 1960 loss: 2.23986945e-06
Iter: 1961 loss: 2.23786333e-06
Iter: 1962 loss: 2.23641973e-06
Iter: 1963 loss: 2.25375902e-06
Iter: 1964 loss: 2.23644383e-06
Iter: 1965 loss: 2.23550387e-06
Iter: 1966 loss: 2.23411075e-06
Iter: 1967 loss: 2.23410461e-06
Iter: 1968 loss: 2.23261077e-06
Iter: 1969 loss: 2.2486729e-06
Iter: 1970 loss: 2.23260076e-06
Iter: 1971 loss: 2.23142683e-06
Iter: 1972 loss: 2.23064626e-06
Iter: 1973 loss: 2.23016673e-06
Iter: 1974 loss: 2.22854601e-06
Iter: 1975 loss: 2.241012e-06
Iter: 1976 loss: 2.22839708e-06
Iter: 1977 loss: 2.22707922e-06
Iter: 1978 loss: 2.22801827e-06
Iter: 1979 loss: 2.22621293e-06
Iter: 1980 loss: 2.22460721e-06
Iter: 1981 loss: 2.23585448e-06
Iter: 1982 loss: 2.22448352e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi3
+ date
Wed Oct 21 17:42:49 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/300_300_300_1 --function f1 --psi 2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb244202d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb266782158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb26679e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb26679ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2666bc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb244182510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2440f1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2441287b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb244128510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2441e2b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2441a56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c1a6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c1ae8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c177bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c0cb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c0b6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c0ad400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c0adb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c083598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c083f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c01f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c01f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2001d9048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb200205598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb200205ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb200194598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb200149620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb20012b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c0f6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c0f6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb20010a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2000ca1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2000ca378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2000ca048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb20009c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb200038620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.024335112
test_loss: 0.02417551
train_loss: 0.012965041
test_loss: 0.013952075
train_loss: 0.0086828135
test_loss: 0.011658531
train_loss: 0.008199213
test_loss: 0.010561342
train_loss: 0.0070527117
test_loss: 0.010344209
train_loss: 0.006650396
test_loss: 0.010270149
train_loss: 0.0070436355
test_loss: 0.010217974
train_loss: 0.006779106
test_loss: 0.00987938
train_loss: 0.006444389
test_loss: 0.0098425485
train_loss: 0.007864776
test_loss: 0.01000008
train_loss: 0.006012438
test_loss: 0.009986135
train_loss: 0.0066129067
test_loss: 0.009723568
train_loss: 0.0066496725
test_loss: 0.009993739
train_loss: 0.0060680034
test_loss: 0.009699962
train_loss: 0.006478503
test_loss: 0.009576189
train_loss: 0.006558867
test_loss: 0.009360086
train_loss: 0.0066930177
test_loss: 0.009966987
train_loss: 0.0061330996
test_loss: 0.009924444
train_loss: 0.0062137553
test_loss: 0.009464211
train_loss: 0.0058308244
test_loss: 0.0097813085
train_loss: 0.006355233
test_loss: 0.010109107
train_loss: 0.0058014384
test_loss: 0.00930709
train_loss: 0.0065679764
test_loss: 0.009600481
train_loss: 0.0061056437
test_loss: 0.009727075
train_loss: 0.0061909435
test_loss: 0.00979185
train_loss: 0.0057271724
test_loss: 0.009590949
train_loss: 0.005738501
test_loss: 0.009224368
train_loss: 0.006089223
test_loss: 0.009446278
train_loss: 0.0054401467
test_loss: 0.00930685
train_loss: 0.005899585
test_loss: 0.009315098
train_loss: 0.008208081
test_loss: 0.009628181
train_loss: 0.0060275975
test_loss: 0.0095725255
train_loss: 0.00530391
test_loss: 0.009507724
train_loss: 0.0057795746
test_loss: 0.009433667
train_loss: 0.0053271954
test_loss: 0.0095887445
train_loss: 0.0057326225
test_loss: 0.0092644775
train_loss: 0.0054691695
test_loss: 0.0093256645
train_loss: 0.0056314324
test_loss: 0.0094224075
train_loss: 0.005352836
test_loss: 0.0096024815
train_loss: 0.006537355
test_loss: 0.009754221
train_loss: 0.00562503
test_loss: 0.009434925
train_loss: 0.0058607203
test_loss: 0.009477029
train_loss: 0.0057738344
test_loss: 0.009631404
train_loss: 0.0056503965
test_loss: 0.00952587
train_loss: 0.0055857776
test_loss: 0.009304414
train_loss: 0.0053638034
test_loss: 0.009398368
train_loss: 0.0054015657
test_loss: 0.009646293
train_loss: 0.005825221
test_loss: 0.009653612
train_loss: 0.00560167
test_loss: 0.009097394
train_loss: 0.005552816
test_loss: 0.009353808
train_loss: 0.005585683
test_loss: 0.009285508
train_loss: 0.0051008845
test_loss: 0.00920605
train_loss: 0.005302959
test_loss: 0.009240256
train_loss: 0.006186002
test_loss: 0.009434053
train_loss: 0.0057585407
test_loss: 0.009323648
train_loss: 0.0062176166
test_loss: 0.0091086235
train_loss: 0.005875958
test_loss: 0.009344882
train_loss: 0.005100077
test_loss: 0.009365092
train_loss: 0.0058632763
test_loss: 0.009602342
train_loss: 0.005841419
test_loss: 0.009280146
train_loss: 0.005588944
test_loss: 0.0091770645
train_loss: 0.005649653
test_loss: 0.009202934
train_loss: 0.005675661
test_loss: 0.009309937
train_loss: 0.0056206086
test_loss: 0.00914776
train_loss: 0.0057133865
test_loss: 0.009300328
train_loss: 0.00512248
test_loss: 0.009159798
train_loss: 0.0053175846
test_loss: 0.009263955
train_loss: 0.0049339617
test_loss: 0.00932397
train_loss: 0.0054459875
test_loss: 0.009291032
train_loss: 0.0061214734
test_loss: 0.009254041
train_loss: 0.0054553063
test_loss: 0.008846104
train_loss: 0.0051498376
test_loss: 0.009257844
train_loss: 0.0053845383
test_loss: 0.009216251
train_loss: 0.0058913995
test_loss: 0.009295368
train_loss: 0.0058363955
test_loss: 0.009290064
train_loss: 0.0056159073
test_loss: 0.009059463
train_loss: 0.0054275747
test_loss: 0.009243063
train_loss: 0.005536981
test_loss: 0.0090877935
train_loss: 0.00607494
test_loss: 0.009536688
train_loss: 0.005314191
test_loss: 0.009253327
train_loss: 0.0055128713
test_loss: 0.009026404
train_loss: 0.005047673
test_loss: 0.008914675
train_loss: 0.0049016722
test_loss: 0.008986811
train_loss: 0.0056990944
test_loss: 0.009060535
train_loss: 0.0049011745
test_loss: 0.009166478
train_loss: 0.0050426745
test_loss: 0.008810946
train_loss: 0.005792767
test_loss: 0.009446043
train_loss: 0.0055217207
test_loss: 0.009038125
train_loss: 0.0053508473
test_loss: 0.009067039
train_loss: 0.005328361
test_loss: 0.009039725
train_loss: 0.005102032
test_loss: 0.009058149
train_loss: 0.005552184
test_loss: 0.009482208
train_loss: 0.0055225086
test_loss: 0.009388375
train_loss: 0.005472242
test_loss: 0.009078789
train_loss: 0.005196567
test_loss: 0.008876919
train_loss: 0.0051113567
test_loss: 0.009198219
train_loss: 0.006186881
test_loss: 0.009491576
train_loss: 0.005467424
test_loss: 0.009401194
train_loss: 0.005129791
test_loss: 0.009115912
train_loss: 0.005217506
test_loss: 0.00893539
train_loss: 0.0053664506
test_loss: 0.009127694
train_loss: 0.0050072493
test_loss: 0.009067264
train_loss: 0.0049768705
test_loss: 0.009214297
train_loss: 0.005339157
test_loss: 0.009200656
train_loss: 0.0051688743
test_loss: 0.009061948
train_loss: 0.0058928146
test_loss: 0.00943499
train_loss: 0.0052372036
test_loss: 0.009271741
train_loss: 0.0048646596
test_loss: 0.009210553
train_loss: 0.005233844
test_loss: 0.009066177
train_loss: 0.005565612
test_loss: 0.009179697
train_loss: 0.005139565
test_loss: 0.008901011
train_loss: 0.0047043995
test_loss: 0.008915963
train_loss: 0.005161483
test_loss: 0.008671917
train_loss: 0.005679846
test_loss: 0.009029563
train_loss: 0.00527384
test_loss: 0.009038208
train_loss: 0.0062967334
test_loss: 0.008962097
train_loss: 0.004924799
test_loss: 0.008925306
train_loss: 0.0054123956
test_loss: 0.00876549
train_loss: 0.005037665
test_loss: 0.008953572
train_loss: 0.004941027
test_loss: 0.009255951
train_loss: 0.005344037
test_loss: 0.0093694
train_loss: 0.0051600793
test_loss: 0.009003835
train_loss: 0.005364921
test_loss: 0.008959032
train_loss: 0.004983204
test_loss: 0.009013603
train_loss: 0.004870207
test_loss: 0.008671134
train_loss: 0.0048052035
test_loss: 0.008725939
train_loss: 0.0052872105
test_loss: 0.009236162
train_loss: 0.0054578986
test_loss: 0.00916567
train_loss: 0.004924884
test_loss: 0.0091108475
train_loss: 0.0051261466
test_loss: 0.008811498
train_loss: 0.005046176
test_loss: 0.008789525
train_loss: 0.005340217
test_loss: 0.009032154
train_loss: 0.005134946
test_loss: 0.009070542
train_loss: 0.0052444683
test_loss: 0.009099982
train_loss: 0.004857349
test_loss: 0.008858741
train_loss: 0.0052069887
test_loss: 0.009031855
train_loss: 0.0051660077
test_loss: 0.008923105
train_loss: 0.005092872
test_loss: 0.008964302
train_loss: 0.0057833744
test_loss: 0.008921821
train_loss: 0.0052964445
test_loss: 0.0091761425
train_loss: 0.0052528055
test_loss: 0.0090645645
train_loss: 0.005005919
test_loss: 0.008933916
train_loss: 0.0052562156
test_loss: 0.008932877
train_loss: 0.0053874794
test_loss: 0.008918093
train_loss: 0.005092026
test_loss: 0.008696788
train_loss: 0.00533054
test_loss: 0.008778208
train_loss: 0.005044678
test_loss: 0.009044456
train_loss: 0.0049811504
test_loss: 0.008856054
train_loss: 0.004981488
test_loss: 0.008615641
train_loss: 0.004712232
test_loss: 0.009020306
train_loss: 0.0048601325
test_loss: 0.008844542
train_loss: 0.004611123
test_loss: 0.009122985
train_loss: 0.005015743
test_loss: 0.008819464
train_loss: 0.0050530555
test_loss: 0.008774345
train_loss: 0.00500419
test_loss: 0.00902846
train_loss: 0.0049176943
test_loss: 0.009172339
train_loss: 0.005024343
test_loss: 0.008940451
train_loss: 0.0050802864
test_loss: 0.009176574
train_loss: 0.0053546336
test_loss: 0.008674
train_loss: 0.005241112
test_loss: 0.008655195
train_loss: 0.0054095015
test_loss: 0.008882292
train_loss: 0.00516283
test_loss: 0.0087918835
train_loss: 0.004688834
test_loss: 0.008737361
train_loss: 0.0054104156
test_loss: 0.009193454
train_loss: 0.0052240184
test_loss: 0.008906629
train_loss: 0.0049168807
test_loss: 0.009092576
train_loss: 0.005068517
test_loss: 0.008956843
train_loss: 0.0051394533
test_loss: 0.008872475
train_loss: 0.0051823184
test_loss: 0.008641337
train_loss: 0.0051258
test_loss: 0.008933845
train_loss: 0.005829287
test_loss: 0.008976279
train_loss: 0.0045923973
test_loss: 0.009057218
train_loss: 0.0054420982
test_loss: 0.009344016
train_loss: 0.0049434206
test_loss: 0.009025575
train_loss: 0.0050323117
test_loss: 0.008973362
train_loss: 0.005148148
test_loss: 0.00874621
train_loss: 0.0047880774
test_loss: 0.00878611
train_loss: 0.005058441
test_loss: 0.008767895
train_loss: 0.005059719
test_loss: 0.008893966
train_loss: 0.004959927
test_loss: 0.00856344
train_loss: 0.004530585
test_loss: 0.0087284045
train_loss: 0.0046921647
test_loss: 0.008962373
train_loss: 0.0047434103
test_loss: 0.008812994
train_loss: 0.0045300364
test_loss: 0.008669784
train_loss: 0.004989406
test_loss: 0.009110839
train_loss: 0.004931357
test_loss: 0.008700837
train_loss: 0.0049731843
test_loss: 0.008994997
train_loss: 0.004793427
test_loss: 0.0088181775
train_loss: 0.0049381447
test_loss: 0.008977819
train_loss: 0.0050969636
test_loss: 0.008948362
train_loss: 0.0049730395
test_loss: 0.009024818
train_loss: 0.0047470564
test_loss: 0.0086406255
train_loss: 0.004841942
test_loss: 0.008779418
train_loss: 0.004776913
test_loss: 0.008996178
train_loss: 0.0048162867
test_loss: 0.008749533
train_loss: 0.0052959793
test_loss: 0.008817854
train_loss: 0.005181524
test_loss: 0.0089497175
train_loss: 0.005026924
test_loss: 0.008776119
train_loss: 0.0052941614
test_loss: 0.009158388
train_loss: 0.0045929244
test_loss: 0.008564571
train_loss: 0.005032112
test_loss: 0.008667385
train_loss: 0.005399922
test_loss: 0.008929363
train_loss: 0.0052109193
test_loss: 0.008920813
train_loss: 0.0050202077
test_loss: 0.008805813
train_loss: 0.0050375117
test_loss: 0.008871871
train_loss: 0.0047253063
test_loss: 0.00875556
train_loss: 0.0049504368
test_loss: 0.008909916
train_loss: 0.0046083983
test_loss: 0.008706041
train_loss: 0.0048363954
test_loss: 0.008770985
train_loss: 0.004636003
test_loss: 0.008595489
train_loss: 0.004994671
test_loss: 0.008975834
train_loss: 0.004967477
test_loss: 0.008631991
train_loss: 0.0051053166
test_loss: 0.009062832
train_loss: 0.005296931
test_loss: 0.008717436
train_loss: 0.004787649
test_loss: 0.0088909045
train_loss: 0.0049464162
test_loss: 0.008864828
train_loss: 0.004515279
test_loss: 0.008557706
train_loss: 0.004607559
test_loss: 0.008702715
train_loss: 0.004979126
test_loss: 0.008670704
train_loss: 0.004748916
test_loss: 0.008602357
train_loss: 0.0057526543
test_loss: 0.008693717
train_loss: 0.0046904497
test_loss: 0.009078881
train_loss: 0.0046490394
test_loss: 0.008669802
train_loss: 0.004333646
test_loss: 0.008387914
train_loss: 0.004783759
test_loss: 0.008664206
train_loss: 0.0052606333
test_loss: 0.008746493
train_loss: 0.0051775556
test_loss: 0.008656042
train_loss: 0.0043204287
test_loss: 0.008692522
train_loss: 0.0041997917
test_loss: 0.008652403
train_loss: 0.004563883
test_loss: 0.008489812
train_loss: 0.0053667836
test_loss: 0.008735639
train_loss: 0.004599234
test_loss: 0.00880236
train_loss: 0.0055703195
test_loss: 0.008971726
train_loss: 0.0050946637
test_loss: 0.00883479
train_loss: 0.005089474
test_loss: 0.008878887
train_loss: 0.0050303075
test_loss: 0.008401807
train_loss: 0.0052916193
test_loss: 0.00877701
train_loss: 0.0043931557
test_loss: 0.0086910715
train_loss: 0.005136224
test_loss: 0.00857314
train_loss: 0.0044816583
test_loss: 0.008677371
train_loss: 0.0050279098
test_loss: 0.008995452
train_loss: 0.0047203675
test_loss: 0.008709365
train_loss: 0.0046899035
test_loss: 0.008695813
train_loss: 0.0049309493
test_loss: 0.008779872
train_loss: 0.0046224617
test_loss: 0.008669183
train_loss: 0.0049891397
test_loss: 0.008646866
train_loss: 0.0045359256
test_loss: 0.008631147
train_loss: 0.004263457
test_loss: 0.008646896
train_loss: 0.004326118
test_loss: 0.008565655
train_loss: 0.0044172714
test_loss: 0.008553373
train_loss: 0.004550811
test_loss: 0.008439674
train_loss: 0.004834348
test_loss: 0.008944841
train_loss: 0.0050045922
test_loss: 0.008632003
train_loss: 0.004866711
test_loss: 0.0089614205
train_loss: 0.0047671488
test_loss: 0.008520131
train_loss: 0.0046901302
test_loss: 0.008965162
train_loss: 0.0048005013
test_loss: 0.008668221
train_loss: 0.004786186
test_loss: 0.008647641
train_loss: 0.004940937
test_loss: 0.008535601
train_loss: 0.00462952
test_loss: 0.008688953
train_loss: 0.0042442484
test_loss: 0.008393932
train_loss: 0.004805018
test_loss: 0.008798287
train_loss: 0.0048524244
test_loss: 0.00836397
train_loss: 0.0047157775
test_loss: 0.008599965
train_loss: 0.0042769965
test_loss: 0.008532259
train_loss: 0.004644205
test_loss: 0.008774101
train_loss: 0.005219206
test_loss: 0.008446107
train_loss: 0.004810307
test_loss: 0.008721701
train_loss: 0.005021982
test_loss: 0.008960767
train_loss: 0.0057378835
test_loss: 0.008723757
train_loss: 0.004742635
test_loss: 0.008692742
train_loss: 0.0049602003
test_loss: 0.0085267555
train_loss: 0.005082355
test_loss: 0.008769581
train_loss: 0.0049191667
test_loss: 0.008393619
train_loss: 0.0049143764
test_loss: 0.0086590005
train_loss: 0.005026066
test_loss: 0.009091521
train_loss: 0.005159756
test_loss: 0.008655109
train_loss: 0.004937278
test_loss: 0.008622664
train_loss: 0.0051776096
test_loss: 0.008521421
train_loss: 0.0050467993
test_loss: 0.00863204
train_loss: 0.0044593695
test_loss: 0.0086002955
train_loss: 0.004938128
test_loss: 0.008487707
train_loss: 0.0054752473
test_loss: 0.009086709
train_loss: 0.004560243
test_loss: 0.008508737
train_loss: 0.0045609768
test_loss: 0.008429596
train_loss: 0.00462311
test_loss: 0.008565019
train_loss: 0.0044498835
test_loss: 0.008368847
train_loss: 0.004235477
test_loss: 0.008408521
train_loss: 0.004863221
test_loss: 0.008487389
train_loss: 0.004498641
test_loss: 0.008634146
train_loss: 0.004649302
test_loss: 0.008714519
train_loss: 0.0047390047
test_loss: 0.008479276
train_loss: 0.004876372
test_loss: 0.008810621
train_loss: 0.0045661116
test_loss: 0.008652976
train_loss: 0.0049656928
test_loss: 0.008484716
train_loss: 0.0044588847
test_loss: 0.008545543
train_loss: 0.00526557
test_loss: 0.0087541295
train_loss: 0.004401755
test_loss: 0.008719889
train_loss: 0.0044823047
test_loss: 0.008488126
train_loss: 0.004961699
test_loss: 0.0083869975
train_loss: 0.0045279716
test_loss: 0.008459359
train_loss: 0.0049114767
test_loss: 0.008818092
train_loss: 0.005031773
test_loss: 0.008657672
train_loss: 0.004737444
test_loss: 0.00853872
train_loss: 0.0047088056
test_loss: 0.008643633
train_loss: 0.0046347063
test_loss: 0.008585879
train_loss: 0.0045624645
test_loss: 0.008592083
train_loss: 0.005007086
test_loss: 0.008682084
train_loss: 0.0047881664
test_loss: 0.00880916
train_loss: 0.004794363
test_loss: 0.0085625695
train_loss: 0.0047497908
test_loss: 0.008386561
train_loss: 0.005113461
test_loss: 0.008688799
train_loss: 0.0046841903
test_loss: 0.008743408
train_loss: 0.004633917
test_loss: 0.008483613
train_loss: 0.0046749716
test_loss: 0.008478747
train_loss: 0.005000561
test_loss: 0.008940704
train_loss: 0.004655576
test_loss: 0.008614439
train_loss: 0.0044435943
test_loss: 0.008326215
train_loss: 0.004972717
test_loss: 0.008708823
train_loss: 0.004619119
test_loss: 0.0087019205
train_loss: 0.0045821792
test_loss: 0.008610612
train_loss: 0.004866075
test_loss: 0.008586408
train_loss: 0.0049193264
test_loss: 0.0089324005
train_loss: 0.0048183724
test_loss: 0.008585873
train_loss: 0.004576438
test_loss: 0.008638012
train_loss: 0.0044335863
test_loss: 0.008968451
train_loss: 0.0045717224
test_loss: 0.00845863
train_loss: 0.004398154
test_loss: 0.0083297165
train_loss: 0.0040656636
test_loss: 0.008488667
train_loss: 0.0044200434
test_loss: 0.008600988
train_loss: 0.005007314
test_loss: 0.008507641
train_loss: 0.0049350075
test_loss: 0.00853624
train_loss: 0.004490303
test_loss: 0.0083524585
train_loss: 0.004428305
test_loss: 0.0084677525
train_loss: 0.0045498833
test_loss: 0.008672325
train_loss: 0.0044611692
test_loss: 0.008694367
train_loss: 0.0046482505
test_loss: 0.008549491
train_loss: 0.004629812
test_loss: 0.008434369
train_loss: 0.0042225476
test_loss: 0.008459696
train_loss: 0.0044493247
test_loss: 0.008301598
train_loss: 0.0046044705
test_loss: 0.008634096
train_loss: 0.0049694246
test_loss: 0.009011519
train_loss: 0.0046517686
test_loss: 0.008626479
train_loss: 0.0042171595
test_loss: 0.008416739
train_loss: 0.0043032737
test_loss: 0.008317626/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.0047378447
test_loss: 0.008483706
train_loss: 0.0049556172
test_loss: 0.008806503
train_loss: 0.004611565
test_loss: 0.008275279
train_loss: 0.0052180816
test_loss: 0.00851609
train_loss: 0.0044586034
test_loss: 0.00835823
train_loss: 0.0042915116
test_loss: 0.008627963
train_loss: 0.0055370135
test_loss: 0.008597112
train_loss: 0.0051425598
test_loss: 0.008443778
train_loss: 0.004618805
test_loss: 0.008408076
train_loss: 0.0047455155
test_loss: 0.008565372
train_loss: 0.0050201793
test_loss: 0.00834515
train_loss: 0.0048681153
test_loss: 0.008688987
train_loss: 0.005814216
test_loss: 0.008562159
train_loss: 0.0052309316
test_loss: 0.008762441
train_loss: 0.0044725104
test_loss: 0.008491805
train_loss: 0.0050106943
test_loss: 0.008860488
train_loss: 0.005245744
test_loss: 0.00842566
train_loss: 0.004550701
test_loss: 0.008499071
train_loss: 0.004412582
test_loss: 0.008344028
train_loss: 0.004798572
test_loss: 0.008548414
train_loss: 0.0047392608
test_loss: 0.008947737
train_loss: 0.0045111603
test_loss: 0.008298544
train_loss: 0.0041659074
test_loss: 0.008430706
train_loss: 0.004188207
test_loss: 0.008200515
train_loss: 0.0040747803
test_loss: 0.008566467
train_loss: 0.004597293
test_loss: 0.008488424
train_loss: 0.004967184
test_loss: 0.008554563
train_loss: 0.0046571833
test_loss: 0.008513119
train_loss: 0.0047632298
test_loss: 0.008895886
train_loss: 0.0042692856
test_loss: 0.00867681
train_loss: 0.0048111286
test_loss: 0.008577217
train_loss: 0.005088299
test_loss: 0.008974833
train_loss: 0.005167743
test_loss: 0.008341906
train_loss: 0.0048353514
test_loss: 0.008827408
train_loss: 0.004276541
test_loss: 0.008581705
train_loss: 0.004506488
test_loss: 0.008805801
train_loss: 0.004493759
test_loss: 0.0085768495
train_loss: 0.004106312
test_loss: 0.00845717
train_loss: 0.0043391627
test_loss: 0.008633044
train_loss: 0.004050913
test_loss: 0.008637809
train_loss: 0.0038448547
test_loss: 0.008213647
train_loss: 0.005018437
test_loss: 0.008377908
train_loss: 0.0046687396
test_loss: 0.00848743
train_loss: 0.004864804
test_loss: 0.008430657
train_loss: 0.004811622
test_loss: 0.008646611
train_loss: 0.004673765
test_loss: 0.00852649
train_loss: 0.0048469855
test_loss: 0.008552074
train_loss: 0.0046885367
test_loss: 0.008605737
train_loss: 0.004866272
test_loss: 0.008780242
train_loss: 0.004264638
test_loss: 0.008300377
train_loss: 0.0050129136
test_loss: 0.008556445
train_loss: 0.0041977502
test_loss: 0.008325353
train_loss: 0.004572096
test_loss: 0.008377217
train_loss: 0.0051937397
test_loss: 0.00862754
train_loss: 0.0046461243
test_loss: 0.008703787
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi3/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338c680d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338bf4bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338bf4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338bf4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338b58ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338ada488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338a84bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338d67c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338a44378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338a446a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3389ac8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3389c1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3389e28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb33897e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb33895c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338946950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338966840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb338966d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3388d8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3389669d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb33886d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb3388a2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2fc3c9048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2fc350598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2fc350ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2fc31a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2fc2d48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2fc2f4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2fc2f47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2fc308c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d407f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d40be6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d40be378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2d405ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2c07dc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2c07fad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.00158203e-05
Iter: 2 loss: 4.37804483e-05
Iter: 3 loss: 0.00013154975
Iter: 4 loss: 4.37582312e-05
Iter: 5 loss: 3.99194141e-05
Iter: 6 loss: 5.8244259e-05
Iter: 7 loss: 3.92297698e-05
Iter: 8 loss: 3.64609223e-05
Iter: 9 loss: 3.02493609e-05
Iter: 10 loss: 0.000115109069
Iter: 11 loss: 2.99003223e-05
Iter: 12 loss: 2.60527486e-05
Iter: 13 loss: 6.40958606e-05
Iter: 14 loss: 2.59270928e-05
Iter: 15 loss: 2.27079945e-05
Iter: 16 loss: 2.72864818e-05
Iter: 17 loss: 2.11315655e-05
Iter: 18 loss: 1.89750353e-05
Iter: 19 loss: 4.66583369e-05
Iter: 20 loss: 1.89598504e-05
Iter: 21 loss: 1.79105955e-05
Iter: 22 loss: 1.78970131e-05
Iter: 23 loss: 1.706689e-05
Iter: 24 loss: 1.56052556e-05
Iter: 25 loss: 2.60632296e-05
Iter: 26 loss: 1.54766167e-05
Iter: 27 loss: 1.46627262e-05
Iter: 28 loss: 2.23490915e-05
Iter: 29 loss: 1.46301772e-05
Iter: 30 loss: 1.40708371e-05
Iter: 31 loss: 1.27947351e-05
Iter: 32 loss: 2.9368226e-05
Iter: 33 loss: 1.27098556e-05
Iter: 34 loss: 1.2017168e-05
Iter: 35 loss: 1.1965376e-05
Iter: 36 loss: 1.14904387e-05
Iter: 37 loss: 1.18706157e-05
Iter: 38 loss: 1.12032103e-05
Iter: 39 loss: 1.11858608e-05
Iter: 40 loss: 1.09264511e-05
Iter: 41 loss: 1.0713642e-05
Iter: 42 loss: 1.0605936e-05
Iter: 43 loss: 1.05062591e-05
Iter: 44 loss: 1.01933874e-05
Iter: 45 loss: 9.94420225e-06
Iter: 46 loss: 9.84895905e-06
Iter: 47 loss: 9.45917236e-06
Iter: 48 loss: 1.07516444e-05
Iter: 49 loss: 9.35225398e-06
Iter: 50 loss: 8.94788172e-06
Iter: 51 loss: 1.05416957e-05
Iter: 52 loss: 8.85525787e-06
Iter: 53 loss: 8.56338829e-06
Iter: 54 loss: 1.06483112e-05
Iter: 55 loss: 8.53738311e-06
Iter: 56 loss: 8.32201567e-06
Iter: 57 loss: 8.32194564e-06
Iter: 58 loss: 8.14984469e-06
Iter: 59 loss: 7.944e-06
Iter: 60 loss: 1.0771957e-05
Iter: 61 loss: 7.94316202e-06
Iter: 62 loss: 7.76191e-06
Iter: 63 loss: 7.84098665e-06
Iter: 64 loss: 7.63805656e-06
Iter: 65 loss: 7.40362793e-06
Iter: 66 loss: 7.99674217e-06
Iter: 67 loss: 7.3226156e-06
Iter: 68 loss: 7.14015232e-06
Iter: 69 loss: 7.17061175e-06
Iter: 70 loss: 7.00283545e-06
Iter: 71 loss: 6.7864562e-06
Iter: 72 loss: 8.69338692e-06
Iter: 73 loss: 6.77543539e-06
Iter: 74 loss: 6.66583219e-06
Iter: 75 loss: 7.64687138e-06
Iter: 76 loss: 6.66067626e-06
Iter: 77 loss: 6.51454275e-06
Iter: 78 loss: 6.65552579e-06
Iter: 79 loss: 6.43120711e-06
Iter: 80 loss: 6.35727247e-06
Iter: 81 loss: 6.43006751e-06
Iter: 82 loss: 6.31532839e-06
Iter: 83 loss: 6.22420703e-06
Iter: 84 loss: 6.16121815e-06
Iter: 85 loss: 6.1278356e-06
Iter: 86 loss: 6.00316616e-06
Iter: 87 loss: 7.22900131e-06
Iter: 88 loss: 5.9988638e-06
Iter: 89 loss: 5.9238987e-06
Iter: 90 loss: 5.92380638e-06
Iter: 91 loss: 5.86407896e-06
Iter: 92 loss: 5.74684054e-06
Iter: 93 loss: 6.53208281e-06
Iter: 94 loss: 5.73499e-06
Iter: 95 loss: 5.65622031e-06
Iter: 96 loss: 5.65167784e-06
Iter: 97 loss: 5.59170257e-06
Iter: 98 loss: 5.51525e-06
Iter: 99 loss: 5.5149485e-06
Iter: 100 loss: 5.46449837e-06
Iter: 101 loss: 5.5522728e-06
Iter: 102 loss: 5.441962e-06
Iter: 103 loss: 5.38333916e-06
Iter: 104 loss: 5.37197275e-06
Iter: 105 loss: 5.33304774e-06
Iter: 106 loss: 5.26139047e-06
Iter: 107 loss: 5.42420685e-06
Iter: 108 loss: 5.23471681e-06
Iter: 109 loss: 5.153247e-06
Iter: 110 loss: 5.54525e-06
Iter: 111 loss: 5.13882151e-06
Iter: 112 loss: 5.12448969e-06
Iter: 113 loss: 5.10315e-06
Iter: 114 loss: 5.08684752e-06
Iter: 115 loss: 5.03674346e-06
Iter: 116 loss: 5.13479836e-06
Iter: 117 loss: 5.00498936e-06
Iter: 118 loss: 4.94139385e-06
Iter: 119 loss: 5.51790163e-06
Iter: 120 loss: 4.93852667e-06
Iter: 121 loss: 4.8894467e-06
Iter: 122 loss: 4.86889667e-06
Iter: 123 loss: 4.84308202e-06
Iter: 124 loss: 4.78847051e-06
Iter: 125 loss: 4.78836228e-06
Iter: 126 loss: 4.75394791e-06
Iter: 127 loss: 4.69711085e-06
Iter: 128 loss: 4.69671977e-06
Iter: 129 loss: 4.6544119e-06
Iter: 130 loss: 4.65124322e-06
Iter: 131 loss: 4.62388198e-06
Iter: 132 loss: 4.60453066e-06
Iter: 133 loss: 4.59503553e-06
Iter: 134 loss: 4.54509427e-06
Iter: 135 loss: 4.64049e-06
Iter: 136 loss: 4.52402219e-06
Iter: 137 loss: 4.48690844e-06
Iter: 138 loss: 4.81856569e-06
Iter: 139 loss: 4.48522042e-06
Iter: 140 loss: 4.44660145e-06
Iter: 141 loss: 4.51577944e-06
Iter: 142 loss: 4.42993041e-06
Iter: 143 loss: 4.39594169e-06
Iter: 144 loss: 4.42596411e-06
Iter: 145 loss: 4.37605559e-06
Iter: 146 loss: 4.33175182e-06
Iter: 147 loss: 4.40843542e-06
Iter: 148 loss: 4.31214266e-06
Iter: 149 loss: 4.33210425e-06
Iter: 150 loss: 4.29984766e-06
Iter: 151 loss: 4.28898238e-06
Iter: 152 loss: 4.26090446e-06
Iter: 153 loss: 4.4721146e-06
Iter: 154 loss: 4.25499957e-06
Iter: 155 loss: 4.22530456e-06
Iter: 156 loss: 4.3210016e-06
Iter: 157 loss: 4.21681762e-06
Iter: 158 loss: 4.18875197e-06
Iter: 159 loss: 4.16513649e-06
Iter: 160 loss: 4.15730938e-06
Iter: 161 loss: 4.12408735e-06
Iter: 162 loss: 4.12355e-06
Iter: 163 loss: 4.10244047e-06
Iter: 164 loss: 4.08378901e-06
Iter: 165 loss: 4.07828429e-06
Iter: 166 loss: 4.04344792e-06
Iter: 167 loss: 4.34178037e-06
Iter: 168 loss: 4.04171897e-06
Iter: 169 loss: 4.02148225e-06
Iter: 170 loss: 4.08690084e-06
Iter: 171 loss: 4.01563557e-06
Iter: 172 loss: 3.99301098e-06
Iter: 173 loss: 3.98935254e-06
Iter: 174 loss: 3.973807e-06
Iter: 175 loss: 3.95322104e-06
Iter: 176 loss: 4.13927501e-06
Iter: 177 loss: 3.95226607e-06
Iter: 178 loss: 3.928571e-06
Iter: 179 loss: 3.95439611e-06
Iter: 180 loss: 3.91561844e-06
Iter: 181 loss: 3.89588376e-06
Iter: 182 loss: 3.9435472e-06
Iter: 183 loss: 3.88883655e-06
Iter: 184 loss: 3.86635384e-06
Iter: 185 loss: 3.86250667e-06
Iter: 186 loss: 3.84717077e-06
Iter: 187 loss: 3.86978536e-06
Iter: 188 loss: 3.83752285e-06
Iter: 189 loss: 3.83103588e-06
Iter: 190 loss: 3.81479049e-06
Iter: 191 loss: 3.96743781e-06
Iter: 192 loss: 3.81252494e-06
Iter: 193 loss: 3.79421226e-06
Iter: 194 loss: 3.81091604e-06
Iter: 195 loss: 3.78353161e-06
Iter: 196 loss: 3.76247817e-06
Iter: 197 loss: 3.80202459e-06
Iter: 198 loss: 3.75373656e-06
Iter: 199 loss: 3.72992781e-06
Iter: 200 loss: 3.83589122e-06
Iter: 201 loss: 3.72520935e-06
Iter: 202 loss: 3.70629596e-06
Iter: 203 loss: 3.75130139e-06
Iter: 204 loss: 3.69918348e-06
Iter: 205 loss: 3.67528833e-06
Iter: 206 loss: 3.76151434e-06
Iter: 207 loss: 3.66951645e-06
Iter: 208 loss: 3.65629603e-06
Iter: 209 loss: 3.69010468e-06
Iter: 210 loss: 3.65177266e-06
Iter: 211 loss: 3.63291019e-06
Iter: 212 loss: 3.64283551e-06
Iter: 213 loss: 3.62055539e-06
Iter: 214 loss: 3.60681224e-06
Iter: 215 loss: 3.76917205e-06
Iter: 216 loss: 3.60676813e-06
Iter: 217 loss: 3.59321598e-06
Iter: 218 loss: 3.5941614e-06
Iter: 219 loss: 3.58286229e-06
Iter: 220 loss: 3.56596206e-06
Iter: 221 loss: 3.61697948e-06
Iter: 222 loss: 3.56104283e-06
Iter: 223 loss: 3.55082807e-06
Iter: 224 loss: 3.65195729e-06
Iter: 225 loss: 3.55059956e-06
Iter: 226 loss: 3.53695123e-06
Iter: 227 loss: 3.53820769e-06
Iter: 228 loss: 3.52651614e-06
Iter: 229 loss: 3.51852759e-06
Iter: 230 loss: 3.51311223e-06
Iter: 231 loss: 3.51013887e-06
Iter: 232 loss: 3.49516631e-06
Iter: 233 loss: 3.49884658e-06
Iter: 234 loss: 3.48441972e-06
Iter: 235 loss: 3.46776142e-06
Iter: 236 loss: 3.52330017e-06
Iter: 237 loss: 3.463128e-06
Iter: 238 loss: 3.44481464e-06
Iter: 239 loss: 3.53905625e-06
Iter: 240 loss: 3.44181171e-06
Iter: 241 loss: 3.4297118e-06
Iter: 242 loss: 3.48632125e-06
Iter: 243 loss: 3.42753765e-06
Iter: 244 loss: 3.41594978e-06
Iter: 245 loss: 3.43700162e-06
Iter: 246 loss: 3.41072882e-06
Iter: 247 loss: 3.39939902e-06
Iter: 248 loss: 3.43155716e-06
Iter: 249 loss: 3.3956876e-06
Iter: 250 loss: 3.38274458e-06
Iter: 251 loss: 3.40644328e-06
Iter: 252 loss: 3.37710162e-06
Iter: 253 loss: 3.36576659e-06
Iter: 254 loss: 3.48870367e-06
Iter: 255 loss: 3.36572384e-06
Iter: 256 loss: 3.35770096e-06
Iter: 257 loss: 3.35455957e-06
Iter: 258 loss: 3.35032564e-06
Iter: 259 loss: 3.33736944e-06
Iter: 260 loss: 3.40528072e-06
Iter: 261 loss: 3.33533262e-06
Iter: 262 loss: 3.3273509e-06
Iter: 263 loss: 3.32708146e-06
Iter: 264 loss: 3.32339027e-06
Iter: 265 loss: 3.31173851e-06
Iter: 266 loss: 3.33547632e-06
Iter: 267 loss: 3.30458124e-06
Iter: 268 loss: 3.29063869e-06
Iter: 269 loss: 3.43707461e-06
Iter: 270 loss: 3.29028308e-06
Iter: 271 loss: 3.27929e-06
Iter: 272 loss: 3.26218424e-06
Iter: 273 loss: 3.26201757e-06
Iter: 274 loss: 3.2487e-06
Iter: 275 loss: 3.24849748e-06
Iter: 276 loss: 3.23743075e-06
Iter: 277 loss: 3.24749362e-06
Iter: 278 loss: 3.23103586e-06
Iter: 279 loss: 3.22062215e-06
Iter: 280 loss: 3.34934748e-06
Iter: 281 loss: 3.22052529e-06
Iter: 282 loss: 3.21351854e-06
Iter: 283 loss: 3.20491836e-06
Iter: 284 loss: 3.20406912e-06
Iter: 285 loss: 3.19392643e-06
Iter: 286 loss: 3.29035379e-06
Iter: 287 loss: 3.19367905e-06
Iter: 288 loss: 3.18517868e-06
Iter: 289 loss: 3.1951231e-06
Iter: 290 loss: 3.18071125e-06
Iter: 291 loss: 3.16997784e-06
Iter: 292 loss: 3.2015555e-06
Iter: 293 loss: 3.16668957e-06
Iter: 294 loss: 3.1628947e-06
Iter: 295 loss: 3.16232854e-06
Iter: 296 loss: 3.15764919e-06
Iter: 297 loss: 3.1482009e-06
Iter: 298 loss: 3.32309742e-06
Iter: 299 loss: 3.14807812e-06
Iter: 300 loss: 3.13980308e-06
Iter: 301 loss: 3.15577245e-06
Iter: 302 loss: 3.13630471e-06
Iter: 303 loss: 3.12770476e-06
Iter: 304 loss: 3.12736802e-06
Iter: 305 loss: 3.12070097e-06
Iter: 306 loss: 3.11009353e-06
Iter: 307 loss: 3.20985123e-06
Iter: 308 loss: 3.10958285e-06
Iter: 309 loss: 3.10073619e-06
Iter: 310 loss: 3.09083975e-06
Iter: 311 loss: 3.08942776e-06
Iter: 312 loss: 3.07950222e-06
Iter: 313 loss: 3.23672361e-06
Iter: 314 loss: 3.07946766e-06
Iter: 315 loss: 3.07174196e-06
Iter: 316 loss: 3.06739275e-06
Iter: 317 loss: 3.06405286e-06
Iter: 318 loss: 3.0564438e-06
Iter: 319 loss: 3.05628282e-06
Iter: 320 loss: 3.04991045e-06
Iter: 321 loss: 3.04199671e-06
Iter: 322 loss: 3.04136802e-06
Iter: 323 loss: 3.03145589e-06
Iter: 324 loss: 3.12550719e-06
Iter: 325 loss: 3.03097568e-06
Iter: 326 loss: 3.02442118e-06
Iter: 327 loss: 3.05357116e-06
Iter: 328 loss: 3.02305807e-06
Iter: 329 loss: 3.01579576e-06
Iter: 330 loss: 3.01747968e-06
Iter: 331 loss: 3.01062437e-06
Iter: 332 loss: 3.00921124e-06
Iter: 333 loss: 3.00644069e-06
Iter: 334 loss: 3.00362444e-06
Iter: 335 loss: 2.99569456e-06
Iter: 336 loss: 3.03171305e-06
Iter: 337 loss: 2.99268822e-06
Iter: 338 loss: 2.98307123e-06
Iter: 339 loss: 3.03623938e-06
Iter: 340 loss: 2.98171062e-06
Iter: 341 loss: 2.97438442e-06
Iter: 342 loss: 2.97879023e-06
Iter: 343 loss: 2.9697469e-06
Iter: 344 loss: 2.96004737e-06
Iter: 345 loss: 3.01546311e-06
Iter: 346 loss: 2.95877726e-06
Iter: 347 loss: 2.95144719e-06
Iter: 348 loss: 2.95285213e-06
Iter: 349 loss: 2.94600295e-06
Iter: 350 loss: 2.93726066e-06
Iter: 351 loss: 3.01217915e-06
Iter: 352 loss: 2.93672565e-06
Iter: 353 loss: 2.92971708e-06
Iter: 354 loss: 2.91882816e-06
Iter: 355 loss: 2.91876768e-06
Iter: 356 loss: 2.90749949e-06
Iter: 357 loss: 3.00067541e-06
Iter: 358 loss: 2.90683079e-06
Iter: 359 loss: 2.89731747e-06
Iter: 360 loss: 2.94197071e-06
Iter: 361 loss: 2.8954687e-06
Iter: 362 loss: 2.89052377e-06
Iter: 363 loss: 2.89048808e-06
Iter: 364 loss: 2.88632418e-06
Iter: 365 loss: 2.88626779e-06
Iter: 366 loss: 2.88295905e-06
Iter: 367 loss: 2.87582816e-06
Iter: 368 loss: 2.93374706e-06
Iter: 369 loss: 2.87542252e-06
Iter: 370 loss: 2.87194439e-06
Iter: 371 loss: 2.86902923e-06
Iter: 372 loss: 2.86799695e-06
Iter: 373 loss: 2.86239788e-06
Iter: 374 loss: 2.86150726e-06
Iter: 375 loss: 2.85766578e-06
Iter: 376 loss: 2.849167e-06
Iter: 377 loss: 2.89412264e-06
Iter: 378 loss: 2.84787416e-06
Iter: 379 loss: 2.84284897e-06
Iter: 380 loss: 2.84160205e-06
Iter: 381 loss: 2.838314e-06
Iter: 382 loss: 2.83253075e-06
Iter: 383 loss: 2.8865386e-06
Iter: 384 loss: 2.83223221e-06
Iter: 385 loss: 2.82675728e-06
Iter: 386 loss: 2.82658743e-06
Iter: 387 loss: 2.82215615e-06
Iter: 388 loss: 2.81576445e-06
Iter: 389 loss: 2.8561094e-06
Iter: 390 loss: 2.81495886e-06
Iter: 391 loss: 2.8088275e-06
Iter: 392 loss: 2.80938593e-06
Iter: 393 loss: 2.80399672e-06
Iter: 394 loss: 2.79672349e-06
Iter: 395 loss: 2.84912903e-06
Iter: 396 loss: 2.79614596e-06
Iter: 397 loss: 2.79029086e-06
Iter: 398 loss: 2.79487608e-06
Iter: 399 loss: 2.78672564e-06
Iter: 400 loss: 2.78471225e-06
Iter: 401 loss: 2.78283278e-06
Iter: 402 loss: 2.78053699e-06
Iter: 403 loss: 2.78248262e-06
Iter: 404 loss: 2.7790752e-06
Iter: 405 loss: 2.77594791e-06
Iter: 406 loss: 2.76853e-06
Iter: 407 loss: 2.86723207e-06
Iter: 408 loss: 2.76814262e-06
Iter: 409 loss: 2.76276864e-06
Iter: 410 loss: 2.81009238e-06
Iter: 411 loss: 2.76249489e-06
Iter: 412 loss: 2.75723778e-06
Iter: 413 loss: 2.76235255e-06
Iter: 414 loss: 2.75437924e-06
Iter: 415 loss: 2.749445e-06
Iter: 416 loss: 2.78481593e-06
Iter: 417 loss: 2.74898935e-06
Iter: 418 loss: 2.74503464e-06
Iter: 419 loss: 2.73723731e-06
Iter: 420 loss: 2.88771844e-06
Iter: 421 loss: 2.73721844e-06
Iter: 422 loss: 2.7297624e-06
Iter: 423 loss: 2.82063161e-06
Iter: 424 loss: 2.72964917e-06
Iter: 425 loss: 2.72364332e-06
Iter: 426 loss: 2.73833894e-06
Iter: 427 loss: 2.72145235e-06
Iter: 428 loss: 2.71613544e-06
Iter: 429 loss: 2.73920205e-06
Iter: 430 loss: 2.71489262e-06
Iter: 431 loss: 2.70968735e-06
Iter: 432 loss: 2.71584986e-06
Iter: 433 loss: 2.70684586e-06
Iter: 434 loss: 2.70298074e-06
Iter: 435 loss: 2.7398487e-06
Iter: 436 loss: 2.70290889e-06
Iter: 437 loss: 2.69874499e-06
Iter: 438 loss: 2.71969589e-06
Iter: 439 loss: 2.6979892e-06
Iter: 440 loss: 2.6945545e-06
Iter: 441 loss: 2.69255861e-06
Iter: 442 loss: 2.6910966e-06
Iter: 443 loss: 2.68635836e-06
Iter: 444 loss: 2.70323471e-06
Iter: 445 loss: 2.68503209e-06
Iter: 446 loss: 2.68113399e-06
Iter: 447 loss: 2.67608971e-06
Iter: 448 loss: 2.67579526e-06
Iter: 449 loss: 2.67145424e-06
Iter: 450 loss: 2.67143923e-06
Iter: 451 loss: 2.66753159e-06
Iter: 452 loss: 2.66569987e-06
Iter: 453 loss: 2.66380039e-06
Iter: 454 loss: 2.6587777e-06
Iter: 455 loss: 2.6857565e-06
Iter: 456 loss: 2.65794415e-06
Iter: 457 loss: 2.65341441e-06
Iter: 458 loss: 2.64973596e-06
Iter: 459 loss: 2.64855271e-06
Iter: 460 loss: 2.64392065e-06
Iter: 461 loss: 2.64386085e-06
Iter: 462 loss: 2.63939364e-06
Iter: 463 loss: 2.63795846e-06
Iter: 464 loss: 2.63540323e-06
Iter: 465 loss: 2.63011793e-06
Iter: 466 loss: 2.67119e-06
Iter: 467 loss: 2.62979484e-06
Iter: 468 loss: 2.62515505e-06
Iter: 469 loss: 2.63134029e-06
Iter: 470 loss: 2.62280719e-06
Iter: 471 loss: 2.62204321e-06
Iter: 472 loss: 2.62029926e-06
Iter: 473 loss: 2.61883724e-06
Iter: 474 loss: 2.61468858e-06
Iter: 475 loss: 2.64284813e-06
Iter: 476 loss: 2.61378e-06
Iter: 477 loss: 2.60877891e-06
Iter: 478 loss: 2.64223377e-06
Iter: 479 loss: 2.60826891e-06
Iter: 480 loss: 2.60509478e-06
Iter: 481 loss: 2.61905143e-06
Iter: 482 loss: 2.6043374e-06
Iter: 483 loss: 2.60189495e-06
Iter: 484 loss: 2.59835133e-06
Iter: 485 loss: 2.59824287e-06
Iter: 486 loss: 2.59426156e-06
Iter: 487 loss: 2.64198525e-06
Iter: 488 loss: 2.59419926e-06
Iter: 489 loss: 2.59075068e-06
Iter: 490 loss: 2.59164472e-06
Iter: 491 loss: 2.58835098e-06
Iter: 492 loss: 2.58416367e-06
Iter: 493 loss: 2.6045625e-06
Iter: 494 loss: 2.58341879e-06
Iter: 495 loss: 2.57995544e-06
Iter: 496 loss: 2.57449756e-06
Iter: 497 loss: 2.57444117e-06
Iter: 498 loss: 2.57063198e-06
Iter: 499 loss: 2.57036208e-06
Iter: 500 loss: 2.56696012e-06
Iter: 501 loss: 2.56491739e-06
Iter: 502 loss: 2.56348198e-06
Iter: 503 loss: 2.5598481e-06
Iter: 504 loss: 2.5597783e-06
Iter: 505 loss: 2.55644454e-06
Iter: 506 loss: 2.57377383e-06
Iter: 507 loss: 2.55584405e-06
Iter: 508 loss: 2.55350187e-06
Iter: 509 loss: 2.54910265e-06
Iter: 510 loss: 2.64555865e-06
Iter: 511 loss: 2.54906399e-06
Iter: 512 loss: 2.5465738e-06
Iter: 513 loss: 2.54658562e-06
Iter: 514 loss: 2.54405154e-06
Iter: 515 loss: 2.54007659e-06
Iter: 516 loss: 2.54001634e-06
Iter: 517 loss: 2.53576445e-06
Iter: 518 loss: 2.57086049e-06
Iter: 519 loss: 2.53551229e-06
Iter: 520 loss: 2.53217e-06
Iter: 521 loss: 2.53196527e-06
Iter: 522 loss: 2.52941823e-06
Iter: 523 loss: 2.52625387e-06
Iter: 524 loss: 2.52630434e-06
Iter: 525 loss: 2.52400014e-06
Iter: 526 loss: 2.52099062e-06
Iter: 527 loss: 2.52080372e-06
Iter: 528 loss: 2.51618303e-06
Iter: 529 loss: 2.53437429e-06
Iter: 530 loss: 2.51515166e-06
Iter: 531 loss: 2.51053552e-06
Iter: 532 loss: 2.50839321e-06
Iter: 533 loss: 2.50621383e-06
Iter: 534 loss: 2.50463381e-06
Iter: 535 loss: 2.50352286e-06
Iter: 536 loss: 2.5010529e-06
Iter: 537 loss: 2.5215204e-06
Iter: 538 loss: 2.50094945e-06
Iter: 539 loss: 2.49883033e-06
Iter: 540 loss: 2.49544837e-06
Iter: 541 loss: 2.49537356e-06
Iter: 542 loss: 2.49224513e-06
Iter: 543 loss: 2.50307539e-06
Iter: 544 loss: 2.49146251e-06
Iter: 545 loss: 2.48840729e-06
Iter: 546 loss: 2.49409368e-06
Iter: 547 loss: 2.48727474e-06
Iter: 548 loss: 2.48335391e-06
Iter: 549 loss: 2.48979541e-06
Iter: 550 loss: 2.48150718e-06
Iter: 551 loss: 2.47824437e-06
Iter: 552 loss: 2.48587526e-06
Iter: 553 loss: 2.47700109e-06
Iter: 554 loss: 2.47375783e-06
Iter: 555 loss: 2.48129027e-06
Iter: 556 loss: 2.47235903e-06
Iter: 557 loss: 2.4693868e-06
Iter: 558 loss: 2.48615333e-06
Iter: 559 loss: 2.46899663e-06
Iter: 560 loss: 2.46564241e-06
Iter: 561 loss: 2.46356785e-06
Iter: 562 loss: 2.46219088e-06
Iter: 563 loss: 2.45797946e-06
Iter: 564 loss: 2.46551804e-06
Iter: 565 loss: 2.4561914e-06
Iter: 566 loss: 2.45171395e-06
Iter: 567 loss: 2.48479955e-06
Iter: 568 loss: 2.45133219e-06
Iter: 569 loss: 2.44903276e-06
Iter: 570 loss: 2.47754247e-06
Iter: 571 loss: 2.44899866e-06
Iter: 572 loss: 2.44601802e-06
Iter: 573 loss: 2.445071e-06
Iter: 574 loss: 2.44323587e-06
Iter: 575 loss: 2.44083822e-06
Iter: 576 loss: 2.44749117e-06
Iter: 577 loss: 2.44004104e-06
Iter: 578 loss: 2.43749901e-06
Iter: 579 loss: 2.43559293e-06
Iter: 580 loss: 2.43470095e-06
Iter: 581 loss: 2.43080717e-06
Iter: 582 loss: 2.46634136e-06
Iter: 583 loss: 2.43070417e-06
Iter: 584 loss: 2.42796182e-06
Iter: 585 loss: 2.42768533e-06
Iter: 586 loss: 2.42573492e-06
Iter: 587 loss: 2.42157e-06
Iter: 588 loss: 2.43134673e-06
Iter: 589 loss: 2.42003034e-06
Iter: 590 loss: 2.41713451e-06
Iter: 591 loss: 2.42956503e-06
Iter: 592 loss: 2.41651173e-06
Iter: 593 loss: 2.41305975e-06
Iter: 594 loss: 2.41841e-06
Iter: 595 loss: 2.41134421e-06
Iter: 596 loss: 2.40850295e-06
Iter: 597 loss: 2.422531e-06
Iter: 598 loss: 2.40794543e-06
Iter: 599 loss: 2.40512918e-06
Iter: 600 loss: 2.40262557e-06
Iter: 601 loss: 2.40184954e-06
Iter: 602 loss: 2.39776955e-06
Iter: 603 loss: 2.42806163e-06
Iter: 604 loss: 2.39737278e-06
Iter: 605 loss: 2.39534802e-06
Iter: 606 loss: 2.39492465e-06
Iter: 607 loss: 2.39325118e-06
Iter: 608 loss: 2.39085375e-06
Iter: 609 loss: 2.39071051e-06
Iter: 610 loss: 2.38860503e-06
Iter: 611 loss: 2.38757593e-06
Iter: 612 loss: 2.38655457e-06
Iter: 613 loss: 2.38287066e-06
Iter: 614 loss: 2.41016278e-06
Iter: 615 loss: 2.38254529e-06
Iter: 616 loss: 2.38014877e-06
Iter: 617 loss: 2.38393113e-06
Iter: 618 loss: 2.37911695e-06
Iter: 619 loss: 2.37585982e-06
Iter: 620 loss: 2.37538302e-06
Iter: 621 loss: 2.37320774e-06
Iter: 622 loss: 2.36954816e-06
Iter: 623 loss: 2.39158658e-06
Iter: 624 loss: 2.36916571e-06
Iter: 625 loss: 2.36553865e-06
Iter: 626 loss: 2.36441224e-06
Iter: 627 loss: 2.36238111e-06
Iter: 628 loss: 2.35958373e-06
Iter: 629 loss: 2.35943389e-06
Iter: 630 loss: 2.35740822e-06
Iter: 631 loss: 2.35393463e-06
Iter: 632 loss: 2.35389348e-06
Iter: 633 loss: 2.34998015e-06
Iter: 634 loss: 2.38095117e-06
Iter: 635 loss: 2.34983781e-06
Iter: 636 loss: 2.34751201e-06
Iter: 637 loss: 2.37699442e-06
Iter: 638 loss: 2.34749768e-06
Iter: 639 loss: 2.3448215e-06
Iter: 640 loss: 2.34499976e-06
Iter: 641 loss: 2.34262575e-06
Iter: 642 loss: 2.34059416e-06
Iter: 643 loss: 2.34033e-06
Iter: 644 loss: 2.33890114e-06
Iter: 645 loss: 2.33563583e-06
Iter: 646 loss: 2.34137929e-06
Iter: 647 loss: 2.33432502e-06
Iter: 648 loss: 2.33130208e-06
Iter: 649 loss: 2.35996276e-06
Iter: 650 loss: 2.33115566e-06
Iter: 651 loss: 2.32912703e-06
Iter: 652 loss: 2.32779757e-06
Iter: 653 loss: 2.32706861e-06
Iter: 654 loss: 2.32348521e-06
Iter: 655 loss: 2.33499122e-06
Iter: 656 loss: 2.32235516e-06
Iter: 657 loss: 2.31919148e-06
Iter: 658 loss: 2.32420302e-06
Iter: 659 loss: 2.31782224e-06
Iter: 660 loss: 2.3142652e-06
Iter: 661 loss: 2.32985212e-06
Iter: 662 loss: 2.31356012e-06
Iter: 663 loss: 2.31074955e-06
Iter: 664 loss: 2.31835202e-06
Iter: 665 loss: 2.30972978e-06
Iter: 666 loss: 2.3060536e-06
Iter: 667 loss: 2.30867454e-06
Iter: 668 loss: 2.30385376e-06
Iter: 669 loss: 2.30093337e-06
Iter: 670 loss: 2.31323202e-06
Iter: 671 loss: 2.3003056e-06
Iter: 672 loss: 2.29727516e-06
Iter: 673 loss: 2.338375e-06
Iter: 674 loss: 2.29728198e-06
Iter: 675 loss: 2.29562693e-06
Iter: 676 loss: 2.29281409e-06
Iter: 677 loss: 2.29277748e-06
Iter: 678 loss: 2.29023385e-06
Iter: 679 loss: 2.29410512e-06
Iter: 680 loss: 2.28911313e-06
Iter: 681 loss: 2.28608246e-06
Iter: 682 loss: 2.30573e-06
Iter: 683 loss: 2.28582053e-06
Iter: 684 loss: 2.28365457e-06
Iter: 685 loss: 2.289843e-06
Iter: 686 loss: 2.28300064e-06
Iter: 687 loss: 2.28066483e-06
Iter: 688 loss: 2.27802593e-06
Iter: 689 loss: 2.27762803e-06
Iter: 690 loss: 2.27447845e-06
Iter: 691 loss: 2.31302397e-06
Iter: 692 loss: 2.27443388e-06
Iter: 693 loss: 2.27188639e-06
Iter: 694 loss: 2.26973771e-06
Iter: 695 loss: 2.26908378e-06
Iter: 696 loss: 2.26594193e-06
Iter: 697 loss: 2.31054605e-06
Iter: 698 loss: 2.26586963e-06
Iter: 699 loss: 2.26366706e-06
Iter: 700 loss: 2.26345537e-06
Iter: 701 loss: 2.26180373e-06
Iter: 702 loss: 2.25820622e-06
Iter: 703 loss: 2.27058877e-06
Iter: 704 loss: 2.2572458e-06
Iter: 705 loss: 2.25556391e-06
Iter: 706 loss: 2.25547819e-06
Iter: 707 loss: 2.25339068e-06
Iter: 708 loss: 2.25086524e-06
Iter: 709 loss: 2.25059603e-06
Iter: 710 loss: 2.24874225e-06
Iter: 711 loss: 2.24836867e-06
Iter: 712 loss: 2.24701284e-06
Iter: 713 loss: 2.24410519e-06
Iter: 714 loss: 2.25763688e-06
Iter: 715 loss: 2.24364658e-06
Iter: 716 loss: 2.24146243e-06
Iter: 717 loss: 2.25801705e-06
Iter: 718 loss: 2.24128075e-06
Iter: 719 loss: 2.23945381e-06
Iter: 720 loss: 2.23728807e-06
Iter: 721 loss: 2.23703069e-06
Iter: 722 loss: 2.23381858e-06
Iter: 723 loss: 2.25411395e-06
Iter: 724 loss: 2.2334284e-06
Iter: 725 loss: 2.23119514e-06
Iter: 726 loss: 2.23413167e-06
Iter: 727 loss: 2.2300444e-06
Iter: 728 loss: 2.22707536e-06
Iter: 729 loss: 2.23571101e-06
Iter: 730 loss: 2.22615836e-06
Iter: 731 loss: 2.22393305e-06
Iter: 732 loss: 2.23313145e-06
Iter: 733 loss: 2.22337053e-06
Iter: 734 loss: 2.22053109e-06
Iter: 735 loss: 2.22087738e-06
Iter: 736 loss: 2.2184372e-06
Iter: 737 loss: 2.21629807e-06
Iter: 738 loss: 2.25027202e-06
Iter: 739 loss: 2.21630626e-06
Iter: 740 loss: 2.21479036e-06
Iter: 741 loss: 2.23408483e-06
Iter: 742 loss: 2.21472874e-06
Iter: 743 loss: 2.21352457e-06
Iter: 744 loss: 2.21018672e-06
Iter: 745 loss: 2.23207257e-06
Iter: 746 loss: 2.2094132e-06
Iter: 747 loss: 2.20654442e-06
Iter: 748 loss: 2.21504865e-06
Iter: 749 loss: 2.20564061e-06
Iter: 750 loss: 2.20278753e-06
Iter: 751 loss: 2.22822268e-06
Iter: 752 loss: 2.20254924e-06
Iter: 753 loss: 2.2004408e-06
Iter: 754 loss: 2.20831248e-06
Iter: 755 loss: 2.19989442e-06
Iter: 756 loss: 2.19764843e-06
Iter: 757 loss: 2.19478488e-06
Iter: 758 loss: 2.19455569e-06
Iter: 759 loss: 2.19182743e-06
Iter: 760 loss: 2.22661902e-06
Iter: 761 loss: 2.19191679e-06
Iter: 762 loss: 2.18948867e-06
Iter: 763 loss: 2.1895487e-06
Iter: 764 loss: 2.18760943e-06
Iter: 765 loss: 2.18491118e-06
Iter: 766 loss: 2.19900267e-06
Iter: 767 loss: 2.18448849e-06
Iter: 768 loss: 2.18170862e-06
Iter: 769 loss: 2.18497439e-06
Iter: 770 loss: 2.18026639e-06
Iter: 771 loss: 2.17763818e-06
Iter: 772 loss: 2.1918861e-06
Iter: 773 loss: 2.17735396e-06
Iter: 774 loss: 2.17610432e-06
Iter: 775 loss: 2.17602087e-06
Iter: 776 loss: 2.17444403e-06
Iter: 777 loss: 2.17139973e-06
Iter: 778 loss: 2.22272183e-06
Iter: 779 loss: 2.17131696e-06
Iter: 780 loss: 2.16916305e-06
Iter: 781 loss: 2.17568163e-06
Iter: 782 loss: 2.1685662e-06
Iter: 783 loss: 2.16632293e-06
Iter: 784 loss: 2.16614899e-06
Iter: 785 loss: 2.16441958e-06
Iter: 786 loss: 2.16246144e-06
Iter: 787 loss: 2.16243029e-06
Iter: 788 loss: 2.16064291e-06
Iter: 789 loss: 2.15862246e-06
Iter: 790 loss: 2.15824275e-06
Iter: 791 loss: 2.15508544e-06
Iter: 792 loss: 2.16740318e-06
Iter: 793 loss: 2.1542719e-06
Iter: 794 loss: 2.15194086e-06
Iter: 795 loss: 2.15469527e-06
Iter: 796 loss: 2.15060186e-06
Iter: 797 loss: 2.14700958e-06
Iter: 798 loss: 2.15838463e-06
Iter: 799 loss: 2.14600709e-06
Iter: 800 loss: 2.1435194e-06
Iter: 801 loss: 2.15076625e-06
Iter: 802 loss: 2.14278225e-06
Iter: 803 loss: 2.14004899e-06
Iter: 804 loss: 2.14637134e-06
Iter: 805 loss: 2.13895873e-06
Iter: 806 loss: 2.13683961e-06
Iter: 807 loss: 2.15827754e-06
Iter: 808 loss: 2.136798e-06
Iter: 809 loss: 2.13480121e-06
Iter: 810 loss: 2.14806278e-06
Iter: 811 loss: 2.13462886e-06
Iter: 812 loss: 2.13342196e-06
Iter: 813 loss: 2.1308108e-06
Iter: 814 loss: 2.17200545e-06
Iter: 815 loss: 2.13075214e-06
Iter: 816 loss: 2.12831355e-06
Iter: 817 loss: 2.13041289e-06
Iter: 818 loss: 2.12677605e-06
Iter: 819 loss: 2.12362556e-06
Iter: 820 loss: 2.14679721e-06
Iter: 821 loss: 2.12328359e-06
Iter: 822 loss: 2.12132863e-06
Iter: 823 loss: 2.13559406e-06
Iter: 824 loss: 2.12104851e-06
Iter: 825 loss: 2.11905171e-06
Iter: 826 loss: 2.11726774e-06
Iter: 827 loss: 2.11674046e-06
Iter: 828 loss: 2.11411862e-06
Iter: 829 loss: 2.12250802e-06
Iter: 830 loss: 2.1132887e-06
Iter: 831 loss: 2.11020802e-06
Iter: 832 loss: 2.11822521e-06
Iter: 833 loss: 2.10911139e-06
Iter: 834 loss: 2.10705525e-06
Iter: 835 loss: 2.1274e-06
Iter: 836 loss: 2.1069543e-06
Iter: 837 loss: 2.10528742e-06
Iter: 838 loss: 2.10340727e-06
Iter: 839 loss: 2.10314147e-06
Iter: 840 loss: 2.10096641e-06
Iter: 841 loss: 2.10093549e-06
Iter: 842 loss: 2.09993846e-06
Iter: 843 loss: 2.09997097e-06
Iter: 844 loss: 2.09918676e-06
Iter: 845 loss: 2.09683367e-06
Iter: 846 loss: 2.11396605e-06
Iter: 847 loss: 2.09645395e-06
Iter: 848 loss: 2.09391419e-06
Iter: 849 loss: 2.10086864e-06
Iter: 850 loss: 2.09318091e-06
Iter: 851 loss: 2.0910602e-06
Iter: 852 loss: 2.09232712e-06
Iter: 853 loss: 2.08964866e-06
Iter: 854 loss: 2.08724805e-06
Iter: 855 loss: 2.11901101e-06
Iter: 856 loss: 2.08721531e-06
Iter: 857 loss: 2.08542383e-06
Iter: 858 loss: 2.08837378e-06
Iter: 859 loss: 2.08476695e-06
Iter: 860 loss: 2.08255233e-06
Iter: 861 loss: 2.08091046e-06
Iter: 862 loss: 2.08021447e-06
Iter: 863 loss: 2.07776338e-06
Iter: 864 loss: 2.09308496e-06
Iter: 865 loss: 2.07750327e-06
Iter: 866 loss: 2.07503626e-06
Iter: 867 loss: 2.08142569e-06
Iter: 868 loss: 2.07417133e-06
Iter: 869 loss: 2.07198809e-06
Iter: 870 loss: 2.07737912e-06
Iter: 871 loss: 2.07119342e-06
Iter: 872 loss: 2.0684347e-06
Iter: 873 loss: 2.07243761e-06
Iter: 874 loss: 2.0671614e-06
Iter: 875 loss: 2.06631421e-06
Iter: 876 loss: 2.06577761e-06
Iter: 877 loss: 2.06462232e-06
Iter: 878 loss: 2.06442974e-06
Iter: 879 loss: 2.06368873e-06
Iter: 880 loss: 2.06214304e-06
Iter: 881 loss: 2.06014147e-06
Iter: 882 loss: 2.06002733e-06
Iter: 883 loss: 2.05774154e-06
Iter: 884 loss: 2.06202776e-06
Iter: 885 loss: 2.05680226e-06
Iter: 886 loss: 2.05403967e-06
Iter: 887 loss: 2.06778577e-06
Iter: 888 loss: 2.05361403e-06
Iter: 889 loss: 2.0520265e-06
Iter: 890 loss: 2.07370431e-06
Iter: 891 loss: 2.0520315e-06
Iter: 892 loss: 2.05077413e-06
Iter: 893 loss: 2.04840785e-06
Iter: 894 loss: 2.04843923e-06
Iter: 895 loss: 2.04567687e-06
Iter: 896 loss: 2.06030813e-06
Iter: 897 loss: 2.04527441e-06
Iter: 898 loss: 2.04318576e-06
Iter: 899 loss: 2.04545859e-06
Iter: 900 loss: 2.04211301e-06
Iter: 901 loss: 2.03921945e-06
Iter: 902 loss: 2.05144966e-06
Iter: 903 loss: 2.03851732e-06
Iter: 904 loss: 2.03663421e-06
Iter: 905 loss: 2.04310572e-06
Iter: 906 loss: 2.03611e-06
Iter: 907 loss: 2.03406853e-06
Iter: 908 loss: 2.04405524e-06
Iter: 909 loss: 2.03371701e-06
Iter: 910 loss: 2.03169338e-06
Iter: 911 loss: 2.04767866e-06
Iter: 912 loss: 2.03164109e-06
Iter: 913 loss: 2.03061381e-06
Iter: 914 loss: 2.02909519e-06
Iter: 915 loss: 2.02912724e-06
Iter: 916 loss: 2.02693673e-06
Iter: 917 loss: 2.02771162e-06
Iter: 918 loss: 2.02548176e-06
Iter: 919 loss: 2.02324918e-06
Iter: 920 loss: 2.02841761e-06
Iter: 921 loss: 2.02250453e-06
Iter: 922 loss: 2.01991179e-06
Iter: 923 loss: 2.03400282e-06
Iter: 924 loss: 2.01957141e-06
Iter: 925 loss: 2.01752732e-06
Iter: 926 loss: 2.03043919e-06
Iter: 927 loss: 2.01727198e-06
Iter: 928 loss: 2.01580178e-06
Iter: 929 loss: 2.01375406e-06
Iter: 930 loss: 2.01358694e-06
Iter: 931 loss: 2.01117132e-06
Iter: 932 loss: 2.02661568e-06
Iter: 933 loss: 2.01078842e-06
Iter: 934 loss: 2.00851377e-06
Iter: 935 loss: 2.01414491e-06
Iter: 936 loss: 2.00763452e-06
Iter: 937 loss: 2.00558202e-06
Iter: 938 loss: 2.01763396e-06
Iter: 939 loss: 2.00526506e-06
Iter: 940 loss: 2.00363775e-06
Iter: 941 loss: 2.00603836e-06
Iter: 942 loss: 2.00282489e-06
Iter: 943 loss: 2.00092472e-06
Iter: 944 loss: 2.03058107e-06
Iter: 945 loss: 2.0009229e-06
Iter: 946 loss: 1.999701e-06
Iter: 947 loss: 1.99888564e-06
Iter: 948 loss: 1.99852047e-06
Iter: 949 loss: 1.99701185e-06
Iter: 950 loss: 1.99613669e-06
Iter: 951 loss: 1.99558963e-06
Iter: 952 loss: 1.99324313e-06
Iter: 953 loss: 2.00248223e-06
Iter: 954 loss: 1.99275064e-06
Iter: 955 loss: 1.99078568e-06
Iter: 956 loss: 1.99043484e-06
Iter: 957 loss: 1.98910629e-06
Iter: 958 loss: 1.98657e-06
Iter: 959 loss: 1.98654675e-06
Iter: 960 loss: 1.98527482e-06
Iter: 961 loss: 1.98882253e-06
Iter: 962 loss: 1.98480234e-06
Iter: 963 loss: 1.98356224e-06
Iter: 964 loss: 1.98103589e-06
Iter: 965 loss: 2.03072887e-06
Iter: 966 loss: 1.98104976e-06
Iter: 967 loss: 1.97861141e-06
Iter: 968 loss: 2.00968498e-06
Iter: 969 loss: 1.97854024e-06
Iter: 970 loss: 1.97627219e-06
Iter: 971 loss: 1.97823601e-06
Iter: 972 loss: 1.97489271e-06
Iter: 973 loss: 1.9728825e-06
Iter: 974 loss: 1.99283249e-06
Iter: 975 loss: 1.9728227e-06
Iter: 976 loss: 1.97180407e-06
Iter: 977 loss: 1.97177519e-06
Iter: 978 loss: 1.97075974e-06
Iter: 979 loss: 1.9688971e-06
Iter: 980 loss: 1.96890369e-06
Iter: 981 loss: 1.96715041e-06
Iter: 982 loss: 1.97148256e-06
Iter: 983 loss: 1.96659698e-06
Iter: 984 loss: 1.9647955e-06
Iter: 985 loss: 1.96436508e-06
Iter: 986 loss: 1.9633444e-06
Iter: 987 loss: 1.96101723e-06
Iter: 988 loss: 1.97417444e-06
Iter: 989 loss: 1.96071073e-06
Iter: 990 loss: 1.95887014e-06
Iter: 991 loss: 1.96445808e-06
Iter: 992 loss: 1.9583606e-06
Iter: 993 loss: 1.95651751e-06
Iter: 994 loss: 1.97076361e-06
Iter: 995 loss: 1.95638245e-06
Iter: 996 loss: 1.95524126e-06
Iter: 997 loss: 1.953744e-06
Iter: 998 loss: 1.95363145e-06
Iter: 999 loss: 1.95106577e-06
Iter: 1000 loss: 1.95601933e-06
Iter: 1001 loss: 1.94997278e-06
Iter: 1002 loss: 1.94782797e-06
Iter: 1003 loss: 1.96114615e-06
Iter: 1004 loss: 1.94759582e-06
Iter: 1005 loss: 1.94551376e-06
Iter: 1006 loss: 1.95039866e-06
Iter: 1007 loss: 1.94470613e-06
Iter: 1008 loss: 1.94377526e-06
Iter: 1009 loss: 1.94368431e-06
Iter: 1010 loss: 1.94266863e-06
Iter: 1011 loss: 1.94363429e-06
Iter: 1012 loss: 1.94208201e-06
Iter: 1013 loss: 1.94083032e-06
Iter: 1014 loss: 1.93913502e-06
Iter: 1015 loss: 1.93903634e-06
Iter: 1016 loss: 1.93741175e-06
Iter: 1017 loss: 1.94689483e-06
Iter: 1018 loss: 1.9371937e-06
Iter: 1019 loss: 1.93561209e-06
Iter: 1020 loss: 1.93403048e-06
Iter: 1021 loss: 1.9336876e-06
Iter: 1022 loss: 1.93173719e-06
Iter: 1023 loss: 1.95654252e-06
Iter: 1024 loss: 1.93175947e-06
Iter: 1025 loss: 1.93016785e-06
Iter: 1026 loss: 1.93369169e-06
Iter: 1027 loss: 1.92950779e-06
Iter: 1028 loss: 1.92787365e-06
Iter: 1029 loss: 1.93506094e-06
Iter: 1030 loss: 1.92751486e-06
Iter: 1031 loss: 1.92634184e-06
Iter: 1032 loss: 1.92508833e-06
Iter: 1033 loss: 1.92496532e-06
Iter: 1034 loss: 1.92284779e-06
Iter: 1035 loss: 1.93469236e-06
Iter: 1036 loss: 1.92256698e-06
Iter: 1037 loss: 1.92083189e-06
Iter: 1038 loss: 1.92778589e-06
Iter: 1039 loss: 1.92051539e-06
Iter: 1040 loss: 1.91884146e-06
Iter: 1041 loss: 1.92525317e-06
Iter: 1042 loss: 1.9183758e-06
Iter: 1043 loss: 1.91736649e-06
Iter: 1044 loss: 1.91730987e-06
Iter: 1045 loss: 1.91644244e-06
Iter: 1046 loss: 1.91440222e-06
Iter: 1047 loss: 1.93033179e-06
Iter: 1048 loss: 1.9139743e-06
Iter: 1049 loss: 1.91145409e-06
Iter: 1050 loss: 1.92401e-06
Iter: 1051 loss: 1.91103845e-06
Iter: 1052 loss: 1.90935543e-06
Iter: 1053 loss: 1.91252525e-06
Iter: 1054 loss: 1.90860692e-06
Iter: 1055 loss: 1.90641231e-06
Iter: 1056 loss: 1.91026061e-06
Iter: 1057 loss: 1.9054288e-06
Iter: 1058 loss: 1.90371156e-06
Iter: 1059 loss: 1.91569097e-06
Iter: 1060 loss: 1.90357696e-06
Iter: 1061 loss: 1.90181674e-06
Iter: 1062 loss: 1.90485207e-06
Iter: 1063 loss: 1.90097273e-06
Iter: 1064 loss: 1.89957188e-06
Iter: 1065 loss: 1.9034743e-06
Iter: 1066 loss: 1.89909053e-06
Iter: 1067 loss: 1.8974232e-06
Iter: 1068 loss: 1.89563377e-06
Iter: 1069 loss: 1.89533671e-06
Iter: 1070 loss: 1.89363936e-06
Iter: 1071 loss: 1.89366301e-06
Iter: 1072 loss: 1.8921055e-06
Iter: 1073 loss: 1.89410912e-06
Iter: 1074 loss: 1.89142963e-06
Iter: 1075 loss: 1.89029515e-06
Iter: 1076 loss: 1.89021318e-06
Iter: 1077 loss: 1.88921604e-06
Iter: 1078 loss: 1.88905778e-06
Iter: 1079 loss: 1.88843046e-06
Iter: 1080 loss: 1.88735146e-06
Iter: 1081 loss: 1.88537319e-06
Iter: 1082 loss: 1.88539605e-06
Iter: 1083 loss: 1.88343415e-06
Iter: 1084 loss: 1.90207174e-06
Iter: 1085 loss: 1.88335309e-06
Iter: 1086 loss: 1.88177978e-06
Iter: 1087 loss: 1.88264869e-06
Iter: 1088 loss: 1.88071454e-06
Iter: 1089 loss: 1.87858097e-06
Iter: 1090 loss: 1.88556635e-06
Iter: 1091 loss: 1.87802596e-06
Iter: 1092 loss: 1.87649539e-06
Iter: 1093 loss: 1.89306706e-06
Iter: 1094 loss: 1.87644605e-06
Iter: 1095 loss: 1.87511375e-06
Iter: 1096 loss: 1.87410922e-06
Iter: 1097 loss: 1.87367095e-06
Iter: 1098 loss: 1.87185242e-06
Iter: 1099 loss: 1.87479236e-06
Iter: 1100 loss: 1.87100704e-06
Iter: 1101 loss: 1.86873331e-06
Iter: 1102 loss: 1.8753592e-06
Iter: 1103 loss: 1.86795046e-06
Iter: 1104 loss: 1.86617081e-06
Iter: 1105 loss: 1.87345358e-06
Iter: 1106 loss: 1.86571083e-06
Iter: 1107 loss: 1.86390048e-06
Iter: 1108 loss: 1.87926048e-06
Iter: 1109 loss: 1.86381271e-06
Iter: 1110 loss: 1.86216789e-06
Iter: 1111 loss: 1.87223611e-06
Iter: 1112 loss: 1.86198258e-06
Iter: 1113 loss: 1.86119689e-06
Iter: 1114 loss: 1.86001319e-06
Iter: 1115 loss: 1.85997942e-06
Iter: 1116 loss: 1.85822773e-06
Iter: 1117 loss: 1.85902138e-06
Iter: 1118 loss: 1.85700105e-06
Iter: 1119 loss: 1.8554822e-06
Iter: 1120 loss: 1.86976172e-06
Iter: 1121 loss: 1.85544195e-06
Iter: 1122 loss: 1.85394219e-06
Iter: 1123 loss: 1.85355532e-06
Iter: 1124 loss: 1.85267982e-06
Iter: 1125 loss: 1.85127192e-06
Iter: 1126 loss: 1.87040632e-06
Iter: 1127 loss: 1.85119018e-06
Iter: 1128 loss: 1.84998839e-06
Iter: 1129 loss: 1.85170427e-06
Iter: 1130 loss: 1.84942667e-06
Iter: 1131 loss: 1.84783448e-06
Iter: 1132 loss: 1.84851126e-06
Iter: 1133 loss: 1.84682835e-06
Iter: 1134 loss: 1.8450944e-06
Iter: 1135 loss: 1.84521048e-06
Iter: 1136 loss: 1.84383168e-06
Iter: 1137 loss: 1.84151622e-06
Iter: 1138 loss: 1.86285899e-06
Iter: 1139 loss: 1.84147382e-06
Iter: 1140 loss: 1.8402078e-06
Iter: 1141 loss: 1.85027329e-06
Iter: 1142 loss: 1.84011446e-06
Iter: 1143 loss: 1.83892485e-06
Iter: 1144 loss: 1.84567705e-06
Iter: 1145 loss: 1.83878456e-06
Iter: 1146 loss: 1.83773341e-06
Iter: 1147 loss: 1.83609518e-06
Iter: 1148 loss: 1.83606517e-06
Iter: 1149 loss: 1.83445161e-06
Iter: 1150 loss: 1.83684847e-06
Iter: 1151 loss: 1.83366535e-06
Iter: 1152 loss: 1.83175462e-06
Iter: 1153 loss: 1.83660484e-06
Iter: 1154 loss: 1.83118141e-06
Iter: 1155 loss: 1.82947656e-06
Iter: 1156 loss: 1.83447526e-06
Iter: 1157 loss: 1.82892086e-06
Iter: 1158 loss: 1.82708845e-06
Iter: 1159 loss: 1.83162592e-06
Iter: 1160 loss: 1.82646158e-06
Iter: 1161 loss: 1.8251344e-06
Iter: 1162 loss: 1.83896691e-06
Iter: 1163 loss: 1.8251576e-06
Iter: 1164 loss: 1.82397662e-06
Iter: 1165 loss: 1.82277699e-06
Iter: 1166 loss: 1.82249812e-06
Iter: 1167 loss: 1.82048007e-06
Iter: 1168 loss: 1.82919541e-06
Iter: 1169 loss: 1.82012957e-06
Iter: 1170 loss: 1.81852533e-06
Iter: 1171 loss: 1.81754058e-06
Iter: 1172 loss: 1.81684118e-06
Iter: 1173 loss: 1.81587802e-06
Iter: 1174 loss: 1.81569089e-06
Iter: 1175 loss: 1.81486121e-06
Iter: 1176 loss: 1.82246617e-06
Iter: 1177 loss: 1.81485495e-06
Iter: 1178 loss: 1.81401947e-06
Iter: 1179 loss: 1.81247788e-06
Iter: 1180 loss: 1.84713565e-06
Iter: 1181 loss: 1.81246639e-06
Iter: 1182 loss: 1.81107566e-06
Iter: 1183 loss: 1.81752353e-06
Iter: 1184 loss: 1.81086989e-06
Iter: 1185 loss: 1.80975712e-06
Iter: 1186 loss: 1.80840357e-06
Iter: 1187 loss: 1.80824213e-06
Iter: 1188 loss: 1.80652603e-06
Iter: 1189 loss: 1.82735471e-06
Iter: 1190 loss: 1.80646089e-06
Iter: 1191 loss: 1.80517509e-06
Iter: 1192 loss: 1.80575512e-06
Iter: 1193 loss: 1.80422035e-06
Iter: 1194 loss: 1.80262248e-06
Iter: 1195 loss: 1.81301039e-06
Iter: 1196 loss: 1.80242796e-06
Iter: 1197 loss: 1.80109771e-06
Iter: 1198 loss: 1.80732638e-06
Iter: 1199 loss: 1.80091888e-06
Iter: 1200 loss: 1.79962967e-06
Iter: 1201 loss: 1.79859558e-06
Iter: 1202 loss: 1.79817857e-06
Iter: 1203 loss: 1.79627909e-06
Iter: 1204 loss: 1.80522682e-06
Iter: 1205 loss: 1.79605911e-06
Iter: 1206 loss: 1.7943471e-06
Iter: 1207 loss: 1.7952791e-06
Iter: 1208 loss: 1.79326037e-06
Iter: 1209 loss: 1.79287247e-06
Iter: 1210 loss: 1.79245217e-06
Iter: 1211 loss: 1.79144195e-06
Iter: 1212 loss: 1.7902637e-06
Iter: 1213 loss: 1.79012227e-06
Iter: 1214 loss: 1.78835273e-06
Iter: 1215 loss: 1.78996288e-06
Iter: 1216 loss: 1.78734751e-06
Iter: 1217 loss: 1.78585321e-06
Iter: 1218 loss: 1.7879745e-06
Iter: 1219 loss: 1.78511686e-06
Iter: 1220 loss: 1.78317407e-06
Iter: 1221 loss: 1.7844975e-06
Iter: 1222 loss: 1.78191181e-06
Iter: 1223 loss: 1.78013909e-06
Iter: 1224 loss: 1.79242147e-06
Iter: 1225 loss: 1.77993104e-06
Iter: 1226 loss: 1.77811671e-06
Iter: 1227 loss: 1.78040159e-06
Iter: 1228 loss: 1.77708284e-06
Iter: 1229 loss: 1.77584661e-06
Iter: 1230 loss: 1.79062522e-06
Iter: 1231 loss: 1.77583343e-06
Iter: 1232 loss: 1.77447566e-06
Iter: 1233 loss: 1.77256186e-06
Iter: 1234 loss: 1.77255163e-06
Iter: 1235 loss: 1.77048059e-06
Iter: 1236 loss: 1.78996993e-06
Iter: 1237 loss: 1.77040693e-06
Iter: 1238 loss: 1.76923504e-06
Iter: 1239 loss: 1.76838216e-06
Iter: 1240 loss: 1.76793412e-06
Iter: 1241 loss: 1.76637877e-06
Iter: 1242 loss: 1.78426239e-06
Iter: 1243 loss: 1.76631193e-06
Iter: 1244 loss: 1.76540982e-06
Iter: 1245 loss: 1.7653781e-06
Iter: 1246 loss: 1.76479307e-06
Iter: 1247 loss: 1.76325545e-06
Iter: 1248 loss: 1.77109723e-06
Iter: 1249 loss: 1.76279354e-06
Iter: 1250 loss: 1.76104254e-06
Iter: 1251 loss: 1.77617267e-06
Iter: 1252 loss: 1.76092203e-06
Iter: 1253 loss: 1.75966738e-06
Iter: 1254 loss: 1.7595894e-06
Iter: 1255 loss: 1.75856769e-06
Iter: 1256 loss: 1.75691275e-06
Iter: 1257 loss: 1.76570779e-06
Iter: 1258 loss: 1.75674347e-06
Iter: 1259 loss: 1.75527066e-06
Iter: 1260 loss: 1.75562275e-06
Iter: 1261 loss: 1.7542136e-06
Iter: 1262 loss: 1.75266803e-06
Iter: 1263 loss: 1.77648167e-06
Iter: 1264 loss: 1.75262846e-06
Iter: 1265 loss: 1.75157311e-06
Iter: 1266 loss: 1.75271339e-06
Iter: 1267 loss: 1.75100786e-06
Iter: 1268 loss: 1.74951379e-06
Iter: 1269 loss: 1.75186597e-06
Iter: 1270 loss: 1.74879278e-06
Iter: 1271 loss: 1.74749073e-06
Iter: 1272 loss: 1.75037917e-06
Iter: 1273 loss: 1.74698312e-06
Iter: 1274 loss: 1.74531124e-06
Iter: 1275 loss: 1.74534716e-06
Iter: 1276 loss: 1.74397758e-06
Iter: 1277 loss: 1.7445891e-06
Iter: 1278 loss: 1.7433116e-06
Iter: 1279 loss: 1.74251556e-06
Iter: 1280 loss: 1.74063769e-06
Iter: 1281 loss: 1.75873902e-06
Iter: 1282 loss: 1.74044158e-06
Iter: 1283 loss: 1.73841011e-06
Iter: 1284 loss: 1.74838294e-06
Iter: 1285 loss: 1.73808382e-06
Iter: 1286 loss: 1.73673243e-06
Iter: 1287 loss: 1.73774697e-06
Iter: 1288 loss: 1.73601143e-06
Iter: 1289 loss: 1.73387525e-06
Iter: 1290 loss: 1.73692479e-06
Iter: 1291 loss: 1.73284741e-06
Iter: 1292 loss: 1.73134163e-06
Iter: 1293 loss: 1.7345335e-06
Iter: 1294 loss: 1.73075262e-06
Iter: 1295 loss: 1.72875673e-06
Iter: 1296 loss: 1.73231729e-06
Iter: 1297 loss: 1.72780847e-06
Iter: 1298 loss: 1.72631451e-06
Iter: 1299 loss: 1.75073967e-06
Iter: 1300 loss: 1.72633952e-06
Iter: 1301 loss: 1.72499267e-06
Iter: 1302 loss: 1.7243442e-06
Iter: 1303 loss: 1.72375371e-06
Iter: 1304 loss: 1.72223463e-06
Iter: 1305 loss: 1.73456613e-06
Iter: 1306 loss: 1.72214925e-06
Iter: 1307 loss: 1.72093735e-06
Iter: 1308 loss: 1.72019918e-06
Iter: 1309 loss: 1.71975739e-06
Iter: 1310 loss: 1.71848535e-06
Iter: 1311 loss: 1.73841863e-06
Iter: 1312 loss: 1.71849706e-06
Iter: 1313 loss: 1.71735292e-06
Iter: 1314 loss: 1.72734985e-06
Iter: 1315 loss: 1.71730926e-06
Iter: 1316 loss: 1.71663396e-06
Iter: 1317 loss: 1.71484794e-06
Iter: 1318 loss: 1.72978707e-06
Iter: 1319 loss: 1.71466695e-06
Iter: 1320 loss: 1.71317447e-06
Iter: 1321 loss: 1.72302487e-06
Iter: 1322 loss: 1.71300485e-06
Iter: 1323 loss: 1.71128863e-06
Iter: 1324 loss: 1.71185013e-06
Iter: 1325 loss: 1.71012346e-06
Iter: 1326 loss: 1.70834721e-06
Iter: 1327 loss: 1.71982913e-06
Iter: 1328 loss: 1.70816452e-06
Iter: 1329 loss: 1.70682586e-06
Iter: 1330 loss: 1.70689077e-06
Iter: 1331 loss: 1.70566022e-06
Iter: 1332 loss: 1.70398948e-06
Iter: 1333 loss: 1.71928991e-06
Iter: 1334 loss: 1.70387318e-06
Iter: 1335 loss: 1.70282158e-06
Iter: 1336 loss: 1.70792839e-06
Iter: 1337 loss: 1.70262126e-06
Iter: 1338 loss: 1.70140049e-06
Iter: 1339 loss: 1.70206022e-06
Iter: 1340 loss: 1.70058752e-06
Iter: 1341 loss: 1.69956832e-06
Iter: 1342 loss: 1.70355815e-06
Iter: 1343 loss: 1.69932468e-06
Iter: 1344 loss: 1.69806697e-06
Iter: 1345 loss: 1.6962332e-06
Iter: 1346 loss: 1.69611394e-06
Iter: 1347 loss: 1.69875318e-06
Iter: 1348 loss: 1.69552493e-06
Iter: 1349 loss: 1.69501061e-06
Iter: 1350 loss: 1.69396026e-06
Iter: 1351 loss: 1.70619342e-06
Iter: 1352 loss: 1.69384225e-06
Iter: 1353 loss: 1.69248835e-06
Iter: 1354 loss: 1.69338364e-06
Iter: 1355 loss: 1.69165583e-06
Iter: 1356 loss: 1.6902145e-06
Iter: 1357 loss: 1.69330724e-06
Iter: 1358 loss: 1.68963629e-06
Iter: 1359 loss: 1.68791701e-06
Iter: 1360 loss: 1.6944706e-06
Iter: 1361 loss: 1.68745044e-06
Iter: 1362 loss: 1.68616577e-06
Iter: 1363 loss: 1.68712347e-06
Iter: 1364 loss: 1.68539964e-06
Iter: 1365 loss: 1.68363374e-06
Iter: 1366 loss: 1.69079681e-06
Iter: 1367 loss: 1.68316797e-06
Iter: 1368 loss: 1.68201825e-06
Iter: 1369 loss: 1.68981614e-06
Iter: 1370 loss: 1.68180486e-06
Iter: 1371 loss: 1.68066856e-06
Iter: 1372 loss: 1.68345241e-06
Iter: 1373 loss: 1.6802469e-06
Iter: 1374 loss: 1.67918574e-06
Iter: 1375 loss: 1.68206816e-06
Iter: 1376 loss: 1.67886196e-06
Iter: 1377 loss: 1.67770202e-06
Iter: 1378 loss: 1.67687335e-06
Iter: 1379 loss: 1.67649227e-06
Iter: 1380 loss: 1.67521853e-06
Iter: 1381 loss: 1.675161e-06
Iter: 1382 loss: 1.67443045e-06
Iter: 1383 loss: 1.68701717e-06
Iter: 1384 loss: 1.67442079e-06
Iter: 1385 loss: 1.67399298e-06
Iter: 1386 loss: 1.67263056e-06
Iter: 1387 loss: 1.67634835e-06
Iter: 1388 loss: 1.67186192e-06
Iter: 1389 loss: 1.67020414e-06
Iter: 1390 loss: 1.68693873e-06
Iter: 1391 loss: 1.67014969e-06
Iter: 1392 loss: 1.66891869e-06
Iter: 1393 loss: 1.66884297e-06
Iter: 1394 loss: 1.66801726e-06
Iter: 1395 loss: 1.66617383e-06
Iter: 1396 loss: 1.68250699e-06
Iter: 1397 loss: 1.66606424e-06
Iter: 1398 loss: 1.66493089e-06
Iter: 1399 loss: 1.66354334e-06
Iter: 1400 loss: 1.6634491e-06
Iter: 1401 loss: 1.66224913e-06
Iter: 1402 loss: 1.66216432e-06
Iter: 1403 loss: 1.66120822e-06
Iter: 1404 loss: 1.66243092e-06
Iter: 1405 loss: 1.66074926e-06
Iter: 1406 loss: 1.65953054e-06
Iter: 1407 loss: 1.66279074e-06
Iter: 1408 loss: 1.65902497e-06
Iter: 1409 loss: 1.6578183e-06
Iter: 1410 loss: 1.65953531e-06
Iter: 1411 loss: 1.65725669e-06
Iter: 1412 loss: 1.6559145e-06
Iter: 1413 loss: 1.66140671e-06
Iter: 1414 loss: 1.6556105e-06
Iter: 1415 loss: 1.65494578e-06
Iter: 1416 loss: 1.65495783e-06
Iter: 1417 loss: 1.65436359e-06
Iter: 1418 loss: 1.65377196e-06
Iter: 1419 loss: 1.65368749e-06
Iter: 1420 loss: 1.65281472e-06
Iter: 1421 loss: 1.65189545e-06
Iter: 1422 loss: 1.65173287e-06
Iter: 1423 loss: 1.65058088e-06
Iter: 1424 loss: 1.65602182e-06
Iter: 1425 loss: 1.65031247e-06
Iter: 1426 loss: 1.64903065e-06
Iter: 1427 loss: 1.6526534e-06
Iter: 1428 loss: 1.64854123e-06
Iter: 1429 loss: 1.64737355e-06
Iter: 1430 loss: 1.65490633e-06
Iter: 1431 loss: 1.64717676e-06
Iter: 1432 loss: 1.64625908e-06
Iter: 1433 loss: 1.64508e-06
Iter: 1434 loss: 1.64497578e-06
Iter: 1435 loss: 1.64384505e-06
Iter: 1436 loss: 1.64382459e-06
Iter: 1437 loss: 1.64300491e-06
Iter: 1438 loss: 1.64329697e-06
Iter: 1439 loss: 1.64242806e-06
Iter: 1440 loss: 1.64139306e-06
Iter: 1441 loss: 1.6436004e-06
Iter: 1442 loss: 1.6409233e-06
Iter: 1443 loss: 1.63995492e-06
Iter: 1444 loss: 1.6441619e-06
Iter: 1445 loss: 1.63981463e-06
Iter: 1446 loss: 1.63904417e-06
Iter: 1447 loss: 1.64511903e-06
Iter: 1448 loss: 1.63897721e-06
Iter: 1449 loss: 1.63806453e-06
Iter: 1450 loss: 1.63869e-06
Iter: 1451 loss: 1.63758148e-06
Iter: 1452 loss: 1.6368441e-06
Iter: 1453 loss: 1.63678305e-06
Iter: 1454 loss: 1.63627692e-06
Iter: 1455 loss: 1.63520212e-06
Iter: 1456 loss: 1.63401558e-06
Iter: 1457 loss: 1.63384448e-06
Iter: 1458 loss: 1.63243908e-06
Iter: 1459 loss: 1.65330789e-06
Iter: 1460 loss: 1.63245249e-06
Iter: 1461 loss: 1.63146706e-06
Iter: 1462 loss: 1.63362279e-06
Iter: 1463 loss: 1.63101527e-06
Iter: 1464 loss: 1.62991637e-06
Iter: 1465 loss: 1.63191498e-06
Iter: 1466 loss: 1.62937158e-06
Iter: 1467 loss: 1.62814649e-06
Iter: 1468 loss: 1.62968877e-06
Iter: 1469 loss: 1.62745243e-06
Iter: 1470 loss: 1.62629794e-06
Iter: 1471 loss: 1.64163509e-06
Iter: 1472 loss: 1.62624224e-06
Iter: 1473 loss: 1.62543199e-06
Iter: 1474 loss: 1.62560423e-06
Iter: 1475 loss: 1.62485503e-06
Iter: 1476 loss: 1.62356093e-06
Iter: 1477 loss: 1.6256763e-06
Iter: 1478 loss: 1.62305651e-06
Iter: 1479 loss: 1.62226183e-06
Iter: 1480 loss: 1.62220715e-06
Iter: 1481 loss: 1.62144511e-06
Iter: 1482 loss: 1.62383265e-06
Iter: 1483 loss: 1.62122944e-06
Iter: 1484 loss: 1.62058234e-06
Iter: 1485 loss: 1.61938306e-06
Iter: 1486 loss: 1.64685503e-06
Iter: 1487 loss: 1.61940625e-06
Iter: 1488 loss: 1.6182164e-06
Iter: 1489 loss: 1.62219885e-06
Iter: 1490 loss: 1.61794469e-06
Iter: 1491 loss: 1.61666128e-06
Iter: 1492 loss: 1.61751166e-06
Iter: 1493 loss: 1.61589458e-06
Iter: 1494 loss: 1.61482319e-06
Iter: 1495 loss: 1.62387937e-06
Iter: 1496 loss: 1.61472633e-06
Iter: 1497 loss: 1.61345577e-06
Iter: 1498 loss: 1.61413652e-06
Iter: 1499 loss: 1.61264848e-06
Iter: 1500 loss: 1.61146011e-06
Iter: 1501 loss: 1.61927824e-06
Iter: 1502 loss: 1.61135426e-06
Iter: 1503 loss: 1.6104309e-06
Iter: 1504 loss: 1.61115349e-06
Iter: 1505 loss: 1.60983359e-06
Iter: 1506 loss: 1.60858144e-06
Iter: 1507 loss: 1.6169638e-06
Iter: 1508 loss: 1.6084723e-06
Iter: 1509 loss: 1.6077189e-06
Iter: 1510 loss: 1.60771719e-06
Iter: 1511 loss: 1.60702484e-06
Iter: 1512 loss: 1.605899e-06
Iter: 1513 loss: 1.61158528e-06
Iter: 1514 loss: 1.60569448e-06
Iter: 1515 loss: 1.60505078e-06
Iter: 1516 loss: 1.60498382e-06
Iter: 1517 loss: 1.60462639e-06
Iter: 1518 loss: 1.60374339e-06
Iter: 1519 loss: 1.61747926e-06
Iter: 1520 loss: 1.60376453e-06
Iter: 1521 loss: 1.6027451e-06
Iter: 1522 loss: 1.6022409e-06
Iter: 1523 loss: 1.60167826e-06
Iter: 1524 loss: 1.60058732e-06
Iter: 1525 loss: 1.61128014e-06
Iter: 1526 loss: 1.60053355e-06
Iter: 1527 loss: 1.59961394e-06
Iter: 1528 loss: 1.59986917e-06
Iter: 1529 loss: 1.5989549e-06
Iter: 1530 loss: 1.59790761e-06
Iter: 1531 loss: 1.60749607e-06
Iter: 1532 loss: 1.59788112e-06
Iter: 1533 loss: 1.59693502e-06
Iter: 1534 loss: 1.59673732e-06
Iter: 1535 loss: 1.59612875e-06
Iter: 1536 loss: 1.59492e-06
Iter: 1537 loss: 1.60082573e-06
Iter: 1538 loss: 1.59470778e-06
Iter: 1539 loss: 1.59353874e-06
Iter: 1540 loss: 1.5982065e-06
Iter: 1541 loss: 1.59325032e-06
Iter: 1542 loss: 1.59201409e-06
Iter: 1543 loss: 1.59435535e-06
Iter: 1544 loss: 1.59153785e-06
Iter: 1545 loss: 1.59059323e-06
Iter: 1546 loss: 1.59296167e-06
Iter: 1547 loss: 1.59028377e-06
Iter: 1548 loss: 1.58936473e-06
Iter: 1549 loss: 1.58939713e-06
Iter: 1550 loss: 1.58859029e-06
Iter: 1551 loss: 1.5883511e-06
Iter: 1552 loss: 1.58795069e-06
Iter: 1553 loss: 1.58721718e-06
Iter: 1554 loss: 1.58660885e-06
Iter: 1555 loss: 1.58641046e-06
Iter: 1556 loss: 1.58512398e-06
Iter: 1557 loss: 1.58727175e-06
Iter: 1558 loss: 1.58451905e-06
Iter: 1559 loss: 1.58328635e-06
Iter: 1560 loss: 1.5865146e-06
Iter: 1561 loss: 1.58283831e-06
Iter: 1562 loss: 1.58137186e-06
Iter: 1563 loss: 1.58650664e-06
Iter: 1564 loss: 1.58098624e-06
Iter: 1565 loss: 1.57984095e-06
Iter: 1566 loss: 1.58844398e-06
Iter: 1567 loss: 1.57977342e-06
Iter: 1568 loss: 1.57874126e-06
Iter: 1569 loss: 1.57742306e-06
Iter: 1570 loss: 1.57743239e-06
Iter: 1571 loss: 1.57613624e-06
Iter: 1572 loss: 1.5761334e-06
Iter: 1573 loss: 1.57529666e-06
Iter: 1574 loss: 1.57652971e-06
Iter: 1575 loss: 1.57484169e-06
Iter: 1576 loss: 1.57371301e-06
Iter: 1577 loss: 1.57403679e-06
Iter: 1578 loss: 1.57285501e-06
Iter: 1579 loss: 1.57247155e-06
Iter: 1580 loss: 1.57226964e-06
Iter: 1581 loss: 1.57167324e-06
Iter: 1582 loss: 1.57162788e-06
Iter: 1583 loss: 1.57115346e-06
Iter: 1584 loss: 1.5703298e-06
Iter: 1585 loss: 1.56954115e-06
Iter: 1586 loss: 1.56932219e-06
Iter: 1587 loss: 1.56833335e-06
Iter: 1588 loss: 1.57069906e-06
Iter: 1589 loss: 1.5679409e-06
Iter: 1590 loss: 1.56667716e-06
Iter: 1591 loss: 1.56811188e-06
Iter: 1592 loss: 1.5660188e-06
Iter: 1593 loss: 1.56499914e-06
Iter: 1594 loss: 1.5715766e-06
Iter: 1595 loss: 1.56490205e-06
Iter: 1596 loss: 1.56389228e-06
Iter: 1597 loss: 1.56603141e-06
Iter: 1598 loss: 1.56345186e-06
Iter: 1599 loss: 1.56227611e-06
Iter: 1600 loss: 1.5648418e-06
Iter: 1601 loss: 1.56189162e-06
Iter: 1602 loss: 1.56066722e-06
Iter: 1603 loss: 1.56277565e-06
Iter: 1604 loss: 1.56019223e-06
Iter: 1605 loss: 1.55909333e-06
Iter: 1606 loss: 1.57089949e-06
Iter: 1607 loss: 1.55908697e-06
Iter: 1608 loss: 1.5582176e-06
Iter: 1609 loss: 1.55756982e-06
Iter: 1610 loss: 1.55726411e-06
Iter: 1611 loss: 1.55610292e-06
Iter: 1612 loss: 1.56573037e-06
Iter: 1613 loss: 1.55601947e-06
Iter: 1614 loss: 1.55530597e-06
Iter: 1615 loss: 1.55529335e-06
Iter: 1616 loss: 1.55487714e-06
Iter: 1617 loss: 1.55395833e-06
Iter: 1618 loss: 1.56333738e-06
Iter: 1619 loss: 1.55385419e-06
Iter: 1620 loss: 1.55265195e-06
Iter: 1621 loss: 1.55577015e-06
Iter: 1622 loss: 1.55229282e-06
Iter: 1623 loss: 1.55116618e-06
Iter: 1624 loss: 1.55152543e-06
Iter: 1625 loss: 1.55045313e-06
Iter: 1626 loss: 1.54899101e-06
Iter: 1627 loss: 1.55757107e-06
Iter: 1628 loss: 1.54881809e-06
Iter: 1629 loss: 1.54781787e-06
Iter: 1630 loss: 1.54850864e-06
Iter: 1631 loss: 1.54724239e-06
Iter: 1632 loss: 1.5457033e-06
Iter: 1633 loss: 1.55307487e-06
Iter: 1634 loss: 1.54545683e-06
Iter: 1635 loss: 1.544474e-06
Iter: 1636 loss: 1.54477607e-06
Iter: 1637 loss: 1.54374015e-06
Iter: 1638 loss: 1.54227655e-06
Iter: 1639 loss: 1.54762301e-06
Iter: 1640 loss: 1.54188933e-06
Iter: 1641 loss: 1.54062946e-06
Iter: 1642 loss: 1.55100668e-06
Iter: 1643 loss: 1.54057386e-06
Iter: 1644 loss: 1.53970279e-06
Iter: 1645 loss: 1.53925987e-06
Iter: 1646 loss: 1.53889221e-06
Iter: 1647 loss: 1.53831775e-06
Iter: 1648 loss: 1.53813755e-06
Iter: 1649 loss: 1.5375482e-06
Iter: 1650 loss: 1.53703877e-06
Iter: 1651 loss: 1.53688939e-06
Iter: 1652 loss: 1.53617361e-06
Iter: 1653 loss: 1.53543897e-06
Iter: 1654 loss: 1.53525707e-06
Iter: 1655 loss: 1.53391136e-06
Iter: 1656 loss: 1.53877409e-06
Iter: 1657 loss: 1.5335238e-06
Iter: 1658 loss: 1.53251108e-06
Iter: 1659 loss: 1.53374936e-06
Iter: 1660 loss: 1.53197391e-06
Iter: 1661 loss: 1.53065844e-06
Iter: 1662 loss: 1.53528924e-06
Iter: 1663 loss: 1.53038309e-06
Iter: 1664 loss: 1.52943664e-06
Iter: 1665 loss: 1.53545216e-06
Iter: 1666 loss: 1.52933e-06
Iter: 1667 loss: 1.5283797e-06
Iter: 1668 loss: 1.52858672e-06
Iter: 1669 loss: 1.52772122e-06
Iter: 1670 loss: 1.52654309e-06
Iter: 1671 loss: 1.52967266e-06
Iter: 1672 loss: 1.52621487e-06
Iter: 1673 loss: 1.52500843e-06
Iter: 1674 loss: 1.53243263e-06
Iter: 1675 loss: 1.5248113e-06
Iter: 1676 loss: 1.52395387e-06
Iter: 1677 loss: 1.52633436e-06
Iter: 1678 loss: 1.52361145e-06
Iter: 1679 loss: 1.52287862e-06
Iter: 1680 loss: 1.52684834e-06
Iter: 1681 loss: 1.52273265e-06
Iter: 1682 loss: 1.5216358e-06
Iter: 1683 loss: 1.52344091e-06
Iter: 1684 loss: 1.52110727e-06
Iter: 1685 loss: 1.52045709e-06
Iter: 1686 loss: 1.52004884e-06
Iter: 1687 loss: 1.51975439e-06
Iter: 1688 loss: 1.51852714e-06
Iter: 1689 loss: 1.51858262e-06
Iter: 1690 loss: 1.51760742e-06
Iter: 1691 loss: 1.51630763e-06
Iter: 1692 loss: 1.52715745e-06
Iter: 1693 loss: 1.51625341e-06
Iter: 1694 loss: 1.51520294e-06
Iter: 1695 loss: 1.51397433e-06
Iter: 1696 loss: 1.51385007e-06
Iter: 1697 loss: 1.51236691e-06
Iter: 1698 loss: 1.53295957e-06
Iter: 1699 loss: 1.51235008e-06
Iter: 1700 loss: 1.51120787e-06
Iter: 1701 loss: 1.51349786e-06
Iter: 1702 loss: 1.51070822e-06
Iter: 1703 loss: 1.50930703e-06
Iter: 1704 loss: 1.51135055e-06
Iter: 1705 loss: 1.50861285e-06
Iter: 1706 loss: 1.50766755e-06
Iter: 1707 loss: 1.51515e-06
Iter: 1708 loss: 1.50761787e-06
Iter: 1709 loss: 1.50653341e-06
Iter: 1710 loss: 1.5069586e-06
Iter: 1711 loss: 1.5058182e-06
Iter: 1712 loss: 1.50479354e-06
Iter: 1713 loss: 1.51464781e-06
Iter: 1714 loss: 1.50473852e-06
Iter: 1715 loss: 1.5041303e-06
Iter: 1716 loss: 1.50410642e-06
Iter: 1717 loss: 1.5036843e-06
Iter: 1718 loss: 1.50248388e-06
Iter: 1719 loss: 1.50764618e-06
Iter: 1720 loss: 1.50199571e-06
Iter: 1721 loss: 1.50084929e-06
Iter: 1722 loss: 1.51508061e-06
Iter: 1723 loss: 1.50086055e-06
Iter: 1724 loss: 1.49996629e-06
Iter: 1725 loss: 1.49916661e-06
Iter: 1726 loss: 1.49899029e-06
Iter: 1727 loss: 1.49762786e-06
Iter: 1728 loss: 1.50709161e-06
Iter: 1729 loss: 1.49749735e-06
Iter: 1730 loss: 1.49641266e-06
Iter: 1731 loss: 1.49696257e-06
Iter: 1732 loss: 1.49568791e-06
Iter: 1733 loss: 1.49432663e-06
Iter: 1734 loss: 1.50580513e-06
Iter: 1735 loss: 1.49428854e-06
Iter: 1736 loss: 1.49326547e-06
Iter: 1737 loss: 1.49420794e-06
Iter: 1738 loss: 1.49266293e-06
Iter: 1739 loss: 1.49154653e-06
Iter: 1740 loss: 1.49507457e-06
Iter: 1741 loss: 1.49123946e-06
Iter: 1742 loss: 1.49020707e-06
Iter: 1743 loss: 1.49879247e-06
Iter: 1744 loss: 1.49012226e-06
Iter: 1745 loss: 1.48926324e-06
Iter: 1746 loss: 1.48923186e-06
Iter: 1747 loss: 1.48848585e-06
Iter: 1748 loss: 1.48819743e-06
Iter: 1749 loss: 1.48795812e-06
Iter: 1750 loss: 1.48751337e-06
Iter: 1751 loss: 1.48683228e-06
Iter: 1752 loss: 1.48682398e-06
Iter: 1753 loss: 1.48609138e-06
Iter: 1754 loss: 1.48524077e-06
Iter: 1755 loss: 1.48517188e-06
Iter: 1756 loss: 1.48393303e-06
Iter: 1757 loss: 1.49164896e-06
Iter: 1758 loss: 1.48385652e-06
Iter: 1759 loss: 1.48255094e-06
Iter: 1760 loss: 1.48214508e-06
Iter: 1761 loss: 1.48144716e-06
Iter: 1762 loss: 1.48021877e-06
Iter: 1763 loss: 1.49143568e-06
Iter: 1764 loss: 1.48016966e-06
Iter: 1765 loss: 1.47916035e-06
Iter: 1766 loss: 1.47960463e-06
Iter: 1767 loss: 1.4784539e-06
Iter: 1768 loss: 1.4773168e-06
Iter: 1769 loss: 1.49062544e-06
Iter: 1770 loss: 1.47731987e-06
Iter: 1771 loss: 1.47650508e-06
Iter: 1772 loss: 1.47557853e-06
Iter: 1773 loss: 1.47541493e-06
Iter: 1774 loss: 1.47417086e-06
Iter: 1775 loss: 1.48611548e-06
Iter: 1776 loss: 1.4740765e-06
Iter: 1777 loss: 1.47330525e-06
Iter: 1778 loss: 1.47921105e-06
Iter: 1779 loss: 1.47326705e-06
Iter: 1780 loss: 1.47272931e-06
Iter: 1781 loss: 1.47320611e-06
Iter: 1782 loss: 1.47234823e-06
Iter: 1783 loss: 1.47129208e-06
Iter: 1784 loss: 1.47439573e-06
Iter: 1785 loss: 1.47096807e-06
Iter: 1786 loss: 1.47038349e-06
Iter: 1787 loss: 1.4699591e-06
Iter: 1788 loss: 1.46971956e-06
Iter: 1789 loss: 1.46874413e-06
Iter: 1790 loss: 1.46878642e-06
Iter: 1791 loss: 1.4679656e-06
Iter: 1792 loss: 1.46669117e-06
Iter: 1793 loss: 1.47876449e-06
Iter: 1794 loss: 1.46660182e-06
Iter: 1795 loss: 1.46575007e-06
Iter: 1796 loss: 1.46461e-06
Iter: 1797 loss: 1.46455909e-06
Iter: 1798 loss: 1.46324226e-06
Iter: 1799 loss: 1.48125923e-06
Iter: 1800 loss: 1.46323669e-06
Iter: 1801 loss: 1.4623472e-06
Iter: 1802 loss: 1.4628647e-06
Iter: 1803 loss: 1.46175046e-06
Iter: 1804 loss: 1.46046841e-06
Iter: 1805 loss: 1.46667776e-06
Iter: 1806 loss: 1.46025263e-06
Iter: 1807 loss: 1.45942477e-06
Iter: 1808 loss: 1.4610996e-06
Iter: 1809 loss: 1.4591069e-06
Iter: 1810 loss: 1.45802119e-06
Iter: 1811 loss: 1.46150057e-06
Iter: 1812 loss: 1.4577837e-06
Iter: 1813 loss: 1.45698255e-06
Iter: 1814 loss: 1.46408934e-06
Iter: 1815 loss: 1.45697527e-06
Iter: 1816 loss: 1.45633339e-06
Iter: 1817 loss: 1.46045409e-06
Iter: 1818 loss: 1.45626973e-06
Iter: 1819 loss: 1.45582965e-06
Iter: 1820 loss: 1.45463241e-06
Iter: 1821 loss: 1.46723232e-06
Iter: 1822 loss: 1.45449712e-06
Iter: 1823 loss: 1.45334275e-06
Iter: 1824 loss: 1.45718252e-06
Iter: 1825 loss: 1.45304864e-06
Iter: 1826 loss: 1.45189097e-06
Iter: 1827 loss: 1.45688273e-06
Iter: 1828 loss: 1.45168872e-06
Iter: 1829 loss: 1.45080458e-06
Iter: 1830 loss: 1.45288959e-06
Iter: 1831 loss: 1.45041827e-06
Iter: 1832 loss: 1.44937212e-06
Iter: 1833 loss: 1.44973365e-06
Iter: 1834 loss: 1.44865396e-06
Iter: 1835 loss: 1.44764647e-06
Iter: 1836 loss: 1.46093612e-06
Iter: 1837 loss: 1.44764158e-06
Iter: 1838 loss: 1.44689022e-06
Iter: 1839 loss: 1.44698356e-06
Iter: 1840 loss: 1.44631815e-06
Iter: 1841 loss: 1.44512205e-06
Iter: 1842 loss: 1.44948899e-06
Iter: 1843 loss: 1.44483283e-06
Iter: 1844 loss: 1.4439903e-06
Iter: 1845 loss: 1.44722344e-06
Iter: 1846 loss: 1.44377827e-06
Iter: 1847 loss: 1.44289902e-06
Iter: 1848 loss: 1.44658736e-06
Iter: 1849 loss: 1.44265482e-06
Iter: 1850 loss: 1.44231899e-06
Iter: 1851 loss: 1.44229284e-06
Iter: 1852 loss: 1.44201272e-06
Iter: 1853 loss: 1.44140063e-06
Iter: 1854 loss: 1.45152717e-06
Iter: 1855 loss: 1.44139688e-06
Iter: 1856 loss: 1.44056889e-06
Iter: 1857 loss: 1.43958323e-06
Iter: 1858 loss: 1.43951627e-06
Iter: 1859 loss: 1.43855937e-06
Iter: 1860 loss: 1.45050171e-06
Iter: 1861 loss: 1.4385339e-06
Iter: 1862 loss: 1.43756631e-06
Iter: 1863 loss: 1.437337e-06
Iter: 1864 loss: 1.43670047e-06
Iter: 1865 loss: 1.4354946e-06
Iter: 1866 loss: 1.44449757e-06
Iter: 1867 loss: 1.43542138e-06
Iter: 1868 loss: 1.43462717e-06
Iter: 1869 loss: 1.43433181e-06
Iter: 1870 loss: 1.43384057e-06
Iter: 1871 loss: 1.432516e-06
Iter: 1872 loss: 1.44352066e-06
Iter: 1873 loss: 1.43246393e-06
Iter: 1874 loss: 1.43153261e-06
Iter: 1875 loss: 1.43435466e-06
Iter: 1876 loss: 1.43133673e-06
Iter: 1877 loss: 1.43034117e-06
Iter: 1878 loss: 1.43028069e-06
Iter: 1879 loss: 1.42953991e-06
Iter: 1880 loss: 1.42893646e-06
Iter: 1881 loss: 1.42883914e-06
Iter: 1882 loss: 1.42840031e-06
Iter: 1883 loss: 1.43071077e-06
Iter: 1884 loss: 1.42834665e-06
Iter: 1885 loss: 1.42774638e-06
Iter: 1886 loss: 1.42698718e-06
Iter: 1887 loss: 1.42691692e-06
Iter: 1888 loss: 1.42609804e-06
Iter: 1889 loss: 1.42666829e-06
Iter: 1890 loss: 1.42557315e-06
Iter: 1891 loss: 1.42456634e-06
Iter: 1892 loss: 1.42433692e-06
Iter: 1893 loss: 1.42374756e-06
Iter: 1894 loss: 1.42279259e-06
Iter: 1895 loss: 1.42282306e-06
Iter: 1896 loss: 1.42214094e-06
Iter: 1897 loss: 1.42142403e-06
Iter: 1898 loss: 1.42138742e-06
Iter: 1899 loss: 1.42018587e-06
Iter: 1900 loss: 1.42624924e-06
Iter: 1901 loss: 1.41995042e-06
Iter: 1902 loss: 1.41912687e-06
Iter: 1903 loss: 1.42187014e-06
Iter: 1904 loss: 1.41893133e-06
Iter: 1905 loss: 1.4179675e-06
Iter: 1906 loss: 1.41997839e-06
Iter: 1907 loss: 1.41753776e-06
Iter: 1908 loss: 1.41667408e-06
Iter: 1909 loss: 1.4204843e-06
Iter: 1910 loss: 1.41644659e-06
Iter: 1911 loss: 1.41571377e-06
Iter: 1912 loss: 1.41696182e-06
Iter: 1913 loss: 1.41538749e-06
Iter: 1914 loss: 1.41463977e-06
Iter: 1915 loss: 1.42384226e-06
Iter: 1916 loss: 1.41463602e-06
Iter: 1917 loss: 1.41405053e-06
Iter: 1918 loss: 1.41844794e-06
Iter: 1919 loss: 1.41401324e-06
Iter: 1920 loss: 1.41366684e-06
Iter: 1921 loss: 1.41270675e-06
Iter: 1922 loss: 1.41823966e-06
Iter: 1923 loss: 1.41246846e-06
Iter: 1924 loss: 1.41130022e-06
Iter: 1925 loss: 1.41912096e-06
Iter: 1926 loss: 1.41121291e-06
Iter: 1927 loss: 1.41040732e-06
Iter: 1928 loss: 1.41084092e-06
Iter: 1929 loss: 1.40991256e-06
Iter: 1930 loss: 1.40885084e-06
Iter: 1931 loss: 1.41344265e-06
Iter: 1932 loss: 1.40854922e-06
Iter: 1933 loss: 1.40748216e-06
Iter: 1934 loss: 1.40823749e-06
Iter: 1935 loss: 1.40687439e-06
Iter: 1936 loss: 1.40580187e-06
Iter: 1937 loss: 1.4134248e-06
Iter: 1938 loss: 1.40575435e-06
Iter: 1939 loss: 1.40480165e-06
Iter: 1940 loss: 1.40526322e-06
Iter: 1941 loss: 1.40416978e-06
Iter: 1942 loss: 1.4032521e-06
Iter: 1943 loss: 1.41644944e-06
Iter: 1944 loss: 1.40327165e-06
Iter: 1945 loss: 1.40279917e-06
Iter: 1946 loss: 1.40239331e-06
Iter: 1947 loss: 1.40226041e-06
Iter: 1948 loss: 1.40138047e-06
Iter: 1949 loss: 1.40552993e-06
Iter: 1950 loss: 1.40118334e-06
Iter: 1951 loss: 1.40068391e-06
Iter: 1952 loss: 1.40066936e-06
Iter: 1953 loss: 1.40029715e-06
Iter: 1954 loss: 1.40006409e-06
Iter: 1955 loss: 1.3998864e-06
Iter: 1956 loss: 1.39927704e-06
Iter: 1957 loss: 1.39816711e-06
Iter: 1958 loss: 1.4216015e-06
Iter: 1959 loss: 1.39812778e-06
Iter: 1960 loss: 1.39714678e-06
Iter: 1961 loss: 1.40647489e-06
Iter: 1962 loss: 1.39709687e-06
Iter: 1963 loss: 1.39627798e-06
Iter: 1964 loss: 1.39671147e-06
Iter: 1965 loss: 1.39574581e-06
Iter: 1966 loss: 1.39516737e-06
Iter: 1967 loss: 1.40230532e-06
Iter: 1968 loss: 1.39513281e-06
Iter: 1969 loss: 1.39445729e-06
Iter: 1970 loss: 1.39325334e-06
Iter: 1971 loss: 1.39325368e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3/300_300_300_1
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0
+ date
Wed Oct 21 19:19:33 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/500_500_500_500_1 --function f1 --psi 2 --phi 0 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7cac3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7cac5158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7cb57510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7cba0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7cbc4c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7cbc4e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7cbc4bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7ca14ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7ca14950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7ca14d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7c9c2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7ca806a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7ca591e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7c9ec2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7c8c2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7ca80ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7c8552f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7c8559d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7c855bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7c7f0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c7c7f07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59b1b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59b1b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59b1b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59ac2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59ac28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59ac2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59a57840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59a57b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59a57bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59a17620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c3410b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c3410b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59ab3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59ab3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8c59a7a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.008160212
test_loss: 0.0077275266
train_loss: 0.004652089
test_loss: 0.0042939954
train_loss: 0.0035315426
test_loss: 0.003606951
train_loss: 0.0032171025
test_loss: 0.0032657553
train_loss: 0.0032141185
test_loss: 0.0033251324
train_loss: 0.0028411052
test_loss: 0.0029535892
train_loss: 0.0028635338
test_loss: 0.0030111289
train_loss: 0.0026414448
test_loss: 0.0027006324
train_loss: 0.0026766476
test_loss: 0.0026476625
train_loss: 0.0028340889
test_loss: 0.0028686905
train_loss: 0.0025584372
test_loss: 0.002783572
train_loss: 0.0027650136
test_loss: 0.003162273
train_loss: 0.0031143634
test_loss: 0.0030841152
train_loss: 0.0029423076
test_loss: 0.0033943325
train_loss: 0.0025447533
test_loss: 0.002701179
train_loss: 0.0024798072
test_loss: 0.002682514
train_loss: 0.0025679572
test_loss: 0.0026303031
train_loss: 0.0025350556
test_loss: 0.0025091248
train_loss: 0.0023997936
test_loss: 0.002684268
train_loss: 0.0025113747
test_loss: 0.0027297689
train_loss: 0.0023962776
test_loss: 0.0026854132
train_loss: 0.0024818387
test_loss: 0.002725923
train_loss: 0.0026393607
test_loss: 0.0027057587
train_loss: 0.0027425971
test_loss: 0.0027395443
train_loss: 0.002537176
test_loss: 0.0026905285
train_loss: 0.0026300296
test_loss: 0.0028668272
train_loss: 0.0024988307
test_loss: 0.0026210265
train_loss: 0.0027750954
test_loss: 0.0025942835
train_loss: 0.0025048521
test_loss: 0.0025278837
train_loss: 0.002527833
test_loss: 0.0026466704
train_loss: 0.0026385994
test_loss: 0.0027415967
train_loss: 0.0025749584
test_loss: 0.0026001316
train_loss: 0.0024514233
test_loss: 0.0025700335
train_loss: 0.0027648667
test_loss: 0.0030685998
train_loss: 0.0024640234
test_loss: 0.0025796895
train_loss: 0.0025605587
test_loss: 0.002597938
train_loss: 0.0025131437
test_loss: 0.0025036323
train_loss: 0.0026244381
test_loss: 0.0025485586
train_loss: 0.0025288404
test_loss: 0.0024245004
train_loss: 0.0023254456
test_loss: 0.0027021812
train_loss: 0.0024210045
test_loss: 0.0025211072
train_loss: 0.002290126
test_loss: 0.0025207999
train_loss: 0.0025198592
test_loss: 0.002576466
train_loss: 0.0022858356
test_loss: 0.0024460172
train_loss: 0.0021845517
test_loss: 0.0024947482
train_loss: 0.0028062956
test_loss: 0.0024527926
train_loss: 0.0024543023
test_loss: 0.0026097721
train_loss: 0.002307476
test_loss: 0.0028976174
train_loss: 0.002559953
test_loss: 0.00266441
train_loss: 0.0025968116
test_loss: 0.0025752077
train_loss: 0.0024113269
test_loss: 0.0024408072
train_loss: 0.0023935642
test_loss: 0.0024928632
train_loss: 0.0024594534
test_loss: 0.0026137847
train_loss: 0.0026412755
test_loss: 0.0025343644
train_loss: 0.0023821662
test_loss: 0.002327876
train_loss: 0.00239082
test_loss: 0.0023919884
train_loss: 0.0024127455
test_loss: 0.0027572042
train_loss: 0.0024289058
test_loss: 0.0023684783
train_loss: 0.00216966
test_loss: 0.0024637291
train_loss: 0.0023782486
test_loss: 0.0023414576
train_loss: 0.0024095243
test_loss: 0.0026009695
train_loss: 0.0027087545
test_loss: 0.0025105248
train_loss: 0.0024263558
test_loss: 0.0025136743
train_loss: 0.0023426132
test_loss: 0.0027119133
train_loss: 0.0022249054
test_loss: 0.002355886
train_loss: 0.0025826723
test_loss: 0.0026051397
train_loss: 0.0024163136
test_loss: 0.002544276
train_loss: 0.0023913258
test_loss: 0.0024197644
train_loss: 0.0024267859
test_loss: 0.0024084854
train_loss: 0.002167762
test_loss: 0.0023924697
train_loss: 0.0021489074
test_loss: 0.0023723491
train_loss: 0.0022405884
test_loss: 0.002330622
train_loss: 0.002374776
test_loss: 0.002530621
train_loss: 0.0023060702
test_loss: 0.0024018835
train_loss: 0.0022528314
test_loss: 0.0024052607
train_loss: 0.0023865195
test_loss: 0.0024047557
train_loss: 0.0022539056
test_loss: 0.0023045854
train_loss: 0.002295815
test_loss: 0.0025861189
train_loss: 0.0022554933
test_loss: 0.002652105
train_loss: 0.0023371673
test_loss: 0.0025482017
train_loss: 0.0025280474
test_loss: 0.002535129
train_loss: 0.0026031057
test_loss: 0.0023428465
train_loss: 0.0022002559
test_loss: 0.002505242
train_loss: 0.0028112903
test_loss: 0.0026168148
train_loss: 0.0022640156
test_loss: 0.0024377413
train_loss: 0.0023711626
test_loss: 0.0024752768
train_loss: 0.0021204427
test_loss: 0.0023868182
train_loss: 0.002173593
test_loss: 0.0023158714
train_loss: 0.0024260066
test_loss: 0.0024398835
train_loss: 0.0025515
test_loss: 0.0025163211
train_loss: 0.0022104068
test_loss: 0.002375268
train_loss: 0.002303692
test_loss: 0.0026429393
train_loss: 0.002751372
test_loss: 0.0024541412
train_loss: 0.0024822694
test_loss: 0.0028112475
train_loss: 0.0022709402
test_loss: 0.0022875129
train_loss: 0.0024204296
test_loss: 0.0024891659
train_loss: 0.002067634
test_loss: 0.0023625842
train_loss: 0.002248257
test_loss: 0.0023414884
train_loss: 0.0023194428
test_loss: 0.0023643693
train_loss: 0.002292241
test_loss: 0.0026415198
train_loss: 0.0023508666
test_loss: 0.0024173032
train_loss: 0.0023470614
test_loss: 0.002477277
train_loss: 0.0025134375
test_loss: 0.0026981754
train_loss: 0.0022015707
test_loss: 0.0023570166
train_loss: 0.0020707026
test_loss: 0.0024294935
train_loss: 0.002220809
test_loss: 0.0023142828
train_loss: 0.0022638447
test_loss: 0.0024366367
train_loss: 0.0023558761
test_loss: 0.0023716826
train_loss: 0.002065347
test_loss: 0.0021747376
train_loss: 0.0021323105
test_loss: 0.0022731174
train_loss: 0.0025733844
test_loss: 0.0024078467
train_loss: 0.0023614198
test_loss: 0.0024946618
train_loss: 0.0023704204
test_loss: 0.0024561111
train_loss: 0.002182038
test_loss: 0.0023685622
train_loss: 0.002034463
test_loss: 0.0023202952
train_loss: 0.0023366818
test_loss: 0.0024002304
train_loss: 0.0023635216
test_loss: 0.0024236913
train_loss: 0.0023605842
test_loss: 0.0026022082
train_loss: 0.0020773348
test_loss: 0.0023944406
train_loss: 0.002283398
test_loss: 0.0026612356
train_loss: 0.0023667044
test_loss: 0.002741823
train_loss: 0.0022908347
test_loss: 0.002629216
train_loss: 0.0022537226
test_loss: 0.0022555948
train_loss: 0.0021990717
test_loss: 0.0022531461
train_loss: 0.0020841784
test_loss: 0.0023093687
train_loss: 0.0022962384
test_loss: 0.0023739703
train_loss: 0.0022104376
test_loss: 0.0022896894
train_loss: 0.0019652913
test_loss: 0.0022693926
train_loss: 0.0022231073
test_loss: 0.0026039963
train_loss: 0.0020884115
test_loss: 0.0022548006
train_loss: 0.002076201
test_loss: 0.0022393803
train_loss: 0.0020623752
test_loss: 0.0024702547
train_loss: 0.0023975258
test_loss: 0.0023411089
train_loss: 0.0022907497
test_loss: 0.002339806
train_loss: 0.0021767684
test_loss: 0.002535772
train_loss: 0.0019719778
test_loss: 0.0021876753
train_loss: 0.002257221
test_loss: 0.0024169458
train_loss: 0.0021988645
test_loss: 0.002438601
train_loss: 0.0020517544
test_loss: 0.0024737695
train_loss: 0.0022720774
test_loss: 0.002276948
train_loss: 0.0021131816
test_loss: 0.0022502684
train_loss: 0.002169868
test_loss: 0.002221758
train_loss: 0.002157259
test_loss: 0.0024534026
train_loss: 0.0021601086
test_loss: 0.002546487
train_loss: 0.0021107155
test_loss: 0.0023737892
train_loss: 0.002263937
test_loss: 0.0024463509
train_loss: 0.002246586
test_loss: 0.0024263721
train_loss: 0.0022625597
test_loss: 0.0024698498
train_loss: 0.002219391
test_loss: 0.0022732583
train_loss: 0.0021152887
test_loss: 0.00226957
train_loss: 0.001960117
test_loss: 0.0024150482
train_loss: 0.0018777278
test_loss: 0.0021097574
train_loss: 0.0025501663
test_loss: 0.0022480509
train_loss: 0.0022092287
test_loss: 0.0022416995
train_loss: 0.0021160268
test_loss: 0.002270246
train_loss: 0.0021910311
test_loss: 0.0022581639
train_loss: 0.0024018746
test_loss: 0.002348701
train_loss: 0.0020885621
test_loss: 0.0022402355
train_loss: 0.0019778933
test_loss: 0.0022718562
train_loss: 0.0020272587
test_loss: 0.0021593985
train_loss: 0.0020619777
test_loss: 0.0023044758
train_loss: 0.002256969
test_loss: 0.0024673068
train_loss: 0.002078722
test_loss: 0.0023024222
train_loss: 0.00196083
test_loss: 0.002377194
train_loss: 0.0020955643
test_loss: 0.0023400532
train_loss: 0.0020258522
test_loss: 0.002202788
train_loss: 0.002023802
test_loss: 0.0024470293
train_loss: 0.0021832304
test_loss: 0.002616948
train_loss: 0.0021961448
test_loss: 0.0023391806
train_loss: 0.0020323033
test_loss: 0.002187432
train_loss: 0.0019981759
test_loss: 0.0023241963
train_loss: 0.002303481
test_loss: 0.0023807734
train_loss: 0.0021865587
test_loss: 0.002358699
train_loss: 0.0022671106
test_loss: 0.0021411453
train_loss: 0.0020219705
test_loss: 0.0024146948
train_loss: 0.0020676553
test_loss: 0.0025028733
train_loss: 0.002244635
test_loss: 0.0023787364
train_loss: 0.0020835458
test_loss: 0.0021809097
train_loss: 0.0021289734
test_loss: 0.0023645395
train_loss: 0.0020181788
test_loss: 0.0021957015
train_loss: 0.002070453
test_loss: 0.0021958468
train_loss: 0.0018912882
test_loss: 0.0022024235
train_loss: 0.0022434238
test_loss: 0.0022401924
train_loss: 0.0022584014
test_loss: 0.0023369319
train_loss: 0.0021811062
test_loss: 0.0023411259
train_loss: 0.0020896408
test_loss: 0.0022382275
train_loss: 0.0021754769
test_loss: 0.002227922
train_loss: 0.0021088761
test_loss: 0.0022153524
train_loss: 0.0022573965
test_loss: 0.0023005179
train_loss: 0.0023096036
test_loss: 0.002406054
train_loss: 0.0020806731
test_loss: 0.00260699
train_loss: 0.0021338107
test_loss: 0.00225676
train_loss: 0.0019575353
test_loss: 0.002433903
train_loss: 0.001977182
test_loss: 0.0022771808
train_loss: 0.0019671512
test_loss: 0.0023653377
train_loss: 0.0019620005
test_loss: 0.0025595436
train_loss: 0.0020836196
test_loss: 0.002229303
train_loss: 0.0022373637
test_loss: 0.0022001932
train_loss: 0.0021191416
test_loss: 0.002457799
train_loss: 0.0020129774
test_loss: 0.002278615
train_loss: 0.0022078252
test_loss: 0.0023540251
train_loss: 0.0020010564
test_loss: 0.0022929693
train_loss: 0.0019662527
test_loss: 0.0020942406
train_loss: 0.002218093
test_loss: 0.002444479
train_loss: 0.001976025
test_loss: 0.0022617767
train_loss: 0.0019412953
test_loss: 0.0023568878
train_loss: 0.0020879528
test_loss: 0.0020743522
train_loss: 0.002126725
test_loss: 0.0028489474
train_loss: 0.0025312759
test_loss: 0.0023692856
train_loss: 0.002058387
test_loss: 0.0021495183
train_loss: 0.0018770099
test_loss: 0.0021825267
train_loss: 0.0022637341
test_loss: 0.0023765678
train_loss: 0.0019444225
test_loss: 0.002195209
train_loss: 0.0020084928
test_loss: 0.0022218982
train_loss: 0.0020410044
test_loss: 0.002304768
train_loss: 0.0020046416
test_loss: 0.002149889
train_loss: 0.0020227118
test_loss: 0.0025805798
train_loss: 0.0020049873
test_loss: 0.0022602873
train_loss: 0.0019599427
test_loss: 0.002268496
train_loss: 0.0019528808
test_loss: 0.0022304277
train_loss: 0.002127694
test_loss: 0.0022222393
train_loss: 0.001952009
test_loss: 0.0023387992
train_loss: 0.002117948
test_loss: 0.0022250009
train_loss: 0.0019089046
test_loss: 0.0022455805
train_loss: 0.002124861
test_loss: 0.002347094
train_loss: 0.002134315
test_loss: 0.002306253
train_loss: 0.001955519
test_loss: 0.0023471573
train_loss: 0.002036494
test_loss: 0.0020531747
train_loss: 0.0022718692
test_loss: 0.0023581476
train_loss: 0.001986833
test_loss: 0.0022249634
train_loss: 0.0019044662
test_loss: 0.0021858371
train_loss: 0.0020970225
test_loss: 0.0022884463
train_loss: 0.0018401657
test_loss: 0.002145722
train_loss: 0.0019566757
test_loss: 0.0022065316
train_loss: 0.0020778854
test_loss: 0.0022074329
train_loss: 0.0020499416
test_loss: 0.0022946647
train_loss: 0.0019632368
test_loss: 0.002436782
train_loss: 0.0020372684
test_loss: 0.0021529347
train_loss: 0.002130582
test_loss: 0.0023072194
train_loss: 0.0018506163
test_loss: 0.0022791298
train_loss: 0.002161112
test_loss: 0.002409064
train_loss: 0.002165485
test_loss: 0.002328644
train_loss: 0.0020732991
test_loss: 0.0023091729
train_loss: 0.0019739298
test_loss: 0.0022596582
train_loss: 0.002205113
test_loss: 0.002205697
train_loss: 0.0024246834
test_loss: 0.0023438567
train_loss: 0.0020542853
test_loss: 0.0023171888
train_loss: 0.0020278215
test_loss: 0.0024036614
train_loss: 0.0018583785
test_loss: 0.0021388724
train_loss: 0.002049106
test_loss: 0.0022532064
train_loss: 0.0019512484
test_loss: 0.0022277473
train_loss: 0.0019958706
test_loss: 0.0022476725
train_loss: 0.0020486012
test_loss: 0.0022878866
train_loss: 0.0020400723
test_loss: 0.0023111762
train_loss: 0.002030611
test_loss: 0.0023820596
train_loss: 0.002212218
test_loss: 0.0021859428
train_loss: 0.0019612673
test_loss: 0.0025754764
train_loss: 0.0019080273
test_loss: 0.0021604977
train_loss: 0.0019383989
test_loss: 0.0024736498
train_loss: 0.0019014683
test_loss: 0.0022268626
train_loss: 0.0018502164
test_loss: 0.0021616502
train_loss: 0.0017717867
test_loss: 0.0022927725
train_loss: 0.0020255474
test_loss: 0.002428092
train_loss: 0.0019222577
test_loss: 0.0023031204
train_loss: 0.0019729862
test_loss: 0.0021813244
train_loss: 0.0019405512
test_loss: 0.0021850395
train_loss: 0.0023419517
test_loss: 0.0027838354
train_loss: 0.0019985233
test_loss: 0.002533737
train_loss: 0.0022177058
test_loss: 0.002523619
train_loss: 0.0022350769
test_loss: 0.0023265912
train_loss: 0.0019264566
test_loss: 0.002193887
train_loss: 0.0018622191
test_loss: 0.0020757774
train_loss: 0.0020306867
test_loss: 0.0021616253
train_loss: 0.0017792584
test_loss: 0.00225521
train_loss: 0.0018281572
test_loss: 0.002432353
train_loss: 0.0019563641
test_loss: 0.0022380145
train_loss: 0.0020789653
test_loss: 0.0023656168
train_loss: 0.0018178441
test_loss: 0.0022344203
train_loss: 0.0018848699
test_loss: 0.0022313015
train_loss: 0.0019455893
test_loss: 0.00219368
train_loss: 0.002119313
test_loss: 0.0021451379
train_loss: 0.0018498355
test_loss: 0.0022108196
train_loss: 0.0019385596
test_loss: 0.0021953066
train_loss: 0.0018847398
test_loss: 0.0021230693
train_loss: 0.0021451146
test_loss: 0.0021563848
train_loss: 0.002214327
test_loss: 0.0022279867
train_loss: 0.0020045722
test_loss: 0.0023102309
train_loss: 0.0020693147
test_loss: 0.0022333039
train_loss: 0.0019506477
test_loss: 0.0022945593
train_loss: 0.0018461111
test_loss: 0.0021907818
train_loss: 0.0018789456
test_loss: 0.0022212684
train_loss: 0.0019297369
test_loss: 0.0021154976
train_loss: 0.0019810377
test_loss: 0.002093549
train_loss: 0.0020323305
test_loss: 0.0021655094
train_loss: 0.0020197567
test_loss: 0.0022983078
train_loss: 0.0019979833
test_loss: 0.0023375496
train_loss: 0.0020127997
test_loss: 0.0021933438
train_loss: 0.0019419295
test_loss: 0.0023060448
train_loss: 0.0020315384
test_loss: 0.002327997
train_loss: 0.0020134281
test_loss: 0.0022677565
train_loss: 0.0020331962
test_loss: 0.0021198275
train_loss: 0.00208045
test_loss: 0.0022224912
train_loss: 0.0018717111
test_loss: 0.002094915
train_loss: 0.0019613695
test_loss: 0.0022333746
train_loss: 0.0019336105
test_loss: 0.002203215
train_loss: 0.0020252503
test_loss: 0.0022212286
train_loss: 0.0019881125
test_loss: 0.0024797542
train_loss: 0.0019725896
test_loss: 0.0023318797
train_loss: 0.0018969113
test_loss: 0.0022478167
train_loss: 0.001830935
test_loss: 0.0021032693
train_loss: 0.0021450594
test_loss: 0.0024736102
train_loss: 0.002026506
test_loss: 0.0021359713
train_loss: 0.0020726959
test_loss: 0.0024533516
train_loss: 0.002076074
test_loss: 0.0021335443
train_loss: 0.001936351
test_loss: 0.0023104998
train_loss: 0.0019612398
test_loss: 0.0021018013
train_loss: 0.0018597936
test_loss: 0.002103638
train_loss: 0.0018468504
test_loss: 0.0021099849
train_loss: 0.0018428515
test_loss: 0.002128346
train_loss: 0.0017884248
test_loss: 0.002326432
train_loss: 0.0019080582
test_loss: 0.002360315
train_loss: 0.0019681817
test_loss: 0.002292347
train_loss: 0.001908258
test_loss: 0.0021037117
train_loss: 0.0019590699
test_loss: 0.0024353848
train_loss: 0.001921068
test_loss: 0.0023809948
train_loss: 0.00189857
test_loss: 0.002248118
train_loss: 0.0018521636
test_loss: 0.0021955483
train_loss: 0.0021267459
test_loss: 0.0022232684
train_loss: 0.0018404403
test_loss: 0.0021144312
train_loss: 0.0019443207
test_loss: 0.0023668502
train_loss: 0.0017611099
test_loss: 0.002192328
train_loss: 0.0017118548
test_loss: 0.0021636933
train_loss: 0.0021646053
test_loss: 0.0024585898
train_loss: 0.0022604363
test_loss: 0.0021800965
train_loss: 0.002013918
test_loss: 0.0020624027
train_loss: 0.0019902193
test_loss: 0.0022199808
train_loss: 0.00186555
test_loss: 0.0021109025
train_loss: 0.0020753953
test_loss: 0.0021844152/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.0019840044
test_loss: 0.0021998698
train_loss: 0.0020549241
test_loss: 0.002203787
train_loss: 0.0020581014
test_loss: 0.0022698068
train_loss: 0.0018141997
test_loss: 0.0024344297
train_loss: 0.0021292688
test_loss: 0.0023187096
train_loss: 0.0020679343
test_loss: 0.0024421338
train_loss: 0.0021589096
test_loss: 0.002067979
train_loss: 0.0018511434
test_loss: 0.0021340728
train_loss: 0.001899987
test_loss: 0.002071939
train_loss: 0.0019550927
test_loss: 0.0025301473
train_loss: 0.0021266905
test_loss: 0.0022394632
train_loss: 0.0018214872
test_loss: 0.0021976503
train_loss: 0.001994082
test_loss: 0.0022237194
train_loss: 0.0021992268
test_loss: 0.0023432414
train_loss: 0.0018355367
test_loss: 0.002249341
train_loss: 0.0019340621
test_loss: 0.0021045988
train_loss: 0.001768533
test_loss: 0.0020860862
train_loss: 0.0020137825
test_loss: 0.0022441691
train_loss: 0.0021094824
test_loss: 0.002108392
train_loss: 0.0020672707
test_loss: 0.0021455316
train_loss: 0.0019251243
test_loss: 0.0024119194
train_loss: 0.0018354976
test_loss: 0.0022251084
train_loss: 0.0018672841
test_loss: 0.0022427335
train_loss: 0.0020074719
test_loss: 0.002288348
train_loss: 0.0019098616
test_loss: 0.0021018826
train_loss: 0.0019421913
test_loss: 0.0022813387
train_loss: 0.0018718296
test_loss: 0.0021483065
train_loss: 0.001792832
test_loss: 0.002064984
train_loss: 0.0018821687
test_loss: 0.0022055742
train_loss: 0.0019254925
test_loss: 0.0020973135
train_loss: 0.0018380119
test_loss: 0.0023030806
train_loss: 0.0019742213
test_loss: 0.0022103565
train_loss: 0.0018301231
test_loss: 0.0022599644
train_loss: 0.0019012948
test_loss: 0.0020985405
train_loss: 0.0019649023
test_loss: 0.0023068178
train_loss: 0.0019460144
test_loss: 0.002131857
train_loss: 0.0022457729
test_loss: 0.0021551594
train_loss: 0.0019464865
test_loss: 0.002207742
train_loss: 0.0021303669
test_loss: 0.0024715757
train_loss: 0.0019794405
test_loss: 0.0021849382
train_loss: 0.0020206755
test_loss: 0.002113949
train_loss: 0.0016862097
test_loss: 0.0020352611
train_loss: 0.0020897507
test_loss: 0.0020847437
train_loss: 0.0017452086
test_loss: 0.0022148807
train_loss: 0.0017410205
test_loss: 0.0020024553
train_loss: 0.001881812
test_loss: 0.0021966877
train_loss: 0.0016967161
test_loss: 0.0020673326
train_loss: 0.0018436783
test_loss: 0.0021624125
train_loss: 0.0018194527
test_loss: 0.0022328077
train_loss: 0.0020855498
test_loss: 0.002157976
train_loss: 0.0020096083
test_loss: 0.0022124355
train_loss: 0.0018718184
test_loss: 0.0021485125
train_loss: 0.0019875932
test_loss: 0.0022441535
train_loss: 0.001977559
test_loss: 0.0023410078
train_loss: 0.0018675523
test_loss: 0.002205808
train_loss: 0.0019005708
test_loss: 0.0021270486
train_loss: 0.0019408822
test_loss: 0.0020989547
train_loss: 0.0018468703
test_loss: 0.0020674886
train_loss: 0.0020678204
test_loss: 0.0023115047
train_loss: 0.0018521824
test_loss: 0.0022185135
train_loss: 0.0018322314
test_loss: 0.002096814
train_loss: 0.0019404392
test_loss: 0.0021387844
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 2 --phi 0 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddff453620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddff4537b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddff453e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddff4536a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddff4f8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddff488730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddff4ae598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddff4aec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddff4061e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddbcefca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddff406d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddbced8bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddbceed950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddbceeda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddbce64ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddbce38b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd986fe598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fddbce64598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd986de268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd986ded90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd986a26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd9865dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd985fea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd98626e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd9862c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd985bf6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd98574a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd98574e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd9853a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd98545ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd98545730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd984bd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd984cff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd9848cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd9848c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdd9848cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.59899308e-06
Iter: 2 loss: 4.62970957e-06
Iter: 3 loss: 4.50312382e-06
Iter: 4 loss: 4.12202462e-06
Iter: 5 loss: 3.92228867e-06
Iter: 6 loss: 3.74777915e-06
Iter: 7 loss: 3.49491984e-06
Iter: 8 loss: 3.84875e-06
Iter: 9 loss: 3.37021152e-06
Iter: 10 loss: 3.12752763e-06
Iter: 11 loss: 3.12681368e-06
Iter: 12 loss: 3.05648518e-06
Iter: 13 loss: 3.06095217e-06
Iter: 14 loss: 3.0016738e-06
Iter: 15 loss: 2.92704817e-06
Iter: 16 loss: 3.2181706e-06
Iter: 17 loss: 2.90986259e-06
Iter: 18 loss: 2.80801123e-06
Iter: 19 loss: 3.00066313e-06
Iter: 20 loss: 2.76513038e-06
Iter: 21 loss: 2.69377688e-06
Iter: 22 loss: 2.60381034e-06
Iter: 23 loss: 2.59676244e-06
Iter: 24 loss: 2.46078298e-06
Iter: 25 loss: 4.33322384e-06
Iter: 26 loss: 2.46035665e-06
Iter: 27 loss: 2.37413883e-06
Iter: 28 loss: 2.25789427e-06
Iter: 29 loss: 2.25187273e-06
Iter: 30 loss: 2.17771867e-06
Iter: 31 loss: 2.19048843e-06
Iter: 32 loss: 2.12207283e-06
Iter: 33 loss: 2.06154277e-06
Iter: 34 loss: 2.96138387e-06
Iter: 35 loss: 2.06146979e-06
Iter: 36 loss: 2.03455465e-06
Iter: 37 loss: 2.03195759e-06
Iter: 38 loss: 2.00741488e-06
Iter: 39 loss: 1.94881864e-06
Iter: 40 loss: 2.60246179e-06
Iter: 41 loss: 1.94297877e-06
Iter: 42 loss: 1.90419348e-06
Iter: 43 loss: 2.28900581e-06
Iter: 44 loss: 1.90287801e-06
Iter: 45 loss: 1.85474244e-06
Iter: 46 loss: 1.92962807e-06
Iter: 47 loss: 1.83207646e-06
Iter: 48 loss: 1.7886498e-06
Iter: 49 loss: 1.81185965e-06
Iter: 50 loss: 1.75988009e-06
Iter: 51 loss: 1.74896763e-06
Iter: 52 loss: 1.74234253e-06
Iter: 53 loss: 1.72744092e-06
Iter: 54 loss: 1.69178566e-06
Iter: 55 loss: 2.08446181e-06
Iter: 56 loss: 1.68821089e-06
Iter: 57 loss: 1.63334823e-06
Iter: 58 loss: 1.66429959e-06
Iter: 59 loss: 1.59756041e-06
Iter: 60 loss: 1.56524538e-06
Iter: 61 loss: 1.55614043e-06
Iter: 62 loss: 1.53806195e-06
Iter: 63 loss: 1.4825556e-06
Iter: 64 loss: 1.60429045e-06
Iter: 65 loss: 1.44906346e-06
Iter: 66 loss: 1.39898839e-06
Iter: 67 loss: 1.39895451e-06
Iter: 68 loss: 1.37039956e-06
Iter: 69 loss: 1.47915785e-06
Iter: 70 loss: 1.36357585e-06
Iter: 71 loss: 1.35065829e-06
Iter: 72 loss: 1.34675201e-06
Iter: 73 loss: 1.34172546e-06
Iter: 74 loss: 1.32741263e-06
Iter: 75 loss: 1.39528947e-06
Iter: 76 loss: 1.32242326e-06
Iter: 77 loss: 1.3005025e-06
Iter: 78 loss: 1.53373526e-06
Iter: 79 loss: 1.29998466e-06
Iter: 80 loss: 1.27903013e-06
Iter: 81 loss: 1.24876522e-06
Iter: 82 loss: 1.24784947e-06
Iter: 83 loss: 1.23062614e-06
Iter: 84 loss: 1.23027701e-06
Iter: 85 loss: 1.2141021e-06
Iter: 86 loss: 1.18209527e-06
Iter: 87 loss: 1.80565598e-06
Iter: 88 loss: 1.18180424e-06
Iter: 89 loss: 1.15792602e-06
Iter: 90 loss: 1.27985254e-06
Iter: 91 loss: 1.15405817e-06
Iter: 92 loss: 1.14089653e-06
Iter: 93 loss: 1.28326428e-06
Iter: 94 loss: 1.14056593e-06
Iter: 95 loss: 1.12766634e-06
Iter: 96 loss: 1.16322531e-06
Iter: 97 loss: 1.12350426e-06
Iter: 98 loss: 1.11639986e-06
Iter: 99 loss: 1.10600035e-06
Iter: 100 loss: 1.10568249e-06
Iter: 101 loss: 1.08793097e-06
Iter: 102 loss: 1.07001574e-06
Iter: 103 loss: 1.06646712e-06
Iter: 104 loss: 1.07212179e-06
Iter: 105 loss: 1.05931474e-06
Iter: 106 loss: 1.05096024e-06
Iter: 107 loss: 1.04307151e-06
Iter: 108 loss: 1.0411618e-06
Iter: 109 loss: 1.0295638e-06
Iter: 110 loss: 1.03856144e-06
Iter: 111 loss: 1.02251022e-06
Iter: 112 loss: 1.01469846e-06
Iter: 113 loss: 1.01369267e-06
Iter: 114 loss: 1.0084151e-06
Iter: 115 loss: 9.93753702e-07
Iter: 116 loss: 1.07152027e-06
Iter: 117 loss: 9.89145747e-07
Iter: 118 loss: 9.8472583e-07
Iter: 119 loss: 9.78847311e-07
Iter: 120 loss: 9.73635565e-07
Iter: 121 loss: 9.60595571e-07
Iter: 122 loss: 1.08493305e-06
Iter: 123 loss: 9.58832516e-07
Iter: 124 loss: 9.42127315e-07
Iter: 125 loss: 9.92253831e-07
Iter: 126 loss: 9.3707007e-07
Iter: 127 loss: 9.42328597e-07
Iter: 128 loss: 9.338371e-07
Iter: 129 loss: 9.31660963e-07
Iter: 130 loss: 9.26668235e-07
Iter: 131 loss: 9.90549324e-07
Iter: 132 loss: 9.26308587e-07
Iter: 133 loss: 9.19344302e-07
Iter: 134 loss: 9.1572565e-07
Iter: 135 loss: 9.12505129e-07
Iter: 136 loss: 9.02794341e-07
Iter: 137 loss: 9.36246693e-07
Iter: 138 loss: 9.00210807e-07
Iter: 139 loss: 8.89195803e-07
Iter: 140 loss: 9.56769213e-07
Iter: 141 loss: 8.87851286e-07
Iter: 142 loss: 8.76083277e-07
Iter: 143 loss: 9.45078682e-07
Iter: 144 loss: 8.74529348e-07
Iter: 145 loss: 8.68053462e-07
Iter: 146 loss: 8.53240067e-07
Iter: 147 loss: 1.04895639e-06
Iter: 148 loss: 8.52297262e-07
Iter: 149 loss: 8.56750262e-07
Iter: 150 loss: 8.46021123e-07
Iter: 151 loss: 8.43087321e-07
Iter: 152 loss: 8.39068605e-07
Iter: 153 loss: 8.38892106e-07
Iter: 154 loss: 8.35859169e-07
Iter: 155 loss: 8.35830122e-07
Iter: 156 loss: 8.32675596e-07
Iter: 157 loss: 8.26055953e-07
Iter: 158 loss: 9.39114443e-07
Iter: 159 loss: 8.25908046e-07
Iter: 160 loss: 8.21199933e-07
Iter: 161 loss: 8.50479466e-07
Iter: 162 loss: 8.20695732e-07
Iter: 163 loss: 8.1499843e-07
Iter: 164 loss: 8.34116577e-07
Iter: 165 loss: 8.13495149e-07
Iter: 166 loss: 8.08154709e-07
Iter: 167 loss: 8.01670467e-07
Iter: 168 loss: 8.01058093e-07
Iter: 169 loss: 7.93380138e-07
Iter: 170 loss: 7.95848734e-07
Iter: 171 loss: 7.87951933e-07
Iter: 172 loss: 7.80516302e-07
Iter: 173 loss: 8.90280887e-07
Iter: 174 loss: 7.80488335e-07
Iter: 175 loss: 7.78500521e-07
Iter: 176 loss: 7.77985065e-07
Iter: 177 loss: 7.75898911e-07
Iter: 178 loss: 7.70182851e-07
Iter: 179 loss: 8.01197757e-07
Iter: 180 loss: 7.68360565e-07
Iter: 181 loss: 7.62926277e-07
Iter: 182 loss: 7.62902403e-07
Iter: 183 loss: 7.58783813e-07
Iter: 184 loss: 7.89079252e-07
Iter: 185 loss: 7.58471515e-07
Iter: 186 loss: 7.56523491e-07
Iter: 187 loss: 7.53360837e-07
Iter: 188 loss: 7.53362542e-07
Iter: 189 loss: 7.48997536e-07
Iter: 190 loss: 8.03489911e-07
Iter: 191 loss: 7.48985656e-07
Iter: 192 loss: 7.47196736e-07
Iter: 193 loss: 7.43366854e-07
Iter: 194 loss: 8.00776888e-07
Iter: 195 loss: 7.43171597e-07
Iter: 196 loss: 7.40860344e-07
Iter: 197 loss: 7.40415089e-07
Iter: 198 loss: 7.38057793e-07
Iter: 199 loss: 7.34744276e-07
Iter: 200 loss: 7.34588298e-07
Iter: 201 loss: 7.30608463e-07
Iter: 202 loss: 7.24194479e-07
Iter: 203 loss: 7.24177937e-07
Iter: 204 loss: 7.15445253e-07
Iter: 205 loss: 7.81598601e-07
Iter: 206 loss: 7.14828047e-07
Iter: 207 loss: 7.1138038e-07
Iter: 208 loss: 7.58803594e-07
Iter: 209 loss: 7.11336725e-07
Iter: 210 loss: 7.07751497e-07
Iter: 211 loss: 7.1576585e-07
Iter: 212 loss: 7.06405558e-07
Iter: 213 loss: 7.04499712e-07
Iter: 214 loss: 7.04317131e-07
Iter: 215 loss: 7.02922e-07
Iter: 216 loss: 7.01360818e-07
Iter: 217 loss: 7.0130659e-07
Iter: 218 loss: 6.99689849e-07
Iter: 219 loss: 6.96161123e-07
Iter: 220 loss: 7.43953365e-07
Iter: 221 loss: 6.95930794e-07
Iter: 222 loss: 6.95067911e-07
Iter: 223 loss: 6.94458663e-07
Iter: 224 loss: 6.92855508e-07
Iter: 225 loss: 6.88266482e-07
Iter: 226 loss: 7.08672133e-07
Iter: 227 loss: 6.86569194e-07
Iter: 228 loss: 6.812636e-07
Iter: 229 loss: 7.45037539e-07
Iter: 230 loss: 6.81192091e-07
Iter: 231 loss: 6.76995171e-07
Iter: 232 loss: 7.1528541e-07
Iter: 233 loss: 6.76804348e-07
Iter: 234 loss: 6.74891908e-07
Iter: 235 loss: 6.70910538e-07
Iter: 236 loss: 7.41220902e-07
Iter: 237 loss: 6.70864893e-07
Iter: 238 loss: 6.685828e-07
Iter: 239 loss: 6.68576945e-07
Iter: 240 loss: 6.66988512e-07
Iter: 241 loss: 6.68989571e-07
Iter: 242 loss: 6.6618054e-07
Iter: 243 loss: 6.63916126e-07
Iter: 244 loss: 6.81411166e-07
Iter: 245 loss: 6.63730361e-07
Iter: 246 loss: 6.62373282e-07
Iter: 247 loss: 6.60567594e-07
Iter: 248 loss: 6.6049472e-07
Iter: 249 loss: 6.58595e-07
Iter: 250 loss: 6.79960863e-07
Iter: 251 loss: 6.58541921e-07
Iter: 252 loss: 6.56412226e-07
Iter: 253 loss: 6.54331757e-07
Iter: 254 loss: 6.53853078e-07
Iter: 255 loss: 6.51880384e-07
Iter: 256 loss: 6.7319678e-07
Iter: 257 loss: 6.51804044e-07
Iter: 258 loss: 6.49707317e-07
Iter: 259 loss: 6.49676906e-07
Iter: 260 loss: 6.48047717e-07
Iter: 261 loss: 6.45940133e-07
Iter: 262 loss: 6.46921251e-07
Iter: 263 loss: 6.44534396e-07
Iter: 264 loss: 6.42722284e-07
Iter: 265 loss: 6.42667828e-07
Iter: 266 loss: 6.41235374e-07
Iter: 267 loss: 6.37314e-07
Iter: 268 loss: 6.60129558e-07
Iter: 269 loss: 6.36187337e-07
Iter: 270 loss: 6.32614785e-07
Iter: 271 loss: 6.71946395e-07
Iter: 272 loss: 6.32548335e-07
Iter: 273 loss: 6.29984697e-07
Iter: 274 loss: 6.34032574e-07
Iter: 275 loss: 6.28801843e-07
Iter: 276 loss: 6.26859958e-07
Iter: 277 loss: 6.26752922e-07
Iter: 278 loss: 6.25739744e-07
Iter: 279 loss: 6.25105713e-07
Iter: 280 loss: 6.24698032e-07
Iter: 281 loss: 6.23370681e-07
Iter: 282 loss: 6.21970855e-07
Iter: 283 loss: 6.21763547e-07
Iter: 284 loss: 6.19917216e-07
Iter: 285 loss: 6.19785453e-07
Iter: 286 loss: 6.18766592e-07
Iter: 287 loss: 6.1674433e-07
Iter: 288 loss: 6.5738277e-07
Iter: 289 loss: 6.16699651e-07
Iter: 290 loss: 6.13433599e-07
Iter: 291 loss: 6.28820715e-07
Iter: 292 loss: 6.1283589e-07
Iter: 293 loss: 6.1003044e-07
Iter: 294 loss: 6.06614094e-07
Iter: 295 loss: 6.06269396e-07
Iter: 296 loss: 6.05166576e-07
Iter: 297 loss: 6.0467255e-07
Iter: 298 loss: 6.03110436e-07
Iter: 299 loss: 6.01789793e-07
Iter: 300 loss: 6.01345562e-07
Iter: 301 loss: 5.99737746e-07
Iter: 302 loss: 6.03250783e-07
Iter: 303 loss: 5.99108887e-07
Iter: 304 loss: 5.97954227e-07
Iter: 305 loss: 5.98870315e-07
Iter: 306 loss: 5.97284725e-07
Iter: 307 loss: 5.96316568e-07
Iter: 308 loss: 5.96250629e-07
Iter: 309 loss: 5.9535688e-07
Iter: 310 loss: 5.94286917e-07
Iter: 311 loss: 5.9419051e-07
Iter: 312 loss: 5.92561264e-07
Iter: 313 loss: 5.90012462e-07
Iter: 314 loss: 5.90009e-07
Iter: 315 loss: 5.88964895e-07
Iter: 316 loss: 5.88254807e-07
Iter: 317 loss: 5.86815077e-07
Iter: 318 loss: 5.86354e-07
Iter: 319 loss: 5.85497787e-07
Iter: 320 loss: 5.84164241e-07
Iter: 321 loss: 5.98984286e-07
Iter: 322 loss: 5.8415111e-07
Iter: 323 loss: 5.82868552e-07
Iter: 324 loss: 5.80883125e-07
Iter: 325 loss: 5.80854305e-07
Iter: 326 loss: 5.79294465e-07
Iter: 327 loss: 5.83737346e-07
Iter: 328 loss: 5.78781055e-07
Iter: 329 loss: 5.76528e-07
Iter: 330 loss: 5.87271529e-07
Iter: 331 loss: 5.76159096e-07
Iter: 332 loss: 5.74776777e-07
Iter: 333 loss: 5.74609146e-07
Iter: 334 loss: 5.7361882e-07
Iter: 335 loss: 5.72255658e-07
Iter: 336 loss: 5.71349176e-07
Iter: 337 loss: 5.70829911e-07
Iter: 338 loss: 5.70151542e-07
Iter: 339 loss: 5.69721465e-07
Iter: 340 loss: 5.68773942e-07
Iter: 341 loss: 5.71002943e-07
Iter: 342 loss: 5.68410712e-07
Iter: 343 loss: 5.67550671e-07
Iter: 344 loss: 5.65469236e-07
Iter: 345 loss: 5.86077817e-07
Iter: 346 loss: 5.65228106e-07
Iter: 347 loss: 5.63597041e-07
Iter: 348 loss: 5.63542e-07
Iter: 349 loss: 5.61854677e-07
Iter: 350 loss: 5.64954917e-07
Iter: 351 loss: 5.61060062e-07
Iter: 352 loss: 5.59481464e-07
Iter: 353 loss: 5.61858087e-07
Iter: 354 loss: 5.58651664e-07
Iter: 355 loss: 5.56543e-07
Iter: 356 loss: 5.68176347e-07
Iter: 357 loss: 5.56234511e-07
Iter: 358 loss: 5.55377653e-07
Iter: 359 loss: 5.53962423e-07
Iter: 360 loss: 5.53975156e-07
Iter: 361 loss: 5.53624432e-07
Iter: 362 loss: 5.53129439e-07
Iter: 363 loss: 5.52683503e-07
Iter: 364 loss: 5.520223e-07
Iter: 365 loss: 5.51992457e-07
Iter: 366 loss: 5.51153448e-07
Iter: 367 loss: 5.49722358e-07
Iter: 368 loss: 5.49720141e-07
Iter: 369 loss: 5.48255116e-07
Iter: 370 loss: 5.48262847e-07
Iter: 371 loss: 5.46885815e-07
Iter: 372 loss: 5.53711061e-07
Iter: 373 loss: 5.46627575e-07
Iter: 374 loss: 5.45261742e-07
Iter: 375 loss: 5.42359032e-07
Iter: 376 loss: 5.87189675e-07
Iter: 377 loss: 5.42262114e-07
Iter: 378 loss: 5.40211545e-07
Iter: 379 loss: 5.69666554e-07
Iter: 380 loss: 5.40188637e-07
Iter: 381 loss: 5.38818881e-07
Iter: 382 loss: 5.5728492e-07
Iter: 383 loss: 5.38834286e-07
Iter: 384 loss: 5.3795776e-07
Iter: 385 loss: 5.37174799e-07
Iter: 386 loss: 5.36952086e-07
Iter: 387 loss: 5.35897129e-07
Iter: 388 loss: 5.35856657e-07
Iter: 389 loss: 5.35373431e-07
Iter: 390 loss: 5.3413396e-07
Iter: 391 loss: 5.47111767e-07
Iter: 392 loss: 5.3397514e-07
Iter: 393 loss: 5.33164211e-07
Iter: 394 loss: 5.33127377e-07
Iter: 395 loss: 5.32259492e-07
Iter: 396 loss: 5.32113631e-07
Iter: 397 loss: 5.31506373e-07
Iter: 398 loss: 5.30525199e-07
Iter: 399 loss: 5.28611281e-07
Iter: 400 loss: 5.66356221e-07
Iter: 401 loss: 5.28580131e-07
Iter: 402 loss: 5.27157795e-07
Iter: 403 loss: 5.27149552e-07
Iter: 404 loss: 5.26315489e-07
Iter: 405 loss: 5.26315489e-07
Iter: 406 loss: 5.25607163e-07
Iter: 407 loss: 5.24035386e-07
Iter: 408 loss: 5.47483182e-07
Iter: 409 loss: 5.23954895e-07
Iter: 410 loss: 5.22418247e-07
Iter: 411 loss: 5.26915414e-07
Iter: 412 loss: 5.21984703e-07
Iter: 413 loss: 5.20616766e-07
Iter: 414 loss: 5.39620601e-07
Iter: 415 loss: 5.20619096e-07
Iter: 416 loss: 5.19299533e-07
Iter: 417 loss: 5.18240881e-07
Iter: 418 loss: 5.17824276e-07
Iter: 419 loss: 5.17157048e-07
Iter: 420 loss: 5.17005731e-07
Iter: 421 loss: 5.16443833e-07
Iter: 422 loss: 5.15198849e-07
Iter: 423 loss: 5.31553e-07
Iter: 424 loss: 5.15134445e-07
Iter: 425 loss: 5.14402529e-07
Iter: 426 loss: 5.14380588e-07
Iter: 427 loss: 5.13811415e-07
Iter: 428 loss: 5.16929276e-07
Iter: 429 loss: 5.13705345e-07
Iter: 430 loss: 5.13306e-07
Iter: 431 loss: 5.12238159e-07
Iter: 432 loss: 5.19889511e-07
Iter: 433 loss: 5.12010388e-07
Iter: 434 loss: 5.10611812e-07
Iter: 435 loss: 5.12290626e-07
Iter: 436 loss: 5.09855795e-07
Iter: 437 loss: 5.08467338e-07
Iter: 438 loss: 5.08391167e-07
Iter: 439 loss: 5.07037385e-07
Iter: 440 loss: 5.07146183e-07
Iter: 441 loss: 5.05967364e-07
Iter: 442 loss: 5.04827085e-07
Iter: 443 loss: 5.04726245e-07
Iter: 444 loss: 5.03827152e-07
Iter: 445 loss: 5.02912144e-07
Iter: 446 loss: 5.02863486e-07
Iter: 447 loss: 5.02096896e-07
Iter: 448 loss: 5.04236709e-07
Iter: 449 loss: 5.01852753e-07
Iter: 450 loss: 5.01372085e-07
Iter: 451 loss: 5.03208867e-07
Iter: 452 loss: 5.01267834e-07
Iter: 453 loss: 5.00639828e-07
Iter: 454 loss: 5.00003921e-07
Iter: 455 loss: 4.99879206e-07
Iter: 456 loss: 4.99166163e-07
Iter: 457 loss: 5.00096917e-07
Iter: 458 loss: 4.98780878e-07
Iter: 459 loss: 4.97870587e-07
Iter: 460 loss: 5.08197274e-07
Iter: 461 loss: 4.97864505e-07
Iter: 462 loss: 4.97145265e-07
Iter: 463 loss: 4.95694621e-07
Iter: 464 loss: 5.20857611e-07
Iter: 465 loss: 4.95638687e-07
Iter: 466 loss: 4.9441536e-07
Iter: 467 loss: 4.96019084e-07
Iter: 468 loss: 4.93770131e-07
Iter: 469 loss: 4.9266805e-07
Iter: 470 loss: 5.09933329e-07
Iter: 471 loss: 4.92656795e-07
Iter: 472 loss: 4.91629123e-07
Iter: 473 loss: 4.97838414e-07
Iter: 474 loss: 4.91471781e-07
Iter: 475 loss: 4.90818593e-07
Iter: 476 loss: 4.89462309e-07
Iter: 477 loss: 5.10837879e-07
Iter: 478 loss: 4.89414845e-07
Iter: 479 loss: 4.88233638e-07
Iter: 480 loss: 5.06984861e-07
Iter: 481 loss: 4.8824171e-07
Iter: 482 loss: 4.87422653e-07
Iter: 483 loss: 4.96444045e-07
Iter: 484 loss: 4.8742595e-07
Iter: 485 loss: 4.86974216e-07
Iter: 486 loss: 4.86625879e-07
Iter: 487 loss: 4.86470299e-07
Iter: 488 loss: 4.85653061e-07
Iter: 489 loss: 4.91571427e-07
Iter: 490 loss: 4.85607757e-07
Iter: 491 loss: 4.85255669e-07
Iter: 492 loss: 4.84729583e-07
Iter: 493 loss: 4.84719635e-07
Iter: 494 loss: 4.84229759e-07
Iter: 495 loss: 4.8419929e-07
Iter: 496 loss: 4.83715496e-07
Iter: 497 loss: 4.82865801e-07
Iter: 498 loss: 4.82857672e-07
Iter: 499 loss: 4.81882864e-07
Iter: 500 loss: 4.7993592e-07
Iter: 501 loss: 5.17434103e-07
Iter: 502 loss: 4.79913069e-07
Iter: 503 loss: 4.78021093e-07
Iter: 504 loss: 5.06835192e-07
Iter: 505 loss: 4.78039283e-07
Iter: 506 loss: 4.77245408e-07
Iter: 507 loss: 4.77220908e-07
Iter: 508 loss: 4.76546404e-07
Iter: 509 loss: 4.75418261e-07
Iter: 510 loss: 4.7540226e-07
Iter: 511 loss: 4.74548216e-07
Iter: 512 loss: 4.82830671e-07
Iter: 513 loss: 4.74528179e-07
Iter: 514 loss: 4.74092616e-07
Iter: 515 loss: 4.78095558e-07
Iter: 516 loss: 4.74065985e-07
Iter: 517 loss: 4.73645628e-07
Iter: 518 loss: 4.73260684e-07
Iter: 519 loss: 4.7313884e-07
Iter: 520 loss: 4.72576517e-07
Iter: 521 loss: 4.800944e-07
Iter: 522 loss: 4.72566569e-07
Iter: 523 loss: 4.72189072e-07
Iter: 524 loss: 4.71426887e-07
Iter: 525 loss: 4.84469808e-07
Iter: 526 loss: 4.71392582e-07
Iter: 527 loss: 4.70472742e-07
Iter: 528 loss: 4.75074131e-07
Iter: 529 loss: 4.7034456e-07
Iter: 530 loss: 4.6917296e-07
Iter: 531 loss: 4.73330914e-07
Iter: 532 loss: 4.68909661e-07
Iter: 533 loss: 4.68328778e-07
Iter: 534 loss: 4.67370256e-07
Iter: 535 loss: 4.67385746e-07
Iter: 536 loss: 4.66308336e-07
Iter: 537 loss: 4.72129557e-07
Iter: 538 loss: 4.66117314e-07
Iter: 539 loss: 4.65545071e-07
Iter: 540 loss: 4.65541689e-07
Iter: 541 loss: 4.64898307e-07
Iter: 542 loss: 4.6403963e-07
Iter: 543 loss: 4.63985373e-07
Iter: 544 loss: 4.6319974e-07
Iter: 545 loss: 4.67941447e-07
Iter: 546 loss: 4.6310123e-07
Iter: 547 loss: 4.62528817e-07
Iter: 548 loss: 4.66508197e-07
Iter: 549 loss: 4.62477715e-07
Iter: 550 loss: 4.61897173e-07
Iter: 551 loss: 4.62005204e-07
Iter: 552 loss: 4.61467295e-07
Iter: 553 loss: 4.60963179e-07
Iter: 554 loss: 4.60953373e-07
Iter: 555 loss: 4.60644856e-07
Iter: 556 loss: 4.60333126e-07
Iter: 557 loss: 4.60263493e-07
Iter: 558 loss: 4.5977518e-07
Iter: 559 loss: 4.59670019e-07
Iter: 560 loss: 4.59389526e-07
Iter: 561 loss: 4.58719398e-07
Iter: 562 loss: 4.69488782e-07
Iter: 563 loss: 4.58712577e-07
Iter: 564 loss: 4.58346705e-07
Iter: 565 loss: 4.57298796e-07
Iter: 566 loss: 4.63126668e-07
Iter: 567 loss: 4.57009889e-07
Iter: 568 loss: 4.55433337e-07
Iter: 569 loss: 4.59899383e-07
Iter: 570 loss: 4.54939965e-07
Iter: 571 loss: 4.54200233e-07
Iter: 572 loss: 4.54111699e-07
Iter: 573 loss: 4.53279029e-07
Iter: 574 loss: 4.54191849e-07
Iter: 575 loss: 4.52840197e-07
Iter: 576 loss: 4.52277874e-07
Iter: 577 loss: 4.53681423e-07
Iter: 578 loss: 4.52076023e-07
Iter: 579 loss: 4.51672179e-07
Iter: 580 loss: 4.55146846e-07
Iter: 581 loss: 4.51656234e-07
Iter: 582 loss: 4.5123835e-07
Iter: 583 loss: 4.514674e-07
Iter: 584 loss: 4.50963284e-07
Iter: 585 loss: 4.50603977e-07
Iter: 586 loss: 4.5394026e-07
Iter: 587 loss: 4.50565182e-07
Iter: 588 loss: 4.50204595e-07
Iter: 589 loss: 4.49928677e-07
Iter: 590 loss: 4.49834459e-07
Iter: 591 loss: 4.4922038e-07
Iter: 592 loss: 4.48443728e-07
Iter: 593 loss: 4.48412294e-07
Iter: 594 loss: 4.47927931e-07
Iter: 595 loss: 4.47751603e-07
Iter: 596 loss: 4.47352477e-07
Iter: 597 loss: 4.46439287e-07
Iter: 598 loss: 4.57440535e-07
Iter: 599 loss: 4.46381875e-07
Iter: 600 loss: 4.4539496e-07
Iter: 601 loss: 4.47600456e-07
Iter: 602 loss: 4.45024341e-07
Iter: 603 loss: 4.44404407e-07
Iter: 604 loss: 4.51296188e-07
Iter: 605 loss: 4.44397983e-07
Iter: 606 loss: 4.43714526e-07
Iter: 607 loss: 4.46119543e-07
Iter: 608 loss: 4.43557099e-07
Iter: 609 loss: 4.43144131e-07
Iter: 610 loss: 4.43118495e-07
Iter: 611 loss: 4.42796761e-07
Iter: 612 loss: 4.42329849e-07
Iter: 613 loss: 4.45981982e-07
Iter: 614 loss: 4.42283408e-07
Iter: 615 loss: 4.41823403e-07
Iter: 616 loss: 4.42569274e-07
Iter: 617 loss: 4.41625957e-07
Iter: 618 loss: 4.41180447e-07
Iter: 619 loss: 4.43351865e-07
Iter: 620 loss: 4.41137e-07
Iter: 621 loss: 4.40707e-07
Iter: 622 loss: 4.41535747e-07
Iter: 623 loss: 4.40566168e-07
Iter: 624 loss: 4.40208822e-07
Iter: 625 loss: 4.39622568e-07
Iter: 626 loss: 4.39625694e-07
Iter: 627 loss: 4.39272526e-07
Iter: 628 loss: 4.39204086e-07
Iter: 629 loss: 4.38834064e-07
Iter: 630 loss: 4.37971295e-07
Iter: 631 loss: 4.48908509e-07
Iter: 632 loss: 4.37948813e-07
Iter: 633 loss: 4.36874956e-07
Iter: 634 loss: 4.37656354e-07
Iter: 635 loss: 4.36229357e-07
Iter: 636 loss: 4.35281748e-07
Iter: 637 loss: 4.4220144e-07
Iter: 638 loss: 4.35185513e-07
Iter: 639 loss: 4.34395872e-07
Iter: 640 loss: 4.46085437e-07
Iter: 641 loss: 4.34388511e-07
Iter: 642 loss: 4.34059785e-07
Iter: 643 loss: 4.33990294e-07
Iter: 644 loss: 4.33784066e-07
Iter: 645 loss: 4.33494279e-07
Iter: 646 loss: 4.35901143e-07
Iter: 647 loss: 4.33462333e-07
Iter: 648 loss: 4.33192042e-07
Iter: 649 loss: 4.33901704e-07
Iter: 650 loss: 4.33088701e-07
Iter: 651 loss: 4.32845468e-07
Iter: 652 loss: 4.33545097e-07
Iter: 653 loss: 4.32792802e-07
Iter: 654 loss: 4.32513588e-07
Iter: 655 loss: 4.33293e-07
Iter: 656 loss: 4.32399787e-07
Iter: 657 loss: 4.32121396e-07
Iter: 658 loss: 4.31437371e-07
Iter: 659 loss: 4.39372e-07
Iter: 660 loss: 4.31361769e-07
Iter: 661 loss: 4.3096091e-07
Iter: 662 loss: 4.30847109e-07
Iter: 663 loss: 4.30336684e-07
Iter: 664 loss: 4.29502251e-07
Iter: 665 loss: 4.29496367e-07
Iter: 666 loss: 4.28621917e-07
Iter: 667 loss: 4.30138385e-07
Iter: 668 loss: 4.28240838e-07
Iter: 669 loss: 4.27603396e-07
Iter: 670 loss: 4.29863292e-07
Iter: 671 loss: 4.2746143e-07
Iter: 672 loss: 4.27014584e-07
Iter: 673 loss: 4.26994063e-07
Iter: 674 loss: 4.2673841e-07
Iter: 675 loss: 4.26413777e-07
Iter: 676 loss: 4.26394735e-07
Iter: 677 loss: 4.26028919e-07
Iter: 678 loss: 4.28002721e-07
Iter: 679 loss: 4.25978044e-07
Iter: 680 loss: 4.2560842e-07
Iter: 681 loss: 4.26793349e-07
Iter: 682 loss: 4.25508148e-07
Iter: 683 loss: 4.25169503e-07
Iter: 684 loss: 4.26085478e-07
Iter: 685 loss: 4.25030777e-07
Iter: 686 loss: 4.24695088e-07
Iter: 687 loss: 4.26463657e-07
Iter: 688 loss: 4.24659675e-07
Iter: 689 loss: 4.24376026e-07
Iter: 690 loss: 4.23799236e-07
Iter: 691 loss: 4.34665111e-07
Iter: 692 loss: 4.23797161e-07
Iter: 693 loss: 4.23421909e-07
Iter: 694 loss: 4.23415173e-07
Iter: 695 loss: 4.2298808e-07
Iter: 696 loss: 4.22717449e-07
Iter: 697 loss: 4.22547203e-07
Iter: 698 loss: 4.21959413e-07
Iter: 699 loss: 4.2141383e-07
Iter: 700 loss: 4.21285449e-07
Iter: 701 loss: 4.20349664e-07
Iter: 702 loss: 4.22161634e-07
Iter: 703 loss: 4.19929904e-07
Iter: 704 loss: 4.19906939e-07
Iter: 705 loss: 4.19450686e-07
Iter: 706 loss: 4.19199722e-07
Iter: 707 loss: 4.18893649e-07
Iter: 708 loss: 4.18847975e-07
Iter: 709 loss: 4.18552645e-07
Iter: 710 loss: 4.19666719e-07
Iter: 711 loss: 4.18464566e-07
Iter: 712 loss: 4.18121203e-07
Iter: 713 loss: 4.20398322e-07
Iter: 714 loss: 4.18087296e-07
Iter: 715 loss: 4.17873053e-07
Iter: 716 loss: 4.1817492e-07
Iter: 717 loss: 4.17743081e-07
Iter: 718 loss: 4.17444397e-07
Iter: 719 loss: 4.18451606e-07
Iter: 720 loss: 4.17355068e-07
Iter: 721 loss: 4.1699397e-07
Iter: 722 loss: 4.16245058e-07
Iter: 723 loss: 4.2920513e-07
Iter: 724 loss: 4.16223912e-07
Iter: 725 loss: 4.15557736e-07
Iter: 726 loss: 4.22777731e-07
Iter: 727 loss: 4.15535624e-07
Iter: 728 loss: 4.14770227e-07
Iter: 729 loss: 4.15962404e-07
Iter: 730 loss: 4.14402962e-07
Iter: 731 loss: 4.13824722e-07
Iter: 732 loss: 4.13533826e-07
Iter: 733 loss: 4.13259386e-07
Iter: 734 loss: 4.12653208e-07
Iter: 735 loss: 4.13666839e-07
Iter: 736 loss: 4.1236521e-07
Iter: 737 loss: 4.12387209e-07
Iter: 738 loss: 4.12087275e-07
Iter: 739 loss: 4.11872179e-07
Iter: 740 loss: 4.11516282e-07
Iter: 741 loss: 4.11515032e-07
Iter: 742 loss: 4.11037206e-07
Iter: 743 loss: 4.11156407e-07
Iter: 744 loss: 4.10713881e-07
Iter: 745 loss: 4.10150363e-07
Iter: 746 loss: 4.10163125e-07
Iter: 747 loss: 4.09867653e-07
Iter: 748 loss: 4.09976849e-07
Iter: 749 loss: 4.09652301e-07
Iter: 750 loss: 4.09258348e-07
Iter: 751 loss: 4.11649125e-07
Iter: 752 loss: 4.09218757e-07
Iter: 753 loss: 4.08828271e-07
Iter: 754 loss: 4.08497982e-07
Iter: 755 loss: 4.08421045e-07
Iter: 756 loss: 4.08009157e-07
Iter: 757 loss: 4.09258149e-07
Iter: 758 loss: 4.07871198e-07
Iter: 759 loss: 4.07289861e-07
Iter: 760 loss: 4.10251118e-07
Iter: 761 loss: 4.07203459e-07
Iter: 762 loss: 4.06801831e-07
Iter: 763 loss: 4.06029017e-07
Iter: 764 loss: 4.22823803e-07
Iter: 765 loss: 4.0602356e-07
Iter: 766 loss: 4.05157181e-07
Iter: 767 loss: 4.05626537e-07
Iter: 768 loss: 4.04578316e-07
Iter: 769 loss: 4.05226302e-07
Iter: 770 loss: 4.04261812e-07
Iter: 771 loss: 4.0397191e-07
Iter: 772 loss: 4.03661488e-07
Iter: 773 loss: 4.03612461e-07
Iter: 774 loss: 4.03266256e-07
Iter: 775 loss: 4.03894234e-07
Iter: 776 loss: 4.03098056e-07
Iter: 777 loss: 4.02891345e-07
Iter: 778 loss: 4.02893505e-07
Iter: 779 loss: 4.02703336e-07
Iter: 780 loss: 4.02440207e-07
Iter: 781 loss: 4.02431624e-07
Iter: 782 loss: 4.02033777e-07
Iter: 783 loss: 4.03988423e-07
Iter: 784 loss: 4.01965423e-07
Iter: 785 loss: 4.01546032e-07
Iter: 786 loss: 4.0144954e-07
Iter: 787 loss: 4.01196644e-07
Iter: 788 loss: 4.00692045e-07
Iter: 789 loss: 4.00847085e-07
Iter: 790 loss: 4.00348029e-07
Iter: 791 loss: 3.9967756e-07
Iter: 792 loss: 4.08511823e-07
Iter: 793 loss: 3.99671251e-07
Iter: 794 loss: 3.99276303e-07
Iter: 795 loss: 3.98645227e-07
Iter: 796 loss: 3.98624366e-07
Iter: 797 loss: 3.97887504e-07
Iter: 798 loss: 3.98054198e-07
Iter: 799 loss: 3.97380148e-07
Iter: 800 loss: 3.97110114e-07
Iter: 801 loss: 3.97053469e-07
Iter: 802 loss: 3.96615519e-07
Iter: 803 loss: 3.96733981e-07
Iter: 804 loss: 3.96294411e-07
Iter: 805 loss: 3.95895881e-07
Iter: 806 loss: 3.95787424e-07
Iter: 807 loss: 3.95522676e-07
Iter: 808 loss: 3.95272394e-07
Iter: 809 loss: 3.95246673e-07
Iter: 810 loss: 3.94986898e-07
Iter: 811 loss: 3.94842971e-07
Iter: 812 loss: 3.94718825e-07
Iter: 813 loss: 3.94420795e-07
Iter: 814 loss: 3.97711915e-07
Iter: 815 loss: 3.94425228e-07
Iter: 816 loss: 3.94208314e-07
Iter: 817 loss: 3.94318363e-07
Iter: 818 loss: 3.94062454e-07
Iter: 819 loss: 3.938151e-07
Iter: 820 loss: 3.93356402e-07
Iter: 821 loss: 3.93345033e-07
Iter: 822 loss: 3.92903758e-07
Iter: 823 loss: 3.9289398e-07
Iter: 824 loss: 3.9251529e-07
Iter: 825 loss: 3.91835101e-07
Iter: 826 loss: 4.06587702e-07
Iter: 827 loss: 3.91818162e-07
Iter: 828 loss: 3.91083688e-07
Iter: 829 loss: 3.91829417e-07
Iter: 830 loss: 3.90693401e-07
Iter: 831 loss: 3.9024232e-07
Iter: 832 loss: 3.90256645e-07
Iter: 833 loss: 3.89843649e-07
Iter: 834 loss: 3.93072185e-07
Iter: 835 loss: 3.89837453e-07
Iter: 836 loss: 3.89637137e-07
Iter: 837 loss: 3.89435513e-07
Iter: 838 loss: 3.89408399e-07
Iter: 839 loss: 3.89167724e-07
Iter: 840 loss: 3.92368e-07
Iter: 841 loss: 3.89172698e-07
Iter: 842 loss: 3.88962576e-07
Iter: 843 loss: 3.88824787e-07
Iter: 844 loss: 3.88750294e-07
Iter: 845 loss: 3.88422706e-07
Iter: 846 loss: 3.90374908e-07
Iter: 847 loss: 3.88364015e-07
Iter: 848 loss: 3.87996153e-07
Iter: 849 loss: 3.88366288e-07
Iter: 850 loss: 3.8782116e-07
Iter: 851 loss: 3.87388354e-07
Iter: 852 loss: 3.86653284e-07
Iter: 853 loss: 3.86654108e-07
Iter: 854 loss: 3.86604825e-07
Iter: 855 loss: 3.8629409e-07
Iter: 856 loss: 3.86039204e-07
Iter: 857 loss: 3.85698058e-07
Iter: 858 loss: 3.85653095e-07
Iter: 859 loss: 3.85298478e-07
Iter: 860 loss: 3.85345885e-07
Iter: 861 loss: 3.85015198e-07
Iter: 862 loss: 3.8459919e-07
Iter: 863 loss: 3.85760359e-07
Iter: 864 loss: 3.84444775e-07
Iter: 865 loss: 3.84107921e-07
Iter: 866 loss: 3.8408308e-07
Iter: 867 loss: 3.83914454e-07
Iter: 868 loss: 3.8349836e-07
Iter: 869 loss: 3.9108e-07
Iter: 870 loss: 3.83504414e-07
Iter: 871 loss: 3.83122142e-07
Iter: 872 loss: 3.85324199e-07
Iter: 873 loss: 3.83060524e-07
Iter: 874 loss: 3.82616463e-07
Iter: 875 loss: 3.85365695e-07
Iter: 876 loss: 3.82567521e-07
Iter: 877 loss: 3.82338044e-07
Iter: 878 loss: 3.82791796e-07
Iter: 879 loss: 3.82272276e-07
Iter: 880 loss: 3.81962536e-07
Iter: 881 loss: 3.82854523e-07
Iter: 882 loss: 3.81880227e-07
Iter: 883 loss: 3.81622726e-07
Iter: 884 loss: 3.81218911e-07
Iter: 885 loss: 3.81206064e-07
Iter: 886 loss: 3.80862105e-07
Iter: 887 loss: 3.80872251e-07
Iter: 888 loss: 3.80491741e-07
Iter: 889 loss: 3.799654e-07
Iter: 890 loss: 3.7993243e-07
Iter: 891 loss: 3.79355839e-07
Iter: 892 loss: 3.80144456e-07
Iter: 893 loss: 3.79050618e-07
Iter: 894 loss: 3.78583394e-07
Iter: 895 loss: 3.7966231e-07
Iter: 896 loss: 3.7843563e-07
Iter: 897 loss: 3.7851953e-07
Iter: 898 loss: 3.78247336e-07
Iter: 899 loss: 3.78138964e-07
Iter: 900 loss: 3.7788908e-07
Iter: 901 loss: 3.82555442e-07
Iter: 902 loss: 3.77864865e-07
Iter: 903 loss: 3.77642039e-07
Iter: 904 loss: 3.77617908e-07
Iter: 905 loss: 3.77419184e-07
Iter: 906 loss: 3.77112315e-07
Iter: 907 loss: 3.77107568e-07
Iter: 908 loss: 3.76893325e-07
Iter: 909 loss: 3.76547575e-07
Iter: 910 loss: 3.76565026e-07
Iter: 911 loss: 3.76046671e-07
Iter: 912 loss: 3.7972282e-07
Iter: 913 loss: 3.7602706e-07
Iter: 914 loss: 3.7561378e-07
Iter: 915 loss: 3.75363499e-07
Iter: 916 loss: 3.75192343e-07
Iter: 917 loss: 3.74832467e-07
Iter: 918 loss: 3.7821232e-07
Iter: 919 loss: 3.74821695e-07
Iter: 920 loss: 3.74470602e-07
Iter: 921 loss: 3.7601464e-07
Iter: 922 loss: 3.74415549e-07
Iter: 923 loss: 3.7420574e-07
Iter: 924 loss: 3.74005481e-07
Iter: 925 loss: 3.73991554e-07
Iter: 926 loss: 3.73686134e-07
Iter: 927 loss: 3.73578246e-07
Iter: 928 loss: 3.73471e-07
Iter: 929 loss: 3.73315686e-07
Iter: 930 loss: 3.73255489e-07
Iter: 931 loss: 3.72993611e-07
Iter: 932 loss: 3.72944896e-07
Iter: 933 loss: 3.72780619e-07
Iter: 934 loss: 3.72478411e-07
Iter: 935 loss: 3.72193142e-07
Iter: 936 loss: 3.72142608e-07
Iter: 937 loss: 3.72095087e-07
Iter: 938 loss: 3.71959e-07
Iter: 939 loss: 3.71788303e-07
Iter: 940 loss: 3.71416377e-07
Iter: 941 loss: 3.77492881e-07
Iter: 942 loss: 3.7140245e-07
Iter: 943 loss: 3.71101748e-07
Iter: 944 loss: 3.71100072e-07
Iter: 945 loss: 3.70872556e-07
Iter: 946 loss: 3.70853343e-07
Iter: 947 loss: 3.70676219e-07
Iter: 948 loss: 3.70382679e-07
Iter: 949 loss: 3.70171733e-07
Iter: 950 loss: 3.70062025e-07
Iter: 951 loss: 3.69632318e-07
Iter: 952 loss: 3.75741877e-07
Iter: 953 loss: 3.69610405e-07
Iter: 954 loss: 3.69409889e-07
Iter: 955 loss: 3.69133488e-07
Iter: 956 loss: 3.69095346e-07
Iter: 957 loss: 3.68754939e-07
Iter: 958 loss: 3.68902818e-07
Iter: 959 loss: 3.68528788e-07
Iter: 960 loss: 3.68207964e-07
Iter: 961 loss: 3.69326017e-07
Iter: 962 loss: 3.68104111e-07
Iter: 963 loss: 3.67985137e-07
Iter: 964 loss: 3.67936167e-07
Iter: 965 loss: 3.67755888e-07
Iter: 966 loss: 3.67408063e-07
Iter: 967 loss: 3.74633032e-07
Iter: 968 loss: 3.67418949e-07
Iter: 969 loss: 3.67049097e-07
Iter: 970 loss: 3.66640137e-07
Iter: 971 loss: 3.6659543e-07
Iter: 972 loss: 3.66135168e-07
Iter: 973 loss: 3.66083896e-07
Iter: 974 loss: 3.65892163e-07
Iter: 975 loss: 3.65638726e-07
Iter: 976 loss: 3.65591404e-07
Iter: 977 loss: 3.65359199e-07
Iter: 978 loss: 3.65356186e-07
Iter: 979 loss: 3.65221e-07
Iter: 980 loss: 3.65067052e-07
Iter: 981 loss: 3.6503792e-07
Iter: 982 loss: 3.6488251e-07
Iter: 983 loss: 3.64894504e-07
Iter: 984 loss: 3.64785677e-07
Iter: 985 loss: 3.64597213e-07
Iter: 986 loss: 3.64607e-07
Iter: 987 loss: 3.64306885e-07
Iter: 988 loss: 3.64815946e-07
Iter: 989 loss: 3.64227276e-07
Iter: 990 loss: 3.6395133e-07
Iter: 991 loss: 3.64410369e-07
Iter: 992 loss: 3.63830139e-07
Iter: 993 loss: 3.6345449e-07
Iter: 994 loss: 3.63809306e-07
Iter: 995 loss: 3.6321677e-07
Iter: 996 loss: 3.6282222e-07
Iter: 997 loss: 3.65687981e-07
Iter: 998 loss: 3.62772482e-07
Iter: 999 loss: 3.62489061e-07
Iter: 1000 loss: 3.6249611e-07
Iter: 1001 loss: 3.62283174e-07
Iter: 1002 loss: 3.61823481e-07
Iter: 1003 loss: 3.67130781e-07
Iter: 1004 loss: 3.61770333e-07
Iter: 1005 loss: 3.61346139e-07
Iter: 1006 loss: 3.61228786e-07
Iter: 1007 loss: 3.61008915e-07
Iter: 1008 loss: 3.6113974e-07
Iter: 1009 loss: 3.60718303e-07
Iter: 1010 loss: 3.60527224e-07
Iter: 1011 loss: 3.60238431e-07
Iter: 1012 loss: 3.60256792e-07
Iter: 1013 loss: 3.59951713e-07
Iter: 1014 loss: 3.63270487e-07
Iter: 1015 loss: 3.59958108e-07
Iter: 1016 loss: 3.59663659e-07
Iter: 1017 loss: 3.59730279e-07
Iter: 1018 loss: 3.59484602e-07
Iter: 1019 loss: 3.59255239e-07
Iter: 1020 loss: 3.5940883e-07
Iter: 1021 loss: 3.59131576e-07
Iter: 1022 loss: 3.58788611e-07
Iter: 1023 loss: 3.59952253e-07
Iter: 1024 loss: 3.58697918e-07
Iter: 1025 loss: 3.58338355e-07
Iter: 1026 loss: 3.6092e-07
Iter: 1027 loss: 3.58303794e-07
Iter: 1028 loss: 3.58038278e-07
Iter: 1029 loss: 3.57766737e-07
Iter: 1030 loss: 3.57731665e-07
Iter: 1031 loss: 3.57263303e-07
Iter: 1032 loss: 3.56599401e-07
Iter: 1033 loss: 3.56585872e-07
Iter: 1034 loss: 3.56234921e-07
Iter: 1035 loss: 3.56164122e-07
Iter: 1036 loss: 3.55846566e-07
Iter: 1037 loss: 3.59507965e-07
Iter: 1038 loss: 3.5584128e-07
Iter: 1039 loss: 3.55666828e-07
Iter: 1040 loss: 3.55266053e-07
Iter: 1041 loss: 3.60498348e-07
Iter: 1042 loss: 3.55234505e-07
Iter: 1043 loss: 3.54999486e-07
Iter: 1044 loss: 3.56313876e-07
Iter: 1045 loss: 3.54974162e-07
Iter: 1046 loss: 3.54605618e-07
Iter: 1047 loss: 3.55829798e-07
Iter: 1048 loss: 3.5452311e-07
Iter: 1049 loss: 3.54298834e-07
Iter: 1050 loss: 3.54127593e-07
Iter: 1051 loss: 3.54033375e-07
Iter: 1052 loss: 3.5379017e-07
Iter: 1053 loss: 3.5379469e-07
Iter: 1054 loss: 3.534862e-07
Iter: 1055 loss: 3.5284387e-07
Iter: 1056 loss: 3.61319763e-07
Iter: 1057 loss: 3.52807888e-07
Iter: 1058 loss: 3.52546834e-07
Iter: 1059 loss: 3.5252927e-07
Iter: 1060 loss: 3.52248662e-07
Iter: 1061 loss: 3.5270233e-07
Iter: 1062 loss: 3.52093394e-07
Iter: 1063 loss: 3.51808865e-07
Iter: 1064 loss: 3.51574585e-07
Iter: 1065 loss: 3.51497107e-07
Iter: 1066 loss: 3.51120093e-07
Iter: 1067 loss: 3.51850019e-07
Iter: 1068 loss: 3.50962466e-07
Iter: 1069 loss: 3.50600487e-07
Iter: 1070 loss: 3.50599692e-07
Iter: 1071 loss: 3.50326331e-07
Iter: 1072 loss: 3.51255608e-07
Iter: 1073 loss: 3.50247319e-07
Iter: 1074 loss: 3.5012431e-07
Iter: 1075 loss: 3.49859079e-07
Iter: 1076 loss: 3.53562427e-07
Iter: 1077 loss: 3.49835773e-07
Iter: 1078 loss: 3.49623e-07
Iter: 1079 loss: 3.49601436e-07
Iter: 1080 loss: 3.49383811e-07
Iter: 1081 loss: 3.5001338e-07
Iter: 1082 loss: 3.49322903e-07
Iter: 1083 loss: 3.49200491e-07
Iter: 1084 loss: 3.48840388e-07
Iter: 1085 loss: 3.50420862e-07
Iter: 1086 loss: 3.48687479e-07
Iter: 1087 loss: 3.48169863e-07
Iter: 1088 loss: 3.48167191e-07
Iter: 1089 loss: 3.47939363e-07
Iter: 1090 loss: 3.47454147e-07
Iter: 1091 loss: 3.55733818e-07
Iter: 1092 loss: 3.47440221e-07
Iter: 1093 loss: 3.46972399e-07
Iter: 1094 loss: 3.51353037e-07
Iter: 1095 loss: 3.46953783e-07
Iter: 1096 loss: 3.46535671e-07
Iter: 1097 loss: 3.48578851e-07
Iter: 1098 loss: 3.46445631e-07
Iter: 1099 loss: 3.46291188e-07
Iter: 1100 loss: 3.46133163e-07
Iter: 1101 loss: 3.46097323e-07
Iter: 1102 loss: 3.45822087e-07
Iter: 1103 loss: 3.48597439e-07
Iter: 1104 loss: 3.45817909e-07
Iter: 1105 loss: 3.45609806e-07
Iter: 1106 loss: 3.47821299e-07
Iter: 1107 loss: 3.45619128e-07
Iter: 1108 loss: 3.45494129e-07
Iter: 1109 loss: 3.45252118e-07
Iter: 1110 loss: 3.4913225e-07
Iter: 1111 loss: 3.4523876e-07
Iter: 1112 loss: 3.44814396e-07
Iter: 1113 loss: 3.44688175e-07
Iter: 1114 loss: 3.44393897e-07
Iter: 1115 loss: 3.44414588e-07
Iter: 1116 loss: 3.44158678e-07
Iter: 1117 loss: 3.43963677e-07
Iter: 1118 loss: 3.43454047e-07
Iter: 1119 loss: 3.47172573e-07
Iter: 1120 loss: 3.43342492e-07
Iter: 1121 loss: 3.42802423e-07
Iter: 1122 loss: 3.47160153e-07
Iter: 1123 loss: 3.4277042e-07
Iter: 1124 loss: 3.42404746e-07
Iter: 1125 loss: 3.47896957e-07
Iter: 1126 loss: 3.42396504e-07
Iter: 1127 loss: 3.42248313e-07
Iter: 1128 loss: 3.41818719e-07
Iter: 1129 loss: 3.45104183e-07
Iter: 1130 loss: 3.41725638e-07
Iter: 1131 loss: 3.41972623e-07
Iter: 1132 loss: 3.41580801e-07
Iter: 1133 loss: 3.41459213e-07
Iter: 1134 loss: 3.4110667e-07
Iter: 1135 loss: 3.42494189e-07
Iter: 1136 loss: 3.40935117e-07
Iter: 1137 loss: 3.40494239e-07
Iter: 1138 loss: 3.42375017e-07
Iter: 1139 loss: 3.40403375e-07
Iter: 1140 loss: 3.40095482e-07
Iter: 1141 loss: 3.4050089e-07
Iter: 1142 loss: 3.39930295e-07
Iter: 1143 loss: 3.39722192e-07
Iter: 1144 loss: 3.39681776e-07
Iter: 1145 loss: 3.39558909e-07
Iter: 1146 loss: 3.39623938e-07
Iter: 1147 loss: 3.39445023e-07
Iter: 1148 loss: 3.39297429e-07
Iter: 1149 loss: 3.3943644e-07
Iter: 1150 loss: 3.39218673e-07
Iter: 1151 loss: 3.38955431e-07
Iter: 1152 loss: 3.39079691e-07
Iter: 1153 loss: 3.38779e-07
Iter: 1154 loss: 3.38514951e-07
Iter: 1155 loss: 3.38334161e-07
Iter: 1156 loss: 3.38256626e-07
Iter: 1157 loss: 3.37936e-07
Iter: 1158 loss: 3.4103121e-07
Iter: 1159 loss: 3.37923041e-07
Iter: 1160 loss: 3.37621316e-07
Iter: 1161 loss: 3.37163499e-07
Iter: 1162 loss: 3.37166398e-07
Iter: 1163 loss: 3.37041058e-07
Iter: 1164 loss: 3.37018093e-07
Iter: 1165 loss: 3.36864844e-07
Iter: 1166 loss: 3.36902218e-07
Iter: 1167 loss: 3.36759229e-07
Iter: 1168 loss: 3.36569911e-07
Iter: 1169 loss: 3.36431526e-07
Iter: 1170 loss: 3.36383323e-07
Iter: 1171 loss: 3.36157882e-07
Iter: 1172 loss: 3.35858573e-07
Iter: 1173 loss: 3.35836944e-07
Iter: 1174 loss: 3.35354798e-07
Iter: 1175 loss: 3.38428663e-07
Iter: 1176 loss: 3.35268794e-07
Iter: 1177 loss: 3.35085815e-07
Iter: 1178 loss: 3.35019251e-07
Iter: 1179 loss: 3.34804213e-07
Iter: 1180 loss: 3.34253514e-07
Iter: 1181 loss: 3.41226212e-07
Iter: 1182 loss: 3.34227195e-07
Iter: 1183 loss: 3.34022e-07
Iter: 1184 loss: 3.34025458e-07
Iter: 1185 loss: 3.33831196e-07
Iter: 1186 loss: 3.34560809e-07
Iter: 1187 loss: 3.33792229e-07
Iter: 1188 loss: 3.33624655e-07
Iter: 1189 loss: 3.3360817e-07
Iter: 1190 loss: 3.33505938e-07
Iter: 1191 loss: 3.33354336e-07
Iter: 1192 loss: 3.35512198e-07
Iter: 1193 loss: 3.33349874e-07
Iter: 1194 loss: 3.33218111e-07
Iter: 1195 loss: 3.3303894e-07
Iter: 1196 loss: 3.33045364e-07
Iter: 1197 loss: 3.327618e-07
Iter: 1198 loss: 3.32708964e-07
Iter: 1199 loss: 3.32541106e-07
Iter: 1200 loss: 3.32436571e-07
Iter: 1201 loss: 3.32371116e-07
Iter: 1202 loss: 3.3221869e-07
Iter: 1203 loss: 3.31758429e-07
Iter: 1204 loss: 3.36312269e-07
Iter: 1205 loss: 3.31685555e-07
Iter: 1206 loss: 3.31307973e-07
Iter: 1207 loss: 3.33637956e-07
Iter: 1208 loss: 3.31254654e-07
Iter: 1209 loss: 3.31038791e-07
Iter: 1210 loss: 3.31463497e-07
Iter: 1211 loss: 3.30937326e-07
Iter: 1212 loss: 3.30618093e-07
Iter: 1213 loss: 3.32646209e-07
Iter: 1214 loss: 3.30587e-07
Iter: 1215 loss: 3.30445033e-07
Iter: 1216 loss: 3.30382193e-07
Iter: 1217 loss: 3.30330266e-07
Iter: 1218 loss: 3.30147827e-07
Iter: 1219 loss: 3.30124e-07
Iter: 1220 loss: 3.30004923e-07
Iter: 1221 loss: 3.298872e-07
Iter: 1222 loss: 3.29852924e-07
Iter: 1223 loss: 3.29759246e-07
Iter: 1224 loss: 3.29594e-07
Iter: 1225 loss: 3.33370622e-07
Iter: 1226 loss: 3.29597583e-07
Iter: 1227 loss: 3.294289e-07
Iter: 1228 loss: 3.29474233e-07
Iter: 1229 loss: 3.29288298e-07
Iter: 1230 loss: 3.29114073e-07
Iter: 1231 loss: 3.29121633e-07
Iter: 1232 loss: 3.28943969e-07
Iter: 1233 loss: 3.28739617e-07
Iter: 1234 loss: 3.28718272e-07
Iter: 1235 loss: 3.28420867e-07
Iter: 1236 loss: 3.28101635e-07
Iter: 1237 loss: 3.28043399e-07
Iter: 1238 loss: 3.27820942e-07
Iter: 1239 loss: 3.31088245e-07
Iter: 1240 loss: 3.27836887e-07
Iter: 1241 loss: 3.27633416e-07
Iter: 1242 loss: 3.29027131e-07
Iter: 1243 loss: 3.27636826e-07
Iter: 1244 loss: 3.27401438e-07
Iter: 1245 loss: 3.27466324e-07
Iter: 1246 loss: 3.27260096e-07
Iter: 1247 loss: 3.27135751e-07
Iter: 1248 loss: 3.26956808e-07
Iter: 1249 loss: 3.26942313e-07
Iter: 1250 loss: 3.26799437e-07
Iter: 1251 loss: 3.26786051e-07
Iter: 1252 loss: 3.26620864e-07
Iter: 1253 loss: 3.26549412e-07
Iter: 1254 loss: 3.26469149e-07
Iter: 1255 loss: 3.26306719e-07
Iter: 1256 loss: 3.26027305e-07
Iter: 1257 loss: 3.26018466e-07
Iter: 1258 loss: 3.25868314e-07
Iter: 1259 loss: 3.25834264e-07
Iter: 1260 loss: 3.25688376e-07
Iter: 1261 loss: 3.26563764e-07
Iter: 1262 loss: 3.25670271e-07
Iter: 1263 loss: 3.25563263e-07
Iter: 1264 loss: 3.25286237e-07
Iter: 1265 loss: 3.28235416e-07
Iter: 1266 loss: 3.25270605e-07
Iter: 1267 loss: 3.25020949e-07
Iter: 1268 loss: 3.28704488e-07
Iter: 1269 loss: 3.25027344e-07
Iter: 1270 loss: 3.24865198e-07
Iter: 1271 loss: 3.26592357e-07
Iter: 1272 loss: 3.24823077e-07
Iter: 1273 loss: 3.24769019e-07
Iter: 1274 loss: 3.24541645e-07
Iter: 1275 loss: 3.25646681e-07
Iter: 1276 loss: 3.24502537e-07
Iter: 1277 loss: 3.24527122e-07
Iter: 1278 loss: 3.24400901e-07
Iter: 1279 loss: 3.2429557e-07
Iter: 1280 loss: 3.24145446e-07
Iter: 1281 loss: 3.24161704e-07
Iter: 1282 loss: 3.240024e-07
Iter: 1283 loss: 3.23899883e-07
Iter: 1284 loss: 3.23832e-07
Iter: 1285 loss: 3.23607537e-07
Iter: 1286 loss: 3.24420114e-07
Iter: 1287 loss: 3.23540803e-07
Iter: 1288 loss: 3.23298536e-07
Iter: 1289 loss: 3.23300299e-07
Iter: 1290 loss: 3.23145457e-07
Iter: 1291 loss: 3.22821393e-07
Iter: 1292 loss: 3.28925438e-07
Iter: 1293 loss: 3.22833444e-07
Iter: 1294 loss: 3.22607548e-07
Iter: 1295 loss: 3.23600801e-07
Iter: 1296 loss: 3.22582196e-07
Iter: 1297 loss: 3.22258245e-07
Iter: 1298 loss: 3.2293849e-07
Iter: 1299 loss: 3.22136543e-07
Iter: 1300 loss: 3.21940547e-07
Iter: 1301 loss: 3.21817197e-07
Iter: 1302 loss: 3.21715873e-07
Iter: 1303 loss: 3.21473806e-07
Iter: 1304 loss: 3.22222945e-07
Iter: 1305 loss: 3.2140548e-07
Iter: 1306 loss: 3.21183e-07
Iter: 1307 loss: 3.21183165e-07
Iter: 1308 loss: 3.21072321e-07
Iter: 1309 loss: 3.20875415e-07
Iter: 1310 loss: 3.20874449e-07
Iter: 1311 loss: 3.20651566e-07
Iter: 1312 loss: 3.22245967e-07
Iter: 1313 loss: 3.20618483e-07
Iter: 1314 loss: 3.20407594e-07
Iter: 1315 loss: 3.21494099e-07
Iter: 1316 loss: 3.20346828e-07
Iter: 1317 loss: 3.20269493e-07
Iter: 1318 loss: 3.20126873e-07
Iter: 1319 loss: 3.23712186e-07
Iter: 1320 loss: 3.20130084e-07
Iter: 1321 loss: 3.19879916e-07
Iter: 1322 loss: 3.19632591e-07
Iter: 1323 loss: 3.19578447e-07
Iter: 1324 loss: 3.19326375e-07
Iter: 1325 loss: 3.23376582e-07
Iter: 1326 loss: 3.19315234e-07
Iter: 1327 loss: 3.19101929e-07
Iter: 1328 loss: 3.21887796e-07
Iter: 1329 loss: 3.19102611e-07
Iter: 1330 loss: 3.18974571e-07
Iter: 1331 loss: 3.18813051e-07
Iter: 1332 loss: 3.18803188e-07
Iter: 1333 loss: 3.18683533e-07
Iter: 1334 loss: 3.20200144e-07
Iter: 1335 loss: 3.18680037e-07
Iter: 1336 loss: 3.18537047e-07
Iter: 1337 loss: 3.18572916e-07
Iter: 1338 loss: 3.18428874e-07
Iter: 1339 loss: 3.18338124e-07
Iter: 1340 loss: 3.18216934e-07
Iter: 1341 loss: 3.18214632e-07
Iter: 1342 loss: 3.17987912e-07
Iter: 1343 loss: 3.17886361e-07
Iter: 1344 loss: 3.17765966e-07
Iter: 1345 loss: 3.17760311e-07
Iter: 1346 loss: 3.17605611e-07
Iter: 1347 loss: 3.17508722e-07
Iter: 1348 loss: 3.17298827e-07
Iter: 1349 loss: 3.20940217e-07
Iter: 1350 loss: 3.17300874e-07
Iter: 1351 loss: 3.17002645e-07
Iter: 1352 loss: 3.17189659e-07
Iter: 1353 loss: 3.16804801e-07
Iter: 1354 loss: 3.16661158e-07
Iter: 1355 loss: 3.16638477e-07
Iter: 1356 loss: 3.16475962e-07
Iter: 1357 loss: 3.16297871e-07
Iter: 1358 loss: 3.16284513e-07
Iter: 1359 loss: 3.16063336e-07
Iter: 1360 loss: 3.16251487e-07
Iter: 1361 loss: 3.15909972e-07
Iter: 1362 loss: 3.15759451e-07
Iter: 1363 loss: 3.17053491e-07
Iter: 1364 loss: 3.15756267e-07
Iter: 1365 loss: 3.15541513e-07
Iter: 1366 loss: 3.15664664e-07
Iter: 1367 loss: 3.15401621e-07
Iter: 1368 loss: 3.15252294e-07
Iter: 1369 loss: 3.152999e-07
Iter: 1370 loss: 3.15152477e-07
Iter: 1371 loss: 3.14983936e-07
Iter: 1372 loss: 3.14695512e-07
Iter: 1373 loss: 3.14703385e-07
Iter: 1374 loss: 3.14512448e-07
Iter: 1375 loss: 3.14474335e-07
Iter: 1376 loss: 3.14219903e-07
Iter: 1377 loss: 3.14625424e-07
Iter: 1378 loss: 3.14071428e-07
Iter: 1379 loss: 3.13892684e-07
Iter: 1380 loss: 3.13784426e-07
Iter: 1381 loss: 3.13687224e-07
Iter: 1382 loss: 3.13580756e-07
Iter: 1383 loss: 3.13568336e-07
Iter: 1384 loss: 3.13412585e-07
Iter: 1385 loss: 3.1314039e-07
Iter: 1386 loss: 3.18996058e-07
Iter: 1387 loss: 3.13140163e-07
Iter: 1388 loss: 3.12896589e-07
Iter: 1389 loss: 3.14516569e-07
Iter: 1390 loss: 3.12877972e-07
Iter: 1391 loss: 3.12771164e-07
Iter: 1392 loss: 3.13147353e-07
Iter: 1393 loss: 3.12736802e-07
Iter: 1394 loss: 3.12550071e-07
Iter: 1395 loss: 3.12661058e-07
Iter: 1396 loss: 3.12456763e-07
Iter: 1397 loss: 3.12301722e-07
Iter: 1398 loss: 3.12345605e-07
Iter: 1399 loss: 3.1218454e-07
Iter: 1400 loss: 3.12022223e-07
Iter: 1401 loss: 3.12025776e-07
Iter: 1402 loss: 3.11889124e-07
Iter: 1403 loss: 3.11745168e-07
Iter: 1404 loss: 3.11719873e-07
Iter: 1405 loss: 3.11648535e-07
Iter: 1406 loss: 3.114165e-07
Iter: 1407 loss: 3.12544614e-07
Iter: 1408 loss: 3.11348657e-07
Iter: 1409 loss: 3.11027748e-07
Iter: 1410 loss: 3.12680442e-07
Iter: 1411 loss: 3.10960843e-07
Iter: 1412 loss: 3.10850851e-07
Iter: 1413 loss: 3.10804296e-07
Iter: 1414 loss: 3.10732901e-07
Iter: 1415 loss: 3.10517208e-07
Iter: 1416 loss: 3.10541111e-07
Iter: 1417 loss: 3.10280399e-07
Iter: 1418 loss: 3.10271872e-07
Iter: 1419 loss: 3.10093213e-07
Iter: 1420 loss: 3.09950138e-07
Iter: 1421 loss: 3.10209771e-07
Iter: 1422 loss: 3.09875361e-07
Iter: 1423 loss: 3.0977867e-07
Iter: 1424 loss: 3.09610755e-07
Iter: 1425 loss: 3.09613426e-07
Iter: 1426 loss: 3.09457164e-07
Iter: 1427 loss: 3.09453185e-07
Iter: 1428 loss: 3.0933856e-07
Iter: 1429 loss: 3.09450911e-07
Iter: 1430 loss: 3.09262305e-07
Iter: 1431 loss: 3.09143019e-07
Iter: 1432 loss: 3.08829129e-07
Iter: 1433 loss: 3.1158865e-07
Iter: 1434 loss: 3.08768875e-07
Iter: 1435 loss: 3.08735395e-07
Iter: 1436 loss: 3.08622731e-07
Iter: 1437 loss: 3.08476729e-07
Iter: 1438 loss: 3.08238555e-07
Iter: 1439 loss: 3.08224969e-07
Iter: 1440 loss: 3.07989524e-07
Iter: 1441 loss: 3.07982191e-07
Iter: 1442 loss: 3.07777668e-07
Iter: 1443 loss: 3.076361e-07
Iter: 1444 loss: 3.07648719e-07
Iter: 1445 loss: 3.07566495e-07
Iter: 1446 loss: 3.07355435e-07
Iter: 1447 loss: 3.09999649e-07
Iter: 1448 loss: 3.07333977e-07
Iter: 1449 loss: 3.07133433e-07
Iter: 1450 loss: 3.07038704e-07
Iter: 1451 loss: 3.06934396e-07
Iter: 1452 loss: 3.06654414e-07
Iter: 1453 loss: 3.06597059e-07
Iter: 1454 loss: 3.06430508e-07
Iter: 1455 loss: 3.06600782e-07
Iter: 1456 loss: 3.06345811e-07
Iter: 1457 loss: 3.06249149e-07
Iter: 1458 loss: 3.05986418e-07
Iter: 1459 loss: 3.06313893e-07
Iter: 1460 loss: 3.05790309e-07
Iter: 1461 loss: 3.05437794e-07
Iter: 1462 loss: 3.08790732e-07
Iter: 1463 loss: 3.0541878e-07
Iter: 1464 loss: 3.05180208e-07
Iter: 1465 loss: 3.0815869e-07
Iter: 1466 loss: 3.05182454e-07
Iter: 1467 loss: 3.04919695e-07
Iter: 1468 loss: 3.04477766e-07
Iter: 1469 loss: 3.04478817e-07
Iter: 1470 loss: 3.04107147e-07
Iter: 1471 loss: 3.06100105e-07
Iter: 1472 loss: 3.04051525e-07
Iter: 1473 loss: 3.03771515e-07
Iter: 1474 loss: 3.0750931e-07
Iter: 1475 loss: 3.03757872e-07
Iter: 1476 loss: 3.03515549e-07
Iter: 1477 loss: 3.03210726e-07
Iter: 1478 loss: 3.03194611e-07
Iter: 1479 loss: 3.03001627e-07
Iter: 1480 loss: 3.02963883e-07
Iter: 1481 loss: 3.02865601e-07
Iter: 1482 loss: 3.02669235e-07
Iter: 1483 loss: 3.05452346e-07
Iter: 1484 loss: 3.02656247e-07
Iter: 1485 loss: 3.02446836e-07
Iter: 1486 loss: 3.03596323e-07
Iter: 1487 loss: 3.0242586e-07
Iter: 1488 loss: 3.02312202e-07
Iter: 1489 loss: 3.03466408e-07
Iter: 1490 loss: 3.02302908e-07
Iter: 1491 loss: 3.02193143e-07
Iter: 1492 loss: 3.02305295e-07
Iter: 1493 loss: 3.02113904e-07
Iter: 1494 loss: 3.01956931e-07
Iter: 1495 loss: 3.01691472e-07
Iter: 1496 loss: 3.0167476e-07
Iter: 1497 loss: 3.01314742e-07
Iter: 1498 loss: 3.01047322e-07
Iter: 1499 loss: 3.00942645e-07
Iter: 1500 loss: 3.01343846e-07
Iter: 1501 loss: 3.00751537e-07
Iter: 1502 loss: 3.00628074e-07
Iter: 1503 loss: 3.00516888e-07
Iter: 1504 loss: 3.00476898e-07
Iter: 1505 loss: 3.00369351e-07
Iter: 1506 loss: 3.00550823e-07
Iter: 1507 loss: 3.00305771e-07
Iter: 1508 loss: 3.00234547e-07
Iter: 1509 loss: 3.00206892e-07
Iter: 1510 loss: 3.00169091e-07
Iter: 1511 loss: 3.00111509e-07
Iter: 1512 loss: 3.00106251e-07
Iter: 1513 loss: 2.99983924e-07
Iter: 1514 loss: 3.0022963e-07
Iter: 1515 loss: 2.9991844e-07
Iter: 1516 loss: 2.99849546e-07
Iter: 1517 loss: 2.99725599e-07
Iter: 1518 loss: 2.99705732e-07
Iter: 1519 loss: 2.99500641e-07
Iter: 1520 loss: 2.99850683e-07
Iter: 1521 loss: 2.99399431e-07
Iter: 1522 loss: 2.99054477e-07
Iter: 1523 loss: 3.00578705e-07
Iter: 1524 loss: 2.99005563e-07
Iter: 1525 loss: 2.98771795e-07
Iter: 1526 loss: 2.99107711e-07
Iter: 1527 loss: 2.98644522e-07
Iter: 1528 loss: 2.98455348e-07
Iter: 1529 loss: 2.98326597e-07
Iter: 1530 loss: 2.98274614e-07
Iter: 1531 loss: 2.98008018e-07
Iter: 1532 loss: 3.00114067e-07
Iter: 1533 loss: 2.98011628e-07
Iter: 1534 loss: 2.97923293e-07
Iter: 1535 loss: 2.97898083e-07
Iter: 1536 loss: 2.97812278e-07
Iter: 1537 loss: 2.97583028e-07
Iter: 1538 loss: 2.99036628e-07
Iter: 1539 loss: 2.97495717e-07
Iter: 1540 loss: 2.97403801e-07
Iter: 1541 loss: 2.97376801e-07
Iter: 1542 loss: 2.97232333e-07
Iter: 1543 loss: 2.97526128e-07
Iter: 1544 loss: 2.97171596e-07
Iter: 1545 loss: 2.97063508e-07
Iter: 1546 loss: 2.96891699e-07
Iter: 1547 loss: 2.96887464e-07
Iter: 1548 loss: 2.96846508e-07
Iter: 1549 loss: 2.96779859e-07
Iter: 1550 loss: 2.96702211e-07
Iter: 1551 loss: 2.9650036e-07
Iter: 1552 loss: 2.97168924e-07
Iter: 1553 loss: 2.96380676e-07
Iter: 1554 loss: 2.96092765e-07
Iter: 1555 loss: 2.96772185e-07
Iter: 1556 loss: 2.95967823e-07
Iter: 1557 loss: 2.95948496e-07
Iter: 1558 loss: 2.95802408e-07
Iter: 1559 loss: 2.95672521e-07
Iter: 1560 loss: 2.95434745e-07
Iter: 1561 loss: 2.97724512e-07
Iter: 1562 loss: 2.95396973e-07
Iter: 1563 loss: 2.95173976e-07
Iter: 1564 loss: 2.96516077e-07
Iter: 1565 loss: 2.95160049e-07
Iter: 1566 loss: 2.95029452e-07
Iter: 1567 loss: 2.96567435e-07
Iter: 1568 loss: 2.95027e-07
Iter: 1569 loss: 2.94883563e-07
Iter: 1570 loss: 2.94911473e-07
Iter: 1571 loss: 2.94770132e-07
Iter: 1572 loss: 2.94661902e-07
Iter: 1573 loss: 2.94516042e-07
Iter: 1574 loss: 2.9452778e-07
Iter: 1575 loss: 2.94298786e-07
Iter: 1576 loss: 2.95838618e-07
Iter: 1577 loss: 2.9427693e-07
Iter: 1578 loss: 2.94045861e-07
Iter: 1579 loss: 2.95042639e-07
Iter: 1580 loss: 2.93975347e-07
Iter: 1581 loss: 2.93836422e-07
Iter: 1582 loss: 2.93541348e-07
Iter: 1583 loss: 2.93548226e-07
Iter: 1584 loss: 2.932764e-07
Iter: 1585 loss: 2.96472166e-07
Iter: 1586 loss: 2.93277168e-07
Iter: 1587 loss: 2.93059628e-07
Iter: 1588 loss: 2.95077598e-07
Iter: 1589 loss: 2.93057866e-07
Iter: 1590 loss: 2.92989114e-07
Iter: 1591 loss: 2.92817191e-07
Iter: 1592 loss: 2.93955168e-07
Iter: 1593 loss: 2.92776775e-07
Iter: 1594 loss: 2.92695802e-07
Iter: 1595 loss: 2.92665732e-07
Iter: 1596 loss: 2.92539966e-07
Iter: 1597 loss: 2.92857123e-07
Iter: 1598 loss: 2.92513249e-07
Iter: 1599 loss: 2.92442792e-07
Iter: 1600 loss: 2.92207574e-07
Iter: 1601 loss: 2.9329442e-07
Iter: 1602 loss: 2.92132569e-07
Iter: 1603 loss: 2.91797e-07
Iter: 1604 loss: 2.93393214e-07
Iter: 1605 loss: 2.91728838e-07
Iter: 1606 loss: 2.91555807e-07
Iter: 1607 loss: 2.91543699e-07
Iter: 1608 loss: 2.9134668e-07
Iter: 1609 loss: 2.91363705e-07
Iter: 1610 loss: 2.91188201e-07
Iter: 1611 loss: 2.91034723e-07
Iter: 1612 loss: 2.90674905e-07
Iter: 1613 loss: 2.95969528e-07
Iter: 1614 loss: 2.90652139e-07
Iter: 1615 loss: 2.9072487e-07
Iter: 1616 loss: 2.90477345e-07
Iter: 1617 loss: 2.90327932e-07
Iter: 1618 loss: 2.90347941e-07
Iter: 1619 loss: 2.902338e-07
Iter: 1620 loss: 2.90070886e-07
Iter: 1621 loss: 2.89815546e-07
Iter: 1622 loss: 2.89799175e-07
Iter: 1623 loss: 2.89741934e-07
Iter: 1624 loss: 2.89647375e-07
Iter: 1625 loss: 2.89543209e-07
Iter: 1626 loss: 2.89533546e-07
Iter: 1627 loss: 2.89443562e-07
Iter: 1628 loss: 2.8936222e-07
Iter: 1629 loss: 2.89176114e-07
Iter: 1630 loss: 2.93486323e-07
Iter: 1631 loss: 2.89182935e-07
Iter: 1632 loss: 2.89157697e-07
Iter: 1633 loss: 2.89094402e-07
Iter: 1634 loss: 2.89003111e-07
Iter: 1635 loss: 2.88938679e-07
Iter: 1636 loss: 2.88915572e-07
Iter: 1637 loss: 2.88758713e-07
Iter: 1638 loss: 2.89130185e-07
Iter: 1639 loss: 2.88683736e-07
Iter: 1640 loss: 2.88521e-07
Iter: 1641 loss: 2.88783269e-07
Iter: 1642 loss: 2.88433199e-07
Iter: 1643 loss: 2.88232116e-07
Iter: 1644 loss: 2.88273441e-07
Iter: 1645 loss: 2.88084436e-07
Iter: 1646 loss: 2.87806245e-07
Iter: 1647 loss: 2.88691467e-07
Iter: 1648 loss: 2.87740903e-07
Iter: 1649 loss: 2.87702449e-07
Iter: 1650 loss: 2.87675761e-07
Iter: 1651 loss: 2.87593025e-07
Iter: 1652 loss: 2.8750236e-07
Iter: 1653 loss: 2.87499347e-07
Iter: 1654 loss: 2.87429032e-07
Iter: 1655 loss: 2.87344506e-07
Iter: 1656 loss: 2.87339702e-07
Iter: 1657 loss: 2.87252334e-07
Iter: 1658 loss: 2.87246195e-07
Iter: 1659 loss: 2.87145667e-07
Iter: 1660 loss: 2.87174714e-07
Iter: 1661 loss: 2.87068758e-07
Iter: 1662 loss: 2.86955753e-07
Iter: 1663 loss: 2.86721416e-07
Iter: 1664 loss: 2.90866609e-07
Iter: 1665 loss: 2.86712805e-07
Iter: 1666 loss: 2.86746058e-07
Iter: 1667 loss: 2.86598294e-07
Iter: 1668 loss: 2.86535851e-07
Iter: 1669 loss: 2.86468037e-07
Iter: 1670 loss: 2.86414036e-07
Iter: 1671 loss: 2.86311206e-07
Iter: 1672 loss: 2.86589909e-07
Iter: 1673 loss: 2.86268545e-07
Iter: 1674 loss: 2.86157132e-07
Iter: 1675 loss: 2.86158638e-07
Iter: 1676 loss: 2.86113902e-07
Iter: 1677 loss: 2.86006923e-07
Iter: 1678 loss: 2.8803376e-07
Iter: 1679 loss: 2.85999477e-07
Iter: 1680 loss: 2.85834972e-07
Iter: 1681 loss: 2.86963342e-07
Iter: 1682 loss: 2.85819709e-07
Iter: 1683 loss: 2.85669955e-07
Iter: 1684 loss: 2.86122884e-07
Iter: 1685 loss: 2.8562647e-07
Iter: 1686 loss: 2.8550815e-07
Iter: 1687 loss: 2.85464409e-07
Iter: 1688 loss: 2.85414501e-07
Iter: 1689 loss: 2.85245051e-07
Iter: 1690 loss: 2.85436244e-07
Iter: 1691 loss: 2.85133041e-07
Iter: 1692 loss: 2.85174451e-07
Iter: 1693 loss: 2.85077e-07
Iter: 1694 loss: 2.85016199e-07
Iter: 1695 loss: 2.84902114e-07
Iter: 1696 loss: 2.86271586e-07
Iter: 1697 loss: 2.84885346e-07
Iter: 1698 loss: 2.84733119e-07
Iter: 1699 loss: 2.84656636e-07
Iter: 1700 loss: 2.84599082e-07
Iter: 1701 loss: 2.84381429e-07
Iter: 1702 loss: 2.86005303e-07
Iter: 1703 loss: 2.84357697e-07
Iter: 1704 loss: 2.84315917e-07
Iter: 1705 loss: 2.84263677e-07
Iter: 1706 loss: 2.84224853e-07
Iter: 1707 loss: 2.84120034e-07
Iter: 1708 loss: 2.85143926e-07
Iter: 1709 loss: 2.84092835e-07
Iter: 1710 loss: 2.83999043e-07
Iter: 1711 loss: 2.84510378e-07
Iter: 1712 loss: 2.83985457e-07
Iter: 1713 loss: 2.83943052e-07
Iter: 1714 loss: 2.8398216e-07
Iter: 1715 loss: 2.83894565e-07
Iter: 1716 loss: 2.83807708e-07
Iter: 1717 loss: 2.84517739e-07
Iter: 1718 loss: 2.83816746e-07
Iter: 1719 loss: 2.83770788e-07
Iter: 1720 loss: 2.83742395e-07
Iter: 1721 loss: 2.83707067e-07
Iter: 1722 loss: 2.83605402e-07
Iter: 1723 loss: 2.83754844e-07
Iter: 1724 loss: 2.83564077e-07
Iter: 1725 loss: 2.83418274e-07
Iter: 1726 loss: 2.83754559e-07
Iter: 1727 loss: 2.83357963e-07
Iter: 1728 loss: 2.8322421e-07
Iter: 1729 loss: 2.83264427e-07
Iter: 1730 loss: 2.83121068e-07
Iter: 1731 loss: 2.83025287e-07
Iter: 1732 loss: 2.83344548e-07
Iter: 1733 loss: 2.82993369e-07
Iter: 1734 loss: 2.82929648e-07
Iter: 1735 loss: 2.82919899e-07
Iter: 1736 loss: 2.82895741e-07
Iter: 1737 loss: 2.82849214e-07
Iter: 1738 loss: 2.83582381e-07
Iter: 1739 loss: 2.82838499e-07
Iter: 1740 loss: 2.82749767e-07
Iter: 1741 loss: 2.82842223e-07
Iter: 1742 loss: 2.82721686e-07
Iter: 1743 loss: 2.82663734e-07
Iter: 1744 loss: 2.82666832e-07
Iter: 1745 loss: 2.82627354e-07
Iter: 1746 loss: 2.82511166e-07
Iter: 1747 loss: 2.82733367e-07
Iter: 1748 loss: 2.82440965e-07
Iter: 1749 loss: 2.82361839e-07
Iter: 1750 loss: 2.82359366e-07
Iter: 1751 loss: 2.82268132e-07
Iter: 1752 loss: 2.82516197e-07
Iter: 1753 loss: 2.82244031e-07
Iter: 1754 loss: 2.82179599e-07
Iter: 1755 loss: 2.8215112e-07
Iter: 1756 loss: 2.82113803e-07
Iter: 1757 loss: 2.82067361e-07
Iter: 1758 loss: 2.82061023e-07
Iter: 1759 loss: 2.82026207e-07
Iter: 1760 loss: 2.81974451e-07
Iter: 1761 loss: 2.81966749e-07
Iter: 1762 loss: 2.81894302e-07
Iter: 1763 loss: 2.81859116e-07
Iter: 1764 loss: 2.81846553e-07
Iter: 1765 loss: 2.8186156e-07
Iter: 1766 loss: 2.81810941e-07
Iter: 1767 loss: 2.8177277e-07
Iter: 1768 loss: 2.81729854e-07
Iter: 1769 loss: 2.8171371e-07
Iter: 1770 loss: 2.81668235e-07
Iter: 1771 loss: 2.81666161e-07
Iter: 1772 loss: 2.81610454e-07
Iter: 1773 loss: 2.81620146e-07
Iter: 1774 loss: 2.81605281e-07
Iter: 1775 loss: 2.81585898e-07
Iter: 1776 loss: 2.81529708e-07
Iter: 1777 loss: 2.82019812e-07
Iter: 1778 loss: 2.81510779e-07
Iter: 1779 loss: 2.81455129e-07
Iter: 1780 loss: 2.81496057e-07
Iter: 1781 loss: 2.81400787e-07
Iter: 1782 loss: 2.81317625e-07
Iter: 1783 loss: 2.81902715e-07
Iter: 1784 loss: 2.81321832e-07
Iter: 1785 loss: 2.81224715e-07
Iter: 1786 loss: 2.81737641e-07
Iter: 1787 loss: 2.8121957e-07
Iter: 1788 loss: 2.81154172e-07
Iter: 1789 loss: 2.81182281e-07
Iter: 1790 loss: 2.81137147e-07
Iter: 1791 loss: 2.81050518e-07
Iter: 1792 loss: 2.81479288e-07
Iter: 1793 loss: 2.81053616e-07
Iter: 1794 loss: 2.8099177e-07
Iter: 1795 loss: 2.80966049e-07
Iter: 1796 loss: 2.80944846e-07
Iter: 1797 loss: 2.80882915e-07
Iter: 1798 loss: 2.80938366e-07
Iter: 1799 loss: 2.80865692e-07
Iter: 1800 loss: 2.80791681e-07
Iter: 1801 loss: 2.80693968e-07
Iter: 1802 loss: 2.80682e-07
Iter: 1803 loss: 2.80664977e-07
Iter: 1804 loss: 2.80593554e-07
Iter: 1805 loss: 2.80555554e-07
Iter: 1806 loss: 2.80442634e-07
Iter: 1807 loss: 2.81223663e-07
Iter: 1808 loss: 2.80424217e-07
Iter: 1809 loss: 2.80282535e-07
Iter: 1810 loss: 2.80626494e-07
Iter: 1811 loss: 2.8022626e-07
Iter: 1812 loss: 2.80177971e-07
Iter: 1813 loss: 2.80146025e-07
Iter: 1814 loss: 2.80105695e-07
Iter: 1815 loss: 2.79990559e-07
Iter: 1816 loss: 2.81418295e-07
Iter: 1817 loss: 2.7998334e-07
Iter: 1818 loss: 2.79926837e-07
Iter: 1819 loss: 2.79926155e-07
Iter: 1820 loss: 2.79862547e-07
Iter: 1821 loss: 2.80093218e-07
Iter: 1822 loss: 2.7985493e-07
Iter: 1823 loss: 2.7981406e-07
Iter: 1824 loss: 2.79717142e-07
Iter: 1825 loss: 2.8072111e-07
Iter: 1826 loss: 2.79709184e-07
Iter: 1827 loss: 2.79694518e-07
Iter: 1828 loss: 2.79656717e-07
Iter: 1829 loss: 2.79604734e-07
Iter: 1830 loss: 2.7952575e-07
Iter: 1831 loss: 2.79528308e-07
Iter: 1832 loss: 2.79461403e-07
Iter: 1833 loss: 2.79408624e-07
Iter: 1834 loss: 2.79364087e-07
Iter: 1835 loss: 2.79271035e-07
Iter: 1836 loss: 2.79991156e-07
Iter: 1837 loss: 2.79267198e-07
Iter: 1838 loss: 2.79197621e-07
Iter: 1839 loss: 2.79372045e-07
Iter: 1840 loss: 2.79167352e-07
Iter: 1841 loss: 2.79064835e-07
Iter: 1842 loss: 2.79174344e-07
Iter: 1843 loss: 2.79023936e-07
Iter: 1844 loss: 2.78950125e-07
Iter: 1845 loss: 2.7907214e-07
Iter: 1846 loss: 2.78933612e-07
Iter: 1847 loss: 2.78810035e-07
Iter: 1848 loss: 2.79559117e-07
Iter: 1849 loss: 2.78792129e-07
Iter: 1850 loss: 2.78714765e-07
Iter: 1851 loss: 2.78548953e-07
Iter: 1852 loss: 2.81559181e-07
Iter: 1853 loss: 2.78553557e-07
Iter: 1854 loss: 2.78582036e-07
Iter: 1855 loss: 2.78499527e-07
Iter: 1856 loss: 2.78466388e-07
Iter: 1857 loss: 2.78399398e-07
Iter: 1858 loss: 2.78418867e-07
Iter: 1859 loss: 2.78344913e-07
Iter: 1860 loss: 2.78368589e-07
Iter: 1861 loss: 2.78312427e-07
Iter: 1862 loss: 2.78264167e-07
Iter: 1863 loss: 2.7842998e-07
Iter: 1864 loss: 2.78257346e-07
Iter: 1865 loss: 2.78185439e-07
Iter: 1866 loss: 2.78201895e-07
Iter: 1867 loss: 2.78141272e-07
Iter: 1868 loss: 2.78059986e-07
Iter: 1869 loss: 2.78142466e-07
Iter: 1870 loss: 2.78008429e-07
Iter: 1871 loss: 2.77906338e-07
Iter: 1872 loss: 2.77830765e-07
Iter: 1873 loss: 2.77811353e-07
Iter: 1874 loss: 2.7761152e-07
Iter: 1875 loss: 2.7800462e-07
Iter: 1876 loss: 2.7756667e-07
Iter: 1877 loss: 2.77431e-07
Iter: 1878 loss: 2.78291679e-07
Iter: 1879 loss: 2.77421861e-07
Iter: 1880 loss: 2.77345407e-07
Iter: 1881 loss: 2.77467677e-07
Iter: 1882 loss: 2.77330741e-07
Iter: 1883 loss: 2.77217765e-07
Iter: 1884 loss: 2.77666857e-07
Iter: 1885 loss: 2.77190509e-07
Iter: 1886 loss: 2.77139122e-07
Iter: 1887 loss: 2.77176838e-07
Iter: 1888 loss: 2.77114566e-07
Iter: 1889 loss: 2.7702913e-07
Iter: 1890 loss: 2.77582103e-07
Iter: 1891 loss: 2.77015829e-07
Iter: 1892 loss: 2.76951482e-07
Iter: 1893 loss: 2.76840524e-07
Iter: 1894 loss: 2.76842115e-07
Iter: 1895 loss: 2.76726638e-07
Iter: 1896 loss: 2.77356236e-07
Iter: 1897 loss: 2.76687757e-07
Iter: 1898 loss: 2.7657407e-07
Iter: 1899 loss: 2.77110274e-07
Iter: 1900 loss: 2.76567334e-07
Iter: 1901 loss: 2.76426817e-07
Iter: 1902 loss: 2.76895207e-07
Iter: 1903 loss: 2.76393223e-07
Iter: 1904 loss: 2.76330979e-07
Iter: 1905 loss: 2.76815143e-07
Iter: 1906 loss: 2.76317081e-07
Iter: 1907 loss: 2.76256316e-07
Iter: 1908 loss: 2.7613811e-07
Iter: 1909 loss: 2.77869731e-07
Iter: 1910 loss: 2.76113099e-07
Iter: 1911 loss: 2.75965675e-07
Iter: 1912 loss: 2.76988544e-07
Iter: 1913 loss: 2.75954e-07
Iter: 1914 loss: 2.758e-07
Iter: 1915 loss: 2.76139275e-07
Iter: 1916 loss: 2.75750807e-07
Iter: 1917 loss: 2.75631407e-07
Iter: 1918 loss: 2.76862579e-07
Iter: 1919 loss: 2.75640701e-07
Iter: 1920 loss: 2.75554811e-07
Iter: 1921 loss: 2.75538127e-07
Iter: 1922 loss: 2.75461275e-07
Iter: 1923 loss: 2.75427936e-07
Iter: 1924 loss: 2.75422678e-07
Iter: 1925 loss: 2.7537746e-07
Iter: 1926 loss: 2.75291654e-07
Iter: 1927 loss: 2.76971548e-07
Iter: 1928 loss: 2.75307741e-07
Iter: 1929 loss: 2.75197834e-07
Iter: 1930 loss: 2.7535242e-07
Iter: 1931 loss: 2.75149745e-07
Iter: 1932 loss: 2.75050354e-07
Iter: 1933 loss: 2.75648887e-07
Iter: 1934 loss: 2.75045835e-07
Iter: 1935 loss: 2.74909553e-07
Iter: 1936 loss: 2.75213608e-07
Iter: 1937 loss: 2.74877834e-07
Iter: 1938 loss: 2.74727427e-07
Iter: 1939 loss: 2.74865442e-07
Iter: 1940 loss: 2.74633123e-07
Iter: 1941 loss: 2.74439401e-07
Iter: 1942 loss: 2.75128457e-07
Iter: 1943 loss: 2.74400861e-07
Iter: 1944 loss: 2.74298e-07
Iter: 1945 loss: 2.74255768e-07
Iter: 1946 loss: 2.74207707e-07
Iter: 1947 loss: 2.74056191e-07
Iter: 1948 loss: 2.75648603e-07
Iter: 1949 loss: 2.74057896e-07
Iter: 1950 loss: 2.73983488e-07
Iter: 1951 loss: 2.7410718e-07
Iter: 1952 loss: 2.73950462e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4
+ date
Wed Oct 21 21:31:41 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/500_500_500_500_1 --function f1 --psi 2 --phi 0.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e3bb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e4b36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e4b3378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e3ad510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e3e9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e2eab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e2ab158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e283598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e283488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e2ab400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e345a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e369620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e369a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e1b0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e1b0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e1660d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e166730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e1669d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e1e0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e166378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f653e2297b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f651cc53a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f651cc826a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f651cb98378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f651cb98a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f651cb52730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f651cbce840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f651cbe3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f64f83d0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f64f83dd378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f651cc10378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f64f835c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f64f835c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f64f83629d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f64f832da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f64f82d8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.0456969
test_loss: 0.04441183
train_loss: 0.019316312
test_loss: 0.018266281
train_loss: 0.00997122
test_loss: 0.010237393
train_loss: 0.007422015
test_loss: 0.007436126
train_loss: 0.0057903575
test_loss: 0.006311771
train_loss: 0.004888648
test_loss: 0.0049237353
train_loss: 0.0039715637
test_loss: 0.0042894124
train_loss: 0.003682652
test_loss: 0.0040192865
train_loss: 0.00358576
test_loss: 0.0037558945
train_loss: 0.0033268016
test_loss: 0.003625452
train_loss: 0.0033495233
test_loss: 0.003463061
train_loss: 0.0030650923
test_loss: 0.003242144
train_loss: 0.0034821648
test_loss: 0.0035467204
train_loss: 0.0029453796
test_loss: 0.0031035014
train_loss: 0.0026567462
test_loss: 0.0029392468
train_loss: 0.0027891542
test_loss: 0.0032324458
train_loss: 0.0030704346
test_loss: 0.0032866772
train_loss: 0.0027328117
test_loss: 0.0033608573
train_loss: 0.0026520018
test_loss: 0.0028672405
train_loss: 0.0026825934
test_loss: 0.0029621313
train_loss: 0.0026964971
test_loss: 0.0029397411
train_loss: 0.0025941213
test_loss: 0.0028064856
train_loss: 0.0028419257
test_loss: 0.0030944122
train_loss: 0.0027533202
test_loss: 0.002810215
train_loss: 0.002641893
test_loss: 0.0028732095
train_loss: 0.0024797223
test_loss: 0.0027493294
train_loss: 0.0024343415
test_loss: 0.0026680036
train_loss: 0.0030324266
test_loss: 0.0031266133
train_loss: 0.0026405002
test_loss: 0.0029484082
train_loss: 0.0024487497
test_loss: 0.0027476447
train_loss: 0.0023532906
test_loss: 0.0026517026
train_loss: 0.0023982725
test_loss: 0.002789542
train_loss: 0.0025288246
test_loss: 0.002608327
train_loss: 0.0023791892
test_loss: 0.002530731
train_loss: 0.0025040878
test_loss: 0.0027914455
train_loss: 0.0023396583
test_loss: 0.0027051156
train_loss: 0.0025330014
test_loss: 0.0026148253
train_loss: 0.0024442128
test_loss: 0.0025436478
train_loss: 0.0026745964
test_loss: 0.0028315778
train_loss: 0.002431965
test_loss: 0.002629492
train_loss: 0.0022351504
test_loss: 0.00266033
train_loss: 0.002270195
test_loss: 0.0025382908
train_loss: 0.0023026385
test_loss: 0.0025286104
train_loss: 0.002660456
test_loss: 0.0026074927
train_loss: 0.0023131329
test_loss: 0.0024991413
train_loss: 0.0022935034
test_loss: 0.002509215
train_loss: 0.002299962
test_loss: 0.00257839
train_loss: 0.002309545
test_loss: 0.0024388772
train_loss: 0.002244938
test_loss: 0.0024173297
train_loss: 0.0022833592
test_loss: 0.002770564
train_loss: 0.0022967877
test_loss: 0.002569073
train_loss: 0.002390032
test_loss: 0.0027093592
train_loss: 0.0022098457
test_loss: 0.002583445
train_loss: 0.0023352662
test_loss: 0.0025786706
train_loss: 0.0022323965
test_loss: 0.0024102763
train_loss: 0.002298377
test_loss: 0.0024381909
train_loss: 0.0023054383
test_loss: 0.0027100255
train_loss: 0.0024318055
test_loss: 0.0026094066
train_loss: 0.002229839
test_loss: 0.0026876794
train_loss: 0.0023024115
test_loss: 0.0025418429
train_loss: 0.0024669454
test_loss: 0.002784641
train_loss: 0.0025601804
test_loss: 0.0026400075
train_loss: 0.0025579976
test_loss: 0.0024605931
train_loss: 0.0024151397
test_loss: 0.0027951226
train_loss: 0.00218412
test_loss: 0.0023807052
train_loss: 0.0022547848
test_loss: 0.0026565513
train_loss: 0.0023511325
test_loss: 0.0025104776
train_loss: 0.0023119876
test_loss: 0.0025205042
train_loss: 0.002558138
test_loss: 0.002843037
train_loss: 0.0021749786
test_loss: 0.0027503828
train_loss: 0.0019899444
test_loss: 0.0022743193
train_loss: 0.0021132089
test_loss: 0.0023174349
train_loss: 0.002092672
test_loss: 0.002328585
train_loss: 0.002268907
test_loss: 0.00244509
train_loss: 0.0023718947
test_loss: 0.0024816087
train_loss: 0.0020646383
test_loss: 0.0024706502
train_loss: 0.0022172118
test_loss: 0.0023824666
train_loss: 0.0022726457
test_loss: 0.0025103143
train_loss: 0.002269726
test_loss: 0.002430122
train_loss: 0.0021789207
test_loss: 0.0028867265
train_loss: 0.002351941
test_loss: 0.0024521914
train_loss: 0.0021911934
test_loss: 0.0025031199
train_loss: 0.0024333787
test_loss: 0.0025711386
train_loss: 0.0022240449
test_loss: 0.0024749457
train_loss: 0.0021299955
test_loss: 0.0024211688
train_loss: 0.002103654
test_loss: 0.0025263978
train_loss: 0.0021739148
test_loss: 0.002420858
train_loss: 0.0022474604
test_loss: 0.0024685187
train_loss: 0.0022003413
test_loss: 0.002482479
train_loss: 0.002355033
test_loss: 0.0024713406
train_loss: 0.0021830783
test_loss: 0.002396562
train_loss: 0.0019882114
test_loss: 0.0024819446
train_loss: 0.0020576292
test_loss: 0.0023473331
train_loss: 0.0022853848
test_loss: 0.0026947656
train_loss: 0.0023388239
test_loss: 0.0025923008
train_loss: 0.0020100153
test_loss: 0.00240119
train_loss: 0.002112247
test_loss: 0.002492172
train_loss: 0.002295185
test_loss: 0.0028295284
train_loss: 0.0021052624
test_loss: 0.002405121
train_loss: 0.0021986363
test_loss: 0.00249181
train_loss: 0.0021989003
test_loss: 0.0024849095
train_loss: 0.0020758356
test_loss: 0.002429182
train_loss: 0.002180648
test_loss: 0.0023768437
train_loss: 0.0020798536
test_loss: 0.0023409252
train_loss: 0.002101355
test_loss: 0.002377048
train_loss: 0.0020703191
test_loss: 0.0025316954
train_loss: 0.0019837
test_loss: 0.00252051
train_loss: 0.0021193454
test_loss: 0.0022322286
train_loss: 0.0022836654
test_loss: 0.0025400298
train_loss: 0.0021348768
test_loss: 0.0023862799
train_loss: 0.0021624544
test_loss: 0.0024410232
train_loss: 0.00217043
test_loss: 0.002242356
train_loss: 0.002193814
test_loss: 0.0023058394
train_loss: 0.0022379663
test_loss: 0.0026323865
train_loss: 0.002314545
test_loss: 0.0024059522
train_loss: 0.0022020973
test_loss: 0.0024140647
train_loss: 0.0020070337
test_loss: 0.0025171007
train_loss: 0.0021377658
test_loss: 0.0025093935
train_loss: 0.0022012996
test_loss: 0.0024295982
train_loss: 0.0020981098
test_loss: 0.0023533362
train_loss: 0.00217949
test_loss: 0.0023950082
train_loss: 0.0018767591
test_loss: 0.0022788625
train_loss: 0.0023831932
test_loss: 0.0025867918
train_loss: 0.0021450566
test_loss: 0.0024816785
train_loss: 0.0025537328
test_loss: 0.0027791013
train_loss: 0.0022458017
test_loss: 0.0024663054
train_loss: 0.0023413738
test_loss: 0.002423577
train_loss: 0.0022139458
test_loss: 0.002588683
train_loss: 0.0022704124
test_loss: 0.0023291253
train_loss: 0.002251353
test_loss: 0.0024194634
train_loss: 0.0020712554
test_loss: 0.002417396
train_loss: 0.0022253564
test_loss: 0.0025135453
train_loss: 0.0020560143
test_loss: 0.002331834
train_loss: 0.001971953
test_loss: 0.0024057364
train_loss: 0.0020518056
test_loss: 0.0022695418
train_loss: 0.002268894
test_loss: 0.002318111
train_loss: 0.0019536158
test_loss: 0.002188241
train_loss: 0.0019565918
test_loss: 0.0022989158
train_loss: 0.0021865906
test_loss: 0.002434986
train_loss: 0.0020116607
test_loss: 0.002181358
train_loss: 0.0022230726
test_loss: 0.0023849024
train_loss: 0.0020893754
test_loss: 0.002427078
train_loss: 0.0020862164
test_loss: 0.00247393
train_loss: 0.002302689
test_loss: 0.0023816728
train_loss: 0.002104599
test_loss: 0.0023773743
train_loss: 0.0022106187
test_loss: 0.0024559523
train_loss: 0.0020934446
test_loss: 0.002458681
train_loss: 0.002086651
test_loss: 0.0025037043
train_loss: 0.0021178296
test_loss: 0.0023957982
train_loss: 0.0020379797
test_loss: 0.0022687912
train_loss: 0.002095099
test_loss: 0.0024221663
train_loss: 0.0021261752
test_loss: 0.0026505264
train_loss: 0.0018798916
test_loss: 0.0023521152
train_loss: 0.0020136642
test_loss: 0.002379442
train_loss: 0.0020569467
test_loss: 0.0023197015
train_loss: 0.0022171182
test_loss: 0.0024849817
train_loss: 0.0020356034
test_loss: 0.0024702523
train_loss: 0.0020245377
test_loss: 0.0024977482
train_loss: 0.0019382241
test_loss: 0.0025408631
train_loss: 0.0020741513
test_loss: 0.002356889
train_loss: 0.0022057355
test_loss: 0.0022392878
train_loss: 0.0023283302
test_loss: 0.0023400548
train_loss: 0.0020325044
test_loss: 0.0022648945
train_loss: 0.0023232743
test_loss: 0.0023979465
train_loss: 0.0020831146
test_loss: 0.0025335348
train_loss: 0.0022528288
test_loss: 0.0022852493
train_loss: 0.0020852285
test_loss: 0.0026028838
train_loss: 0.0019991668
test_loss: 0.0021524618
train_loss: 0.0019545474
test_loss: 0.0023325745
train_loss: 0.0018421947
test_loss: 0.0022837603
train_loss: 0.0018218065
test_loss: 0.0022119568
train_loss: 0.0020350248
test_loss: 0.0023725934
train_loss: 0.0019784346
test_loss: 0.002310938
train_loss: 0.0019810605
test_loss: 0.0022199333
train_loss: 0.0020829663
test_loss: 0.0026199634
train_loss: 0.0018681228
test_loss: 0.002112015
train_loss: 0.0019830833
test_loss: 0.0023828738
train_loss: 0.0020898683
test_loss: 0.0022739554
train_loss: 0.0018818504
test_loss: 0.0022508681
train_loss: 0.0020535851
test_loss: 0.0023305265
train_loss: 0.0021152901
test_loss: 0.0023251132
train_loss: 0.002166053
test_loss: 0.0022767892
train_loss: 0.002096234
test_loss: 0.002282332
train_loss: 0.0020685534
test_loss: 0.002393797
train_loss: 0.0019068903
test_loss: 0.0023393251
train_loss: 0.0020981457
test_loss: 0.002404569
train_loss: 0.0021072007
test_loss: 0.0023667994
train_loss: 0.0019553322
test_loss: 0.002430797
train_loss: 0.0020030146
test_loss: 0.0022891092
train_loss: 0.0020009156
test_loss: 0.0024326658
train_loss: 0.0020033447
test_loss: 0.0022347264
train_loss: 0.0020994048
test_loss: 0.0027019626
train_loss: 0.0020261677
test_loss: 0.0024175828
train_loss: 0.002252268
test_loss: 0.0026229648
train_loss: 0.002118971
test_loss: 0.0028200115
train_loss: 0.0019818807
test_loss: 0.002478318
train_loss: 0.0019278688
test_loss: 0.0022882551
train_loss: 0.001975428
test_loss: 0.0023399773
train_loss: 0.002013493
test_loss: 0.0024470338
train_loss: 0.0018745582
test_loss: 0.0022198695
train_loss: 0.0021434783
test_loss: 0.002416654
train_loss: 0.0021016398
test_loss: 0.002423738
train_loss: 0.0019775261
test_loss: 0.0023268443
train_loss: 0.0019080951
test_loss: 0.0023815657
train_loss: 0.0020314164
test_loss: 0.0022334473
train_loss: 0.0021240287
test_loss: 0.0024132708
train_loss: 0.0019621588
test_loss: 0.0022745572
train_loss: 0.002101805
test_loss: 0.0023419
train_loss: 0.002197192
test_loss: 0.0023446288
train_loss: 0.0018894507
test_loss: 0.0022944869
train_loss: 0.0019926343
test_loss: 0.0022160641
train_loss: 0.0020124502
test_loss: 0.002365897
train_loss: 0.0018112537
test_loss: 0.0022113868
train_loss: 0.0020945075
test_loss: 0.00224506
train_loss: 0.0019801885
test_loss: 0.0021984838
train_loss: 0.0019250297
test_loss: 0.0022346135
train_loss: 0.0020782775
test_loss: 0.0024338707
train_loss: 0.0022171496
test_loss: 0.0024032209
train_loss: 0.002012455
test_loss: 0.002374669
train_loss: 0.002274543
test_loss: 0.0024112181
train_loss: 0.0019983575
test_loss: 0.0022521343
train_loss: 0.0021583028
test_loss: 0.0024817341
train_loss: 0.0021062167
test_loss: 0.0023365612
train_loss: 0.001955062
test_loss: 0.0024468622
train_loss: 0.0022574924
test_loss: 0.0023303023
train_loss: 0.0019020597
test_loss: 0.0022853077
train_loss: 0.0019177997
test_loss: 0.002298741
train_loss: 0.0024083972
test_loss: 0.0022620587
train_loss: 0.0021177903
test_loss: 0.0024097487
train_loss: 0.0019121143
test_loss: 0.00222814
train_loss: 0.0020021338
test_loss: 0.002305261
train_loss: 0.002002926
test_loss: 0.0024092055
train_loss: 0.0019958904
test_loss: 0.002264305
train_loss: 0.002048593
test_loss: 0.002272057
train_loss: 0.0020875698
test_loss: 0.0024493574
train_loss: 0.0020004357
test_loss: 0.0024850736
train_loss: 0.0019059619
test_loss: 0.0023554042
train_loss: 0.0019069555
test_loss: 0.0022655104
train_loss: 0.0019622592
test_loss: 0.0022059612
train_loss: 0.001938884
test_loss: 0.0022553697
train_loss: 0.0020421592
test_loss: 0.002380142
train_loss: 0.0019464369
test_loss: 0.002407175
train_loss: 0.0020066092
test_loss: 0.0023057181
train_loss: 0.00201242
test_loss: 0.002251224
train_loss: 0.0024148116
test_loss: 0.00258309
train_loss: 0.0020066733
test_loss: 0.0025580835
train_loss: 0.0019745084
test_loss: 0.0024274078
train_loss: 0.0018303723
test_loss: 0.0022053295
train_loss: 0.0019261597
test_loss: 0.002349371
train_loss: 0.0020508086
test_loss: 0.0024848925
train_loss: 0.0023415922
test_loss: 0.00230706
train_loss: 0.0019800814
test_loss: 0.0021214928
train_loss: 0.0021289904
test_loss: 0.0022241382
train_loss: 0.0019561164
test_loss: 0.002255186
train_loss: 0.0017514975
test_loss: 0.0024151506
train_loss: 0.0019433245
test_loss: 0.0023678285
train_loss: 0.00191918
test_loss: 0.0022142986
train_loss: 0.0018543894
test_loss: 0.002202151
train_loss: 0.0018678581
test_loss: 0.0021734536
train_loss: 0.0017949264
test_loss: 0.002199758
train_loss: 0.0020358106
test_loss: 0.0021598511
train_loss: 0.0020832703
test_loss: 0.00233974
train_loss: 0.0018247504
test_loss: 0.00223314
train_loss: 0.0020566843
test_loss: 0.0023600718
train_loss: 0.001832941
test_loss: 0.002447044
train_loss: 0.0026236405
test_loss: 0.0023104493
train_loss: 0.0020041622
test_loss: 0.002249681
train_loss: 0.0019707752
test_loss: 0.0024568515
train_loss: 0.002005366
test_loss: 0.0024126354
train_loss: 0.0019903365
test_loss: 0.002185451
train_loss: 0.0018917273
test_loss: 0.0022889748
train_loss: 0.0020203697
test_loss: 0.0021864101
train_loss: 0.0019030771
test_loss: 0.0023946695
train_loss: 0.0018947828
test_loss: 0.002168282
train_loss: 0.0020007857
test_loss: 0.0023681805
train_loss: 0.0019317653
test_loss: 0.00231776
train_loss: 0.0017857528
test_loss: 0.0021550872
train_loss: 0.0020449143
test_loss: 0.0023459971
train_loss: 0.0018938219
test_loss: 0.0023178312
train_loss: 0.0019276396
test_loss: 0.0021107984
train_loss: 0.0020303875
test_loss: 0.0023647363
train_loss: 0.0019942163
test_loss: 0.002285189
train_loss: 0.0019179755
test_loss: 0.002169578
train_loss: 0.0020624397
test_loss: 0.0022409149
train_loss: 0.001942692
test_loss: 0.0021260923
train_loss: 0.002005273
test_loss: 0.0022805221
train_loss: 0.0017986492
test_loss: 0.00227857
train_loss: 0.0019145464
test_loss: 0.0022411665
train_loss: 0.0018084198
test_loss: 0.0023223537
train_loss: 0.002052029
test_loss: 0.0021765996
train_loss: 0.0019236163
test_loss: 0.0024406994
train_loss: 0.0019786493
test_loss: 0.0022020794
train_loss: 0.0019832917
test_loss: 0.0022417866
train_loss: 0.001956462
test_loss: 0.0022447906
train_loss: 0.0019872447
test_loss: 0.0024088104
train_loss: 0.0019104707
test_loss: 0.0023111661
train_loss: 0.0022800758
test_loss: 0.0023477064
train_loss: 0.0019388538
test_loss: 0.002127298
train_loss: 0.002051044
test_loss: 0.0022385316
train_loss: 0.0019745883
test_loss: 0.0022313793
train_loss: 0.0018889916
test_loss: 0.0022402406
train_loss: 0.0020019975
test_loss: 0.0022794772
train_loss: 0.0019619474
test_loss: 0.0022516185
train_loss: 0.0019772246
test_loss: 0.0023203245
train_loss: 0.0018571472
test_loss: 0.002244971
train_loss: 0.001962108
test_loss: 0.0021844215
train_loss: 0.001742498
test_loss: 0.0021321143
train_loss: 0.0018822785
test_loss: 0.0021723527
train_loss: 0.0022091577
test_loss: 0.00223683
train_loss: 0.0019438034
test_loss: 0.0024149697
train_loss: 0.0021195714
test_loss: 0.0022343274
train_loss: 0.0018818029
test_loss: 0.0022294044
train_loss: 0.0020051429
test_loss: 0.0022585257
train_loss: 0.002190897
test_loss: 0.0023506093
train_loss: 0.0018791805
test_loss: 0.0021739479
train_loss: 0.0020851989
test_loss: 0.0024955547
train_loss: 0.0019866675
test_loss: 0.0021419812
train_loss: 0.0018611446
test_loss: 0.0023693533
train_loss: 0.0019304124
test_loss: 0.0022932303
train_loss: 0.0018910528
test_loss: 0.0022087363
train_loss: 0.0021016013
test_loss: 0.0022502746
train_loss: 0.0019721673
test_loss: 0.002406846
train_loss: 0.0018889024
test_loss: 0.0022552952
train_loss: 0.0019532528
test_loss: 0.0022753135
train_loss: 0.0019803601
test_loss: 0.002378495
train_loss: 0.0019699195
test_loss: 0.0021887035
train_loss: 0.0021149039
test_loss: 0.00213408
train_loss: 0.0018865231
test_loss: 0.0022777228
train_loss: 0.0019104782
test_loss: 0.0022004514
train_loss: 0.0019231741
test_loss: 0.0021549112
train_loss: 0.0017288341
test_loss: 0.0021734561
train_loss: 0.0017069152
test_loss: 0.0021284237
train_loss: 0.0019263928
test_loss: 0.0022091148
train_loss: 0.001936283
test_loss: 0.0022857566
train_loss: 0.0019814484
test_loss: 0.0027334122
train_loss: 0.0020754593
test_loss: 0.00225162
train_loss: 0.0019270058
test_loss: 0.0021620584
train_loss: 0.0018581649
test_loss: 0.0021622065
train_loss: 0.001781476/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0021685916
train_loss: 0.0018677497
test_loss: 0.002266556
train_loss: 0.0018317592
test_loss: 0.0020705059
train_loss: 0.0017098561
test_loss: 0.0022056182
train_loss: 0.0020170128
test_loss: 0.002380545
train_loss: 0.0017617778
test_loss: 0.0021677995
train_loss: 0.001959893
test_loss: 0.00219272
train_loss: 0.0019914876
test_loss: 0.0022329942
train_loss: 0.0020456424
test_loss: 0.0022462434
train_loss: 0.0020183069
test_loss: 0.0022755566
train_loss: 0.0022501573
test_loss: 0.0023719603
train_loss: 0.0018458029
test_loss: 0.0023550107
train_loss: 0.0018754869
test_loss: 0.0022987644
train_loss: 0.0017895544
test_loss: 0.0021230045
train_loss: 0.0019155617
test_loss: 0.0021676053
train_loss: 0.0020178203
test_loss: 0.0021972356
train_loss: 0.0021484103
test_loss: 0.0025556106
train_loss: 0.0020083948
test_loss: 0.0023082707
train_loss: 0.0022271124
test_loss: 0.0024074567
train_loss: 0.0018012671
test_loss: 0.0021978933
train_loss: 0.0017562539
test_loss: 0.0021264288
train_loss: 0.002051681
test_loss: 0.0023278706
train_loss: 0.0018280067
test_loss: 0.002221437
train_loss: 0.0018382923
test_loss: 0.0024561686
train_loss: 0.0018730677
test_loss: 0.0022259182
train_loss: 0.0019409915
test_loss: 0.0024870646
train_loss: 0.0020700912
test_loss: 0.0021895804
train_loss: 0.0019124019
test_loss: 0.0024066654
train_loss: 0.0019244784
test_loss: 0.0023852065
train_loss: 0.0018530966
test_loss: 0.0023174211
train_loss: 0.002000327
test_loss: 0.0020954395
train_loss: 0.0017715599
test_loss: 0.0021757782
train_loss: 0.0018446578
test_loss: 0.0021574786
train_loss: 0.0019371468
test_loss: 0.002329193
train_loss: 0.0018659061
test_loss: 0.0022149272
train_loss: 0.0021424561
test_loss: 0.0022915301
train_loss: 0.0017523884
test_loss: 0.00217126
train_loss: 0.0018600123
test_loss: 0.0021422429
train_loss: 0.0018141138
test_loss: 0.0023690816
train_loss: 0.0018202973
test_loss: 0.0023273176
train_loss: 0.0018047988
test_loss: 0.0021064254
train_loss: 0.0017874419
test_loss: 0.0022088801
train_loss: 0.001859013
test_loss: 0.002240261
train_loss: 0.0018356696
test_loss: 0.0021435893
train_loss: 0.002073313
test_loss: 0.0023355207
train_loss: 0.0018527666
test_loss: 0.0020895973
train_loss: 0.001813064
test_loss: 0.002117444
train_loss: 0.0019486747
test_loss: 0.0021423858
train_loss: 0.0021096189
test_loss: 0.0026335584
train_loss: 0.001966574
test_loss: 0.0021667622
train_loss: 0.0019414793
test_loss: 0.00208122
train_loss: 0.0019560158
test_loss: 0.0023162616
train_loss: 0.0021285636
test_loss: 0.002217163
train_loss: 0.0018751307
test_loss: 0.0022151137
train_loss: 0.001794649
test_loss: 0.0022439526
train_loss: 0.0017412023
test_loss: 0.0022458585
train_loss: 0.0019859378
test_loss: 0.00220988
train_loss: 0.0017687213
test_loss: 0.0022277767
train_loss: 0.0018729111
test_loss: 0.002070637
train_loss: 0.0019999584
test_loss: 0.0020800615
train_loss: 0.0022014724
test_loss: 0.0021919045
train_loss: 0.0017424915
test_loss: 0.0021499074
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54646ff1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54647bff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54647cb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5464730d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5464739e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54646bf400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5464672f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f546460c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f546462d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54645ecf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f546462d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f546460cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54645c19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f546455b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f546451e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f546451e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54645446a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54644e4c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54644bc048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54644bcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5407fbf510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5407f72a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5407f72f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5407f50048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5407f50b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5407f04b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5407ec8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5407ec8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5407ef06a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5407ef0bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53e07458c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53e06e68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53e06fca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53e06f7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53e06cfa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f53e06d2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.13991472e-06
Iter: 2 loss: 4.33437526e-06
Iter: 3 loss: 4.32747265e-06
Iter: 4 loss: 3.94715971e-06
Iter: 5 loss: 3.93497749e-06
Iter: 6 loss: 3.63937488e-06
Iter: 7 loss: 3.42102976e-06
Iter: 8 loss: 5.05966909e-06
Iter: 9 loss: 3.40393308e-06
Iter: 10 loss: 3.16955175e-06
Iter: 11 loss: 3.44091222e-06
Iter: 12 loss: 3.04430637e-06
Iter: 13 loss: 2.95204313e-06
Iter: 14 loss: 2.84072939e-06
Iter: 15 loss: 2.82983729e-06
Iter: 16 loss: 2.76941932e-06
Iter: 17 loss: 2.73772639e-06
Iter: 18 loss: 2.66597681e-06
Iter: 19 loss: 2.59315357e-06
Iter: 20 loss: 2.57905936e-06
Iter: 21 loss: 2.48477318e-06
Iter: 22 loss: 2.39171e-06
Iter: 23 loss: 2.37187442e-06
Iter: 24 loss: 2.30074465e-06
Iter: 25 loss: 2.28209319e-06
Iter: 26 loss: 2.21826349e-06
Iter: 27 loss: 2.46402692e-06
Iter: 28 loss: 2.20317042e-06
Iter: 29 loss: 2.16516423e-06
Iter: 30 loss: 2.10018493e-06
Iter: 31 loss: 2.10012922e-06
Iter: 32 loss: 2.03235209e-06
Iter: 33 loss: 2.43135423e-06
Iter: 34 loss: 2.02364481e-06
Iter: 35 loss: 1.9750621e-06
Iter: 36 loss: 2.00259933e-06
Iter: 37 loss: 1.94345557e-06
Iter: 38 loss: 1.91609593e-06
Iter: 39 loss: 1.90463254e-06
Iter: 40 loss: 1.88016088e-06
Iter: 41 loss: 1.84702094e-06
Iter: 42 loss: 1.8453585e-06
Iter: 43 loss: 1.81784708e-06
Iter: 44 loss: 1.81773066e-06
Iter: 45 loss: 1.78955395e-06
Iter: 46 loss: 1.72523573e-06
Iter: 47 loss: 2.56210524e-06
Iter: 48 loss: 1.72097725e-06
Iter: 49 loss: 1.69454859e-06
Iter: 50 loss: 1.69319549e-06
Iter: 51 loss: 1.66338032e-06
Iter: 52 loss: 1.68009706e-06
Iter: 53 loss: 1.64402093e-06
Iter: 54 loss: 1.60592197e-06
Iter: 55 loss: 1.5252358e-06
Iter: 56 loss: 2.84398538e-06
Iter: 57 loss: 1.52283189e-06
Iter: 58 loss: 1.54458098e-06
Iter: 59 loss: 1.49715606e-06
Iter: 60 loss: 1.47475544e-06
Iter: 61 loss: 1.42463978e-06
Iter: 62 loss: 2.12246528e-06
Iter: 63 loss: 1.42189811e-06
Iter: 64 loss: 1.36960773e-06
Iter: 65 loss: 1.62982144e-06
Iter: 66 loss: 1.36075641e-06
Iter: 67 loss: 1.33413221e-06
Iter: 68 loss: 1.43764373e-06
Iter: 69 loss: 1.32785658e-06
Iter: 70 loss: 1.30460535e-06
Iter: 71 loss: 1.34491074e-06
Iter: 72 loss: 1.29435466e-06
Iter: 73 loss: 1.28906e-06
Iter: 74 loss: 1.28283932e-06
Iter: 75 loss: 1.27282055e-06
Iter: 76 loss: 1.24548615e-06
Iter: 77 loss: 1.42234285e-06
Iter: 78 loss: 1.23863379e-06
Iter: 79 loss: 1.211773e-06
Iter: 80 loss: 1.21153391e-06
Iter: 81 loss: 1.18641765e-06
Iter: 82 loss: 1.17616901e-06
Iter: 83 loss: 1.16281626e-06
Iter: 84 loss: 1.14450381e-06
Iter: 85 loss: 1.17703939e-06
Iter: 86 loss: 1.13642079e-06
Iter: 87 loss: 1.11117583e-06
Iter: 88 loss: 1.31425077e-06
Iter: 89 loss: 1.10949975e-06
Iter: 90 loss: 1.10082215e-06
Iter: 91 loss: 1.09119628e-06
Iter: 92 loss: 1.08985205e-06
Iter: 93 loss: 1.07861933e-06
Iter: 94 loss: 1.13710109e-06
Iter: 95 loss: 1.07683536e-06
Iter: 96 loss: 1.06266657e-06
Iter: 97 loss: 1.11449913e-06
Iter: 98 loss: 1.05917081e-06
Iter: 99 loss: 1.05175627e-06
Iter: 100 loss: 1.03298441e-06
Iter: 101 loss: 1.1995611e-06
Iter: 102 loss: 1.0300198e-06
Iter: 103 loss: 1.00215686e-06
Iter: 104 loss: 1.12796579e-06
Iter: 105 loss: 9.96742529e-07
Iter: 106 loss: 9.8071223e-07
Iter: 107 loss: 1.11340501e-06
Iter: 108 loss: 9.79763854e-07
Iter: 109 loss: 9.68955e-07
Iter: 110 loss: 9.68931658e-07
Iter: 111 loss: 9.61145929e-07
Iter: 112 loss: 9.57601173e-07
Iter: 113 loss: 9.53735366e-07
Iter: 114 loss: 9.47705814e-07
Iter: 115 loss: 1.03189814e-06
Iter: 116 loss: 9.47748276e-07
Iter: 117 loss: 9.41900623e-07
Iter: 118 loss: 9.38531798e-07
Iter: 119 loss: 9.36069739e-07
Iter: 120 loss: 9.28853638e-07
Iter: 121 loss: 9.30627834e-07
Iter: 122 loss: 9.23504729e-07
Iter: 123 loss: 9.19283821e-07
Iter: 124 loss: 9.18587716e-07
Iter: 125 loss: 9.1338552e-07
Iter: 126 loss: 8.99274937e-07
Iter: 127 loss: 9.87078238e-07
Iter: 128 loss: 8.95544872e-07
Iter: 129 loss: 8.83220707e-07
Iter: 130 loss: 9.92675723e-07
Iter: 131 loss: 8.82640052e-07
Iter: 132 loss: 8.77863101e-07
Iter: 133 loss: 8.77488787e-07
Iter: 134 loss: 8.72245323e-07
Iter: 135 loss: 8.586332e-07
Iter: 136 loss: 9.66482162e-07
Iter: 137 loss: 8.56081328e-07
Iter: 138 loss: 8.46425962e-07
Iter: 139 loss: 9.18192143e-07
Iter: 140 loss: 8.45635554e-07
Iter: 141 loss: 8.36600066e-07
Iter: 142 loss: 8.25700056e-07
Iter: 143 loss: 8.24610481e-07
Iter: 144 loss: 8.21883305e-07
Iter: 145 loss: 8.17594241e-07
Iter: 146 loss: 8.10957204e-07
Iter: 147 loss: 8.29661872e-07
Iter: 148 loss: 8.08861614e-07
Iter: 149 loss: 8.04440504e-07
Iter: 150 loss: 8.01709518e-07
Iter: 151 loss: 7.99921793e-07
Iter: 152 loss: 7.93985464e-07
Iter: 153 loss: 7.93951585e-07
Iter: 154 loss: 7.916301e-07
Iter: 155 loss: 7.85283305e-07
Iter: 156 loss: 8.2437748e-07
Iter: 157 loss: 7.83636267e-07
Iter: 158 loss: 7.78684e-07
Iter: 159 loss: 7.78194135e-07
Iter: 160 loss: 7.73289571e-07
Iter: 161 loss: 7.78667868e-07
Iter: 162 loss: 7.70563815e-07
Iter: 163 loss: 7.6621734e-07
Iter: 164 loss: 7.59471504e-07
Iter: 165 loss: 7.59350826e-07
Iter: 166 loss: 7.51304e-07
Iter: 167 loss: 8.70004669e-07
Iter: 168 loss: 7.51278e-07
Iter: 169 loss: 7.44654244e-07
Iter: 170 loss: 7.58410351e-07
Iter: 171 loss: 7.41980841e-07
Iter: 172 loss: 7.38648e-07
Iter: 173 loss: 7.33025956e-07
Iter: 174 loss: 7.33003276e-07
Iter: 175 loss: 7.26107146e-07
Iter: 176 loss: 7.62962259e-07
Iter: 177 loss: 7.25032862e-07
Iter: 178 loss: 7.19747163e-07
Iter: 179 loss: 7.16030456e-07
Iter: 180 loss: 7.14144107e-07
Iter: 181 loss: 7.06926699e-07
Iter: 182 loss: 7.92851e-07
Iter: 183 loss: 7.06814717e-07
Iter: 184 loss: 7.01140095e-07
Iter: 185 loss: 7.01043575e-07
Iter: 186 loss: 6.96624852e-07
Iter: 187 loss: 7.00783175e-07
Iter: 188 loss: 6.94151481e-07
Iter: 189 loss: 6.92316121e-07
Iter: 190 loss: 6.8851341e-07
Iter: 191 loss: 7.57613293e-07
Iter: 192 loss: 6.88447e-07
Iter: 193 loss: 6.84451777e-07
Iter: 194 loss: 6.95416816e-07
Iter: 195 loss: 6.83157737e-07
Iter: 196 loss: 6.77700768e-07
Iter: 197 loss: 7.04105673e-07
Iter: 198 loss: 6.76703962e-07
Iter: 199 loss: 6.74299599e-07
Iter: 200 loss: 6.69944143e-07
Iter: 201 loss: 7.74819966e-07
Iter: 202 loss: 6.69925839e-07
Iter: 203 loss: 6.66379e-07
Iter: 204 loss: 6.66173548e-07
Iter: 205 loss: 6.63037554e-07
Iter: 206 loss: 6.63221385e-07
Iter: 207 loss: 6.60579076e-07
Iter: 208 loss: 6.5807842e-07
Iter: 209 loss: 6.58525209e-07
Iter: 210 loss: 6.56180532e-07
Iter: 211 loss: 6.52608492e-07
Iter: 212 loss: 6.66111191e-07
Iter: 213 loss: 6.5177062e-07
Iter: 214 loss: 6.50671666e-07
Iter: 215 loss: 6.50292236e-07
Iter: 216 loss: 6.49028379e-07
Iter: 217 loss: 6.45399268e-07
Iter: 218 loss: 6.60486819e-07
Iter: 219 loss: 6.43964881e-07
Iter: 220 loss: 6.37687492e-07
Iter: 221 loss: 6.49823619e-07
Iter: 222 loss: 6.3506991e-07
Iter: 223 loss: 6.34799505e-07
Iter: 224 loss: 6.33087041e-07
Iter: 225 loss: 6.30901354e-07
Iter: 226 loss: 6.26585461e-07
Iter: 227 loss: 7.0983549e-07
Iter: 228 loss: 6.26516112e-07
Iter: 229 loss: 6.2286324e-07
Iter: 230 loss: 6.40968892e-07
Iter: 231 loss: 6.22232903e-07
Iter: 232 loss: 6.21282936e-07
Iter: 233 loss: 6.20834157e-07
Iter: 234 loss: 6.19955927e-07
Iter: 235 loss: 6.17370574e-07
Iter: 236 loss: 6.26828864e-07
Iter: 237 loss: 6.16226487e-07
Iter: 238 loss: 6.14578425e-07
Iter: 239 loss: 6.14404144e-07
Iter: 240 loss: 6.12538656e-07
Iter: 241 loss: 6.15751105e-07
Iter: 242 loss: 6.11695384e-07
Iter: 243 loss: 6.09614517e-07
Iter: 244 loss: 6.0453749e-07
Iter: 245 loss: 6.58419253e-07
Iter: 246 loss: 6.03999808e-07
Iter: 247 loss: 5.99089617e-07
Iter: 248 loss: 6.51155233e-07
Iter: 249 loss: 5.98973088e-07
Iter: 250 loss: 5.96262566e-07
Iter: 251 loss: 5.96264954e-07
Iter: 252 loss: 5.93712798e-07
Iter: 253 loss: 5.94012818e-07
Iter: 254 loss: 5.91803882e-07
Iter: 255 loss: 5.89698402e-07
Iter: 256 loss: 5.91730554e-07
Iter: 257 loss: 5.88496562e-07
Iter: 258 loss: 5.86054625e-07
Iter: 259 loss: 5.87684553e-07
Iter: 260 loss: 5.84541795e-07
Iter: 261 loss: 5.84584e-07
Iter: 262 loss: 5.83458927e-07
Iter: 263 loss: 5.82378959e-07
Iter: 264 loss: 5.7974944e-07
Iter: 265 loss: 6.06840786e-07
Iter: 266 loss: 5.79425773e-07
Iter: 267 loss: 5.76419666e-07
Iter: 268 loss: 5.84787927e-07
Iter: 269 loss: 5.7544969e-07
Iter: 270 loss: 5.74075784e-07
Iter: 271 loss: 5.73682769e-07
Iter: 272 loss: 5.7293e-07
Iter: 273 loss: 5.70471229e-07
Iter: 274 loss: 5.72999284e-07
Iter: 275 loss: 5.68542362e-07
Iter: 276 loss: 5.65706728e-07
Iter: 277 loss: 5.65667165e-07
Iter: 278 loss: 5.64024504e-07
Iter: 279 loss: 5.64025186e-07
Iter: 280 loss: 5.63035542e-07
Iter: 281 loss: 5.60197577e-07
Iter: 282 loss: 5.70955933e-07
Iter: 283 loss: 5.58967088e-07
Iter: 284 loss: 5.55899646e-07
Iter: 285 loss: 5.90114041e-07
Iter: 286 loss: 5.5582143e-07
Iter: 287 loss: 5.54099188e-07
Iter: 288 loss: 5.64225388e-07
Iter: 289 loss: 5.53855443e-07
Iter: 290 loss: 5.52318625e-07
Iter: 291 loss: 5.70991574e-07
Iter: 292 loss: 5.52286849e-07
Iter: 293 loss: 5.51566359e-07
Iter: 294 loss: 5.50541642e-07
Iter: 295 loss: 5.50479967e-07
Iter: 296 loss: 5.48698836e-07
Iter: 297 loss: 5.46090064e-07
Iter: 298 loss: 5.46010142e-07
Iter: 299 loss: 5.46353817e-07
Iter: 300 loss: 5.44574903e-07
Iter: 301 loss: 5.43397846e-07
Iter: 302 loss: 5.41506211e-07
Iter: 303 loss: 5.41467e-07
Iter: 304 loss: 5.39208827e-07
Iter: 305 loss: 5.35370305e-07
Iter: 306 loss: 5.35372067e-07
Iter: 307 loss: 5.34099229e-07
Iter: 308 loss: 5.33585421e-07
Iter: 309 loss: 5.32244542e-07
Iter: 310 loss: 5.46648209e-07
Iter: 311 loss: 5.32227375e-07
Iter: 312 loss: 5.31427759e-07
Iter: 313 loss: 5.29916e-07
Iter: 314 loss: 5.61812612e-07
Iter: 315 loss: 5.29897761e-07
Iter: 316 loss: 5.29018052e-07
Iter: 317 loss: 5.29001454e-07
Iter: 318 loss: 5.27945303e-07
Iter: 319 loss: 5.2669111e-07
Iter: 320 loss: 5.26528879e-07
Iter: 321 loss: 5.24733082e-07
Iter: 322 loss: 5.24414872e-07
Iter: 323 loss: 5.23228778e-07
Iter: 324 loss: 5.2104042e-07
Iter: 325 loss: 5.2437224e-07
Iter: 326 loss: 5.20006665e-07
Iter: 327 loss: 5.18519073e-07
Iter: 328 loss: 5.18256e-07
Iter: 329 loss: 5.17191324e-07
Iter: 330 loss: 5.16251248e-07
Iter: 331 loss: 5.15942077e-07
Iter: 332 loss: 5.14717271e-07
Iter: 333 loss: 5.1494e-07
Iter: 334 loss: 5.1378936e-07
Iter: 335 loss: 5.12404483e-07
Iter: 336 loss: 5.24016514e-07
Iter: 337 loss: 5.12337e-07
Iter: 338 loss: 5.113871e-07
Iter: 339 loss: 5.12115e-07
Iter: 340 loss: 5.1079968e-07
Iter: 341 loss: 5.09184133e-07
Iter: 342 loss: 5.20382457e-07
Iter: 343 loss: 5.09051233e-07
Iter: 344 loss: 5.08173912e-07
Iter: 345 loss: 5.06825131e-07
Iter: 346 loss: 5.06771642e-07
Iter: 347 loss: 5.05233174e-07
Iter: 348 loss: 5.11721112e-07
Iter: 349 loss: 5.04862157e-07
Iter: 350 loss: 5.02625539e-07
Iter: 351 loss: 5.08652306e-07
Iter: 352 loss: 5.0187e-07
Iter: 353 loss: 5.00827696e-07
Iter: 354 loss: 5.01460818e-07
Iter: 355 loss: 5.00172746e-07
Iter: 356 loss: 4.98979261e-07
Iter: 357 loss: 5.14090459e-07
Iter: 358 loss: 4.98969484e-07
Iter: 359 loss: 4.98029e-07
Iter: 360 loss: 4.95996176e-07
Iter: 361 loss: 5.26037354e-07
Iter: 362 loss: 4.95907784e-07
Iter: 363 loss: 4.941528e-07
Iter: 364 loss: 4.9780283e-07
Iter: 365 loss: 4.93436119e-07
Iter: 366 loss: 4.9200213e-07
Iter: 367 loss: 4.91995308e-07
Iter: 368 loss: 4.90729e-07
Iter: 369 loss: 4.95149379e-07
Iter: 370 loss: 4.90423645e-07
Iter: 371 loss: 4.89738568e-07
Iter: 372 loss: 4.88735509e-07
Iter: 373 loss: 4.88749606e-07
Iter: 374 loss: 4.87252635e-07
Iter: 375 loss: 4.91552726e-07
Iter: 376 loss: 4.86788622e-07
Iter: 377 loss: 4.85731789e-07
Iter: 378 loss: 4.91073138e-07
Iter: 379 loss: 4.85591841e-07
Iter: 380 loss: 4.84258692e-07
Iter: 381 loss: 4.90289494e-07
Iter: 382 loss: 4.83987492e-07
Iter: 383 loss: 4.82865403e-07
Iter: 384 loss: 4.80785275e-07
Iter: 385 loss: 5.29398505e-07
Iter: 386 loss: 4.80768904e-07
Iter: 387 loss: 4.79010964e-07
Iter: 388 loss: 4.83146e-07
Iter: 389 loss: 4.78343622e-07
Iter: 390 loss: 4.77837659e-07
Iter: 391 loss: 4.77525759e-07
Iter: 392 loss: 4.7683389e-07
Iter: 393 loss: 4.77729941e-07
Iter: 394 loss: 4.76477567e-07
Iter: 395 loss: 4.75961315e-07
Iter: 396 loss: 4.74779426e-07
Iter: 397 loss: 4.92538675e-07
Iter: 398 loss: 4.74754e-07
Iter: 399 loss: 4.74801112e-07
Iter: 400 loss: 4.74147868e-07
Iter: 401 loss: 4.73788759e-07
Iter: 402 loss: 4.72918742e-07
Iter: 403 loss: 4.80870256e-07
Iter: 404 loss: 4.72785217e-07
Iter: 405 loss: 4.71395879e-07
Iter: 406 loss: 4.7035644e-07
Iter: 407 loss: 4.69918916e-07
Iter: 408 loss: 4.70334498e-07
Iter: 409 loss: 4.69102758e-07
Iter: 410 loss: 4.68489333e-07
Iter: 411 loss: 4.6784362e-07
Iter: 412 loss: 4.67717285e-07
Iter: 413 loss: 4.66689954e-07
Iter: 414 loss: 4.66230375e-07
Iter: 415 loss: 4.65675782e-07
Iter: 416 loss: 4.64598429e-07
Iter: 417 loss: 4.75190291e-07
Iter: 418 loss: 4.64546474e-07
Iter: 419 loss: 4.63810579e-07
Iter: 420 loss: 4.75580038e-07
Iter: 421 loss: 4.63814615e-07
Iter: 422 loss: 4.63293304e-07
Iter: 423 loss: 4.61874492e-07
Iter: 424 loss: 4.74704621e-07
Iter: 425 loss: 4.61668691e-07
Iter: 426 loss: 4.60297485e-07
Iter: 427 loss: 4.63391558e-07
Iter: 428 loss: 4.59782427e-07
Iter: 429 loss: 4.58255698e-07
Iter: 430 loss: 4.67079474e-07
Iter: 431 loss: 4.58034634e-07
Iter: 432 loss: 4.56865507e-07
Iter: 433 loss: 4.56846323e-07
Iter: 434 loss: 4.56469536e-07
Iter: 435 loss: 4.55544495e-07
Iter: 436 loss: 4.66844e-07
Iter: 437 loss: 4.55460906e-07
Iter: 438 loss: 4.54622324e-07
Iter: 439 loss: 4.54629344e-07
Iter: 440 loss: 4.53741393e-07
Iter: 441 loss: 4.53457858e-07
Iter: 442 loss: 4.52974973e-07
Iter: 443 loss: 4.52249708e-07
Iter: 444 loss: 4.51260348e-07
Iter: 445 loss: 4.51199497e-07
Iter: 446 loss: 4.50269511e-07
Iter: 447 loss: 4.50230118e-07
Iter: 448 loss: 4.49373e-07
Iter: 449 loss: 4.5161724e-07
Iter: 450 loss: 4.49075742e-07
Iter: 451 loss: 4.4845541e-07
Iter: 452 loss: 4.47703343e-07
Iter: 453 loss: 4.47660653e-07
Iter: 454 loss: 4.46980465e-07
Iter: 455 loss: 4.46970205e-07
Iter: 456 loss: 4.46440481e-07
Iter: 457 loss: 4.49309624e-07
Iter: 458 loss: 4.46354818e-07
Iter: 459 loss: 4.46006709e-07
Iter: 460 loss: 4.45077177e-07
Iter: 461 loss: 4.49235841e-07
Iter: 462 loss: 4.4468544e-07
Iter: 463 loss: 4.43340213e-07
Iter: 464 loss: 4.52631951e-07
Iter: 465 loss: 4.43196711e-07
Iter: 466 loss: 4.42580699e-07
Iter: 467 loss: 4.42495434e-07
Iter: 468 loss: 4.41867115e-07
Iter: 469 loss: 4.40081294e-07
Iter: 470 loss: 4.50383936e-07
Iter: 471 loss: 4.39572318e-07
Iter: 472 loss: 4.3854979e-07
Iter: 473 loss: 4.38486438e-07
Iter: 474 loss: 4.37614034e-07
Iter: 475 loss: 4.43757131e-07
Iter: 476 loss: 4.37549971e-07
Iter: 477 loss: 4.36988842e-07
Iter: 478 loss: 4.36051096e-07
Iter: 479 loss: 4.36051437e-07
Iter: 480 loss: 4.35449124e-07
Iter: 481 loss: 4.4023426e-07
Iter: 482 loss: 4.35424226e-07
Iter: 483 loss: 4.34890296e-07
Iter: 484 loss: 4.40063445e-07
Iter: 485 loss: 4.3487708e-07
Iter: 486 loss: 4.34423868e-07
Iter: 487 loss: 4.33574144e-07
Iter: 488 loss: 4.50628875e-07
Iter: 489 loss: 4.33559023e-07
Iter: 490 loss: 4.32771714e-07
Iter: 491 loss: 4.38137477e-07
Iter: 492 loss: 4.32714415e-07
Iter: 493 loss: 4.31997421e-07
Iter: 494 loss: 4.37968538e-07
Iter: 495 loss: 4.31962803e-07
Iter: 496 loss: 4.31299583e-07
Iter: 497 loss: 4.29983686e-07
Iter: 498 loss: 4.53199732e-07
Iter: 499 loss: 4.29951541e-07
Iter: 500 loss: 4.28885414e-07
Iter: 501 loss: 4.325519e-07
Iter: 502 loss: 4.28626379e-07
Iter: 503 loss: 4.2751708e-07
Iter: 504 loss: 4.28515278e-07
Iter: 505 loss: 4.26854712e-07
Iter: 506 loss: 4.26914283e-07
Iter: 507 loss: 4.26360657e-07
Iter: 508 loss: 4.25918131e-07
Iter: 509 loss: 4.24667348e-07
Iter: 510 loss: 4.32349168e-07
Iter: 511 loss: 4.24366391e-07
Iter: 512 loss: 4.23343522e-07
Iter: 513 loss: 4.30965969e-07
Iter: 514 loss: 4.23260161e-07
Iter: 515 loss: 4.22659781e-07
Iter: 516 loss: 4.3063568e-07
Iter: 517 loss: 4.22650231e-07
Iter: 518 loss: 4.21982719e-07
Iter: 519 loss: 4.21435573e-07
Iter: 520 loss: 4.21229174e-07
Iter: 521 loss: 4.20658864e-07
Iter: 522 loss: 4.21582399e-07
Iter: 523 loss: 4.20421827e-07
Iter: 524 loss: 4.19687467e-07
Iter: 525 loss: 4.18614434e-07
Iter: 526 loss: 4.1856191e-07
Iter: 527 loss: 4.1841048e-07
Iter: 528 loss: 4.1784233e-07
Iter: 529 loss: 4.17255762e-07
Iter: 530 loss: 4.16987461e-07
Iter: 531 loss: 4.16701937e-07
Iter: 532 loss: 4.15909852e-07
Iter: 533 loss: 4.15333204e-07
Iter: 534 loss: 4.15059247e-07
Iter: 535 loss: 4.14753686e-07
Iter: 536 loss: 4.14462903e-07
Iter: 537 loss: 4.1413216e-07
Iter: 538 loss: 4.13710779e-07
Iter: 539 loss: 4.13659478e-07
Iter: 540 loss: 4.13169943e-07
Iter: 541 loss: 4.12971758e-07
Iter: 542 loss: 4.12686461e-07
Iter: 543 loss: 4.12756691e-07
Iter: 544 loss: 4.1239781e-07
Iter: 545 loss: 4.12182942e-07
Iter: 546 loss: 4.11629685e-07
Iter: 547 loss: 4.1640061e-07
Iter: 548 loss: 4.11543738e-07
Iter: 549 loss: 4.10683754e-07
Iter: 550 loss: 4.09887036e-07
Iter: 551 loss: 4.09674442e-07
Iter: 552 loss: 4.08624885e-07
Iter: 553 loss: 4.22289588e-07
Iter: 554 loss: 4.08612806e-07
Iter: 555 loss: 4.07685093e-07
Iter: 556 loss: 4.16152915e-07
Iter: 557 loss: 4.07680034e-07
Iter: 558 loss: 4.07104437e-07
Iter: 559 loss: 4.06319174e-07
Iter: 560 loss: 4.06299364e-07
Iter: 561 loss: 4.05669823e-07
Iter: 562 loss: 4.08737264e-07
Iter: 563 loss: 4.0561136e-07
Iter: 564 loss: 4.05121483e-07
Iter: 565 loss: 4.05132681e-07
Iter: 566 loss: 4.04795031e-07
Iter: 567 loss: 4.04462526e-07
Iter: 568 loss: 4.04404858e-07
Iter: 569 loss: 4.03943233e-07
Iter: 570 loss: 4.03328016e-07
Iter: 571 loss: 4.03301101e-07
Iter: 572 loss: 4.02328283e-07
Iter: 573 loss: 4.12077497e-07
Iter: 574 loss: 4.02283604e-07
Iter: 575 loss: 4.01372461e-07
Iter: 576 loss: 4.05167896e-07
Iter: 577 loss: 4.01192466e-07
Iter: 578 loss: 4.00742124e-07
Iter: 579 loss: 3.99792555e-07
Iter: 580 loss: 4.14791145e-07
Iter: 581 loss: 3.99773171e-07
Iter: 582 loss: 3.98720459e-07
Iter: 583 loss: 4.06669386e-07
Iter: 584 loss: 3.98656425e-07
Iter: 585 loss: 3.98319457e-07
Iter: 586 loss: 3.98247437e-07
Iter: 587 loss: 3.9786795e-07
Iter: 588 loss: 3.97152e-07
Iter: 589 loss: 4.14496128e-07
Iter: 590 loss: 3.97150473e-07
Iter: 591 loss: 3.96586358e-07
Iter: 592 loss: 3.97549087e-07
Iter: 593 loss: 3.96301857e-07
Iter: 594 loss: 3.95582e-07
Iter: 595 loss: 3.9501532e-07
Iter: 596 loss: 3.94773849e-07
Iter: 597 loss: 3.95341857e-07
Iter: 598 loss: 3.9437947e-07
Iter: 599 loss: 3.9410304e-07
Iter: 600 loss: 3.93706e-07
Iter: 601 loss: 3.93705534e-07
Iter: 602 loss: 3.93194455e-07
Iter: 603 loss: 3.9380069e-07
Iter: 604 loss: 3.92951705e-07
Iter: 605 loss: 3.92352831e-07
Iter: 606 loss: 3.98854553e-07
Iter: 607 loss: 3.92352518e-07
Iter: 608 loss: 3.92022571e-07
Iter: 609 loss: 3.91634273e-07
Iter: 610 loss: 3.91584564e-07
Iter: 611 loss: 3.90944024e-07
Iter: 612 loss: 3.90335913e-07
Iter: 613 loss: 3.90148585e-07
Iter: 614 loss: 3.90187608e-07
Iter: 615 loss: 3.89684885e-07
Iter: 616 loss: 3.89391403e-07
Iter: 617 loss: 3.88848775e-07
Iter: 618 loss: 4.01783808e-07
Iter: 619 loss: 3.88844882e-07
Iter: 620 loss: 3.88218098e-07
Iter: 621 loss: 3.88302624e-07
Iter: 622 loss: 3.87708496e-07
Iter: 623 loss: 3.87184343e-07
Iter: 624 loss: 3.90750643e-07
Iter: 625 loss: 3.87130342e-07
Iter: 626 loss: 3.86701373e-07
Iter: 627 loss: 3.87746e-07
Iter: 628 loss: 3.86551392e-07
Iter: 629 loss: 3.86389786e-07
Iter: 630 loss: 3.86263082e-07
Iter: 631 loss: 3.86115232e-07
Iter: 632 loss: 3.85585111e-07
Iter: 633 loss: 3.8651811e-07
Iter: 634 loss: 3.85234728e-07
Iter: 635 loss: 3.84438891e-07
Iter: 636 loss: 3.93902098e-07
Iter: 637 loss: 3.84445968e-07
Iter: 638 loss: 3.8383115e-07
Iter: 639 loss: 3.90484928e-07
Iter: 640 loss: 3.83815404e-07
Iter: 641 loss: 3.83539657e-07
Iter: 642 loss: 3.82801687e-07
Iter: 643 loss: 3.9056215e-07
Iter: 644 loss: 3.82748198e-07
Iter: 645 loss: 3.82148755e-07
Iter: 646 loss: 3.82131333e-07
Iter: 647 loss: 3.81748691e-07
Iter: 648 loss: 3.84848647e-07
Iter: 649 loss: 3.81720412e-07
Iter: 650 loss: 3.81478742e-07
Iter: 651 loss: 3.8079952e-07
Iter: 652 loss: 3.87232717e-07
Iter: 653 loss: 3.80732587e-07
Iter: 654 loss: 3.80350457e-07
Iter: 655 loss: 3.80311064e-07
Iter: 656 loss: 3.79926405e-07
Iter: 657 loss: 3.8079429e-07
Iter: 658 loss: 3.79806465e-07
Iter: 659 loss: 3.79498971e-07
Iter: 660 loss: 3.78737e-07
Iter: 661 loss: 3.84491329e-07
Iter: 662 loss: 3.78577198e-07
Iter: 663 loss: 3.77773915e-07
Iter: 664 loss: 3.87607173e-07
Iter: 665 loss: 3.77775592e-07
Iter: 666 loss: 3.77313114e-07
Iter: 667 loss: 3.80156621e-07
Iter: 668 loss: 3.77270737e-07
Iter: 669 loss: 3.76767844e-07
Iter: 670 loss: 3.79472823e-07
Iter: 671 loss: 3.76690366e-07
Iter: 672 loss: 3.76428403e-07
Iter: 673 loss: 3.7593685e-07
Iter: 674 loss: 3.87273417e-07
Iter: 675 loss: 3.75929403e-07
Iter: 676 loss: 3.75190552e-07
Iter: 677 loss: 3.74843154e-07
Iter: 678 loss: 3.74450849e-07
Iter: 679 loss: 3.74014519e-07
Iter: 680 loss: 3.73909984e-07
Iter: 681 loss: 3.73457368e-07
Iter: 682 loss: 3.75496882e-07
Iter: 683 loss: 3.73370483e-07
Iter: 684 loss: 3.72998443e-07
Iter: 685 loss: 3.72707746e-07
Iter: 686 loss: 3.72576181e-07
Iter: 687 loss: 3.72358045e-07
Iter: 688 loss: 3.72297905e-07
Iter: 689 loss: 3.72134537e-07
Iter: 690 loss: 3.71887495e-07
Iter: 691 loss: 3.7188974e-07
Iter: 692 loss: 3.71573066e-07
Iter: 693 loss: 3.71145632e-07
Iter: 694 loss: 3.71141482e-07
Iter: 695 loss: 3.70900921e-07
Iter: 696 loss: 3.70850785e-07
Iter: 697 loss: 3.70497105e-07
Iter: 698 loss: 3.69983e-07
Iter: 699 loss: 3.69955444e-07
Iter: 700 loss: 3.69275966e-07
Iter: 701 loss: 3.69624786e-07
Iter: 702 loss: 3.68790722e-07
Iter: 703 loss: 3.6862042e-07
Iter: 704 loss: 3.68419023e-07
Iter: 705 loss: 3.68162233e-07
Iter: 706 loss: 3.6749725e-07
Iter: 707 loss: 3.72340821e-07
Iter: 708 loss: 3.67368216e-07
Iter: 709 loss: 3.66907386e-07
Iter: 710 loss: 3.71888319e-07
Iter: 711 loss: 3.66902782e-07
Iter: 712 loss: 3.66600432e-07
Iter: 713 loss: 3.66501183e-07
Iter: 714 loss: 3.66337531e-07
Iter: 715 loss: 3.66003889e-07
Iter: 716 loss: 3.65986068e-07
Iter: 717 loss: 3.65658877e-07
Iter: 718 loss: 3.65659275e-07
Iter: 719 loss: 3.65407828e-07
Iter: 720 loss: 3.64959533e-07
Iter: 721 loss: 3.65206745e-07
Iter: 722 loss: 3.64697257e-07
Iter: 723 loss: 3.63972219e-07
Iter: 724 loss: 3.66125562e-07
Iter: 725 loss: 3.63760421e-07
Iter: 726 loss: 3.63196449e-07
Iter: 727 loss: 3.63042375e-07
Iter: 728 loss: 3.62699495e-07
Iter: 729 loss: 3.62193646e-07
Iter: 730 loss: 3.63119938e-07
Iter: 731 loss: 3.61967608e-07
Iter: 732 loss: 3.61827347e-07
Iter: 733 loss: 3.6165693e-07
Iter: 734 loss: 3.61499701e-07
Iter: 735 loss: 3.61336731e-07
Iter: 736 loss: 3.61296429e-07
Iter: 737 loss: 3.61067521e-07
Iter: 738 loss: 3.60466316e-07
Iter: 739 loss: 3.65683036e-07
Iter: 740 loss: 3.6037045e-07
Iter: 741 loss: 3.60659044e-07
Iter: 742 loss: 3.60052951e-07
Iter: 743 loss: 3.59808297e-07
Iter: 744 loss: 3.59418095e-07
Iter: 745 loss: 3.59404112e-07
Iter: 746 loss: 3.58979605e-07
Iter: 747 loss: 3.58512921e-07
Iter: 748 loss: 3.58435813e-07
Iter: 749 loss: 3.57927405e-07
Iter: 750 loss: 3.61568425e-07
Iter: 751 loss: 3.57897875e-07
Iter: 752 loss: 3.57343851e-07
Iter: 753 loss: 3.61105776e-07
Iter: 754 loss: 3.57298234e-07
Iter: 755 loss: 3.57029478e-07
Iter: 756 loss: 3.57051704e-07
Iter: 757 loss: 3.5683567e-07
Iter: 758 loss: 3.56521753e-07
Iter: 759 loss: 3.59912576e-07
Iter: 760 loss: 3.56527494e-07
Iter: 761 loss: 3.56306543e-07
Iter: 762 loss: 3.55723955e-07
Iter: 763 loss: 3.5885256e-07
Iter: 764 loss: 3.55535462e-07
Iter: 765 loss: 3.55028789e-07
Iter: 766 loss: 3.55009036e-07
Iter: 767 loss: 3.54663939e-07
Iter: 768 loss: 3.58840111e-07
Iter: 769 loss: 3.54644101e-07
Iter: 770 loss: 3.54452197e-07
Iter: 771 loss: 3.5389246e-07
Iter: 772 loss: 3.55985719e-07
Iter: 773 loss: 3.53647749e-07
Iter: 774 loss: 3.53098983e-07
Iter: 775 loss: 3.53101825e-07
Iter: 776 loss: 3.52821132e-07
Iter: 777 loss: 3.52819143e-07
Iter: 778 loss: 3.52545811e-07
Iter: 779 loss: 3.51917777e-07
Iter: 780 loss: 3.61631749e-07
Iter: 781 loss: 3.51863719e-07
Iter: 782 loss: 3.51368726e-07
Iter: 783 loss: 3.53375555e-07
Iter: 784 loss: 3.51252766e-07
Iter: 785 loss: 3.50695046e-07
Iter: 786 loss: 3.51245546e-07
Iter: 787 loss: 3.50411483e-07
Iter: 788 loss: 3.50450875e-07
Iter: 789 loss: 3.50157023e-07
Iter: 790 loss: 3.49988227e-07
Iter: 791 loss: 3.49770716e-07
Iter: 792 loss: 3.49762445e-07
Iter: 793 loss: 3.49480416e-07
Iter: 794 loss: 3.4915081e-07
Iter: 795 loss: 3.49086292e-07
Iter: 796 loss: 3.48653799e-07
Iter: 797 loss: 3.51151328e-07
Iter: 798 loss: 3.4857743e-07
Iter: 799 loss: 3.48123081e-07
Iter: 800 loss: 3.49148081e-07
Iter: 801 loss: 3.47941864e-07
Iter: 802 loss: 3.47240075e-07
Iter: 803 loss: 3.49910039e-07
Iter: 804 loss: 3.47050161e-07
Iter: 805 loss: 3.46693383e-07
Iter: 806 loss: 3.46484512e-07
Iter: 807 loss: 3.46317535e-07
Iter: 808 loss: 3.45894193e-07
Iter: 809 loss: 3.48000611e-07
Iter: 810 loss: 3.45790625e-07
Iter: 811 loss: 3.45373707e-07
Iter: 812 loss: 3.50617711e-07
Iter: 813 loss: 3.45371575e-07
Iter: 814 loss: 3.45210623e-07
Iter: 815 loss: 3.44842874e-07
Iter: 816 loss: 3.4949079e-07
Iter: 817 loss: 3.44808967e-07
Iter: 818 loss: 3.4456454e-07
Iter: 819 loss: 3.44534385e-07
Iter: 820 loss: 3.44260059e-07
Iter: 821 loss: 3.44155296e-07
Iter: 822 loss: 3.44053404e-07
Iter: 823 loss: 3.43803947e-07
Iter: 824 loss: 3.4467638e-07
Iter: 825 loss: 3.43731188e-07
Iter: 826 loss: 3.43336495e-07
Iter: 827 loss: 3.43073168e-07
Iter: 828 loss: 3.42929923e-07
Iter: 829 loss: 3.42485805e-07
Iter: 830 loss: 3.42927905e-07
Iter: 831 loss: 3.42232312e-07
Iter: 832 loss: 3.41865928e-07
Iter: 833 loss: 3.41765201e-07
Iter: 834 loss: 3.41547576e-07
Iter: 835 loss: 3.41070574e-07
Iter: 836 loss: 3.46210925e-07
Iter: 837 loss: 3.41047439e-07
Iter: 838 loss: 3.40679179e-07
Iter: 839 loss: 3.40715047e-07
Iter: 840 loss: 3.40360373e-07
Iter: 841 loss: 3.39928306e-07
Iter: 842 loss: 3.46283883e-07
Iter: 843 loss: 3.39942e-07
Iter: 844 loss: 3.39573404e-07
Iter: 845 loss: 3.4116988e-07
Iter: 846 loss: 3.39467476e-07
Iter: 847 loss: 3.39214e-07
Iter: 848 loss: 3.38625341e-07
Iter: 849 loss: 3.45755211e-07
Iter: 850 loss: 3.38569691e-07
Iter: 851 loss: 3.38115086e-07
Iter: 852 loss: 3.43985164e-07
Iter: 853 loss: 3.38126313e-07
Iter: 854 loss: 3.37963883e-07
Iter: 855 loss: 3.37944414e-07
Iter: 856 loss: 3.37746712e-07
Iter: 857 loss: 3.37266044e-07
Iter: 858 loss: 3.44609958e-07
Iter: 859 loss: 3.37253e-07
Iter: 860 loss: 3.37014882e-07
Iter: 861 loss: 3.3997884e-07
Iter: 862 loss: 3.36995413e-07
Iter: 863 loss: 3.36692352e-07
Iter: 864 loss: 3.37039921e-07
Iter: 865 loss: 3.3651574e-07
Iter: 866 loss: 3.36139323e-07
Iter: 867 loss: 3.35718084e-07
Iter: 868 loss: 3.35677157e-07
Iter: 869 loss: 3.35531325e-07
Iter: 870 loss: 3.35378957e-07
Iter: 871 loss: 3.35251826e-07
Iter: 872 loss: 3.34887659e-07
Iter: 873 loss: 3.38179e-07
Iter: 874 loss: 3.34838631e-07
Iter: 875 loss: 3.34469803e-07
Iter: 876 loss: 3.3644119e-07
Iter: 877 loss: 3.34417166e-07
Iter: 878 loss: 3.34361431e-07
Iter: 879 loss: 3.34296288e-07
Iter: 880 loss: 3.3416336e-07
Iter: 881 loss: 3.33843218e-07
Iter: 882 loss: 3.36283222e-07
Iter: 883 loss: 3.33770856e-07
Iter: 884 loss: 3.33364142e-07
Iter: 885 loss: 3.33629885e-07
Iter: 886 loss: 3.33079925e-07
Iter: 887 loss: 3.32483722e-07
Iter: 888 loss: 3.3319651e-07
Iter: 889 loss: 3.32163438e-07
Iter: 890 loss: 3.32124813e-07
Iter: 891 loss: 3.31880273e-07
Iter: 892 loss: 3.31606685e-07
Iter: 893 loss: 3.31523324e-07
Iter: 894 loss: 3.31380875e-07
Iter: 895 loss: 3.31106747e-07
Iter: 896 loss: 3.31739841e-07
Iter: 897 loss: 3.30999484e-07
Iter: 898 loss: 3.30677892e-07
Iter: 899 loss: 3.33291268e-07
Iter: 900 loss: 3.30655098e-07
Iter: 901 loss: 3.3049011e-07
Iter: 902 loss: 3.30307e-07
Iter: 903 loss: 3.30286298e-07
Iter: 904 loss: 3.30034368e-07
Iter: 905 loss: 3.3224137e-07
Iter: 906 loss: 3.30035363e-07
Iter: 907 loss: 3.29762514e-07
Iter: 908 loss: 3.29328685e-07
Iter: 909 loss: 3.29306943e-07
Iter: 910 loss: 3.28966962e-07
Iter: 911 loss: 3.2979051e-07
Iter: 912 loss: 3.28836393e-07
Iter: 913 loss: 3.28568092e-07
Iter: 914 loss: 3.32573791e-07
Iter: 915 loss: 3.28584633e-07
Iter: 916 loss: 3.28273075e-07
Iter: 917 loss: 3.28028619e-07
Iter: 918 loss: 3.27947816e-07
Iter: 919 loss: 3.27618864e-07
Iter: 920 loss: 3.27596126e-07
Iter: 921 loss: 3.27365598e-07
Iter: 922 loss: 3.26902239e-07
Iter: 923 loss: 3.2755014e-07
Iter: 924 loss: 3.26651076e-07
Iter: 925 loss: 3.26224665e-07
Iter: 926 loss: 3.31063518e-07
Iter: 927 loss: 3.26213353e-07
Iter: 928 loss: 3.25811129e-07
Iter: 929 loss: 3.28302406e-07
Iter: 930 loss: 3.25772533e-07
Iter: 931 loss: 3.2553794e-07
Iter: 932 loss: 3.25367e-07
Iter: 933 loss: 3.25282855e-07
Iter: 934 loss: 3.25235305e-07
Iter: 935 loss: 3.25168e-07
Iter: 936 loss: 3.25064605e-07
Iter: 937 loss: 3.24861702e-07
Iter: 938 loss: 3.27359771e-07
Iter: 939 loss: 3.2484806e-07
Iter: 940 loss: 3.2458928e-07
Iter: 941 loss: 3.26084944e-07
Iter: 942 loss: 3.24545169e-07
Iter: 943 loss: 3.24390243e-07
Iter: 944 loss: 3.26620693e-07
Iter: 945 loss: 3.24390498e-07
Iter: 946 loss: 3.2426118e-07
Iter: 947 loss: 3.23955163e-07
Iter: 948 loss: 3.26010593e-07
Iter: 949 loss: 3.23889566e-07
Iter: 950 loss: 3.23455367e-07
Iter: 951 loss: 3.25576877e-07
Iter: 952 loss: 3.23383631e-07
Iter: 953 loss: 3.23097964e-07
Iter: 954 loss: 3.2309012e-07
Iter: 955 loss: 3.22888923e-07
Iter: 956 loss: 3.22376394e-07
Iter: 957 loss: 3.27117675e-07
Iter: 958 loss: 3.22324354e-07
Iter: 959 loss: 3.21987216e-07
Iter: 960 loss: 3.25702956e-07
Iter: 961 loss: 3.21976529e-07
Iter: 962 loss: 3.21758932e-07
Iter: 963 loss: 3.22656604e-07
Iter: 964 loss: 3.21719028e-07
Iter: 965 loss: 3.21475397e-07
Iter: 966 loss: 3.23171889e-07
Iter: 967 loss: 3.21448567e-07
Iter: 968 loss: 3.21341133e-07
Iter: 969 loss: 3.2114724e-07
Iter: 970 loss: 3.21148065e-07
Iter: 971 loss: 3.20919696e-07
Iter: 972 loss: 3.24040286e-07
Iter: 973 loss: 3.20921686e-07
Iter: 974 loss: 3.20710171e-07
Iter: 975 loss: 3.20227628e-07
Iter: 976 loss: 3.25626644e-07
Iter: 977 loss: 3.20183e-07
Iter: 978 loss: 3.19815967e-07
Iter: 979 loss: 3.1980602e-07
Iter: 980 loss: 3.1952186e-07
Iter: 981 loss: 3.19161359e-07
Iter: 982 loss: 3.19168237e-07
Iter: 983 loss: 3.18749699e-07
Iter: 984 loss: 3.19799028e-07
Iter: 985 loss: 3.1863226e-07
Iter: 986 loss: 3.18382575e-07
Iter: 987 loss: 3.20856373e-07
Iter: 988 loss: 3.1838178e-07
Iter: 989 loss: 3.18141446e-07
Iter: 990 loss: 3.19356047e-07
Iter: 991 loss: 3.181392e-07
Iter: 992 loss: 3.17926293e-07
Iter: 993 loss: 3.17680872e-07
Iter: 994 loss: 3.17678257e-07
Iter: 995 loss: 3.17403021e-07
Iter: 996 loss: 3.17433063e-07
Iter: 997 loss: 3.17200374e-07
Iter: 998 loss: 3.16737783e-07
Iter: 999 loss: 3.17581566e-07
Iter: 1000 loss: 3.16531128e-07
Iter: 1001 loss: 3.1623e-07
Iter: 1002 loss: 3.20140202e-07
Iter: 1003 loss: 3.16248133e-07
Iter: 1004 loss: 3.15959369e-07
Iter: 1005 loss: 3.18439191e-07
Iter: 1006 loss: 3.15943225e-07
Iter: 1007 loss: 3.15789293e-07
Iter: 1008 loss: 3.15545776e-07
Iter: 1009 loss: 3.15535942e-07
Iter: 1010 loss: 3.1534654e-07
Iter: 1011 loss: 3.15347592e-07
Iter: 1012 loss: 3.1512505e-07
Iter: 1013 loss: 3.14747922e-07
Iter: 1014 loss: 3.14738799e-07
Iter: 1015 loss: 3.14445771e-07
Iter: 1016 loss: 3.16181655e-07
Iter: 1017 loss: 3.14408965e-07
Iter: 1018 loss: 3.14130375e-07
Iter: 1019 loss: 3.1666093e-07
Iter: 1020 loss: 3.14140692e-07
Iter: 1021 loss: 3.13965671e-07
Iter: 1022 loss: 3.13524538e-07
Iter: 1023 loss: 3.1777077e-07
Iter: 1024 loss: 3.13464625e-07
Iter: 1025 loss: 3.13156647e-07
Iter: 1026 loss: 3.15221911e-07
Iter: 1027 loss: 3.13145222e-07
Iter: 1028 loss: 3.12965767e-07
Iter: 1029 loss: 3.12949709e-07
Iter: 1030 loss: 3.12787648e-07
Iter: 1031 loss: 3.12814336e-07
Iter: 1032 loss: 3.12655857e-07
Iter: 1033 loss: 3.12436413e-07
Iter: 1034 loss: 3.12132158e-07
Iter: 1035 loss: 3.12137445e-07
Iter: 1036 loss: 3.11632903e-07
Iter: 1037 loss: 3.1257008e-07
Iter: 1038 loss: 3.11396946e-07
Iter: 1039 loss: 3.11404733e-07
Iter: 1040 loss: 3.11215899e-07
Iter: 1041 loss: 3.11029851e-07
Iter: 1042 loss: 3.10734322e-07
Iter: 1043 loss: 3.17876243e-07
Iter: 1044 loss: 3.10734123e-07
Iter: 1045 loss: 3.10467527e-07
Iter: 1046 loss: 3.11367984e-07
Iter: 1047 loss: 3.10401788e-07
Iter: 1048 loss: 3.10308707e-07
Iter: 1049 loss: 3.102829e-07
Iter: 1050 loss: 3.1018206e-07
Iter: 1051 loss: 3.09922598e-07
Iter: 1052 loss: 3.11283429e-07
Iter: 1053 loss: 3.09865641e-07
Iter: 1054 loss: 3.09578809e-07
Iter: 1055 loss: 3.11175569e-07
Iter: 1056 loss: 3.09563859e-07
Iter: 1057 loss: 3.09243831e-07
Iter: 1058 loss: 3.11562815e-07
Iter: 1059 loss: 3.09230757e-07
Iter: 1060 loss: 3.09027143e-07
Iter: 1061 loss: 3.08535277e-07
Iter: 1062 loss: 3.14255885e-07
Iter: 1063 loss: 3.08498869e-07
Iter: 1064 loss: 3.08140329e-07
Iter: 1065 loss: 3.11214166e-07
Iter: 1066 loss: 3.08140244e-07
Iter: 1067 loss: 3.07891355e-07
Iter: 1068 loss: 3.07905225e-07
Iter: 1069 loss: 3.07671939e-07
Iter: 1070 loss: 3.07557855e-07
Iter: 1071 loss: 3.07466678e-07
Iter: 1072 loss: 3.07261274e-07
Iter: 1073 loss: 3.07228959e-07
Iter: 1074 loss: 3.07076533e-07
Iter: 1075 loss: 3.0679945e-07
Iter: 1076 loss: 3.08815402e-07
Iter: 1077 loss: 3.06761137e-07
Iter: 1078 loss: 3.06433577e-07
Iter: 1079 loss: 3.08311257e-07
Iter: 1080 loss: 3.06374716e-07
Iter: 1081 loss: 3.06222631e-07
Iter: 1082 loss: 3.05996252e-07
Iter: 1083 loss: 3.05997474e-07
Iter: 1084 loss: 3.05818304e-07
Iter: 1085 loss: 3.05808385e-07
Iter: 1086 loss: 3.05636718e-07
Iter: 1087 loss: 3.05418865e-07
Iter: 1088 loss: 3.05392774e-07
Iter: 1089 loss: 3.05219544e-07
Iter: 1090 loss: 3.05696858e-07
Iter: 1091 loss: 3.05149456e-07
Iter: 1092 loss: 3.04956302e-07
Iter: 1093 loss: 3.07645394e-07
Iter: 1094 loss: 3.04962583e-07
Iter: 1095 loss: 3.04819025e-07
Iter: 1096 loss: 3.04533e-07
Iter: 1097 loss: 3.0914282e-07
Iter: 1098 loss: 3.04520512e-07
Iter: 1099 loss: 3.04143441e-07
Iter: 1100 loss: 3.03704383e-07
Iter: 1101 loss: 3.0366283e-07
Iter: 1102 loss: 3.03447678e-07
Iter: 1103 loss: 3.03348116e-07
Iter: 1104 loss: 3.0311412e-07
Iter: 1105 loss: 3.04828262e-07
Iter: 1106 loss: 3.03075637e-07
Iter: 1107 loss: 3.02926424e-07
Iter: 1108 loss: 3.02675886e-07
Iter: 1109 loss: 3.07543189e-07
Iter: 1110 loss: 3.02673e-07
Iter: 1111 loss: 3.02572658e-07
Iter: 1112 loss: 3.02555122e-07
Iter: 1113 loss: 3.02392607e-07
Iter: 1114 loss: 3.02550404e-07
Iter: 1115 loss: 3.02347019e-07
Iter: 1116 loss: 3.02193882e-07
Iter: 1117 loss: 3.01991975e-07
Iter: 1118 loss: 3.01966054e-07
Iter: 1119 loss: 3.01821188e-07
Iter: 1120 loss: 3.01811838e-07
Iter: 1121 loss: 3.01664414e-07
Iter: 1122 loss: 3.01325599e-07
Iter: 1123 loss: 3.05278775e-07
Iter: 1124 loss: 3.012766e-07
Iter: 1125 loss: 3.00820204e-07
Iter: 1126 loss: 3.01863821e-07
Iter: 1127 loss: 3.00657319e-07
Iter: 1128 loss: 3.00432646e-07
Iter: 1129 loss: 3.0038504e-07
Iter: 1130 loss: 3.0024961e-07
Iter: 1131 loss: 2.99983071e-07
Iter: 1132 loss: 3.05164065e-07
Iter: 1133 loss: 2.99993076e-07
Iter: 1134 loss: 2.9975746e-07
Iter: 1135 loss: 3.00298552e-07
Iter: 1136 loss: 2.99653379e-07
Iter: 1137 loss: 2.99481826e-07
Iter: 1138 loss: 3.00348034e-07
Iter: 1139 loss: 2.99454257e-07
Iter: 1140 loss: 2.99222506e-07
Iter: 1141 loss: 3.00985448e-07
Iter: 1142 loss: 2.99214548e-07
Iter: 1143 loss: 2.99020826e-07
Iter: 1144 loss: 2.98694886e-07
Iter: 1145 loss: 3.05349317e-07
Iter: 1146 loss: 2.98709438e-07
Iter: 1147 loss: 2.98314035e-07
Iter: 1148 loss: 2.97912209e-07
Iter: 1149 loss: 2.97826375e-07
Iter: 1150 loss: 2.98166867e-07
Iter: 1151 loss: 2.97606306e-07
Iter: 1152 loss: 2.97440408e-07
Iter: 1153 loss: 2.9756211e-07
Iter: 1154 loss: 2.97316802e-07
Iter: 1155 loss: 2.9721258e-07
Iter: 1156 loss: 2.97047478e-07
Iter: 1157 loss: 2.97015049e-07
Iter: 1158 loss: 2.96942375e-07
Iter: 1159 loss: 2.96906535e-07
Iter: 1160 loss: 2.96823487e-07
Iter: 1161 loss: 2.9664443e-07
Iter: 1162 loss: 2.98152145e-07
Iter: 1163 loss: 2.96617657e-07
Iter: 1164 loss: 2.9636405e-07
Iter: 1165 loss: 2.96964885e-07
Iter: 1166 loss: 2.96263551e-07
Iter: 1167 loss: 2.96078269e-07
Iter: 1168 loss: 2.96060648e-07
Iter: 1169 loss: 2.95915441e-07
Iter: 1170 loss: 2.95644611e-07
Iter: 1171 loss: 2.95646601e-07
Iter: 1172 loss: 2.9533237e-07
Iter: 1173 loss: 2.96302062e-07
Iter: 1174 loss: 2.9524864e-07
Iter: 1175 loss: 2.95089535e-07
Iter: 1176 loss: 2.95077626e-07
Iter: 1177 loss: 2.94986478e-07
Iter: 1178 loss: 2.94711924e-07
Iter: 1179 loss: 2.96663814e-07
Iter: 1180 loss: 2.94677506e-07
Iter: 1181 loss: 2.94326014e-07
Iter: 1182 loss: 2.94954333e-07
Iter: 1183 loss: 2.94174924e-07
Iter: 1184 loss: 2.9415898e-07
Iter: 1185 loss: 2.94041229e-07
Iter: 1186 loss: 2.93923478e-07
Iter: 1187 loss: 2.93584321e-07
Iter: 1188 loss: 2.97692452e-07
Iter: 1189 loss: 2.93557662e-07
Iter: 1190 loss: 2.93270489e-07
Iter: 1191 loss: 2.94922756e-07
Iter: 1192 loss: 2.93243261e-07
Iter: 1193 loss: 2.93134519e-07
Iter: 1194 loss: 2.93120593e-07
Iter: 1195 loss: 2.93015091e-07
Iter: 1196 loss: 2.92787462e-07
Iter: 1197 loss: 2.94518884e-07
Iter: 1198 loss: 2.92718823e-07
Iter: 1199 loss: 2.9250279e-07
Iter: 1200 loss: 2.93713953e-07
Iter: 1201 loss: 2.92484373e-07
Iter: 1202 loss: 2.92280504e-07
Iter: 1203 loss: 2.94470738e-07
Iter: 1204 loss: 2.92287268e-07
Iter: 1205 loss: 2.9208698e-07
Iter: 1206 loss: 2.91739667e-07
Iter: 1207 loss: 2.9173566e-07
Iter: 1208 loss: 2.91471139e-07
Iter: 1209 loss: 2.95180314e-07
Iter: 1210 loss: 2.91469235e-07
Iter: 1211 loss: 2.91253031e-07
Iter: 1212 loss: 2.92099827e-07
Iter: 1213 loss: 2.91196585e-07
Iter: 1214 loss: 2.91018665e-07
Iter: 1215 loss: 2.90698665e-07
Iter: 1216 loss: 2.90711853e-07
Iter: 1217 loss: 2.90487719e-07
Iter: 1218 loss: 2.91728497e-07
Iter: 1219 loss: 2.90479079e-07
Iter: 1220 loss: 2.90380711e-07
Iter: 1221 loss: 2.90384889e-07
Iter: 1222 loss: 2.90238177e-07
Iter: 1223 loss: 2.90137962e-07
Iter: 1224 loss: 2.90105e-07
Iter: 1225 loss: 2.89958308e-07
Iter: 1226 loss: 2.89787266e-07
Iter: 1227 loss: 2.89768138e-07
Iter: 1228 loss: 2.89468232e-07
Iter: 1229 loss: 2.91528664e-07
Iter: 1230 loss: 2.8944072e-07
Iter: 1231 loss: 2.8913513e-07
Iter: 1232 loss: 2.90312329e-07
Iter: 1233 loss: 2.89030822e-07
Iter: 1234 loss: 2.88888202e-07
Iter: 1235 loss: 2.88585852e-07
Iter: 1236 loss: 2.9327839e-07
Iter: 1237 loss: 2.8857329e-07
Iter: 1238 loss: 2.88303625e-07
Iter: 1239 loss: 2.91195221e-07
Iter: 1240 loss: 2.88294473e-07
Iter: 1241 loss: 2.88193661e-07
Iter: 1242 loss: 2.88170497e-07
Iter: 1243 loss: 2.88080969e-07
Iter: 1244 loss: 2.87861212e-07
Iter: 1245 loss: 2.89610398e-07
Iter: 1246 loss: 2.87814942e-07
Iter: 1247 loss: 2.87577166e-07
Iter: 1248 loss: 2.88398269e-07
Iter: 1249 loss: 2.87515775e-07
Iter: 1250 loss: 2.87286696e-07
Iter: 1251 loss: 2.90497383e-07
Iter: 1252 loss: 2.87287406e-07
Iter: 1253 loss: 2.87074442e-07
Iter: 1254 loss: 2.86927332e-07
Iter: 1255 loss: 2.86854061e-07
Iter: 1256 loss: 2.8668893e-07
Iter: 1257 loss: 2.87216e-07
Iter: 1258 loss: 2.86629842e-07
Iter: 1259 loss: 2.86449165e-07
Iter: 1260 loss: 2.88333695e-07
Iter: 1261 loss: 2.86450643e-07
Iter: 1262 loss: 2.8632158e-07
Iter: 1263 loss: 2.86131637e-07
Iter: 1264 loss: 2.86136583e-07
Iter: 1265 loss: 2.85964518e-07
Iter: 1266 loss: 2.86282216e-07
Iter: 1267 loss: 2.85880162e-07
Iter: 1268 loss: 2.85699059e-07
Iter: 1269 loss: 2.85716254e-07
Iter: 1270 loss: 2.85541e-07
Iter: 1271 loss: 2.85217084e-07
Iter: 1272 loss: 2.9189215e-07
Iter: 1273 loss: 2.85210973e-07
Iter: 1274 loss: 2.84934913e-07
Iter: 1275 loss: 2.84737553e-07
Iter: 1276 loss: 2.84649332e-07
Iter: 1277 loss: 2.84589873e-07
Iter: 1278 loss: 2.84466353e-07
Iter: 1279 loss: 2.842973e-07
Iter: 1280 loss: 2.84642653e-07
Iter: 1281 loss: 2.84236307e-07
Iter: 1282 loss: 2.84136433e-07
Iter: 1283 loss: 2.83904058e-07
Iter: 1284 loss: 2.86387461e-07
Iter: 1285 loss: 2.8386566e-07
Iter: 1286 loss: 2.83830076e-07
Iter: 1287 loss: 2.83757885e-07
Iter: 1288 loss: 2.8362382e-07
Iter: 1289 loss: 2.83486315e-07
Iter: 1290 loss: 2.83458178e-07
Iter: 1291 loss: 2.83223528e-07
Iter: 1292 loss: 2.83377858e-07
Iter: 1293 loss: 2.83079203e-07
Iter: 1294 loss: 2.82792371e-07
Iter: 1295 loss: 2.87223799e-07
Iter: 1296 loss: 2.82805246e-07
Iter: 1297 loss: 2.8265805e-07
Iter: 1298 loss: 2.82445029e-07
Iter: 1299 loss: 2.82443494e-07
Iter: 1300 loss: 2.82201313e-07
Iter: 1301 loss: 2.83740405e-07
Iter: 1302 loss: 2.82156918e-07
Iter: 1303 loss: 2.82002048e-07
Iter: 1304 loss: 2.83853296e-07
Iter: 1305 loss: 2.82005118e-07
Iter: 1306 loss: 2.81938071e-07
Iter: 1307 loss: 2.818187e-07
Iter: 1308 loss: 2.81828193e-07
Iter: 1309 loss: 2.81633277e-07
Iter: 1310 loss: 2.81406784e-07
Iter: 1311 loss: 2.81408802e-07
Iter: 1312 loss: 2.81145731e-07
Iter: 1313 loss: 2.82941301e-07
Iter: 1314 loss: 2.81125949e-07
Iter: 1315 loss: 2.80836332e-07
Iter: 1316 loss: 2.8254459e-07
Iter: 1317 loss: 2.80819279e-07
Iter: 1318 loss: 2.80602933e-07
Iter: 1319 loss: 2.8034421e-07
Iter: 1320 loss: 2.80336252e-07
Iter: 1321 loss: 2.80072271e-07
Iter: 1322 loss: 2.81135669e-07
Iter: 1323 loss: 2.80004485e-07
Iter: 1324 loss: 2.79821336e-07
Iter: 1325 loss: 2.79803515e-07
Iter: 1326 loss: 2.79707649e-07
Iter: 1327 loss: 2.79523704e-07
Iter: 1328 loss: 2.839069e-07
Iter: 1329 loss: 2.79511596e-07
Iter: 1330 loss: 2.79365281e-07
Iter: 1331 loss: 2.7936926e-07
Iter: 1332 loss: 2.7918972e-07
Iter: 1333 loss: 2.78967349e-07
Iter: 1334 loss: 2.78948335e-07
Iter: 1335 loss: 2.7881336e-07
Iter: 1336 loss: 2.80740039e-07
Iter: 1337 loss: 2.78809466e-07
Iter: 1338 loss: 2.7866173e-07
Iter: 1339 loss: 2.78911443e-07
Iter: 1340 loss: 2.78616653e-07
Iter: 1341 loss: 2.78469855e-07
Iter: 1342 loss: 2.78362279e-07
Iter: 1343 loss: 2.78323824e-07
Iter: 1344 loss: 2.78154744e-07
Iter: 1345 loss: 2.78843629e-07
Iter: 1346 loss: 2.78111e-07
Iter: 1347 loss: 2.77946754e-07
Iter: 1348 loss: 2.78740742e-07
Iter: 1349 loss: 2.77927029e-07
Iter: 1350 loss: 2.77717902e-07
Iter: 1351 loss: 2.78170432e-07
Iter: 1352 loss: 2.77658728e-07
Iter: 1353 loss: 2.77503432e-07
Iter: 1354 loss: 2.7718005e-07
Iter: 1355 loss: 2.8250679e-07
Iter: 1356 loss: 2.77197074e-07
Iter: 1357 loss: 2.76925107e-07
Iter: 1358 loss: 2.76917632e-07
Iter: 1359 loss: 2.76674967e-07
Iter: 1360 loss: 2.77238485e-07
Iter: 1361 loss: 2.76583904e-07
Iter: 1362 loss: 2.7648673e-07
Iter: 1363 loss: 2.76408173e-07
Iter: 1364 loss: 2.76348828e-07
Iter: 1365 loss: 2.76168066e-07
Iter: 1366 loss: 2.76186313e-07
Iter: 1367 loss: 2.76091669e-07
Iter: 1368 loss: 2.75973775e-07
Iter: 1369 loss: 2.75969796e-07
Iter: 1370 loss: 2.75883934e-07
Iter: 1371 loss: 2.75882286e-07
Iter: 1372 loss: 2.75780309e-07
Iter: 1373 loss: 2.7558e-07
Iter: 1374 loss: 2.78935033e-07
Iter: 1375 loss: 2.75565355e-07
Iter: 1376 loss: 2.75328745e-07
Iter: 1377 loss: 2.75457467e-07
Iter: 1378 loss: 2.75183879e-07
Iter: 1379 loss: 2.74868256e-07
Iter: 1380 loss: 2.75315472e-07
Iter: 1381 loss: 2.74720065e-07
Iter: 1382 loss: 2.74475326e-07
Iter: 1383 loss: 2.74470437e-07
Iter: 1384 loss: 2.74266796e-07
Iter: 1385 loss: 2.74560023e-07
Iter: 1386 loss: 2.74189375e-07
Iter: 1387 loss: 2.74048546e-07
Iter: 1388 loss: 2.7386136e-07
Iter: 1389 loss: 2.73872473e-07
Iter: 1390 loss: 2.73833024e-07
Iter: 1391 loss: 2.73741819e-07
Iter: 1392 loss: 2.73642058e-07
Iter: 1393 loss: 2.73577484e-07
Iter: 1394 loss: 2.73540792e-07
Iter: 1395 loss: 2.73415253e-07
Iter: 1396 loss: 2.73300884e-07
Iter: 1397 loss: 2.73259019e-07
Iter: 1398 loss: 2.73025648e-07
Iter: 1399 loss: 2.73040939e-07
Iter: 1400 loss: 2.7290892e-07
Iter: 1401 loss: 2.72737566e-07
Iter: 1402 loss: 2.72721195e-07
Iter: 1403 loss: 2.72602563e-07
Iter: 1404 loss: 2.72572521e-07
Iter: 1405 loss: 2.72488251e-07
Iter: 1406 loss: 2.72339832e-07
Iter: 1407 loss: 2.72339719e-07
Iter: 1408 loss: 2.72171633e-07
Iter: 1409 loss: 2.72385392e-07
Iter: 1410 loss: 2.72054109e-07
Iter: 1411 loss: 2.7188878e-07
Iter: 1412 loss: 2.72133718e-07
Iter: 1413 loss: 2.71753606e-07
Iter: 1414 loss: 2.71708927e-07
Iter: 1415 loss: 2.7165936e-07
Iter: 1416 loss: 2.71579069e-07
Iter: 1417 loss: 2.71394583e-07
Iter: 1418 loss: 2.72307034e-07
Iter: 1419 loss: 2.71305964e-07
Iter: 1420 loss: 2.7109229e-07
Iter: 1421 loss: 2.71082683e-07
Iter: 1422 loss: 2.70881571e-07
Iter: 1423 loss: 2.71348142e-07
Iter: 1424 loss: 2.70791475e-07
Iter: 1425 loss: 2.70659e-07
Iter: 1426 loss: 2.70714679e-07
Iter: 1427 loss: 2.70555e-07
Iter: 1428 loss: 2.70433702e-07
Iter: 1429 loss: 2.71785694e-07
Iter: 1430 loss: 2.70436885e-07
Iter: 1431 loss: 2.70345907e-07
Iter: 1432 loss: 2.70341502e-07
Iter: 1433 loss: 2.70252485e-07
Iter: 1434 loss: 2.70160115e-07
Iter: 1435 loss: 2.70303474e-07
Iter: 1436 loss: 2.70121234e-07
Iter: 1437 loss: 2.69939221e-07
Iter: 1438 loss: 2.70592267e-07
Iter: 1439 loss: 2.69909634e-07
Iter: 1440 loss: 2.69776308e-07
Iter: 1441 loss: 2.69630959e-07
Iter: 1442 loss: 2.69625417e-07
Iter: 1443 loss: 2.69400971e-07
Iter: 1444 loss: 2.69593613e-07
Iter: 1445 loss: 2.69268469e-07
Iter: 1446 loss: 2.69016624e-07
Iter: 1447 loss: 2.70672331e-07
Iter: 1448 loss: 2.68981466e-07
Iter: 1449 loss: 2.68771402e-07
Iter: 1450 loss: 2.71190288e-07
Iter: 1451 loss: 2.68780695e-07
Iter: 1452 loss: 2.68677951e-07
Iter: 1453 loss: 2.68535359e-07
Iter: 1454 loss: 2.71828981e-07
Iter: 1455 loss: 2.68534905e-07
Iter: 1456 loss: 2.68456517e-07
Iter: 1457 loss: 2.68444637e-07
Iter: 1458 loss: 2.68359571e-07
Iter: 1459 loss: 2.68303268e-07
Iter: 1460 loss: 2.68272601e-07
Iter: 1461 loss: 2.68137256e-07
Iter: 1462 loss: 2.68533853e-07
Iter: 1463 loss: 2.68092862e-07
Iter: 1464 loss: 2.67915652e-07
Iter: 1465 loss: 2.68121852e-07
Iter: 1466 loss: 2.67848236e-07
Iter: 1467 loss: 2.67685721e-07
Iter: 1468 loss: 2.67596107e-07
Iter: 1469 loss: 2.67501719e-07
Iter: 1470 loss: 2.67321e-07
Iter: 1471 loss: 2.69604072e-07
Iter: 1472 loss: 2.67319251e-07
Iter: 1473 loss: 2.67087785e-07
Iter: 1474 loss: 2.66894119e-07
Iter: 1475 loss: 2.66838839e-07
Iter: 1476 loss: 2.66611579e-07
Iter: 1477 loss: 2.67039979e-07
Iter: 1478 loss: 2.66488712e-07
Iter: 1479 loss: 2.66297434e-07
Iter: 1480 loss: 2.66562608e-07
Iter: 1481 loss: 2.66178887e-07
Iter: 1482 loss: 2.65998381e-07
Iter: 1483 loss: 2.68089281e-07
Iter: 1484 loss: 2.66007646e-07
Iter: 1485 loss: 2.65872472e-07
Iter: 1486 loss: 2.67439475e-07
Iter: 1487 loss: 2.65876679e-07
Iter: 1488 loss: 2.65801106e-07
Iter: 1489 loss: 2.65622532e-07
Iter: 1490 loss: 2.67715166e-07
Iter: 1491 loss: 2.65601955e-07
Iter: 1492 loss: 2.65417157e-07
Iter: 1493 loss: 2.67226568e-07
Iter: 1494 loss: 2.65411472e-07
Iter: 1495 loss: 2.65260951e-07
Iter: 1496 loss: 2.66665438e-07
Iter: 1497 loss: 2.65248985e-07
Iter: 1498 loss: 2.6515869e-07
Iter: 1499 loss: 2.64986056e-07
Iter: 1500 loss: 2.64977189e-07
Iter: 1501 loss: 2.6487578e-07
Iter: 1502 loss: 2.64865974e-07
Iter: 1503 loss: 2.64772837e-07
Iter: 1504 loss: 2.64622827e-07
Iter: 1505 loss: 2.67903943e-07
Iter: 1506 loss: 2.64620297e-07
Iter: 1507 loss: 2.64453121e-07
Iter: 1508 loss: 2.64960079e-07
Iter: 1509 loss: 2.6437732e-07
Iter: 1510 loss: 2.64202839e-07
Iter: 1511 loss: 2.66588586e-07
Iter: 1512 loss: 2.64205426e-07
Iter: 1513 loss: 2.64098702e-07
Iter: 1514 loss: 2.63860812e-07
Iter: 1515 loss: 2.66956732e-07
Iter: 1516 loss: 2.63846772e-07
Iter: 1517 loss: 2.6362639e-07
Iter: 1518 loss: 2.64804669e-07
Iter: 1519 loss: 2.63586287e-07
Iter: 1520 loss: 2.63414051e-07
Iter: 1521 loss: 2.63901512e-07
Iter: 1522 loss: 2.63383725e-07
Iter: 1523 loss: 2.63243379e-07
Iter: 1524 loss: 2.64912302e-07
Iter: 1525 loss: 2.63231271e-07
Iter: 1526 loss: 2.63120569e-07
Iter: 1527 loss: 2.6355147e-07
Iter: 1528 loss: 2.63098286e-07
Iter: 1529 loss: 2.63051618e-07
Iter: 1530 loss: 2.62888506e-07
Iter: 1531 loss: 2.65526324e-07
Iter: 1532 loss: 2.62894389e-07
Iter: 1533 loss: 2.62700951e-07
Iter: 1534 loss: 2.62975846e-07
Iter: 1535 loss: 2.62593289e-07
Iter: 1536 loss: 2.62471417e-07
Iter: 1537 loss: 2.62433986e-07
Iter: 1538 loss: 2.62325443e-07
Iter: 1539 loss: 2.62044125e-07
Iter: 1540 loss: 2.64526733e-07
Iter: 1541 loss: 2.61981569e-07
Iter: 1542 loss: 2.61883741e-07
Iter: 1543 loss: 2.6185009e-07
Iter: 1544 loss: 2.61710056e-07
Iter: 1545 loss: 2.61903097e-07
Iter: 1546 loss: 2.61641674e-07
Iter: 1547 loss: 2.61546859e-07
Iter: 1548 loss: 2.61405262e-07
Iter: 1549 loss: 2.61402818e-07
Iter: 1550 loss: 2.61297828e-07
Iter: 1551 loss: 2.61295583e-07
Iter: 1552 loss: 2.61187495e-07
Iter: 1553 loss: 2.61380762e-07
Iter: 1554 loss: 2.61138297e-07
Iter: 1555 loss: 2.61045102e-07
Iter: 1556 loss: 2.60923343e-07
Iter: 1557 loss: 2.60908024e-07
Iter: 1558 loss: 2.60701341e-07
Iter: 1559 loss: 2.62250182e-07
Iter: 1560 loss: 2.6071092e-07
Iter: 1561 loss: 2.60470813e-07
Iter: 1562 loss: 2.60851039e-07
Iter: 1563 loss: 2.60396433e-07
Iter: 1564 loss: 2.60198334e-07
Iter: 1565 loss: 2.59970591e-07
Iter: 1566 loss: 2.5994234e-07
Iter: 1567 loss: 2.5975362e-07
Iter: 1568 loss: 2.59767091e-07
Iter: 1569 loss: 2.59631349e-07
Iter: 1570 loss: 2.60906688e-07
Iter: 1571 loss: 2.59615803e-07
Iter: 1572 loss: 2.59543697e-07
Iter: 1573 loss: 2.59370609e-07
Iter: 1574 loss: 2.61883599e-07
Iter: 1575 loss: 2.59352419e-07
Iter: 1576 loss: 2.59279943e-07
Iter: 1577 loss: 2.59255501e-07
Iter: 1578 loss: 2.59146702e-07
Iter: 1579 loss: 2.58960142e-07
Iter: 1580 loss: 2.58953889e-07
Iter: 1581 loss: 2.58812e-07
Iter: 1582 loss: 2.58908898e-07
Iter: 1583 loss: 2.58707786e-07
Iter: 1584 loss: 2.58526939e-07
Iter: 1585 loss: 2.60270269e-07
Iter: 1586 loss: 2.58516593e-07
Iter: 1587 loss: 2.58349075e-07
Iter: 1588 loss: 2.59232e-07
Iter: 1589 loss: 2.5834305e-07
Iter: 1590 loss: 2.58244313e-07
Iter: 1591 loss: 2.58177e-07
Iter: 1592 loss: 2.5813182e-07
Iter: 1593 loss: 2.58012903e-07
Iter: 1594 loss: 2.59832632e-07
Iter: 1595 loss: 2.58006935e-07
Iter: 1596 loss: 2.57914508e-07
Iter: 1597 loss: 2.57829242e-07
Iter: 1598 loss: 2.57802043e-07
Iter: 1599 loss: 2.57624862e-07
Iter: 1600 loss: 2.57542894e-07
Iter: 1601 loss: 2.57467889e-07
Iter: 1602 loss: 2.57279424e-07
Iter: 1603 loss: 2.59585022e-07
Iter: 1604 loss: 2.57273484e-07
Iter: 1605 loss: 2.57081467e-07
Iter: 1606 loss: 2.57891315e-07
Iter: 1607 loss: 2.57067882e-07
Iter: 1608 loss: 2.56960732e-07
Iter: 1609 loss: 2.56836017e-07
Iter: 1610 loss: 2.56818737e-07
Iter: 1611 loss: 2.56720398e-07
Iter: 1612 loss: 2.56731084e-07
Iter: 1613 loss: 2.56619529e-07
Iter: 1614 loss: 2.5653668e-07
Iter: 1615 loss: 2.5651218e-07
Iter: 1616 loss: 2.56398152e-07
Iter: 1617 loss: 2.56373426e-07
Iter: 1618 loss: 2.56306038e-07
Iter: 1619 loss: 2.56194397e-07
Iter: 1620 loss: 2.57700947e-07
Iter: 1621 loss: 2.56189736e-07
Iter: 1622 loss: 2.56054847e-07
Iter: 1623 loss: 2.56086139e-07
Iter: 1624 loss: 2.55943121e-07
Iter: 1625 loss: 2.55793367e-07
Iter: 1626 loss: 2.56051976e-07
Iter: 1627 loss: 2.55714241e-07
Iter: 1628 loss: 2.55534871e-07
Iter: 1629 loss: 2.5681004e-07
Iter: 1630 loss: 2.55534815e-07
Iter: 1631 loss: 2.55425448e-07
Iter: 1632 loss: 2.55317929e-07
Iter: 1633 loss: 2.55269526e-07
Iter: 1634 loss: 2.55100616e-07
Iter: 1635 loss: 2.55530892e-07
Iter: 1636 loss: 2.55037037e-07
Iter: 1637 loss: 2.54932559e-07
Iter: 1638 loss: 2.56521218e-07
Iter: 1639 loss: 2.54929148e-07
Iter: 1640 loss: 2.54833083e-07
Iter: 1641 loss: 2.55081204e-07
Iter: 1642 loss: 2.54796305e-07
Iter: 1643 loss: 2.54701206e-07
Iter: 1644 loss: 2.54584791e-07
Iter: 1645 loss: 2.54582517e-07
Iter: 1646 loss: 2.54448878e-07
Iter: 1647 loss: 2.56189168e-07
Iter: 1648 loss: 2.54457063e-07
Iter: 1649 loss: 2.54297674e-07
Iter: 1650 loss: 2.54360742e-07
Iter: 1651 loss: 2.54188734e-07
Iter: 1652 loss: 2.54053276e-07
Iter: 1653 loss: 2.53904489e-07
Iter: 1654 loss: 2.53879449e-07
Iter: 1655 loss: 2.53783071e-07
Iter: 1656 loss: 2.53769173e-07
Iter: 1657 loss: 2.53655941e-07
Iter: 1658 loss: 2.53615099e-07
Iter: 1659 loss: 2.53566668e-07
Iter: 1660 loss: 2.53454061e-07
Iter: 1661 loss: 2.54044267e-07
Iter: 1662 loss: 2.5344832e-07
Iter: 1663 loss: 2.53341966e-07
Iter: 1664 loss: 2.53537905e-07
Iter: 1665 loss: 2.53288732e-07
Iter: 1666 loss: 2.53159385e-07
Iter: 1667 loss: 2.53089411e-07
Iter: 1668 loss: 2.53044448e-07
Iter: 1669 loss: 2.5289512e-07
Iter: 1670 loss: 2.53295099e-07
Iter: 1671 loss: 2.52827846e-07
Iter: 1672 loss: 2.52699806e-07
Iter: 1673 loss: 2.52716944e-07
Iter: 1674 loss: 2.52593509e-07
Iter: 1675 loss: 2.52389043e-07
Iter: 1676 loss: 2.56513317e-07
Iter: 1677 loss: 2.52392795e-07
Iter: 1678 loss: 2.52237015e-07
Iter: 1679 loss: 2.52621334e-07
Iter: 1680 loss: 2.5215752e-07
Iter: 1681 loss: 2.5208675e-07
Iter: 1682 loss: 2.52080156e-07
Iter: 1683 loss: 2.51977724e-07
Iter: 1684 loss: 2.51817596e-07
Iter: 1685 loss: 2.51820097e-07
Iter: 1686 loss: 2.51647862e-07
Iter: 1687 loss: 2.51717807e-07
Iter: 1688 loss: 2.51558816e-07
Iter: 1689 loss: 2.51425774e-07
Iter: 1690 loss: 2.5140352e-07
Iter: 1691 loss: 2.51317488e-07
Iter: 1692 loss: 2.51120184e-07
Iter: 1693 loss: 2.55165844e-07
Iter: 1694 loss: 2.51120696e-07
Iter: 1695 loss: 2.50966878e-07
Iter: 1696 loss: 2.5324195e-07
Iter: 1697 loss: 2.50970629e-07
Iter: 1698 loss: 2.50822524e-07
Iter: 1699 loss: 2.5084924e-07
Iter: 1700 loss: 2.50707927e-07
Iter: 1701 loss: 2.50578637e-07
Iter: 1702 loss: 2.50795409e-07
Iter: 1703 loss: 2.50514461e-07
Iter: 1704 loss: 2.50407084e-07
Iter: 1705 loss: 2.50523101e-07
Iter: 1706 loss: 2.50335944e-07
Iter: 1707 loss: 2.50268897e-07
Iter: 1708 loss: 2.50267874e-07
Iter: 1709 loss: 2.50162543e-07
Iter: 1710 loss: 2.50056075e-07
Iter: 1711 loss: 2.50038397e-07
Iter: 1712 loss: 2.49881907e-07
Iter: 1713 loss: 2.49681563e-07
Iter: 1714 loss: 2.49668176e-07
Iter: 1715 loss: 2.49737468e-07
Iter: 1716 loss: 2.49542893e-07
Iter: 1717 loss: 2.49463938e-07
Iter: 1718 loss: 2.49289087e-07
Iter: 1719 loss: 2.51363048e-07
Iter: 1720 loss: 2.49273853e-07
Iter: 1721 loss: 2.4916892e-07
Iter: 1722 loss: 2.49159967e-07
Iter: 1723 loss: 2.49065295e-07
Iter: 1724 loss: 2.492134e-07
Iter: 1725 loss: 2.49018825e-07
Iter: 1726 loss: 2.48953654e-07
Iter: 1727 loss: 2.49088714e-07
Iter: 1728 loss: 2.48932508e-07
Iter: 1729 loss: 2.48849915e-07
Iter: 1730 loss: 2.48824335e-07
Iter: 1731 loss: 2.4875709e-07
Iter: 1732 loss: 2.48685978e-07
Iter: 1733 loss: 2.48524174e-07
Iter: 1734 loss: 2.51741341e-07
Iter: 1735 loss: 2.48530796e-07
Iter: 1736 loss: 2.48271846e-07
Iter: 1737 loss: 2.48517409e-07
Iter: 1738 loss: 2.48121125e-07
Iter: 1739 loss: 2.47898726e-07
Iter: 1740 loss: 2.50139408e-07
Iter: 1741 loss: 2.4787488e-07
Iter: 1742 loss: 2.47739507e-07
Iter: 1743 loss: 2.47724358e-07
Iter: 1744 loss: 2.47633466e-07
Iter: 1745 loss: 2.474832e-07
Iter: 1746 loss: 2.47460321e-07
Iter: 1747 loss: 2.47314489e-07
Iter: 1748 loss: 2.47786062e-07
Iter: 1749 loss: 2.47257134e-07
Iter: 1750 loss: 2.4715132e-07
Iter: 1751 loss: 2.47141429e-07
Iter: 1752 loss: 2.47050366e-07
Iter: 1753 loss: 2.46914567e-07
Iter: 1754 loss: 2.46904733e-07
Iter: 1755 loss: 2.46771179e-07
Iter: 1756 loss: 2.46678184e-07
Iter: 1757 loss: 2.46626598e-07
Iter: 1758 loss: 2.46542527e-07
Iter: 1759 loss: 2.46509586e-07
Iter: 1760 loss: 2.46395246e-07
Iter: 1761 loss: 2.4643731e-07
Iter: 1762 loss: 2.46324021e-07
Iter: 1763 loss: 2.46236567e-07
Iter: 1764 loss: 2.46030396e-07
Iter: 1765 loss: 2.49706034e-07
Iter: 1766 loss: 2.46026843e-07
Iter: 1767 loss: 2.45876976e-07
Iter: 1768 loss: 2.48430126e-07
Iter: 1769 loss: 2.45869501e-07
Iter: 1770 loss: 2.45729808e-07
Iter: 1771 loss: 2.46581294e-07
Iter: 1772 loss: 2.45712215e-07
Iter: 1773 loss: 2.45607254e-07
Iter: 1774 loss: 2.453977e-07
Iter: 1775 loss: 2.48171318e-07
Iter: 1776 loss: 2.45380164e-07
Iter: 1777 loss: 2.45244109e-07
Iter: 1778 loss: 2.45219923e-07
Iter: 1779 loss: 2.45090689e-07
Iter: 1780 loss: 2.45216796e-07
Iter: 1781 loss: 2.45009176e-07
Iter: 1782 loss: 2.44872467e-07
Iter: 1783 loss: 2.44844159e-07
Iter: 1784 loss: 2.44735759e-07
Iter: 1785 loss: 2.44628495e-07
Iter: 1786 loss: 2.45568231e-07
Iter: 1787 loss: 2.4461184e-07
Iter: 1788 loss: 2.44535897e-07
Iter: 1789 loss: 2.44536807e-07
Iter: 1790 loss: 2.44457567e-07
Iter: 1791 loss: 2.44295e-07
Iter: 1792 loss: 2.45851169e-07
Iter: 1793 loss: 2.44273025e-07
Iter: 1794 loss: 2.44113608e-07
Iter: 1795 loss: 2.4490609e-07
Iter: 1796 loss: 2.4408061e-07
Iter: 1797 loss: 2.43908914e-07
Iter: 1798 loss: 2.45570391e-07
Iter: 1799 loss: 2.43907778e-07
Iter: 1800 loss: 2.43790907e-07
Iter: 1801 loss: 2.43513171e-07
Iter: 1802 loss: 2.47470155e-07
Iter: 1803 loss: 2.43487193e-07
Iter: 1804 loss: 2.43232734e-07
Iter: 1805 loss: 2.43856391e-07
Iter: 1806 loss: 2.43169893e-07
Iter: 1807 loss: 2.43055581e-07
Iter: 1808 loss: 2.43027785e-07
Iter: 1809 loss: 2.42928962e-07
Iter: 1810 loss: 2.43438791e-07
Iter: 1811 loss: 2.42905514e-07
Iter: 1812 loss: 2.42847165e-07
Iter: 1813 loss: 2.42733847e-07
Iter: 1814 loss: 2.45223816e-07
Iter: 1815 loss: 2.42727623e-07
Iter: 1816 loss: 2.42647332e-07
Iter: 1817 loss: 2.4262738e-07
Iter: 1818 loss: 2.42548595e-07
Iter: 1819 loss: 2.4243559e-07
Iter: 1820 loss: 2.42423283e-07
Iter: 1821 loss: 2.42272847e-07
Iter: 1822 loss: 2.42241128e-07
Iter: 1823 loss: 2.42141425e-07
Iter: 1824 loss: 2.41935709e-07
Iter: 1825 loss: 2.4380364e-07
Iter: 1826 loss: 2.41951398e-07
Iter: 1827 loss: 2.41795192e-07
Iter: 1828 loss: 2.43578796e-07
Iter: 1829 loss: 2.41798233e-07
Iter: 1830 loss: 2.41714162e-07
Iter: 1831 loss: 2.41541898e-07
Iter: 1832 loss: 2.44616444e-07
Iter: 1833 loss: 2.41539453e-07
Iter: 1834 loss: 2.41495172e-07
Iter: 1835 loss: 2.41453449e-07
Iter: 1836 loss: 2.41386203e-07
Iter: 1837 loss: 2.41271806e-07
Iter: 1838 loss: 2.41268253e-07
Iter: 1839 loss: 2.41140185e-07
Iter: 1840 loss: 2.41226758e-07
Iter: 1841 loss: 2.41041164e-07
Iter: 1842 loss: 2.40887971e-07
Iter: 1843 loss: 2.40881832e-07
Iter: 1844 loss: 2.40790087e-07
Iter: 1845 loss: 2.40765416e-07
Iter: 1846 loss: 2.40670545e-07
Iter: 1847 loss: 2.40572888e-07
Iter: 1848 loss: 2.41185802e-07
Iter: 1849 loss: 2.40564134e-07
Iter: 1850 loss: 2.40432e-07
Iter: 1851 loss: 2.40743475e-07
Iter: 1852 loss: 2.40383656e-07
Iter: 1853 loss: 2.40303308e-07
Iter: 1854 loss: 2.40256782e-07
Iter: 1855 loss: 2.4021557e-07
Iter: 1856 loss: 2.40108506e-07
Iter: 1857 loss: 2.40216878e-07
Iter: 1858 loss: 2.40031341e-07
Iter: 1859 loss: 2.39961736e-07
Iter: 1860 loss: 2.39945052e-07
Iter: 1861 loss: 2.39848106e-07
Iter: 1862 loss: 2.39720436e-07
Iter: 1863 loss: 2.39721288e-07
Iter: 1864 loss: 2.3952569e-07
Iter: 1865 loss: 2.39385741e-07
Iter: 1866 loss: 2.39340437e-07
Iter: 1867 loss: 2.3929627e-07
Iter: 1868 loss: 2.39225415e-07
Iter: 1869 loss: 2.39108317e-07
Iter: 1870 loss: 2.38877874e-07
Iter: 1871 loss: 2.42988506e-07
Iter: 1872 loss: 2.38880602e-07
Iter: 1873 loss: 2.38752818e-07
Iter: 1874 loss: 2.39467141e-07
Iter: 1875 loss: 2.38724482e-07
Iter: 1876 loss: 2.38690092e-07
Iter: 1877 loss: 2.38675369e-07
Iter: 1878 loss: 2.38620032e-07
Iter: 1879 loss: 2.38505663e-07
Iter: 1880 loss: 2.40353984e-07
Iter: 1881 loss: 2.38517e-07
Iter: 1882 loss: 2.38392687e-07
Iter: 1883 loss: 2.39510314e-07
Iter: 1884 loss: 2.38402265e-07
Iter: 1885 loss: 2.38266537e-07
Iter: 1886 loss: 2.38332461e-07
Iter: 1887 loss: 2.38199092e-07
Iter: 1888 loss: 2.38082521e-07
Iter: 1889 loss: 2.37862764e-07
Iter: 1890 loss: 2.37863219e-07
Iter: 1891 loss: 2.3761234e-07
Iter: 1892 loss: 2.39027685e-07
Iter: 1893 loss: 2.37580011e-07
Iter: 1894 loss: 2.37407249e-07
Iter: 1895 loss: 2.39665241e-07
Iter: 1896 loss: 2.37418135e-07
Iter: 1897 loss: 2.37299446e-07
Iter: 1898 loss: 2.37272573e-07
Iter: 1899 loss: 2.37179492e-07
Iter: 1900 loss: 2.37056923e-07
Iter: 1901 loss: 2.37152946e-07
Iter: 1902 loss: 2.36968e-07
Iter: 1903 loss: 2.36886478e-07
Iter: 1904 loss: 2.36884233e-07
Iter: 1905 loss: 2.36795643e-07
Iter: 1906 loss: 2.36720922e-07
Iter: 1907 loss: 2.36697133e-07
Iter: 1908 loss: 2.36622725e-07
Iter: 1909 loss: 2.3683107e-07
Iter: 1910 loss: 2.36577506e-07
Iter: 1911 loss: 2.36471394e-07
Iter: 1912 loss: 2.36859051e-07
Iter: 1913 loss: 2.36428448e-07
Iter: 1914 loss: 2.36310186e-07
Iter: 1915 loss: 2.36103773e-07
Iter: 1916 loss: 2.36103858e-07
Iter: 1917 loss: 2.36038716e-07
Iter: 1918 loss: 2.36004325e-07
Iter: 1919 loss: 2.35892543e-07
Iter: 1920 loss: 2.35797103e-07
Iter: 1921 loss: 2.35780092e-07
Iter: 1922 loss: 2.35644677e-07
Iter: 1923 loss: 2.35804251e-07
Iter: 1924 loss: 2.35576124e-07
Iter: 1925 loss: 2.35464412e-07
Iter: 1926 loss: 2.37103222e-07
Iter: 1927 loss: 2.35460448e-07
Iter: 1928 loss: 2.35349248e-07
Iter: 1929 loss: 2.35489424e-07
Iter: 1930 loss: 2.35268828e-07
Iter: 1931 loss: 2.35162375e-07
Iter: 1932 loss: 2.35149656e-07
Iter: 1933 loss: 2.35069848e-07
Iter: 1934 loss: 2.34943826e-07
Iter: 1935 loss: 2.35122982e-07
Iter: 1936 loss: 2.34865041e-07
Iter: 1937 loss: 2.34774021e-07
Iter: 1938 loss: 2.34771605e-07
Iter: 1939 loss: 2.34673678e-07
Iter: 1940 loss: 2.34603434e-07
Iter: 1941 loss: 2.34568503e-07
Iter: 1942 loss: 2.34488084e-07
Iter: 1943 loss: 2.34811978e-07
Iter: 1944 loss: 2.34479543e-07
Iter: 1945 loss: 2.34365984e-07
Iter: 1946 loss: 2.34571942e-07
Iter: 1947 loss: 2.3432716e-07
Iter: 1948 loss: 2.34226974e-07
Iter: 1949 loss: 2.34081043e-07
Iter: 1950 loss: 2.34075856e-07
Iter: 1951 loss: 2.33992353e-07
Iter: 1952 loss: 2.33976863e-07
Iter: 1953 loss: 2.33871958e-07
Iter: 1954 loss: 2.33697733e-07
Iter: 1955 loss: 2.33682869e-07
Iter: 1956 loss: 2.33535573e-07
Iter: 1957 loss: 2.33711319e-07
Iter: 1958 loss: 2.33433866e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8
+ date
Wed Oct 21 23:43:48 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/500_500_500_500_1 --function f1 --psi 2 --phi 0.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8baf96730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bae3e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bae288c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8baed9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8baedf6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bad88a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bad6f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8badc0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff86c843378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff86c843730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8bad32a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff86c793620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff86c793a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff86c82dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff86c82d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff86c765268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff86c765730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff830267620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8301c5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8301f8d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff83021b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff83022dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff830164620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff830110268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff830110a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff86c7e6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8301a89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8301bc620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8300722f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff83007bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff83008b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff83002a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff83002a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8300196a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e4767ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff7e476fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.017615316
test_loss: 0.016847076
train_loss: 0.0070188497
test_loss: 0.006672334
train_loss: 0.004133524
test_loss: 0.0043001743
train_loss: 0.0031517216
test_loss: 0.0034147317
train_loss: 0.0029653385
test_loss: 0.0028647648
train_loss: 0.0028117697
test_loss: 0.0029592346
train_loss: 0.0024830978
test_loss: 0.0030059176
train_loss: 0.0025824485
test_loss: 0.0029627474
train_loss: 0.0027377089
test_loss: 0.0027639535
train_loss: 0.0025851966
test_loss: 0.0027780975
train_loss: 0.002376382
test_loss: 0.0027620425
train_loss: 0.0025204252
test_loss: 0.0027680746
train_loss: 0.0025045406
test_loss: 0.0028689052
train_loss: 0.0022443975
test_loss: 0.0026388397
train_loss: 0.0022913597
test_loss: 0.0025898176
train_loss: 0.0026754856
test_loss: 0.00286904
train_loss: 0.0022761258
test_loss: 0.002779333
train_loss: 0.0025197337
test_loss: 0.002986152
train_loss: 0.0023085524
test_loss: 0.0026012622
train_loss: 0.0024771183
test_loss: 0.0027298876
train_loss: 0.0028740093
test_loss: 0.0025923077
train_loss: 0.002423736
test_loss: 0.0027623726
train_loss: 0.0026205909
test_loss: 0.003037651
train_loss: 0.002535705
test_loss: 0.0026810823
train_loss: 0.0021447246
test_loss: 0.0025597622
train_loss: 0.0022833943
test_loss: 0.0025351506
train_loss: 0.0020949384
test_loss: 0.0024907151
train_loss: 0.0022760243
test_loss: 0.0025263894
train_loss: 0.0024012954
test_loss: 0.0026274945
train_loss: 0.0021900637
test_loss: 0.0025538756
train_loss: 0.0021585592
test_loss: 0.0026031802
train_loss: 0.002101943
test_loss: 0.002519281
train_loss: 0.0020161658
test_loss: 0.0025291757
train_loss: 0.0022680403
test_loss: 0.002593491
train_loss: 0.0021722112
test_loss: 0.0026164271
train_loss: 0.002060126
test_loss: 0.0024165972
train_loss: 0.002261641
test_loss: 0.002517233
train_loss: 0.0022273997
test_loss: 0.0024820988
train_loss: 0.0022023416
test_loss: 0.002550483
train_loss: 0.0023081563
test_loss: 0.0025694761
train_loss: 0.0021542269
test_loss: 0.0024520974
train_loss: 0.00217378
test_loss: 0.0024576394
train_loss: 0.002179645
test_loss: 0.002456305
train_loss: 0.0020696018
test_loss: 0.0024870597
train_loss: 0.002165024
test_loss: 0.0023966227
train_loss: 0.0019151928
test_loss: 0.0022462003
train_loss: 0.0021047245
test_loss: 0.0024205865
train_loss: 0.0022024333
test_loss: 0.002432134
train_loss: 0.002138614
test_loss: 0.0024420265
train_loss: 0.0021984845
test_loss: 0.0026022526
train_loss: 0.0021235505
test_loss: 0.002655972
train_loss: 0.002095076
test_loss: 0.0025649953
train_loss: 0.0021620232
test_loss: 0.002567317
train_loss: 0.0021616605
test_loss: 0.0025249221
train_loss: 0.0020862904
test_loss: 0.002563145
train_loss: 0.0023751734
test_loss: 0.0026920722
train_loss: 0.0020810512
test_loss: 0.0024585745
train_loss: 0.0019671589
test_loss: 0.00236714
train_loss: 0.0022464783
test_loss: 0.0025757833
train_loss: 0.0024878448
test_loss: 0.002652826
train_loss: 0.0023539714
test_loss: 0.0025380168
train_loss: 0.0021505689
test_loss: 0.0024157064
train_loss: 0.0021091772
test_loss: 0.00238341
train_loss: 0.0020671757
test_loss: 0.0024049522
train_loss: 0.002018892
test_loss: 0.002417694
train_loss: 0.0019107616
test_loss: 0.002554478
train_loss: 0.0024845884
test_loss: 0.002699029
train_loss: 0.0024474196
test_loss: 0.002557311
train_loss: 0.002404639
test_loss: 0.0026715396
train_loss: 0.0020572557
test_loss: 0.0024922553
train_loss: 0.0021048656
test_loss: 0.002459456
train_loss: 0.00212435
test_loss: 0.002418707
train_loss: 0.0023232305
test_loss: 0.0027899086
train_loss: 0.0023776907
test_loss: 0.0025262865
train_loss: 0.002105398
test_loss: 0.0024951636
train_loss: 0.0021849396
test_loss: 0.0026646573
train_loss: 0.002041969
test_loss: 0.0025129605
train_loss: 0.0021901203
test_loss: 0.0024029887
train_loss: 0.002375729
test_loss: 0.002519152
train_loss: 0.0019883034
test_loss: 0.0024218985
train_loss: 0.0019737747
test_loss: 0.0024561896
train_loss: 0.0022276475
test_loss: 0.0025613701
train_loss: 0.0021360128
test_loss: 0.0025862071
train_loss: 0.0020139623
test_loss: 0.0023395598
train_loss: 0.0020425366
test_loss: 0.0024367566
train_loss: 0.0020061384
test_loss: 0.0024224024
train_loss: 0.0020789413
test_loss: 0.0024619424
train_loss: 0.002003798
test_loss: 0.0023459843
train_loss: 0.0020587998
test_loss: 0.0025156515
train_loss: 0.002004055
test_loss: 0.0024048246
train_loss: 0.0020756982
test_loss: 0.002475568
train_loss: 0.0024142773
test_loss: 0.0025255575
train_loss: 0.001979192
test_loss: 0.0023621258
train_loss: 0.0018955537
test_loss: 0.0023073354
train_loss: 0.0020765923
test_loss: 0.002276369
train_loss: 0.0021724114
test_loss: 0.002441745
train_loss: 0.0021010693
test_loss: 0.0025285503
train_loss: 0.0023587877
test_loss: 0.002509134
train_loss: 0.0022298396
test_loss: 0.0025507817
train_loss: 0.0020869793
test_loss: 0.002502963
train_loss: 0.0019585364
test_loss: 0.0023523022
train_loss: 0.0019569562
test_loss: 0.0023607337
train_loss: 0.0020217486
test_loss: 0.0023712646
train_loss: 0.0021483537
test_loss: 0.0024080735
train_loss: 0.0024036327
test_loss: 0.002459067
train_loss: 0.0022867029
test_loss: 0.0024407187
train_loss: 0.00213958
test_loss: 0.002422139
train_loss: 0.002248159
test_loss: 0.0025779672
train_loss: 0.0022801433
test_loss: 0.0026042797
train_loss: 0.0020896136
test_loss: 0.0024580148
train_loss: 0.0021687944
test_loss: 0.0024925217
train_loss: 0.0021406785
test_loss: 0.002399652
train_loss: 0.001921811
test_loss: 0.0024330923
train_loss: 0.0021290518
test_loss: 0.0024219137
train_loss: 0.0020882003
test_loss: 0.0025644356
train_loss: 0.0018671093
test_loss: 0.002312888
train_loss: 0.0020089834
test_loss: 0.0026979658
train_loss: 0.0022265192
test_loss: 0.002325599
train_loss: 0.0020862343
test_loss: 0.002332824
train_loss: 0.0021758524
test_loss: 0.0023454875
train_loss: 0.002077187
test_loss: 0.0023202517
train_loss: 0.001984981
test_loss: 0.0023403796
train_loss: 0.0020203374
test_loss: 0.002483274
train_loss: 0.0022790716
test_loss: 0.0026731936
train_loss: 0.0024521067
test_loss: 0.0027272352
train_loss: 0.001982786
test_loss: 0.00239897
train_loss: 0.0021130557
test_loss: 0.0025214348
train_loss: 0.0022153193
test_loss: 0.0025161013
train_loss: 0.0019702392
test_loss: 0.0025301897
train_loss: 0.0020247335
test_loss: 0.0024033054
train_loss: 0.0024867402
test_loss: 0.00239615
train_loss: 0.0020161078
test_loss: 0.0023928443
train_loss: 0.0020710463
test_loss: 0.0024259498
train_loss: 0.002111461
test_loss: 0.002356645
train_loss: 0.0019439281
test_loss: 0.0024685154
train_loss: 0.002267241
test_loss: 0.0024099085
train_loss: 0.002102326
test_loss: 0.0023189017
train_loss: 0.0021319767
test_loss: 0.002397213
train_loss: 0.0020580841
test_loss: 0.0023810104
train_loss: 0.0019564114
test_loss: 0.0023882063
train_loss: 0.0024808256
test_loss: 0.0024377299
train_loss: 0.0020709857
test_loss: 0.0025261738
train_loss: 0.0020254725
test_loss: 0.0025792634
train_loss: 0.0020498997
test_loss: 0.0024720852
train_loss: 0.0021180552
test_loss: 0.0024008339
train_loss: 0.0019446351
test_loss: 0.0024309207
train_loss: 0.0019665458
test_loss: 0.0023626736
train_loss: 0.0020857614
test_loss: 0.0023624944
train_loss: 0.0024103501
test_loss: 0.0024875398
train_loss: 0.002195493
test_loss: 0.0024137925
train_loss: 0.0021953913
test_loss: 0.0024941426
train_loss: 0.002099609
test_loss: 0.002313753
train_loss: 0.0021270183
test_loss: 0.0023470814
train_loss: 0.0022086015
test_loss: 0.002505207
train_loss: 0.0021062903
test_loss: 0.0023313956
train_loss: 0.0020061939
test_loss: 0.002318562
train_loss: 0.002012173
test_loss: 0.0022437451
train_loss: 0.0021198797
test_loss: 0.0023835327
train_loss: 0.0019808742
test_loss: 0.0024362921
train_loss: 0.0020271535
test_loss: 0.0024404724
train_loss: 0.0018739554
test_loss: 0.0023448018
train_loss: 0.0022681295
test_loss: 0.0024419709
train_loss: 0.0021297126
test_loss: 0.0023715603
train_loss: 0.002021974
test_loss: 0.0023522684
train_loss: 0.0020316152
test_loss: 0.0024136598
train_loss: 0.0021844204
test_loss: 0.0024755371
train_loss: 0.0019073035
test_loss: 0.0025682184
train_loss: 0.0019537152
test_loss: 0.002358651
train_loss: 0.0019774316
test_loss: 0.0022678296
train_loss: 0.0018415339
test_loss: 0.00241446
train_loss: 0.0019717065
test_loss: 0.0023978704
train_loss: 0.0021044826
test_loss: 0.0025618135
train_loss: 0.002073421
test_loss: 0.0025381155
train_loss: 0.0020150417
test_loss: 0.002341998
train_loss: 0.0021664752
test_loss: 0.0025290074
train_loss: 0.0020590085
test_loss: 0.0025574905
train_loss: 0.0020348318
test_loss: 0.0026441365
train_loss: 0.0019694988
test_loss: 0.0023378807
train_loss: 0.0021417509
test_loss: 0.0024463895
train_loss: 0.0020700225
test_loss: 0.0023231846
train_loss: 0.0020194834
test_loss: 0.0022965374
train_loss: 0.0018365056
test_loss: 0.0023171005
train_loss: 0.0020761122
test_loss: 0.0023557742
train_loss: 0.0021248008
test_loss: 0.0024266897
train_loss: 0.0021404433
test_loss: 0.0024666854
train_loss: 0.002152054
test_loss: 0.0022957153
train_loss: 0.00200383
test_loss: 0.0023477061
train_loss: 0.002146183
test_loss: 0.0026982734
train_loss: 0.0020663743
test_loss: 0.0024708663
train_loss: 0.0020984411
test_loss: 0.0025119502
train_loss: 0.0019559376
test_loss: 0.0024991184
train_loss: 0.0021034929
test_loss: 0.0024395485
train_loss: 0.0020736116
test_loss: 0.0023083258
train_loss: 0.0019982276
test_loss: 0.00242545
train_loss: 0.001971847
test_loss: 0.002347266
train_loss: 0.002034558
test_loss: 0.002255135
train_loss: 0.0019101006
test_loss: 0.0023513655
train_loss: 0.0019795615
test_loss: 0.0023658094
train_loss: 0.0021481682
test_loss: 0.0023664136
train_loss: 0.0020517122
test_loss: 0.0022344086
train_loss: 0.0020325945
test_loss: 0.0024708952
train_loss: 0.0018002868
test_loss: 0.002335799
train_loss: 0.0022326384
test_loss: 0.0027048006
train_loss: 0.0019952375
test_loss: 0.0023239646
train_loss: 0.0020709801
test_loss: 0.0024297745
train_loss: 0.0021582297
test_loss: 0.0024520145
train_loss: 0.0020258368
test_loss: 0.002614587
train_loss: 0.0019503634
test_loss: 0.0025799123
train_loss: 0.002017315
test_loss: 0.0025076834
train_loss: 0.002300771
test_loss: 0.0023580017
train_loss: 0.0020956108
test_loss: 0.0024354963
train_loss: 0.0021465623
test_loss: 0.0025747481
train_loss: 0.0020805234
test_loss: 0.002421096
train_loss: 0.0021111486
test_loss: 0.0024551612
train_loss: 0.0022236162
test_loss: 0.0025064433
train_loss: 0.002230546
test_loss: 0.0024050467
train_loss: 0.0019458348
test_loss: 0.0022544651
train_loss: 0.0021351485
test_loss: 0.0024463714
train_loss: 0.0022195345
test_loss: 0.0024196913
train_loss: 0.0020786263
test_loss: 0.0023060942
train_loss: 0.00212211
test_loss: 0.0026436176
train_loss: 0.0021433681
test_loss: 0.0023259316
train_loss: 0.0020317214
test_loss: 0.0023796763
train_loss: 0.0022338233
test_loss: 0.0025995732
train_loss: 0.0021714
test_loss: 0.0025266875
train_loss: 0.0025871359
test_loss: 0.0024167206
train_loss: 0.0018631071
test_loss: 0.0023821613
train_loss: 0.0022244381
test_loss: 0.0023916678
train_loss: 0.0019936096
test_loss: 0.0022735344
train_loss: 0.0019232212
test_loss: 0.0023775762
train_loss: 0.0017800158
test_loss: 0.0021874139
train_loss: 0.0018933548
test_loss: 0.0022806393
train_loss: 0.0021497705
test_loss: 0.0025008891
train_loss: 0.0023313973
test_loss: 0.0023346448
train_loss: 0.0021297457
test_loss: 0.0023219048
train_loss: 0.0019472255
test_loss: 0.0023141939
train_loss: 0.0020356309
test_loss: 0.00251795
train_loss: 0.002415272
test_loss: 0.002512761
train_loss: 0.0018569541
test_loss: 0.0023744933
train_loss: 0.001988174
test_loss: 0.0024802745
train_loss: 0.0019626787
test_loss: 0.0023112458
train_loss: 0.0020502512
test_loss: 0.0024952642
train_loss: 0.0018741445
test_loss: 0.00240659
train_loss: 0.0021138606
test_loss: 0.0023585798
train_loss: 0.0020336127
test_loss: 0.002484029
train_loss: 0.0019555255
test_loss: 0.0022233594
train_loss: 0.0020887307
test_loss: 0.0022653767
train_loss: 0.001961127
test_loss: 0.0022196332
train_loss: 0.0020663664
test_loss: 0.0023587234
train_loss: 0.002010798
test_loss: 0.0023901737
train_loss: 0.0019259433
test_loss: 0.0026156544
train_loss: 0.002082738
test_loss: 0.0026121482
train_loss: 0.002062497
test_loss: 0.0023939724
train_loss: 0.0019024448
test_loss: 0.0025034829
train_loss: 0.0020831975
test_loss: 0.002638048
train_loss: 0.0020558015
test_loss: 0.0024558178
train_loss: 0.0017824923
test_loss: 0.002222483
train_loss: 0.0019531953
test_loss: 0.0026092632
train_loss: 0.002178407
test_loss: 0.002502842
train_loss: 0.0020089343
test_loss: 0.0024942735
train_loss: 0.002188305
test_loss: 0.0023590717
train_loss: 0.002047334
test_loss: 0.0024300092
train_loss: 0.0020629829
test_loss: 0.0023880543
train_loss: 0.0019611241
test_loss: 0.002437201
train_loss: 0.001978991
test_loss: 0.0024690037
train_loss: 0.0019641526
test_loss: 0.0022697144
train_loss: 0.001984168
test_loss: 0.0022766327
train_loss: 0.0019331046
test_loss: 0.002291003
train_loss: 0.0019899919
test_loss: 0.0022875797
train_loss: 0.0022040422
test_loss: 0.0023042012
train_loss: 0.0020210643
test_loss: 0.0024880315
train_loss: 0.0019034024
test_loss: 0.0023430125
train_loss: 0.0019863409
test_loss: 0.002272469
train_loss: 0.0021764194
test_loss: 0.0022321807
train_loss: 0.002043581
test_loss: 0.002281041
train_loss: 0.0020670258
test_loss: 0.0023964045
train_loss: 0.0018648215
test_loss: 0.002343933
train_loss: 0.0021349445
test_loss: 0.0023522414
train_loss: 0.0018708548
test_loss: 0.0022812558
train_loss: 0.0019392448
test_loss: 0.002289815
train_loss: 0.002108278
test_loss: 0.0027849032
train_loss: 0.0020566059
test_loss: 0.0022804406
train_loss: 0.0019743259
test_loss: 0.0025166983
train_loss: 0.001891211
test_loss: 0.002418088
train_loss: 0.0019649544
test_loss: 0.0023101622
train_loss: 0.0018282903
test_loss: 0.0021618088
train_loss: 0.0018675558
test_loss: 0.0023044962
train_loss: 0.0018927731
test_loss: 0.0023576366
train_loss: 0.0018370836
test_loss: 0.0021706924
train_loss: 0.0019559963
test_loss: 0.0023560645
train_loss: 0.00190713
test_loss: 0.0022641623
train_loss: 0.001950032
test_loss: 0.0023464335
train_loss: 0.0021420687
test_loss: 0.0025004384
train_loss: 0.00215935
test_loss: 0.0025575047
train_loss: 0.002037109
test_loss: 0.0022997553
train_loss: 0.0021334537
test_loss: 0.0024699962
train_loss: 0.002480528
test_loss: 0.0025074442
train_loss: 0.0019643074
test_loss: 0.0023913186
train_loss: 0.0020267738
test_loss: 0.002568628
train_loss: 0.0018507259
test_loss: 0.002416567
train_loss: 0.0019237276
test_loss: 0.0024248448
train_loss: 0.0018868104
test_loss: 0.002267999
train_loss: 0.0020516363
test_loss: 0.002383463
train_loss: 0.0017876399
test_loss: 0.0021731637
train_loss: 0.0019336245
test_loss: 0.0023184603
train_loss: 0.0019662597
test_loss: 0.0023698253
train_loss: 0.0019323818
test_loss: 0.0025473707
train_loss: 0.002232871
test_loss: 0.0022396396
train_loss: 0.0018739706
test_loss: 0.0022700771
train_loss: 0.0018646277
test_loss: 0.0022128394
train_loss: 0.0018707374
test_loss: 0.0023002487
train_loss: 0.0018048887
test_loss: 0.0022981097
train_loss: 0.0018592733
test_loss: 0.0023333277
train_loss: 0.0020201786
test_loss: 0.002280966
train_loss: 0.0018058008
test_loss: 0.002436902
train_loss: 0.0019370255
test_loss: 0.0023534866
train_loss: 0.0018220169
test_loss: 0.0022890456
train_loss: 0.0018519397
test_loss: 0.0023572263
train_loss: 0.0019869336
test_loss: 0.002436485
train_loss: 0.0018547757
test_loss: 0.0023646003
train_loss: 0.0019555876
test_loss: 0.0024030183
train_loss: 0.0020327508
test_loss: 0.0022899513
train_loss: 0.001963985
test_loss: 0.0023179417
train_loss: 0.0021324093
test_loss: 0.0022284011
train_loss: 0.0019415459
test_loss: 0.002420021
train_loss: 0.0022004962
test_loss: 0.0024819947
train_loss: 0.0020314017
test_loss: 0.002642965
train_loss: 0.0018921306
test_loss: 0.002212974
train_loss: 0.0019866046
test_loss: 0.0025517882
train_loss: 0.0019086833
test_loss: 0.0024431385
train_loss: 0.0019480048
test_loss: 0.0023538133
train_loss: 0.001899567
test_loss: 0.002307008
train_loss: 0.002065996
test_loss: 0.0024092179
train_loss: 0.0018023202
test_loss: 0.0021567335
train_loss: 0.002022815
test_loss: 0.0025511733
train_loss: 0.0019023698
test_loss: 0.0021600462
train_loss: 0.0018852002
test_loss: 0.0024189723
train_loss: 0.0021002125
test_loss: 0.0022688927/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

train_loss: 0.0019362138
test_loss: 0.0023331183
train_loss: 0.00216304
test_loss: 0.0023550137
train_loss: 0.0019466216
test_loss: 0.0023823718
train_loss: 0.002165089
test_loss: 0.0025040675
train_loss: 0.0019089052
test_loss: 0.0023540372
train_loss: 0.0021785046
test_loss: 0.0023132605
train_loss: 0.0019992315
test_loss: 0.0023909374
train_loss: 0.002097474
test_loss: 0.0024146484
train_loss: 0.0019038543
test_loss: 0.0022804453
train_loss: 0.0018154045
test_loss: 0.0021554509
train_loss: 0.0018592369
test_loss: 0.002231045
train_loss: 0.001745252
test_loss: 0.0021726263
train_loss: 0.0019493016
test_loss: 0.0022533366
train_loss: 0.002265906
test_loss: 0.002631587
train_loss: 0.0019687864
test_loss: 0.002354061
train_loss: 0.0020914515
test_loss: 0.002368391
train_loss: 0.0018459096
test_loss: 0.0023152593
train_loss: 0.0020832862
test_loss: 0.0022641618
train_loss: 0.002030102
test_loss: 0.0022923986
train_loss: 0.0018106849
test_loss: 0.0022982373
train_loss: 0.0018747491
test_loss: 0.00225054
train_loss: 0.0017530008
test_loss: 0.0022412152
train_loss: 0.0018979983
test_loss: 0.0022304943
train_loss: 0.0018840169
test_loss: 0.0022165796
train_loss: 0.0020174773
test_loss: 0.002320048
train_loss: 0.0017812192
test_loss: 0.0024093965
train_loss: 0.0021099448
test_loss: 0.0026245872
train_loss: 0.0019908885
test_loss: 0.0025106524
train_loss: 0.0018215193
test_loss: 0.0024642204
train_loss: 0.001824463
test_loss: 0.002307091
train_loss: 0.0016990141
test_loss: 0.0021571503
train_loss: 0.0019428483
test_loss: 0.0024077562
train_loss: 0.0019793364
test_loss: 0.0023909917
train_loss: 0.001901949
test_loss: 0.002269952
train_loss: 0.0020452102
test_loss: 0.0024459122
train_loss: 0.0021849337
test_loss: 0.0023576522
train_loss: 0.0017774258
test_loss: 0.002202581
train_loss: 0.0018030049
test_loss: 0.0022285273
train_loss: 0.00181767
test_loss: 0.0024554078
train_loss: 0.0018974761
test_loss: 0.0024276092
train_loss: 0.0020222932
test_loss: 0.0024271489
train_loss: 0.001946468
test_loss: 0.0024662078
train_loss: 0.0018020212
test_loss: 0.002347228
train_loss: 0.0019763734
test_loss: 0.0023289802
train_loss: 0.0019396342
test_loss: 0.0023185534
train_loss: 0.001892381
test_loss: 0.002181638
train_loss: 0.0019535695
test_loss: 0.0024635335
train_loss: 0.0021097464
test_loss: 0.0022831243
train_loss: 0.0019180237
test_loss: 0.0022898444
train_loss: 0.0019768083
test_loss: 0.0023055773
train_loss: 0.0020029158
test_loss: 0.0022281744
train_loss: 0.0021196366
test_loss: 0.0022254472
train_loss: 0.0019117219
test_loss: 0.002281778
train_loss: 0.0018111748
test_loss: 0.002228983
train_loss: 0.0019439957
test_loss: 0.0023371885
train_loss: 0.001981496
test_loss: 0.0022366657
train_loss: 0.001988626
test_loss: 0.0022503182
train_loss: 0.0021029753
test_loss: 0.0025467875
train_loss: 0.002032646
test_loss: 0.0026883287
train_loss: 0.0018575436
test_loss: 0.0023283695
train_loss: 0.0018375721
test_loss: 0.002222739
train_loss: 0.001981725
test_loss: 0.0023382623
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb76191a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7618d4048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7618d4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb761812d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb76182a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb76178a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb76176b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb76170f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb76170fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7616d8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7616d89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7616ae730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7616ae8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb76164bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb76164b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb761631b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb761624488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb761624268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb76157db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb76157d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb761547730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb761547b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb76150b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7614dc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7614dcb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7247f16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7247af1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7247c9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb724760598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb724760ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb724722620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7246d52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7246d5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb724711b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6fc4e99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb6fc495378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.43094882e-06
Iter: 2 loss: 5.05115759e-06
Iter: 3 loss: 2.70049786e-05
Iter: 4 loss: 5.0511112e-06
Iter: 5 loss: 4.40113126e-06
Iter: 6 loss: 6.22890821e-06
Iter: 7 loss: 4.19340495e-06
Iter: 8 loss: 3.94982544e-06
Iter: 9 loss: 4.12714e-06
Iter: 10 loss: 3.79958192e-06
Iter: 11 loss: 3.49804827e-06
Iter: 12 loss: 6.03890794e-06
Iter: 13 loss: 3.48069148e-06
Iter: 14 loss: 3.32710738e-06
Iter: 15 loss: 3.44355726e-06
Iter: 16 loss: 3.23340532e-06
Iter: 17 loss: 3.16503815e-06
Iter: 18 loss: 3.16076898e-06
Iter: 19 loss: 3.10034238e-06
Iter: 20 loss: 3.02141984e-06
Iter: 21 loss: 3.01639875e-06
Iter: 22 loss: 2.89537229e-06
Iter: 23 loss: 2.82657402e-06
Iter: 24 loss: 2.77435129e-06
Iter: 25 loss: 2.69794418e-06
Iter: 26 loss: 2.67744804e-06
Iter: 27 loss: 2.59586045e-06
Iter: 28 loss: 2.48757533e-06
Iter: 29 loss: 2.48130027e-06
Iter: 30 loss: 2.36634128e-06
Iter: 31 loss: 2.49731602e-06
Iter: 32 loss: 2.30462729e-06
Iter: 33 loss: 2.2131544e-06
Iter: 34 loss: 2.80319318e-06
Iter: 35 loss: 2.20314541e-06
Iter: 36 loss: 2.12984241e-06
Iter: 37 loss: 2.12346117e-06
Iter: 38 loss: 2.06912546e-06
Iter: 39 loss: 2.09238056e-06
Iter: 40 loss: 2.03694981e-06
Iter: 41 loss: 2.00497038e-06
Iter: 42 loss: 1.95085931e-06
Iter: 43 loss: 1.95070083e-06
Iter: 44 loss: 1.89381831e-06
Iter: 45 loss: 2.05685774e-06
Iter: 46 loss: 1.87596152e-06
Iter: 47 loss: 1.82015106e-06
Iter: 48 loss: 2.42265742e-06
Iter: 49 loss: 1.81895075e-06
Iter: 50 loss: 1.78445168e-06
Iter: 51 loss: 1.73347757e-06
Iter: 52 loss: 1.73230194e-06
Iter: 53 loss: 1.69404495e-06
Iter: 54 loss: 1.69057296e-06
Iter: 55 loss: 1.66258212e-06
Iter: 56 loss: 1.6290935e-06
Iter: 57 loss: 1.62560866e-06
Iter: 58 loss: 1.59438707e-06
Iter: 59 loss: 1.68527367e-06
Iter: 60 loss: 1.58467537e-06
Iter: 61 loss: 1.54718896e-06
Iter: 62 loss: 1.82970734e-06
Iter: 63 loss: 1.54423242e-06
Iter: 64 loss: 1.52083567e-06
Iter: 65 loss: 1.50006599e-06
Iter: 66 loss: 1.49407265e-06
Iter: 67 loss: 1.45088e-06
Iter: 68 loss: 1.44544492e-06
Iter: 69 loss: 1.41467285e-06
Iter: 70 loss: 1.36141239e-06
Iter: 71 loss: 1.59755064e-06
Iter: 72 loss: 1.35085975e-06
Iter: 73 loss: 1.30602018e-06
Iter: 74 loss: 1.57985e-06
Iter: 75 loss: 1.30059823e-06
Iter: 76 loss: 1.27250405e-06
Iter: 77 loss: 1.27180658e-06
Iter: 78 loss: 1.25975669e-06
Iter: 79 loss: 1.23579309e-06
Iter: 80 loss: 1.70321118e-06
Iter: 81 loss: 1.23555651e-06
Iter: 82 loss: 1.22371637e-06
Iter: 83 loss: 1.22225981e-06
Iter: 84 loss: 1.20896107e-06
Iter: 85 loss: 1.18716321e-06
Iter: 86 loss: 1.18705066e-06
Iter: 87 loss: 1.17491788e-06
Iter: 88 loss: 1.17456773e-06
Iter: 89 loss: 1.16184424e-06
Iter: 90 loss: 1.14963416e-06
Iter: 91 loss: 1.14681245e-06
Iter: 92 loss: 1.12745442e-06
Iter: 93 loss: 1.11891882e-06
Iter: 94 loss: 1.10907479e-06
Iter: 95 loss: 1.0987012e-06
Iter: 96 loss: 1.09534494e-06
Iter: 97 loss: 1.0836477e-06
Iter: 98 loss: 1.08415293e-06
Iter: 99 loss: 1.07438802e-06
Iter: 100 loss: 1.06115726e-06
Iter: 101 loss: 1.03774528e-06
Iter: 102 loss: 1.03771947e-06
Iter: 103 loss: 1.02662852e-06
Iter: 104 loss: 1.02540719e-06
Iter: 105 loss: 1.01518901e-06
Iter: 106 loss: 1.01636704e-06
Iter: 107 loss: 1.00741659e-06
Iter: 108 loss: 9.94600896e-07
Iter: 109 loss: 9.94514721e-07
Iter: 110 loss: 9.88719876e-07
Iter: 111 loss: 9.77793547e-07
Iter: 112 loss: 1.22098049e-06
Iter: 113 loss: 9.77734e-07
Iter: 114 loss: 9.66216817e-07
Iter: 115 loss: 1.07830647e-06
Iter: 116 loss: 9.65794356e-07
Iter: 117 loss: 9.5277619e-07
Iter: 118 loss: 9.48118213e-07
Iter: 119 loss: 9.40880341e-07
Iter: 120 loss: 9.30768635e-07
Iter: 121 loss: 1.01791647e-06
Iter: 122 loss: 9.3016223e-07
Iter: 123 loss: 9.20682169e-07
Iter: 124 loss: 9.52866571e-07
Iter: 125 loss: 9.1803588e-07
Iter: 126 loss: 9.11436587e-07
Iter: 127 loss: 9.01476767e-07
Iter: 128 loss: 9.01263263e-07
Iter: 129 loss: 8.95975404e-07
Iter: 130 loss: 8.95209155e-07
Iter: 131 loss: 8.89466378e-07
Iter: 132 loss: 8.9084034e-07
Iter: 133 loss: 8.85260249e-07
Iter: 134 loss: 8.78966205e-07
Iter: 135 loss: 8.71479301e-07
Iter: 136 loss: 8.70676558e-07
Iter: 137 loss: 8.57774523e-07
Iter: 138 loss: 8.68147708e-07
Iter: 139 loss: 8.49986691e-07
Iter: 140 loss: 8.38881078e-07
Iter: 141 loss: 1.01504611e-06
Iter: 142 loss: 8.3889546e-07
Iter: 143 loss: 8.27740166e-07
Iter: 144 loss: 8.84201313e-07
Iter: 145 loss: 8.25924815e-07
Iter: 146 loss: 8.20208811e-07
Iter: 147 loss: 8.19376226e-07
Iter: 148 loss: 8.15415603e-07
Iter: 149 loss: 8.08159086e-07
Iter: 150 loss: 8.46641e-07
Iter: 151 loss: 8.07034041e-07
Iter: 152 loss: 7.99367911e-07
Iter: 153 loss: 8.27356189e-07
Iter: 154 loss: 7.97441203e-07
Iter: 155 loss: 7.93903496e-07
Iter: 156 loss: 7.97951e-07
Iter: 157 loss: 7.91994e-07
Iter: 158 loss: 7.86294436e-07
Iter: 159 loss: 8.02442628e-07
Iter: 160 loss: 7.84477038e-07
Iter: 161 loss: 7.79007962e-07
Iter: 162 loss: 7.74647447e-07
Iter: 163 loss: 7.72993758e-07
Iter: 164 loss: 7.67586812e-07
Iter: 165 loss: 8.13495262e-07
Iter: 166 loss: 7.67252573e-07
Iter: 167 loss: 7.61588808e-07
Iter: 168 loss: 7.82953748e-07
Iter: 169 loss: 7.60245371e-07
Iter: 170 loss: 7.55674193e-07
Iter: 171 loss: 7.47826221e-07
Iter: 172 loss: 7.47813317e-07
Iter: 173 loss: 7.40880296e-07
Iter: 174 loss: 7.52868914e-07
Iter: 175 loss: 7.37781079e-07
Iter: 176 loss: 7.313796e-07
Iter: 177 loss: 8.30533054e-07
Iter: 178 loss: 7.31387786e-07
Iter: 179 loss: 7.25128e-07
Iter: 180 loss: 7.54868211e-07
Iter: 181 loss: 7.23998255e-07
Iter: 182 loss: 7.2025432e-07
Iter: 183 loss: 7.15497379e-07
Iter: 184 loss: 7.15112208e-07
Iter: 185 loss: 7.12077167e-07
Iter: 186 loss: 7.11811822e-07
Iter: 187 loss: 7.08479206e-07
Iter: 188 loss: 7.04242211e-07
Iter: 189 loss: 7.0393736e-07
Iter: 190 loss: 6.99818656e-07
Iter: 191 loss: 7.36409504e-07
Iter: 192 loss: 6.99623172e-07
Iter: 193 loss: 6.95570407e-07
Iter: 194 loss: 7.02578063e-07
Iter: 195 loss: 6.9371265e-07
Iter: 196 loss: 6.89489866e-07
Iter: 197 loss: 6.87835097e-07
Iter: 198 loss: 6.85576424e-07
Iter: 199 loss: 6.80144694e-07
Iter: 200 loss: 6.93413085e-07
Iter: 201 loss: 6.78193942e-07
Iter: 202 loss: 6.71362557e-07
Iter: 203 loss: 7.26193548e-07
Iter: 204 loss: 6.70942313e-07
Iter: 205 loss: 6.67752659e-07
Iter: 206 loss: 6.648566e-07
Iter: 207 loss: 6.64139634e-07
Iter: 208 loss: 6.58883778e-07
Iter: 209 loss: 6.57952228e-07
Iter: 210 loss: 6.54427595e-07
Iter: 211 loss: 6.49943331e-07
Iter: 212 loss: 7.20049115e-07
Iter: 213 loss: 6.49941285e-07
Iter: 214 loss: 6.46871058e-07
Iter: 215 loss: 6.46880437e-07
Iter: 216 loss: 6.44898364e-07
Iter: 217 loss: 6.40022847e-07
Iter: 218 loss: 6.88469186e-07
Iter: 219 loss: 6.39399332e-07
Iter: 220 loss: 6.36702453e-07
Iter: 221 loss: 6.36617301e-07
Iter: 222 loss: 6.33724767e-07
Iter: 223 loss: 6.38430492e-07
Iter: 224 loss: 6.32401225e-07
Iter: 225 loss: 6.29725946e-07
Iter: 226 loss: 6.300927e-07
Iter: 227 loss: 6.27659517e-07
Iter: 228 loss: 6.24870495e-07
Iter: 229 loss: 6.24849e-07
Iter: 230 loss: 6.23242101e-07
Iter: 231 loss: 6.18692923e-07
Iter: 232 loss: 6.45130513e-07
Iter: 233 loss: 6.17432079e-07
Iter: 234 loss: 6.12633244e-07
Iter: 235 loss: 6.75172942e-07
Iter: 236 loss: 6.12579129e-07
Iter: 237 loss: 6.08539096e-07
Iter: 238 loss: 6.3785e-07
Iter: 239 loss: 6.08191556e-07
Iter: 240 loss: 6.0566731e-07
Iter: 241 loss: 6.0511843e-07
Iter: 242 loss: 6.03516469e-07
Iter: 243 loss: 5.99916632e-07
Iter: 244 loss: 5.96591349e-07
Iter: 245 loss: 5.95743643e-07
Iter: 246 loss: 5.92382207e-07
Iter: 247 loss: 6.39716575e-07
Iter: 248 loss: 5.92370156e-07
Iter: 249 loss: 5.90127911e-07
Iter: 250 loss: 6.18638921e-07
Iter: 251 loss: 5.90117736e-07
Iter: 252 loss: 5.87647492e-07
Iter: 253 loss: 5.84387919e-07
Iter: 254 loss: 5.84192207e-07
Iter: 255 loss: 5.81291488e-07
Iter: 256 loss: 5.86430076e-07
Iter: 257 loss: 5.80015353e-07
Iter: 258 loss: 5.78153276e-07
Iter: 259 loss: 5.77972969e-07
Iter: 260 loss: 5.76284833e-07
Iter: 261 loss: 5.72115255e-07
Iter: 262 loss: 6.13301381e-07
Iter: 263 loss: 5.71587861e-07
Iter: 264 loss: 5.71652322e-07
Iter: 265 loss: 5.69924907e-07
Iter: 266 loss: 5.68682822e-07
Iter: 267 loss: 5.65591336e-07
Iter: 268 loss: 5.97519e-07
Iter: 269 loss: 5.65224695e-07
Iter: 270 loss: 5.63031563e-07
Iter: 271 loss: 5.9523893e-07
Iter: 272 loss: 5.63041965e-07
Iter: 273 loss: 5.61352635e-07
Iter: 274 loss: 5.7158843e-07
Iter: 275 loss: 5.61164711e-07
Iter: 276 loss: 5.59909e-07
Iter: 277 loss: 5.57122803e-07
Iter: 278 loss: 6.00948738e-07
Iter: 279 loss: 5.57045155e-07
Iter: 280 loss: 5.53376e-07
Iter: 281 loss: 5.62785544e-07
Iter: 282 loss: 5.52095798e-07
Iter: 283 loss: 5.48833e-07
Iter: 284 loss: 5.64119091e-07
Iter: 285 loss: 5.48218111e-07
Iter: 286 loss: 5.45594389e-07
Iter: 287 loss: 5.7014455e-07
Iter: 288 loss: 5.4547661e-07
Iter: 289 loss: 5.42474368e-07
Iter: 290 loss: 5.44462182e-07
Iter: 291 loss: 5.40617236e-07
Iter: 292 loss: 5.38399718e-07
Iter: 293 loss: 5.39510552e-07
Iter: 294 loss: 5.36922812e-07
Iter: 295 loss: 5.35774177e-07
Iter: 296 loss: 5.3565492e-07
Iter: 297 loss: 5.34395326e-07
Iter: 298 loss: 5.33121465e-07
Iter: 299 loss: 5.32885e-07
Iter: 300 loss: 5.31165085e-07
Iter: 301 loss: 5.37151152e-07
Iter: 302 loss: 5.30776276e-07
Iter: 303 loss: 5.28606506e-07
Iter: 304 loss: 5.35487629e-07
Iter: 305 loss: 5.28001465e-07
Iter: 306 loss: 5.26431336e-07
Iter: 307 loss: 5.23450183e-07
Iter: 308 loss: 5.91742378e-07
Iter: 309 loss: 5.23455242e-07
Iter: 310 loss: 5.21914274e-07
Iter: 311 loss: 5.2153473e-07
Iter: 312 loss: 5.19631215e-07
Iter: 313 loss: 5.16659952e-07
Iter: 314 loss: 5.16625676e-07
Iter: 315 loss: 5.14289468e-07
Iter: 316 loss: 5.24334496e-07
Iter: 317 loss: 5.13809539e-07
Iter: 318 loss: 5.11813539e-07
Iter: 319 loss: 5.09869e-07
Iter: 320 loss: 5.09432255e-07
Iter: 321 loss: 5.10712539e-07
Iter: 322 loss: 5.08194546e-07
Iter: 323 loss: 5.07254072e-07
Iter: 324 loss: 5.06123683e-07
Iter: 325 loss: 5.06017e-07
Iter: 326 loss: 5.04366426e-07
Iter: 327 loss: 5.03610522e-07
Iter: 328 loss: 5.02757e-07
Iter: 329 loss: 5.00034389e-07
Iter: 330 loss: 5.28608552e-07
Iter: 331 loss: 4.99946339e-07
Iter: 332 loss: 4.97650831e-07
Iter: 333 loss: 4.96593884e-07
Iter: 334 loss: 4.95442691e-07
Iter: 335 loss: 4.93363643e-07
Iter: 336 loss: 5.05375453e-07
Iter: 337 loss: 4.9310006e-07
Iter: 338 loss: 4.9067387e-07
Iter: 339 loss: 4.96863e-07
Iter: 340 loss: 4.89842932e-07
Iter: 341 loss: 4.88530134e-07
Iter: 342 loss: 4.88808951e-07
Iter: 343 loss: 4.87531224e-07
Iter: 344 loss: 4.86192221e-07
Iter: 345 loss: 5.0066518e-07
Iter: 346 loss: 4.86163458e-07
Iter: 347 loss: 4.8485856e-07
Iter: 348 loss: 4.87611544e-07
Iter: 349 loss: 4.84368627e-07
Iter: 350 loss: 4.83468511e-07
Iter: 351 loss: 4.82256837e-07
Iter: 352 loss: 4.82200562e-07
Iter: 353 loss: 4.80394306e-07
Iter: 354 loss: 4.78137054e-07
Iter: 355 loss: 4.77948447e-07
Iter: 356 loss: 4.78307811e-07
Iter: 357 loss: 4.76312692e-07
Iter: 358 loss: 4.75165677e-07
Iter: 359 loss: 4.73768978e-07
Iter: 360 loss: 4.73663135e-07
Iter: 361 loss: 4.72031502e-07
Iter: 362 loss: 4.7438283e-07
Iter: 363 loss: 4.71229214e-07
Iter: 364 loss: 4.69543778e-07
Iter: 365 loss: 4.96221446e-07
Iter: 366 loss: 4.69523286e-07
Iter: 367 loss: 4.68669384e-07
Iter: 368 loss: 4.67200351e-07
Iter: 369 loss: 4.67203023e-07
Iter: 370 loss: 4.66078689e-07
Iter: 371 loss: 4.66049102e-07
Iter: 372 loss: 4.65009464e-07
Iter: 373 loss: 4.6536627e-07
Iter: 374 loss: 4.64318674e-07
Iter: 375 loss: 4.63223927e-07
Iter: 376 loss: 4.63455251e-07
Iter: 377 loss: 4.62423912e-07
Iter: 378 loss: 4.60858274e-07
Iter: 379 loss: 4.6336632e-07
Iter: 380 loss: 4.60139745e-07
Iter: 381 loss: 4.5845286e-07
Iter: 382 loss: 4.82812e-07
Iter: 383 loss: 4.58434897e-07
Iter: 384 loss: 4.57647417e-07
Iter: 385 loss: 4.55926511e-07
Iter: 386 loss: 4.80826543e-07
Iter: 387 loss: 4.55826068e-07
Iter: 388 loss: 4.53873383e-07
Iter: 389 loss: 4.63331446e-07
Iter: 390 loss: 4.53583027e-07
Iter: 391 loss: 4.52252664e-07
Iter: 392 loss: 4.56299801e-07
Iter: 393 loss: 4.51879401e-07
Iter: 394 loss: 4.51270267e-07
Iter: 395 loss: 4.51204841e-07
Iter: 396 loss: 4.50371203e-07
Iter: 397 loss: 4.48456575e-07
Iter: 398 loss: 4.70249716e-07
Iter: 399 loss: 4.48264416e-07
Iter: 400 loss: 4.46705172e-07
Iter: 401 loss: 4.51963189e-07
Iter: 402 loss: 4.46259037e-07
Iter: 403 loss: 4.44600488e-07
Iter: 404 loss: 4.4562637e-07
Iter: 405 loss: 4.43503865e-07
Iter: 406 loss: 4.43019644e-07
Iter: 407 loss: 4.42508394e-07
Iter: 408 loss: 4.41769e-07
Iter: 409 loss: 4.40831059e-07
Iter: 410 loss: 4.40752927e-07
Iter: 411 loss: 4.39893597e-07
Iter: 412 loss: 4.49361039e-07
Iter: 413 loss: 4.39880807e-07
Iter: 414 loss: 4.39035205e-07
Iter: 415 loss: 4.40495057e-07
Iter: 416 loss: 4.38647021e-07
Iter: 417 loss: 4.37780329e-07
Iter: 418 loss: 4.37126062e-07
Iter: 419 loss: 4.36871119e-07
Iter: 420 loss: 4.35539022e-07
Iter: 421 loss: 4.49949738e-07
Iter: 422 loss: 4.35493632e-07
Iter: 423 loss: 4.34447884e-07
Iter: 424 loss: 4.34637457e-07
Iter: 425 loss: 4.3363724e-07
Iter: 426 loss: 4.32688182e-07
Iter: 427 loss: 4.31289436e-07
Iter: 428 loss: 4.31223555e-07
Iter: 429 loss: 4.30935813e-07
Iter: 430 loss: 4.3035584e-07
Iter: 431 loss: 4.29623043e-07
Iter: 432 loss: 4.30228908e-07
Iter: 433 loss: 4.29210644e-07
Iter: 434 loss: 4.28575362e-07
Iter: 435 loss: 4.27000884e-07
Iter: 436 loss: 4.41855548e-07
Iter: 437 loss: 4.26758646e-07
Iter: 438 loss: 4.24930278e-07
Iter: 439 loss: 4.41108227e-07
Iter: 440 loss: 4.24857603e-07
Iter: 441 loss: 4.2400211e-07
Iter: 442 loss: 4.23945352e-07
Iter: 443 loss: 4.23148492e-07
Iter: 444 loss: 4.22013159e-07
Iter: 445 loss: 4.21981099e-07
Iter: 446 loss: 4.20742168e-07
Iter: 447 loss: 4.26517545e-07
Iter: 448 loss: 4.20518631e-07
Iter: 449 loss: 4.19413027e-07
Iter: 450 loss: 4.31398632e-07
Iter: 451 loss: 4.19406888e-07
Iter: 452 loss: 4.18915e-07
Iter: 453 loss: 4.17844916e-07
Iter: 454 loss: 4.34884839e-07
Iter: 455 loss: 4.17828687e-07
Iter: 456 loss: 4.17569566e-07
Iter: 457 loss: 4.17253602e-07
Iter: 458 loss: 4.16836656e-07
Iter: 459 loss: 4.16218114e-07
Iter: 460 loss: 4.16201402e-07
Iter: 461 loss: 4.15432169e-07
Iter: 462 loss: 4.15223212e-07
Iter: 463 loss: 4.14695421e-07
Iter: 464 loss: 4.13419201e-07
Iter: 465 loss: 4.17349668e-07
Iter: 466 loss: 4.13042187e-07
Iter: 467 loss: 4.11920297e-07
Iter: 468 loss: 4.11890767e-07
Iter: 469 loss: 4.11014184e-07
Iter: 470 loss: 4.09821183e-07
Iter: 471 loss: 4.09775794e-07
Iter: 472 loss: 4.08657627e-07
Iter: 473 loss: 4.10198822e-07
Iter: 474 loss: 4.08115909e-07
Iter: 475 loss: 4.07517803e-07
Iter: 476 loss: 4.06699456e-07
Iter: 477 loss: 4.0663295e-07
Iter: 478 loss: 4.05803803e-07
Iter: 479 loss: 4.05770635e-07
Iter: 480 loss: 4.05262824e-07
Iter: 481 loss: 4.04432171e-07
Iter: 482 loss: 4.04432285e-07
Iter: 483 loss: 4.0401531e-07
Iter: 484 loss: 4.03939907e-07
Iter: 485 loss: 4.03447189e-07
Iter: 486 loss: 4.02474427e-07
Iter: 487 loss: 4.22454804e-07
Iter: 488 loss: 4.02463058e-07
Iter: 489 loss: 4.0132943e-07
Iter: 490 loss: 4.03873685e-07
Iter: 491 loss: 4.00890684e-07
Iter: 492 loss: 3.99876797e-07
Iter: 493 loss: 3.99880747e-07
Iter: 494 loss: 3.99304838e-07
Iter: 495 loss: 3.98193436e-07
Iter: 496 loss: 4.20469689e-07
Iter: 497 loss: 3.98201792e-07
Iter: 498 loss: 3.97043777e-07
Iter: 499 loss: 3.98963607e-07
Iter: 500 loss: 3.96557255e-07
Iter: 501 loss: 3.95362974e-07
Iter: 502 loss: 3.99056518e-07
Iter: 503 loss: 3.94992924e-07
Iter: 504 loss: 3.94052108e-07
Iter: 505 loss: 4.00803231e-07
Iter: 506 loss: 3.93968e-07
Iter: 507 loss: 3.93398864e-07
Iter: 508 loss: 4.01570219e-07
Iter: 509 loss: 3.93397357e-07
Iter: 510 loss: 3.92707534e-07
Iter: 511 loss: 3.91784681e-07
Iter: 512 loss: 3.91749438e-07
Iter: 513 loss: 3.90715741e-07
Iter: 514 loss: 3.92833812e-07
Iter: 515 loss: 3.90302546e-07
Iter: 516 loss: 3.89385377e-07
Iter: 517 loss: 3.94991105e-07
Iter: 518 loss: 3.89302016e-07
Iter: 519 loss: 3.88474803e-07
Iter: 520 loss: 3.95163852e-07
Iter: 521 loss: 3.8843919e-07
Iter: 522 loss: 3.88089859e-07
Iter: 523 loss: 3.87552319e-07
Iter: 524 loss: 3.87552063e-07
Iter: 525 loss: 3.86718057e-07
Iter: 526 loss: 3.94229431e-07
Iter: 527 loss: 3.86690374e-07
Iter: 528 loss: 3.86214197e-07
Iter: 529 loss: 3.85426461e-07
Iter: 530 loss: 3.85420634e-07
Iter: 531 loss: 3.84630795e-07
Iter: 532 loss: 3.89565855e-07
Iter: 533 loss: 3.84525862e-07
Iter: 534 loss: 3.83787381e-07
Iter: 535 loss: 3.89687671e-07
Iter: 536 loss: 3.83719168e-07
Iter: 537 loss: 3.830678e-07
Iter: 538 loss: 3.81833615e-07
Iter: 539 loss: 4.04225631e-07
Iter: 540 loss: 3.81799396e-07
Iter: 541 loss: 3.80623931e-07
Iter: 542 loss: 3.83343547e-07
Iter: 543 loss: 3.80182229e-07
Iter: 544 loss: 3.79354475e-07
Iter: 545 loss: 3.9113786e-07
Iter: 546 loss: 3.79372864e-07
Iter: 547 loss: 3.78773024e-07
Iter: 548 loss: 3.86659849e-07
Iter: 549 loss: 3.78776917e-07
Iter: 550 loss: 3.78393e-07
Iter: 551 loss: 3.77669835e-07
Iter: 552 loss: 3.90407536e-07
Iter: 553 loss: 3.7766344e-07
Iter: 554 loss: 3.76795128e-07
Iter: 555 loss: 3.78263337e-07
Iter: 556 loss: 3.76412061e-07
Iter: 557 loss: 3.76021376e-07
Iter: 558 loss: 3.75868836e-07
Iter: 559 loss: 3.75542157e-07
Iter: 560 loss: 3.74766842e-07
Iter: 561 loss: 3.83901067e-07
Iter: 562 loss: 3.74734782e-07
Iter: 563 loss: 3.74143724e-07
Iter: 564 loss: 3.74084436e-07
Iter: 565 loss: 3.73672151e-07
Iter: 566 loss: 3.73389241e-07
Iter: 567 loss: 3.73251169e-07
Iter: 568 loss: 3.72653744e-07
Iter: 569 loss: 3.72513483e-07
Iter: 570 loss: 3.72093609e-07
Iter: 571 loss: 3.71521e-07
Iter: 572 loss: 3.80837832e-07
Iter: 573 loss: 3.71525459e-07
Iter: 574 loss: 3.71017279e-07
Iter: 575 loss: 3.72865401e-07
Iter: 576 loss: 3.70865848e-07
Iter: 577 loss: 3.70532348e-07
Iter: 578 loss: 3.6973006e-07
Iter: 579 loss: 3.80284803e-07
Iter: 580 loss: 3.69670857e-07
Iter: 581 loss: 3.68599103e-07
Iter: 582 loss: 3.72884074e-07
Iter: 583 loss: 3.68356382e-07
Iter: 584 loss: 3.68128127e-07
Iter: 585 loss: 3.67882478e-07
Iter: 586 loss: 3.67515099e-07
Iter: 587 loss: 3.66518634e-07
Iter: 588 loss: 3.7454484e-07
Iter: 589 loss: 3.66317238e-07
Iter: 590 loss: 3.6529525e-07
Iter: 591 loss: 3.69120528e-07
Iter: 592 loss: 3.65073731e-07
Iter: 593 loss: 3.64724372e-07
Iter: 594 loss: 3.64588686e-07
Iter: 595 loss: 3.64206585e-07
Iter: 596 loss: 3.6351355e-07
Iter: 597 loss: 3.80722184e-07
Iter: 598 loss: 3.63505535e-07
Iter: 599 loss: 3.6292181e-07
Iter: 600 loss: 3.72202e-07
Iter: 601 loss: 3.62922606e-07
Iter: 602 loss: 3.62474026e-07
Iter: 603 loss: 3.63761501e-07
Iter: 604 loss: 3.62336721e-07
Iter: 605 loss: 3.62089281e-07
Iter: 606 loss: 3.61480403e-07
Iter: 607 loss: 3.68220896e-07
Iter: 608 loss: 3.61436093e-07
Iter: 609 loss: 3.60914498e-07
Iter: 610 loss: 3.6090168e-07
Iter: 611 loss: 3.60478566e-07
Iter: 612 loss: 3.61805064e-07
Iter: 613 loss: 3.60301954e-07
Iter: 614 loss: 3.59900241e-07
Iter: 615 loss: 3.59154228e-07
Iter: 616 loss: 3.59155649e-07
Iter: 617 loss: 3.58068803e-07
Iter: 618 loss: 3.59202261e-07
Iter: 619 loss: 3.57456088e-07
Iter: 620 loss: 3.57112071e-07
Iter: 621 loss: 3.56911244e-07
Iter: 622 loss: 3.56342099e-07
Iter: 623 loss: 3.56044183e-07
Iter: 624 loss: 3.5580095e-07
Iter: 625 loss: 3.55096631e-07
Iter: 626 loss: 3.55113087e-07
Iter: 627 loss: 3.54560314e-07
Iter: 628 loss: 3.54318558e-07
Iter: 629 loss: 3.54207373e-07
Iter: 630 loss: 3.53893853e-07
Iter: 631 loss: 3.53750892e-07
Iter: 632 loss: 3.53589e-07
Iter: 633 loss: 3.53220258e-07
Iter: 634 loss: 3.54188842e-07
Iter: 635 loss: 3.53102052e-07
Iter: 636 loss: 3.52504514e-07
Iter: 637 loss: 3.53052144e-07
Iter: 638 loss: 3.52204211e-07
Iter: 639 loss: 3.51787463e-07
Iter: 640 loss: 3.51186628e-07
Iter: 641 loss: 3.51158434e-07
Iter: 642 loss: 3.50315702e-07
Iter: 643 loss: 3.55847078e-07
Iter: 644 loss: 3.50250843e-07
Iter: 645 loss: 3.49801496e-07
Iter: 646 loss: 3.49778304e-07
Iter: 647 loss: 3.49488801e-07
Iter: 648 loss: 3.48873243e-07
Iter: 649 loss: 3.58105098e-07
Iter: 650 loss: 3.48822368e-07
Iter: 651 loss: 3.48219203e-07
Iter: 652 loss: 3.5188657e-07
Iter: 653 loss: 3.4816378e-07
Iter: 654 loss: 3.47694083e-07
Iter: 655 loss: 3.54764666e-07
Iter: 656 loss: 3.47693344e-07
Iter: 657 loss: 3.47279126e-07
Iter: 658 loss: 3.4699957e-07
Iter: 659 loss: 3.46856069e-07
Iter: 660 loss: 3.46373781e-07
Iter: 661 loss: 3.45947541e-07
Iter: 662 loss: 3.45811173e-07
Iter: 663 loss: 3.45182769e-07
Iter: 664 loss: 3.54030874e-07
Iter: 665 loss: 3.45193143e-07
Iter: 666 loss: 3.44477172e-07
Iter: 667 loss: 3.45985939e-07
Iter: 668 loss: 3.44231154e-07
Iter: 669 loss: 3.43742e-07
Iter: 670 loss: 3.44636533e-07
Iter: 671 loss: 3.43551108e-07
Iter: 672 loss: 3.43120718e-07
Iter: 673 loss: 3.4797489e-07
Iter: 674 loss: 3.4311222e-07
Iter: 675 loss: 3.42807766e-07
Iter: 676 loss: 3.42254395e-07
Iter: 677 loss: 3.52835286e-07
Iter: 678 loss: 3.42250956e-07
Iter: 679 loss: 3.4169102e-07
Iter: 680 loss: 3.43527802e-07
Iter: 681 loss: 3.41561417e-07
Iter: 682 loss: 3.4127504e-07
Iter: 683 loss: 3.41233488e-07
Iter: 684 loss: 3.40972e-07
Iter: 685 loss: 3.40339511e-07
Iter: 686 loss: 3.45054957e-07
Iter: 687 loss: 3.40198369e-07
Iter: 688 loss: 3.39624563e-07
Iter: 689 loss: 3.45535e-07
Iter: 690 loss: 3.39623455e-07
Iter: 691 loss: 3.39233736e-07
Iter: 692 loss: 3.39259458e-07
Iter: 693 loss: 3.38899895e-07
Iter: 694 loss: 3.3803866e-07
Iter: 695 loss: 3.46072824e-07
Iter: 696 loss: 3.37940435e-07
Iter: 697 loss: 3.37291738e-07
Iter: 698 loss: 3.4249598e-07
Iter: 699 loss: 3.37262065e-07
Iter: 700 loss: 3.36847449e-07
Iter: 701 loss: 3.42318089e-07
Iter: 702 loss: 3.36836791e-07
Iter: 703 loss: 3.3645469e-07
Iter: 704 loss: 3.36461198e-07
Iter: 705 loss: 3.36120593e-07
Iter: 706 loss: 3.35683922e-07
Iter: 707 loss: 3.36871722e-07
Iter: 708 loss: 3.35539482e-07
Iter: 709 loss: 3.35088316e-07
Iter: 710 loss: 3.38490679e-07
Iter: 711 loss: 3.35027948e-07
Iter: 712 loss: 3.34753963e-07
Iter: 713 loss: 3.34260051e-07
Iter: 714 loss: 3.34264371e-07
Iter: 715 loss: 3.33726859e-07
Iter: 716 loss: 3.36756386e-07
Iter: 717 loss: 3.33650377e-07
Iter: 718 loss: 3.33361243e-07
Iter: 719 loss: 3.33353682e-07
Iter: 720 loss: 3.33124376e-07
Iter: 721 loss: 3.32668662e-07
Iter: 722 loss: 3.421288e-07
Iter: 723 loss: 3.32670311e-07
Iter: 724 loss: 3.3229361e-07
Iter: 725 loss: 3.33482149e-07
Iter: 726 loss: 3.32160909e-07
Iter: 727 loss: 3.31842273e-07
Iter: 728 loss: 3.3183386e-07
Iter: 729 loss: 3.31627746e-07
Iter: 730 loss: 3.30977684e-07
Iter: 731 loss: 3.3394997e-07
Iter: 732 loss: 3.30739539e-07
Iter: 733 loss: 3.30054178e-07
Iter: 734 loss: 3.33809851e-07
Iter: 735 loss: 3.29957629e-07
Iter: 736 loss: 3.29490803e-07
Iter: 737 loss: 3.29479064e-07
Iter: 738 loss: 3.29012948e-07
Iter: 739 loss: 3.29006468e-07
Iter: 740 loss: 3.28641022e-07
Iter: 741 loss: 3.28295698e-07
Iter: 742 loss: 3.29727015e-07
Iter: 743 loss: 3.28235558e-07
Iter: 744 loss: 3.27890973e-07
Iter: 745 loss: 3.29587778e-07
Iter: 746 loss: 3.27818611e-07
Iter: 747 loss: 3.27621223e-07
Iter: 748 loss: 3.27194044e-07
Iter: 749 loss: 3.34627771e-07
Iter: 750 loss: 3.27178782e-07
Iter: 751 loss: 3.26811517e-07
Iter: 752 loss: 3.32265699e-07
Iter: 753 loss: 3.26802933e-07
Iter: 754 loss: 3.264729e-07
Iter: 755 loss: 3.28314115e-07
Iter: 756 loss: 3.26436975e-07
Iter: 757 loss: 3.26115781e-07
Iter: 758 loss: 3.25394467e-07
Iter: 759 loss: 3.34158216e-07
Iter: 760 loss: 3.25311419e-07
Iter: 761 loss: 3.24721839e-07
Iter: 762 loss: 3.24722407e-07
Iter: 763 loss: 3.24297361e-07
Iter: 764 loss: 3.28890053e-07
Iter: 765 loss: 3.24304494e-07
Iter: 766 loss: 3.24062853e-07
Iter: 767 loss: 3.23656081e-07
Iter: 768 loss: 3.23670776e-07
Iter: 769 loss: 3.23238908e-07
Iter: 770 loss: 3.24115888e-07
Iter: 771 loss: 3.23067582e-07
Iter: 772 loss: 3.22715607e-07
Iter: 773 loss: 3.23404265e-07
Iter: 774 loss: 3.22553973e-07
Iter: 775 loss: 3.2218145e-07
Iter: 776 loss: 3.22196769e-07
Iter: 777 loss: 3.21988466e-07
Iter: 778 loss: 3.2170334e-07
Iter: 779 loss: 3.21713e-07
Iter: 780 loss: 3.21324904e-07
Iter: 781 loss: 3.22276435e-07
Iter: 782 loss: 3.21161934e-07
Iter: 783 loss: 3.20550328e-07
Iter: 784 loss: 3.21028324e-07
Iter: 785 loss: 3.20175133e-07
Iter: 786 loss: 3.19745936e-07
Iter: 787 loss: 3.19685341e-07
Iter: 788 loss: 3.19377307e-07
Iter: 789 loss: 3.19220277e-07
Iter: 790 loss: 3.19148796e-07
Iter: 791 loss: 3.18891e-07
Iter: 792 loss: 3.18472473e-07
Iter: 793 loss: 3.28725491e-07
Iter: 794 loss: 3.18473241e-07
Iter: 795 loss: 3.18116975e-07
Iter: 796 loss: 3.19992694e-07
Iter: 797 loss: 3.18044584e-07
Iter: 798 loss: 3.17859303e-07
Iter: 799 loss: 3.17854585e-07
Iter: 800 loss: 3.17669134e-07
Iter: 801 loss: 3.1718281e-07
Iter: 802 loss: 3.21465308e-07
Iter: 803 loss: 3.17124318e-07
Iter: 804 loss: 3.16571317e-07
Iter: 805 loss: 3.18615804e-07
Iter: 806 loss: 3.16457431e-07
Iter: 807 loss: 3.15935608e-07
Iter: 808 loss: 3.1705386e-07
Iter: 809 loss: 3.15750896e-07
Iter: 810 loss: 3.15341481e-07
Iter: 811 loss: 3.20895197e-07
Iter: 812 loss: 3.15347904e-07
Iter: 813 loss: 3.14951478e-07
Iter: 814 loss: 3.15360467e-07
Iter: 815 loss: 3.14737207e-07
Iter: 816 loss: 3.14424597e-07
Iter: 817 loss: 3.1478362e-07
Iter: 818 loss: 3.14291242e-07
Iter: 819 loss: 3.14037123e-07
Iter: 820 loss: 3.14031382e-07
Iter: 821 loss: 3.13834875e-07
Iter: 822 loss: 3.13488272e-07
Iter: 823 loss: 3.13483156e-07
Iter: 824 loss: 3.13129476e-07
Iter: 825 loss: 3.14763383e-07
Iter: 826 loss: 3.1305342e-07
Iter: 827 loss: 3.12616748e-07
Iter: 828 loss: 3.14304543e-07
Iter: 829 loss: 3.12504227e-07
Iter: 830 loss: 3.12254059e-07
Iter: 831 loss: 3.11790473e-07
Iter: 832 loss: 3.1178746e-07
Iter: 833 loss: 3.11575377e-07
Iter: 834 loss: 3.11525326e-07
Iter: 835 loss: 3.11221328e-07
Iter: 836 loss: 3.1109056e-07
Iter: 837 loss: 3.10958086e-07
Iter: 838 loss: 3.10664035e-07
Iter: 839 loss: 3.10719372e-07
Iter: 840 loss: 3.10454084e-07
Iter: 841 loss: 3.10103445e-07
Iter: 842 loss: 3.09928197e-07
Iter: 843 loss: 3.09759287e-07
Iter: 844 loss: 3.09270149e-07
Iter: 845 loss: 3.12889512e-07
Iter: 846 loss: 3.09233059e-07
Iter: 847 loss: 3.08845017e-07
Iter: 848 loss: 3.13564101e-07
Iter: 849 loss: 3.08843823e-07
Iter: 850 loss: 3.08509101e-07
Iter: 851 loss: 3.08278288e-07
Iter: 852 loss: 3.08150049e-07
Iter: 853 loss: 3.07806829e-07
Iter: 854 loss: 3.09665069e-07
Iter: 855 loss: 3.0777403e-07
Iter: 856 loss: 3.0743351e-07
Iter: 857 loss: 3.08508163e-07
Iter: 858 loss: 3.0732636e-07
Iter: 859 loss: 3.06978365e-07
Iter: 860 loss: 3.06878121e-07
Iter: 861 loss: 3.06684797e-07
Iter: 862 loss: 3.06446623e-07
Iter: 863 loss: 3.06446708e-07
Iter: 864 loss: 3.06225758e-07
Iter: 865 loss: 3.06366303e-07
Iter: 866 loss: 3.06109143e-07
Iter: 867 loss: 3.05890296e-07
Iter: 868 loss: 3.05504926e-07
Iter: 869 loss: 3.05506546e-07
Iter: 870 loss: 3.05488584e-07
Iter: 871 loss: 3.05303672e-07
Iter: 872 loss: 3.05149456e-07
Iter: 873 loss: 3.04817092e-07
Iter: 874 loss: 3.09167376e-07
Iter: 875 loss: 3.04791968e-07
Iter: 876 loss: 3.04392529e-07
Iter: 877 loss: 3.04286431e-07
Iter: 878 loss: 3.04034984e-07
Iter: 879 loss: 3.03459473e-07
Iter: 880 loss: 3.05980905e-07
Iter: 881 loss: 3.03376027e-07
Iter: 882 loss: 3.03175455e-07
Iter: 883 loss: 3.03157265e-07
Iter: 884 loss: 3.02880096e-07
Iter: 885 loss: 3.02557225e-07
Iter: 886 loss: 3.02546312e-07
Iter: 887 loss: 3.02235264e-07
Iter: 888 loss: 3.02928754e-07
Iter: 889 loss: 3.02133657e-07
Iter: 890 loss: 3.01778e-07
Iter: 891 loss: 3.04355098e-07
Iter: 892 loss: 3.01742546e-07
Iter: 893 loss: 3.01447187e-07
Iter: 894 loss: 3.01308745e-07
Iter: 895 loss: 3.01142876e-07
Iter: 896 loss: 3.00849706e-07
Iter: 897 loss: 3.04084551e-07
Iter: 898 loss: 3.00839105e-07
Iter: 899 loss: 3.00541984e-07
Iter: 900 loss: 3.00199645e-07
Iter: 901 loss: 3.00166676e-07
Iter: 902 loss: 2.99763059e-07
Iter: 903 loss: 3.01221746e-07
Iter: 904 loss: 2.99639225e-07
Iter: 905 loss: 2.9946338e-07
Iter: 906 loss: 2.99447322e-07
Iter: 907 loss: 2.99274262e-07
Iter: 908 loss: 2.99080284e-07
Iter: 909 loss: 2.99065e-07
Iter: 910 loss: 2.98798142e-07
Iter: 911 loss: 2.98602345e-07
Iter: 912 loss: 2.98496957e-07
Iter: 913 loss: 2.98075406e-07
Iter: 914 loss: 2.98718703e-07
Iter: 915 loss: 2.97858975e-07
Iter: 916 loss: 2.97637627e-07
Iter: 917 loss: 2.97552702e-07
Iter: 918 loss: 2.97274738e-07
Iter: 919 loss: 2.9695974e-07
Iter: 920 loss: 2.96919211e-07
Iter: 921 loss: 2.96584432e-07
Iter: 922 loss: 2.97436429e-07
Iter: 923 loss: 2.96486377e-07
Iter: 924 loss: 2.96199971e-07
Iter: 925 loss: 2.96208384e-07
Iter: 926 loss: 2.96012274e-07
Iter: 927 loss: 2.95760628e-07
Iter: 928 loss: 2.95734765e-07
Iter: 929 loss: 2.95631764e-07
Iter: 930 loss: 2.95610619e-07
Iter: 931 loss: 2.95495852e-07
Iter: 932 loss: 2.95237101e-07
Iter: 933 loss: 2.99733102e-07
Iter: 934 loss: 2.95227238e-07
Iter: 935 loss: 2.94908318e-07
Iter: 936 loss: 2.95340726e-07
Iter: 937 loss: 2.94778857e-07
Iter: 938 loss: 2.94442941e-07
Iter: 939 loss: 2.96911594e-07
Iter: 940 loss: 2.94433761e-07
Iter: 941 loss: 2.94055866e-07
Iter: 942 loss: 2.94999069e-07
Iter: 943 loss: 2.93933397e-07
Iter: 944 loss: 2.93654409e-07
Iter: 945 loss: 2.93462335e-07
Iter: 946 loss: 2.93374114e-07
Iter: 947 loss: 2.92981383e-07
Iter: 948 loss: 2.93969606e-07
Iter: 949 loss: 2.92863263e-07
Iter: 950 loss: 2.92685058e-07
Iter: 951 loss: 2.92655841e-07
Iter: 952 loss: 2.92479115e-07
Iter: 953 loss: 2.9222349e-07
Iter: 954 loss: 2.92198706e-07
Iter: 955 loss: 2.91916763e-07
Iter: 956 loss: 2.919829e-07
Iter: 957 loss: 2.91696438e-07
Iter: 958 loss: 2.91421543e-07
Iter: 959 loss: 2.91425636e-07
Iter: 960 loss: 2.91185671e-07
Iter: 961 loss: 2.91204856e-07
Iter: 962 loss: 2.91016022e-07
Iter: 963 loss: 2.90750336e-07
Iter: 964 loss: 2.90680106e-07
Iter: 965 loss: 2.90526941e-07
Iter: 966 loss: 2.90043744e-07
Iter: 967 loss: 2.93516678e-07
Iter: 968 loss: 2.89999548e-07
Iter: 969 loss: 2.89777034e-07
Iter: 970 loss: 2.89531215e-07
Iter: 971 loss: 2.89513338e-07
Iter: 972 loss: 2.89295201e-07
Iter: 973 loss: 2.91927364e-07
Iter: 974 loss: 2.89291e-07
Iter: 975 loss: 2.89050888e-07
Iter: 976 loss: 2.88994244e-07
Iter: 977 loss: 2.88827437e-07
Iter: 978 loss: 2.88591281e-07
Iter: 979 loss: 2.88602678e-07
Iter: 980 loss: 2.88398098e-07
Iter: 981 loss: 2.8806511e-07
Iter: 982 loss: 2.88292057e-07
Iter: 983 loss: 2.87844784e-07
Iter: 984 loss: 2.87581145e-07
Iter: 985 loss: 2.8755079e-07
Iter: 986 loss: 2.87322479e-07
Iter: 987 loss: 2.87125943e-07
Iter: 988 loss: 2.87067309e-07
Iter: 989 loss: 2.8676962e-07
Iter: 990 loss: 2.86758734e-07
Iter: 991 loss: 2.86525221e-07
Iter: 992 loss: 2.86209087e-07
Iter: 993 loss: 2.90793082e-07
Iter: 994 loss: 2.86192289e-07
Iter: 995 loss: 2.85971112e-07
Iter: 996 loss: 2.87073703e-07
Iter: 997 loss: 2.85908442e-07
Iter: 998 loss: 2.85769119e-07
Iter: 999 loss: 2.85487204e-07
Iter: 1000 loss: 2.90598251e-07
Iter: 1001 loss: 2.85487488e-07
Iter: 1002 loss: 2.85218562e-07
Iter: 1003 loss: 2.85214185e-07
Iter: 1004 loss: 2.85034389e-07
Iter: 1005 loss: 2.8471004e-07
Iter: 1006 loss: 2.92284824e-07
Iter: 1007 loss: 2.84719761e-07
Iter: 1008 loss: 2.84378899e-07
Iter: 1009 loss: 2.85360727e-07
Iter: 1010 loss: 2.84280617e-07
Iter: 1011 loss: 2.84022406e-07
Iter: 1012 loss: 2.83997508e-07
Iter: 1013 loss: 2.83834311e-07
Iter: 1014 loss: 2.83466704e-07
Iter: 1015 loss: 2.9152352e-07
Iter: 1016 loss: 2.83460508e-07
Iter: 1017 loss: 2.83184676e-07
Iter: 1018 loss: 2.8364579e-07
Iter: 1019 loss: 2.83072666e-07
Iter: 1020 loss: 2.82926464e-07
Iter: 1021 loss: 2.82899776e-07
Iter: 1022 loss: 2.82722112e-07
Iter: 1023 loss: 2.8257864e-07
Iter: 1024 loss: 2.82532682e-07
Iter: 1025 loss: 2.82291069e-07
Iter: 1026 loss: 2.82365193e-07
Iter: 1027 loss: 2.82122897e-07
Iter: 1028 loss: 2.81950975e-07
Iter: 1029 loss: 2.81933524e-07
Iter: 1030 loss: 2.81778341e-07
Iter: 1031 loss: 2.81485e-07
Iter: 1032 loss: 2.87566365e-07
Iter: 1033 loss: 2.8147565e-07
Iter: 1034 loss: 2.81114438e-07
Iter: 1035 loss: 2.82178291e-07
Iter: 1036 loss: 2.8100385e-07
Iter: 1037 loss: 2.80560357e-07
Iter: 1038 loss: 2.83868332e-07
Iter: 1039 loss: 2.80535801e-07
Iter: 1040 loss: 2.8033358e-07
Iter: 1041 loss: 2.80116353e-07
Iter: 1042 loss: 2.80083469e-07
Iter: 1043 loss: 2.79747098e-07
Iter: 1044 loss: 2.81857837e-07
Iter: 1045 loss: 2.79723167e-07
Iter: 1046 loss: 2.79402514e-07
Iter: 1047 loss: 2.81759952e-07
Iter: 1048 loss: 2.79390861e-07
Iter: 1049 loss: 2.79243181e-07
Iter: 1050 loss: 2.78983947e-07
Iter: 1051 loss: 2.78984714e-07
Iter: 1052 loss: 2.78715e-07
Iter: 1053 loss: 2.80286088e-07
Iter: 1054 loss: 2.786918e-07
Iter: 1055 loss: 2.78507457e-07
Iter: 1056 loss: 2.78510214e-07
Iter: 1057 loss: 2.78370578e-07
Iter: 1058 loss: 2.78175634e-07
Iter: 1059 loss: 2.78174e-07
Iter: 1060 loss: 2.77941524e-07
Iter: 1061 loss: 2.77870924e-07
Iter: 1062 loss: 2.77728645e-07
Iter: 1063 loss: 2.77448692e-07
Iter: 1064 loss: 2.77430331e-07
Iter: 1065 loss: 2.77219328e-07
Iter: 1066 loss: 2.76979705e-07
Iter: 1067 loss: 2.76950971e-07
Iter: 1068 loss: 2.76663513e-07
Iter: 1069 loss: 2.78722553e-07
Iter: 1070 loss: 2.76636854e-07
Iter: 1071 loss: 2.76333139e-07
Iter: 1072 loss: 2.77311841e-07
Iter: 1073 loss: 2.76242304e-07
Iter: 1074 loss: 2.76050542e-07
Iter: 1075 loss: 2.75953084e-07
Iter: 1076 loss: 2.75870946e-07
Iter: 1077 loss: 2.75732617e-07
Iter: 1078 loss: 2.75709255e-07
Iter: 1079 loss: 2.75512178e-07
Iter: 1080 loss: 2.75300835e-07
Iter: 1081 loss: 2.75283128e-07
Iter: 1082 loss: 2.75070477e-07
Iter: 1083 loss: 2.74925583e-07
Iter: 1084 loss: 2.74882609e-07
Iter: 1085 loss: 2.74632868e-07
Iter: 1086 loss: 2.74627411e-07
Iter: 1087 loss: 2.74382e-07
Iter: 1088 loss: 2.74563121e-07
Iter: 1089 loss: 2.74260685e-07
Iter: 1090 loss: 2.74041497e-07
Iter: 1091 loss: 2.73973853e-07
Iter: 1092 loss: 2.73831574e-07
Iter: 1093 loss: 2.73564467e-07
Iter: 1094 loss: 2.76051594e-07
Iter: 1095 loss: 2.73559465e-07
Iter: 1096 loss: 2.73343858e-07
Iter: 1097 loss: 2.74341517e-07
Iter: 1098 loss: 2.7327107e-07
Iter: 1099 loss: 2.73103097e-07
Iter: 1100 loss: 2.73019396e-07
Iter: 1101 loss: 2.72961415e-07
Iter: 1102 loss: 2.72744671e-07
Iter: 1103 loss: 2.7508645e-07
Iter: 1104 loss: 2.72735548e-07
Iter: 1105 loss: 2.72510846e-07
Iter: 1106 loss: 2.72648208e-07
Iter: 1107 loss: 2.72377861e-07
Iter: 1108 loss: 2.72247291e-07
Iter: 1109 loss: 2.72057e-07
Iter: 1110 loss: 2.72035948e-07
Iter: 1111 loss: 2.71840975e-07
Iter: 1112 loss: 2.71835461e-07
Iter: 1113 loss: 2.71650435e-07
Iter: 1114 loss: 2.71275212e-07
Iter: 1115 loss: 2.7845735e-07
Iter: 1116 loss: 2.7126174e-07
Iter: 1117 loss: 2.70940376e-07
Iter: 1118 loss: 2.72752857e-07
Iter: 1119 loss: 2.70900557e-07
Iter: 1120 loss: 2.70729288e-07
Iter: 1121 loss: 2.70736905e-07
Iter: 1122 loss: 2.70615203e-07
Iter: 1123 loss: 2.7029435e-07
Iter: 1124 loss: 2.74229535e-07
Iter: 1125 loss: 2.7027852e-07
Iter: 1126 loss: 2.70038754e-07
Iter: 1127 loss: 2.72096827e-07
Iter: 1128 loss: 2.70014681e-07
Iter: 1129 loss: 2.69826273e-07
Iter: 1130 loss: 2.71558434e-07
Iter: 1131 loss: 2.69820049e-07
Iter: 1132 loss: 2.69650172e-07
Iter: 1133 loss: 2.69465545e-07
Iter: 1134 loss: 2.69408645e-07
Iter: 1135 loss: 2.69153702e-07
Iter: 1136 loss: 2.70459907e-07
Iter: 1137 loss: 2.69127668e-07
Iter: 1138 loss: 2.68930364e-07
Iter: 1139 loss: 2.70993894e-07
Iter: 1140 loss: 2.68920985e-07
Iter: 1141 loss: 2.68780497e-07
Iter: 1142 loss: 2.6852868e-07
Iter: 1143 loss: 2.74466458e-07
Iter: 1144 loss: 2.685222e-07
Iter: 1145 loss: 2.68280246e-07
Iter: 1146 loss: 2.70448368e-07
Iter: 1147 loss: 2.68270242e-07
Iter: 1148 loss: 2.68077201e-07
Iter: 1149 loss: 2.7030444e-07
Iter: 1150 loss: 2.68072142e-07
Iter: 1151 loss: 2.67972297e-07
Iter: 1152 loss: 2.6769e-07
Iter: 1153 loss: 2.69401056e-07
Iter: 1154 loss: 2.67596391e-07
Iter: 1155 loss: 2.67459086e-07
Iter: 1156 loss: 2.67396217e-07
Iter: 1157 loss: 2.67189847e-07
Iter: 1158 loss: 2.67211135e-07
Iter: 1159 loss: 2.67043902e-07
Iter: 1160 loss: 2.66823292e-07
Iter: 1161 loss: 2.66534585e-07
Iter: 1162 loss: 2.66500535e-07
Iter: 1163 loss: 2.66278391e-07
Iter: 1164 loss: 2.66261111e-07
Iter: 1165 loss: 2.66079667e-07
Iter: 1166 loss: 2.66929021e-07
Iter: 1167 loss: 2.66012592e-07
Iter: 1168 loss: 2.65874377e-07
Iter: 1169 loss: 2.65733092e-07
Iter: 1170 loss: 2.65707712e-07
Iter: 1171 loss: 2.65533572e-07
Iter: 1172 loss: 2.65540621e-07
Iter: 1173 loss: 2.654252e-07
Iter: 1174 loss: 2.65463456e-07
Iter: 1175 loss: 2.65309495e-07
Iter: 1176 loss: 2.65171963e-07
Iter: 1177 loss: 2.65126971e-07
Iter: 1178 loss: 2.65049835e-07
Iter: 1179 loss: 2.64923614e-07
Iter: 1180 loss: 2.64929412e-07
Iter: 1181 loss: 2.64787275e-07
Iter: 1182 loss: 2.64541171e-07
Iter: 1183 loss: 2.69645085e-07
Iter: 1184 loss: 2.64543615e-07
Iter: 1185 loss: 2.64282249e-07
Iter: 1186 loss: 2.65008367e-07
Iter: 1187 loss: 2.64214094e-07
Iter: 1188 loss: 2.63936272e-07
Iter: 1189 loss: 2.66005941e-07
Iter: 1190 loss: 2.63916547e-07
Iter: 1191 loss: 2.63721887e-07
Iter: 1192 loss: 2.63472629e-07
Iter: 1193 loss: 2.63446566e-07
Iter: 1194 loss: 2.63199183e-07
Iter: 1195 loss: 2.6415114e-07
Iter: 1196 loss: 2.63142709e-07
Iter: 1197 loss: 2.62955439e-07
Iter: 1198 loss: 2.65502223e-07
Iter: 1199 loss: 2.62955496e-07
Iter: 1200 loss: 2.62813813e-07
Iter: 1201 loss: 2.6276274e-07
Iter: 1202 loss: 2.62681681e-07
Iter: 1203 loss: 2.62489152e-07
Iter: 1204 loss: 2.62680516e-07
Iter: 1205 loss: 2.62390898e-07
Iter: 1206 loss: 2.62097899e-07
Iter: 1207 loss: 2.63484424e-07
Iter: 1208 loss: 2.62036451e-07
Iter: 1209 loss: 2.61823573e-07
Iter: 1210 loss: 2.61870355e-07
Iter: 1211 loss: 2.616583e-07
Iter: 1212 loss: 2.61437663e-07
Iter: 1213 loss: 2.62489351e-07
Iter: 1214 loss: 2.61372918e-07
Iter: 1215 loss: 2.6109862e-07
Iter: 1216 loss: 2.62442654e-07
Iter: 1217 loss: 2.61046e-07
Iter: 1218 loss: 2.60875282e-07
Iter: 1219 loss: 2.60671868e-07
Iter: 1220 loss: 2.60637904e-07
Iter: 1221 loss: 2.60545164e-07
Iter: 1222 loss: 2.60497075e-07
Iter: 1223 loss: 2.60392198e-07
Iter: 1224 loss: 2.60237869e-07
Iter: 1225 loss: 2.60230706e-07
Iter: 1226 loss: 2.60043123e-07
Iter: 1227 loss: 2.59991026e-07
Iter: 1228 loss: 2.5986688e-07
Iter: 1229 loss: 2.59690239e-07
Iter: 1230 loss: 2.59690438e-07
Iter: 1231 loss: 2.59517748e-07
Iter: 1232 loss: 2.59586784e-07
Iter: 1233 loss: 2.59386695e-07
Iter: 1234 loss: 2.59170946e-07
Iter: 1235 loss: 2.59186493e-07
Iter: 1236 loss: 2.59024318e-07
Iter: 1237 loss: 2.58813259e-07
Iter: 1238 loss: 2.58825111e-07
Iter: 1239 loss: 2.58684508e-07
Iter: 1240 loss: 2.58582816e-07
Iter: 1241 loss: 2.58524381e-07
Iter: 1242 loss: 2.58291578e-07
Iter: 1243 loss: 2.58678284e-07
Iter: 1244 loss: 2.58205205e-07
Iter: 1245 loss: 2.5806105e-07
Iter: 1246 loss: 2.58051443e-07
Iter: 1247 loss: 2.57945089e-07
Iter: 1248 loss: 2.57762309e-07
Iter: 1249 loss: 2.5776859e-07
Iter: 1250 loss: 2.57606388e-07
Iter: 1251 loss: 2.59144514e-07
Iter: 1252 loss: 2.57598089e-07
Iter: 1253 loss: 2.57442508e-07
Iter: 1254 loss: 2.57937245e-07
Iter: 1255 loss: 2.57393793e-07
Iter: 1256 loss: 2.57278543e-07
Iter: 1257 loss: 2.57056314e-07
Iter: 1258 loss: 2.61719492e-07
Iter: 1259 loss: 2.57037215e-07
Iter: 1260 loss: 2.56760671e-07
Iter: 1261 loss: 2.58672515e-07
Iter: 1262 loss: 2.56754959e-07
Iter: 1263 loss: 2.56523521e-07
Iter: 1264 loss: 2.58710827e-07
Iter: 1265 loss: 2.5650445e-07
Iter: 1266 loss: 2.56360437e-07
Iter: 1267 loss: 2.56275086e-07
Iter: 1268 loss: 2.56229384e-07
Iter: 1269 loss: 2.56052289e-07
Iter: 1270 loss: 2.57842089e-07
Iter: 1271 loss: 2.56032649e-07
Iter: 1272 loss: 2.55911232e-07
Iter: 1273 loss: 2.56108393e-07
Iter: 1274 loss: 2.55810846e-07
Iter: 1275 loss: 2.55683148e-07
Iter: 1276 loss: 2.55618517e-07
Iter: 1277 loss: 2.5552805e-07
Iter: 1278 loss: 2.55361385e-07
Iter: 1279 loss: 2.553694e-07
Iter: 1280 loss: 2.55228201e-07
Iter: 1281 loss: 2.55245197e-07
Iter: 1282 loss: 2.55102151e-07
Iter: 1283 loss: 2.54953761e-07
Iter: 1284 loss: 2.55079271e-07
Iter: 1285 loss: 2.5487418e-07
Iter: 1286 loss: 2.54664883e-07
Iter: 1287 loss: 2.56108535e-07
Iter: 1288 loss: 2.54655504e-07
Iter: 1289 loss: 2.54505494e-07
Iter: 1290 loss: 2.54181828e-07
Iter: 1291 loss: 2.59562569e-07
Iter: 1292 loss: 2.54164206e-07
Iter: 1293 loss: 2.53870496e-07
Iter: 1294 loss: 2.56680437e-07
Iter: 1295 loss: 2.53868222e-07
Iter: 1296 loss: 2.53758742e-07
Iter: 1297 loss: 2.53749164e-07
Iter: 1298 loss: 2.53624393e-07
Iter: 1299 loss: 2.53466084e-07
Iter: 1300 loss: 2.53464862e-07
Iter: 1301 loss: 2.53274237e-07
Iter: 1302 loss: 2.55035587e-07
Iter: 1303 loss: 2.53264403e-07
Iter: 1304 loss: 2.53112688e-07
Iter: 1305 loss: 2.53549103e-07
Iter: 1306 loss: 2.53070226e-07
Iter: 1307 loss: 2.529477e-07
Iter: 1308 loss: 2.52859309e-07
Iter: 1309 loss: 2.52783735e-07
Iter: 1310 loss: 2.52603883e-07
Iter: 1311 loss: 2.54293e-07
Iter: 1312 loss: 2.52592969e-07
Iter: 1313 loss: 2.52429089e-07
Iter: 1314 loss: 2.52892846e-07
Iter: 1315 loss: 2.52365453e-07
Iter: 1316 loss: 2.52210384e-07
Iter: 1317 loss: 2.52090814e-07
Iter: 1318 loss: 2.52063955e-07
Iter: 1319 loss: 2.5183067e-07
Iter: 1320 loss: 2.54538236e-07
Iter: 1321 loss: 2.51850111e-07
Iter: 1322 loss: 2.51666506e-07
Iter: 1323 loss: 2.51487279e-07
Iter: 1324 loss: 2.51456612e-07
Iter: 1325 loss: 2.51215766e-07
Iter: 1326 loss: 2.51663465e-07
Iter: 1327 loss: 2.51147924e-07
Iter: 1328 loss: 2.51018889e-07
Iter: 1329 loss: 2.51015791e-07
Iter: 1330 loss: 2.50890253e-07
Iter: 1331 loss: 2.50840571e-07
Iter: 1332 loss: 2.50757552e-07
Iter: 1333 loss: 2.50660037e-07
Iter: 1334 loss: 2.5124308e-07
Iter: 1335 loss: 2.50641875e-07
Iter: 1336 loss: 2.50501927e-07
Iter: 1337 loss: 2.50945135e-07
Iter: 1338 loss: 2.50452814e-07
Iter: 1339 loss: 2.50328924e-07
Iter: 1340 loss: 2.50214384e-07
Iter: 1341 loss: 2.50178346e-07
Iter: 1342 loss: 2.49987579e-07
Iter: 1343 loss: 2.51321495e-07
Iter: 1344 loss: 2.49962966e-07
Iter: 1345 loss: 2.49782687e-07
Iter: 1346 loss: 2.5076838e-07
Iter: 1347 loss: 2.49765122e-07
Iter: 1348 loss: 2.49633558e-07
Iter: 1349 loss: 2.49449528e-07
Iter: 1350 loss: 2.49439125e-07
Iter: 1351 loss: 2.49238383e-07
Iter: 1352 loss: 2.49241708e-07
Iter: 1353 loss: 2.49100566e-07
Iter: 1354 loss: 2.49099571e-07
Iter: 1355 loss: 2.48981536e-07
Iter: 1356 loss: 2.48836443e-07
Iter: 1357 loss: 2.48778264e-07
Iter: 1358 loss: 2.4871531e-07
Iter: 1359 loss: 2.4855072e-07
Iter: 1360 loss: 2.50900769e-07
Iter: 1361 loss: 2.48548531e-07
Iter: 1362 loss: 2.48399573e-07
Iter: 1363 loss: 2.48676685e-07
Iter: 1364 loss: 2.48338466e-07
Iter: 1365 loss: 2.48203378e-07
Iter: 1366 loss: 2.48149263e-07
Iter: 1367 loss: 2.48061497e-07
Iter: 1368 loss: 2.47869821e-07
Iter: 1369 loss: 2.50274354e-07
Iter: 1370 loss: 2.4786803e-07
Iter: 1371 loss: 2.47719925e-07
Iter: 1372 loss: 2.47653873e-07
Iter: 1373 loss: 2.47581852e-07
Iter: 1374 loss: 2.47418427e-07
Iter: 1375 loss: 2.48182801e-07
Iter: 1376 loss: 2.47388272e-07
Iter: 1377 loss: 2.47226041e-07
Iter: 1378 loss: 2.48422907e-07
Iter: 1379 loss: 2.47209e-07
Iter: 1380 loss: 2.47081942e-07
Iter: 1381 loss: 2.46924429e-07
Iter: 1382 loss: 2.46912208e-07
Iter: 1383 loss: 2.46776096e-07
Iter: 1384 loss: 2.48816434e-07
Iter: 1385 loss: 2.46782861e-07
Iter: 1386 loss: 2.46655077e-07
Iter: 1387 loss: 2.46674148e-07
Iter: 1388 loss: 2.46559978e-07
Iter: 1389 loss: 2.46420797e-07
Iter: 1390 loss: 2.46310179e-07
Iter: 1391 loss: 2.46260129e-07
Iter: 1392 loss: 2.46093606e-07
Iter: 1393 loss: 2.47948378e-07
Iter: 1394 loss: 2.46085563e-07
Iter: 1395 loss: 2.45916311e-07
Iter: 1396 loss: 2.46955835e-07
Iter: 1397 loss: 2.45877231e-07
Iter: 1398 loss: 2.45769e-07
Iter: 1399 loss: 2.45635761e-07
Iter: 1400 loss: 2.45630474e-07
Iter: 1401 loss: 2.45462275e-07
Iter: 1402 loss: 2.45464832e-07
Iter: 1403 loss: 2.45317807e-07
Iter: 1404 loss: 2.4520196e-07
Iter: 1405 loss: 2.45166717e-07
Iter: 1406 loss: 2.4499127e-07
Iter: 1407 loss: 2.45883257e-07
Iter: 1408 loss: 2.44961456e-07
Iter: 1409 loss: 2.44815197e-07
Iter: 1410 loss: 2.46544801e-07
Iter: 1411 loss: 2.44814629e-07
Iter: 1412 loss: 2.44725612e-07
Iter: 1413 loss: 2.44607e-07
Iter: 1414 loss: 2.44584157e-07
Iter: 1415 loss: 2.44433295e-07
Iter: 1416 loss: 2.45347621e-07
Iter: 1417 loss: 2.44429089e-07
Iter: 1418 loss: 2.44268e-07
Iter: 1419 loss: 2.44598198e-07
Iter: 1420 loss: 2.44206205e-07
Iter: 1421 loss: 2.44079e-07
Iter: 1422 loss: 2.43919317e-07
Iter: 1423 loss: 2.43911785e-07
Iter: 1424 loss: 2.43678272e-07
Iter: 1425 loss: 2.44249293e-07
Iter: 1426 loss: 2.4361006e-07
Iter: 1427 loss: 2.43442173e-07
Iter: 1428 loss: 2.43428133e-07
Iter: 1429 loss: 2.43346165e-07
Iter: 1430 loss: 2.43242056e-07
Iter: 1431 loss: 2.43235604e-07
Iter: 1432 loss: 2.43071099e-07
Iter: 1433 loss: 2.44461035e-07
Iter: 1434 loss: 2.43052938e-07
Iter: 1435 loss: 2.4292666e-07
Iter: 1436 loss: 2.4293098e-07
Iter: 1437 loss: 2.42810984e-07
Iter: 1438 loss: 2.4262846e-07
Iter: 1439 loss: 2.42787138e-07
Iter: 1440 loss: 2.4254075e-07
Iter: 1441 loss: 2.42377155e-07
Iter: 1442 loss: 2.42362717e-07
Iter: 1443 loss: 2.42206255e-07
Iter: 1444 loss: 2.42146484e-07
Iter: 1445 loss: 2.42060366e-07
Iter: 1446 loss: 2.41897396e-07
Iter: 1447 loss: 2.42495787e-07
Iter: 1448 loss: 2.41852376e-07
Iter: 1449 loss: 2.4167349e-07
Iter: 1450 loss: 2.42647872e-07
Iter: 1451 loss: 2.41638759e-07
Iter: 1452 loss: 2.41535474e-07
Iter: 1453 loss: 2.41392e-07
Iter: 1454 loss: 2.41373442e-07
Iter: 1455 loss: 2.41198563e-07
Iter: 1456 loss: 2.41562105e-07
Iter: 1457 loss: 2.4110895e-07
Iter: 1458 loss: 2.4103997e-07
Iter: 1459 loss: 2.41002567e-07
Iter: 1460 loss: 2.40916251e-07
Iter: 1461 loss: 2.40760642e-07
Iter: 1462 loss: 2.43617365e-07
Iter: 1463 loss: 2.40757203e-07
Iter: 1464 loss: 2.40575275e-07
Iter: 1465 loss: 2.42004432e-07
Iter: 1466 loss: 2.40568454e-07
Iter: 1467 loss: 2.40382406e-07
Iter: 1468 loss: 2.40890017e-07
Iter: 1469 loss: 2.40311863e-07
Iter: 1470 loss: 2.40181862e-07
Iter: 1471 loss: 2.40039356e-07
Iter: 1472 loss: 2.40012525e-07
Iter: 1473 loss: 2.3985109e-07
Iter: 1474 loss: 2.39839409e-07
Iter: 1475 loss: 2.39687267e-07
Iter: 1476 loss: 2.39793081e-07
Iter: 1477 loss: 2.39606436e-07
Iter: 1478 loss: 2.39473025e-07
Iter: 1479 loss: 2.39698466e-07
Iter: 1480 loss: 2.39401544e-07
Iter: 1481 loss: 2.39236385e-07
Iter: 1482 loss: 2.40707124e-07
Iter: 1483 loss: 2.3922945e-07
Iter: 1484 loss: 2.3913708e-07
Iter: 1485 loss: 2.39003384e-07
Iter: 1486 loss: 2.3898761e-07
Iter: 1487 loss: 2.38821457e-07
Iter: 1488 loss: 2.38911213e-07
Iter: 1489 loss: 2.38737698e-07
Iter: 1490 loss: 2.38569385e-07
Iter: 1491 loss: 2.38573023e-07
Iter: 1492 loss: 2.38423524e-07
Iter: 1493 loss: 2.38245832e-07
Iter: 1494 loss: 2.38223507e-07
Iter: 1495 loss: 2.37993873e-07
Iter: 1496 loss: 2.38865482e-07
Iter: 1497 loss: 2.37947646e-07
Iter: 1498 loss: 2.37716193e-07
Iter: 1499 loss: 2.39341091e-07
Iter: 1500 loss: 2.37697009e-07
Iter: 1501 loss: 2.37559306e-07
Iter: 1502 loss: 2.37430427e-07
Iter: 1503 loss: 2.37407932e-07
Iter: 1504 loss: 2.37293136e-07
Iter: 1505 loss: 2.37281313e-07
Iter: 1506 loss: 2.37163277e-07
Iter: 1507 loss: 2.37415392e-07
Iter: 1508 loss: 2.37120105e-07
Iter: 1509 loss: 2.36991298e-07
Iter: 1510 loss: 2.37044333e-07
Iter: 1511 loss: 2.36931299e-07
Iter: 1512 loss: 2.36815538e-07
Iter: 1513 loss: 2.38019311e-07
Iter: 1514 loss: 2.36822245e-07
Iter: 1515 loss: 2.36715664e-07
Iter: 1516 loss: 2.36586814e-07
Iter: 1517 loss: 2.36588065e-07
Iter: 1518 loss: 2.3640186e-07
Iter: 1519 loss: 2.36165846e-07
Iter: 1520 loss: 2.36146207e-07
Iter: 1521 loss: 2.35909667e-07
Iter: 1522 loss: 2.3589169e-07
Iter: 1523 loss: 2.3567199e-07
Iter: 1524 loss: 2.36418785e-07
Iter: 1525 loss: 2.35609662e-07
Iter: 1526 loss: 2.3544743e-07
Iter: 1527 loss: 2.35321224e-07
Iter: 1528 loss: 2.35257545e-07
Iter: 1529 loss: 2.35088095e-07
Iter: 1530 loss: 2.3507684e-07
Iter: 1531 loss: 2.3498562e-07
Iter: 1532 loss: 2.34887096e-07
Iter: 1533 loss: 2.34866292e-07
Iter: 1534 loss: 2.34742686e-07
Iter: 1535 loss: 2.35545542e-07
Iter: 1536 loss: 2.34720346e-07
Iter: 1537 loss: 2.34598559e-07
Iter: 1538 loss: 2.35093978e-07
Iter: 1539 loss: 2.34577698e-07
Iter: 1540 loss: 2.34482116e-07
Iter: 1541 loss: 2.34454362e-07
Iter: 1542 loss: 2.34379669e-07
Iter: 1543 loss: 2.34261734e-07
Iter: 1544 loss: 2.35097502e-07
Iter: 1545 loss: 2.34235358e-07
Iter: 1546 loss: 2.34082989e-07
Iter: 1547 loss: 2.34024753e-07
Iter: 1548 loss: 2.33943013e-07
Iter: 1549 loss: 2.33759238e-07
Iter: 1550 loss: 2.33650354e-07
Iter: 1551 loss: 2.33580124e-07
Iter: 1552 loss: 2.33323874e-07
Iter: 1553 loss: 2.35445498e-07
Iter: 1554 loss: 2.33295083e-07
Iter: 1555 loss: 2.33121611e-07
Iter: 1556 loss: 2.35639064e-07
Iter: 1557 loss: 2.33125775e-07
Iter: 1558 loss: 2.3301898e-07
Iter: 1559 loss: 2.3287889e-07
Iter: 1560 loss: 2.32871059e-07
Iter: 1561 loss: 2.32777381e-07
Iter: 1562 loss: 2.32772578e-07
Iter: 1563 loss: 2.32683291e-07
Iter: 1564 loss: 2.32549709e-07
Iter: 1565 loss: 2.32552665e-07
Iter: 1566 loss: 2.32408041e-07
Iter: 1567 loss: 2.32822615e-07
Iter: 1568 loss: 2.32344476e-07
Iter: 1569 loss: 2.32195362e-07
Iter: 1570 loss: 2.33493665e-07
Iter: 1571 loss: 2.32190644e-07
Iter: 1572 loss: 2.32089775e-07
Iter: 1573 loss: 2.32079969e-07
Iter: 1574 loss: 2.32003515e-07
Iter: 1575 loss: 2.31861264e-07
Iter: 1576 loss: 2.32593905e-07
Iter: 1577 loss: 2.31825965e-07
Iter: 1578 loss: 2.31677589e-07
Iter: 1579 loss: 2.3186567e-07
Iter: 1580 loss: 2.31572457e-07
Iter: 1581 loss: 2.3144969e-07
Iter: 1582 loss: 2.31335562e-07
Iter: 1583 loss: 2.31290869e-07
Iter: 1584 loss: 2.31067318e-07
Iter: 1585 loss: 2.31601149e-07
Iter: 1586 loss: 2.31011612e-07
Iter: 1587 loss: 2.30917465e-07
Iter: 1588 loss: 2.30874974e-07
Iter: 1589 loss: 2.30798804e-07
Iter: 1590 loss: 2.30615385e-07
Iter: 1591 loss: 2.33643135e-07
Iter: 1592 loss: 2.30619307e-07
Iter: 1593 loss: 2.3046448e-07
Iter: 1594 loss: 2.32539378e-07
Iter: 1595 loss: 2.30465503e-07
Iter: 1596 loss: 2.30322726e-07
Iter: 1597 loss: 2.30473e-07
Iter: 1598 loss: 2.30239777e-07
Iter: 1599 loss: 2.30108824e-07
Iter: 1600 loss: 2.30071862e-07
Iter: 1601 loss: 2.29989382e-07
Iter: 1602 loss: 2.29826384e-07
Iter: 1603 loss: 2.29839401e-07
Iter: 1604 loss: 2.29739413e-07
Iter: 1605 loss: 2.29699694e-07
Iter: 1606 loss: 2.29624874e-07
Iter: 1607 loss: 2.29514626e-07
Iter: 1608 loss: 2.30246485e-07
Iter: 1609 loss: 2.29501836e-07
Iter: 1610 loss: 2.29402929e-07
Iter: 1611 loss: 2.29704838e-07
Iter: 1612 loss: 2.29371423e-07
Iter: 1613 loss: 2.29279379e-07
Iter: 1614 loss: 2.29098362e-07
Iter: 1615 loss: 2.29109929e-07
Iter: 1616 loss: 2.28920968e-07
Iter: 1617 loss: 2.29406794e-07
Iter: 1618 loss: 2.28862461e-07
Iter: 1619 loss: 2.28743076e-07
Iter: 1620 loss: 2.287386e-07
Iter: 1621 loss: 2.2858282e-07
Iter: 1622 loss: 2.28466206e-07
Iter: 1623 loss: 2.28414535e-07
Iter: 1624 loss: 2.28287533e-07
Iter: 1625 loss: 2.28859307e-07
Iter: 1626 loss: 2.28252702e-07
Iter: 1627 loss: 2.28135733e-07
Iter: 1628 loss: 2.29491988e-07
Iter: 1629 loss: 2.28118566e-07
Iter: 1630 loss: 2.28020397e-07
Iter: 1631 loss: 2.27858635e-07
Iter: 1632 loss: 2.31301158e-07
Iter: 1633 loss: 2.27868611e-07
Iter: 1634 loss: 2.27770528e-07
Iter: 1635 loss: 2.27763906e-07
Iter: 1636 loss: 2.27663136e-07
Iter: 1637 loss: 2.2758033e-07
Iter: 1638 loss: 2.27559184e-07
Iter: 1639 loss: 2.27409e-07
Iter: 1640 loss: 2.2801143e-07
Iter: 1641 loss: 2.27375523e-07
Iter: 1642 loss: 2.27227673e-07
Iter: 1643 loss: 2.27703509e-07
Iter: 1644 loss: 2.2720036e-07
Iter: 1645 loss: 2.27076256e-07
Iter: 1646 loss: 2.27043643e-07
Iter: 1647 loss: 2.26945048e-07
Iter: 1648 loss: 2.26819182e-07
Iter: 1649 loss: 2.26877617e-07
Iter: 1650 loss: 2.26707158e-07
Iter: 1651 loss: 2.26541e-07
Iter: 1652 loss: 2.27709464e-07
Iter: 1653 loss: 2.2653073e-07
Iter: 1654 loss: 2.26397873e-07
Iter: 1655 loss: 2.27813246e-07
Iter: 1656 loss: 2.26387897e-07
Iter: 1657 loss: 2.26325596e-07
Iter: 1658 loss: 2.26175615e-07
Iter: 1659 loss: 2.28209046e-07
Iter: 1660 loss: 2.26171309e-07
Iter: 1661 loss: 2.26048343e-07
Iter: 1662 loss: 2.2603966e-07
Iter: 1663 loss: 2.25894667e-07
Iter: 1664 loss: 2.2586859e-07
Iter: 1665 loss: 2.25801131e-07
Iter: 1666 loss: 2.25683749e-07
Iter: 1667 loss: 2.26163507e-07
Iter: 1668 loss: 2.2566033e-07
Iter: 1669 loss: 2.25500202e-07
Iter: 1670 loss: 2.25685369e-07
Iter: 1671 loss: 2.2543334e-07
Iter: 1672 loss: 2.2530962e-07
Iter: 1673 loss: 2.25811505e-07
Iter: 1674 loss: 2.25291188e-07
Iter: 1675 loss: 2.25174801e-07
Iter: 1676 loss: 2.25320974e-07
Iter: 1677 loss: 2.25115159e-07
Iter: 1678 loss: 2.24987105e-07
Iter: 1679 loss: 2.25197525e-07
Iter: 1680 loss: 2.24924605e-07
Iter: 1681 loss: 2.24815864e-07
Iter: 1682 loss: 2.24805675e-07
Iter: 1683 loss: 2.24721475e-07
Iter: 1684 loss: 2.24557795e-07
Iter: 1685 loss: 2.24899992e-07
Iter: 1686 loss: 2.24522154e-07
Iter: 1687 loss: 2.24417278e-07
Iter: 1688 loss: 2.24414364e-07
Iter: 1689 loss: 2.24325873e-07
Iter: 1690 loss: 2.24211888e-07
Iter: 1691 loss: 2.24205706e-07
Iter: 1692 loss: 2.24084857e-07
Iter: 1693 loss: 2.24564772e-07
Iter: 1694 loss: 2.24044058e-07
Iter: 1695 loss: 2.23908017e-07
Iter: 1696 loss: 2.25011888e-07
Iter: 1697 loss: 2.23910746e-07
Iter: 1698 loss: 2.2382406e-07
Iter: 1699 loss: 2.23740457e-07
Iter: 1700 loss: 2.23732371e-07
Iter: 1701 loss: 2.23642203e-07
Iter: 1702 loss: 2.23628689e-07
Iter: 1703 loss: 2.23580656e-07
Iter: 1704 loss: 2.23479617e-07
Iter: 1705 loss: 2.23488712e-07
Iter: 1706 loss: 2.23384262e-07
Iter: 1707 loss: 2.24472174e-07
Iter: 1708 loss: 2.23382131e-07
Iter: 1709 loss: 2.23304795e-07
Iter: 1710 loss: 2.23320342e-07
Iter: 1711 loss: 2.23259804e-07
Iter: 1712 loss: 2.23163411e-07
Iter: 1713 loss: 2.23202477e-07
Iter: 1714 loss: 2.23094474e-07
Iter: 1715 loss: 2.22964303e-07
Iter: 1716 loss: 2.22898336e-07
Iter: 1717 loss: 2.22822933e-07
Iter: 1718 loss: 2.22691682e-07
Iter: 1719 loss: 2.2271135e-07
Iter: 1720 loss: 2.22583111e-07
Iter: 1721 loss: 2.23153e-07
Iter: 1722 loss: 2.22563415e-07
Iter: 1723 loss: 2.22464266e-07
Iter: 1724 loss: 2.22374439e-07
Iter: 1725 loss: 2.22355638e-07
Iter: 1726 loss: 2.22297842e-07
Iter: 1727 loss: 2.2229e-07
Iter: 1728 loss: 2.22243e-07
Iter: 1729 loss: 2.22112419e-07
Iter: 1730 loss: 2.24547435e-07
Iter: 1731 loss: 2.22117379e-07
Iter: 1732 loss: 2.22006378e-07
Iter: 1733 loss: 2.22523454e-07
Iter: 1734 loss: 2.21962281e-07
Iter: 1735 loss: 2.21867595e-07
Iter: 1736 loss: 2.23172549e-07
Iter: 1737 loss: 2.21859523e-07
Iter: 1738 loss: 2.21822859e-07
Iter: 1739 loss: 2.21813679e-07
Iter: 1740 loss: 2.21791069e-07
Iter: 1741 loss: 2.21701896e-07
Iter: 1742 loss: 2.22024468e-07
Iter: 1743 loss: 2.216903e-07
Iter: 1744 loss: 2.21615892e-07
Iter: 1745 loss: 2.21577935e-07
Iter: 1746 loss: 2.21545449e-07
Iter: 1747 loss: 2.21417693e-07
Iter: 1748 loss: 2.21574965e-07
Iter: 1749 loss: 2.21368481e-07
Iter: 1750 loss: 2.21220503e-07
Iter: 1751 loss: 2.2137057e-07
Iter: 1752 loss: 2.21152021e-07
Iter: 1753 loss: 2.21018894e-07
Iter: 1754 loss: 2.21700404e-07
Iter: 1755 loss: 2.20980809e-07
Iter: 1756 loss: 2.20871684e-07
Iter: 1757 loss: 2.20880253e-07
Iter: 1758 loss: 2.20809454e-07
Iter: 1759 loss: 2.20692172e-07
Iter: 1760 loss: 2.20699036e-07
Iter: 1761 loss: 2.20661661e-07
Iter: 1762 loss: 2.20645205e-07
Iter: 1763 loss: 2.20608896e-07
Iter: 1764 loss: 2.20542304e-07
Iter: 1765 loss: 2.20533579e-07
Iter: 1766 loss: 2.20436576e-07
Iter: 1767 loss: 2.20422379e-07
Iter: 1768 loss: 2.20363546e-07
Iter: 1769 loss: 2.2032269e-07
Iter: 1770 loss: 2.20312558e-07
Iter: 1771 loss: 2.20239457e-07
Iter: 1772 loss: 2.2014909e-07
Iter: 1773 loss: 2.20158682e-07
Iter: 1774 loss: 2.20067406e-07
Iter: 1775 loss: 2.2128404e-07
Iter: 1776 loss: 2.20072081e-07
Iter: 1777 loss: 2.19994831e-07
Iter: 1778 loss: 2.20010236e-07
Iter: 1779 loss: 2.19931422e-07
Iter: 1780 loss: 2.19859302e-07
Iter: 1781 loss: 2.19795766e-07
Iter: 1782 loss: 2.19768e-07
Iter: 1783 loss: 2.1965208e-07
Iter: 1784 loss: 2.20375227e-07
Iter: 1785 loss: 2.1963551e-07
Iter: 1786 loss: 2.19555389e-07
Iter: 1787 loss: 2.20456585e-07
Iter: 1788 loss: 2.19556227e-07
Iter: 1789 loss: 2.19487475e-07
Iter: 1790 loss: 2.19569273e-07
Iter: 1791 loss: 2.19435194e-07
Iter: 1792 loss: 2.19356409e-07
Iter: 1793 loss: 2.19281219e-07
Iter: 1794 loss: 2.19250168e-07
Iter: 1795 loss: 2.19141668e-07
Iter: 1796 loss: 2.19139423e-07
Iter: 1797 loss: 2.19076682e-07
Iter: 1798 loss: 2.19013742e-07
Iter: 1799 loss: 2.19000185e-07
Iter: 1800 loss: 2.18883116e-07
Iter: 1801 loss: 2.18937188e-07
Iter: 1802 loss: 2.18820986e-07
Iter: 1803 loss: 2.18735352e-07
Iter: 1804 loss: 2.18723045e-07
Iter: 1805 loss: 2.18667765e-07
Iter: 1806 loss: 2.18643066e-07
Iter: 1807 loss: 2.18605891e-07
Iter: 1808 loss: 2.18557858e-07
Iter: 1809 loss: 2.19234693e-07
Iter: 1810 loss: 2.18566854e-07
Iter: 1811 loss: 2.18495387e-07
Iter: 1812 loss: 2.18379114e-07
Iter: 1813 loss: 2.20550689e-07
Iter: 1814 loss: 2.18372435e-07
Iter: 1815 loss: 2.18263921e-07
Iter: 1816 loss: 2.18469324e-07
Iter: 1817 loss: 2.18221643e-07
Iter: 1818 loss: 2.18082135e-07
Iter: 1819 loss: 2.18272945e-07
Iter: 1820 loss: 2.18011508e-07
Iter: 1821 loss: 2.17909715e-07
Iter: 1822 loss: 2.17899384e-07
Iter: 1823 loss: 2.17813565e-07
Iter: 1824 loss: 2.17775522e-07
Iter: 1825 loss: 2.17729209e-07
Iter: 1826 loss: 2.17635986e-07
Iter: 1827 loss: 2.17650822e-07
Iter: 1828 loss: 2.17531507e-07
Iter: 1829 loss: 2.17378499e-07
Iter: 1830 loss: 2.17383601e-07
Iter: 1831 loss: 2.17326615e-07
Iter: 1832 loss: 2.17225463e-07
Iter: 1833 loss: 2.17224283e-07
Iter: 1834 loss: 2.17095277e-07
Iter: 1835 loss: 2.17785924e-07
Iter: 1836 loss: 2.17083482e-07
Iter: 1837 loss: 2.16952216e-07
Iter: 1838 loss: 2.17902439e-07
Iter: 1839 loss: 2.16939327e-07
Iter: 1840 loss: 2.16862958e-07
Iter: 1841 loss: 2.16808218e-07
Iter: 1842 loss: 2.16779952e-07
Iter: 1843 loss: 2.16679339e-07
Iter: 1844 loss: 2.17886438e-07
Iter: 1845 loss: 2.166841e-07
Iter: 1846 loss: 2.16610943e-07
Iter: 1847 loss: 2.16464855e-07
Iter: 1848 loss: 2.18483e-07
Iter: 1849 loss: 2.16455291e-07
Iter: 1850 loss: 2.16299014e-07
Iter: 1851 loss: 2.17062237e-07
Iter: 1852 loss: 2.16274387e-07
Iter: 1853 loss: 2.1615287e-07
Iter: 1854 loss: 2.16655e-07
Iter: 1855 loss: 2.16128015e-07
Iter: 1856 loss: 2.16021732e-07
Iter: 1857 loss: 2.16026194e-07
Iter: 1858 loss: 2.15957698e-07
Iter: 1859 loss: 2.15826773e-07
Iter: 1860 loss: 2.18514742e-07
Iter: 1861 loss: 2.15837275e-07
Iter: 1862 loss: 2.15755762e-07
Iter: 1863 loss: 2.15736648e-07
Iter: 1864 loss: 2.15651241e-07
Iter: 1865 loss: 2.15566814e-07
Iter: 1866 loss: 2.15547686e-07
Iter: 1867 loss: 2.15426638e-07
Iter: 1868 loss: 2.15491042e-07
Iter: 1869 loss: 2.15385015e-07
Iter: 1870 loss: 2.1523968e-07
Iter: 1871 loss: 2.16083535e-07
Iter: 1872 loss: 2.15234564e-07
Iter: 1873 loss: 2.15147722e-07
Iter: 1874 loss: 2.15147224e-07
Iter: 1875 loss: 2.15101664e-07
Iter: 1876 loss: 2.15000412e-07
Iter: 1877 loss: 2.16555208e-07
Iter: 1878 loss: 2.15007958e-07
Iter: 1879 loss: 2.14891372e-07
Iter: 1880 loss: 2.14942631e-07
Iter: 1881 loss: 2.14814065e-07
Iter: 1882 loss: 2.14718e-07
Iter: 1883 loss: 2.14711136e-07
Iter: 1884 loss: 2.14613891e-07
Iter: 1885 loss: 2.14500076e-07
Iter: 1886 loss: 2.14491479e-07
Iter: 1887 loss: 2.14363524e-07
Iter: 1888 loss: 2.14390866e-07
Iter: 1889 loss: 2.14262599e-07
Iter: 1890 loss: 2.14132285e-07
Iter: 1891 loss: 2.15346148e-07
Iter: 1892 loss: 2.14135056e-07
Iter: 1893 loss: 2.14026571e-07
Iter: 1894 loss: 2.15005443e-07
Iter: 1895 loss: 2.14031218e-07
Iter: 1896 loss: 2.13969827e-07
Iter: 1897 loss: 2.13889919e-07
Iter: 1898 loss: 2.13869924e-07
Iter: 1899 loss: 2.13777753e-07
Iter: 1900 loss: 2.14581391e-07
Iter: 1901 loss: 2.13777625e-07
Iter: 1902 loss: 2.13686718e-07
Iter: 1903 loss: 2.13993019e-07
Iter: 1904 loss: 2.13657657e-07
Iter: 1905 loss: 2.13585437e-07
Iter: 1906 loss: 2.13455507e-07
Iter: 1907 loss: 2.1604211e-07
Iter: 1908 loss: 2.13454186e-07
Iter: 1909 loss: 2.13293106e-07
Iter: 1910 loss: 2.14466439e-07
Iter: 1911 loss: 2.13295465e-07
Iter: 1912 loss: 2.13154891e-07
Iter: 1913 loss: 2.14874134e-07
Iter: 1914 loss: 2.13147587e-07
Iter: 1915 loss: 2.13077385e-07
Iter: 1916 loss: 2.12974726e-07
Iter: 1917 loss: 2.12977966e-07
Iter: 1918 loss: 2.12925457e-07
Iter: 1919 loss: 2.12921918e-07
Iter: 1920 loss: 2.12852754e-07
Iter: 1921 loss: 2.12893696e-07
Iter: 1922 loss: 2.12806412e-07
Iter: 1923 loss: 2.12748546e-07
Iter: 1924 loss: 2.12702474e-07
Iter: 1925 loss: 2.12667359e-07
Iter: 1926 loss: 2.12590891e-07
Iter: 1927 loss: 2.13044501e-07
Iter: 1928 loss: 2.12571791e-07
Iter: 1929 loss: 2.1246565e-07
Iter: 1930 loss: 2.13037282e-07
Iter: 1931 loss: 2.12444448e-07
Iter: 1932 loss: 2.12376847e-07
Iter: 1933 loss: 2.12296115e-07
Iter: 1934 loss: 2.12282458e-07
Iter: 1935 loss: 2.12136655e-07
Iter: 1936 loss: 2.12749868e-07
Iter: 1937 loss: 2.12123155e-07
Iter: 1938 loss: 2.11998255e-07
Iter: 1939 loss: 2.12908787e-07
Iter: 1940 loss: 2.11983846e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2
+ date
Thu Oct 22 01:56:12 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/500_500_500_500_1 --function f1 --psi 2 --phi 1.2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5ea49a7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e823be2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5ea4953b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e823be268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e823fdbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c3bee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e82341048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e82369840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e82369730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e82341598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c2ebae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c2e68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c2e6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c330d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c330268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c341bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c371840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c25b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c232b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c22be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c193840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c193510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c193378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c135598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c135b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c283488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c1d59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c1d2488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c0aa378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c0bcb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e5c0cf488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e401f3158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e401f3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e40208ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e401f52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5e401b6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.018286021
test_loss: 0.017205423
train_loss: 0.0072684404
test_loss: 0.007122626
train_loss: 0.0041268184
test_loss: 0.0042546336
train_loss: 0.0034296336
test_loss: 0.003498844
train_loss: 0.0029913862
test_loss: 0.0033237075
train_loss: 0.0026414944
test_loss: 0.003287865
train_loss: 0.0029236078
test_loss: 0.0031416814
train_loss: 0.002570057
test_loss: 0.002812962
train_loss: 0.0026022573
test_loss: 0.0027090309
train_loss: 0.0027322671
test_loss: 0.0028932546
train_loss: 0.0023899483
test_loss: 0.0027009402
train_loss: 0.0024849689
test_loss: 0.0027210435
train_loss: 0.0024657631
test_loss: 0.0028084477
train_loss: 0.002275818
test_loss: 0.002661894
train_loss: 0.0026701028
test_loss: 0.0029144068
train_loss: 0.0023651035
test_loss: 0.0027460302
train_loss: 0.0022407826
test_loss: 0.0027792894
train_loss: 0.0022814283
test_loss: 0.0028457013
train_loss: 0.0025706147
test_loss: 0.002775408
train_loss: 0.002592572
test_loss: 0.003007529
train_loss: 0.0024752985
test_loss: 0.0027351184
train_loss: 0.0022919485
test_loss: 0.0028648686
train_loss: 0.0022826293
test_loss: 0.002638756
train_loss: 0.002365582
test_loss: 0.0027989522
train_loss: 0.0024492391
test_loss: 0.0030501818
train_loss: 0.0023931933
test_loss: 0.002696806
train_loss: 0.0022755817
test_loss: 0.0026529972
train_loss: 0.0022129468
test_loss: 0.0025325897
train_loss: 0.0024199535
test_loss: 0.002859859
train_loss: 0.0020696162
test_loss: 0.002588356
train_loss: 0.0021761204
test_loss: 0.0026893525
train_loss: 0.0022233096
test_loss: 0.002671577
train_loss: 0.002187029
test_loss: 0.0025909047
train_loss: 0.0021317536
test_loss: 0.0025853838
train_loss: 0.0023788766
test_loss: 0.0026899357
train_loss: 0.0023725564
test_loss: 0.0028995115
train_loss: 0.0021135225
test_loss: 0.002516835
train_loss: 0.0021099371
test_loss: 0.002640914
train_loss: 0.0022956273
test_loss: 0.0026729626
train_loss: 0.0022943872
test_loss: 0.0026973223
train_loss: 0.0024314865
test_loss: 0.0027218205
train_loss: 0.0022419756
test_loss: 0.0026116117
train_loss: 0.0023227548
test_loss: 0.0026267355
train_loss: 0.0024320753
test_loss: 0.0026874072
train_loss: 0.002163672
test_loss: 0.0026865366
train_loss: 0.0021036016
test_loss: 0.0025736871
train_loss: 0.0021864288
test_loss: 0.0027600199
train_loss: 0.002319692
test_loss: 0.0028801435
train_loss: 0.0023808498
test_loss: 0.0027649058
train_loss: 0.0020952057
test_loss: 0.0026847809
train_loss: 0.002287014
test_loss: 0.0026659556
train_loss: 0.002131747
test_loss: 0.0024926614
train_loss: 0.0022559674
test_loss: 0.0027864843
train_loss: 0.0021917743
test_loss: 0.002726421
train_loss: 0.0024281207
test_loss: 0.0027039107
train_loss: 0.0022018796
test_loss: 0.0026454946
train_loss: 0.0021787505
test_loss: 0.0027380725
train_loss: 0.0021119956
test_loss: 0.0025852728
train_loss: 0.0024279887
test_loss: 0.002907869
train_loss: 0.0023412504
test_loss: 0.0026591776
train_loss: 0.0024858825
test_loss: 0.0026529874
train_loss: 0.0021846504
test_loss: 0.002653771
train_loss: 0.002261556
test_loss: 0.0028901151
train_loss: 0.002360798
test_loss: 0.0026846721
train_loss: 0.0022326342
test_loss: 0.0025851973
train_loss: 0.002391973
test_loss: 0.0027290552
train_loss: 0.0022493629
test_loss: 0.0026595674
train_loss: 0.0022289515
test_loss: 0.0027817474
train_loss: 0.0019614235
test_loss: 0.0024908474
train_loss: 0.002245196
test_loss: 0.0026896037
train_loss: 0.0022660883
test_loss: 0.0027453166
train_loss: 0.0022961898
test_loss: 0.0026468753
train_loss: 0.0023386749
test_loss: 0.0026094706
train_loss: 0.0024095285
test_loss: 0.002644023
train_loss: 0.0022518667
test_loss: 0.0025090617
train_loss: 0.002184349
test_loss: 0.0026114106
train_loss: 0.0023990166
test_loss: 0.0027908492
train_loss: 0.0023472104
test_loss: 0.0027187613
train_loss: 0.002328632
test_loss: 0.0026365642
train_loss: 0.0021290982
test_loss: 0.0028728
train_loss: 0.0021908744
test_loss: 0.002617032
train_loss: 0.0022368159
test_loss: 0.0027683896
train_loss: 0.0019751177
test_loss: 0.0024969557
train_loss: 0.0021199915
test_loss: 0.002618835
train_loss: 0.0021725632
test_loss: 0.0026029155
train_loss: 0.0022158662
test_loss: 0.002568595
train_loss: 0.0022735952
test_loss: 0.0025320917
train_loss: 0.002207955
test_loss: 0.0025980112
train_loss: 0.002233806
test_loss: 0.0027552815
train_loss: 0.002459878
test_loss: 0.0026148418
train_loss: 0.0024702402
test_loss: 0.0026905066
train_loss: 0.0021571447
test_loss: 0.0024339557
train_loss: 0.0021999057
test_loss: 0.002674048
train_loss: 0.002174222
test_loss: 0.0024965778
train_loss: 0.0021029094
test_loss: 0.0026770118
train_loss: 0.002199675
test_loss: 0.0027472845
train_loss: 0.0021340717
test_loss: 0.0026028077
train_loss: 0.0020440128
test_loss: 0.0026478872
train_loss: 0.002119802
test_loss: 0.0029236039
train_loss: 0.0021415586
test_loss: 0.0026626019
train_loss: 0.0021762291
test_loss: 0.0028668025
train_loss: 0.0021020935
test_loss: 0.0026891602
train_loss: 0.0024308506
test_loss: 0.002797671
train_loss: 0.0022078531
test_loss: 0.0025967152
train_loss: 0.0023661982
test_loss: 0.002670516
train_loss: 0.0023159906
test_loss: 0.0025643338
train_loss: 0.0021254837
test_loss: 0.0027191436
train_loss: 0.0021203402
test_loss: 0.002810391
train_loss: 0.0022435756
test_loss: 0.0026106543
train_loss: 0.0024369983
test_loss: 0.0025270956
train_loss: 0.002036918
test_loss: 0.00255568
train_loss: 0.0022854614
test_loss: 0.0025404776
train_loss: 0.002198282
test_loss: 0.0025835894
train_loss: 0.0020637074
test_loss: 0.0025535247
train_loss: 0.0023428476
test_loss: 0.002706831
train_loss: 0.0025390207
test_loss: 0.0029959546
train_loss: 0.0024639927
test_loss: 0.0029413323
train_loss: 0.0022860235
test_loss: 0.0025255526
train_loss: 0.002175067
test_loss: 0.0025953015
train_loss: 0.0022115994
test_loss: 0.0024880846
train_loss: 0.0023222996
test_loss: 0.0024858254
train_loss: 0.002129856
test_loss: 0.002635862
train_loss: 0.002391939
test_loss: 0.0027312464
train_loss: 0.0022459119
test_loss: 0.0025576248
train_loss: 0.002163878
test_loss: 0.0025368242
train_loss: 0.0022514241
test_loss: 0.0025596097
train_loss: 0.002079072
test_loss: 0.0025389027
train_loss: 0.0022697921
test_loss: 0.002449462
train_loss: 0.002169661
test_loss: 0.002515999
train_loss: 0.0019876047
test_loss: 0.002597833
train_loss: 0.0022422634
test_loss: 0.002880858
train_loss: 0.0022653935
test_loss: 0.002701116
train_loss: 0.0022062517
test_loss: 0.002712862
train_loss: 0.0020101536
test_loss: 0.0025920772
train_loss: 0.0020932995
test_loss: 0.0025542767
train_loss: 0.002258407
test_loss: 0.002622828
train_loss: 0.002153188
test_loss: 0.0024755015
train_loss: 0.002131868
test_loss: 0.0025717416
train_loss: 0.0020741967
test_loss: 0.0025458683
train_loss: 0.0020281076
test_loss: 0.002475728
train_loss: 0.0021610789
test_loss: 0.0027429513
train_loss: 0.0020335596
test_loss: 0.002506244
train_loss: 0.0020100367
test_loss: 0.0024615163
train_loss: 0.0023148532
test_loss: 0.002711673
train_loss: 0.002196523
test_loss: 0.0024362975
train_loss: 0.0019672993
test_loss: 0.0025541193
train_loss: 0.0021026789
test_loss: 0.0027259288
train_loss: 0.0021843691
test_loss: 0.0025735097
train_loss: 0.0020548354
test_loss: 0.0025957315
train_loss: 0.0020878762
test_loss: 0.0024843
train_loss: 0.0020321482
test_loss: 0.002614166
train_loss: 0.0021450655
test_loss: 0.0025616207
train_loss: 0.001932976
test_loss: 0.0024274415
train_loss: 0.001890891
test_loss: 0.0024003885
train_loss: 0.0022413742
test_loss: 0.0025257226
train_loss: 0.0021899042
test_loss: 0.00250877
train_loss: 0.0020389343
test_loss: 0.0024750584
train_loss: 0.0022016866
test_loss: 0.002596054
train_loss: 0.0021661846
test_loss: 0.002890321
train_loss: 0.0021476215
test_loss: 0.0027475392
train_loss: 0.0022054273
test_loss: 0.0024468692
train_loss: 0.0020949948
test_loss: 0.0024178005
train_loss: 0.0020689007
test_loss: 0.0025470064
train_loss: 0.0021029254
test_loss: 0.002560529
train_loss: 0.0022146062
test_loss: 0.002486955
train_loss: 0.0022474206
test_loss: 0.0024895216
train_loss: 0.0020059298
test_loss: 0.0025516558
train_loss: 0.0022206958
test_loss: 0.0025663706
train_loss: 0.002303216
test_loss: 0.0028091888
train_loss: 0.0020557868
test_loss: 0.0025609196
train_loss: 0.0020056034
test_loss: 0.0025258546
train_loss: 0.002152703
test_loss: 0.002659571
train_loss: 0.001984161
test_loss: 0.0025310386
train_loss: 0.002205466
test_loss: 0.002519282
train_loss: 0.0021281934
test_loss: 0.0026877397
train_loss: 0.0020920523
test_loss: 0.002411195
train_loss: 0.0022772888
test_loss: 0.0026397142
train_loss: 0.0020293691
test_loss: 0.0028400107
train_loss: 0.0020869714
test_loss: 0.0024784678
train_loss: 0.0022283816
test_loss: 0.002535857
train_loss: 0.0022286933
test_loss: 0.0026193222
train_loss: 0.0021231442
test_loss: 0.0026360855
train_loss: 0.0020023556
test_loss: 0.0025565051
train_loss: 0.0020937363
test_loss: 0.0025888935
train_loss: 0.0022269136
test_loss: 0.002408971
train_loss: 0.0021052025
test_loss: 0.0026236335
train_loss: 0.0023480507
test_loss: 0.0024910367
train_loss: 0.0022357802
test_loss: 0.0027342604
train_loss: 0.0020524657
test_loss: 0.0026462472
train_loss: 0.0022720587
test_loss: 0.002561857
train_loss: 0.0022669546
test_loss: 0.0025543948
train_loss: 0.0021050978
test_loss: 0.0024947247
train_loss: 0.0022630235
test_loss: 0.0025145104
train_loss: 0.0020781131
test_loss: 0.0024569929
train_loss: 0.002333334
test_loss: 0.002472893
train_loss: 0.0020780433
test_loss: 0.0024487216
train_loss: 0.0021297303
test_loss: 0.0024724335
train_loss: 0.0022508986
test_loss: 0.002457177
train_loss: 0.0021549463
test_loss: 0.0025029297
train_loss: 0.0021691602
test_loss: 0.0025758205
train_loss: 0.0022010722
test_loss: 0.0025478567
train_loss: 0.001987526
test_loss: 0.0024379832
train_loss: 0.0026243813
test_loss: 0.002666137
train_loss: 0.0021439102
test_loss: 0.002592656
train_loss: 0.002020257
test_loss: 0.002414254
train_loss: 0.002062519
test_loss: 0.0025357723
train_loss: 0.0023187527
test_loss: 0.0024798482
train_loss: 0.0019253733
test_loss: 0.0024956495
train_loss: 0.0021314844
test_loss: 0.0026441528
train_loss: 0.002041929
test_loss: 0.002648086
train_loss: 0.0023268738
test_loss: 0.0025154138
train_loss: 0.0019789457
test_loss: 0.0024650444
train_loss: 0.0023080288
test_loss: 0.002663983
train_loss: 0.0024314912
test_loss: 0.0027136598
train_loss: 0.0023184305
test_loss: 0.0025866246
train_loss: 0.0023103685
test_loss: 0.0025352947
train_loss: 0.0023162873
test_loss: 0.0026491934
train_loss: 0.0020991839
test_loss: 0.0025142825
train_loss: 0.0021860977
test_loss: 0.0026114236
train_loss: 0.0019953935
test_loss: 0.0024214264
train_loss: 0.0021461667
test_loss: 0.002577447
train_loss: 0.002270634
test_loss: 0.002748792
train_loss: 0.0021689131
test_loss: 0.0027700316
train_loss: 0.002193152
test_loss: 0.002586682
train_loss: 0.0022186292
test_loss: 0.0025929713
train_loss: 0.0022263157
test_loss: 0.0024110887
train_loss: 0.0022320442
test_loss: 0.0027869567
train_loss: 0.0021585277
test_loss: 0.0026733072
train_loss: 0.0021755341
test_loss: 0.002620677
train_loss: 0.0021292611
test_loss: 0.002956893
train_loss: 0.0023491695
test_loss: 0.002493265
train_loss: 0.002205516
test_loss: 0.0025690943
train_loss: 0.0020871246
test_loss: 0.0025597461
train_loss: 0.0022225464
test_loss: 0.0024705264
train_loss: 0.0021087157
test_loss: 0.0024917985
train_loss: 0.001910637
test_loss: 0.0023692343
train_loss: 0.0022728276
test_loss: 0.0024943298
train_loss: 0.002153145
test_loss: 0.0025912113
train_loss: 0.002069237
test_loss: 0.002485565
train_loss: 0.0020327829
test_loss: 0.0024642353
train_loss: 0.0021380193
test_loss: 0.0024478333
train_loss: 0.0020661093
test_loss: 0.0024387646
train_loss: 0.0020913784
test_loss: 0.002574052
train_loss: 0.0020888785
test_loss: 0.0026444057
train_loss: 0.002183731
test_loss: 0.002526885
train_loss: 0.0020423736
test_loss: 0.0024152726
train_loss: 0.0023424497
test_loss: 0.0024045443
train_loss: 0.0022015756
test_loss: 0.0026203992
train_loss: 0.0020530391
test_loss: 0.002620512
train_loss: 0.0021120876
test_loss: 0.002514839
train_loss: 0.0022148315
test_loss: 0.002663595
train_loss: 0.00217377
test_loss: 0.002552154
train_loss: 0.0019294715
test_loss: 0.0023675028
train_loss: 0.002078204
test_loss: 0.0025491558
train_loss: 0.0021358724
test_loss: 0.0026495198
train_loss: 0.0020857188
test_loss: 0.0025413935
train_loss: 0.0021015657
test_loss: 0.0025746909
train_loss: 0.0020093033
test_loss: 0.0024853938
train_loss: 0.0019593008
test_loss: 0.0024975245
train_loss: 0.0019603749
test_loss: 0.0023746656
train_loss: 0.0021214848
test_loss: 0.0024989555
train_loss: 0.0020814603
test_loss: 0.0024942704
train_loss: 0.0021174075
test_loss: 0.0024453688
train_loss: 0.002006761
test_loss: 0.0024793027
train_loss: 0.0020158493
test_loss: 0.0023555781
train_loss: 0.0020127515
test_loss: 0.002515269
train_loss: 0.0021120198
test_loss: 0.002613533
train_loss: 0.0020392903
test_loss: 0.0025853862
train_loss: 0.0020645568
test_loss: 0.002588787
train_loss: 0.0019389193
test_loss: 0.0024084188
train_loss: 0.0023286766
test_loss: 0.0025225172
train_loss: 0.0022268188
test_loss: 0.0027722085
train_loss: 0.0022187512
test_loss: 0.002613243
train_loss: 0.0019866186
test_loss: 0.0025357625
train_loss: 0.0019958806
test_loss: 0.0025278276
train_loss: 0.001957927
test_loss: 0.0025404803
train_loss: 0.001929682
test_loss: 0.00248459
train_loss: 0.0018995125
test_loss: 0.0025459926
train_loss: 0.0022181424
test_loss: 0.002563109
train_loss: 0.0023459634
test_loss: 0.002561383
train_loss: 0.0021848495
test_loss: 0.002581686
train_loss: 0.0020986034
test_loss: 0.0026413498
train_loss: 0.0019740663
test_loss: 0.0023811976
train_loss: 0.0018989904
test_loss: 0.0024592173
train_loss: 0.0020991606
test_loss: 0.002431342
train_loss: 0.0022583257
test_loss: 0.0025057432
train_loss: 0.0020144326
test_loss: 0.0024203085
train_loss: 0.0020623817
test_loss: 0.0023248713
train_loss: 0.002137053
test_loss: 0.0024395857
train_loss: 0.0019850954
test_loss: 0.0023484759
train_loss: 0.0021289447
test_loss: 0.0024359329
train_loss: 0.0020844678
test_loss: 0.0026527205
train_loss: 0.0021815295
test_loss: 0.002460468
train_loss: 0.0020362104
test_loss: 0.002514621
train_loss: 0.0020899419
test_loss: 0.002400395
train_loss: 0.0019240838
test_loss: 0.0024761488
train_loss: 0.0022847317
test_loss: 0.0026609385
train_loss: 0.0022075488
test_loss: 0.0024374162
train_loss: 0.0021094694
test_loss: 0.0024024132
train_loss: 0.0020512734
test_loss: 0.0025852907
train_loss: 0.0020664814
test_loss: 0.0025519019
train_loss: 0.0020374474
test_loss: 0.0025816448
train_loss: 0.0020086672
test_loss: 0.0025209875
train_loss: 0.0021944533
test_loss: 0.0024835472
train_loss: 0.0021567682
test_loss: 0.0026610726
train_loss: 0.0024890564
test_loss: 0.0025643294
train_loss: 0.002186002
test_loss: 0.00263525
train_loss: 0.0022357749
test_loss: 0.0025423937
train_loss: 0.0020876601
test_loss: 0.0025453558
train_loss: 0.0021458678
test_loss: 0.0024408782
train_loss: 0.0018799951
test_loss: 0.0023284492
train_loss: 0.00225136
test_loss: 0.0025731055
train_loss: 0.0022499145
test_loss: 0.0026136404
train_loss: 0.0021929436
test_loss: 0.0026377381
train_loss: 0.0020235637
test_loss: 0.0024845777
train_loss: 0.0018775912
test_loss: 0.0023513078
train_loss: 0.0020165408
test_loss: 0.0024511772
train_loss: 0.0020826808
test_loss: 0.0025997688
train_loss: 0.0023040038
test_loss: 0.002548733
train_loss: 0.002056257
test_loss: 0.00250021
train_loss: 0.00208519
test_loss: 0.0026216153
train_loss: 0.0021269778
test_loss: 0.002522495
train_loss: 0.0023368092
test_loss: 0.0024630134
train_loss: 0.0020605053
test_loss: 0.0024805574
train_loss: 0.0019158504
test_loss: 0.0023326676
train_loss: 0.0018769286
test_loss: 0.0024865102
train_loss: 0.0020698023
test_loss: 0.0025346794
train_loss: 0.0019372001
test_loss: 0.0023370883
train_loss: 0.0020616143
test_loss: 0.0024599049
train_loss: 0.0021196343
test_loss: 0.0026220756
train_loss: 0.0019580806
test_loss: 0.0024993075
train_loss: 0.00211415
test_loss: 0.002374129
train_loss: 0.002133795
test_loss: 0.002552052
train_loss: 0.0022855492
test_loss: 0.002685656
train_loss: 0.0022653572
test_loss: 0.002605955
train_loss: 0.0020963605
test_loss: 0.0024766396
train_loss: 0.0020099857
test_loss: 0.002465611
train_loss: 0.0021042386
test_loss: 0.0023771948
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.001959973
test_loss: 0.0023903893
train_loss: 0.0019510405
test_loss: 0.0024326895
train_loss: 0.0020078085
test_loss: 0.0023543113
train_loss: 0.0022098194
test_loss: 0.0024104032
train_loss: 0.002104506
test_loss: 0.0024408796
train_loss: 0.0021819668
test_loss: 0.0024834261
train_loss: 0.0018706316
test_loss: 0.002377528
train_loss: 0.0019641295
test_loss: 0.0023398448
train_loss: 0.0019909488
test_loss: 0.0024028954
train_loss: 0.0021101274
test_loss: 0.002405006
train_loss: 0.002115398
test_loss: 0.002477792
train_loss: 0.001996885
test_loss: 0.0025204138
train_loss: 0.0020358425
test_loss: 0.0024085096
train_loss: 0.002095689
test_loss: 0.0024890658
train_loss: 0.00200035
test_loss: 0.0025852134
train_loss: 0.0019498472
test_loss: 0.002558073
train_loss: 0.0019891532
test_loss: 0.0025158017
train_loss: 0.001980702
test_loss: 0.002510632
train_loss: 0.0020626858
test_loss: 0.0024803346
train_loss: 0.002142469
test_loss: 0.0025148625
train_loss: 0.0020588215
test_loss: 0.002377915
train_loss: 0.002185566
test_loss: 0.0026934273
train_loss: 0.001970241
test_loss: 0.002438506
train_loss: 0.0021371678
test_loss: 0.0025195018
train_loss: 0.0020008506
test_loss: 0.0027627056
train_loss: 0.0021741667
test_loss: 0.0023940704
train_loss: 0.001992835
test_loss: 0.0024271833
train_loss: 0.0019019638
test_loss: 0.0024041748
train_loss: 0.0020547088
test_loss: 0.002313509
train_loss: 0.0022690254
test_loss: 0.002583445
train_loss: 0.0022129393
test_loss: 0.002579153
train_loss: 0.0020316301
test_loss: 0.0024014334
train_loss: 0.0021444045
test_loss: 0.0024006693
train_loss: 0.0019644923
test_loss: 0.0024379934
train_loss: 0.0019219707
test_loss: 0.0024701715
train_loss: 0.0021546339
test_loss: 0.0024413038
train_loss: 0.0022018398
test_loss: 0.002570256
train_loss: 0.0020533367
test_loss: 0.0025673485
train_loss: 0.002070683
test_loss: 0.0023412535
train_loss: 0.0019220855
test_loss: 0.0024382365
train_loss: 0.0020929896
test_loss: 0.002521958
train_loss: 0.0023125221
test_loss: 0.0026149738
train_loss: 0.0020856736
test_loss: 0.0024758035
train_loss: 0.0020048197
test_loss: 0.0024721825
train_loss: 0.0023092846
test_loss: 0.0029134434
train_loss: 0.0022136143
test_loss: 0.002799344
train_loss: 0.0023070434
test_loss: 0.002559927
train_loss: 0.0020394404
test_loss: 0.0024769648
train_loss: 0.0020049869
test_loss: 0.0025467814
train_loss: 0.002148967
test_loss: 0.0026843906
train_loss: 0.0021531433
test_loss: 0.0023527222
train_loss: 0.0019489941
test_loss: 0.0024060537
train_loss: 0.0019223685
test_loss: 0.0023594995
train_loss: 0.0019574268
test_loss: 0.002381992
train_loss: 0.0018621497
test_loss: 0.0023167664
train_loss: 0.0019114487
test_loss: 0.0024347873
train_loss: 0.0022479591
test_loss: 0.002461319
train_loss: 0.0022784811
test_loss: 0.0024138852
train_loss: 0.0021053904
test_loss: 0.0025113518
train_loss: 0.002171628
test_loss: 0.002559834
train_loss: 0.0019802914
test_loss: 0.0024007552
train_loss: 0.0020539383
test_loss: 0.0024059457
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee42c6158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee43bd510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee43bd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee42ebd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee430e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee4250950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee424a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee41f2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee41f2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee41b5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee41b59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee418d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee418d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee4149ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee40e88c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee410eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee4107730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee4107ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee40549d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee4054730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee401f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee3fd6d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee3fd67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee3fbe378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ee3fbe620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ea72cf400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ea7285840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ea72ac840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ea7244620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ea725f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ea72049d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ea7204d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ea71c2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9ea71e8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e801139d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e800bc510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.80793801e-06
Iter: 2 loss: 5.73675925e-06
Iter: 3 loss: 5.68292671e-06
Iter: 4 loss: 5.20726462e-06
Iter: 5 loss: 5.87430759e-06
Iter: 6 loss: 4.97282099e-06
Iter: 7 loss: 4.71098065e-06
Iter: 8 loss: 5.41313329e-06
Iter: 9 loss: 4.62386379e-06
Iter: 10 loss: 4.32773595e-06
Iter: 11 loss: 5.34993433e-06
Iter: 12 loss: 4.24907284e-06
Iter: 13 loss: 4.0412915e-06
Iter: 14 loss: 4.58336899e-06
Iter: 15 loss: 3.97098393e-06
Iter: 16 loss: 3.81212203e-06
Iter: 17 loss: 5.46647561e-06
Iter: 18 loss: 3.80786e-06
Iter: 19 loss: 3.73723674e-06
Iter: 20 loss: 3.5873461e-06
Iter: 21 loss: 6.01125066e-06
Iter: 22 loss: 3.58254192e-06
Iter: 23 loss: 3.37610982e-06
Iter: 24 loss: 4.34690173e-06
Iter: 25 loss: 3.33819776e-06
Iter: 26 loss: 3.18998173e-06
Iter: 27 loss: 5.33492312e-06
Iter: 28 loss: 3.18976709e-06
Iter: 29 loss: 3.10433552e-06
Iter: 30 loss: 2.91043352e-06
Iter: 31 loss: 5.49014476e-06
Iter: 32 loss: 2.89854097e-06
Iter: 33 loss: 2.73741216e-06
Iter: 34 loss: 3.91181402e-06
Iter: 35 loss: 2.72381294e-06
Iter: 36 loss: 2.59728722e-06
Iter: 37 loss: 3.03591673e-06
Iter: 38 loss: 2.56400608e-06
Iter: 39 loss: 2.45689034e-06
Iter: 40 loss: 2.57431748e-06
Iter: 41 loss: 2.39843598e-06
Iter: 42 loss: 2.2951474e-06
Iter: 43 loss: 2.97412407e-06
Iter: 44 loss: 2.2842039e-06
Iter: 45 loss: 2.23737015e-06
Iter: 46 loss: 2.22837571e-06
Iter: 47 loss: 2.18993682e-06
Iter: 48 loss: 2.11664883e-06
Iter: 49 loss: 3.67503617e-06
Iter: 50 loss: 2.11628753e-06
Iter: 51 loss: 2.04769776e-06
Iter: 52 loss: 2.73444039e-06
Iter: 53 loss: 2.04545722e-06
Iter: 54 loss: 1.98325188e-06
Iter: 55 loss: 2.16172748e-06
Iter: 56 loss: 1.96365227e-06
Iter: 57 loss: 1.93285359e-06
Iter: 58 loss: 2.0313646e-06
Iter: 59 loss: 1.92406333e-06
Iter: 60 loss: 1.88255876e-06
Iter: 61 loss: 1.8835035e-06
Iter: 62 loss: 1.84963596e-06
Iter: 63 loss: 1.80913935e-06
Iter: 64 loss: 1.88781485e-06
Iter: 65 loss: 1.79236815e-06
Iter: 66 loss: 1.75928517e-06
Iter: 67 loss: 2.0030875e-06
Iter: 68 loss: 1.75660784e-06
Iter: 69 loss: 1.72124101e-06
Iter: 70 loss: 1.79026438e-06
Iter: 71 loss: 1.70669773e-06
Iter: 72 loss: 1.67253984e-06
Iter: 73 loss: 1.60986758e-06
Iter: 74 loss: 3.06770471e-06
Iter: 75 loss: 1.60983313e-06
Iter: 76 loss: 1.54174745e-06
Iter: 77 loss: 1.79244694e-06
Iter: 78 loss: 1.52502696e-06
Iter: 79 loss: 1.44452383e-06
Iter: 80 loss: 1.73479089e-06
Iter: 81 loss: 1.42419685e-06
Iter: 82 loss: 1.38353653e-06
Iter: 83 loss: 1.67517169e-06
Iter: 84 loss: 1.38001815e-06
Iter: 85 loss: 1.36483675e-06
Iter: 86 loss: 1.36179574e-06
Iter: 87 loss: 1.34737752e-06
Iter: 88 loss: 1.31541969e-06
Iter: 89 loss: 1.77686775e-06
Iter: 90 loss: 1.31387355e-06
Iter: 91 loss: 1.28918566e-06
Iter: 92 loss: 1.62294873e-06
Iter: 93 loss: 1.28912927e-06
Iter: 94 loss: 1.27315298e-06
Iter: 95 loss: 1.41759074e-06
Iter: 96 loss: 1.27245141e-06
Iter: 97 loss: 1.26261511e-06
Iter: 98 loss: 1.23853613e-06
Iter: 99 loss: 1.48096444e-06
Iter: 100 loss: 1.23556401e-06
Iter: 101 loss: 1.21914718e-06
Iter: 102 loss: 1.21656421e-06
Iter: 103 loss: 1.20492177e-06
Iter: 104 loss: 1.17142167e-06
Iter: 105 loss: 1.32074626e-06
Iter: 106 loss: 1.15890612e-06
Iter: 107 loss: 1.14620798e-06
Iter: 108 loss: 1.13946385e-06
Iter: 109 loss: 1.11904046e-06
Iter: 110 loss: 1.12414193e-06
Iter: 111 loss: 1.10413248e-06
Iter: 112 loss: 1.08907057e-06
Iter: 113 loss: 1.12942882e-06
Iter: 114 loss: 1.08418249e-06
Iter: 115 loss: 1.06848222e-06
Iter: 116 loss: 1.04724052e-06
Iter: 117 loss: 1.04611013e-06
Iter: 118 loss: 1.02613103e-06
Iter: 119 loss: 1.23834911e-06
Iter: 120 loss: 1.02570027e-06
Iter: 121 loss: 1.00917055e-06
Iter: 122 loss: 1.03229331e-06
Iter: 123 loss: 1.00099669e-06
Iter: 124 loss: 9.89262844e-07
Iter: 125 loss: 9.87471822e-07
Iter: 126 loss: 9.77791e-07
Iter: 127 loss: 9.67751816e-07
Iter: 128 loss: 9.65915e-07
Iter: 129 loss: 9.54277084e-07
Iter: 130 loss: 9.69521579e-07
Iter: 131 loss: 9.4830034e-07
Iter: 132 loss: 9.30585429e-07
Iter: 133 loss: 1.04425874e-06
Iter: 134 loss: 9.28602958e-07
Iter: 135 loss: 9.21431649e-07
Iter: 136 loss: 9.1760586e-07
Iter: 137 loss: 9.14364307e-07
Iter: 138 loss: 9.03747605e-07
Iter: 139 loss: 9.44680721e-07
Iter: 140 loss: 9.01201759e-07
Iter: 141 loss: 8.92167577e-07
Iter: 142 loss: 9.82191523e-07
Iter: 143 loss: 8.91891432e-07
Iter: 144 loss: 8.85849317e-07
Iter: 145 loss: 8.73575743e-07
Iter: 146 loss: 1.09712983e-06
Iter: 147 loss: 8.73413342e-07
Iter: 148 loss: 8.64016556e-07
Iter: 149 loss: 8.64021786e-07
Iter: 150 loss: 8.5515785e-07
Iter: 151 loss: 8.71903524e-07
Iter: 152 loss: 8.51380605e-07
Iter: 153 loss: 8.40759071e-07
Iter: 154 loss: 8.31123e-07
Iter: 155 loss: 8.28409156e-07
Iter: 156 loss: 8.15942371e-07
Iter: 157 loss: 8.64777633e-07
Iter: 158 loss: 8.13015106e-07
Iter: 159 loss: 8.02798752e-07
Iter: 160 loss: 8.38274332e-07
Iter: 161 loss: 8.00094199e-07
Iter: 162 loss: 7.92812102e-07
Iter: 163 loss: 8.86336e-07
Iter: 164 loss: 7.92733886e-07
Iter: 165 loss: 7.86096734e-07
Iter: 166 loss: 8.23444452e-07
Iter: 167 loss: 7.85106749e-07
Iter: 168 loss: 7.82034249e-07
Iter: 169 loss: 7.74258694e-07
Iter: 170 loss: 8.45656928e-07
Iter: 171 loss: 7.73168949e-07
Iter: 172 loss: 7.73630063e-07
Iter: 173 loss: 7.69251642e-07
Iter: 174 loss: 7.6609615e-07
Iter: 175 loss: 7.57478233e-07
Iter: 176 loss: 8.09669928e-07
Iter: 177 loss: 7.55132191e-07
Iter: 178 loss: 7.46211754e-07
Iter: 179 loss: 8.01691385e-07
Iter: 180 loss: 7.45119905e-07
Iter: 181 loss: 7.39075745e-07
Iter: 182 loss: 7.39067559e-07
Iter: 183 loss: 7.35053732e-07
Iter: 184 loss: 7.34195851e-07
Iter: 185 loss: 7.31515627e-07
Iter: 186 loss: 7.26172232e-07
Iter: 187 loss: 7.2686862e-07
Iter: 188 loss: 7.22083655e-07
Iter: 189 loss: 7.17347802e-07
Iter: 190 loss: 7.17051591e-07
Iter: 191 loss: 7.13683448e-07
Iter: 192 loss: 7.06180799e-07
Iter: 193 loss: 8.13949157e-07
Iter: 194 loss: 7.05823766e-07
Iter: 195 loss: 6.97679184e-07
Iter: 196 loss: 7.19969535e-07
Iter: 197 loss: 6.95001859e-07
Iter: 198 loss: 6.86178055e-07
Iter: 199 loss: 7.31221633e-07
Iter: 200 loss: 6.8473048e-07
Iter: 201 loss: 6.81853805e-07
Iter: 202 loss: 6.80879623e-07
Iter: 203 loss: 6.78070364e-07
Iter: 204 loss: 6.72283591e-07
Iter: 205 loss: 7.71811756e-07
Iter: 206 loss: 6.72164845e-07
Iter: 207 loss: 6.67654717e-07
Iter: 208 loss: 6.88566104e-07
Iter: 209 loss: 6.66830033e-07
Iter: 210 loss: 6.63206606e-07
Iter: 211 loss: 7.06197966e-07
Iter: 212 loss: 6.6312839e-07
Iter: 213 loss: 6.59640534e-07
Iter: 214 loss: 6.55831514e-07
Iter: 215 loss: 6.55210442e-07
Iter: 216 loss: 6.51714117e-07
Iter: 217 loss: 6.52383449e-07
Iter: 218 loss: 6.49063622e-07
Iter: 219 loss: 6.45969749e-07
Iter: 220 loss: 6.45851117e-07
Iter: 221 loss: 6.42732914e-07
Iter: 222 loss: 6.38947483e-07
Iter: 223 loss: 6.38589484e-07
Iter: 224 loss: 6.32073466e-07
Iter: 225 loss: 6.39139785e-07
Iter: 226 loss: 6.28522855e-07
Iter: 227 loss: 6.26023166e-07
Iter: 228 loss: 6.25569726e-07
Iter: 229 loss: 6.22794801e-07
Iter: 230 loss: 6.16699936e-07
Iter: 231 loss: 7.11662437e-07
Iter: 232 loss: 6.16490581e-07
Iter: 233 loss: 6.10598534e-07
Iter: 234 loss: 6.35094125e-07
Iter: 235 loss: 6.09306198e-07
Iter: 236 loss: 6.06095284e-07
Iter: 237 loss: 6.33832315e-07
Iter: 238 loss: 6.05904461e-07
Iter: 239 loss: 6.03782041e-07
Iter: 240 loss: 6.36467348e-07
Iter: 241 loss: 6.03779e-07
Iter: 242 loss: 6.01654051e-07
Iter: 243 loss: 5.97060193e-07
Iter: 244 loss: 6.6775965e-07
Iter: 245 loss: 5.96906659e-07
Iter: 246 loss: 5.93139816e-07
Iter: 247 loss: 6.01936108e-07
Iter: 248 loss: 5.91768753e-07
Iter: 249 loss: 5.88272314e-07
Iter: 250 loss: 5.88287548e-07
Iter: 251 loss: 5.85974362e-07
Iter: 252 loss: 5.82449e-07
Iter: 253 loss: 5.82364805e-07
Iter: 254 loss: 5.77999288e-07
Iter: 255 loss: 5.87142608e-07
Iter: 256 loss: 5.76279547e-07
Iter: 257 loss: 5.72947101e-07
Iter: 258 loss: 5.87889133e-07
Iter: 259 loss: 5.72351667e-07
Iter: 260 loss: 5.6873e-07
Iter: 261 loss: 5.92784e-07
Iter: 262 loss: 5.68340397e-07
Iter: 263 loss: 5.6568166e-07
Iter: 264 loss: 5.62902926e-07
Iter: 265 loss: 5.62381217e-07
Iter: 266 loss: 5.59630905e-07
Iter: 267 loss: 5.95028041e-07
Iter: 268 loss: 5.59616751e-07
Iter: 269 loss: 5.57530541e-07
Iter: 270 loss: 5.69422127e-07
Iter: 271 loss: 5.57233079e-07
Iter: 272 loss: 5.55230145e-07
Iter: 273 loss: 5.50104915e-07
Iter: 274 loss: 5.91067419e-07
Iter: 275 loss: 5.49169613e-07
Iter: 276 loss: 5.465821e-07
Iter: 277 loss: 5.4649729e-07
Iter: 278 loss: 5.44463944e-07
Iter: 279 loss: 5.65364587e-07
Iter: 280 loss: 5.44448085e-07
Iter: 281 loss: 5.42292071e-07
Iter: 282 loss: 5.40693236e-07
Iter: 283 loss: 5.39995654e-07
Iter: 284 loss: 5.37935648e-07
Iter: 285 loss: 5.36841753e-07
Iter: 286 loss: 5.35918105e-07
Iter: 287 loss: 5.34633784e-07
Iter: 288 loss: 5.34198534e-07
Iter: 289 loss: 5.32716683e-07
Iter: 290 loss: 5.31364265e-07
Iter: 291 loss: 5.30984039e-07
Iter: 292 loss: 5.28727e-07
Iter: 293 loss: 5.2650762e-07
Iter: 294 loss: 5.2604662e-07
Iter: 295 loss: 5.21512106e-07
Iter: 296 loss: 5.27558541e-07
Iter: 297 loss: 5.19273499e-07
Iter: 298 loss: 5.1608987e-07
Iter: 299 loss: 5.1608697e-07
Iter: 300 loss: 5.13594443e-07
Iter: 301 loss: 5.27900852e-07
Iter: 302 loss: 5.13265491e-07
Iter: 303 loss: 5.11151711e-07
Iter: 304 loss: 5.113356e-07
Iter: 305 loss: 5.09524739e-07
Iter: 306 loss: 5.07435857e-07
Iter: 307 loss: 5.12117936e-07
Iter: 308 loss: 5.06644596e-07
Iter: 309 loss: 5.04203513e-07
Iter: 310 loss: 5.24380084e-07
Iter: 311 loss: 5.04037473e-07
Iter: 312 loss: 5.02578473e-07
Iter: 313 loss: 5.01010049e-07
Iter: 314 loss: 5.00821329e-07
Iter: 315 loss: 4.98236147e-07
Iter: 316 loss: 5.07862183e-07
Iter: 317 loss: 4.9766868e-07
Iter: 318 loss: 4.96038751e-07
Iter: 319 loss: 4.95950928e-07
Iter: 320 loss: 4.9514972e-07
Iter: 321 loss: 4.93127686e-07
Iter: 322 loss: 5.13579096e-07
Iter: 323 loss: 4.92905087e-07
Iter: 324 loss: 4.9076715e-07
Iter: 325 loss: 4.94664505e-07
Iter: 326 loss: 4.89858508e-07
Iter: 327 loss: 4.88010073e-07
Iter: 328 loss: 4.87903549e-07
Iter: 329 loss: 4.8688355e-07
Iter: 330 loss: 4.84644e-07
Iter: 331 loss: 5.16787964e-07
Iter: 332 loss: 4.84539896e-07
Iter: 333 loss: 4.82027303e-07
Iter: 334 loss: 4.92383549e-07
Iter: 335 loss: 4.81450684e-07
Iter: 336 loss: 4.79431549e-07
Iter: 337 loss: 4.81932716e-07
Iter: 338 loss: 4.78428433e-07
Iter: 339 loss: 4.75771031e-07
Iter: 340 loss: 5.05071966e-07
Iter: 341 loss: 4.7575395e-07
Iter: 342 loss: 4.74160373e-07
Iter: 343 loss: 4.74078888e-07
Iter: 344 loss: 4.72876479e-07
Iter: 345 loss: 4.71161485e-07
Iter: 346 loss: 4.82811345e-07
Iter: 347 loss: 4.7097069e-07
Iter: 348 loss: 4.69176456e-07
Iter: 349 loss: 4.71011617e-07
Iter: 350 loss: 4.68188318e-07
Iter: 351 loss: 4.66412359e-07
Iter: 352 loss: 4.67097578e-07
Iter: 353 loss: 4.65140175e-07
Iter: 354 loss: 4.65148361e-07
Iter: 355 loss: 4.64410107e-07
Iter: 356 loss: 4.63794805e-07
Iter: 357 loss: 4.62324294e-07
Iter: 358 loss: 4.78994707e-07
Iter: 359 loss: 4.6215365e-07
Iter: 360 loss: 4.60418676e-07
Iter: 361 loss: 4.65283449e-07
Iter: 362 loss: 4.59857631e-07
Iter: 363 loss: 4.58930543e-07
Iter: 364 loss: 4.58907806e-07
Iter: 365 loss: 4.57882294e-07
Iter: 366 loss: 4.56387824e-07
Iter: 367 loss: 4.56344537e-07
Iter: 368 loss: 4.54432922e-07
Iter: 369 loss: 4.52974461e-07
Iter: 370 loss: 4.52384143e-07
Iter: 371 loss: 4.49882805e-07
Iter: 372 loss: 4.76266962e-07
Iter: 373 loss: 4.49836506e-07
Iter: 374 loss: 4.48336692e-07
Iter: 375 loss: 4.50728038e-07
Iter: 376 loss: 4.47669606e-07
Iter: 377 loss: 4.45471102e-07
Iter: 378 loss: 4.59763982e-07
Iter: 379 loss: 4.45228068e-07
Iter: 380 loss: 4.44205284e-07
Iter: 381 loss: 4.4362244e-07
Iter: 382 loss: 4.43176e-07
Iter: 383 loss: 4.4201073e-07
Iter: 384 loss: 4.59756933e-07
Iter: 385 loss: 4.42012549e-07
Iter: 386 loss: 4.41040783e-07
Iter: 387 loss: 4.41432292e-07
Iter: 388 loss: 4.40344138e-07
Iter: 389 loss: 4.39233389e-07
Iter: 390 loss: 4.38058578e-07
Iter: 391 loss: 4.37853316e-07
Iter: 392 loss: 4.3728781e-07
Iter: 393 loss: 4.3697176e-07
Iter: 394 loss: 4.36e-07
Iter: 395 loss: 4.34593488e-07
Iter: 396 loss: 4.34586497e-07
Iter: 397 loss: 4.32904898e-07
Iter: 398 loss: 4.33112433e-07
Iter: 399 loss: 4.31652722e-07
Iter: 400 loss: 4.30089187e-07
Iter: 401 loss: 4.30027058e-07
Iter: 402 loss: 4.28882572e-07
Iter: 403 loss: 4.28516e-07
Iter: 404 loss: 4.27813916e-07
Iter: 405 loss: 4.26517914e-07
Iter: 406 loss: 4.25584801e-07
Iter: 407 loss: 4.25105753e-07
Iter: 408 loss: 4.23861138e-07
Iter: 409 loss: 4.388055e-07
Iter: 410 loss: 4.23824474e-07
Iter: 411 loss: 4.22676834e-07
Iter: 412 loss: 4.23473409e-07
Iter: 413 loss: 4.21965353e-07
Iter: 414 loss: 4.20113111e-07
Iter: 415 loss: 4.30350184e-07
Iter: 416 loss: 4.19833782e-07
Iter: 417 loss: 4.18870627e-07
Iter: 418 loss: 4.17930522e-07
Iter: 419 loss: 4.17733759e-07
Iter: 420 loss: 4.1669918e-07
Iter: 421 loss: 4.16660498e-07
Iter: 422 loss: 4.15656331e-07
Iter: 423 loss: 4.14341685e-07
Iter: 424 loss: 4.1427748e-07
Iter: 425 loss: 4.1344174e-07
Iter: 426 loss: 4.13400812e-07
Iter: 427 loss: 4.12649229e-07
Iter: 428 loss: 4.14148332e-07
Iter: 429 loss: 4.12354808e-07
Iter: 430 loss: 4.11846202e-07
Iter: 431 loss: 4.10944807e-07
Iter: 432 loss: 4.10935172e-07
Iter: 433 loss: 4.10095168e-07
Iter: 434 loss: 4.19502101e-07
Iter: 435 loss: 4.10093122e-07
Iter: 436 loss: 4.09044048e-07
Iter: 437 loss: 4.09337304e-07
Iter: 438 loss: 4.08308608e-07
Iter: 439 loss: 4.07302963e-07
Iter: 440 loss: 4.07703624e-07
Iter: 441 loss: 4.06632552e-07
Iter: 442 loss: 4.05243327e-07
Iter: 443 loss: 4.04820668e-07
Iter: 444 loss: 4.03984103e-07
Iter: 445 loss: 4.02727096e-07
Iter: 446 loss: 4.18506033e-07
Iter: 447 loss: 4.02705581e-07
Iter: 448 loss: 4.0159e-07
Iter: 449 loss: 4.09902242e-07
Iter: 450 loss: 4.01517383e-07
Iter: 451 loss: 4.00673201e-07
Iter: 452 loss: 4.00221893e-07
Iter: 453 loss: 3.99827456e-07
Iter: 454 loss: 3.98696812e-07
Iter: 455 loss: 4.02631059e-07
Iter: 456 loss: 3.98378802e-07
Iter: 457 loss: 3.97545023e-07
Iter: 458 loss: 4.08603853e-07
Iter: 459 loss: 3.97553038e-07
Iter: 460 loss: 3.96860571e-07
Iter: 461 loss: 3.95536631e-07
Iter: 462 loss: 4.25281826e-07
Iter: 463 loss: 3.95522591e-07
Iter: 464 loss: 3.95232917e-07
Iter: 465 loss: 3.94925e-07
Iter: 466 loss: 3.94362132e-07
Iter: 467 loss: 3.934814e-07
Iter: 468 loss: 3.93485493e-07
Iter: 469 loss: 3.92332424e-07
Iter: 470 loss: 3.91995854e-07
Iter: 471 loss: 3.91358185e-07
Iter: 472 loss: 3.90749591e-07
Iter: 473 loss: 3.90601485e-07
Iter: 474 loss: 3.89817245e-07
Iter: 475 loss: 3.88916732e-07
Iter: 476 loss: 3.88799435e-07
Iter: 477 loss: 3.8779757e-07
Iter: 478 loss: 3.87900542e-07
Iter: 479 loss: 3.87037318e-07
Iter: 480 loss: 3.85872283e-07
Iter: 481 loss: 3.9397429e-07
Iter: 482 loss: 3.85764565e-07
Iter: 483 loss: 3.85074543e-07
Iter: 484 loss: 3.89317876e-07
Iter: 485 loss: 3.84984531e-07
Iter: 486 loss: 3.84254889e-07
Iter: 487 loss: 3.86813554e-07
Iter: 488 loss: 3.8404508e-07
Iter: 489 loss: 3.8338672e-07
Iter: 490 loss: 3.82335031e-07
Iter: 491 loss: 3.8232443e-07
Iter: 492 loss: 3.81361474e-07
Iter: 493 loss: 3.81332882e-07
Iter: 494 loss: 3.80597641e-07
Iter: 495 loss: 3.81900122e-07
Iter: 496 loss: 3.80270137e-07
Iter: 497 loss: 3.79507071e-07
Iter: 498 loss: 3.80437598e-07
Iter: 499 loss: 3.79120308e-07
Iter: 500 loss: 3.78324245e-07
Iter: 501 loss: 3.8891352e-07
Iter: 502 loss: 3.78327798e-07
Iter: 503 loss: 3.7783289e-07
Iter: 504 loss: 3.76658591e-07
Iter: 505 loss: 3.87542229e-07
Iter: 506 loss: 3.76457365e-07
Iter: 507 loss: 3.75537809e-07
Iter: 508 loss: 3.88388202e-07
Iter: 509 loss: 3.75537297e-07
Iter: 510 loss: 3.74933506e-07
Iter: 511 loss: 3.82489077e-07
Iter: 512 loss: 3.74921541e-07
Iter: 513 loss: 3.74385309e-07
Iter: 514 loss: 3.73398336e-07
Iter: 515 loss: 3.94999802e-07
Iter: 516 loss: 3.7343392e-07
Iter: 517 loss: 3.72416537e-07
Iter: 518 loss: 3.72516354e-07
Iter: 519 loss: 3.71634371e-07
Iter: 520 loss: 3.70453449e-07
Iter: 521 loss: 3.79468304e-07
Iter: 522 loss: 3.70382793e-07
Iter: 523 loss: 3.69392836e-07
Iter: 524 loss: 3.76464214e-07
Iter: 525 loss: 3.69315785e-07
Iter: 526 loss: 3.68302722e-07
Iter: 527 loss: 3.69495581e-07
Iter: 528 loss: 3.67760833e-07
Iter: 529 loss: 3.6717006e-07
Iter: 530 loss: 3.67517657e-07
Iter: 531 loss: 3.66825873e-07
Iter: 532 loss: 3.66165665e-07
Iter: 533 loss: 3.75297105e-07
Iter: 534 loss: 3.66176437e-07
Iter: 535 loss: 3.65661947e-07
Iter: 536 loss: 3.65509607e-07
Iter: 537 loss: 3.65216039e-07
Iter: 538 loss: 3.64735712e-07
Iter: 539 loss: 3.72225855e-07
Iter: 540 loss: 3.64765583e-07
Iter: 541 loss: 3.64312598e-07
Iter: 542 loss: 3.64473806e-07
Iter: 543 loss: 3.64004194e-07
Iter: 544 loss: 3.6352688e-07
Iter: 545 loss: 3.62301819e-07
Iter: 546 loss: 3.76330604e-07
Iter: 547 loss: 3.62197284e-07
Iter: 548 loss: 3.61073063e-07
Iter: 549 loss: 3.75073e-07
Iter: 550 loss: 3.61055783e-07
Iter: 551 loss: 3.60186959e-07
Iter: 552 loss: 3.60431358e-07
Iter: 553 loss: 3.59550882e-07
Iter: 554 loss: 3.58992622e-07
Iter: 555 loss: 3.58905851e-07
Iter: 556 loss: 3.58229897e-07
Iter: 557 loss: 3.57225758e-07
Iter: 558 loss: 3.57161241e-07
Iter: 559 loss: 3.56609547e-07
Iter: 560 loss: 3.62561963e-07
Iter: 561 loss: 3.56569728e-07
Iter: 562 loss: 3.56001777e-07
Iter: 563 loss: 3.56752025e-07
Iter: 564 loss: 3.55726627e-07
Iter: 565 loss: 3.55090407e-07
Iter: 566 loss: 3.5462449e-07
Iter: 567 loss: 3.54423662e-07
Iter: 568 loss: 3.53767206e-07
Iter: 569 loss: 3.53743417e-07
Iter: 570 loss: 3.53236686e-07
Iter: 571 loss: 3.5390832e-07
Iter: 572 loss: 3.52950735e-07
Iter: 573 loss: 3.5249235e-07
Iter: 574 loss: 3.56694358e-07
Iter: 575 loss: 3.52455089e-07
Iter: 576 loss: 3.52013842e-07
Iter: 577 loss: 3.51293238e-07
Iter: 578 loss: 3.51281358e-07
Iter: 579 loss: 3.50666653e-07
Iter: 580 loss: 3.5028026e-07
Iter: 581 loss: 3.50044189e-07
Iter: 582 loss: 3.48865228e-07
Iter: 583 loss: 3.5403724e-07
Iter: 584 loss: 3.48638366e-07
Iter: 585 loss: 3.47919666e-07
Iter: 586 loss: 3.54019221e-07
Iter: 587 loss: 3.47903239e-07
Iter: 588 loss: 3.47358366e-07
Iter: 589 loss: 3.55057693e-07
Iter: 590 loss: 3.47360469e-07
Iter: 591 loss: 3.46986269e-07
Iter: 592 loss: 3.46256684e-07
Iter: 593 loss: 3.59099772e-07
Iter: 594 loss: 3.46226358e-07
Iter: 595 loss: 3.45528235e-07
Iter: 596 loss: 3.5337905e-07
Iter: 597 loss: 3.45518231e-07
Iter: 598 loss: 3.45000899e-07
Iter: 599 loss: 3.47541402e-07
Iter: 600 loss: 3.44921261e-07
Iter: 601 loss: 3.44494879e-07
Iter: 602 loss: 3.43805738e-07
Iter: 603 loss: 3.43815941e-07
Iter: 604 loss: 3.43473e-07
Iter: 605 loss: 3.43415934e-07
Iter: 606 loss: 3.4305782e-07
Iter: 607 loss: 3.4291503e-07
Iter: 608 loss: 3.42721194e-07
Iter: 609 loss: 3.42227821e-07
Iter: 610 loss: 3.46473428e-07
Iter: 611 loss: 3.42213184e-07
Iter: 612 loss: 3.41833072e-07
Iter: 613 loss: 3.41291809e-07
Iter: 614 loss: 3.41235733e-07
Iter: 615 loss: 3.40442398e-07
Iter: 616 loss: 3.40407297e-07
Iter: 617 loss: 3.39797737e-07
Iter: 618 loss: 3.3895293e-07
Iter: 619 loss: 3.39670919e-07
Iter: 620 loss: 3.38463963e-07
Iter: 621 loss: 3.37465764e-07
Iter: 622 loss: 3.45136868e-07
Iter: 623 loss: 3.37409233e-07
Iter: 624 loss: 3.37047197e-07
Iter: 625 loss: 3.36957044e-07
Iter: 626 loss: 3.36630649e-07
Iter: 627 loss: 3.36302719e-07
Iter: 628 loss: 3.3624093e-07
Iter: 629 loss: 3.35790247e-07
Iter: 630 loss: 3.35288547e-07
Iter: 631 loss: 3.35206778e-07
Iter: 632 loss: 3.34825074e-07
Iter: 633 loss: 3.34740065e-07
Iter: 634 loss: 3.34299727e-07
Iter: 635 loss: 3.33981319e-07
Iter: 636 loss: 3.33811357e-07
Iter: 637 loss: 3.33150638e-07
Iter: 638 loss: 3.34120841e-07
Iter: 639 loss: 3.32795821e-07
Iter: 640 loss: 3.32381461e-07
Iter: 641 loss: 3.32361651e-07
Iter: 642 loss: 3.31932057e-07
Iter: 643 loss: 3.31213585e-07
Iter: 644 loss: 3.31206195e-07
Iter: 645 loss: 3.30610106e-07
Iter: 646 loss: 3.39568e-07
Iter: 647 loss: 3.30617524e-07
Iter: 648 loss: 3.30202909e-07
Iter: 649 loss: 3.30278738e-07
Iter: 650 loss: 3.29911245e-07
Iter: 651 loss: 3.29448142e-07
Iter: 652 loss: 3.29735627e-07
Iter: 653 loss: 3.29165317e-07
Iter: 654 loss: 3.2860774e-07
Iter: 655 loss: 3.28445481e-07
Iter: 656 loss: 3.2812369e-07
Iter: 657 loss: 3.27335101e-07
Iter: 658 loss: 3.32980022e-07
Iter: 659 loss: 3.27275416e-07
Iter: 660 loss: 3.26783152e-07
Iter: 661 loss: 3.26770419e-07
Iter: 662 loss: 3.26438567e-07
Iter: 663 loss: 3.25823294e-07
Iter: 664 loss: 3.40875772e-07
Iter: 665 loss: 3.25825397e-07
Iter: 666 loss: 3.25095527e-07
Iter: 667 loss: 3.24995028e-07
Iter: 668 loss: 3.24474172e-07
Iter: 669 loss: 3.23590484e-07
Iter: 670 loss: 3.28158137e-07
Iter: 671 loss: 3.23424075e-07
Iter: 672 loss: 3.22874655e-07
Iter: 673 loss: 3.22830459e-07
Iter: 674 loss: 3.2252413e-07
Iter: 675 loss: 3.22506196e-07
Iter: 676 loss: 3.22294113e-07
Iter: 677 loss: 3.2190394e-07
Iter: 678 loss: 3.25931524e-07
Iter: 679 loss: 3.21917781e-07
Iter: 680 loss: 3.21608297e-07
Iter: 681 loss: 3.21121775e-07
Iter: 682 loss: 3.21114015e-07
Iter: 683 loss: 3.2058594e-07
Iter: 684 loss: 3.2139954e-07
Iter: 685 loss: 3.20317156e-07
Iter: 686 loss: 3.19948469e-07
Iter: 687 loss: 3.19936476e-07
Iter: 688 loss: 3.19614145e-07
Iter: 689 loss: 3.19059552e-07
Iter: 690 loss: 3.19072285e-07
Iter: 691 loss: 3.18250557e-07
Iter: 692 loss: 3.19119891e-07
Iter: 693 loss: 3.17801209e-07
Iter: 694 loss: 3.17174255e-07
Iter: 695 loss: 3.21902036e-07
Iter: 696 loss: 3.17122442e-07
Iter: 697 loss: 3.16532464e-07
Iter: 698 loss: 3.2117407e-07
Iter: 699 loss: 3.16484318e-07
Iter: 700 loss: 3.16151727e-07
Iter: 701 loss: 3.15711e-07
Iter: 702 loss: 3.15673816e-07
Iter: 703 loss: 3.15195848e-07
Iter: 704 loss: 3.16868096e-07
Iter: 705 loss: 3.1505391e-07
Iter: 706 loss: 3.14595411e-07
Iter: 707 loss: 3.14363831e-07
Iter: 708 loss: 3.14167181e-07
Iter: 709 loss: 3.13808783e-07
Iter: 710 loss: 3.1373429e-07
Iter: 711 loss: 3.13361625e-07
Iter: 712 loss: 3.1373844e-07
Iter: 713 loss: 3.13154828e-07
Iter: 714 loss: 3.12753258e-07
Iter: 715 loss: 3.13119727e-07
Iter: 716 loss: 3.12516363e-07
Iter: 717 loss: 3.11932865e-07
Iter: 718 loss: 3.16326407e-07
Iter: 719 loss: 3.11889153e-07
Iter: 720 loss: 3.11557358e-07
Iter: 721 loss: 3.10838089e-07
Iter: 722 loss: 3.24207889e-07
Iter: 723 loss: 3.1085284e-07
Iter: 724 loss: 3.10589314e-07
Iter: 725 loss: 3.10542617e-07
Iter: 726 loss: 3.10235919e-07
Iter: 727 loss: 3.10109726e-07
Iter: 728 loss: 3.09957329e-07
Iter: 729 loss: 3.09583584e-07
Iter: 730 loss: 3.10094521e-07
Iter: 731 loss: 3.09414304e-07
Iter: 732 loss: 3.0907421e-07
Iter: 733 loss: 3.13970048e-07
Iter: 734 loss: 3.09085067e-07
Iter: 735 loss: 3.08758047e-07
Iter: 736 loss: 3.0860312e-07
Iter: 737 loss: 3.0845132e-07
Iter: 738 loss: 3.08034203e-07
Iter: 739 loss: 3.07616119e-07
Iter: 740 loss: 3.07523493e-07
Iter: 741 loss: 3.06854815e-07
Iter: 742 loss: 3.10504674e-07
Iter: 743 loss: 3.06747722e-07
Iter: 744 loss: 3.0610471e-07
Iter: 745 loss: 3.06762786e-07
Iter: 746 loss: 3.0569683e-07
Iter: 747 loss: 3.05258823e-07
Iter: 748 loss: 3.05211074e-07
Iter: 749 loss: 3.04948e-07
Iter: 750 loss: 3.04761755e-07
Iter: 751 loss: 3.0464949e-07
Iter: 752 loss: 3.043059e-07
Iter: 753 loss: 3.09582134e-07
Iter: 754 loss: 3.04322214e-07
Iter: 755 loss: 3.04055334e-07
Iter: 756 loss: 3.03645152e-07
Iter: 757 loss: 3.03646146e-07
Iter: 758 loss: 3.0322505e-07
Iter: 759 loss: 3.0429652e-07
Iter: 760 loss: 3.03064724e-07
Iter: 761 loss: 3.02791847e-07
Iter: 762 loss: 3.02788521e-07
Iter: 763 loss: 3.02510898e-07
Iter: 764 loss: 3.01923137e-07
Iter: 765 loss: 3.10958285e-07
Iter: 766 loss: 3.01908727e-07
Iter: 767 loss: 3.01484533e-07
Iter: 768 loss: 3.081621e-07
Iter: 769 loss: 3.01484e-07
Iter: 770 loss: 3.01074124e-07
Iter: 771 loss: 3.02414463e-07
Iter: 772 loss: 3.00930168e-07
Iter: 773 loss: 3.00614943e-07
Iter: 774 loss: 3.00252736e-07
Iter: 775 loss: 3.00210559e-07
Iter: 776 loss: 2.99601567e-07
Iter: 777 loss: 3.00054211e-07
Iter: 778 loss: 2.99252406e-07
Iter: 779 loss: 2.98728452e-07
Iter: 780 loss: 3.01335291e-07
Iter: 781 loss: 2.98639293e-07
Iter: 782 loss: 2.98324579e-07
Iter: 783 loss: 2.98300222e-07
Iter: 784 loss: 2.98031154e-07
Iter: 785 loss: 2.97924259e-07
Iter: 786 loss: 2.97774562e-07
Iter: 787 loss: 2.9741588e-07
Iter: 788 loss: 3.00897881e-07
Iter: 789 loss: 2.97439243e-07
Iter: 790 loss: 2.97092583e-07
Iter: 791 loss: 2.96680895e-07
Iter: 792 loss: 2.96640366e-07
Iter: 793 loss: 2.96164728e-07
Iter: 794 loss: 2.96176893e-07
Iter: 795 loss: 2.95778875e-07
Iter: 796 loss: 2.95327197e-07
Iter: 797 loss: 3.00950489e-07
Iter: 798 loss: 2.95303181e-07
Iter: 799 loss: 2.94860342e-07
Iter: 800 loss: 2.96380961e-07
Iter: 801 loss: 2.94754159e-07
Iter: 802 loss: 2.94416e-07
Iter: 803 loss: 2.94891038e-07
Iter: 804 loss: 2.94278436e-07
Iter: 805 loss: 2.94007293e-07
Iter: 806 loss: 2.96967414e-07
Iter: 807 loss: 2.9401383e-07
Iter: 808 loss: 2.93724497e-07
Iter: 809 loss: 2.93525147e-07
Iter: 810 loss: 2.93420896e-07
Iter: 811 loss: 2.93061277e-07
Iter: 812 loss: 2.92685e-07
Iter: 813 loss: 2.92606899e-07
Iter: 814 loss: 2.9207331e-07
Iter: 815 loss: 2.95117104e-07
Iter: 816 loss: 2.92011265e-07
Iter: 817 loss: 2.91519541e-07
Iter: 818 loss: 2.93218676e-07
Iter: 819 loss: 2.91392325e-07
Iter: 820 loss: 2.90925584e-07
Iter: 821 loss: 2.9693453e-07
Iter: 822 loss: 2.90935617e-07
Iter: 823 loss: 2.90672972e-07
Iter: 824 loss: 2.90705742e-07
Iter: 825 loss: 2.90462253e-07
Iter: 826 loss: 2.90028623e-07
Iter: 827 loss: 2.92038067e-07
Iter: 828 loss: 2.89971126e-07
Iter: 829 loss: 2.89739063e-07
Iter: 830 loss: 2.89428641e-07
Iter: 831 loss: 2.89409343e-07
Iter: 832 loss: 2.89012405e-07
Iter: 833 loss: 2.89973229e-07
Iter: 834 loss: 2.88885246e-07
Iter: 835 loss: 2.88566468e-07
Iter: 836 loss: 2.91794407e-07
Iter: 837 loss: 2.88556436e-07
Iter: 838 loss: 2.88198265e-07
Iter: 839 loss: 2.88551519e-07
Iter: 840 loss: 2.88043395e-07
Iter: 841 loss: 2.87722571e-07
Iter: 842 loss: 2.88111e-07
Iter: 843 loss: 2.87556304e-07
Iter: 844 loss: 2.87198162e-07
Iter: 845 loss: 2.9138269e-07
Iter: 846 loss: 2.87195718e-07
Iter: 847 loss: 2.86956265e-07
Iter: 848 loss: 2.86529655e-07
Iter: 849 loss: 2.95870166e-07
Iter: 850 loss: 2.86526586e-07
Iter: 851 loss: 2.86076983e-07
Iter: 852 loss: 2.88291801e-07
Iter: 853 loss: 2.86001239e-07
Iter: 854 loss: 2.85656682e-07
Iter: 855 loss: 2.86278151e-07
Iter: 856 loss: 2.85542541e-07
Iter: 857 loss: 2.85241043e-07
Iter: 858 loss: 2.8523354e-07
Iter: 859 loss: 2.84999828e-07
Iter: 860 loss: 2.84915643e-07
Iter: 861 loss: 2.84779532e-07
Iter: 862 loss: 2.84449982e-07
Iter: 863 loss: 2.87058924e-07
Iter: 864 loss: 2.84444809e-07
Iter: 865 loss: 2.8421556e-07
Iter: 866 loss: 2.83830786e-07
Iter: 867 loss: 2.8383829e-07
Iter: 868 loss: 2.83406195e-07
Iter: 869 loss: 2.8326582e-07
Iter: 870 loss: 2.83000929e-07
Iter: 871 loss: 2.82573467e-07
Iter: 872 loss: 2.82569431e-07
Iter: 873 loss: 2.82213875e-07
Iter: 874 loss: 2.83998446e-07
Iter: 875 loss: 2.82169708e-07
Iter: 876 loss: 2.81848145e-07
Iter: 877 loss: 2.81768848e-07
Iter: 878 loss: 2.81579219e-07
Iter: 879 loss: 2.81335844e-07
Iter: 880 loss: 2.81316147e-07
Iter: 881 loss: 2.81120435e-07
Iter: 882 loss: 2.8086319e-07
Iter: 883 loss: 2.80843551e-07
Iter: 884 loss: 2.805767e-07
Iter: 885 loss: 2.80491378e-07
Iter: 886 loss: 2.80333154e-07
Iter: 887 loss: 2.79871983e-07
Iter: 888 loss: 2.81817933e-07
Iter: 889 loss: 2.7975139e-07
Iter: 890 loss: 2.79544395e-07
Iter: 891 loss: 2.79532202e-07
Iter: 892 loss: 2.79286184e-07
Iter: 893 loss: 2.78985567e-07
Iter: 894 loss: 2.78955213e-07
Iter: 895 loss: 2.78635611e-07
Iter: 896 loss: 2.82601661e-07
Iter: 897 loss: 2.78632371e-07
Iter: 898 loss: 2.78375609e-07
Iter: 899 loss: 2.780844e-07
Iter: 900 loss: 2.780248e-07
Iter: 901 loss: 2.77677458e-07
Iter: 902 loss: 2.7892861e-07
Iter: 903 loss: 2.77616465e-07
Iter: 904 loss: 2.7731673e-07
Iter: 905 loss: 2.77983418e-07
Iter: 906 loss: 2.77206652e-07
Iter: 907 loss: 2.76934486e-07
Iter: 908 loss: 2.76935225e-07
Iter: 909 loss: 2.76763046e-07
Iter: 910 loss: 2.7657245e-07
Iter: 911 loss: 2.76541726e-07
Iter: 912 loss: 2.76232129e-07
Iter: 913 loss: 2.79267624e-07
Iter: 914 loss: 2.76210244e-07
Iter: 915 loss: 2.75995774e-07
Iter: 916 loss: 2.75563082e-07
Iter: 917 loss: 2.81765097e-07
Iter: 918 loss: 2.75536678e-07
Iter: 919 loss: 2.75178792e-07
Iter: 920 loss: 2.79096e-07
Iter: 921 loss: 2.75179e-07
Iter: 922 loss: 2.74882893e-07
Iter: 923 loss: 2.74469471e-07
Iter: 924 loss: 2.74462508e-07
Iter: 925 loss: 2.74309826e-07
Iter: 926 loss: 2.74198612e-07
Iter: 927 loss: 2.73928265e-07
Iter: 928 loss: 2.74319035e-07
Iter: 929 loss: 2.73796104e-07
Iter: 930 loss: 2.73553582e-07
Iter: 931 loss: 2.74503122e-07
Iter: 932 loss: 2.73527036e-07
Iter: 933 loss: 2.73319017e-07
Iter: 934 loss: 2.73048784e-07
Iter: 935 loss: 2.73022607e-07
Iter: 936 loss: 2.72634367e-07
Iter: 937 loss: 2.73529679e-07
Iter: 938 loss: 2.72499619e-07
Iter: 939 loss: 2.72187378e-07
Iter: 940 loss: 2.73640438e-07
Iter: 941 loss: 2.72090347e-07
Iter: 942 loss: 2.71828611e-07
Iter: 943 loss: 2.75715365e-07
Iter: 944 loss: 2.7180775e-07
Iter: 945 loss: 2.71652397e-07
Iter: 946 loss: 2.71427723e-07
Iter: 947 loss: 2.71407941e-07
Iter: 948 loss: 2.71095416e-07
Iter: 949 loss: 2.75076843e-07
Iter: 950 loss: 2.71072906e-07
Iter: 951 loss: 2.70881372e-07
Iter: 952 loss: 2.70797813e-07
Iter: 953 loss: 2.7069575e-07
Iter: 954 loss: 2.70400335e-07
Iter: 955 loss: 2.70148121e-07
Iter: 956 loss: 2.70068142e-07
Iter: 957 loss: 2.69677685e-07
Iter: 958 loss: 2.72919351e-07
Iter: 959 loss: 2.69659438e-07
Iter: 960 loss: 2.69358338e-07
Iter: 961 loss: 2.69841564e-07
Iter: 962 loss: 2.69227201e-07
Iter: 963 loss: 2.69043028e-07
Iter: 964 loss: 2.69007728e-07
Iter: 965 loss: 2.68858969e-07
Iter: 966 loss: 2.68482211e-07
Iter: 967 loss: 2.74011711e-07
Iter: 968 loss: 2.68476697e-07
Iter: 969 loss: 2.68238637e-07
Iter: 970 loss: 2.68224966e-07
Iter: 971 loss: 2.68015469e-07
Iter: 972 loss: 2.67790881e-07
Iter: 973 loss: 2.67735345e-07
Iter: 974 loss: 2.67490066e-07
Iter: 975 loss: 2.68640804e-07
Iter: 976 loss: 2.67436292e-07
Iter: 977 loss: 2.6717089e-07
Iter: 978 loss: 2.68762221e-07
Iter: 979 loss: 2.67149233e-07
Iter: 980 loss: 2.66910888e-07
Iter: 981 loss: 2.66991606e-07
Iter: 982 loss: 2.66736265e-07
Iter: 983 loss: 2.6656457e-07
Iter: 984 loss: 2.66555219e-07
Iter: 985 loss: 2.66453924e-07
Iter: 986 loss: 2.66137363e-07
Iter: 987 loss: 2.68288289e-07
Iter: 988 loss: 2.66071766e-07
Iter: 989 loss: 2.65658912e-07
Iter: 990 loss: 2.68050314e-07
Iter: 991 loss: 2.65607667e-07
Iter: 992 loss: 2.65319926e-07
Iter: 993 loss: 2.65803607e-07
Iter: 994 loss: 2.65178642e-07
Iter: 995 loss: 2.64901189e-07
Iter: 996 loss: 2.68427414e-07
Iter: 997 loss: 2.64904713e-07
Iter: 998 loss: 2.64658411e-07
Iter: 999 loss: 2.65668461e-07
Iter: 1000 loss: 2.64584855e-07
Iter: 1001 loss: 2.64416485e-07
Iter: 1002 loss: 2.64349268e-07
Iter: 1003 loss: 2.64246069e-07
Iter: 1004 loss: 2.64025118e-07
Iter: 1005 loss: 2.66639063e-07
Iter: 1006 loss: 2.64006871e-07
Iter: 1007 loss: 2.63854815e-07
Iter: 1008 loss: 2.63501676e-07
Iter: 1009 loss: 2.69011764e-07
Iter: 1010 loss: 2.63491529e-07
Iter: 1011 loss: 2.63227264e-07
Iter: 1012 loss: 2.63219533e-07
Iter: 1013 loss: 2.62948646e-07
Iter: 1014 loss: 2.63422947e-07
Iter: 1015 loss: 2.62845333e-07
Iter: 1016 loss: 2.62572911e-07
Iter: 1017 loss: 2.62954416e-07
Iter: 1018 loss: 2.62445951e-07
Iter: 1019 loss: 2.62116e-07
Iter: 1020 loss: 2.63822017e-07
Iter: 1021 loss: 2.62074138e-07
Iter: 1022 loss: 2.61818883e-07
Iter: 1023 loss: 2.61511872e-07
Iter: 1024 loss: 2.61477368e-07
Iter: 1025 loss: 2.61134232e-07
Iter: 1026 loss: 2.61491522e-07
Iter: 1027 loss: 2.60919762e-07
Iter: 1028 loss: 2.60544368e-07
Iter: 1029 loss: 2.65353606e-07
Iter: 1030 loss: 2.60551474e-07
Iter: 1031 loss: 2.60385832e-07
Iter: 1032 loss: 2.60389726e-07
Iter: 1033 loss: 2.6025225e-07
Iter: 1034 loss: 2.60082373e-07
Iter: 1035 loss: 2.60072795e-07
Iter: 1036 loss: 2.5985446e-07
Iter: 1037 loss: 2.60738943e-07
Iter: 1038 loss: 2.59813362e-07
Iter: 1039 loss: 2.59586727e-07
Iter: 1040 loss: 2.59814215e-07
Iter: 1041 loss: 2.59434159e-07
Iter: 1042 loss: 2.59159833e-07
Iter: 1043 loss: 2.59175465e-07
Iter: 1044 loss: 2.58971426e-07
Iter: 1045 loss: 2.58689511e-07
Iter: 1046 loss: 2.6146779e-07
Iter: 1047 loss: 2.58671463e-07
Iter: 1048 loss: 2.58346205e-07
Iter: 1049 loss: 2.58519805e-07
Iter: 1050 loss: 2.58143473e-07
Iter: 1051 loss: 2.57895096e-07
Iter: 1052 loss: 2.59918579e-07
Iter: 1053 loss: 2.57892367e-07
Iter: 1054 loss: 2.57672895e-07
Iter: 1055 loss: 2.57721297e-07
Iter: 1056 loss: 2.57494946e-07
Iter: 1057 loss: 2.57277492e-07
Iter: 1058 loss: 2.57215788e-07
Iter: 1059 loss: 2.57067654e-07
Iter: 1060 loss: 2.56704453e-07
Iter: 1061 loss: 2.57288292e-07
Iter: 1062 loss: 2.56550209e-07
Iter: 1063 loss: 2.56394344e-07
Iter: 1064 loss: 2.5635714e-07
Iter: 1065 loss: 2.56186581e-07
Iter: 1066 loss: 2.56316042e-07
Iter: 1067 loss: 2.56049901e-07
Iter: 1068 loss: 2.55813973e-07
Iter: 1069 loss: 2.55978648e-07
Iter: 1070 loss: 2.55667118e-07
Iter: 1071 loss: 2.5539336e-07
Iter: 1072 loss: 2.57021441e-07
Iter: 1073 loss: 2.55365279e-07
Iter: 1074 loss: 2.55107523e-07
Iter: 1075 loss: 2.54979057e-07
Iter: 1076 loss: 2.54860311e-07
Iter: 1077 loss: 2.54613269e-07
Iter: 1078 loss: 2.55180339e-07
Iter: 1079 loss: 2.54519023e-07
Iter: 1080 loss: 2.54276614e-07
Iter: 1081 loss: 2.5427596e-07
Iter: 1082 loss: 2.5412379e-07
Iter: 1083 loss: 2.54171539e-07
Iter: 1084 loss: 2.53999985e-07
Iter: 1085 loss: 2.53836788e-07
Iter: 1086 loss: 2.54946769e-07
Iter: 1087 loss: 2.53808935e-07
Iter: 1088 loss: 2.53617458e-07
Iter: 1089 loss: 2.53384968e-07
Iter: 1090 loss: 2.53369649e-07
Iter: 1091 loss: 2.53111921e-07
Iter: 1092 loss: 2.52974019e-07
Iter: 1093 loss: 2.52853596e-07
Iter: 1094 loss: 2.52438326e-07
Iter: 1095 loss: 2.55194323e-07
Iter: 1096 loss: 2.52414139e-07
Iter: 1097 loss: 2.52304119e-07
Iter: 1098 loss: 2.52259099e-07
Iter: 1099 loss: 2.52097124e-07
Iter: 1100 loss: 2.51845449e-07
Iter: 1101 loss: 2.58408591e-07
Iter: 1102 loss: 2.51851333e-07
Iter: 1103 loss: 2.51547533e-07
Iter: 1104 loss: 2.52134612e-07
Iter: 1105 loss: 2.51431e-07
Iter: 1106 loss: 2.51075761e-07
Iter: 1107 loss: 2.53770736e-07
Iter: 1108 loss: 2.51045833e-07
Iter: 1109 loss: 2.50849183e-07
Iter: 1110 loss: 2.50973528e-07
Iter: 1111 loss: 2.50742971e-07
Iter: 1112 loss: 2.50535408e-07
Iter: 1113 loss: 2.50686554e-07
Iter: 1114 loss: 2.50412199e-07
Iter: 1115 loss: 2.50222627e-07
Iter: 1116 loss: 2.50206142e-07
Iter: 1117 loss: 2.50060054e-07
Iter: 1118 loss: 2.49859227e-07
Iter: 1119 loss: 2.49846806e-07
Iter: 1120 loss: 2.4959985e-07
Iter: 1121 loss: 2.52347519e-07
Iter: 1122 loss: 2.49597917e-07
Iter: 1123 loss: 2.49409936e-07
Iter: 1124 loss: 2.49260381e-07
Iter: 1125 loss: 2.4920962e-07
Iter: 1126 loss: 2.48917559e-07
Iter: 1127 loss: 2.48992109e-07
Iter: 1128 loss: 2.48713405e-07
Iter: 1129 loss: 2.48351512e-07
Iter: 1130 loss: 2.49006462e-07
Iter: 1131 loss: 2.48212103e-07
Iter: 1132 loss: 2.48180783e-07
Iter: 1133 loss: 2.48033643e-07
Iter: 1134 loss: 2.47899919e-07
Iter: 1135 loss: 2.47677463e-07
Iter: 1136 loss: 2.47681214e-07
Iter: 1137 loss: 2.47488856e-07
Iter: 1138 loss: 2.48646643e-07
Iter: 1139 loss: 2.47475555e-07
Iter: 1140 loss: 2.47281378e-07
Iter: 1141 loss: 2.47640287e-07
Iter: 1142 loss: 2.47184e-07
Iter: 1143 loss: 2.46971922e-07
Iter: 1144 loss: 2.4671121e-07
Iter: 1145 loss: 2.46689893e-07
Iter: 1146 loss: 2.46459535e-07
Iter: 1147 loss: 2.49425113e-07
Iter: 1148 loss: 2.46451918e-07
Iter: 1149 loss: 2.46272691e-07
Iter: 1150 loss: 2.47315228e-07
Iter: 1151 loss: 2.46234663e-07
Iter: 1152 loss: 2.46045659e-07
Iter: 1153 loss: 2.45862793e-07
Iter: 1154 loss: 2.45814817e-07
Iter: 1155 loss: 2.45640535e-07
Iter: 1156 loss: 2.45632805e-07
Iter: 1157 loss: 2.45525172e-07
Iter: 1158 loss: 2.45328692e-07
Iter: 1159 loss: 2.45327584e-07
Iter: 1160 loss: 2.45089211e-07
Iter: 1161 loss: 2.4537681e-07
Iter: 1162 loss: 2.44979901e-07
Iter: 1163 loss: 2.44671753e-07
Iter: 1164 loss: 2.45098761e-07
Iter: 1165 loss: 2.44495283e-07
Iter: 1166 loss: 2.44409705e-07
Iter: 1167 loss: 2.44315231e-07
Iter: 1168 loss: 2.4416326e-07
Iter: 1169 loss: 2.43803811e-07
Iter: 1170 loss: 2.47660154e-07
Iter: 1171 loss: 2.43758365e-07
Iter: 1172 loss: 2.43524624e-07
Iter: 1173 loss: 2.45731485e-07
Iter: 1174 loss: 2.43498e-07
Iter: 1175 loss: 2.43285911e-07
Iter: 1176 loss: 2.44953867e-07
Iter: 1177 loss: 2.43263685e-07
Iter: 1178 loss: 2.43106513e-07
Iter: 1179 loss: 2.42980576e-07
Iter: 1180 loss: 2.42946214e-07
Iter: 1181 loss: 2.42690419e-07
Iter: 1182 loss: 2.43553245e-07
Iter: 1183 loss: 2.42635394e-07
Iter: 1184 loss: 2.42439882e-07
Iter: 1185 loss: 2.42454746e-07
Iter: 1186 loss: 2.42358851e-07
Iter: 1187 loss: 2.42218221e-07
Iter: 1188 loss: 2.42212423e-07
Iter: 1189 loss: 2.41973311e-07
Iter: 1190 loss: 2.43176316e-07
Iter: 1191 loss: 2.41934e-07
Iter: 1192 loss: 2.41730561e-07
Iter: 1193 loss: 2.41642567e-07
Iter: 1194 loss: 2.41534224e-07
Iter: 1195 loss: 2.41246084e-07
Iter: 1196 loss: 2.41770749e-07
Iter: 1197 loss: 2.41138764e-07
Iter: 1198 loss: 2.40917132e-07
Iter: 1199 loss: 2.41626424e-07
Iter: 1200 loss: 2.40833486e-07
Iter: 1201 loss: 2.40708e-07
Iter: 1202 loss: 2.40684585e-07
Iter: 1203 loss: 2.40601025e-07
Iter: 1204 loss: 2.40391643e-07
Iter: 1205 loss: 2.43064278e-07
Iter: 1206 loss: 2.40375243e-07
Iter: 1207 loss: 2.40154293e-07
Iter: 1208 loss: 2.40893939e-07
Iter: 1209 loss: 2.40097222e-07
Iter: 1210 loss: 2.39874169e-07
Iter: 1211 loss: 2.42109166e-07
Iter: 1212 loss: 2.39862288e-07
Iter: 1213 loss: 2.39719412e-07
Iter: 1214 loss: 2.39476122e-07
Iter: 1215 loss: 2.39464356e-07
Iter: 1216 loss: 2.39226665e-07
Iter: 1217 loss: 2.42012817e-07
Iter: 1218 loss: 2.39218252e-07
Iter: 1219 loss: 2.38973797e-07
Iter: 1220 loss: 2.39352147e-07
Iter: 1221 loss: 2.38850049e-07
Iter: 1222 loss: 2.38590587e-07
Iter: 1223 loss: 2.38683953e-07
Iter: 1224 loss: 2.38401412e-07
Iter: 1225 loss: 2.38177051e-07
Iter: 1226 loss: 2.38172305e-07
Iter: 1227 loss: 2.38009335e-07
Iter: 1228 loss: 2.37747557e-07
Iter: 1229 loss: 2.37737893e-07
Iter: 1230 loss: 2.37514428e-07
Iter: 1231 loss: 2.38072516e-07
Iter: 1232 loss: 2.37461279e-07
Iter: 1233 loss: 2.3722383e-07
Iter: 1234 loss: 2.38268257e-07
Iter: 1235 loss: 2.37174476e-07
Iter: 1236 loss: 2.37008521e-07
Iter: 1237 loss: 2.36996385e-07
Iter: 1238 loss: 2.36901428e-07
Iter: 1239 loss: 2.36656874e-07
Iter: 1240 loss: 2.3873605e-07
Iter: 1241 loss: 2.36599135e-07
Iter: 1242 loss: 2.36359654e-07
Iter: 1243 loss: 2.37450834e-07
Iter: 1244 loss: 2.3629832e-07
Iter: 1245 loss: 2.35983094e-07
Iter: 1246 loss: 2.37447395e-07
Iter: 1247 loss: 2.35947823e-07
Iter: 1248 loss: 2.35747407e-07
Iter: 1249 loss: 2.35770287e-07
Iter: 1250 loss: 2.35597724e-07
Iter: 1251 loss: 2.35377229e-07
Iter: 1252 loss: 2.35529754e-07
Iter: 1253 loss: 2.35223965e-07
Iter: 1254 loss: 2.35083334e-07
Iter: 1255 loss: 2.35055495e-07
Iter: 1256 loss: 2.34929573e-07
Iter: 1257 loss: 2.34716339e-07
Iter: 1258 loss: 2.40017243e-07
Iter: 1259 loss: 2.34716964e-07
Iter: 1260 loss: 2.34549873e-07
Iter: 1261 loss: 2.34530944e-07
Iter: 1262 loss: 2.34413662e-07
Iter: 1263 loss: 2.3445709e-07
Iter: 1264 loss: 2.34301524e-07
Iter: 1265 loss: 2.34124769e-07
Iter: 1266 loss: 2.33829695e-07
Iter: 1267 loss: 2.33820629e-07
Iter: 1268 loss: 2.33526791e-07
Iter: 1269 loss: 2.35838172e-07
Iter: 1270 loss: 2.33510164e-07
Iter: 1271 loss: 2.33342831e-07
Iter: 1272 loss: 2.33339222e-07
Iter: 1273 loss: 2.33199103e-07
Iter: 1274 loss: 2.3301439e-07
Iter: 1275 loss: 2.33019534e-07
Iter: 1276 loss: 2.32820724e-07
Iter: 1277 loss: 2.33355479e-07
Iter: 1278 loss: 2.32755411e-07
Iter: 1279 loss: 2.32638911e-07
Iter: 1280 loss: 2.32640119e-07
Iter: 1281 loss: 2.32538341e-07
Iter: 1282 loss: 2.32276136e-07
Iter: 1283 loss: 2.34930681e-07
Iter: 1284 loss: 2.32253498e-07
Iter: 1285 loss: 2.31996566e-07
Iter: 1286 loss: 2.33083185e-07
Iter: 1287 loss: 2.31964364e-07
Iter: 1288 loss: 2.31674932e-07
Iter: 1289 loss: 2.31946e-07
Iter: 1290 loss: 2.31530493e-07
Iter: 1291 loss: 2.3129229e-07
Iter: 1292 loss: 2.34360101e-07
Iter: 1293 loss: 2.31279259e-07
Iter: 1294 loss: 2.31062643e-07
Iter: 1295 loss: 2.31993027e-07
Iter: 1296 loss: 2.3100381e-07
Iter: 1297 loss: 2.30837088e-07
Iter: 1298 loss: 2.30933466e-07
Iter: 1299 loss: 2.30731217e-07
Iter: 1300 loss: 2.30533885e-07
Iter: 1301 loss: 2.31876527e-07
Iter: 1302 loss: 2.30515255e-07
Iter: 1303 loss: 2.30348689e-07
Iter: 1304 loss: 2.30157752e-07
Iter: 1305 loss: 2.3013402e-07
Iter: 1306 loss: 2.29947304e-07
Iter: 1307 loss: 2.3034724e-07
Iter: 1308 loss: 2.29862067e-07
Iter: 1309 loss: 2.29764012e-07
Iter: 1310 loss: 2.29729352e-07
Iter: 1311 loss: 2.2965105e-07
Iter: 1312 loss: 2.29433311e-07
Iter: 1313 loss: 2.32086947e-07
Iter: 1314 loss: 2.29413843e-07
Iter: 1315 loss: 2.29160889e-07
Iter: 1316 loss: 2.30109492e-07
Iter: 1317 loss: 2.29111976e-07
Iter: 1318 loss: 2.28902181e-07
Iter: 1319 loss: 2.28802676e-07
Iter: 1320 loss: 2.28710604e-07
Iter: 1321 loss: 2.28476608e-07
Iter: 1322 loss: 2.28465439e-07
Iter: 1323 loss: 2.28310597e-07
Iter: 1324 loss: 2.28166058e-07
Iter: 1325 loss: 2.28116065e-07
Iter: 1326 loss: 2.27954132e-07
Iter: 1327 loss: 2.29331278e-07
Iter: 1328 loss: 2.27913787e-07
Iter: 1329 loss: 2.27757681e-07
Iter: 1330 loss: 2.28815779e-07
Iter: 1331 loss: 2.27742206e-07
Iter: 1332 loss: 2.27633876e-07
Iter: 1333 loss: 2.27472938e-07
Iter: 1334 loss: 2.27466444e-07
Iter: 1335 loss: 2.27343719e-07
Iter: 1336 loss: 2.27348295e-07
Iter: 1337 loss: 2.27239767e-07
Iter: 1338 loss: 2.27077493e-07
Iter: 1339 loss: 2.30951457e-07
Iter: 1340 loss: 2.27070586e-07
Iter: 1341 loss: 2.26930695e-07
Iter: 1342 loss: 2.26930069e-07
Iter: 1343 loss: 2.26801063e-07
Iter: 1344 loss: 2.26718583e-07
Iter: 1345 loss: 2.2667966e-07
Iter: 1346 loss: 2.26463783e-07
Iter: 1347 loss: 2.26288492e-07
Iter: 1348 loss: 2.26242435e-07
Iter: 1349 loss: 2.25959582e-07
Iter: 1350 loss: 2.27150963e-07
Iter: 1351 loss: 2.25917518e-07
Iter: 1352 loss: 2.25695402e-07
Iter: 1353 loss: 2.2594736e-07
Iter: 1354 loss: 2.25574837e-07
Iter: 1355 loss: 2.25405159e-07
Iter: 1356 loss: 2.25395326e-07
Iter: 1357 loss: 2.25232696e-07
Iter: 1358 loss: 2.25181893e-07
Iter: 1359 loss: 2.25098617e-07
Iter: 1360 loss: 2.24957986e-07
Iter: 1361 loss: 2.25698173e-07
Iter: 1362 loss: 2.24917812e-07
Iter: 1363 loss: 2.24754586e-07
Iter: 1364 loss: 2.25414254e-07
Iter: 1365 loss: 2.24715535e-07
Iter: 1366 loss: 2.24567245e-07
Iter: 1367 loss: 2.24500738e-07
Iter: 1368 loss: 2.24417519e-07
Iter: 1369 loss: 2.24304685e-07
Iter: 1370 loss: 2.26140457e-07
Iter: 1371 loss: 2.24304216e-07
Iter: 1372 loss: 2.24169312e-07
Iter: 1373 loss: 2.24089931e-07
Iter: 1374 loss: 2.24027204e-07
Iter: 1375 loss: 2.23857697e-07
Iter: 1376 loss: 2.24218098e-07
Iter: 1377 loss: 2.23798494e-07
Iter: 1378 loss: 2.2360166e-07
Iter: 1379 loss: 2.2551238e-07
Iter: 1380 loss: 2.23598093e-07
Iter: 1381 loss: 2.23500848e-07
Iter: 1382 loss: 2.23425857e-07
Iter: 1383 loss: 2.23394181e-07
Iter: 1384 loss: 2.23250879e-07
Iter: 1385 loss: 2.23068014e-07
Iter: 1386 loss: 2.23052609e-07
Iter: 1387 loss: 2.22809035e-07
Iter: 1388 loss: 2.25301378e-07
Iter: 1389 loss: 2.22806662e-07
Iter: 1390 loss: 2.22643905e-07
Iter: 1391 loss: 2.24621033e-07
Iter: 1392 loss: 2.22650627e-07
Iter: 1393 loss: 2.22521109e-07
Iter: 1394 loss: 2.22415622e-07
Iter: 1395 loss: 2.22374098e-07
Iter: 1396 loss: 2.22153503e-07
Iter: 1397 loss: 2.22515425e-07
Iter: 1398 loss: 2.22047063e-07
Iter: 1399 loss: 2.21796071e-07
Iter: 1400 loss: 2.24540514e-07
Iter: 1401 loss: 2.21784219e-07
Iter: 1402 loss: 2.21686662e-07
Iter: 1403 loss: 2.21500031e-07
Iter: 1404 loss: 2.2150661e-07
Iter: 1405 loss: 2.21366705e-07
Iter: 1406 loss: 2.21365042e-07
Iter: 1407 loss: 2.21246978e-07
Iter: 1408 loss: 2.21182134e-07
Iter: 1409 loss: 2.2116518e-07
Iter: 1410 loss: 2.20982869e-07
Iter: 1411 loss: 2.21114064e-07
Iter: 1412 loss: 2.20870064e-07
Iter: 1413 loss: 2.20787527e-07
Iter: 1414 loss: 2.20784557e-07
Iter: 1415 loss: 2.20656347e-07
Iter: 1416 loss: 2.20397723e-07
Iter: 1417 loss: 2.24916363e-07
Iter: 1418 loss: 2.2040318e-07
Iter: 1419 loss: 2.2020626e-07
Iter: 1420 loss: 2.20682892e-07
Iter: 1421 loss: 2.20141885e-07
Iter: 1422 loss: 2.19912721e-07
Iter: 1423 loss: 2.19888406e-07
Iter: 1424 loss: 2.19714281e-07
Iter: 1425 loss: 2.19433289e-07
Iter: 1426 loss: 2.21465839e-07
Iter: 1427 loss: 2.1940032e-07
Iter: 1428 loss: 2.19266838e-07
Iter: 1429 loss: 2.19264706e-07
Iter: 1430 loss: 2.19163383e-07
Iter: 1431 loss: 2.18986017e-07
Iter: 1432 loss: 2.22887991e-07
Iter: 1433 loss: 2.18985562e-07
Iter: 1434 loss: 2.18791229e-07
Iter: 1435 loss: 2.19378791e-07
Iter: 1436 loss: 2.18720459e-07
Iter: 1437 loss: 2.18567465e-07
Iter: 1438 loss: 2.18574243e-07
Iter: 1439 loss: 2.18464152e-07
Iter: 1440 loss: 2.18274437e-07
Iter: 1441 loss: 2.22530105e-07
Iter: 1442 loss: 2.18279098e-07
Iter: 1443 loss: 2.18074916e-07
Iter: 1444 loss: 2.19467509e-07
Iter: 1445 loss: 2.18059654e-07
Iter: 1446 loss: 2.17863288e-07
Iter: 1447 loss: 2.18587829e-07
Iter: 1448 loss: 2.17812101e-07
Iter: 1449 loss: 2.17684359e-07
Iter: 1450 loss: 2.17917446e-07
Iter: 1451 loss: 2.1762537e-07
Iter: 1452 loss: 2.17544084e-07
Iter: 1453 loss: 2.17543885e-07
Iter: 1454 loss: 2.17467132e-07
Iter: 1455 loss: 2.17279876e-07
Iter: 1456 loss: 2.18755247e-07
Iter: 1457 loss: 2.17240483e-07
Iter: 1458 loss: 2.17051209e-07
Iter: 1459 loss: 2.17390635e-07
Iter: 1460 loss: 2.16960544e-07
Iter: 1461 loss: 2.16732772e-07
Iter: 1462 loss: 2.17381711e-07
Iter: 1463 loss: 2.16671197e-07
Iter: 1464 loss: 2.16379e-07
Iter: 1465 loss: 2.16797233e-07
Iter: 1466 loss: 2.16234568e-07
Iter: 1467 loss: 2.16029463e-07
Iter: 1468 loss: 2.18413845e-07
Iter: 1469 loss: 2.16028042e-07
Iter: 1470 loss: 2.1582791e-07
Iter: 1471 loss: 2.16875463e-07
Iter: 1472 loss: 2.15796803e-07
Iter: 1473 loss: 2.15661316e-07
Iter: 1474 loss: 2.15619622e-07
Iter: 1475 loss: 2.15540865e-07
Iter: 1476 loss: 2.1536475e-07
Iter: 1477 loss: 2.15774463e-07
Iter: 1478 loss: 2.15308603e-07
Iter: 1479 loss: 2.15197304e-07
Iter: 1480 loss: 2.15183775e-07
Iter: 1481 loss: 2.15108074e-07
Iter: 1482 loss: 2.149186e-07
Iter: 1483 loss: 2.17673e-07
Iter: 1484 loss: 2.14912902e-07
Iter: 1485 loss: 2.14796074e-07
Iter: 1486 loss: 2.16780194e-07
Iter: 1487 loss: 2.1479508e-07
Iter: 1488 loss: 2.14677755e-07
Iter: 1489 loss: 2.14883116e-07
Iter: 1490 loss: 2.14625274e-07
Iter: 1491 loss: 2.14511658e-07
Iter: 1492 loss: 2.14708848e-07
Iter: 1493 loss: 2.14448946e-07
Iter: 1494 loss: 2.14271907e-07
Iter: 1495 loss: 2.1475455e-07
Iter: 1496 loss: 2.14205826e-07
Iter: 1497 loss: 2.14097412e-07
Iter: 1498 loss: 2.13902766e-07
Iter: 1499 loss: 2.13904e-07
Iter: 1500 loss: 2.13676969e-07
Iter: 1501 loss: 2.13923244e-07
Iter: 1502 loss: 2.13546542e-07
Iter: 1503 loss: 2.13246565e-07
Iter: 1504 loss: 2.14789594e-07
Iter: 1505 loss: 2.13204e-07
Iter: 1506 loss: 2.13039712e-07
Iter: 1507 loss: 2.13818211e-07
Iter: 1508 loss: 2.13008335e-07
Iter: 1509 loss: 2.1289199e-07
Iter: 1510 loss: 2.12896481e-07
Iter: 1511 loss: 2.12792258e-07
Iter: 1512 loss: 2.12644636e-07
Iter: 1513 loss: 2.1624389e-07
Iter: 1514 loss: 2.12633893e-07
Iter: 1515 loss: 2.1244297e-07
Iter: 1516 loss: 2.13171546e-07
Iter: 1517 loss: 2.12385658e-07
Iter: 1518 loss: 2.12285073e-07
Iter: 1519 loss: 2.12266301e-07
Iter: 1520 loss: 2.12169354e-07
Iter: 1521 loss: 2.11965542e-07
Iter: 1522 loss: 2.15194802e-07
Iter: 1523 loss: 2.11958593e-07
Iter: 1524 loss: 2.11750873e-07
Iter: 1525 loss: 2.12969397e-07
Iter: 1526 loss: 2.11746539e-07
Iter: 1527 loss: 2.11589594e-07
Iter: 1528 loss: 2.13067949e-07
Iter: 1529 loss: 2.11553e-07
Iter: 1530 loss: 2.11442227e-07
Iter: 1531 loss: 2.11519534e-07
Iter: 1532 loss: 2.11377397e-07
Iter: 1533 loss: 2.11233953e-07
Iter: 1534 loss: 2.12154148e-07
Iter: 1535 loss: 2.11219259e-07
Iter: 1536 loss: 2.11135884e-07
Iter: 1537 loss: 2.11002828e-07
Iter: 1538 loss: 2.11010786e-07
Iter: 1539 loss: 2.10839644e-07
Iter: 1540 loss: 2.11005741e-07
Iter: 1541 loss: 2.10739955e-07
Iter: 1542 loss: 2.10557801e-07
Iter: 1543 loss: 2.11624453e-07
Iter: 1544 loss: 2.10511331e-07
Iter: 1545 loss: 2.10344609e-07
Iter: 1546 loss: 2.10151725e-07
Iter: 1547 loss: 2.10119239e-07
Iter: 1548 loss: 2.1011806e-07
Iter: 1549 loss: 2.10001801e-07
Iter: 1550 loss: 2.09877385e-07
Iter: 1551 loss: 2.09772395e-07
Iter: 1552 loss: 2.09747895e-07
Iter: 1553 loss: 2.09576612e-07
Iter: 1554 loss: 2.10796728e-07
Iter: 1555 loss: 2.09582524e-07
Iter: 1556 loss: 2.09382009e-07
Iter: 1557 loss: 2.09625256e-07
Iter: 1558 loss: 2.09306023e-07
Iter: 1559 loss: 2.09160589e-07
Iter: 1560 loss: 2.0926133e-07
Iter: 1561 loss: 2.0906711e-07
Iter: 1562 loss: 2.0903336e-07
Iter: 1563 loss: 2.08980595e-07
Iter: 1564 loss: 2.08930516e-07
Iter: 1565 loss: 2.08850935e-07
Iter: 1566 loss: 2.11085251e-07
Iter: 1567 loss: 2.08844199e-07
Iter: 1568 loss: 2.08693763e-07
Iter: 1569 loss: 2.08985099e-07
Iter: 1570 loss: 2.08635214e-07
Iter: 1571 loss: 2.08469899e-07
Iter: 1572 loss: 2.08417703e-07
Iter: 1573 loss: 2.08333745e-07
Iter: 1574 loss: 2.0815456e-07
Iter: 1575 loss: 2.0870462e-07
Iter: 1576 loss: 2.08093141e-07
Iter: 1577 loss: 2.07911839e-07
Iter: 1578 loss: 2.07861e-07
Iter: 1579 loss: 2.07730537e-07
Iter: 1580 loss: 2.07525687e-07
Iter: 1581 loss: 2.09485222e-07
Iter: 1582 loss: 2.07525943e-07
Iter: 1583 loss: 2.07384133e-07
Iter: 1584 loss: 2.09188e-07
Iter: 1585 loss: 2.07394706e-07
Iter: 1586 loss: 2.07276088e-07
Iter: 1587 loss: 2.07355413e-07
Iter: 1588 loss: 2.07204565e-07
Iter: 1589 loss: 2.07076042e-07
Iter: 1590 loss: 2.07562479e-07
Iter: 1591 loss: 2.07036308e-07
Iter: 1592 loss: 2.06885375e-07
Iter: 1593 loss: 2.07116599e-07
Iter: 1594 loss: 2.06824282e-07
Iter: 1595 loss: 2.06690231e-07
Iter: 1596 loss: 2.06554404e-07
Iter: 1597 loss: 2.0651683e-07
Iter: 1598 loss: 2.06440674e-07
Iter: 1599 loss: 2.06400557e-07
Iter: 1600 loss: 2.06334278e-07
Iter: 1601 loss: 2.06296448e-07
Iter: 1602 loss: 2.06271054e-07
Iter: 1603 loss: 2.06170043e-07
Iter: 1604 loss: 2.06527886e-07
Iter: 1605 loss: 2.06131375e-07
Iter: 1606 loss: 2.06021099e-07
Iter: 1607 loss: 2.06002213e-07
Iter: 1608 loss: 2.05910155e-07
Iter: 1609 loss: 2.05744726e-07
Iter: 1610 loss: 2.05663838e-07
Iter: 1611 loss: 2.05601339e-07
Iter: 1612 loss: 2.05386726e-07
Iter: 1613 loss: 2.06501241e-07
Iter: 1614 loss: 2.05351526e-07
Iter: 1615 loss: 2.05120386e-07
Iter: 1616 loss: 2.05224666e-07
Iter: 1617 loss: 2.04989973e-07
Iter: 1618 loss: 2.04834365e-07
Iter: 1619 loss: 2.04830954e-07
Iter: 1620 loss: 2.04679921e-07
Iter: 1621 loss: 2.05002351e-07
Iter: 1622 loss: 2.04595253e-07
Iter: 1623 loss: 2.04489965e-07
Iter: 1624 loss: 2.04477615e-07
Iter: 1625 loss: 2.04374629e-07
Iter: 1626 loss: 2.04191707e-07
Iter: 1627 loss: 2.06547085e-07
Iter: 1628 loss: 2.04202266e-07
Iter: 1629 loss: 2.04084273e-07
Iter: 1630 loss: 2.03947749e-07
Iter: 1631 loss: 2.03940459e-07
Iter: 1632 loss: 2.03830751e-07
Iter: 1633 loss: 2.05270624e-07
Iter: 1634 loss: 2.03822893e-07
Iter: 1635 loss: 2.03705071e-07
Iter: 1636 loss: 2.03762895e-07
Iter: 1637 loss: 2.03632396e-07
Iter: 1638 loss: 2.03507028e-07
Iter: 1639 loss: 2.03522134e-07
Iter: 1640 loss: 2.0343164e-07
Iter: 1641 loss: 2.03325641e-07
Iter: 1642 loss: 2.05038958e-07
Iter: 1643 loss: 2.03324845e-07
Iter: 1644 loss: 2.03224346e-07
Iter: 1645 loss: 2.03139393e-07
Iter: 1646 loss: 2.0312558e-07
Iter: 1647 loss: 2.02947987e-07
Iter: 1648 loss: 2.02881893e-07
Iter: 1649 loss: 2.02801743e-07
Iter: 1650 loss: 2.02578804e-07
Iter: 1651 loss: 2.04010689e-07
Iter: 1652 loss: 2.02552599e-07
Iter: 1653 loss: 2.02388208e-07
Iter: 1654 loss: 2.02949025e-07
Iter: 1655 loss: 2.02329133e-07
Iter: 1656 loss: 2.02221756e-07
Iter: 1657 loss: 2.02214949e-07
Iter: 1658 loss: 2.02127126e-07
Iter: 1659 loss: 2.02002695e-07
Iter: 1660 loss: 2.01980399e-07
Iter: 1661 loss: 2.01880695e-07
Iter: 1662 loss: 2.0188223e-07
Iter: 1663 loss: 2.01792432e-07
Iter: 1664 loss: 2.01647097e-07
Iter: 1665 loss: 2.01640717e-07
Iter: 1666 loss: 2.01520081e-07
Iter: 1667 loss: 2.03293013e-07
Iter: 1668 loss: 2.01516912e-07
Iter: 1669 loss: 2.01398706e-07
Iter: 1670 loss: 2.01413116e-07
Iter: 1671 loss: 2.01315032e-07
Iter: 1672 loss: 2.01191739e-07
Iter: 1673 loss: 2.01158343e-07
Iter: 1674 loss: 2.01079615e-07
Iter: 1675 loss: 2.0093745e-07
Iter: 1676 loss: 2.00933584e-07
Iter: 1677 loss: 2.00843417e-07
Iter: 1678 loss: 2.00709792e-07
Iter: 1679 loss: 2.00706609e-07
Iter: 1680 loss: 2.00564628e-07
Iter: 1681 loss: 2.00722027e-07
Iter: 1682 loss: 2.00483328e-07
Iter: 1683 loss: 2.00278635e-07
Iter: 1684 loss: 2.00463433e-07
Iter: 1685 loss: 2.00155625e-07
Iter: 1686 loss: 1.99976455e-07
Iter: 1687 loss: 2.02165694e-07
Iter: 1688 loss: 1.99965626e-07
Iter: 1689 loss: 1.9981664e-07
Iter: 1690 loss: 2.01037551e-07
Iter: 1691 loss: 1.99798961e-07
Iter: 1692 loss: 1.99693815e-07
Iter: 1693 loss: 1.99646706e-07
Iter: 1694 loss: 1.99615329e-07
Iter: 1695 loss: 1.99445779e-07
Iter: 1696 loss: 2.00772988e-07
Iter: 1697 loss: 1.99432449e-07
Iter: 1698 loss: 1.99319231e-07
Iter: 1699 loss: 1.992303e-07
Iter: 1700 loss: 1.99184797e-07
Iter: 1701 loss: 1.9904688e-07
Iter: 1702 loss: 1.99045104e-07
Iter: 1703 loss: 1.98951156e-07
Iter: 1704 loss: 1.98860903e-07
Iter: 1705 loss: 1.98829895e-07
Iter: 1706 loss: 1.98742882e-07
Iter: 1707 loss: 1.99090294e-07
Iter: 1708 loss: 1.987015e-07
Iter: 1709 loss: 1.98591493e-07
Iter: 1710 loss: 1.9873751e-07
Iter: 1711 loss: 1.98524958e-07
Iter: 1712 loss: 1.98389301e-07
Iter: 1713 loss: 1.98462914e-07
Iter: 1714 loss: 1.98290664e-07
Iter: 1715 loss: 1.9815019e-07
Iter: 1716 loss: 1.98186029e-07
Iter: 1717 loss: 1.98045257e-07
Iter: 1718 loss: 1.97827333e-07
Iter: 1719 loss: 1.98125107e-07
Iter: 1720 loss: 1.97711316e-07
Iter: 1721 loss: 1.9765065e-07
Iter: 1722 loss: 1.97604592e-07
Iter: 1723 loss: 1.97499133e-07
Iter: 1724 loss: 1.97610134e-07
Iter: 1725 loss: 1.97442716e-07
Iter: 1726 loss: 1.97342118e-07
Iter: 1727 loss: 1.97477931e-07
Iter: 1728 loss: 1.97280343e-07
Iter: 1729 loss: 1.97117529e-07
Iter: 1730 loss: 1.97501365e-07
Iter: 1731 loss: 1.97067521e-07
Iter: 1732 loss: 1.96956364e-07
Iter: 1733 loss: 1.97193202e-07
Iter: 1734 loss: 1.96917384e-07
Iter: 1735 loss: 1.96777137e-07
Iter: 1736 loss: 1.97411026e-07
Iter: 1737 loss: 1.96765185e-07
Iter: 1738 loss: 1.96683118e-07
Iter: 1739 loss: 1.96541407e-07
Iter: 1740 loss: 1.99333812e-07
Iter: 1741 loss: 1.96534103e-07
Iter: 1742 loss: 1.96404059e-07
Iter: 1743 loss: 1.96411349e-07
Iter: 1744 loss: 1.96282173e-07
Iter: 1745 loss: 1.96271728e-07
Iter: 1746 loss: 1.96197647e-07
Iter: 1747 loss: 1.96035245e-07
Iter: 1748 loss: 1.95909422e-07
Iter: 1749 loss: 1.9584175e-07
Iter: 1750 loss: 1.95652461e-07
Iter: 1751 loss: 1.96706992e-07
Iter: 1752 loss: 1.95613694e-07
Iter: 1753 loss: 1.9545314e-07
Iter: 1754 loss: 1.96549294e-07
Iter: 1755 loss: 1.95432307e-07
Iter: 1756 loss: 1.95333854e-07
Iter: 1757 loss: 1.95333683e-07
Iter: 1758 loss: 1.95238741e-07
Iter: 1759 loss: 1.95164546e-07
Iter: 1760 loss: 1.95136082e-07
Iter: 1761 loss: 1.95061119e-07
Iter: 1762 loss: 1.95062e-07
Iter: 1763 loss: 1.9500014e-07
Iter: 1764 loss: 1.94906406e-07
Iter: 1765 loss: 1.94905567e-07
Iter: 1766 loss: 1.94837639e-07
Iter: 1767 loss: 1.94831301e-07
Iter: 1768 loss: 1.94745809e-07
Iter: 1769 loss: 1.94575165e-07
Iter: 1770 loss: 1.96856817e-07
Iter: 1771 loss: 1.94555611e-07
Iter: 1772 loss: 1.94388235e-07
Iter: 1773 loss: 1.95315877e-07
Iter: 1774 loss: 1.94370273e-07
Iter: 1775 loss: 1.94238538e-07
Iter: 1776 loss: 1.95074904e-07
Iter: 1777 loss: 1.94209605e-07
Iter: 1778 loss: 1.94081053e-07
Iter: 1779 loss: 1.93992776e-07
Iter: 1780 loss: 1.93941e-07
Iter: 1781 loss: 1.9382901e-07
Iter: 1782 loss: 1.94153188e-07
Iter: 1783 loss: 1.937983e-07
Iter: 1784 loss: 1.93654529e-07
Iter: 1785 loss: 1.94000776e-07
Iter: 1786 loss: 1.93619258e-07
Iter: 1787 loss: 1.93475501e-07
Iter: 1788 loss: 1.94038421e-07
Iter: 1789 loss: 1.93439632e-07
Iter: 1790 loss: 1.93293118e-07
Iter: 1791 loss: 1.94512666e-07
Iter: 1792 loss: 1.93278808e-07
Iter: 1793 loss: 1.93205537e-07
Iter: 1794 loss: 1.93199384e-07
Iter: 1795 loss: 1.93121e-07
Iter: 1796 loss: 1.92995628e-07
Iter: 1797 loss: 1.93630427e-07
Iter: 1798 loss: 1.92986676e-07
Iter: 1799 loss: 1.92838939e-07
Iter: 1800 loss: 1.9281859e-07
Iter: 1801 loss: 1.92724315e-07
Iter: 1802 loss: 1.92619609e-07
Iter: 1803 loss: 1.92613413e-07
Iter: 1804 loss: 1.92541023e-07
Iter: 1805 loss: 1.92430065e-07
Iter: 1806 loss: 1.92421695e-07
Iter: 1807 loss: 1.92291509e-07
Iter: 1808 loss: 1.92404016e-07
Iter: 1809 loss: 1.92220966e-07
Iter: 1810 loss: 1.92056362e-07
Iter: 1811 loss: 1.93939869e-07
Iter: 1812 loss: 1.9205757e-07
Iter: 1813 loss: 1.91957497e-07
Iter: 1814 loss: 1.91928521e-07
Iter: 1815 loss: 1.91867301e-07
Iter: 1816 loss: 1.91737598e-07
Iter: 1817 loss: 1.91609871e-07
Iter: 1818 loss: 1.91570479e-07
Iter: 1819 loss: 1.91353578e-07
Iter: 1820 loss: 1.92115891e-07
Iter: 1821 loss: 1.91299364e-07
Iter: 1822 loss: 1.91148445e-07
Iter: 1823 loss: 1.92633152e-07
Iter: 1824 loss: 1.91151116e-07
Iter: 1825 loss: 1.91047832e-07
Iter: 1826 loss: 1.92051601e-07
Iter: 1827 loss: 1.91041522e-07
Iter: 1828 loss: 1.90941776e-07
Iter: 1829 loss: 1.91034459e-07
Iter: 1830 loss: 1.90881309e-07
Iter: 1831 loss: 1.90803803e-07
Iter: 1832 loss: 1.91021257e-07
Iter: 1833 loss: 1.90772241e-07
Iter: 1834 loss: 1.90685952e-07
Iter: 1835 loss: 1.91158136e-07
Iter: 1836 loss: 1.90666171e-07
Iter: 1837 loss: 1.90599934e-07
Iter: 1838 loss: 1.90667436e-07
Iter: 1839 loss: 1.90574738e-07
Iter: 1840 loss: 1.90468924e-07
Iter: 1841 loss: 1.90730361e-07
Iter: 1842 loss: 1.90425084e-07
Iter: 1843 loss: 1.90336308e-07
Iter: 1844 loss: 1.90271862e-07
Iter: 1845 loss: 1.90227e-07
Iter: 1846 loss: 1.90095477e-07
Iter: 1847 loss: 1.9002735e-07
Iter: 1848 loss: 1.89958257e-07
Iter: 1849 loss: 1.89855243e-07
Iter: 1850 loss: 1.89838929e-07
Iter: 1851 loss: 1.89734209e-07
Iter: 1852 loss: 1.89747652e-07
Iter: 1853 loss: 1.89652383e-07
Iter: 1854 loss: 1.89539151e-07
Iter: 1855 loss: 1.89702178e-07
Iter: 1856 loss: 1.89482947e-07
Iter: 1857 loss: 1.89364727e-07
Iter: 1858 loss: 1.89370951e-07
Iter: 1859 loss: 1.8927291e-07
Iter: 1860 loss: 1.89143108e-07
Iter: 1861 loss: 1.89352335e-07
Iter: 1862 loss: 1.8907e-07
Iter: 1863 loss: 1.88933413e-07
Iter: 1864 loss: 1.88932361e-07
Iter: 1865 loss: 1.88821076e-07
Iter: 1866 loss: 1.89450532e-07
Iter: 1867 loss: 1.88791432e-07
Iter: 1868 loss: 1.88743599e-07
Iter: 1869 loss: 1.88621684e-07
Iter: 1870 loss: 1.88617591e-07
Iter: 1871 loss: 1.88551354e-07
Iter: 1872 loss: 1.88535211e-07
Iter: 1873 loss: 1.88489e-07
Iter: 1874 loss: 1.88381676e-07
Iter: 1875 loss: 1.90743805e-07
Iter: 1876 loss: 1.8837288e-07
Iter: 1877 loss: 1.88264679e-07
Iter: 1878 loss: 1.89827887e-07
Iter: 1879 loss: 1.88258227e-07
Iter: 1880 loss: 1.88200644e-07
Iter: 1881 loss: 1.88101978e-07
Iter: 1882 loss: 1.8809672e-07
Iter: 1883 loss: 1.87989301e-07
Iter: 1884 loss: 1.88908359e-07
Iter: 1885 loss: 1.87981612e-07
Iter: 1886 loss: 1.87850503e-07
Iter: 1887 loss: 1.88443522e-07
Iter: 1888 loss: 1.87840698e-07
Iter: 1889 loss: 1.87732297e-07
Iter: 1890 loss: 1.87594708e-07
Iter: 1891 loss: 1.87576092e-07
Iter: 1892 loss: 1.87424547e-07
Iter: 1893 loss: 1.87711663e-07
Iter: 1894 loss: 1.87342749e-07
Iter: 1895 loss: 1.87159443e-07
Iter: 1896 loss: 1.87909762e-07
Iter: 1897 loss: 1.87128393e-07
Iter: 1898 loss: 1.87011381e-07
Iter: 1899 loss: 1.87140472e-07
Iter: 1900 loss: 1.86933335e-07
Iter: 1901 loss: 1.86871205e-07
Iter: 1902 loss: 1.86852617e-07
Iter: 1903 loss: 1.86784945e-07
Iter: 1904 loss: 1.86772155e-07
Iter: 1905 loss: 1.86719149e-07
Iter: 1906 loss: 1.8663728e-07
Iter: 1907 loss: 1.86633926e-07
Iter: 1908 loss: 1.86566893e-07
Iter: 1909 loss: 1.86487796e-07
Iter: 1910 loss: 1.86477479e-07
Iter: 1911 loss: 1.86429617e-07
Iter: 1912 loss: 1.86359529e-07
Iter: 1913 loss: 1.86368865e-07
Iter: 1914 loss: 1.86229215e-07
Iter: 1915 loss: 1.86735349e-07
Iter: 1916 loss: 1.86186071e-07
Iter: 1917 loss: 1.8608327e-07
Iter: 1918 loss: 1.85921891e-07
Iter: 1919 loss: 1.8593532e-07
Iter: 1920 loss: 1.85779427e-07
Iter: 1921 loss: 1.87228409e-07
Iter: 1922 loss: 1.85784586e-07
Iter: 1923 loss: 1.85642563e-07
Iter: 1924 loss: 1.86253814e-07
Iter: 1925 loss: 1.85625595e-07
Iter: 1926 loss: 1.85502e-07
Iter: 1927 loss: 1.85625154e-07
Iter: 1928 loss: 1.85434e-07
Iter: 1929 loss: 1.85347233e-07
Iter: 1930 loss: 1.85212983e-07
Iter: 1931 loss: 1.85220244e-07
Iter: 1932 loss: 1.85074015e-07
Iter: 1933 loss: 1.86638943e-07
Iter: 1934 loss: 1.85081419e-07
Iter: 1935 loss: 1.84987073e-07
Iter: 1936 loss: 1.85913407e-07
Iter: 1937 loss: 1.84989858e-07
Iter: 1938 loss: 1.84896493e-07
Iter: 1939 loss: 1.84943843e-07
Iter: 1940 loss: 1.84828338e-07
Iter: 1941 loss: 1.84757411e-07
Iter: 1942 loss: 1.84684893e-07
Iter: 1943 loss: 1.84655136e-07
Iter: 1944 loss: 1.84530421e-07
Iter: 1945 loss: 1.8453224e-07
Iter: 1946 loss: 1.84463303e-07
Iter: 1947 loss: 1.8460166e-07
Iter: 1948 loss: 1.84431585e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6
+ date
Thu Oct 22 04:08:32 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/500_500_500_500_1 --function f1 --psi 2 --phi 1.6 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c8280598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c82bcc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c82bc0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c81d3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c81df598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c81df840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c815c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c80b0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c80b0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c8110598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c80b0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c027c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c027ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c8029620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c8029400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c020c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c020c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c8057730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c016eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c0150d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c0150f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c01fdae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c01ba8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c0143510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c0143b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c004c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c00de598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c00f90d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54b06772f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54b0699ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c0045620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54b06086a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54b0608730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54b05f5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54c009a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f54b05a2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.02396752
test_loss: 0.022404699
train_loss: 0.008863793
test_loss: 0.0090092765
train_loss: 0.0055000233
test_loss: 0.005710873
train_loss: 0.004434407
test_loss: 0.0045338566
train_loss: 0.0036706484
test_loss: 0.004004798
train_loss: 0.003215829
test_loss: 0.0037420169
train_loss: 0.0036924796
test_loss: 0.0036589638
train_loss: 0.0030061922
test_loss: 0.0035067054
train_loss: 0.003131394
test_loss: 0.0037317742
train_loss: 0.0032420112
test_loss: 0.0036028007
train_loss: 0.0031834727
test_loss: 0.0034874557
train_loss: 0.0029917578
test_loss: 0.0034722602
train_loss: 0.0030317612
test_loss: 0.0034118402
train_loss: 0.0029314065
test_loss: 0.0036657867
train_loss: 0.0028649608
test_loss: 0.0032554448
train_loss: 0.00280435
test_loss: 0.0033617436
train_loss: 0.0025922258
test_loss: 0.003200696
train_loss: 0.002901686
test_loss: 0.0031100737
train_loss: 0.003191846
test_loss: 0.0032830022
train_loss: 0.0028434652
test_loss: 0.0036494683
train_loss: 0.0029380373
test_loss: 0.0034644746
train_loss: 0.002724166
test_loss: 0.0035837207
train_loss: 0.0029896284
test_loss: 0.0032906637
train_loss: 0.0029405213
test_loss: 0.0032300262
train_loss: 0.0027719927
test_loss: 0.0031746442
train_loss: 0.0025615143
test_loss: 0.0031377187
train_loss: 0.002632151
test_loss: 0.0032269976
train_loss: 0.0026342815
test_loss: 0.0032710852
train_loss: 0.0027329451
test_loss: 0.0031795923
train_loss: 0.0025585291
test_loss: 0.0029959404
train_loss: 0.0028842716
test_loss: 0.0032931762
train_loss: 0.0026717205
test_loss: 0.0029605667
train_loss: 0.0027377852
test_loss: 0.0032981455
train_loss: 0.0027957095
test_loss: 0.0030348967
train_loss: 0.002709305
test_loss: 0.0029888232
train_loss: 0.0027199327
test_loss: 0.0031169571
train_loss: 0.002808989
test_loss: 0.0032392342
train_loss: 0.0026648564
test_loss: 0.0030309467
train_loss: 0.0024927356
test_loss: 0.002930652
train_loss: 0.0026514072
test_loss: 0.003151601
train_loss: 0.0026725507
test_loss: 0.0031436037
train_loss: 0.0025108329
test_loss: 0.0029842865
train_loss: 0.0024418216
test_loss: 0.0030527678
train_loss: 0.0025892658
test_loss: 0.0028785728
train_loss: 0.0024218704
test_loss: 0.0029142718
train_loss: 0.0027224936
test_loss: 0.0031243118
train_loss: 0.0025918125
test_loss: 0.0030061193
train_loss: 0.0027506778
test_loss: 0.0029058075
train_loss: 0.0024095073
test_loss: 0.0030227117
train_loss: 0.002589673
test_loss: 0.0028844862
train_loss: 0.002677286
test_loss: 0.003002954
train_loss: 0.0026945719
test_loss: 0.003084269
train_loss: 0.002572372
test_loss: 0.0029285592
train_loss: 0.002493329
test_loss: 0.0030684422
train_loss: 0.0025139519
test_loss: 0.0030036184
train_loss: 0.0025253205
test_loss: 0.0030019355
train_loss: 0.002623412
test_loss: 0.0029062
train_loss: 0.0025254036
test_loss: 0.0028608942
train_loss: 0.0025395744
test_loss: 0.003000886
train_loss: 0.0025506467
test_loss: 0.0030489543
train_loss: 0.0023899826
test_loss: 0.002848268
train_loss: 0.00253913
test_loss: 0.0029591902
train_loss: 0.0025032314
test_loss: 0.002905499
train_loss: 0.0025448855
test_loss: 0.0030823143
train_loss: 0.002492022
test_loss: 0.0029099695
train_loss: 0.002370324
test_loss: 0.0029339867
train_loss: 0.0028812198
test_loss: 0.0031797986
train_loss: 0.0024783765
test_loss: 0.0029880044
train_loss: 0.0025768448
test_loss: 0.0031244792
train_loss: 0.0024201865
test_loss: 0.0030294424
train_loss: 0.0025759994
test_loss: 0.0029241266
train_loss: 0.0026417016
test_loss: 0.0029826162
train_loss: 0.0025665816
test_loss: 0.00290221
train_loss: 0.002632605
test_loss: 0.0032490396
train_loss: 0.0026633441
test_loss: 0.0030085205
train_loss: 0.0024054928
test_loss: 0.0028419285
train_loss: 0.0022967022
test_loss: 0.0027723098
train_loss: 0.0025835186
test_loss: 0.0028829488
train_loss: 0.00237498
test_loss: 0.0029187594
train_loss: 0.0024893254
test_loss: 0.0030301237
train_loss: 0.0025399318
test_loss: 0.0029487412
train_loss: 0.0024668903
test_loss: 0.0030427552
train_loss: 0.0022384163
test_loss: 0.0027941496
train_loss: 0.0024984519
test_loss: 0.002994975
train_loss: 0.0024496564
test_loss: 0.002826945
train_loss: 0.0023514284
test_loss: 0.0029280689
train_loss: 0.0024480652
test_loss: 0.002859847
train_loss: 0.0028359087
test_loss: 0.003295287
train_loss: 0.0023316322
test_loss: 0.0029402238
train_loss: 0.0024808086
test_loss: 0.0027774028
train_loss: 0.0024160529
test_loss: 0.0029440746
train_loss: 0.0027050942
test_loss: 0.0029424345
train_loss: 0.002470598
test_loss: 0.0029759596
train_loss: 0.0023602266
test_loss: 0.0029084664
train_loss: 0.002599465
test_loss: 0.0030660639
train_loss: 0.002551675
test_loss: 0.0028740151
train_loss: 0.002398021
test_loss: 0.0029569257
train_loss: 0.0027023854
test_loss: 0.002936035
train_loss: 0.002551767
test_loss: 0.0028621412
train_loss: 0.0024352309
test_loss: 0.00285093
train_loss: 0.002590613
test_loss: 0.0028672381
train_loss: 0.002528281
test_loss: 0.0030823757
train_loss: 0.0027546687
test_loss: 0.0030501997
train_loss: 0.0024638837
test_loss: 0.0029597168
train_loss: 0.0024939303
test_loss: 0.003050852
train_loss: 0.0025008696
test_loss: 0.003048992
train_loss: 0.0024756435
test_loss: 0.0030650152
train_loss: 0.002260532
test_loss: 0.0029615182
train_loss: 0.002539267
test_loss: 0.0027393394
train_loss: 0.0023957656
test_loss: 0.003016399
train_loss: 0.0028127301
test_loss: 0.0030558247
train_loss: 0.002469517
test_loss: 0.0029521028
train_loss: 0.0025011094
test_loss: 0.0028373185
train_loss: 0.0024947636
test_loss: 0.0028268243
train_loss: 0.002588715
test_loss: 0.002922854
train_loss: 0.0024490464
test_loss: 0.002776667
train_loss: 0.0023986537
test_loss: 0.0028458873
train_loss: 0.0022487661
test_loss: 0.0027998614
train_loss: 0.0022825566
test_loss: 0.0028901552
train_loss: 0.0025326384
test_loss: 0.002893747
train_loss: 0.0026274626
test_loss: 0.002924133
train_loss: 0.0023125845
test_loss: 0.002908609
train_loss: 0.0024833796
test_loss: 0.002852452
train_loss: 0.0022823445
test_loss: 0.0029866365
train_loss: 0.0024939943
test_loss: 0.0029534225
train_loss: 0.0023073407
test_loss: 0.0029575364
train_loss: 0.0024983848
test_loss: 0.002944519
train_loss: 0.0022140308
test_loss: 0.0028116538
train_loss: 0.0023931493
test_loss: 0.0028139756
train_loss: 0.002587163
test_loss: 0.0029522586
train_loss: 0.0024181046
test_loss: 0.0029041562
train_loss: 0.0025484883
test_loss: 0.0030530118
train_loss: 0.002525986
test_loss: 0.0029674189
train_loss: 0.0023945975
test_loss: 0.0028719844
train_loss: 0.0021817335
test_loss: 0.0028234902
train_loss: 0.0024349377
test_loss: 0.0028493223
train_loss: 0.002392413
test_loss: 0.002852505
train_loss: 0.0023839097
test_loss: 0.0029999875
train_loss: 0.0023663896
test_loss: 0.0029781398
train_loss: 0.0022407426
test_loss: 0.0027374732
train_loss: 0.0023403238
test_loss: 0.0028012756
train_loss: 0.0026830488
test_loss: 0.003120805
train_loss: 0.0024123867
test_loss: 0.0031153245
train_loss: 0.0025396515
test_loss: 0.0029102673
train_loss: 0.0026358976
test_loss: 0.0031807267
train_loss: 0.002417746
test_loss: 0.0028115958
train_loss: 0.002268628
test_loss: 0.002765889
train_loss: 0.0024165334
test_loss: 0.003331867
train_loss: 0.00244133
test_loss: 0.0029902644
train_loss: 0.0023265562
test_loss: 0.002866699
train_loss: 0.002467784
test_loss: 0.002803729
train_loss: 0.0024785784
test_loss: 0.0028028563
train_loss: 0.0023370087
test_loss: 0.0028996798
train_loss: 0.002467199
test_loss: 0.0031236673
train_loss: 0.00228409
test_loss: 0.0030206342
train_loss: 0.002398258
test_loss: 0.0029717071
train_loss: 0.0026885986
test_loss: 0.0031908655
train_loss: 0.0026264742
test_loss: 0.0030989773
train_loss: 0.0031094926
test_loss: 0.0029770192
train_loss: 0.0024010753
test_loss: 0.0028951722
train_loss: 0.0025200455
test_loss: 0.0028310157
train_loss: 0.0023852082
test_loss: 0.0028491798
train_loss: 0.00232769
test_loss: 0.002921699
train_loss: 0.0024915093
test_loss: 0.003024541
train_loss: 0.0025721265
test_loss: 0.0028546222
train_loss: 0.0024991154
test_loss: 0.0030669596
train_loss: 0.0025477977
test_loss: 0.0028824985
train_loss: 0.0025403837
test_loss: 0.0030169045
train_loss: 0.0023942504
test_loss: 0.0030575232
train_loss: 0.0022754697
test_loss: 0.0029441577
train_loss: 0.0025515219
test_loss: 0.0030952513
train_loss: 0.0026047295
test_loss: 0.0030492959
train_loss: 0.002424833
test_loss: 0.00297382
train_loss: 0.0024930153
test_loss: 0.0030289711
train_loss: 0.0025302684
test_loss: 0.0028408093
train_loss: 0.0024397438
test_loss: 0.0027113855
train_loss: 0.002681546
test_loss: 0.0029005443
train_loss: 0.0024407075
test_loss: 0.0029943485
train_loss: 0.0025179517
test_loss: 0.0027790267
train_loss: 0.002593648
test_loss: 0.0029656657
train_loss: 0.002445051
test_loss: 0.0027139501
train_loss: 0.0024369108
test_loss: 0.0029990615
train_loss: 0.002421077
test_loss: 0.002859757
train_loss: 0.0023744917
test_loss: 0.0029117675
train_loss: 0.0023499723
test_loss: 0.0028486685
train_loss: 0.0022735992
test_loss: 0.0029330011
train_loss: 0.002531494
test_loss: 0.0030934184
train_loss: 0.0024230161
test_loss: 0.00294591
train_loss: 0.0025441675
test_loss: 0.0028972097
train_loss: 0.0023356103
test_loss: 0.0030027407
train_loss: 0.0025923536
test_loss: 0.002951323
train_loss: 0.0025782976
test_loss: 0.0029937201
train_loss: 0.0024309284
test_loss: 0.0029545391
train_loss: 0.0023949775
test_loss: 0.0028097387
train_loss: 0.0027126938
test_loss: 0.0029066422
train_loss: 0.002323317
test_loss: 0.0026928275
train_loss: 0.0022664578
test_loss: 0.0027157268
train_loss: 0.0023799145
test_loss: 0.0028061124
train_loss: 0.0021929406
test_loss: 0.0028948074
train_loss: 0.0023752158
test_loss: 0.002900229
train_loss: 0.0024053832
test_loss: 0.003036712
train_loss: 0.0023759736
test_loss: 0.0029005646
train_loss: 0.0024371408
test_loss: 0.0031550059
train_loss: 0.0024448454
test_loss: 0.0028917189
train_loss: 0.0024331245
test_loss: 0.0028622444
train_loss: 0.0023098213
test_loss: 0.0028881978
train_loss: 0.0024303072
test_loss: 0.0027278338
train_loss: 0.002330646
test_loss: 0.0029481738
train_loss: 0.0022329472
test_loss: 0.0029175028
train_loss: 0.0023411093
test_loss: 0.0027247802
train_loss: 0.0023823706
test_loss: 0.0027711587
train_loss: 0.0023683682
test_loss: 0.0028898495
train_loss: 0.0024511742
test_loss: 0.0029278444
train_loss: 0.0025335697
test_loss: 0.002901268
train_loss: 0.0025389125
test_loss: 0.0030447305
train_loss: 0.0023432889
test_loss: 0.003255446
train_loss: 0.0024315896
test_loss: 0.002907062
train_loss: 0.0025270781
test_loss: 0.003062187
train_loss: 0.0025996943
test_loss: 0.0029480746
train_loss: 0.0024467611
test_loss: 0.0028114165
train_loss: 0.002146023
test_loss: 0.0028281803
train_loss: 0.002378393
test_loss: 0.0028592872
train_loss: 0.002610572
test_loss: 0.0027927246
train_loss: 0.002707134
test_loss: 0.0031091948
train_loss: 0.0023431396
test_loss: 0.003058558
train_loss: 0.0028265733
test_loss: 0.0029026512
train_loss: 0.0025204162
test_loss: 0.002826118
train_loss: 0.0023466605
test_loss: 0.0028047655
train_loss: 0.0023101417
test_loss: 0.0027976744
train_loss: 0.0027901554
test_loss: 0.0030856999
train_loss: 0.0030877953
test_loss: 0.0030764781
train_loss: 0.002450916
test_loss: 0.0030484328
train_loss: 0.0024619089
test_loss: 0.0029468331
train_loss: 0.002344122
test_loss: 0.0028347243
train_loss: 0.0023605912
test_loss: 0.0028013804
train_loss: 0.0025187577
test_loss: 0.0028345254
train_loss: 0.002721387
test_loss: 0.0029705542
train_loss: 0.0023786714
test_loss: 0.0029021783
train_loss: 0.0023070294
test_loss: 0.0030192358
train_loss: 0.0022645863
test_loss: 0.0028151863
train_loss: 0.0022617017
test_loss: 0.0028147316
train_loss: 0.0026002594
test_loss: 0.002944854
train_loss: 0.0025436203
test_loss: 0.002948416
train_loss: 0.002395589
test_loss: 0.0029237883
train_loss: 0.0024303712
test_loss: 0.0026927923
train_loss: 0.0021702484
test_loss: 0.0026385065
train_loss: 0.0025337534
test_loss: 0.002746068
train_loss: 0.0021290788
test_loss: 0.0027345929
train_loss: 0.0021893797
test_loss: 0.0028393602
train_loss: 0.0024739762
test_loss: 0.002944829
train_loss: 0.002490181
test_loss: 0.0029992312
train_loss: 0.0023142435
test_loss: 0.0028854723
train_loss: 0.0024534871
test_loss: 0.0028777157
train_loss: 0.0024108181
test_loss: 0.002943106
train_loss: 0.0023586147
test_loss: 0.0029238423
train_loss: 0.002354356
test_loss: 0.0029065092
train_loss: 0.0022783168
test_loss: 0.0028710098
train_loss: 0.0024142875
test_loss: 0.002768144
train_loss: 0.0025367062
test_loss: 0.0028462135
train_loss: 0.0025301343
test_loss: 0.0028710773
train_loss: 0.0024262907
test_loss: 0.0028523426
train_loss: 0.0024088076
test_loss: 0.0029707225
train_loss: 0.0023205718
test_loss: 0.0028370533
train_loss: 0.0023224412
test_loss: 0.002957706
train_loss: 0.0023243537
test_loss: 0.0029157111
train_loss: 0.0024314597
test_loss: 0.0029085716
train_loss: 0.0022457442
test_loss: 0.0026496241
train_loss: 0.0022930473
test_loss: 0.0028097578
train_loss: 0.0022606058
test_loss: 0.0027682774
train_loss: 0.0023676541
test_loss: 0.0027654676
train_loss: 0.0022549438
test_loss: 0.0027657254
train_loss: 0.0022809138
test_loss: 0.002786127
train_loss: 0.0024118125
test_loss: 0.0028333464
train_loss: 0.0022973362
test_loss: 0.0028054614
train_loss: 0.0021375983
test_loss: 0.0027337861
train_loss: 0.0023404863
test_loss: 0.002809053
train_loss: 0.002347033
test_loss: 0.0028375566
train_loss: 0.002247651
test_loss: 0.0027954623
train_loss: 0.0021093248
test_loss: 0.0026681677
train_loss: 0.0024399902
test_loss: 0.0026581746
train_loss: 0.0023131915
test_loss: 0.0028088032
train_loss: 0.0023529564
test_loss: 0.002731115
train_loss: 0.002591113
test_loss: 0.0029955271
train_loss: 0.0023992613
test_loss: 0.002844104
train_loss: 0.002371659
test_loss: 0.0028461327
train_loss: 0.0023221914
test_loss: 0.0029247974
train_loss: 0.002257294
test_loss: 0.0028591522
train_loss: 0.0022060927
test_loss: 0.0027339738
train_loss: 0.0021979716
test_loss: 0.0028021403
train_loss: 0.0025352202
test_loss: 0.003081261
train_loss: 0.0026899586
test_loss: 0.0028574085
train_loss: 0.002405571
test_loss: 0.00329392
train_loss: 0.0025063201
test_loss: 0.00280828
train_loss: 0.0025130599
test_loss: 0.0030099223
train_loss: 0.0025746783
test_loss: 0.0028322157
train_loss: 0.0022504404
test_loss: 0.0027687245
train_loss: 0.0024165113
test_loss: 0.0028902607
train_loss: 0.0023053526
test_loss: 0.0029003914
train_loss: 0.002329666
test_loss: 0.00296759
train_loss: 0.0022708364
test_loss: 0.0029081546
train_loss: 0.0024849845
test_loss: 0.0028326765
train_loss: 0.002201891
test_loss: 0.0026655467
train_loss: 0.002336994
test_loss: 0.0029033774
train_loss: 0.0021390582
test_loss: 0.0028012034
train_loss: 0.0024268043
test_loss: 0.0028689273
train_loss: 0.0024271496
test_loss: 0.0027842945
train_loss: 0.0022030617
test_loss: 0.002818695
train_loss: 0.0024383464
test_loss: 0.0027597465
train_loss: 0.002429208
test_loss: 0.0028764827
train_loss: 0.002535687
test_loss: 0.0030345689
train_loss: 0.0022410916
test_loss: 0.002669708
train_loss: 0.002235775
test_loss: 0.002840843
train_loss: 0.0021255477
test_loss: 0.002712069
train_loss: 0.002282149
test_loss: 0.0027432516
train_loss: 0.0026189378
test_loss: 0.0031516245
train_loss: 0.0022624126
test_loss: 0.0029054175
train_loss: 0.002306573
test_loss: 0.0027243085
train_loss: 0.00219313
test_loss: 0.0027488223
train_loss: 0.0022701595
test_loss: 0.0029346975
train_loss: 0.0025068016
test_loss: 0.0029699563
train_loss: 0.0023703938
test_loss: 0.0030232
train_loss: 0.002539013
test_loss: 0.002910877
train_loss: 0.0023108525
test_loss: 0.0028872518
train_loss: 0.0024306003
test_loss: 0.002825494
train_loss: 0.002193765
test_loss: 0.0028232725
train_loss: 0.0022747105
test_loss: 0.002865107
train_loss: 0.002268021
test_loss: 0.0025897517
train_loss: 0.0022174655
test_loss: 0.0027139655
train_loss: 0.0025821116
test_loss: 0.0029384221
train_loss: 0.0022690552
test_loss: 0.0028262753
train_loss: 0.002200554
test_loss: 0.0027420504
train_loss: 0.0023616643
test_loss: 0.0030636902
train_loss: 0.0025358906
test_loss: 0.002844314
train_loss: 0.00267162
test_loss: 0.0029162902
train_loss: 0.0022836141
test_loss: 0.0027650872
train_loss: 0.0021413146
test_loss: 0.002792793
train_loss: 0.0023203143
test_loss: 0.002797549
train_loss: 0.0023364157
test_loss: 0.0029285701
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0023127627
test_loss: 0.0028267852
train_loss: 0.0022128741
test_loss: 0.002928953
train_loss: 0.0022030617
test_loss: 0.002807617
train_loss: 0.0022115647
test_loss: 0.0027236037
train_loss: 0.002602494
test_loss: 0.002926804
train_loss: 0.0022678305
test_loss: 0.0028991
train_loss: 0.0023948846
test_loss: 0.0029083297
train_loss: 0.0023073112
test_loss: 0.002934928
train_loss: 0.0022398727
test_loss: 0.0027466766
train_loss: 0.0022437377
test_loss: 0.0027752265
train_loss: 0.0023875758
test_loss: 0.002760709
train_loss: 0.0021860194
test_loss: 0.0027692511
train_loss: 0.0023738004
test_loss: 0.0030028722
train_loss: 0.0022699405
test_loss: 0.0027849055
train_loss: 0.0023913777
test_loss: 0.0029597946
train_loss: 0.002332964
test_loss: 0.002658954
train_loss: 0.002320135
test_loss: 0.002768834
train_loss: 0.0024989988
test_loss: 0.0027574527
train_loss: 0.0025235717
test_loss: 0.0028384642
train_loss: 0.0024298825
test_loss: 0.0027730642
train_loss: 0.0025561433
test_loss: 0.0028379953
train_loss: 0.0022234195
test_loss: 0.002763829
train_loss: 0.0023274915
test_loss: 0.0028655382
train_loss: 0.002229907
test_loss: 0.002742804
train_loss: 0.0021461893
test_loss: 0.0028232422
train_loss: 0.0024120957
test_loss: 0.0029063819
train_loss: 0.002376657
test_loss: 0.0028977136
train_loss: 0.0025004435
test_loss: 0.0031310748
train_loss: 0.002356086
test_loss: 0.0028653592
train_loss: 0.0021008097
test_loss: 0.002705413
train_loss: 0.0023042269
test_loss: 0.002687475
train_loss: 0.0021374994
test_loss: 0.0027121503
train_loss: 0.0022360764
test_loss: 0.002811631
train_loss: 0.0022009443
test_loss: 0.0029052154
train_loss: 0.0022326873
test_loss: 0.0027776517
train_loss: 0.0021276646
test_loss: 0.0025846825
train_loss: 0.0022062461
test_loss: 0.0028394863
train_loss: 0.002488727
test_loss: 0.002795054
train_loss: 0.0023236885
test_loss: 0.0029256148
train_loss: 0.0022785058
test_loss: 0.0026962368
train_loss: 0.0022009416
test_loss: 0.0029519652
train_loss: 0.0023915237
test_loss: 0.002735729
train_loss: 0.002249586
test_loss: 0.0028349962
train_loss: 0.0024341713
test_loss: 0.0027815895
train_loss: 0.002295445
test_loss: 0.0029997008
train_loss: 0.0022318878
test_loss: 0.0027836545
train_loss: 0.0022684415
test_loss: 0.0027654697
train_loss: 0.002297614
test_loss: 0.0029054831
train_loss: 0.0024282557
test_loss: 0.0028104319
train_loss: 0.0021494878
test_loss: 0.0025857566
train_loss: 0.0021675727
test_loss: 0.0027064476
train_loss: 0.0022356126
test_loss: 0.0028494613
train_loss: 0.002407244
test_loss: 0.0028013731
train_loss: 0.0023653351
test_loss: 0.0028783958
train_loss: 0.0023691845
test_loss: 0.002983958
train_loss: 0.002755682
test_loss: 0.0030955093
train_loss: 0.002538149
test_loss: 0.003046924
train_loss: 0.0022566733
test_loss: 0.002900818
train_loss: 0.0024397026
test_loss: 0.0027216794
train_loss: 0.002306197
test_loss: 0.002756237
train_loss: 0.0023160009
test_loss: 0.0028808713
train_loss: 0.002317356
test_loss: 0.0028772682
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.6 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf164c72f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf164c7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf164a2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf163c6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf1634b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf16499b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf1633a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf162e3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf162e31e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf162e32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf1633a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf1627b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf1629d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf16238a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf161d18c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf161fcc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf161fb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf16197730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf1616d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf161fb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf16115c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf1616d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdec341f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf1616d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf16115598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdec33dd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdec339c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdec33c0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdec3356620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdec33c0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdec3311510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdec3311ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdec3311b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdec333a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde9c256ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde9c207268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.04993842e-05
Iter: 2 loss: 8.31548186e-06
Iter: 3 loss: 8.28683915e-06
Iter: 4 loss: 7.38341896e-06
Iter: 5 loss: 9.86189207e-06
Iter: 6 loss: 7.08963034e-06
Iter: 7 loss: 6.58777481e-06
Iter: 8 loss: 1.08314334e-05
Iter: 9 loss: 6.55911208e-06
Iter: 10 loss: 6.16824036e-06
Iter: 11 loss: 6.9133107e-06
Iter: 12 loss: 6.00359817e-06
Iter: 13 loss: 5.75370632e-06
Iter: 14 loss: 6.81263282e-06
Iter: 15 loss: 5.70096e-06
Iter: 16 loss: 5.45155945e-06
Iter: 17 loss: 5.15680949e-06
Iter: 18 loss: 5.12493762e-06
Iter: 19 loss: 4.90469347e-06
Iter: 20 loss: 6.20072842e-06
Iter: 21 loss: 4.87616626e-06
Iter: 22 loss: 4.64786854e-06
Iter: 23 loss: 5.6043018e-06
Iter: 24 loss: 4.59928833e-06
Iter: 25 loss: 4.39620317e-06
Iter: 26 loss: 4.62717708e-06
Iter: 27 loss: 4.28707335e-06
Iter: 28 loss: 4.06186518e-06
Iter: 29 loss: 4.14691658e-06
Iter: 30 loss: 3.90490732e-06
Iter: 31 loss: 3.69788722e-06
Iter: 32 loss: 4.45768273e-06
Iter: 33 loss: 3.64666107e-06
Iter: 34 loss: 3.44682303e-06
Iter: 35 loss: 4.53111898e-06
Iter: 36 loss: 3.41713348e-06
Iter: 37 loss: 3.29324234e-06
Iter: 38 loss: 3.30278135e-06
Iter: 39 loss: 3.19680885e-06
Iter: 40 loss: 3.06160155e-06
Iter: 41 loss: 3.06143738e-06
Iter: 42 loss: 3.00428019e-06
Iter: 43 loss: 3.00325701e-06
Iter: 44 loss: 2.95241489e-06
Iter: 45 loss: 2.89324e-06
Iter: 46 loss: 2.8864356e-06
Iter: 47 loss: 2.80276163e-06
Iter: 48 loss: 3.66800168e-06
Iter: 49 loss: 2.80041058e-06
Iter: 50 loss: 2.74902072e-06
Iter: 51 loss: 2.64339633e-06
Iter: 52 loss: 4.5143729e-06
Iter: 53 loss: 2.64140817e-06
Iter: 54 loss: 2.54696124e-06
Iter: 55 loss: 3.92800257e-06
Iter: 56 loss: 2.5467707e-06
Iter: 57 loss: 2.47031835e-06
Iter: 58 loss: 2.44351668e-06
Iter: 59 loss: 2.40021359e-06
Iter: 60 loss: 2.31258628e-06
Iter: 61 loss: 2.70776036e-06
Iter: 62 loss: 2.29563e-06
Iter: 63 loss: 2.24397832e-06
Iter: 64 loss: 2.43927661e-06
Iter: 65 loss: 2.23164466e-06
Iter: 66 loss: 2.16155036e-06
Iter: 67 loss: 2.2751567e-06
Iter: 68 loss: 2.12938085e-06
Iter: 69 loss: 2.0832822e-06
Iter: 70 loss: 2.08306074e-06
Iter: 71 loss: 2.046394e-06
Iter: 72 loss: 1.98736461e-06
Iter: 73 loss: 2.30849196e-06
Iter: 74 loss: 1.9786321e-06
Iter: 75 loss: 1.92900461e-06
Iter: 76 loss: 1.98228508e-06
Iter: 77 loss: 1.90166213e-06
Iter: 78 loss: 1.84769192e-06
Iter: 79 loss: 1.87315072e-06
Iter: 80 loss: 1.81132009e-06
Iter: 81 loss: 1.80989673e-06
Iter: 82 loss: 1.77943662e-06
Iter: 83 loss: 1.75745492e-06
Iter: 84 loss: 1.83237501e-06
Iter: 85 loss: 1.75161586e-06
Iter: 86 loss: 1.73443516e-06
Iter: 87 loss: 1.74145976e-06
Iter: 88 loss: 1.72257035e-06
Iter: 89 loss: 1.68749671e-06
Iter: 90 loss: 1.64288417e-06
Iter: 91 loss: 1.63966035e-06
Iter: 92 loss: 1.6102988e-06
Iter: 93 loss: 1.79932704e-06
Iter: 94 loss: 1.60700347e-06
Iter: 95 loss: 1.57488421e-06
Iter: 96 loss: 1.6465558e-06
Iter: 97 loss: 1.56262706e-06
Iter: 98 loss: 1.53655969e-06
Iter: 99 loss: 1.56994929e-06
Iter: 100 loss: 1.52326925e-06
Iter: 101 loss: 1.49319499e-06
Iter: 102 loss: 1.51988934e-06
Iter: 103 loss: 1.47568107e-06
Iter: 104 loss: 1.45062438e-06
Iter: 105 loss: 1.45031174e-06
Iter: 106 loss: 1.43500529e-06
Iter: 107 loss: 1.42807289e-06
Iter: 108 loss: 1.42035628e-06
Iter: 109 loss: 1.39837675e-06
Iter: 110 loss: 1.44794581e-06
Iter: 111 loss: 1.39006499e-06
Iter: 112 loss: 1.36452786e-06
Iter: 113 loss: 1.32840603e-06
Iter: 114 loss: 1.32704895e-06
Iter: 115 loss: 1.29433602e-06
Iter: 116 loss: 1.54427494e-06
Iter: 117 loss: 1.29181649e-06
Iter: 118 loss: 1.29391537e-06
Iter: 119 loss: 1.2802044e-06
Iter: 120 loss: 1.27046712e-06
Iter: 121 loss: 1.25795759e-06
Iter: 122 loss: 1.25701627e-06
Iter: 123 loss: 1.24354438e-06
Iter: 124 loss: 1.37221514e-06
Iter: 125 loss: 1.24308485e-06
Iter: 126 loss: 1.23170094e-06
Iter: 127 loss: 1.22437518e-06
Iter: 128 loss: 1.21990206e-06
Iter: 129 loss: 1.20392133e-06
Iter: 130 loss: 1.20418554e-06
Iter: 131 loss: 1.19129868e-06
Iter: 132 loss: 1.18106357e-06
Iter: 133 loss: 1.17970581e-06
Iter: 134 loss: 1.17045488e-06
Iter: 135 loss: 1.14885074e-06
Iter: 136 loss: 1.41069268e-06
Iter: 137 loss: 1.14707973e-06
Iter: 138 loss: 1.12867929e-06
Iter: 139 loss: 1.3381175e-06
Iter: 140 loss: 1.12835551e-06
Iter: 141 loss: 1.11374334e-06
Iter: 142 loss: 1.15909302e-06
Iter: 143 loss: 1.10949168e-06
Iter: 144 loss: 1.09323923e-06
Iter: 145 loss: 1.14855766e-06
Iter: 146 loss: 1.08884797e-06
Iter: 147 loss: 1.0768199e-06
Iter: 148 loss: 1.06625509e-06
Iter: 149 loss: 1.06306948e-06
Iter: 150 loss: 1.04576259e-06
Iter: 151 loss: 1.15522903e-06
Iter: 152 loss: 1.04376113e-06
Iter: 153 loss: 1.03188813e-06
Iter: 154 loss: 1.15812122e-06
Iter: 155 loss: 1.03157845e-06
Iter: 156 loss: 1.01978355e-06
Iter: 157 loss: 1.07380322e-06
Iter: 158 loss: 1.01756541e-06
Iter: 159 loss: 1.01313594e-06
Iter: 160 loss: 1.01337901e-06
Iter: 161 loss: 1.00968907e-06
Iter: 162 loss: 1.00034845e-06
Iter: 163 loss: 1.00348586e-06
Iter: 164 loss: 9.93781327e-07
Iter: 165 loss: 9.86199666e-07
Iter: 166 loss: 1.01027422e-06
Iter: 167 loss: 9.84007102e-07
Iter: 168 loss: 9.76981e-07
Iter: 169 loss: 9.73699684e-07
Iter: 170 loss: 9.70220185e-07
Iter: 171 loss: 9.59267823e-07
Iter: 172 loss: 1.09197572e-06
Iter: 173 loss: 9.59142767e-07
Iter: 174 loss: 9.52719e-07
Iter: 175 loss: 9.44020826e-07
Iter: 176 loss: 9.4356642e-07
Iter: 177 loss: 9.31576608e-07
Iter: 178 loss: 9.8237831e-07
Iter: 179 loss: 9.29030477e-07
Iter: 180 loss: 9.1916354e-07
Iter: 181 loss: 9.81746894e-07
Iter: 182 loss: 9.18094884e-07
Iter: 183 loss: 9.07419405e-07
Iter: 184 loss: 9.1721671e-07
Iter: 185 loss: 9.01278781e-07
Iter: 186 loss: 8.93875381e-07
Iter: 187 loss: 8.90137471e-07
Iter: 188 loss: 8.86580892e-07
Iter: 189 loss: 8.84220356e-07
Iter: 190 loss: 8.81830317e-07
Iter: 191 loss: 8.76254092e-07
Iter: 192 loss: 8.73257591e-07
Iter: 193 loss: 8.70742213e-07
Iter: 194 loss: 8.64359663e-07
Iter: 195 loss: 8.81367896e-07
Iter: 196 loss: 8.62259412e-07
Iter: 197 loss: 8.57560963e-07
Iter: 198 loss: 9.15241458e-07
Iter: 199 loss: 8.57592227e-07
Iter: 200 loss: 8.54093059e-07
Iter: 201 loss: 8.44184e-07
Iter: 202 loss: 8.83879238e-07
Iter: 203 loss: 8.40051712e-07
Iter: 204 loss: 8.29329622e-07
Iter: 205 loss: 9.67878577e-07
Iter: 206 loss: 8.29234864e-07
Iter: 207 loss: 8.23041091e-07
Iter: 208 loss: 9.12161397e-07
Iter: 209 loss: 8.23069172e-07
Iter: 210 loss: 8.18787669e-07
Iter: 211 loss: 8.14090697e-07
Iter: 212 loss: 8.13342865e-07
Iter: 213 loss: 8.0414685e-07
Iter: 214 loss: 8.1404e-07
Iter: 215 loss: 7.9909114e-07
Iter: 216 loss: 7.93997515e-07
Iter: 217 loss: 8.55811436e-07
Iter: 218 loss: 7.93953461e-07
Iter: 219 loss: 7.88458e-07
Iter: 220 loss: 7.88844829e-07
Iter: 221 loss: 7.84066287e-07
Iter: 222 loss: 7.78766207e-07
Iter: 223 loss: 8.01483623e-07
Iter: 224 loss: 7.77665377e-07
Iter: 225 loss: 7.73369777e-07
Iter: 226 loss: 7.86786813e-07
Iter: 227 loss: 7.7208847e-07
Iter: 228 loss: 7.66183803e-07
Iter: 229 loss: 7.99311408e-07
Iter: 230 loss: 7.65352638e-07
Iter: 231 loss: 7.62864829e-07
Iter: 232 loss: 7.57997839e-07
Iter: 233 loss: 8.52255141e-07
Iter: 234 loss: 7.57990847e-07
Iter: 235 loss: 7.53582071e-07
Iter: 236 loss: 7.53484869e-07
Iter: 237 loss: 7.50554477e-07
Iter: 238 loss: 7.46606e-07
Iter: 239 loss: 7.46442e-07
Iter: 240 loss: 7.40665484e-07
Iter: 241 loss: 7.45885359e-07
Iter: 242 loss: 7.3733338e-07
Iter: 243 loss: 7.33122647e-07
Iter: 244 loss: 7.72085627e-07
Iter: 245 loss: 7.32936428e-07
Iter: 246 loss: 7.28063924e-07
Iter: 247 loss: 7.3308172e-07
Iter: 248 loss: 7.25381483e-07
Iter: 249 loss: 7.21687684e-07
Iter: 250 loss: 7.33951538e-07
Iter: 251 loss: 7.20675075e-07
Iter: 252 loss: 7.16796194e-07
Iter: 253 loss: 7.11433358e-07
Iter: 254 loss: 7.11174266e-07
Iter: 255 loss: 7.06887306e-07
Iter: 256 loss: 7.06156e-07
Iter: 257 loss: 7.03835099e-07
Iter: 258 loss: 7.00984685e-07
Iter: 259 loss: 7.0073861e-07
Iter: 260 loss: 6.97058397e-07
Iter: 261 loss: 7.34983246e-07
Iter: 262 loss: 6.96934251e-07
Iter: 263 loss: 6.93196966e-07
Iter: 264 loss: 7.09222e-07
Iter: 265 loss: 6.92450669e-07
Iter: 266 loss: 6.89907665e-07
Iter: 267 loss: 6.84868667e-07
Iter: 268 loss: 7.80531252e-07
Iter: 269 loss: 6.84768906e-07
Iter: 270 loss: 6.82494601e-07
Iter: 271 loss: 6.82065433e-07
Iter: 272 loss: 6.79400728e-07
Iter: 273 loss: 6.74442958e-07
Iter: 274 loss: 7.90353624e-07
Iter: 275 loss: 6.74417947e-07
Iter: 276 loss: 6.69746612e-07
Iter: 277 loss: 6.98195549e-07
Iter: 278 loss: 6.69187955e-07
Iter: 279 loss: 6.66005064e-07
Iter: 280 loss: 6.63182448e-07
Iter: 281 loss: 6.62361401e-07
Iter: 282 loss: 6.59527927e-07
Iter: 283 loss: 6.59098305e-07
Iter: 284 loss: 6.56465204e-07
Iter: 285 loss: 6.534666e-07
Iter: 286 loss: 6.53106383e-07
Iter: 287 loss: 6.49386209e-07
Iter: 288 loss: 6.59884108e-07
Iter: 289 loss: 6.48218247e-07
Iter: 290 loss: 6.4437188e-07
Iter: 291 loss: 6.63332571e-07
Iter: 292 loss: 6.43730914e-07
Iter: 293 loss: 6.4033793e-07
Iter: 294 loss: 6.5829181e-07
Iter: 295 loss: 6.39903078e-07
Iter: 296 loss: 6.37220296e-07
Iter: 297 loss: 6.33420655e-07
Iter: 298 loss: 6.33279399e-07
Iter: 299 loss: 6.34282685e-07
Iter: 300 loss: 6.31279249e-07
Iter: 301 loss: 6.29917281e-07
Iter: 302 loss: 6.26821361e-07
Iter: 303 loss: 6.67988161e-07
Iter: 304 loss: 6.26665553e-07
Iter: 305 loss: 6.23518247e-07
Iter: 306 loss: 6.35462584e-07
Iter: 307 loss: 6.22765185e-07
Iter: 308 loss: 6.2138713e-07
Iter: 309 loss: 6.2129584e-07
Iter: 310 loss: 6.2003727e-07
Iter: 311 loss: 6.16734724e-07
Iter: 312 loss: 6.41246288e-07
Iter: 313 loss: 6.16039415e-07
Iter: 314 loss: 6.12466863e-07
Iter: 315 loss: 6.27140366e-07
Iter: 316 loss: 6.11680946e-07
Iter: 317 loss: 6.07948948e-07
Iter: 318 loss: 6.18417744e-07
Iter: 319 loss: 6.06750632e-07
Iter: 320 loss: 6.03737703e-07
Iter: 321 loss: 6.334555e-07
Iter: 322 loss: 6.03587523e-07
Iter: 323 loss: 6.00438739e-07
Iter: 324 loss: 5.99143618e-07
Iter: 325 loss: 5.9748163e-07
Iter: 326 loss: 5.94829828e-07
Iter: 327 loss: 6.03456613e-07
Iter: 328 loss: 5.94095297e-07
Iter: 329 loss: 5.90920308e-07
Iter: 330 loss: 5.93590698e-07
Iter: 331 loss: 5.89096942e-07
Iter: 332 loss: 5.8701653e-07
Iter: 333 loss: 5.86825e-07
Iter: 334 loss: 5.85390922e-07
Iter: 335 loss: 5.82008795e-07
Iter: 336 loss: 6.23264953e-07
Iter: 337 loss: 5.81754591e-07
Iter: 338 loss: 5.79093069e-07
Iter: 339 loss: 6.16661168e-07
Iter: 340 loss: 5.79079142e-07
Iter: 341 loss: 5.7765368e-07
Iter: 342 loss: 5.77604624e-07
Iter: 343 loss: 5.76339971e-07
Iter: 344 loss: 5.73117575e-07
Iter: 345 loss: 6.00961471e-07
Iter: 346 loss: 5.72643103e-07
Iter: 347 loss: 5.7092069e-07
Iter: 348 loss: 5.70878115e-07
Iter: 349 loss: 5.69068277e-07
Iter: 350 loss: 5.6901672e-07
Iter: 351 loss: 5.67588302e-07
Iter: 352 loss: 5.65557457e-07
Iter: 353 loss: 5.64182642e-07
Iter: 354 loss: 5.63453341e-07
Iter: 355 loss: 5.60899082e-07
Iter: 356 loss: 5.91477942e-07
Iter: 357 loss: 5.60880835e-07
Iter: 358 loss: 5.59197531e-07
Iter: 359 loss: 5.67216375e-07
Iter: 360 loss: 5.5891644e-07
Iter: 361 loss: 5.56900488e-07
Iter: 362 loss: 5.52977667e-07
Iter: 363 loss: 6.33059585e-07
Iter: 364 loss: 5.52953338e-07
Iter: 365 loss: 5.50055233e-07
Iter: 366 loss: 5.75838271e-07
Iter: 367 loss: 5.49918241e-07
Iter: 368 loss: 5.48132618e-07
Iter: 369 loss: 5.6923966e-07
Iter: 370 loss: 5.48145465e-07
Iter: 371 loss: 5.46609613e-07
Iter: 372 loss: 5.46635761e-07
Iter: 373 loss: 5.45378612e-07
Iter: 374 loss: 5.43074407e-07
Iter: 375 loss: 5.4544e-07
Iter: 376 loss: 5.41838574e-07
Iter: 377 loss: 5.41545489e-07
Iter: 378 loss: 5.40690621e-07
Iter: 379 loss: 5.39880318e-07
Iter: 380 loss: 5.37473852e-07
Iter: 381 loss: 5.49674382e-07
Iter: 382 loss: 5.36664061e-07
Iter: 383 loss: 5.35264178e-07
Iter: 384 loss: 5.35161689e-07
Iter: 385 loss: 5.33811431e-07
Iter: 386 loss: 5.35649235e-07
Iter: 387 loss: 5.33090429e-07
Iter: 388 loss: 5.31363867e-07
Iter: 389 loss: 5.29571196e-07
Iter: 390 loss: 5.29215242e-07
Iter: 391 loss: 5.2742223e-07
Iter: 392 loss: 5.31868807e-07
Iter: 393 loss: 5.26751933e-07
Iter: 394 loss: 5.24512416e-07
Iter: 395 loss: 5.40706708e-07
Iter: 396 loss: 5.24314032e-07
Iter: 397 loss: 5.22348728e-07
Iter: 398 loss: 5.24688573e-07
Iter: 399 loss: 5.21355787e-07
Iter: 400 loss: 5.1944437e-07
Iter: 401 loss: 5.20762e-07
Iter: 402 loss: 5.18283514e-07
Iter: 403 loss: 5.16318778e-07
Iter: 404 loss: 5.24806808e-07
Iter: 405 loss: 5.15958732e-07
Iter: 406 loss: 5.13822613e-07
Iter: 407 loss: 5.26125689e-07
Iter: 408 loss: 5.13544364e-07
Iter: 409 loss: 5.12272777e-07
Iter: 410 loss: 5.14222961e-07
Iter: 411 loss: 5.11753058e-07
Iter: 412 loss: 5.10237101e-07
Iter: 413 loss: 5.26291331e-07
Iter: 414 loss: 5.10201801e-07
Iter: 415 loss: 5.09086931e-07
Iter: 416 loss: 5.07143909e-07
Iter: 417 loss: 5.5581711e-07
Iter: 418 loss: 5.07150389e-07
Iter: 419 loss: 5.05422236e-07
Iter: 420 loss: 5.13790837e-07
Iter: 421 loss: 5.05129776e-07
Iter: 422 loss: 5.0336331e-07
Iter: 423 loss: 5.15545082e-07
Iter: 424 loss: 5.03210401e-07
Iter: 425 loss: 5.02168859e-07
Iter: 426 loss: 5.00224417e-07
Iter: 427 loss: 5.42134387e-07
Iter: 428 loss: 5.00224928e-07
Iter: 429 loss: 4.98316354e-07
Iter: 430 loss: 5.12986674e-07
Iter: 431 loss: 4.98175154e-07
Iter: 432 loss: 4.96802045e-07
Iter: 433 loss: 5.01885097e-07
Iter: 434 loss: 4.96476105e-07
Iter: 435 loss: 4.946628e-07
Iter: 436 loss: 4.98248767e-07
Iter: 437 loss: 4.93902576e-07
Iter: 438 loss: 4.92707613e-07
Iter: 439 loss: 4.93149969e-07
Iter: 440 loss: 4.91887e-07
Iter: 441 loss: 4.90047e-07
Iter: 442 loss: 4.94223173e-07
Iter: 443 loss: 4.89336969e-07
Iter: 444 loss: 4.87969146e-07
Iter: 445 loss: 5.04003424e-07
Iter: 446 loss: 4.87948796e-07
Iter: 447 loss: 4.86563522e-07
Iter: 448 loss: 4.86415615e-07
Iter: 449 loss: 4.85435521e-07
Iter: 450 loss: 4.84424277e-07
Iter: 451 loss: 4.8424414e-07
Iter: 452 loss: 4.83686563e-07
Iter: 453 loss: 4.82087898e-07
Iter: 454 loss: 4.91530443e-07
Iter: 455 loss: 4.81612233e-07
Iter: 456 loss: 4.80759923e-07
Iter: 457 loss: 4.80597e-07
Iter: 458 loss: 4.79625385e-07
Iter: 459 loss: 4.79362427e-07
Iter: 460 loss: 4.78775291e-07
Iter: 461 loss: 4.77316235e-07
Iter: 462 loss: 4.77261096e-07
Iter: 463 loss: 4.76095636e-07
Iter: 464 loss: 4.74751317e-07
Iter: 465 loss: 4.77112849e-07
Iter: 466 loss: 4.74137948e-07
Iter: 467 loss: 4.72650129e-07
Iter: 468 loss: 4.87555837e-07
Iter: 469 loss: 4.72610225e-07
Iter: 470 loss: 4.71296687e-07
Iter: 471 loss: 4.73476092e-07
Iter: 472 loss: 4.70676e-07
Iter: 473 loss: 4.69258254e-07
Iter: 474 loss: 4.68843865e-07
Iter: 475 loss: 4.6799596e-07
Iter: 476 loss: 4.66296768e-07
Iter: 477 loss: 4.71327581e-07
Iter: 478 loss: 4.65846142e-07
Iter: 479 loss: 4.64334505e-07
Iter: 480 loss: 4.82711528e-07
Iter: 481 loss: 4.64364973e-07
Iter: 482 loss: 4.63157647e-07
Iter: 483 loss: 4.66866709e-07
Iter: 484 loss: 4.6281815e-07
Iter: 485 loss: 4.61601928e-07
Iter: 486 loss: 4.68080287e-07
Iter: 487 loss: 4.61405222e-07
Iter: 488 loss: 4.60575848e-07
Iter: 489 loss: 4.59894466e-07
Iter: 490 loss: 4.59674368e-07
Iter: 491 loss: 4.58624044e-07
Iter: 492 loss: 4.60923161e-07
Iter: 493 loss: 4.58211957e-07
Iter: 494 loss: 4.56629124e-07
Iter: 495 loss: 4.62671863e-07
Iter: 496 loss: 4.56301507e-07
Iter: 497 loss: 4.55535428e-07
Iter: 498 loss: 4.55284237e-07
Iter: 499 loss: 4.54859844e-07
Iter: 500 loss: 4.53628218e-07
Iter: 501 loss: 4.54123665e-07
Iter: 502 loss: 4.5270707e-07
Iter: 503 loss: 4.51171303e-07
Iter: 504 loss: 4.57201537e-07
Iter: 505 loss: 4.50831834e-07
Iter: 506 loss: 4.49455911e-07
Iter: 507 loss: 4.66806966e-07
Iter: 508 loss: 4.49419986e-07
Iter: 509 loss: 4.48536213e-07
Iter: 510 loss: 4.47582181e-07
Iter: 511 loss: 4.47422224e-07
Iter: 512 loss: 4.45704927e-07
Iter: 513 loss: 4.49354189e-07
Iter: 514 loss: 4.44982106e-07
Iter: 515 loss: 4.44073606e-07
Iter: 516 loss: 4.53482528e-07
Iter: 517 loss: 4.44032054e-07
Iter: 518 loss: 4.42932674e-07
Iter: 519 loss: 4.46284844e-07
Iter: 520 loss: 4.42638623e-07
Iter: 521 loss: 4.41690872e-07
Iter: 522 loss: 4.46641479e-07
Iter: 523 loss: 4.41536201e-07
Iter: 524 loss: 4.40946394e-07
Iter: 525 loss: 4.40175e-07
Iter: 526 loss: 4.40116366e-07
Iter: 527 loss: 4.3886439e-07
Iter: 528 loss: 4.40855842e-07
Iter: 529 loss: 4.38294848e-07
Iter: 530 loss: 4.37175487e-07
Iter: 531 loss: 4.37169319e-07
Iter: 532 loss: 4.36529803e-07
Iter: 533 loss: 4.34983406e-07
Iter: 534 loss: 4.52684958e-07
Iter: 535 loss: 4.34825381e-07
Iter: 536 loss: 4.33347964e-07
Iter: 537 loss: 4.39935775e-07
Iter: 538 loss: 4.33067214e-07
Iter: 539 loss: 4.32007795e-07
Iter: 540 loss: 4.41935043e-07
Iter: 541 loss: 4.31932e-07
Iter: 542 loss: 4.31054019e-07
Iter: 543 loss: 4.34130527e-07
Iter: 544 loss: 4.30825821e-07
Iter: 545 loss: 4.29742471e-07
Iter: 546 loss: 4.30895398e-07
Iter: 547 loss: 4.29138368e-07
Iter: 548 loss: 4.28341252e-07
Iter: 549 loss: 4.27680703e-07
Iter: 550 loss: 4.27455035e-07
Iter: 551 loss: 4.2606203e-07
Iter: 552 loss: 4.36180585e-07
Iter: 553 loss: 4.25948429e-07
Iter: 554 loss: 4.25210317e-07
Iter: 555 loss: 4.25161346e-07
Iter: 556 loss: 4.24612836e-07
Iter: 557 loss: 4.24088228e-07
Iter: 558 loss: 4.23991963e-07
Iter: 559 loss: 4.23282472e-07
Iter: 560 loss: 4.27034848e-07
Iter: 561 loss: 4.23143149e-07
Iter: 562 loss: 4.2241993e-07
Iter: 563 loss: 4.21050373e-07
Iter: 564 loss: 4.47499559e-07
Iter: 565 loss: 4.21033462e-07
Iter: 566 loss: 4.20381269e-07
Iter: 567 loss: 4.2007585e-07
Iter: 568 loss: 4.19539106e-07
Iter: 569 loss: 4.1915186e-07
Iter: 570 loss: 4.19021148e-07
Iter: 571 loss: 4.18074933e-07
Iter: 572 loss: 4.17290607e-07
Iter: 573 loss: 4.17034357e-07
Iter: 574 loss: 4.15896494e-07
Iter: 575 loss: 4.22146059e-07
Iter: 576 loss: 4.15684326e-07
Iter: 577 loss: 4.14469781e-07
Iter: 578 loss: 4.16970181e-07
Iter: 579 loss: 4.13973737e-07
Iter: 580 loss: 4.13143255e-07
Iter: 581 loss: 4.13130465e-07
Iter: 582 loss: 4.12448031e-07
Iter: 583 loss: 4.11418171e-07
Iter: 584 loss: 4.11387731e-07
Iter: 585 loss: 4.10301737e-07
Iter: 586 loss: 4.13172927e-07
Iter: 587 loss: 4.09943056e-07
Iter: 588 loss: 4.08942952e-07
Iter: 589 loss: 4.08933175e-07
Iter: 590 loss: 4.08419e-07
Iter: 591 loss: 4.07855424e-07
Iter: 592 loss: 4.07782352e-07
Iter: 593 loss: 4.06951074e-07
Iter: 594 loss: 4.08667859e-07
Iter: 595 loss: 4.06633e-07
Iter: 596 loss: 4.05550708e-07
Iter: 597 loss: 4.07923721e-07
Iter: 598 loss: 4.05111621e-07
Iter: 599 loss: 4.04406251e-07
Iter: 600 loss: 4.12561377e-07
Iter: 601 loss: 4.04415971e-07
Iter: 602 loss: 4.03781314e-07
Iter: 603 loss: 4.02837145e-07
Iter: 604 loss: 4.02822e-07
Iter: 605 loss: 4.01824934e-07
Iter: 606 loss: 4.02594083e-07
Iter: 607 loss: 4.01216852e-07
Iter: 608 loss: 4.00308068e-07
Iter: 609 loss: 4.08410756e-07
Iter: 610 loss: 4.0027723e-07
Iter: 611 loss: 3.9947912e-07
Iter: 612 loss: 3.98488396e-07
Iter: 613 loss: 3.98416802e-07
Iter: 614 loss: 3.98354331e-07
Iter: 615 loss: 3.97813551e-07
Iter: 616 loss: 3.97402687e-07
Iter: 617 loss: 3.96555265e-07
Iter: 618 loss: 4.13094796e-07
Iter: 619 loss: 3.96515674e-07
Iter: 620 loss: 3.95449433e-07
Iter: 621 loss: 4.00461147e-07
Iter: 622 loss: 3.95227801e-07
Iter: 623 loss: 3.94828191e-07
Iter: 624 loss: 3.9471945e-07
Iter: 625 loss: 3.94280505e-07
Iter: 626 loss: 3.93132979e-07
Iter: 627 loss: 4.04324339e-07
Iter: 628 loss: 3.92989193e-07
Iter: 629 loss: 3.92001482e-07
Iter: 630 loss: 3.98112718e-07
Iter: 631 loss: 3.91853689e-07
Iter: 632 loss: 3.90924157e-07
Iter: 633 loss: 3.9685807e-07
Iter: 634 loss: 3.9080922e-07
Iter: 635 loss: 3.90235044e-07
Iter: 636 loss: 3.92821562e-07
Iter: 637 loss: 3.90137615e-07
Iter: 638 loss: 3.8959638e-07
Iter: 639 loss: 3.90482512e-07
Iter: 640 loss: 3.89349083e-07
Iter: 641 loss: 3.88752795e-07
Iter: 642 loss: 3.88254222e-07
Iter: 643 loss: 3.88093781e-07
Iter: 644 loss: 3.8728939e-07
Iter: 645 loss: 3.89239858e-07
Iter: 646 loss: 3.86994884e-07
Iter: 647 loss: 3.86145558e-07
Iter: 648 loss: 3.88463718e-07
Iter: 649 loss: 3.8590764e-07
Iter: 650 loss: 3.85010537e-07
Iter: 651 loss: 3.87659867e-07
Iter: 652 loss: 3.84770601e-07
Iter: 653 loss: 3.84088253e-07
Iter: 654 loss: 3.84093568e-07
Iter: 655 loss: 3.83636859e-07
Iter: 656 loss: 3.8313658e-07
Iter: 657 loss: 3.83030056e-07
Iter: 658 loss: 3.82682089e-07
Iter: 659 loss: 3.82574939e-07
Iter: 660 loss: 3.82295696e-07
Iter: 661 loss: 3.81669736e-07
Iter: 662 loss: 3.90925806e-07
Iter: 663 loss: 3.81652058e-07
Iter: 664 loss: 3.80888878e-07
Iter: 665 loss: 3.81303579e-07
Iter: 666 loss: 3.80426542e-07
Iter: 667 loss: 3.80049698e-07
Iter: 668 loss: 3.79957129e-07
Iter: 669 loss: 3.7956903e-07
Iter: 670 loss: 3.78956599e-07
Iter: 671 loss: 3.78967286e-07
Iter: 672 loss: 3.78144193e-07
Iter: 673 loss: 3.83568022e-07
Iter: 674 loss: 3.78057166e-07
Iter: 675 loss: 3.7746122e-07
Iter: 676 loss: 3.77916166e-07
Iter: 677 loss: 3.77102737e-07
Iter: 678 loss: 3.76355274e-07
Iter: 679 loss: 3.76068272e-07
Iter: 680 loss: 3.75679377e-07
Iter: 681 loss: 3.74746094e-07
Iter: 682 loss: 3.76714723e-07
Iter: 683 loss: 3.74362259e-07
Iter: 684 loss: 3.73541582e-07
Iter: 685 loss: 3.804453e-07
Iter: 686 loss: 3.7347678e-07
Iter: 687 loss: 3.72934437e-07
Iter: 688 loss: 3.78310915e-07
Iter: 689 loss: 3.72935119e-07
Iter: 690 loss: 3.72471504e-07
Iter: 691 loss: 3.74779376e-07
Iter: 692 loss: 3.72386154e-07
Iter: 693 loss: 3.72069678e-07
Iter: 694 loss: 3.74335116e-07
Iter: 695 loss: 3.72055695e-07
Iter: 696 loss: 3.71740043e-07
Iter: 697 loss: 3.70928461e-07
Iter: 698 loss: 3.76585973e-07
Iter: 699 loss: 3.70717601e-07
Iter: 700 loss: 3.69932309e-07
Iter: 701 loss: 3.7585977e-07
Iter: 702 loss: 3.69885413e-07
Iter: 703 loss: 3.69323743e-07
Iter: 704 loss: 3.75337947e-07
Iter: 705 loss: 3.69308708e-07
Iter: 706 loss: 3.68794559e-07
Iter: 707 loss: 3.68303e-07
Iter: 708 loss: 3.68162887e-07
Iter: 709 loss: 3.67382682e-07
Iter: 710 loss: 3.70658256e-07
Iter: 711 loss: 3.67202517e-07
Iter: 712 loss: 3.66602137e-07
Iter: 713 loss: 3.71225724e-07
Iter: 714 loss: 3.66547113e-07
Iter: 715 loss: 3.6620537e-07
Iter: 716 loss: 3.65351241e-07
Iter: 717 loss: 3.71918645e-07
Iter: 718 loss: 3.65216493e-07
Iter: 719 loss: 3.64378138e-07
Iter: 720 loss: 3.64361057e-07
Iter: 721 loss: 3.63831134e-07
Iter: 722 loss: 3.6343323e-07
Iter: 723 loss: 3.6323712e-07
Iter: 724 loss: 3.62764752e-07
Iter: 725 loss: 3.62758726e-07
Iter: 726 loss: 3.62305e-07
Iter: 727 loss: 3.63855492e-07
Iter: 728 loss: 3.6220419e-07
Iter: 729 loss: 3.6177741e-07
Iter: 730 loss: 3.62617953e-07
Iter: 731 loss: 3.61589741e-07
Iter: 732 loss: 3.61085256e-07
Iter: 733 loss: 3.61410201e-07
Iter: 734 loss: 3.60748913e-07
Iter: 735 loss: 3.60161891e-07
Iter: 736 loss: 3.59516633e-07
Iter: 737 loss: 3.59417356e-07
Iter: 738 loss: 3.58851622e-07
Iter: 739 loss: 3.58847586e-07
Iter: 740 loss: 3.58279323e-07
Iter: 741 loss: 3.59496255e-07
Iter: 742 loss: 3.5809569e-07
Iter: 743 loss: 3.5762352e-07
Iter: 744 loss: 3.57737122e-07
Iter: 745 loss: 3.57273677e-07
Iter: 746 loss: 3.56818873e-07
Iter: 747 loss: 3.59616251e-07
Iter: 748 loss: 3.56757084e-07
Iter: 749 loss: 3.56227872e-07
Iter: 750 loss: 3.56214628e-07
Iter: 751 loss: 3.55768435e-07
Iter: 752 loss: 3.55180362e-07
Iter: 753 loss: 3.5564031e-07
Iter: 754 loss: 3.54790131e-07
Iter: 755 loss: 3.54161159e-07
Iter: 756 loss: 3.57563749e-07
Iter: 757 loss: 3.54072228e-07
Iter: 758 loss: 3.5339275e-07
Iter: 759 loss: 3.54460781e-07
Iter: 760 loss: 3.53104156e-07
Iter: 761 loss: 3.5216965e-07
Iter: 762 loss: 3.60916033e-07
Iter: 763 loss: 3.52136851e-07
Iter: 764 loss: 3.51821825e-07
Iter: 765 loss: 3.5265316e-07
Iter: 766 loss: 3.5170126e-07
Iter: 767 loss: 3.51261264e-07
Iter: 768 loss: 3.50496578e-07
Iter: 769 loss: 3.50508344e-07
Iter: 770 loss: 3.49850552e-07
Iter: 771 loss: 3.5111384e-07
Iter: 772 loss: 3.49555933e-07
Iter: 773 loss: 3.49098116e-07
Iter: 774 loss: 3.49055142e-07
Iter: 775 loss: 3.48685177e-07
Iter: 776 loss: 3.48716497e-07
Iter: 777 loss: 3.48401159e-07
Iter: 778 loss: 3.47971536e-07
Iter: 779 loss: 3.47608591e-07
Iter: 780 loss: 3.47513094e-07
Iter: 781 loss: 3.47066816e-07
Iter: 782 loss: 3.47051383e-07
Iter: 783 loss: 3.46666752e-07
Iter: 784 loss: 3.46107356e-07
Iter: 785 loss: 3.46063047e-07
Iter: 786 loss: 3.45388258e-07
Iter: 787 loss: 3.46864226e-07
Iter: 788 loss: 3.45096453e-07
Iter: 789 loss: 3.44333046e-07
Iter: 790 loss: 3.46294598e-07
Iter: 791 loss: 3.44039393e-07
Iter: 792 loss: 3.43848058e-07
Iter: 793 loss: 3.43681023e-07
Iter: 794 loss: 3.43349711e-07
Iter: 795 loss: 3.42912926e-07
Iter: 796 loss: 3.42887944e-07
Iter: 797 loss: 3.42375614e-07
Iter: 798 loss: 3.46774982e-07
Iter: 799 loss: 3.42379082e-07
Iter: 800 loss: 3.42012214e-07
Iter: 801 loss: 3.4162349e-07
Iter: 802 loss: 3.41579266e-07
Iter: 803 loss: 3.41084103e-07
Iter: 804 loss: 3.44101579e-07
Iter: 805 loss: 3.41000373e-07
Iter: 806 loss: 3.40662268e-07
Iter: 807 loss: 3.43858858e-07
Iter: 808 loss: 3.40650217e-07
Iter: 809 loss: 3.40323936e-07
Iter: 810 loss: 3.39702638e-07
Iter: 811 loss: 3.39698374e-07
Iter: 812 loss: 3.3932568e-07
Iter: 813 loss: 3.42048224e-07
Iter: 814 loss: 3.39250107e-07
Iter: 815 loss: 3.38847457e-07
Iter: 816 loss: 3.39884139e-07
Iter: 817 loss: 3.38690796e-07
Iter: 818 loss: 3.38233974e-07
Iter: 819 loss: 3.3849426e-07
Iter: 820 loss: 3.37955044e-07
Iter: 821 loss: 3.37404657e-07
Iter: 822 loss: 3.37296825e-07
Iter: 823 loss: 3.36980065e-07
Iter: 824 loss: 3.36396511e-07
Iter: 825 loss: 3.36417258e-07
Iter: 826 loss: 3.358561e-07
Iter: 827 loss: 3.39124711e-07
Iter: 828 loss: 3.35783426e-07
Iter: 829 loss: 3.35492587e-07
Iter: 830 loss: 3.35432617e-07
Iter: 831 loss: 3.35206892e-07
Iter: 832 loss: 3.34777269e-07
Iter: 833 loss: 3.36304652e-07
Iter: 834 loss: 3.34634194e-07
Iter: 835 loss: 3.34208551e-07
Iter: 836 loss: 3.34091908e-07
Iter: 837 loss: 3.33864875e-07
Iter: 838 loss: 3.33457365e-07
Iter: 839 loss: 3.36376786e-07
Iter: 840 loss: 3.33422093e-07
Iter: 841 loss: 3.32969648e-07
Iter: 842 loss: 3.33858679e-07
Iter: 843 loss: 3.32791558e-07
Iter: 844 loss: 3.32355029e-07
Iter: 845 loss: 3.31977873e-07
Iter: 846 loss: 3.31884479e-07
Iter: 847 loss: 3.31245701e-07
Iter: 848 loss: 3.35598145e-07
Iter: 849 loss: 3.3123365e-07
Iter: 850 loss: 3.30746673e-07
Iter: 851 loss: 3.33539219e-07
Iter: 852 loss: 3.30673686e-07
Iter: 853 loss: 3.30209389e-07
Iter: 854 loss: 3.29374188e-07
Iter: 855 loss: 3.48407553e-07
Iter: 856 loss: 3.29339741e-07
Iter: 857 loss: 3.28782221e-07
Iter: 858 loss: 3.36496e-07
Iter: 859 loss: 3.28784665e-07
Iter: 860 loss: 3.28365445e-07
Iter: 861 loss: 3.328189e-07
Iter: 862 loss: 3.28360727e-07
Iter: 863 loss: 3.27928461e-07
Iter: 864 loss: 3.28479018e-07
Iter: 865 loss: 3.27744203e-07
Iter: 866 loss: 3.27502335e-07
Iter: 867 loss: 3.27539027e-07
Iter: 868 loss: 3.27348232e-07
Iter: 869 loss: 3.26874897e-07
Iter: 870 loss: 3.27390978e-07
Iter: 871 loss: 3.2666361e-07
Iter: 872 loss: 3.26203349e-07
Iter: 873 loss: 3.25966255e-07
Iter: 874 loss: 3.25798368e-07
Iter: 875 loss: 3.25539e-07
Iter: 876 loss: 3.25424224e-07
Iter: 877 loss: 3.25174682e-07
Iter: 878 loss: 3.24690689e-07
Iter: 879 loss: 3.24691598e-07
Iter: 880 loss: 3.24163949e-07
Iter: 881 loss: 3.24978259e-07
Iter: 882 loss: 3.23916254e-07
Iter: 883 loss: 3.23443942e-07
Iter: 884 loss: 3.29595366e-07
Iter: 885 loss: 3.23430868e-07
Iter: 886 loss: 3.23015399e-07
Iter: 887 loss: 3.23153017e-07
Iter: 888 loss: 3.22724134e-07
Iter: 889 loss: 3.22347432e-07
Iter: 890 loss: 3.22698895e-07
Iter: 891 loss: 3.22088908e-07
Iter: 892 loss: 3.21697712e-07
Iter: 893 loss: 3.2606124e-07
Iter: 894 loss: 3.21680147e-07
Iter: 895 loss: 3.21301059e-07
Iter: 896 loss: 3.23713579e-07
Iter: 897 loss: 3.21276389e-07
Iter: 898 loss: 3.21021417e-07
Iter: 899 loss: 3.20525771e-07
Iter: 900 loss: 3.30615791e-07
Iter: 901 loss: 3.20513664e-07
Iter: 902 loss: 3.20248887e-07
Iter: 903 loss: 3.20243288e-07
Iter: 904 loss: 3.19991301e-07
Iter: 905 loss: 3.19516232e-07
Iter: 906 loss: 3.28874478e-07
Iter: 907 loss: 3.19511628e-07
Iter: 908 loss: 3.18928073e-07
Iter: 909 loss: 3.22712083e-07
Iter: 910 loss: 3.18864693e-07
Iter: 911 loss: 3.18505016e-07
Iter: 912 loss: 3.18526304e-07
Iter: 913 loss: 3.18307627e-07
Iter: 914 loss: 3.17761106e-07
Iter: 915 loss: 3.23835e-07
Iter: 916 loss: 3.17711056e-07
Iter: 917 loss: 3.17171867e-07
Iter: 918 loss: 3.20256959e-07
Iter: 919 loss: 3.17129434e-07
Iter: 920 loss: 3.16664654e-07
Iter: 921 loss: 3.20661e-07
Iter: 922 loss: 3.16636886e-07
Iter: 923 loss: 3.1633968e-07
Iter: 924 loss: 3.16612272e-07
Iter: 925 loss: 3.16140671e-07
Iter: 926 loss: 3.15766329e-07
Iter: 927 loss: 3.1599069e-07
Iter: 928 loss: 3.1551366e-07
Iter: 929 loss: 3.15380106e-07
Iter: 930 loss: 3.15296745e-07
Iter: 931 loss: 3.15080456e-07
Iter: 932 loss: 3.14552125e-07
Iter: 933 loss: 3.20926318e-07
Iter: 934 loss: 3.14512931e-07
Iter: 935 loss: 3.14088538e-07
Iter: 936 loss: 3.16581776e-07
Iter: 937 loss: 3.14019928e-07
Iter: 938 loss: 3.13564499e-07
Iter: 939 loss: 3.14245739e-07
Iter: 940 loss: 3.13321948e-07
Iter: 941 loss: 3.12879223e-07
Iter: 942 loss: 3.13986419e-07
Iter: 943 loss: 3.12718385e-07
Iter: 944 loss: 3.12360584e-07
Iter: 945 loss: 3.14597116e-07
Iter: 946 loss: 3.12331252e-07
Iter: 947 loss: 3.11965863e-07
Iter: 948 loss: 3.12464266e-07
Iter: 949 loss: 3.11789961e-07
Iter: 950 loss: 3.11460809e-07
Iter: 951 loss: 3.11045426e-07
Iter: 952 loss: 3.11011433e-07
Iter: 953 loss: 3.10557169e-07
Iter: 954 loss: 3.17136198e-07
Iter: 955 loss: 3.10570613e-07
Iter: 956 loss: 3.10266614e-07
Iter: 957 loss: 3.11168634e-07
Iter: 958 loss: 3.1020511e-07
Iter: 959 loss: 3.0980425e-07
Iter: 960 loss: 3.09468618e-07
Iter: 961 loss: 3.09379175e-07
Iter: 962 loss: 3.08990536e-07
Iter: 963 loss: 3.14563408e-07
Iter: 964 loss: 3.08991929e-07
Iter: 965 loss: 3.086368e-07
Iter: 966 loss: 3.10169582e-07
Iter: 967 loss: 3.08571487e-07
Iter: 968 loss: 3.08326719e-07
Iter: 969 loss: 3.07897807e-07
Iter: 970 loss: 3.17396484e-07
Iter: 971 loss: 3.07893373e-07
Iter: 972 loss: 3.07473471e-07
Iter: 973 loss: 3.12052748e-07
Iter: 974 loss: 3.07458151e-07
Iter: 975 loss: 3.07073094e-07
Iter: 976 loss: 3.07277503e-07
Iter: 977 loss: 3.06814968e-07
Iter: 978 loss: 3.06377672e-07
Iter: 979 loss: 3.06307641e-07
Iter: 980 loss: 3.06038288e-07
Iter: 981 loss: 3.05775615e-07
Iter: 982 loss: 3.05680089e-07
Iter: 983 loss: 3.05436231e-07
Iter: 984 loss: 3.05134677e-07
Iter: 985 loss: 3.05093835e-07
Iter: 986 loss: 3.04616208e-07
Iter: 987 loss: 3.04651394e-07
Iter: 988 loss: 3.04216258e-07
Iter: 989 loss: 3.03779927e-07
Iter: 990 loss: 3.06643017e-07
Iter: 991 loss: 3.0371956e-07
Iter: 992 loss: 3.03402942e-07
Iter: 993 loss: 3.07648747e-07
Iter: 994 loss: 3.03387367e-07
Iter: 995 loss: 3.03189836e-07
Iter: 996 loss: 3.02963e-07
Iter: 997 loss: 3.02920711e-07
Iter: 998 loss: 3.02739807e-07
Iter: 999 loss: 3.02699789e-07
Iter: 1000 loss: 3.02539604e-07
Iter: 1001 loss: 3.02320672e-07
Iter: 1002 loss: 3.02288413e-07
Iter: 1003 loss: 3.01967901e-07
Iter: 1004 loss: 3.01591228e-07
Iter: 1005 loss: 3.01564683e-07
Iter: 1006 loss: 3.01040359e-07
Iter: 1007 loss: 3.01047578e-07
Iter: 1008 loss: 3.00786098e-07
Iter: 1009 loss: 3.00751623e-07
Iter: 1010 loss: 3.00571514e-07
Iter: 1011 loss: 3.00275246e-07
Iter: 1012 loss: 3.01598817e-07
Iter: 1013 loss: 3.00192568e-07
Iter: 1014 loss: 2.9981814e-07
Iter: 1015 loss: 3.01237407e-07
Iter: 1016 loss: 2.99731255e-07
Iter: 1017 loss: 2.99386215e-07
Iter: 1018 loss: 2.99069086e-07
Iter: 1019 loss: 2.98994024e-07
Iter: 1020 loss: 2.985077e-07
Iter: 1021 loss: 2.9861917e-07
Iter: 1022 loss: 2.98173063e-07
Iter: 1023 loss: 2.97913289e-07
Iter: 1024 loss: 2.97836209e-07
Iter: 1025 loss: 2.97597694e-07
Iter: 1026 loss: 2.97760096e-07
Iter: 1027 loss: 2.97447e-07
Iter: 1028 loss: 2.97101195e-07
Iter: 1029 loss: 2.97812e-07
Iter: 1030 loss: 2.96969745e-07
Iter: 1031 loss: 2.96614076e-07
Iter: 1032 loss: 3.01178318e-07
Iter: 1033 loss: 2.9661561e-07
Iter: 1034 loss: 2.9646759e-07
Iter: 1035 loss: 2.96150688e-07
Iter: 1036 loss: 3.00439751e-07
Iter: 1037 loss: 2.96147249e-07
Iter: 1038 loss: 2.95899099e-07
Iter: 1039 loss: 2.99447493e-07
Iter: 1040 loss: 2.9588486e-07
Iter: 1041 loss: 2.95605844e-07
Iter: 1042 loss: 2.95145185e-07
Iter: 1043 loss: 2.95147572e-07
Iter: 1044 loss: 2.94559925e-07
Iter: 1045 loss: 2.98688747e-07
Iter: 1046 loss: 2.94534232e-07
Iter: 1047 loss: 2.94233e-07
Iter: 1048 loss: 2.97877563e-07
Iter: 1049 loss: 2.94244103e-07
Iter: 1050 loss: 2.93993878e-07
Iter: 1051 loss: 2.93605041e-07
Iter: 1052 loss: 2.93600323e-07
Iter: 1053 loss: 2.93217283e-07
Iter: 1054 loss: 2.9369545e-07
Iter: 1055 loss: 2.93033423e-07
Iter: 1056 loss: 2.92621451e-07
Iter: 1057 loss: 2.94524909e-07
Iter: 1058 loss: 2.92499806e-07
Iter: 1059 loss: 2.9220422e-07
Iter: 1060 loss: 2.94475484e-07
Iter: 1061 loss: 2.92171762e-07
Iter: 1062 loss: 2.91908691e-07
Iter: 1063 loss: 2.9288438e-07
Iter: 1064 loss: 2.91818537e-07
Iter: 1065 loss: 2.91652128e-07
Iter: 1066 loss: 2.93577017e-07
Iter: 1067 loss: 2.91634365e-07
Iter: 1068 loss: 2.9142285e-07
Iter: 1069 loss: 2.90901738e-07
Iter: 1070 loss: 2.95887503e-07
Iter: 1071 loss: 2.90811016e-07
Iter: 1072 loss: 2.90414789e-07
Iter: 1073 loss: 2.92316827e-07
Iter: 1074 loss: 2.90335606e-07
Iter: 1075 loss: 2.89967602e-07
Iter: 1076 loss: 2.9206879e-07
Iter: 1077 loss: 2.89913658e-07
Iter: 1078 loss: 2.89562053e-07
Iter: 1079 loss: 2.90500623e-07
Iter: 1080 loss: 2.89474428e-07
Iter: 1081 loss: 2.89184101e-07
Iter: 1082 loss: 2.89088746e-07
Iter: 1083 loss: 2.88930266e-07
Iter: 1084 loss: 2.88672879e-07
Iter: 1085 loss: 2.8865071e-07
Iter: 1086 loss: 2.88466936e-07
Iter: 1087 loss: 2.8799505e-07
Iter: 1088 loss: 2.93106666e-07
Iter: 1089 loss: 2.87947245e-07
Iter: 1090 loss: 2.874595e-07
Iter: 1091 loss: 2.90881928e-07
Iter: 1092 loss: 2.87428975e-07
Iter: 1093 loss: 2.87137368e-07
Iter: 1094 loss: 2.87650494e-07
Iter: 1095 loss: 2.86992702e-07
Iter: 1096 loss: 2.86686259e-07
Iter: 1097 loss: 2.90954574e-07
Iter: 1098 loss: 2.86686969e-07
Iter: 1099 loss: 2.86471419e-07
Iter: 1100 loss: 2.86668808e-07
Iter: 1101 loss: 2.86358159e-07
Iter: 1102 loss: 2.86063e-07
Iter: 1103 loss: 2.88030435e-07
Iter: 1104 loss: 2.86033838e-07
Iter: 1105 loss: 2.85838041e-07
Iter: 1106 loss: 2.85564681e-07
Iter: 1107 loss: 2.85552062e-07
Iter: 1108 loss: 2.8524039e-07
Iter: 1109 loss: 2.85305703e-07
Iter: 1110 loss: 2.84988744e-07
Iter: 1111 loss: 2.84663201e-07
Iter: 1112 loss: 2.8805465e-07
Iter: 1113 loss: 2.84647342e-07
Iter: 1114 loss: 2.84336977e-07
Iter: 1115 loss: 2.85133524e-07
Iter: 1116 loss: 2.84236023e-07
Iter: 1117 loss: 2.83940892e-07
Iter: 1118 loss: 2.84535389e-07
Iter: 1119 loss: 2.8382982e-07
Iter: 1120 loss: 2.8360023e-07
Iter: 1121 loss: 2.85158706e-07
Iter: 1122 loss: 2.83577265e-07
Iter: 1123 loss: 2.83297823e-07
Iter: 1124 loss: 2.83298533e-07
Iter: 1125 loss: 2.83071614e-07
Iter: 1126 loss: 2.82820736e-07
Iter: 1127 loss: 2.82745475e-07
Iter: 1128 loss: 2.82569914e-07
Iter: 1129 loss: 2.82147425e-07
Iter: 1130 loss: 2.83858498e-07
Iter: 1131 loss: 2.8207387e-07
Iter: 1132 loss: 2.81725363e-07
Iter: 1133 loss: 2.8218011e-07
Iter: 1134 loss: 2.81561853e-07
Iter: 1135 loss: 2.81239977e-07
Iter: 1136 loss: 2.81253335e-07
Iter: 1137 loss: 2.80994385e-07
Iter: 1138 loss: 2.80886582e-07
Iter: 1139 loss: 2.8077514e-07
Iter: 1140 loss: 2.80436808e-07
Iter: 1141 loss: 2.81803864e-07
Iter: 1142 loss: 2.80344409e-07
Iter: 1143 loss: 2.80211509e-07
Iter: 1144 loss: 2.80182235e-07
Iter: 1145 loss: 2.80034612e-07
Iter: 1146 loss: 2.79718847e-07
Iter: 1147 loss: 2.82552776e-07
Iter: 1148 loss: 2.79691761e-07
Iter: 1149 loss: 2.79440343e-07
Iter: 1150 loss: 2.79456572e-07
Iter: 1151 loss: 2.79240567e-07
Iter: 1152 loss: 2.79006883e-07
Iter: 1153 loss: 2.78975705e-07
Iter: 1154 loss: 2.78651271e-07
Iter: 1155 loss: 2.80341169e-07
Iter: 1156 loss: 2.78602272e-07
Iter: 1157 loss: 2.78355401e-07
Iter: 1158 loss: 2.81035454e-07
Iter: 1159 loss: 2.78367622e-07
Iter: 1160 loss: 2.78172394e-07
Iter: 1161 loss: 2.77878428e-07
Iter: 1162 loss: 2.77885562e-07
Iter: 1163 loss: 2.77507922e-07
Iter: 1164 loss: 2.77536458e-07
Iter: 1165 loss: 2.77237717e-07
Iter: 1166 loss: 2.76882673e-07
Iter: 1167 loss: 2.81537325e-07
Iter: 1168 loss: 2.76856269e-07
Iter: 1169 loss: 2.76584103e-07
Iter: 1170 loss: 2.7655642e-07
Iter: 1171 loss: 2.76340614e-07
Iter: 1172 loss: 2.76030107e-07
Iter: 1173 loss: 2.76033944e-07
Iter: 1174 loss: 2.75823112e-07
Iter: 1175 loss: 2.75797078e-07
Iter: 1176 loss: 2.75646983e-07
Iter: 1177 loss: 2.75475031e-07
Iter: 1178 loss: 2.7546298e-07
Iter: 1179 loss: 2.7529569e-07
Iter: 1180 loss: 2.75299385e-07
Iter: 1181 loss: 2.75156e-07
Iter: 1182 loss: 2.74968301e-07
Iter: 1183 loss: 2.7474556e-07
Iter: 1184 loss: 2.74705428e-07
Iter: 1185 loss: 2.74590519e-07
Iter: 1186 loss: 2.74551269e-07
Iter: 1187 loss: 2.74418454e-07
Iter: 1188 loss: 2.74212766e-07
Iter: 1189 loss: 2.7420495e-07
Iter: 1190 loss: 2.73926275e-07
Iter: 1191 loss: 2.7611992e-07
Iter: 1192 loss: 2.73907062e-07
Iter: 1193 loss: 2.73713169e-07
Iter: 1194 loss: 2.73582572e-07
Iter: 1195 loss: 2.73495345e-07
Iter: 1196 loss: 2.73148629e-07
Iter: 1197 loss: 2.73349031e-07
Iter: 1198 loss: 2.72904e-07
Iter: 1199 loss: 2.7260262e-07
Iter: 1200 loss: 2.73170826e-07
Iter: 1201 loss: 2.72460511e-07
Iter: 1202 loss: 2.72093075e-07
Iter: 1203 loss: 2.74666093e-07
Iter: 1204 loss: 2.72053029e-07
Iter: 1205 loss: 2.71802691e-07
Iter: 1206 loss: 2.73711237e-07
Iter: 1207 loss: 2.71779328e-07
Iter: 1208 loss: 2.7152322e-07
Iter: 1209 loss: 2.71635116e-07
Iter: 1210 loss: 2.71344561e-07
Iter: 1211 loss: 2.71260575e-07
Iter: 1212 loss: 2.71238406e-07
Iter: 1213 loss: 2.71102181e-07
Iter: 1214 loss: 2.70889757e-07
Iter: 1215 loss: 2.75193827e-07
Iter: 1216 loss: 2.70863268e-07
Iter: 1217 loss: 2.70631119e-07
Iter: 1218 loss: 2.70862529e-07
Iter: 1219 loss: 2.70499299e-07
Iter: 1220 loss: 2.70219829e-07
Iter: 1221 loss: 2.72327441e-07
Iter: 1222 loss: 2.7018487e-07
Iter: 1223 loss: 2.69910913e-07
Iter: 1224 loss: 2.70637e-07
Iter: 1225 loss: 2.69814166e-07
Iter: 1226 loss: 2.69631784e-07
Iter: 1227 loss: 2.71250059e-07
Iter: 1228 loss: 2.69642953e-07
Iter: 1229 loss: 2.69468046e-07
Iter: 1230 loss: 2.69201678e-07
Iter: 1231 loss: 2.69188661e-07
Iter: 1232 loss: 2.68930535e-07
Iter: 1233 loss: 2.69592476e-07
Iter: 1234 loss: 2.68836175e-07
Iter: 1235 loss: 2.68526946e-07
Iter: 1236 loss: 2.69286915e-07
Iter: 1237 loss: 2.68437333e-07
Iter: 1238 loss: 2.68132624e-07
Iter: 1239 loss: 2.68288602e-07
Iter: 1240 loss: 2.6792e-07
Iter: 1241 loss: 2.6771238e-07
Iter: 1242 loss: 2.67672135e-07
Iter: 1243 loss: 2.67522665e-07
Iter: 1244 loss: 2.67476e-07
Iter: 1245 loss: 2.67371206e-07
Iter: 1246 loss: 2.67171629e-07
Iter: 1247 loss: 2.69607398e-07
Iter: 1248 loss: 2.67169213e-07
Iter: 1249 loss: 2.66987058e-07
Iter: 1250 loss: 2.67350089e-07
Iter: 1251 loss: 2.66911e-07
Iter: 1252 loss: 2.66799645e-07
Iter: 1253 loss: 2.66464355e-07
Iter: 1254 loss: 2.68283713e-07
Iter: 1255 loss: 2.66411604e-07
Iter: 1256 loss: 2.66018731e-07
Iter: 1257 loss: 2.69302376e-07
Iter: 1258 loss: 2.66008442e-07
Iter: 1259 loss: 2.65779875e-07
Iter: 1260 loss: 2.68189751e-07
Iter: 1261 loss: 2.65780585e-07
Iter: 1262 loss: 2.65585754e-07
Iter: 1263 loss: 2.66224845e-07
Iter: 1264 loss: 2.65536983e-07
Iter: 1265 loss: 2.65425285e-07
Iter: 1266 loss: 2.65222127e-07
Iter: 1267 loss: 2.65219256e-07
Iter: 1268 loss: 2.6489937e-07
Iter: 1269 loss: 2.67591304e-07
Iter: 1270 loss: 2.64904941e-07
Iter: 1271 loss: 2.64731057e-07
Iter: 1272 loss: 2.64581189e-07
Iter: 1273 loss: 2.64537903e-07
Iter: 1274 loss: 2.64231915e-07
Iter: 1275 loss: 2.64791083e-07
Iter: 1276 loss: 2.64086026e-07
Iter: 1277 loss: 2.6377117e-07
Iter: 1278 loss: 2.68589929e-07
Iter: 1279 loss: 2.63781743e-07
Iter: 1280 loss: 2.63557922e-07
Iter: 1281 loss: 2.64151481e-07
Iter: 1282 loss: 2.63468564e-07
Iter: 1283 loss: 2.63311961e-07
Iter: 1284 loss: 2.63315115e-07
Iter: 1285 loss: 2.6317656e-07
Iter: 1286 loss: 2.62942649e-07
Iter: 1287 loss: 2.67703e-07
Iter: 1288 loss: 2.6291346e-07
Iter: 1289 loss: 2.62711808e-07
Iter: 1290 loss: 2.62746369e-07
Iter: 1291 loss: 2.62543267e-07
Iter: 1292 loss: 2.62297476e-07
Iter: 1293 loss: 2.64841475e-07
Iter: 1294 loss: 2.62293838e-07
Iter: 1295 loss: 2.62107221e-07
Iter: 1296 loss: 2.6282504e-07
Iter: 1297 loss: 2.6206942e-07
Iter: 1298 loss: 2.61828831e-07
Iter: 1299 loss: 2.6213354e-07
Iter: 1300 loss: 2.61705338e-07
Iter: 1301 loss: 2.61500361e-07
Iter: 1302 loss: 2.62223e-07
Iter: 1303 loss: 2.61440562e-07
Iter: 1304 loss: 2.61208527e-07
Iter: 1305 loss: 2.61489788e-07
Iter: 1306 loss: 2.61108028e-07
Iter: 1307 loss: 2.60847798e-07
Iter: 1308 loss: 2.61101434e-07
Iter: 1309 loss: 2.60683208e-07
Iter: 1310 loss: 2.60432273e-07
Iter: 1311 loss: 2.60391687e-07
Iter: 1312 loss: 2.60233605e-07
Iter: 1313 loss: 2.6001905e-07
Iter: 1314 loss: 2.59986678e-07
Iter: 1315 loss: 2.59829051e-07
Iter: 1316 loss: 2.60082118e-07
Iter: 1317 loss: 2.59749413e-07
Iter: 1318 loss: 2.59530168e-07
Iter: 1319 loss: 2.60809941e-07
Iter: 1320 loss: 2.59525507e-07
Iter: 1321 loss: 2.59406193e-07
Iter: 1322 loss: 2.5928864e-07
Iter: 1323 loss: 2.59264709e-07
Iter: 1324 loss: 2.59061665e-07
Iter: 1325 loss: 2.58760736e-07
Iter: 1326 loss: 2.58736435e-07
Iter: 1327 loss: 2.58413621e-07
Iter: 1328 loss: 2.60107868e-07
Iter: 1329 loss: 2.5836016e-07
Iter: 1330 loss: 2.58130768e-07
Iter: 1331 loss: 2.5814191e-07
Iter: 1332 loss: 2.57958902e-07
Iter: 1333 loss: 2.58071225e-07
Iter: 1334 loss: 2.57822876e-07
Iter: 1335 loss: 2.57584873e-07
Iter: 1336 loss: 2.58148731e-07
Iter: 1337 loss: 2.57531042e-07
Iter: 1338 loss: 2.57311768e-07
Iter: 1339 loss: 2.58105899e-07
Iter: 1340 loss: 2.57254271e-07
Iter: 1341 loss: 2.57058645e-07
Iter: 1342 loss: 2.56763826e-07
Iter: 1343 loss: 2.56756721e-07
Iter: 1344 loss: 2.56486516e-07
Iter: 1345 loss: 2.581275e-07
Iter: 1346 loss: 2.56449653e-07
Iter: 1347 loss: 2.56182602e-07
Iter: 1348 loss: 2.57693273e-07
Iter: 1349 loss: 2.56123485e-07
Iter: 1350 loss: 2.55951392e-07
Iter: 1351 loss: 2.58799389e-07
Iter: 1352 loss: 2.55941245e-07
Iter: 1353 loss: 2.55818065e-07
Iter: 1354 loss: 2.55931582e-07
Iter: 1355 loss: 2.55744112e-07
Iter: 1356 loss: 2.55595e-07
Iter: 1357 loss: 2.55261085e-07
Iter: 1358 loss: 2.62551964e-07
Iter: 1359 loss: 2.55276149e-07
Iter: 1360 loss: 2.54998668e-07
Iter: 1361 loss: 2.55590095e-07
Iter: 1362 loss: 2.54891035e-07
Iter: 1363 loss: 2.54586382e-07
Iter: 1364 loss: 2.56185103e-07
Iter: 1365 loss: 2.54525389e-07
Iter: 1366 loss: 2.54353893e-07
Iter: 1367 loss: 2.56860176e-07
Iter: 1368 loss: 2.54345935e-07
Iter: 1369 loss: 2.54190468e-07
Iter: 1370 loss: 2.54084711e-07
Iter: 1371 loss: 2.54034944e-07
Iter: 1372 loss: 2.53854864e-07
Iter: 1373 loss: 2.54577117e-07
Iter: 1374 loss: 2.53812971e-07
Iter: 1375 loss: 2.53570818e-07
Iter: 1376 loss: 2.53523183e-07
Iter: 1377 loss: 2.53355211e-07
Iter: 1378 loss: 2.53136818e-07
Iter: 1379 loss: 2.53429619e-07
Iter: 1380 loss: 2.53033818e-07
Iter: 1381 loss: 2.52784901e-07
Iter: 1382 loss: 2.5390213e-07
Iter: 1383 loss: 2.52719076e-07
Iter: 1384 loss: 2.5259726e-07
Iter: 1385 loss: 2.52578303e-07
Iter: 1386 loss: 2.52460609e-07
Iter: 1387 loss: 2.52360849e-07
Iter: 1388 loss: 2.52317705e-07
Iter: 1389 loss: 2.52111789e-07
Iter: 1390 loss: 2.52943323e-07
Iter: 1391 loss: 2.52053184e-07
Iter: 1392 loss: 2.519009e-07
Iter: 1393 loss: 2.51632514e-07
Iter: 1394 loss: 2.56401279e-07
Iter: 1395 loss: 2.51617394e-07
Iter: 1396 loss: 2.51299781e-07
Iter: 1397 loss: 2.54422218e-07
Iter: 1398 loss: 2.51308364e-07
Iter: 1399 loss: 2.51094491e-07
Iter: 1400 loss: 2.51536278e-07
Iter: 1401 loss: 2.51035772e-07
Iter: 1402 loss: 2.50814765e-07
Iter: 1403 loss: 2.53635562e-07
Iter: 1404 loss: 2.50795779e-07
Iter: 1405 loss: 2.50688373e-07
Iter: 1406 loss: 2.5045216e-07
Iter: 1407 loss: 2.53939731e-07
Iter: 1408 loss: 2.5043073e-07
Iter: 1409 loss: 2.50258722e-07
Iter: 1410 loss: 2.50261934e-07
Iter: 1411 loss: 2.50141852e-07
Iter: 1412 loss: 2.49978768e-07
Iter: 1413 loss: 2.49969645e-07
Iter: 1414 loss: 2.49788059e-07
Iter: 1415 loss: 2.49847687e-07
Iter: 1416 loss: 2.49608092e-07
Iter: 1417 loss: 2.49431167e-07
Iter: 1418 loss: 2.5145053e-07
Iter: 1419 loss: 2.49430599e-07
Iter: 1420 loss: 2.49207574e-07
Iter: 1421 loss: 2.49858772e-07
Iter: 1422 loss: 2.49135212e-07
Iter: 1423 loss: 2.48961186e-07
Iter: 1424 loss: 2.49731954e-07
Iter: 1425 loss: 2.48935578e-07
Iter: 1426 loss: 2.4881129e-07
Iter: 1427 loss: 2.48738871e-07
Iter: 1428 loss: 2.48682568e-07
Iter: 1429 loss: 2.48474379e-07
Iter: 1430 loss: 2.48536736e-07
Iter: 1431 loss: 2.48328661e-07
Iter: 1432 loss: 2.48071842e-07
Iter: 1433 loss: 2.48288671e-07
Iter: 1434 loss: 2.47923026e-07
Iter: 1435 loss: 2.47792485e-07
Iter: 1436 loss: 2.47751643e-07
Iter: 1437 loss: 2.476188e-07
Iter: 1438 loss: 2.47575372e-07
Iter: 1439 loss: 2.47512105e-07
Iter: 1440 loss: 2.47325829e-07
Iter: 1441 loss: 2.47369883e-07
Iter: 1442 loss: 2.47180765e-07
Iter: 1443 loss: 2.46991135e-07
Iter: 1444 loss: 2.4999548e-07
Iter: 1445 loss: 2.46987611e-07
Iter: 1446 loss: 2.46855848e-07
Iter: 1447 loss: 2.46558187e-07
Iter: 1448 loss: 2.51049414e-07
Iter: 1449 loss: 2.46549803e-07
Iter: 1450 loss: 2.4628045e-07
Iter: 1451 loss: 2.48858868e-07
Iter: 1452 loss: 2.46237391e-07
Iter: 1453 loss: 2.46069476e-07
Iter: 1454 loss: 2.470411e-07
Iter: 1455 loss: 2.46053503e-07
Iter: 1456 loss: 2.45840226e-07
Iter: 1457 loss: 2.4630549e-07
Iter: 1458 loss: 2.45742683e-07
Iter: 1459 loss: 2.45602621e-07
Iter: 1460 loss: 2.45642298e-07
Iter: 1461 loss: 2.45538416e-07
Iter: 1462 loss: 2.45305415e-07
Iter: 1463 loss: 2.4548774e-07
Iter: 1464 loss: 2.45192098e-07
Iter: 1465 loss: 2.45034471e-07
Iter: 1466 loss: 2.45232286e-07
Iter: 1467 loss: 2.44965378e-07
Iter: 1468 loss: 2.44729392e-07
Iter: 1469 loss: 2.44991526e-07
Iter: 1470 loss: 2.44605161e-07
Iter: 1471 loss: 2.44445857e-07
Iter: 1472 loss: 2.44430083e-07
Iter: 1473 loss: 2.44306534e-07
Iter: 1474 loss: 2.44017855e-07
Iter: 1475 loss: 2.47895031e-07
Iter: 1476 loss: 2.43994378e-07
Iter: 1477 loss: 2.43717295e-07
Iter: 1478 loss: 2.46662609e-07
Iter: 1479 loss: 2.43703198e-07
Iter: 1480 loss: 2.43486454e-07
Iter: 1481 loss: 2.44056707e-07
Iter: 1482 loss: 2.43403264e-07
Iter: 1483 loss: 2.43214799e-07
Iter: 1484 loss: 2.43409033e-07
Iter: 1485 loss: 2.4309017e-07
Iter: 1486 loss: 2.42907646e-07
Iter: 1487 loss: 2.43126323e-07
Iter: 1488 loss: 2.427889e-07
Iter: 1489 loss: 2.42745159e-07
Iter: 1490 loss: 2.42646877e-07
Iter: 1491 loss: 2.42582701e-07
Iter: 1492 loss: 2.42445822e-07
Iter: 1493 loss: 2.4243576e-07
Iter: 1494 loss: 2.42298256e-07
Iter: 1495 loss: 2.42651652e-07
Iter: 1496 loss: 2.42230215e-07
Iter: 1497 loss: 2.42061503e-07
Iter: 1498 loss: 2.42474471e-07
Iter: 1499 loss: 2.4198124e-07
Iter: 1500 loss: 2.41802127e-07
Iter: 1501 loss: 2.41685314e-07
Iter: 1502 loss: 2.41648593e-07
Iter: 1503 loss: 2.4143759e-07
Iter: 1504 loss: 2.41434918e-07
Iter: 1505 loss: 2.41244152e-07
Iter: 1506 loss: 2.4151143e-07
Iter: 1507 loss: 2.41152549e-07
Iter: 1508 loss: 2.40962208e-07
Iter: 1509 loss: 2.40884816e-07
Iter: 1510 loss: 2.4077562e-07
Iter: 1511 loss: 2.40569364e-07
Iter: 1512 loss: 2.42828719e-07
Iter: 1513 loss: 2.4054998e-07
Iter: 1514 loss: 2.40354836e-07
Iter: 1515 loss: 2.40241434e-07
Iter: 1516 loss: 2.40154435e-07
Iter: 1517 loss: 2.39938288e-07
Iter: 1518 loss: 2.40752598e-07
Iter: 1519 loss: 2.39907934e-07
Iter: 1520 loss: 2.39682549e-07
Iter: 1521 loss: 2.39835799e-07
Iter: 1522 loss: 2.39568351e-07
Iter: 1523 loss: 2.39615645e-07
Iter: 1524 loss: 2.39479675e-07
Iter: 1525 loss: 2.39397139e-07
Iter: 1526 loss: 2.3915166e-07
Iter: 1527 loss: 2.39960457e-07
Iter: 1528 loss: 2.3905335e-07
Iter: 1529 loss: 2.38931307e-07
Iter: 1530 loss: 2.38901265e-07
Iter: 1531 loss: 2.38780416e-07
Iter: 1532 loss: 2.38581748e-07
Iter: 1533 loss: 2.38593486e-07
Iter: 1534 loss: 2.38374142e-07
Iter: 1535 loss: 2.38750147e-07
Iter: 1536 loss: 2.38268342e-07
Iter: 1537 loss: 2.38084027e-07
Iter: 1538 loss: 2.38591923e-07
Iter: 1539 loss: 2.37998876e-07
Iter: 1540 loss: 2.37806788e-07
Iter: 1541 loss: 2.40056124e-07
Iter: 1542 loss: 2.37825262e-07
Iter: 1543 loss: 2.37658085e-07
Iter: 1544 loss: 2.37550452e-07
Iter: 1545 loss: 2.37491719e-07
Iter: 1546 loss: 2.37345077e-07
Iter: 1547 loss: 2.38587688e-07
Iter: 1548 loss: 2.37327129e-07
Iter: 1549 loss: 2.37206677e-07
Iter: 1550 loss: 2.37242219e-07
Iter: 1551 loss: 2.37111806e-07
Iter: 1552 loss: 2.36933872e-07
Iter: 1553 loss: 2.36913053e-07
Iter: 1554 loss: 2.3676175e-07
Iter: 1555 loss: 2.36559174e-07
Iter: 1556 loss: 2.36708161e-07
Iter: 1557 loss: 2.36461204e-07
Iter: 1558 loss: 2.36434033e-07
Iter: 1559 loss: 2.36323331e-07
Iter: 1560 loss: 2.36265009e-07
Iter: 1561 loss: 2.36051505e-07
Iter: 1562 loss: 2.36664306e-07
Iter: 1563 loss: 2.35940391e-07
Iter: 1564 loss: 2.35707802e-07
Iter: 1565 loss: 2.3748089e-07
Iter: 1566 loss: 2.35670555e-07
Iter: 1567 loss: 2.35467837e-07
Iter: 1568 loss: 2.36916037e-07
Iter: 1569 loss: 2.35464171e-07
Iter: 1570 loss: 2.35255186e-07
Iter: 1571 loss: 2.35533165e-07
Iter: 1572 loss: 2.35168443e-07
Iter: 1573 loss: 2.35013985e-07
Iter: 1574 loss: 2.34825919e-07
Iter: 1575 loss: 2.34809931e-07
Iter: 1576 loss: 2.34732028e-07
Iter: 1577 loss: 2.34677501e-07
Iter: 1578 loss: 2.34574969e-07
Iter: 1579 loss: 2.34512669e-07
Iter: 1580 loss: 2.34457843e-07
Iter: 1581 loss: 2.34268242e-07
Iter: 1582 loss: 2.34104235e-07
Iter: 1583 loss: 2.34076765e-07
Iter: 1584 loss: 2.33789706e-07
Iter: 1585 loss: 2.34938398e-07
Iter: 1586 loss: 2.33745538e-07
Iter: 1587 loss: 2.33561593e-07
Iter: 1588 loss: 2.33552214e-07
Iter: 1589 loss: 2.33419343e-07
Iter: 1590 loss: 2.3336969e-07
Iter: 1591 loss: 2.33314609e-07
Iter: 1592 loss: 2.3316197e-07
Iter: 1593 loss: 2.33175768e-07
Iter: 1594 loss: 2.33052447e-07
Iter: 1595 loss: 2.33150629e-07
Iter: 1596 loss: 2.32986793e-07
Iter: 1597 loss: 2.32877596e-07
Iter: 1598 loss: 2.32647182e-07
Iter: 1599 loss: 2.36514623e-07
Iter: 1600 loss: 2.32661407e-07
Iter: 1601 loss: 2.32441351e-07
Iter: 1602 loss: 2.3498562e-07
Iter: 1603 loss: 2.324376e-07
Iter: 1604 loss: 2.32246663e-07
Iter: 1605 loss: 2.32768372e-07
Iter: 1606 loss: 2.32193656e-07
Iter: 1607 loss: 2.31995955e-07
Iter: 1608 loss: 2.3210977e-07
Iter: 1609 loss: 2.31853008e-07
Iter: 1610 loss: 2.3166109e-07
Iter: 1611 loss: 2.32200478e-07
Iter: 1612 loss: 2.31604659e-07
Iter: 1613 loss: 2.31392988e-07
Iter: 1614 loss: 2.33403554e-07
Iter: 1615 loss: 2.313962e-07
Iter: 1616 loss: 2.3128726e-07
Iter: 1617 loss: 2.3116516e-07
Iter: 1618 loss: 2.31130173e-07
Iter: 1619 loss: 2.30938383e-07
Iter: 1620 loss: 2.31018049e-07
Iter: 1621 loss: 2.30794683e-07
Iter: 1622 loss: 2.30540707e-07
Iter: 1623 loss: 2.30756697e-07
Iter: 1624 loss: 2.3040144e-07
Iter: 1625 loss: 2.30239294e-07
Iter: 1626 loss: 2.30221573e-07
Iter: 1627 loss: 2.30090791e-07
Iter: 1628 loss: 2.30715386e-07
Iter: 1629 loss: 2.30079095e-07
Iter: 1630 loss: 2.29932283e-07
Iter: 1631 loss: 2.30368528e-07
Iter: 1632 loss: 2.29898021e-07
Iter: 1633 loss: 2.29731455e-07
Iter: 1634 loss: 2.29904089e-07
Iter: 1635 loss: 2.2965358e-07
Iter: 1636 loss: 2.29531722e-07
Iter: 1637 loss: 2.29315475e-07
Iter: 1638 loss: 2.34322385e-07
Iter: 1639 loss: 2.29312647e-07
Iter: 1640 loss: 2.29147588e-07
Iter: 1641 loss: 2.29153301e-07
Iter: 1642 loss: 2.29024607e-07
Iter: 1643 loss: 2.2924921e-07
Iter: 1644 loss: 2.28964424e-07
Iter: 1645 loss: 2.28813633e-07
Iter: 1646 loss: 2.28805519e-07
Iter: 1647 loss: 2.28691363e-07
Iter: 1648 loss: 2.28530638e-07
Iter: 1649 loss: 2.29448347e-07
Iter: 1650 loss: 2.28513414e-07
Iter: 1651 loss: 2.28346025e-07
Iter: 1652 loss: 2.28819459e-07
Iter: 1653 loss: 2.28278793e-07
Iter: 1654 loss: 2.28146433e-07
Iter: 1655 loss: 2.27953876e-07
Iter: 1656 loss: 2.27964506e-07
Iter: 1657 loss: 2.27674974e-07
Iter: 1658 loss: 2.29100237e-07
Iter: 1659 loss: 2.27649139e-07
Iter: 1660 loss: 2.27468504e-07
Iter: 1661 loss: 2.27452801e-07
Iter: 1662 loss: 2.27313819e-07
Iter: 1663 loss: 2.27150338e-07
Iter: 1664 loss: 2.27133015e-07
Iter: 1665 loss: 2.26990934e-07
Iter: 1666 loss: 2.27166936e-07
Iter: 1667 loss: 2.26914665e-07
Iter: 1668 loss: 2.26770197e-07
Iter: 1669 loss: 2.27271698e-07
Iter: 1670 loss: 2.26719919e-07
Iter: 1671 loss: 2.26601728e-07
Iter: 1672 loss: 2.26525671e-07
Iter: 1673 loss: 2.26467222e-07
Iter: 1674 loss: 2.26307435e-07
Iter: 1675 loss: 2.26897569e-07
Iter: 1676 loss: 2.26267403e-07
Iter: 1677 loss: 2.26102387e-07
Iter: 1678 loss: 2.26806662e-07
Iter: 1679 loss: 2.26081255e-07
Iter: 1680 loss: 2.25970183e-07
Iter: 1681 loss: 2.25930549e-07
Iter: 1682 loss: 2.25854905e-07
Iter: 1683 loss: 2.25704639e-07
Iter: 1684 loss: 2.27058237e-07
Iter: 1685 loss: 2.2568841e-07
Iter: 1686 loss: 2.25546245e-07
Iter: 1687 loss: 2.26191162e-07
Iter: 1688 loss: 2.25504309e-07
Iter: 1689 loss: 2.25397343e-07
Iter: 1690 loss: 2.25163859e-07
Iter: 1691 loss: 2.28025073e-07
Iter: 1692 loss: 2.25154366e-07
Iter: 1693 loss: 2.24872437e-07
Iter: 1694 loss: 2.27194903e-07
Iter: 1695 loss: 2.2487167e-07
Iter: 1696 loss: 2.24688577e-07
Iter: 1697 loss: 2.25046705e-07
Iter: 1698 loss: 2.2459605e-07
Iter: 1699 loss: 2.24489455e-07
Iter: 1700 loss: 2.24472586e-07
Iter: 1701 loss: 2.24382987e-07
Iter: 1702 loss: 2.24301345e-07
Iter: 1703 loss: 2.24264369e-07
Iter: 1704 loss: 2.24123738e-07
Iter: 1705 loss: 2.24669591e-07
Iter: 1706 loss: 2.24076558e-07
Iter: 1707 loss: 2.23945818e-07
Iter: 1708 loss: 2.23902802e-07
Iter: 1709 loss: 2.23794615e-07
Iter: 1710 loss: 2.23630607e-07
Iter: 1711 loss: 2.23843458e-07
Iter: 1712 loss: 2.23511677e-07
Iter: 1713 loss: 2.23344273e-07
Iter: 1714 loss: 2.23362477e-07
Iter: 1715 loss: 2.23233798e-07
Iter: 1716 loss: 2.23039905e-07
Iter: 1717 loss: 2.23043273e-07
Iter: 1718 loss: 2.22863576e-07
Iter: 1719 loss: 2.22861317e-07
Iter: 1720 loss: 2.22736176e-07
Iter: 1721 loss: 2.2307573e-07
Iter: 1722 loss: 2.22679319e-07
Iter: 1723 loss: 2.22517443e-07
Iter: 1724 loss: 2.22337007e-07
Iter: 1725 loss: 2.22312664e-07
Iter: 1726 loss: 2.22070298e-07
Iter: 1727 loss: 2.22338272e-07
Iter: 1728 loss: 2.21944944e-07
Iter: 1729 loss: 2.21765745e-07
Iter: 1730 loss: 2.21757858e-07
Iter: 1731 loss: 2.21658908e-07
Iter: 1732 loss: 2.21653295e-07
Iter: 1733 loss: 2.21573771e-07
Iter: 1734 loss: 2.21475887e-07
Iter: 1735 loss: 2.21443116e-07
Iter: 1736 loss: 2.2132761e-07
Iter: 1737 loss: 2.22126488e-07
Iter: 1738 loss: 2.21302955e-07
Iter: 1739 loss: 2.21187861e-07
Iter: 1740 loss: 2.21035862e-07
Iter: 1741 loss: 2.21027904e-07
Iter: 1742 loss: 2.20871073e-07
Iter: 1743 loss: 2.22028802e-07
Iter: 1744 loss: 2.20856435e-07
Iter: 1745 loss: 2.2073236e-07
Iter: 1746 loss: 2.21071275e-07
Iter: 1747 loss: 2.20680448e-07
Iter: 1748 loss: 2.20473751e-07
Iter: 1749 loss: 2.2045738e-07
Iter: 1750 loss: 2.20282743e-07
Iter: 1751 loss: 2.20150127e-07
Iter: 1752 loss: 2.22350295e-07
Iter: 1753 loss: 2.20151648e-07
Iter: 1754 loss: 2.19998469e-07
Iter: 1755 loss: 2.20136158e-07
Iter: 1756 loss: 2.19902702e-07
Iter: 1757 loss: 2.19763081e-07
Iter: 1758 loss: 2.19768808e-07
Iter: 1759 loss: 2.19628333e-07
Iter: 1760 loss: 2.19415725e-07
Iter: 1761 loss: 2.19572414e-07
Iter: 1762 loss: 2.19291053e-07
Iter: 1763 loss: 2.19126846e-07
Iter: 1764 loss: 2.2108722e-07
Iter: 1765 loss: 2.19129106e-07
Iter: 1766 loss: 2.1901576e-07
Iter: 1767 loss: 2.19013899e-07
Iter: 1768 loss: 2.18921869e-07
Iter: 1769 loss: 2.18854453e-07
Iter: 1770 loss: 2.18839872e-07
Iter: 1771 loss: 2.18732282e-07
Iter: 1772 loss: 2.192088e-07
Iter: 1773 loss: 2.18721183e-07
Iter: 1774 loss: 2.18589761e-07
Iter: 1775 loss: 2.18358537e-07
Iter: 1776 loss: 2.22833563e-07
Iter: 1777 loss: 2.18347552e-07
Iter: 1778 loss: 2.18169276e-07
Iter: 1779 loss: 2.20965177e-07
Iter: 1780 loss: 2.18180361e-07
Iter: 1781 loss: 2.18016027e-07
Iter: 1782 loss: 2.18324246e-07
Iter: 1783 loss: 2.17960789e-07
Iter: 1784 loss: 2.17808463e-07
Iter: 1785 loss: 2.18098904e-07
Iter: 1786 loss: 2.17746305e-07
Iter: 1787 loss: 2.17603684e-07
Iter: 1788 loss: 2.18656467e-07
Iter: 1789 loss: 2.17582908e-07
Iter: 1790 loss: 2.17458677e-07
Iter: 1791 loss: 2.17760672e-07
Iter: 1792 loss: 2.17402487e-07
Iter: 1793 loss: 2.17287351e-07
Iter: 1794 loss: 2.17059764e-07
Iter: 1795 loss: 2.20875066e-07
Iter: 1796 loss: 2.17059466e-07
Iter: 1797 loss: 2.16880963e-07
Iter: 1798 loss: 2.19333728e-07
Iter: 1799 loss: 2.16892118e-07
Iter: 1800 loss: 2.16767603e-07
Iter: 1801 loss: 2.17249749e-07
Iter: 1802 loss: 2.16723251e-07
Iter: 1803 loss: 2.16569219e-07
Iter: 1804 loss: 2.17807411e-07
Iter: 1805 loss: 2.16559769e-07
Iter: 1806 loss: 2.16490918e-07
Iter: 1807 loss: 2.16335408e-07
Iter: 1808 loss: 2.19479702e-07
Iter: 1809 loss: 2.16341192e-07
Iter: 1810 loss: 2.16156337e-07
Iter: 1811 loss: 2.17769582e-07
Iter: 1812 loss: 2.161635e-07
Iter: 1813 loss: 2.1605905e-07
Iter: 1814 loss: 2.16045464e-07
Iter: 1815 loss: 2.15962416e-07
Iter: 1816 loss: 2.15811539e-07
Iter: 1817 loss: 2.161011e-07
Iter: 1818 loss: 2.15737785e-07
Iter: 1819 loss: 2.15621313e-07
Iter: 1820 loss: 2.17039315e-07
Iter: 1821 loss: 2.15609276e-07
Iter: 1822 loss: 2.15503562e-07
Iter: 1823 loss: 2.15389463e-07
Iter: 1824 loss: 2.15374143e-07
Iter: 1825 loss: 2.15287258e-07
Iter: 1826 loss: 2.15266397e-07
Iter: 1827 loss: 2.15210463e-07
Iter: 1828 loss: 2.15145391e-07
Iter: 1829 loss: 2.15133838e-07
Iter: 1830 loss: 2.14994515e-07
Iter: 1831 loss: 2.14943611e-07
Iter: 1832 loss: 2.14875968e-07
Iter: 1833 loss: 2.14697963e-07
Iter: 1834 loss: 2.14743181e-07
Iter: 1835 loss: 2.14558867e-07
Iter: 1836 loss: 2.14494364e-07
Iter: 1837 loss: 2.14437151e-07
Iter: 1838 loss: 2.14339593e-07
Iter: 1839 loss: 2.14499153e-07
Iter: 1840 loss: 2.14262769e-07
Iter: 1841 loss: 2.14152877e-07
Iter: 1842 loss: 2.13930051e-07
Iter: 1843 loss: 2.17663029e-07
Iter: 1844 loss: 2.13914177e-07
Iter: 1845 loss: 2.13753452e-07
Iter: 1846 loss: 2.16276675e-07
Iter: 1847 loss: 2.13750738e-07
Iter: 1848 loss: 2.1359628e-07
Iter: 1849 loss: 2.1424249e-07
Iter: 1850 loss: 2.13560639e-07
Iter: 1851 loss: 2.13442277e-07
Iter: 1852 loss: 2.13440018e-07
Iter: 1853 loss: 2.13345146e-07
Iter: 1854 loss: 2.13215941e-07
Iter: 1855 loss: 2.13774257e-07
Iter: 1856 loss: 2.13189509e-07
Iter: 1857 loss: 2.13087318e-07
Iter: 1858 loss: 2.14510777e-07
Iter: 1859 loss: 2.13083197e-07
Iter: 1860 loss: 2.12985825e-07
Iter: 1861 loss: 2.12906684e-07
Iter: 1862 loss: 2.12902677e-07
Iter: 1863 loss: 2.12760597e-07
Iter: 1864 loss: 2.14173184e-07
Iter: 1865 loss: 2.12755111e-07
Iter: 1866 loss: 2.12663508e-07
Iter: 1867 loss: 2.12603126e-07
Iter: 1868 loss: 2.12557211e-07
Iter: 1869 loss: 2.12404345e-07
Iter: 1870 loss: 2.12628805e-07
Iter: 1871 loss: 2.12320117e-07
Iter: 1872 loss: 2.12267466e-07
Iter: 1873 loss: 2.12243137e-07
Iter: 1874 loss: 2.12145238e-07
Iter: 1875 loss: 2.11969208e-07
Iter: 1876 loss: 2.15027939e-07
Iter: 1877 loss: 2.11970047e-07
Iter: 1878 loss: 2.11832713e-07
Iter: 1879 loss: 2.12116078e-07
Iter: 1880 loss: 2.11759726e-07
Iter: 1881 loss: 2.11562622e-07
Iter: 1882 loss: 2.11528459e-07
Iter: 1883 loss: 2.11411276e-07
Iter: 1884 loss: 2.11246416e-07
Iter: 1885 loss: 2.11267633e-07
Iter: 1886 loss: 2.11104535e-07
Iter: 1887 loss: 2.1135908e-07
Iter: 1888 loss: 2.11044096e-07
Iter: 1889 loss: 2.10911367e-07
Iter: 1890 loss: 2.11230798e-07
Iter: 1891 loss: 2.10838422e-07
Iter: 1892 loss: 2.10736346e-07
Iter: 1893 loss: 2.10757548e-07
Iter: 1894 loss: 2.10647244e-07
Iter: 1895 loss: 2.10547284e-07
Iter: 1896 loss: 2.10544599e-07
Iter: 1897 loss: 2.10460286e-07
Iter: 1898 loss: 2.10400202e-07
Iter: 1899 loss: 2.10349071e-07
Iter: 1900 loss: 2.10239335e-07
Iter: 1901 loss: 2.11234422e-07
Iter: 1902 loss: 2.10227924e-07
Iter: 1903 loss: 2.10148912e-07
Iter: 1904 loss: 2.10138822e-07
Iter: 1905 loss: 2.10096687e-07
Iter: 1906 loss: 2.09938619e-07
Iter: 1907 loss: 2.09931017e-07
Iter: 1908 loss: 2.09826155e-07
Iter: 1909 loss: 2.09686803e-07
Iter: 1910 loss: 2.11611422e-07
Iter: 1911 loss: 2.09683051e-07
Iter: 1912 loss: 2.09551374e-07
Iter: 1913 loss: 2.10105199e-07
Iter: 1914 loss: 2.09523222e-07
Iter: 1915 loss: 2.09444821e-07
Iter: 1916 loss: 2.09343128e-07
Iter: 1917 loss: 2.09333891e-07
Iter: 1918 loss: 2.09168263e-07
Iter: 1919 loss: 2.09123399e-07
Iter: 1920 loss: 2.09027036e-07
Iter: 1921 loss: 2.08854658e-07
Iter: 1922 loss: 2.10456164e-07
Iter: 1923 loss: 2.08847865e-07
Iter: 1924 loss: 2.08715974e-07
Iter: 1925 loss: 2.09732661e-07
Iter: 1926 loss: 2.08708897e-07
Iter: 1927 loss: 2.08585533e-07
Iter: 1928 loss: 2.08671395e-07
Iter: 1929 loss: 2.08533621e-07
Iter: 1930 loss: 2.08399413e-07
Iter: 1931 loss: 2.0832374e-07
Iter: 1932 loss: 2.0825712e-07
Iter: 1933 loss: 2.08145394e-07
Iter: 1934 loss: 2.08143973e-07
Iter: 1935 loss: 2.08043716e-07
Iter: 1936 loss: 2.08121165e-07
Iter: 1937 loss: 2.07982836e-07
Iter: 1938 loss: 2.07858548e-07
Iter: 1939 loss: 2.07987853e-07
Iter: 1940 loss: 2.07813542e-07
Iter: 1941 loss: 2.07686668e-07
Iter: 1942 loss: 2.08293088e-07
Iter: 1943 loss: 2.07683968e-07
Iter: 1944 loss: 2.07529936e-07
Iter: 1945 loss: 2.07524863e-07
Iter: 1946 loss: 2.0741976e-07
Iter: 1947 loss: 2.07332832e-07
Iter: 1948 loss: 2.07323779e-07
Iter: 1949 loss: 2.07245364e-07
Iter: 1950 loss: 2.07172789e-07
Iter: 1951 loss: 2.07159e-07
Iter: 1952 loss: 2.07031732e-07
Iter: 1953 loss: 2.07047833e-07
Iter: 1954 loss: 2.06936249e-07
Iter: 1955 loss: 2.06773336e-07
Iter: 1956 loss: 2.06778211e-07
Iter: 1957 loss: 2.06656196e-07
Iter: 1958 loss: 2.06494832e-07
Iter: 1959 loss: 2.07899035e-07
Iter: 1960 loss: 2.06484287e-07
Iter: 1961 loss: 2.06341923e-07
Iter: 1962 loss: 2.06968252e-07
Iter: 1963 loss: 2.06315804e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2
+ date
Thu Oct 22 06:18:42 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/500_500_500_500_1 --function f1 --psi 2 --phi 2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1bb9a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1bc77268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1bcc89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1bcc8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1bc08488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1bc0bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1baabae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1bb78950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1bb78488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1bb2fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1bb4aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1ba7e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1ba7eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1ba12158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1b9f6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1b99b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1b99b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef84acae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1b97a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef84481e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef8448950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef8474378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef8422620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1ba24488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1ba36598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3f1ba366a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ed01ad950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ed0175620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ed01751e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef83ed488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ed00eb268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ed0101620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ed01010d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ed00f6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ed00789d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ed01356a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.04571793
test_loss: 0.043996323
train_loss: 0.01767628
test_loss: 0.018083623
train_loss: 0.009905461
test_loss: 0.010773419
train_loss: 0.0071682953
test_loss: 0.007960642
train_loss: 0.006050442
test_loss: 0.0070203077
train_loss: 0.0063365265
test_loss: 0.0066233636
train_loss: 0.005337013
test_loss: 0.006194602
train_loss: 0.0053020036
test_loss: 0.006112797
train_loss: 0.005567889
test_loss: 0.0057658916
train_loss: 0.0050018076
test_loss: 0.005481035
train_loss: 0.0045043607
test_loss: 0.0053777657
train_loss: 0.0046354276
test_loss: 0.005127265
train_loss: 0.004404698
test_loss: 0.005260785
train_loss: 0.0044969246
test_loss: 0.0054386724
train_loss: 0.004697279
test_loss: 0.0051887417
train_loss: 0.0043424587
test_loss: 0.0051867985
train_loss: 0.003934278
test_loss: 0.0047339397
train_loss: 0.004067983
test_loss: 0.004773248
train_loss: 0.004545923
test_loss: 0.005029854
train_loss: 0.004121206
test_loss: 0.0047103073
train_loss: 0.004348742
test_loss: 0.0049005225
train_loss: 0.004118357
test_loss: 0.004510754
train_loss: 0.0047999984
test_loss: 0.0051024356
train_loss: 0.0040754867
test_loss: 0.0047722925
train_loss: 0.0041907914
test_loss: 0.0047390643
train_loss: 0.0041034296
test_loss: 0.004502744
train_loss: 0.004281788
test_loss: 0.004732244
train_loss: 0.0037148425
test_loss: 0.004441961
train_loss: 0.0037902282
test_loss: 0.004452601
train_loss: 0.004063499
test_loss: 0.0044379076
train_loss: 0.003790096
test_loss: 0.004926757
train_loss: 0.0041313935
test_loss: 0.004493591
train_loss: 0.0040233335
test_loss: 0.0041646394
train_loss: 0.0035367967
test_loss: 0.0042622974
train_loss: 0.0037028748
test_loss: 0.0042353193
train_loss: 0.004291781
test_loss: 0.0044344836
train_loss: 0.0037350724
test_loss: 0.0045121815
train_loss: 0.0041239043
test_loss: 0.004454425
train_loss: 0.0037563208
test_loss: 0.004343141
train_loss: 0.0038397906
test_loss: 0.0042652413
train_loss: 0.0036264844
test_loss: 0.0042379643
train_loss: 0.0037733323
test_loss: 0.0042413585
train_loss: 0.003693819
test_loss: 0.0042730705
train_loss: 0.0041463003
test_loss: 0.0042790053
train_loss: 0.0037413505
test_loss: 0.004465838
train_loss: 0.004004681
test_loss: 0.0042274673
train_loss: 0.003457914
test_loss: 0.004395606
train_loss: 0.003416691
test_loss: 0.004247542
train_loss: 0.0033158995
test_loss: 0.004063294
train_loss: 0.0034127121
test_loss: 0.0039733937
train_loss: 0.0036653404
test_loss: 0.0045202537
train_loss: 0.0037011572
test_loss: 0.004268268
train_loss: 0.0036845752
test_loss: 0.0040914714
train_loss: 0.003924437
test_loss: 0.0042193485
train_loss: 0.0038882464
test_loss: 0.0041299406
train_loss: 0.0033700764
test_loss: 0.004273495
train_loss: 0.0037508106
test_loss: 0.004524139
train_loss: 0.0037666112
test_loss: 0.0042331545
train_loss: 0.0039797234
test_loss: 0.0044660163
train_loss: 0.003906733
test_loss: 0.0045644203
train_loss: 0.004082966
test_loss: 0.004526678
train_loss: 0.003937952
test_loss: 0.0043684
train_loss: 0.0034824149
test_loss: 0.0042985408
train_loss: 0.003783667
test_loss: 0.0046354886
train_loss: 0.0036836956
test_loss: 0.0042830734
train_loss: 0.0035997084
test_loss: 0.004237701
train_loss: 0.003649975
test_loss: 0.004142058
train_loss: 0.0037368224
test_loss: 0.004122125
train_loss: 0.003471988
test_loss: 0.0046320492
train_loss: 0.003505791
test_loss: 0.0042341566
train_loss: 0.0034377086
test_loss: 0.003992923
train_loss: 0.0032837708
test_loss: 0.0042607877
train_loss: 0.0033517124
test_loss: 0.004100044
train_loss: 0.003339734
test_loss: 0.0040618135
train_loss: 0.0035327792
test_loss: 0.0044286586
train_loss: 0.003808422
test_loss: 0.004370973
train_loss: 0.0034748013
test_loss: 0.004338037
train_loss: 0.0034239362
test_loss: 0.0042199222
train_loss: 0.0037996797
test_loss: 0.0041801585
train_loss: 0.0032974454
test_loss: 0.0040821093
train_loss: 0.0034378152
test_loss: 0.0038776011
train_loss: 0.0035952122
test_loss: 0.0043587894
train_loss: 0.0036122303
test_loss: 0.004420906
train_loss: 0.0033651418
test_loss: 0.0038747406
train_loss: 0.0035984293
test_loss: 0.004138498
train_loss: 0.0035735967
test_loss: 0.004213181
train_loss: 0.0033312067
test_loss: 0.0039563165
train_loss: 0.0034725382
test_loss: 0.0040493603
train_loss: 0.0035784668
test_loss: 0.0041877115
train_loss: 0.0034587693
test_loss: 0.0040592076
train_loss: 0.0032964745
test_loss: 0.003914403
train_loss: 0.0032953732
test_loss: 0.003958866
train_loss: 0.00343202
test_loss: 0.004134577
train_loss: 0.004072117
test_loss: 0.004463235
train_loss: 0.0033327853
test_loss: 0.0038571956
train_loss: 0.0032766846
test_loss: 0.0042007416
train_loss: 0.0035048425
test_loss: 0.0039632646
train_loss: 0.0031310935
test_loss: 0.0039582658
train_loss: 0.003332512
test_loss: 0.003917816
train_loss: 0.0032304479
test_loss: 0.0040781656
train_loss: 0.0035315955
test_loss: 0.0044292146
train_loss: 0.0035513553
test_loss: 0.004136915
train_loss: 0.00330011
test_loss: 0.004107426
train_loss: 0.0035434654
test_loss: 0.0042556846
train_loss: 0.0036798483
test_loss: 0.0040767435
train_loss: 0.0035049675
test_loss: 0.0043185875
train_loss: 0.003340242
test_loss: 0.0039650286
train_loss: 0.0034925751
test_loss: 0.004040468
train_loss: 0.0033971556
test_loss: 0.0042640073
train_loss: 0.003295591
test_loss: 0.004064209
train_loss: 0.0037575501
test_loss: 0.0042933333
train_loss: 0.0037197073
test_loss: 0.0040748175
train_loss: 0.0034138793
test_loss: 0.0039617713
train_loss: 0.0031724963
test_loss: 0.0041532037
train_loss: 0.0036058838
test_loss: 0.004325871
train_loss: 0.0034747017
test_loss: 0.004312144
train_loss: 0.0036840816
test_loss: 0.0048872256
train_loss: 0.003444482
test_loss: 0.0042211036
train_loss: 0.0038740318
test_loss: 0.004507228
train_loss: 0.0035782554
test_loss: 0.0040316926
train_loss: 0.0034649007
test_loss: 0.0041086255
train_loss: 0.0033904994
test_loss: 0.0042214645
train_loss: 0.0035763201
test_loss: 0.0041044233
train_loss: 0.0033710652
test_loss: 0.0044331416
train_loss: 0.0034572845
test_loss: 0.004378467
train_loss: 0.0036288255
test_loss: 0.004189675
train_loss: 0.0035154386
test_loss: 0.004183367
train_loss: 0.003312415
test_loss: 0.004060141
train_loss: 0.003425985
test_loss: 0.0039512175
train_loss: 0.0034679018
test_loss: 0.004105509
train_loss: 0.003463131
test_loss: 0.003963669
train_loss: 0.0033549333
test_loss: 0.0037588018
train_loss: 0.0033425665
test_loss: 0.003963042
train_loss: 0.0030559204
test_loss: 0.0038128756
train_loss: 0.0032514972
test_loss: 0.0038930739
train_loss: 0.004119955
test_loss: 0.0040556276
train_loss: 0.0034995535
test_loss: 0.0041882973
train_loss: 0.0036695108
test_loss: 0.0046096505
train_loss: 0.0037005362
test_loss: 0.004004176
train_loss: 0.0034575313
test_loss: 0.0038876992
train_loss: 0.0031956509
test_loss: 0.0038072625
train_loss: 0.0033571804
test_loss: 0.004029675
train_loss: 0.003439155
test_loss: 0.004014611
train_loss: 0.0034319465
test_loss: 0.0040510874
train_loss: 0.003186809
test_loss: 0.004008245
train_loss: 0.0031426433
test_loss: 0.003997791
train_loss: 0.0036157088
test_loss: 0.0043796874
train_loss: 0.0033251576
test_loss: 0.0045863762
train_loss: 0.0034119664
test_loss: 0.004104897
train_loss: 0.0033800711
test_loss: 0.003912392
train_loss: 0.0030599334
test_loss: 0.0038950662
train_loss: 0.0029688454
test_loss: 0.0036713541
train_loss: 0.0033651013
test_loss: 0.0040951273
train_loss: 0.003270806
test_loss: 0.0040309364
train_loss: 0.0031772084
test_loss: 0.003802809
train_loss: 0.0033783838
test_loss: 0.004092861
train_loss: 0.003605138
test_loss: 0.0040017213
train_loss: 0.003018661
test_loss: 0.003865612
train_loss: 0.0033011022
test_loss: 0.00399376
train_loss: 0.0033134033
test_loss: 0.003925159
train_loss: 0.0034846743
test_loss: 0.004180987
train_loss: 0.0032456887
test_loss: 0.003883589
train_loss: 0.003285241
test_loss: 0.0042046467
train_loss: 0.0030805978
test_loss: 0.0040450296
train_loss: 0.0034091787
test_loss: 0.003980574
train_loss: 0.0035143082
test_loss: 0.00400252
train_loss: 0.0033812753
test_loss: 0.004105645
train_loss: 0.0031783637
test_loss: 0.003993966
train_loss: 0.0033728378
test_loss: 0.0040179975
train_loss: 0.00348682
test_loss: 0.0038637095
train_loss: 0.0034348317
test_loss: 0.0039127087
train_loss: 0.0032021496
test_loss: 0.004096451
train_loss: 0.003296631
test_loss: 0.003834709
train_loss: 0.0033513114
test_loss: 0.0038292082
train_loss: 0.0034707657
test_loss: 0.0039288746
train_loss: 0.0029699663
test_loss: 0.0038797734
train_loss: 0.0032505076
test_loss: 0.0038937267
train_loss: 0.0033770027
test_loss: 0.004260711
train_loss: 0.003257536
test_loss: 0.0039322204
train_loss: 0.003239372
test_loss: 0.0040756916
train_loss: 0.0030214903
test_loss: 0.003962996
train_loss: 0.0033310312
test_loss: 0.003956063
train_loss: 0.0029965322
test_loss: 0.0038489506
train_loss: 0.0033895662
test_loss: 0.0040006586
train_loss: 0.0032039192
test_loss: 0.0038776742
train_loss: 0.0035034574
test_loss: 0.004024306
train_loss: 0.0035238932
test_loss: 0.0042552846
train_loss: 0.003342972
test_loss: 0.004113027
train_loss: 0.0031754675
test_loss: 0.0040334677
train_loss: 0.0031600895
test_loss: 0.004004238
train_loss: 0.0031327924
test_loss: 0.0036178576
train_loss: 0.003225062
test_loss: 0.003851989
train_loss: 0.0032622684
test_loss: 0.00409258
train_loss: 0.003698233
test_loss: 0.0038032292
train_loss: 0.0034733168
test_loss: 0.0039987676
train_loss: 0.0033226567
test_loss: 0.004235249
train_loss: 0.0034561502
test_loss: 0.004033205
train_loss: 0.0029757223
test_loss: 0.0037454653
train_loss: 0.0030201706
test_loss: 0.004033269
train_loss: 0.0034498593
test_loss: 0.0045654494
train_loss: 0.0033643888
test_loss: 0.0041026575
train_loss: 0.0036048687
test_loss: 0.0040174033
train_loss: 0.002976057
test_loss: 0.0037487352
train_loss: 0.003197135
test_loss: 0.0040556234
train_loss: 0.0031353915
test_loss: 0.003809221
train_loss: 0.0031118337
test_loss: 0.0037643397
train_loss: 0.0033842248
test_loss: 0.003895055
train_loss: 0.003455681
test_loss: 0.0045517893
train_loss: 0.003135985
test_loss: 0.0039434424
train_loss: 0.0032509349
test_loss: 0.0041750693
train_loss: 0.003204872
test_loss: 0.0041051395
train_loss: 0.0031920434
test_loss: 0.0038884026
train_loss: 0.0030595919
test_loss: 0.0037312047
train_loss: 0.0035958341
test_loss: 0.0038924455
train_loss: 0.0032786976
test_loss: 0.003910147
train_loss: 0.00296554
test_loss: 0.0040331692
train_loss: 0.0034753617
test_loss: 0.003899649
train_loss: 0.0030871672
test_loss: 0.0036042514
train_loss: 0.003351862
test_loss: 0.0042231847
train_loss: 0.0031372963
test_loss: 0.004028816
train_loss: 0.0035254024
test_loss: 0.004113106
train_loss: 0.0033008046
test_loss: 0.003993865
train_loss: 0.0034512964
test_loss: 0.003907923
train_loss: 0.0031470703
test_loss: 0.0039814645
train_loss: 0.0031999098
test_loss: 0.0040478646
train_loss: 0.0029016566
test_loss: 0.0037835543
train_loss: 0.0032485952
test_loss: 0.0038598252
train_loss: 0.0033902593
test_loss: 0.0041961675
train_loss: 0.0030703952
test_loss: 0.0037465324
train_loss: 0.0033496227
test_loss: 0.003943962
train_loss: 0.0030767631
test_loss: 0.0038482223
train_loss: 0.0033434296
test_loss: 0.004160569
train_loss: 0.0032719928
test_loss: 0.0039474443
train_loss: 0.0031692581
test_loss: 0.0039564576
train_loss: 0.0034680679
test_loss: 0.0040993188
train_loss: 0.0033106287
test_loss: 0.0039041738
train_loss: 0.0033336787
test_loss: 0.004240973
train_loss: 0.003552529
test_loss: 0.0041067414
train_loss: 0.003246347
test_loss: 0.004028062
train_loss: 0.0034130868
test_loss: 0.004035893
train_loss: 0.0030913681
test_loss: 0.0038106707
train_loss: 0.0031667731
test_loss: 0.003846332
train_loss: 0.0031086844
test_loss: 0.0038149038
train_loss: 0.0035354798
test_loss: 0.0039737774
train_loss: 0.0035523772
test_loss: 0.004788107
train_loss: 0.003672383
test_loss: 0.0040318226
train_loss: 0.0032063115
test_loss: 0.0038244005
train_loss: 0.003217724
test_loss: 0.0037442138
train_loss: 0.0030649286
test_loss: 0.0040609925
train_loss: 0.003712907
test_loss: 0.004023459
train_loss: 0.0037235115
test_loss: 0.0040943353
train_loss: 0.0030958673
test_loss: 0.003960777
train_loss: 0.0032315613
test_loss: 0.003978319
train_loss: 0.0031846915
test_loss: 0.0039389897
train_loss: 0.0033284542
test_loss: 0.0039503495
train_loss: 0.0032843796
test_loss: 0.00409551
train_loss: 0.0033126837
test_loss: 0.004128932
train_loss: 0.002999464
test_loss: 0.0039787064
train_loss: 0.0030406122
test_loss: 0.0038919093
train_loss: 0.0030198356
test_loss: 0.0038650122
train_loss: 0.0030669784
test_loss: 0.0039298376
train_loss: 0.0030517327
test_loss: 0.003975525
train_loss: 0.0033053444
test_loss: 0.0038999384
train_loss: 0.0033269315
test_loss: 0.0042272783
train_loss: 0.0035005365
test_loss: 0.004295229
train_loss: 0.00307172
test_loss: 0.004267464
train_loss: 0.003290622
test_loss: 0.004062928
train_loss: 0.003257622
test_loss: 0.0042044763
train_loss: 0.0034139007
test_loss: 0.004027076
train_loss: 0.003978629
test_loss: 0.0042348057
train_loss: 0.0031340509
test_loss: 0.0039081955
train_loss: 0.003002041
test_loss: 0.0038728903
train_loss: 0.0034035305
test_loss: 0.0039483295
train_loss: 0.0032376507
test_loss: 0.0038173595
train_loss: 0.003523769
test_loss: 0.0040964056
train_loss: 0.0030485028
test_loss: 0.0038080572
train_loss: 0.0032471346
test_loss: 0.0041106814
train_loss: 0.0031045554
test_loss: 0.0038358476
train_loss: 0.0032113246
test_loss: 0.003930317
train_loss: 0.003443065
test_loss: 0.0038564918
train_loss: 0.0028996617
test_loss: 0.0038410763
train_loss: 0.002923386
test_loss: 0.003886355
train_loss: 0.0027455033
test_loss: 0.0036010249
train_loss: 0.0029985288
test_loss: 0.0038321516
train_loss: 0.0031095487
test_loss: 0.0037544619
train_loss: 0.0032312272
test_loss: 0.0036544497
train_loss: 0.0033759533
test_loss: 0.004101098
train_loss: 0.0033974324
test_loss: 0.003928798
train_loss: 0.003408162
test_loss: 0.0040141856
train_loss: 0.0031129168
test_loss: 0.003910202
train_loss: 0.003024187
test_loss: 0.0039148084
train_loss: 0.0031440216
test_loss: 0.003705209
train_loss: 0.0031425462
test_loss: 0.003957248
train_loss: 0.0034307884
test_loss: 0.0039567715
train_loss: 0.0035745203
test_loss: 0.0040124874
train_loss: 0.002927918
test_loss: 0.0039189765
train_loss: 0.0030794076
test_loss: 0.004001381
train_loss: 0.003029072
test_loss: 0.0037566004
train_loss: 0.0030132327
test_loss: 0.003912466
train_loss: 0.0031587537
test_loss: 0.004129641
train_loss: 0.0036597312
test_loss: 0.004081466
train_loss: 0.0028910427
test_loss: 0.0037486649
train_loss: 0.0034092837
test_loss: 0.004097002
train_loss: 0.0033103772
test_loss: 0.0039741932
train_loss: 0.0034298138
test_loss: 0.0038657687
train_loss: 0.0034994818
test_loss: 0.0040063486
train_loss: 0.003177734
test_loss: 0.0041418485
train_loss: 0.0033097076
test_loss: 0.0039678114
train_loss: 0.0031865998
test_loss: 0.003954528
train_loss: 0.003196506
test_loss: 0.00390801
train_loss: 0.0036372798
test_loss: 0.0039190645
train_loss: 0.0029590023
test_loss: 0.003933305
train_loss: 0.0033185035
test_loss: 0.0039979992
train_loss: 0.0030187392
test_loss: 0.0041283383
train_loss: 0.0031818252
test_loss: 0.0038514081
train_loss: 0.003141618
test_loss: 0.0038759105
train_loss: 0.0030704306
test_loss: 0.003889272
train_loss: 0.0029965073
test_loss: 0.0038065873
train_loss: 0.0031201704
test_loss: 0.0038106837
train_loss: 0.0031924392
test_loss: 0.0037751242
train_loss: 0.0036392487
test_loss: 0.004335644
train_loss: 0.0031090057
test_loss: 0.0042745494
train_loss: 0.0031412486
test_loss: 0.003980337
train_loss: 0.0033704462
test_loss: 0.0043379357
train_loss: 0.0037428993
test_loss: 0.004043312
train_loss: 0.0029335814
test_loss: 0.0037943805
train_loss: 0.0030190765
test_loss: 0.0037713512
train_loss: 0.0030403205
test_loss: 0.0039553866
train_loss: 0.0033552935
test_loss: 0.0040038736
train_loss: 0.0035674295
test_loss: 0.003811244
train_loss: 0.0032873424
test_loss: 0.004143774
train_loss: 0.0034250373
test_loss: 0.004140264
train_loss: 0.0032227407
test_loss: 0.0041476027
train_loss: 0.003451345
test_loss: 0.003858742
train_loss: 0.0032469875
test_loss: 0.0037172108
train_loss: 0.0031893975
test_loss: 0.003990604
train_loss: 0.0031506463
test_loss: 0.003928008
train_loss: 0.0031259353
test_loss: 0.003892017
train_loss: 0.0032638335
test_loss: 0.0039859186
train_loss: 0.0031871228/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0038463855
train_loss: 0.0028969985
test_loss: 0.0037630168
train_loss: 0.003129417
test_loss: 0.0041539213
train_loss: 0.0034092776
test_loss: 0.003748319
train_loss: 0.0031820075
test_loss: 0.0038043968
train_loss: 0.0028668465
test_loss: 0.0037673714
train_loss: 0.0034166812
test_loss: 0.0040476066
train_loss: 0.0031822016
test_loss: 0.003954099
train_loss: 0.0032676093
test_loss: 0.0038917754
train_loss: 0.0030515883
test_loss: 0.0039583496
train_loss: 0.0030038403
test_loss: 0.0037632268
train_loss: 0.0031200869
test_loss: 0.0040799375
train_loss: 0.0031737047
test_loss: 0.0041124565
train_loss: 0.0031244212
test_loss: 0.003927039
train_loss: 0.0031741154
test_loss: 0.0037638205
train_loss: 0.0030175843
test_loss: 0.0038768258
train_loss: 0.003290273
test_loss: 0.0039874446
train_loss: 0.0031713033
test_loss: 0.004052012
train_loss: 0.002987914
test_loss: 0.0035957096
train_loss: 0.0029477207
test_loss: 0.0039397697
train_loss: 0.003334524
test_loss: 0.0040338202
train_loss: 0.0030974399
test_loss: 0.004495471
train_loss: 0.0032072223
test_loss: 0.0038090635
train_loss: 0.0029793428
test_loss: 0.003807474
train_loss: 0.0033493787
test_loss: 0.0039903615
train_loss: 0.0031811004
test_loss: 0.003910865
train_loss: 0.0032071555
test_loss: 0.0037624266
train_loss: 0.00301283
test_loss: 0.003768951
train_loss: 0.003263358
test_loss: 0.0038269
train_loss: 0.0030108967
test_loss: 0.003832428
train_loss: 0.003078664
test_loss: 0.003831451
train_loss: 0.0033154848
test_loss: 0.004008015
train_loss: 0.0029442306
test_loss: 0.0036642973
train_loss: 0.0031453394
test_loss: 0.0036986815
train_loss: 0.0029650112
test_loss: 0.0039456063
train_loss: 0.0036198208
test_loss: 0.004143482
train_loss: 0.003444215
test_loss: 0.0037212062
train_loss: 0.0029628868
test_loss: 0.003770436
train_loss: 0.0027735282
test_loss: 0.0040079714
train_loss: 0.0029537617
test_loss: 0.003885805
train_loss: 0.002821943
test_loss: 0.0036483957
train_loss: 0.0030130367
test_loss: 0.0037169338
train_loss: 0.00335174
test_loss: 0.0039827316
train_loss: 0.0031746952
test_loss: 0.0038500126
train_loss: 0.0029183584
test_loss: 0.003885449
train_loss: 0.0031376635
test_loss: 0.0038913656
train_loss: 0.003165514
test_loss: 0.0038041635
train_loss: 0.002985937
test_loss: 0.0037245816
train_loss: 0.0030466602
test_loss: 0.0038869204
train_loss: 0.0032810124
test_loss: 0.004050829
train_loss: 0.003220674
test_loss: 0.0040037
train_loss: 0.0032196892
test_loss: 0.0039119963
train_loss: 0.0032095052
test_loss: 0.004113635
train_loss: 0.0031605759
test_loss: 0.0038448921
train_loss: 0.003029071
test_loss: 0.0037496628
train_loss: 0.0029135114
test_loss: 0.0036080815
train_loss: 0.0028376444
test_loss: 0.00402145
train_loss: 0.0029432515
test_loss: 0.0035985147
train_loss: 0.0031638606
test_loss: 0.0038386555
train_loss: 0.0030109894
test_loss: 0.0038430563
train_loss: 0.0030225758
test_loss: 0.004417842
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 2 --phi 2 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430d440d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430e31510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430e61ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430d962f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430db9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430db9bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430cf6048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430c99048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430cdf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430c65488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430c06840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430c3a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430c3aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430be3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430be3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430b901e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430bab2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430b77ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430b337b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430b2e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430b2eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9430af5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93f1e1d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93f1dce840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93f1dce730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93f1de0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93f1dce620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93f1d57950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93f1d576a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93f1d1d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93f1d1d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93cc45b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93cc45b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93cc4192f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93cc3ca840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f93cc377378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.5782816e-05
Iter: 2 loss: 3.71372626e-05
Iter: 3 loss: 1.94122513e-05
Iter: 4 loss: 1.75604819e-05
Iter: 5 loss: 1.91209219e-05
Iter: 6 loss: 1.64662306e-05
Iter: 7 loss: 1.56543374e-05
Iter: 8 loss: 1.82382828e-05
Iter: 9 loss: 1.54232694e-05
Iter: 10 loss: 1.45603553e-05
Iter: 11 loss: 1.36874733e-05
Iter: 12 loss: 1.35152613e-05
Iter: 13 loss: 1.26156274e-05
Iter: 14 loss: 2.29160669e-05
Iter: 15 loss: 1.26006635e-05
Iter: 16 loss: 1.18762546e-05
Iter: 17 loss: 1.12966845e-05
Iter: 18 loss: 1.10762321e-05
Iter: 19 loss: 1.00317138e-05
Iter: 20 loss: 1.60740219e-05
Iter: 21 loss: 9.89195178e-06
Iter: 22 loss: 9.2059845e-06
Iter: 23 loss: 9.26914163e-06
Iter: 24 loss: 8.67535528e-06
Iter: 25 loss: 8.07545803e-06
Iter: 26 loss: 8.07373908e-06
Iter: 27 loss: 7.70279075e-06
Iter: 28 loss: 7.89078604e-06
Iter: 29 loss: 7.45547595e-06
Iter: 30 loss: 7.10114546e-06
Iter: 31 loss: 7.47120112e-06
Iter: 32 loss: 6.90462048e-06
Iter: 33 loss: 6.61515969e-06
Iter: 34 loss: 6.62408274e-06
Iter: 35 loss: 6.38614301e-06
Iter: 36 loss: 6.25887606e-06
Iter: 37 loss: 6.19543925e-06
Iter: 38 loss: 6.012835e-06
Iter: 39 loss: 6.06687308e-06
Iter: 40 loss: 5.88141711e-06
Iter: 41 loss: 5.71101691e-06
Iter: 42 loss: 5.73431635e-06
Iter: 43 loss: 5.58130068e-06
Iter: 44 loss: 5.42820544e-06
Iter: 45 loss: 5.42812813e-06
Iter: 46 loss: 5.31328124e-06
Iter: 47 loss: 5.02928924e-06
Iter: 48 loss: 7.81842209e-06
Iter: 49 loss: 4.99231965e-06
Iter: 50 loss: 4.87010038e-06
Iter: 51 loss: 4.83219e-06
Iter: 52 loss: 4.72182091e-06
Iter: 53 loss: 4.58661589e-06
Iter: 54 loss: 4.57447823e-06
Iter: 55 loss: 4.37637846e-06
Iter: 56 loss: 5.80543292e-06
Iter: 57 loss: 4.35957e-06
Iter: 58 loss: 4.23639676e-06
Iter: 59 loss: 4.07969674e-06
Iter: 60 loss: 4.06790332e-06
Iter: 61 loss: 3.9018737e-06
Iter: 62 loss: 3.90166406e-06
Iter: 63 loss: 3.79346466e-06
Iter: 64 loss: 3.80594383e-06
Iter: 65 loss: 3.71068381e-06
Iter: 66 loss: 3.60120453e-06
Iter: 67 loss: 3.59990281e-06
Iter: 68 loss: 3.56713213e-06
Iter: 69 loss: 3.56624082e-06
Iter: 70 loss: 3.52815209e-06
Iter: 71 loss: 3.41981172e-06
Iter: 72 loss: 3.93717164e-06
Iter: 73 loss: 3.38181053e-06
Iter: 74 loss: 3.2888961e-06
Iter: 75 loss: 4.50022844e-06
Iter: 76 loss: 3.28795522e-06
Iter: 77 loss: 3.22411688e-06
Iter: 78 loss: 3.46630395e-06
Iter: 79 loss: 3.20885079e-06
Iter: 80 loss: 3.13892019e-06
Iter: 81 loss: 3.08961603e-06
Iter: 82 loss: 3.06483298e-06
Iter: 83 loss: 2.99245676e-06
Iter: 84 loss: 3.28711849e-06
Iter: 85 loss: 2.9768089e-06
Iter: 86 loss: 2.90955154e-06
Iter: 87 loss: 3.35369214e-06
Iter: 88 loss: 2.90238449e-06
Iter: 89 loss: 2.85286819e-06
Iter: 90 loss: 2.84067914e-06
Iter: 91 loss: 2.80931431e-06
Iter: 92 loss: 2.73888918e-06
Iter: 93 loss: 3.07017126e-06
Iter: 94 loss: 2.72589523e-06
Iter: 95 loss: 2.67195264e-06
Iter: 96 loss: 2.67763016e-06
Iter: 97 loss: 2.63043148e-06
Iter: 98 loss: 2.56097519e-06
Iter: 99 loss: 3.16952219e-06
Iter: 100 loss: 2.55737586e-06
Iter: 101 loss: 2.51591609e-06
Iter: 102 loss: 2.6722023e-06
Iter: 103 loss: 2.50602125e-06
Iter: 104 loss: 2.46376067e-06
Iter: 105 loss: 3.01369755e-06
Iter: 106 loss: 2.46344234e-06
Iter: 107 loss: 2.43516388e-06
Iter: 108 loss: 2.4214014e-06
Iter: 109 loss: 2.40787904e-06
Iter: 110 loss: 2.3764519e-06
Iter: 111 loss: 2.338169e-06
Iter: 112 loss: 2.33459582e-06
Iter: 113 loss: 2.28275758e-06
Iter: 114 loss: 2.91735205e-06
Iter: 115 loss: 2.28236149e-06
Iter: 116 loss: 2.2515917e-06
Iter: 117 loss: 2.44441799e-06
Iter: 118 loss: 2.24806445e-06
Iter: 119 loss: 2.22691688e-06
Iter: 120 loss: 2.17781303e-06
Iter: 121 loss: 2.79402775e-06
Iter: 122 loss: 2.17425486e-06
Iter: 123 loss: 2.15536875e-06
Iter: 124 loss: 2.14937336e-06
Iter: 125 loss: 2.1305159e-06
Iter: 126 loss: 2.11014094e-06
Iter: 127 loss: 2.10694816e-06
Iter: 128 loss: 2.06121717e-06
Iter: 129 loss: 2.17107436e-06
Iter: 130 loss: 2.04476464e-06
Iter: 131 loss: 2.00742306e-06
Iter: 132 loss: 2.01859575e-06
Iter: 133 loss: 1.98054704e-06
Iter: 134 loss: 1.93912729e-06
Iter: 135 loss: 2.24907649e-06
Iter: 136 loss: 1.93585674e-06
Iter: 137 loss: 1.90995115e-06
Iter: 138 loss: 2.16435251e-06
Iter: 139 loss: 1.90905939e-06
Iter: 140 loss: 1.88630884e-06
Iter: 141 loss: 2.11157931e-06
Iter: 142 loss: 1.88554168e-06
Iter: 143 loss: 1.8767031e-06
Iter: 144 loss: 1.85656279e-06
Iter: 145 loss: 2.12222858e-06
Iter: 146 loss: 1.85516024e-06
Iter: 147 loss: 1.8250721e-06
Iter: 148 loss: 1.89430011e-06
Iter: 149 loss: 1.81372616e-06
Iter: 150 loss: 1.79555332e-06
Iter: 151 loss: 1.94094628e-06
Iter: 152 loss: 1.79435551e-06
Iter: 153 loss: 1.77386028e-06
Iter: 154 loss: 1.7771273e-06
Iter: 155 loss: 1.75852756e-06
Iter: 156 loss: 1.735811e-06
Iter: 157 loss: 1.79730864e-06
Iter: 158 loss: 1.72837792e-06
Iter: 159 loss: 1.70484191e-06
Iter: 160 loss: 1.68898464e-06
Iter: 161 loss: 1.68021506e-06
Iter: 162 loss: 1.66518589e-06
Iter: 163 loss: 1.66322832e-06
Iter: 164 loss: 1.65014478e-06
Iter: 165 loss: 1.6385319e-06
Iter: 166 loss: 1.63508321e-06
Iter: 167 loss: 1.60764444e-06
Iter: 168 loss: 1.69561599e-06
Iter: 169 loss: 1.5998246e-06
Iter: 170 loss: 1.58441219e-06
Iter: 171 loss: 1.58391947e-06
Iter: 172 loss: 1.57181785e-06
Iter: 173 loss: 1.55247687e-06
Iter: 174 loss: 1.55239229e-06
Iter: 175 loss: 1.53354563e-06
Iter: 176 loss: 1.59209037e-06
Iter: 177 loss: 1.52801636e-06
Iter: 178 loss: 1.52165421e-06
Iter: 179 loss: 1.50843903e-06
Iter: 180 loss: 1.73460546e-06
Iter: 181 loss: 1.50801202e-06
Iter: 182 loss: 1.49727452e-06
Iter: 183 loss: 1.49723053e-06
Iter: 184 loss: 1.48845879e-06
Iter: 185 loss: 1.47912738e-06
Iter: 186 loss: 1.4773459e-06
Iter: 187 loss: 1.4562163e-06
Iter: 188 loss: 1.53711937e-06
Iter: 189 loss: 1.45123363e-06
Iter: 190 loss: 1.43644581e-06
Iter: 191 loss: 1.4521637e-06
Iter: 192 loss: 1.42816384e-06
Iter: 193 loss: 1.41286068e-06
Iter: 194 loss: 1.42049566e-06
Iter: 195 loss: 1.40243378e-06
Iter: 196 loss: 1.38679206e-06
Iter: 197 loss: 1.5399047e-06
Iter: 198 loss: 1.38637063e-06
Iter: 199 loss: 1.37434586e-06
Iter: 200 loss: 1.41400585e-06
Iter: 201 loss: 1.37105815e-06
Iter: 202 loss: 1.36020219e-06
Iter: 203 loss: 1.37638267e-06
Iter: 204 loss: 1.35500227e-06
Iter: 205 loss: 1.33932735e-06
Iter: 206 loss: 1.33461322e-06
Iter: 207 loss: 1.32526623e-06
Iter: 208 loss: 1.34633456e-06
Iter: 209 loss: 1.32093896e-06
Iter: 210 loss: 1.31717138e-06
Iter: 211 loss: 1.30599915e-06
Iter: 212 loss: 1.33844082e-06
Iter: 213 loss: 1.30023375e-06
Iter: 214 loss: 1.28638635e-06
Iter: 215 loss: 1.38839675e-06
Iter: 216 loss: 1.28532542e-06
Iter: 217 loss: 1.2766061e-06
Iter: 218 loss: 1.28091062e-06
Iter: 219 loss: 1.27078931e-06
Iter: 220 loss: 1.25777706e-06
Iter: 221 loss: 1.35886728e-06
Iter: 222 loss: 1.25683243e-06
Iter: 223 loss: 1.25036013e-06
Iter: 224 loss: 1.27643807e-06
Iter: 225 loss: 1.24887276e-06
Iter: 226 loss: 1.24158862e-06
Iter: 227 loss: 1.22813185e-06
Iter: 228 loss: 1.55207636e-06
Iter: 229 loss: 1.22817596e-06
Iter: 230 loss: 1.21706353e-06
Iter: 231 loss: 1.3431893e-06
Iter: 232 loss: 1.2168548e-06
Iter: 233 loss: 1.20749235e-06
Iter: 234 loss: 1.19929041e-06
Iter: 235 loss: 1.19684501e-06
Iter: 236 loss: 1.1882471e-06
Iter: 237 loss: 1.31040701e-06
Iter: 238 loss: 1.18829564e-06
Iter: 239 loss: 1.1786824e-06
Iter: 240 loss: 1.17389664e-06
Iter: 241 loss: 1.16928391e-06
Iter: 242 loss: 1.16147521e-06
Iter: 243 loss: 1.16149931e-06
Iter: 244 loss: 1.15684145e-06
Iter: 245 loss: 1.22793472e-06
Iter: 246 loss: 1.15684895e-06
Iter: 247 loss: 1.15282671e-06
Iter: 248 loss: 1.14089517e-06
Iter: 249 loss: 1.17876323e-06
Iter: 250 loss: 1.13494912e-06
Iter: 251 loss: 1.12440807e-06
Iter: 252 loss: 1.18478874e-06
Iter: 253 loss: 1.12296425e-06
Iter: 254 loss: 1.11571933e-06
Iter: 255 loss: 1.20935351e-06
Iter: 256 loss: 1.11574275e-06
Iter: 257 loss: 1.11053532e-06
Iter: 258 loss: 1.12469024e-06
Iter: 259 loss: 1.10893086e-06
Iter: 260 loss: 1.10263932e-06
Iter: 261 loss: 1.10405847e-06
Iter: 262 loss: 1.09801965e-06
Iter: 263 loss: 1.09078394e-06
Iter: 264 loss: 1.11420798e-06
Iter: 265 loss: 1.08880681e-06
Iter: 266 loss: 1.08081235e-06
Iter: 267 loss: 1.08261565e-06
Iter: 268 loss: 1.07491473e-06
Iter: 269 loss: 1.06637117e-06
Iter: 270 loss: 1.09253483e-06
Iter: 271 loss: 1.0638164e-06
Iter: 272 loss: 1.0544253e-06
Iter: 273 loss: 1.07984226e-06
Iter: 274 loss: 1.05130903e-06
Iter: 275 loss: 1.0474289e-06
Iter: 276 loss: 1.04734113e-06
Iter: 277 loss: 1.04412948e-06
Iter: 278 loss: 1.04442472e-06
Iter: 279 loss: 1.04166872e-06
Iter: 280 loss: 1.03619152e-06
Iter: 281 loss: 1.08749543e-06
Iter: 282 loss: 1.03598279e-06
Iter: 283 loss: 1.0332509e-06
Iter: 284 loss: 1.03071727e-06
Iter: 285 loss: 1.03020682e-06
Iter: 286 loss: 1.02641684e-06
Iter: 287 loss: 1.01804017e-06
Iter: 288 loss: 1.13424221e-06
Iter: 289 loss: 1.01760065e-06
Iter: 290 loss: 1.01101966e-06
Iter: 291 loss: 1.01092724e-06
Iter: 292 loss: 1.004674e-06
Iter: 293 loss: 1.02094418e-06
Iter: 294 loss: 1.00255863e-06
Iter: 295 loss: 9.95597e-07
Iter: 296 loss: 1.03534342e-06
Iter: 297 loss: 9.94601123e-07
Iter: 298 loss: 9.90967806e-07
Iter: 299 loss: 9.90257718e-07
Iter: 300 loss: 9.87769681e-07
Iter: 301 loss: 9.81386393e-07
Iter: 302 loss: 9.99143595e-07
Iter: 303 loss: 9.7930581e-07
Iter: 304 loss: 9.74462864e-07
Iter: 305 loss: 9.76452725e-07
Iter: 306 loss: 9.71011104e-07
Iter: 307 loss: 9.63317461e-07
Iter: 308 loss: 9.85153179e-07
Iter: 309 loss: 9.60889452e-07
Iter: 310 loss: 9.57146767e-07
Iter: 311 loss: 9.952563e-07
Iter: 312 loss: 9.56984422e-07
Iter: 313 loss: 9.54008669e-07
Iter: 314 loss: 9.847613e-07
Iter: 315 loss: 9.53977633e-07
Iter: 316 loss: 9.50847891e-07
Iter: 317 loss: 9.51464585e-07
Iter: 318 loss: 9.48435172e-07
Iter: 319 loss: 9.45136719e-07
Iter: 320 loss: 9.4449797e-07
Iter: 321 loss: 9.42357929e-07
Iter: 322 loss: 9.37653e-07
Iter: 323 loss: 9.40380346e-07
Iter: 324 loss: 9.34535137e-07
Iter: 325 loss: 9.28377858e-07
Iter: 326 loss: 9.22946072e-07
Iter: 327 loss: 9.21303e-07
Iter: 328 loss: 9.17841533e-07
Iter: 329 loss: 9.1681e-07
Iter: 330 loss: 9.13347378e-07
Iter: 331 loss: 9.25482539e-07
Iter: 332 loss: 9.12334883e-07
Iter: 333 loss: 9.0770834e-07
Iter: 334 loss: 9.06736659e-07
Iter: 335 loss: 9.03671605e-07
Iter: 336 loss: 9.00435225e-07
Iter: 337 loss: 9.01099213e-07
Iter: 338 loss: 8.97899668e-07
Iter: 339 loss: 8.92371816e-07
Iter: 340 loss: 9.23917128e-07
Iter: 341 loss: 8.91586296e-07
Iter: 342 loss: 8.87540068e-07
Iter: 343 loss: 8.9138689e-07
Iter: 344 loss: 8.8513616e-07
Iter: 345 loss: 8.79481e-07
Iter: 346 loss: 8.96901724e-07
Iter: 347 loss: 8.7778426e-07
Iter: 348 loss: 8.78471042e-07
Iter: 349 loss: 8.76018419e-07
Iter: 350 loss: 8.7462962e-07
Iter: 351 loss: 8.70409167e-07
Iter: 352 loss: 8.83558528e-07
Iter: 353 loss: 8.68443465e-07
Iter: 354 loss: 8.63547712e-07
Iter: 355 loss: 9.21507137e-07
Iter: 356 loss: 8.63459491e-07
Iter: 357 loss: 8.60223906e-07
Iter: 358 loss: 8.58202611e-07
Iter: 359 loss: 8.57038685e-07
Iter: 360 loss: 8.5120223e-07
Iter: 361 loss: 8.6203e-07
Iter: 362 loss: 8.48650643e-07
Iter: 363 loss: 8.44679903e-07
Iter: 364 loss: 8.67898e-07
Iter: 365 loss: 8.4414296e-07
Iter: 366 loss: 8.40799544e-07
Iter: 367 loss: 8.52948176e-07
Iter: 368 loss: 8.39949848e-07
Iter: 369 loss: 8.36865524e-07
Iter: 370 loss: 8.6646e-07
Iter: 371 loss: 8.36741606e-07
Iter: 372 loss: 8.34502828e-07
Iter: 373 loss: 8.32801902e-07
Iter: 374 loss: 8.3203463e-07
Iter: 375 loss: 8.29449107e-07
Iter: 376 loss: 8.29053306e-07
Iter: 377 loss: 8.27153599e-07
Iter: 378 loss: 8.2189797e-07
Iter: 379 loss: 8.36428455e-07
Iter: 380 loss: 8.20079777e-07
Iter: 381 loss: 8.16575323e-07
Iter: 382 loss: 8.34365608e-07
Iter: 383 loss: 8.15985572e-07
Iter: 384 loss: 8.12706219e-07
Iter: 385 loss: 8.65194409e-07
Iter: 386 loss: 8.12703377e-07
Iter: 387 loss: 8.10913434e-07
Iter: 388 loss: 8.10360234e-07
Iter: 389 loss: 8.09328128e-07
Iter: 390 loss: 8.07525169e-07
Iter: 391 loss: 8.06024104e-07
Iter: 392 loss: 8.05520131e-07
Iter: 393 loss: 8.02220086e-07
Iter: 394 loss: 8.16721467e-07
Iter: 395 loss: 8.01508236e-07
Iter: 396 loss: 7.99062036e-07
Iter: 397 loss: 7.96704398e-07
Iter: 398 loss: 7.96170184e-07
Iter: 399 loss: 7.90785236e-07
Iter: 400 loss: 8.27738916e-07
Iter: 401 loss: 7.90274271e-07
Iter: 402 loss: 7.87140266e-07
Iter: 403 loss: 7.86186376e-07
Iter: 404 loss: 7.84265126e-07
Iter: 405 loss: 7.7994855e-07
Iter: 406 loss: 8.16394618e-07
Iter: 407 loss: 7.7967519e-07
Iter: 408 loss: 7.77015202e-07
Iter: 409 loss: 8.10746712e-07
Iter: 410 loss: 7.76972115e-07
Iter: 411 loss: 7.75489411e-07
Iter: 412 loss: 7.71858595e-07
Iter: 413 loss: 8.09632695e-07
Iter: 414 loss: 7.71398959e-07
Iter: 415 loss: 7.67606878e-07
Iter: 416 loss: 7.93680101e-07
Iter: 417 loss: 7.67191295e-07
Iter: 418 loss: 7.65365598e-07
Iter: 419 loss: 7.65211382e-07
Iter: 420 loss: 7.63521371e-07
Iter: 421 loss: 7.74568662e-07
Iter: 422 loss: 7.63236358e-07
Iter: 423 loss: 7.6219635e-07
Iter: 424 loss: 7.59024203e-07
Iter: 425 loss: 7.74064119e-07
Iter: 426 loss: 7.57862381e-07
Iter: 427 loss: 7.55043629e-07
Iter: 428 loss: 8.00615737e-07
Iter: 429 loss: 7.55056305e-07
Iter: 430 loss: 7.52902338e-07
Iter: 431 loss: 7.54845701e-07
Iter: 432 loss: 7.5171522e-07
Iter: 433 loss: 7.4900646e-07
Iter: 434 loss: 7.57692646e-07
Iter: 435 loss: 7.48212358e-07
Iter: 436 loss: 7.45893203e-07
Iter: 437 loss: 7.46702e-07
Iter: 438 loss: 7.44321937e-07
Iter: 439 loss: 7.40904397e-07
Iter: 440 loss: 7.64141873e-07
Iter: 441 loss: 7.40652524e-07
Iter: 442 loss: 7.38740255e-07
Iter: 443 loss: 7.44889178e-07
Iter: 444 loss: 7.38207859e-07
Iter: 445 loss: 7.36130119e-07
Iter: 446 loss: 7.41362214e-07
Iter: 447 loss: 7.35338631e-07
Iter: 448 loss: 7.3309144e-07
Iter: 449 loss: 7.4070897e-07
Iter: 450 loss: 7.32475371e-07
Iter: 451 loss: 7.30911665e-07
Iter: 452 loss: 7.28346663e-07
Iter: 453 loss: 7.28377529e-07
Iter: 454 loss: 7.27180918e-07
Iter: 455 loss: 7.26618737e-07
Iter: 456 loss: 7.24873416e-07
Iter: 457 loss: 7.27767826e-07
Iter: 458 loss: 7.24041854e-07
Iter: 459 loss: 7.22648338e-07
Iter: 460 loss: 7.19882905e-07
Iter: 461 loss: 7.71959208e-07
Iter: 462 loss: 7.19887282e-07
Iter: 463 loss: 7.17142029e-07
Iter: 464 loss: 7.2328703e-07
Iter: 465 loss: 7.16077579e-07
Iter: 466 loss: 7.13691861e-07
Iter: 467 loss: 7.38582116e-07
Iter: 468 loss: 7.1357158e-07
Iter: 469 loss: 7.11811253e-07
Iter: 470 loss: 7.1240953e-07
Iter: 471 loss: 7.10599352e-07
Iter: 472 loss: 7.07819197e-07
Iter: 473 loss: 7.20512958e-07
Iter: 474 loss: 7.07287199e-07
Iter: 475 loss: 7.05573711e-07
Iter: 476 loss: 7.06599849e-07
Iter: 477 loss: 7.04480499e-07
Iter: 478 loss: 7.02206819e-07
Iter: 479 loss: 7.1973011e-07
Iter: 480 loss: 7.02034868e-07
Iter: 481 loss: 7.00422277e-07
Iter: 482 loss: 7.06122876e-07
Iter: 483 loss: 7.00040118e-07
Iter: 484 loss: 6.98293832e-07
Iter: 485 loss: 6.97483927e-07
Iter: 486 loss: 6.96617576e-07
Iter: 487 loss: 6.94051096e-07
Iter: 488 loss: 7.10281597e-07
Iter: 489 loss: 6.93778247e-07
Iter: 490 loss: 6.92741594e-07
Iter: 491 loss: 7.06133335e-07
Iter: 492 loss: 6.92702713e-07
Iter: 493 loss: 6.9129544e-07
Iter: 494 loss: 6.88211571e-07
Iter: 495 loss: 7.33770435e-07
Iter: 496 loss: 6.8805889e-07
Iter: 497 loss: 6.8578629e-07
Iter: 498 loss: 6.92104152e-07
Iter: 499 loss: 6.85054601e-07
Iter: 500 loss: 6.83081453e-07
Iter: 501 loss: 6.82775294e-07
Iter: 502 loss: 6.813749e-07
Iter: 503 loss: 6.78321442e-07
Iter: 504 loss: 6.93437187e-07
Iter: 505 loss: 6.778547e-07
Iter: 506 loss: 6.75494e-07
Iter: 507 loss: 6.91970399e-07
Iter: 508 loss: 6.75274521e-07
Iter: 509 loss: 6.73849399e-07
Iter: 510 loss: 6.78084234e-07
Iter: 511 loss: 6.73405793e-07
Iter: 512 loss: 6.71313273e-07
Iter: 513 loss: 6.69545614e-07
Iter: 514 loss: 6.68914936e-07
Iter: 515 loss: 6.67088784e-07
Iter: 516 loss: 6.67069571e-07
Iter: 517 loss: 6.6521568e-07
Iter: 518 loss: 6.65583343e-07
Iter: 519 loss: 6.63837113e-07
Iter: 520 loss: 6.62159209e-07
Iter: 521 loss: 6.74848934e-07
Iter: 522 loss: 6.62095204e-07
Iter: 523 loss: 6.60646265e-07
Iter: 524 loss: 6.60691228e-07
Iter: 525 loss: 6.59559362e-07
Iter: 526 loss: 6.58143676e-07
Iter: 527 loss: 6.58080353e-07
Iter: 528 loss: 6.57271812e-07
Iter: 529 loss: 6.56393581e-07
Iter: 530 loss: 6.56218504e-07
Iter: 531 loss: 6.55149051e-07
Iter: 532 loss: 6.524632e-07
Iter: 533 loss: 6.80211883e-07
Iter: 534 loss: 6.52166e-07
Iter: 535 loss: 6.49357162e-07
Iter: 536 loss: 6.8776626e-07
Iter: 537 loss: 6.49287756e-07
Iter: 538 loss: 6.47261288e-07
Iter: 539 loss: 6.46446551e-07
Iter: 540 loss: 6.45357318e-07
Iter: 541 loss: 6.42927716e-07
Iter: 542 loss: 6.72108627e-07
Iter: 543 loss: 6.4284643e-07
Iter: 544 loss: 6.41219e-07
Iter: 545 loss: 6.43397e-07
Iter: 546 loss: 6.40448263e-07
Iter: 547 loss: 6.38723463e-07
Iter: 548 loss: 6.46174385e-07
Iter: 549 loss: 6.38325844e-07
Iter: 550 loss: 6.36907885e-07
Iter: 551 loss: 6.44292129e-07
Iter: 552 loss: 6.36662946e-07
Iter: 553 loss: 6.35369361e-07
Iter: 554 loss: 6.40261192e-07
Iter: 555 loss: 6.35008803e-07
Iter: 556 loss: 6.34057528e-07
Iter: 557 loss: 6.32415777e-07
Iter: 558 loss: 6.32389344e-07
Iter: 559 loss: 6.31614512e-07
Iter: 560 loss: 6.31281864e-07
Iter: 561 loss: 6.30256352e-07
Iter: 562 loss: 6.32177603e-07
Iter: 563 loss: 6.29907731e-07
Iter: 564 loss: 6.29175588e-07
Iter: 565 loss: 6.2811182e-07
Iter: 566 loss: 6.28080613e-07
Iter: 567 loss: 6.26394e-07
Iter: 568 loss: 6.28344537e-07
Iter: 569 loss: 6.25539428e-07
Iter: 570 loss: 6.23906772e-07
Iter: 571 loss: 6.25419545e-07
Iter: 572 loss: 6.22971129e-07
Iter: 573 loss: 6.2100014e-07
Iter: 574 loss: 6.3155062e-07
Iter: 575 loss: 6.20751052e-07
Iter: 576 loss: 6.19201785e-07
Iter: 577 loss: 6.20116907e-07
Iter: 578 loss: 6.18245338e-07
Iter: 579 loss: 6.16218813e-07
Iter: 580 loss: 6.33235231e-07
Iter: 581 loss: 6.16169132e-07
Iter: 582 loss: 6.14995429e-07
Iter: 583 loss: 6.20857634e-07
Iter: 584 loss: 6.14776695e-07
Iter: 585 loss: 6.13725319e-07
Iter: 586 loss: 6.17832825e-07
Iter: 587 loss: 6.13466796e-07
Iter: 588 loss: 6.12553e-07
Iter: 589 loss: 6.14538237e-07
Iter: 590 loss: 6.12074132e-07
Iter: 591 loss: 6.10873258e-07
Iter: 592 loss: 6.10020606e-07
Iter: 593 loss: 6.09536869e-07
Iter: 594 loss: 6.09866561e-07
Iter: 595 loss: 6.0877e-07
Iter: 596 loss: 6.08309335e-07
Iter: 597 loss: 6.07029506e-07
Iter: 598 loss: 6.10658049e-07
Iter: 599 loss: 6.06319304e-07
Iter: 600 loss: 6.04106845e-07
Iter: 601 loss: 6.16017701e-07
Iter: 602 loss: 6.0372156e-07
Iter: 603 loss: 6.02315254e-07
Iter: 604 loss: 6.05348077e-07
Iter: 605 loss: 6.01733632e-07
Iter: 606 loss: 6.00254339e-07
Iter: 607 loss: 6.02715943e-07
Iter: 608 loss: 5.99534815e-07
Iter: 609 loss: 5.97904602e-07
Iter: 610 loss: 6.01488239e-07
Iter: 611 loss: 5.97366125e-07
Iter: 612 loss: 5.95805659e-07
Iter: 613 loss: 6.02747548e-07
Iter: 614 loss: 5.95567599e-07
Iter: 615 loss: 5.94073754e-07
Iter: 616 loss: 5.97661824e-07
Iter: 617 loss: 5.93609116e-07
Iter: 618 loss: 5.92066726e-07
Iter: 619 loss: 6.01253e-07
Iter: 620 loss: 5.91872379e-07
Iter: 621 loss: 5.90884838e-07
Iter: 622 loss: 5.97127894e-07
Iter: 623 loss: 5.90727041e-07
Iter: 624 loss: 5.89728529e-07
Iter: 625 loss: 5.8885513e-07
Iter: 626 loss: 5.88594787e-07
Iter: 627 loss: 5.8821297e-07
Iter: 628 loss: 5.8794393e-07
Iter: 629 loss: 5.87339969e-07
Iter: 630 loss: 5.86437181e-07
Iter: 631 loss: 5.86412852e-07
Iter: 632 loss: 5.85384555e-07
Iter: 633 loss: 5.84908662e-07
Iter: 634 loss: 5.84419581e-07
Iter: 635 loss: 5.83110079e-07
Iter: 636 loss: 5.90717377e-07
Iter: 637 loss: 5.82948189e-07
Iter: 638 loss: 5.81824622e-07
Iter: 639 loss: 5.82022665e-07
Iter: 640 loss: 5.80983624e-07
Iter: 641 loss: 5.79545656e-07
Iter: 642 loss: 5.86972874e-07
Iter: 643 loss: 5.79266725e-07
Iter: 644 loss: 5.78096092e-07
Iter: 645 loss: 5.79565835e-07
Iter: 646 loss: 5.77470587e-07
Iter: 647 loss: 5.76173136e-07
Iter: 648 loss: 5.75514377e-07
Iter: 649 loss: 5.748999e-07
Iter: 650 loss: 5.73503087e-07
Iter: 651 loss: 5.73421175e-07
Iter: 652 loss: 5.72548061e-07
Iter: 653 loss: 5.74302646e-07
Iter: 654 loss: 5.72135377e-07
Iter: 655 loss: 5.70978273e-07
Iter: 656 loss: 5.72470242e-07
Iter: 657 loss: 5.70388806e-07
Iter: 658 loss: 5.69278029e-07
Iter: 659 loss: 5.75960541e-07
Iter: 660 loss: 5.69071744e-07
Iter: 661 loss: 5.68237e-07
Iter: 662 loss: 5.8070674e-07
Iter: 663 loss: 5.68241376e-07
Iter: 664 loss: 5.67713755e-07
Iter: 665 loss: 5.66604854e-07
Iter: 666 loss: 5.8196224e-07
Iter: 667 loss: 5.66564779e-07
Iter: 668 loss: 5.65591961e-07
Iter: 669 loss: 5.69443898e-07
Iter: 670 loss: 5.65373625e-07
Iter: 671 loss: 5.64362e-07
Iter: 672 loss: 5.65083951e-07
Iter: 673 loss: 5.63734034e-07
Iter: 674 loss: 5.62463129e-07
Iter: 675 loss: 5.68795087e-07
Iter: 676 loss: 5.62191531e-07
Iter: 677 loss: 5.61014701e-07
Iter: 678 loss: 5.61331831e-07
Iter: 679 loss: 5.60240721e-07
Iter: 680 loss: 5.58960153e-07
Iter: 681 loss: 5.62200285e-07
Iter: 682 loss: 5.58559066e-07
Iter: 683 loss: 5.57245301e-07
Iter: 684 loss: 5.64772108e-07
Iter: 685 loss: 5.57041687e-07
Iter: 686 loss: 5.56179941e-07
Iter: 687 loss: 5.61327965e-07
Iter: 688 loss: 5.56007876e-07
Iter: 689 loss: 5.55039833e-07
Iter: 690 loss: 5.56456484e-07
Iter: 691 loss: 5.54478277e-07
Iter: 692 loss: 5.53500854e-07
Iter: 693 loss: 5.57426176e-07
Iter: 694 loss: 5.53303039e-07
Iter: 695 loss: 5.52653546e-07
Iter: 696 loss: 5.52639847e-07
Iter: 697 loss: 5.52093297e-07
Iter: 698 loss: 5.51743369e-07
Iter: 699 loss: 5.51571134e-07
Iter: 700 loss: 5.50666641e-07
Iter: 701 loss: 5.49872311e-07
Iter: 702 loss: 5.49666765e-07
Iter: 703 loss: 5.48515288e-07
Iter: 704 loss: 5.5077362e-07
Iter: 705 loss: 5.48011712e-07
Iter: 706 loss: 5.47151046e-07
Iter: 707 loss: 5.56138502e-07
Iter: 708 loss: 5.47108925e-07
Iter: 709 loss: 5.46161402e-07
Iter: 710 loss: 5.4559689e-07
Iter: 711 loss: 5.45260491e-07
Iter: 712 loss: 5.44068939e-07
Iter: 713 loss: 5.53281666e-07
Iter: 714 loss: 5.43989756e-07
Iter: 715 loss: 5.43031547e-07
Iter: 716 loss: 5.42356304e-07
Iter: 717 loss: 5.41971133e-07
Iter: 718 loss: 5.40695282e-07
Iter: 719 loss: 5.52039467e-07
Iter: 720 loss: 5.40586768e-07
Iter: 721 loss: 5.3954534e-07
Iter: 722 loss: 5.46771616e-07
Iter: 723 loss: 5.3951635e-07
Iter: 724 loss: 5.38714517e-07
Iter: 725 loss: 5.418309e-07
Iter: 726 loss: 5.38516417e-07
Iter: 727 loss: 5.38036431e-07
Iter: 728 loss: 5.40622636e-07
Iter: 729 loss: 5.37976234e-07
Iter: 730 loss: 5.37284791e-07
Iter: 731 loss: 5.38757831e-07
Iter: 732 loss: 5.3708186e-07
Iter: 733 loss: 5.36445896e-07
Iter: 734 loss: 5.36319931e-07
Iter: 735 loss: 5.35964205e-07
Iter: 736 loss: 5.35185336e-07
Iter: 737 loss: 5.3466e-07
Iter: 738 loss: 5.34379751e-07
Iter: 739 loss: 5.33068658e-07
Iter: 740 loss: 5.38058259e-07
Iter: 741 loss: 5.32799049e-07
Iter: 742 loss: 5.31664512e-07
Iter: 743 loss: 5.32899719e-07
Iter: 744 loss: 5.31035539e-07
Iter: 745 loss: 5.29849785e-07
Iter: 746 loss: 5.42559121e-07
Iter: 747 loss: 5.29886734e-07
Iter: 748 loss: 5.29113549e-07
Iter: 749 loss: 5.29279077e-07
Iter: 750 loss: 5.2854125e-07
Iter: 751 loss: 5.2746077e-07
Iter: 752 loss: 5.29225474e-07
Iter: 753 loss: 5.26881763e-07
Iter: 754 loss: 5.25734663e-07
Iter: 755 loss: 5.28827229e-07
Iter: 756 loss: 5.2536933e-07
Iter: 757 loss: 5.24379516e-07
Iter: 758 loss: 5.24355869e-07
Iter: 759 loss: 5.23803521e-07
Iter: 760 loss: 5.24604957e-07
Iter: 761 loss: 5.23514927e-07
Iter: 762 loss: 5.23018741e-07
Iter: 763 loss: 5.23001802e-07
Iter: 764 loss: 5.22607763e-07
Iter: 765 loss: 5.22226799e-07
Iter: 766 loss: 5.22218613e-07
Iter: 767 loss: 5.21637162e-07
Iter: 768 loss: 5.215756e-07
Iter: 769 loss: 5.21214588e-07
Iter: 770 loss: 5.20160086e-07
Iter: 771 loss: 5.2092264e-07
Iter: 772 loss: 5.19552259e-07
Iter: 773 loss: 5.18571824e-07
Iter: 774 loss: 5.21438778e-07
Iter: 775 loss: 5.183025e-07
Iter: 776 loss: 5.17177e-07
Iter: 777 loss: 5.18825232e-07
Iter: 778 loss: 5.16643638e-07
Iter: 779 loss: 5.15633133e-07
Iter: 780 loss: 5.21358459e-07
Iter: 781 loss: 5.1549307e-07
Iter: 782 loss: 5.1453992e-07
Iter: 783 loss: 5.1775595e-07
Iter: 784 loss: 5.14288672e-07
Iter: 785 loss: 5.13498662e-07
Iter: 786 loss: 5.13444036e-07
Iter: 787 loss: 5.12848771e-07
Iter: 788 loss: 5.11839744e-07
Iter: 789 loss: 5.2059346e-07
Iter: 790 loss: 5.11776307e-07
Iter: 791 loss: 5.11081566e-07
Iter: 792 loss: 5.18461889e-07
Iter: 793 loss: 5.11046665e-07
Iter: 794 loss: 5.10549739e-07
Iter: 795 loss: 5.12051258e-07
Iter: 796 loss: 5.10438213e-07
Iter: 797 loss: 5.09811e-07
Iter: 798 loss: 5.11269093e-07
Iter: 799 loss: 5.09590109e-07
Iter: 800 loss: 5.09179642e-07
Iter: 801 loss: 5.08433686e-07
Iter: 802 loss: 5.08426922e-07
Iter: 803 loss: 5.07417553e-07
Iter: 804 loss: 5.11109761e-07
Iter: 805 loss: 5.07148286e-07
Iter: 806 loss: 5.06387e-07
Iter: 807 loss: 5.10012569e-07
Iter: 808 loss: 5.06278184e-07
Iter: 809 loss: 5.05585717e-07
Iter: 810 loss: 5.04823e-07
Iter: 811 loss: 5.04728405e-07
Iter: 812 loss: 5.03537194e-07
Iter: 813 loss: 5.11248345e-07
Iter: 814 loss: 5.03497404e-07
Iter: 815 loss: 5.02444e-07
Iter: 816 loss: 5.07055574e-07
Iter: 817 loss: 5.02284422e-07
Iter: 818 loss: 5.01447289e-07
Iter: 819 loss: 5.02017713e-07
Iter: 820 loss: 5.00925921e-07
Iter: 821 loss: 4.99775467e-07
Iter: 822 loss: 5.0305448e-07
Iter: 823 loss: 4.99388875e-07
Iter: 824 loss: 4.98699478e-07
Iter: 825 loss: 5.07790105e-07
Iter: 826 loss: 4.98723409e-07
Iter: 827 loss: 4.98180384e-07
Iter: 828 loss: 5.00652789e-07
Iter: 829 loss: 4.98096938e-07
Iter: 830 loss: 4.97589213e-07
Iter: 831 loss: 4.99815314e-07
Iter: 832 loss: 4.97456881e-07
Iter: 833 loss: 4.96969733e-07
Iter: 834 loss: 4.96357359e-07
Iter: 835 loss: 4.96321604e-07
Iter: 836 loss: 4.95678819e-07
Iter: 837 loss: 4.957e-07
Iter: 838 loss: 4.95192694e-07
Iter: 839 loss: 4.94368408e-07
Iter: 840 loss: 5.02897478e-07
Iter: 841 loss: 4.94339133e-07
Iter: 842 loss: 4.93744437e-07
Iter: 843 loss: 4.92810898e-07
Iter: 844 loss: 4.92768265e-07
Iter: 845 loss: 4.91782316e-07
Iter: 846 loss: 4.98664576e-07
Iter: 847 loss: 4.91712171e-07
Iter: 848 loss: 4.9083917e-07
Iter: 849 loss: 4.94612721e-07
Iter: 850 loss: 4.90652155e-07
Iter: 851 loss: 4.90033756e-07
Iter: 852 loss: 4.9213412e-07
Iter: 853 loss: 4.89865215e-07
Iter: 854 loss: 4.89095385e-07
Iter: 855 loss: 4.89492322e-07
Iter: 856 loss: 4.88571459e-07
Iter: 857 loss: 4.87822376e-07
Iter: 858 loss: 4.90793184e-07
Iter: 859 loss: 4.87593638e-07
Iter: 860 loss: 4.86776969e-07
Iter: 861 loss: 4.93434925e-07
Iter: 862 loss: 4.86723138e-07
Iter: 863 loss: 4.86032548e-07
Iter: 864 loss: 4.89569175e-07
Iter: 865 loss: 4.85929377e-07
Iter: 866 loss: 4.85287956e-07
Iter: 867 loss: 4.86360818e-07
Iter: 868 loss: 4.85017949e-07
Iter: 869 loss: 4.84572183e-07
Iter: 870 loss: 4.83874942e-07
Iter: 871 loss: 4.8382276e-07
Iter: 872 loss: 4.83045483e-07
Iter: 873 loss: 4.86702902e-07
Iter: 874 loss: 4.82929579e-07
Iter: 875 loss: 4.82127064e-07
Iter: 876 loss: 4.83750114e-07
Iter: 877 loss: 4.81859388e-07
Iter: 878 loss: 4.81101893e-07
Iter: 879 loss: 4.8469542e-07
Iter: 880 loss: 4.81027485e-07
Iter: 881 loss: 4.80447738e-07
Iter: 882 loss: 4.79816947e-07
Iter: 883 loss: 4.79729238e-07
Iter: 884 loss: 4.78722e-07
Iter: 885 loss: 4.87368197e-07
Iter: 886 loss: 4.7869e-07
Iter: 887 loss: 4.78036441e-07
Iter: 888 loss: 4.80224344e-07
Iter: 889 loss: 4.77879269e-07
Iter: 890 loss: 4.77269623e-07
Iter: 891 loss: 4.79368566e-07
Iter: 892 loss: 4.77087838e-07
Iter: 893 loss: 4.76505733e-07
Iter: 894 loss: 4.76472138e-07
Iter: 895 loss: 4.76011195e-07
Iter: 896 loss: 4.75306706e-07
Iter: 897 loss: 4.75309548e-07
Iter: 898 loss: 4.748604e-07
Iter: 899 loss: 4.7643212e-07
Iter: 900 loss: 4.74761606e-07
Iter: 901 loss: 4.74394596e-07
Iter: 902 loss: 4.73950763e-07
Iter: 903 loss: 4.73883745e-07
Iter: 904 loss: 4.73246018e-07
Iter: 905 loss: 4.7385555e-07
Iter: 906 loss: 4.72864144e-07
Iter: 907 loss: 4.72149225e-07
Iter: 908 loss: 4.73337167e-07
Iter: 909 loss: 4.71807823e-07
Iter: 910 loss: 4.71148979e-07
Iter: 911 loss: 4.80441599e-07
Iter: 912 loss: 4.71164299e-07
Iter: 913 loss: 4.70755651e-07
Iter: 914 loss: 4.70412687e-07
Iter: 915 loss: 4.70316593e-07
Iter: 916 loss: 4.69432024e-07
Iter: 917 loss: 4.69571205e-07
Iter: 918 loss: 4.687451e-07
Iter: 919 loss: 4.67962906e-07
Iter: 920 loss: 4.74985768e-07
Iter: 921 loss: 4.67900975e-07
Iter: 922 loss: 4.67275697e-07
Iter: 923 loss: 4.69692793e-07
Iter: 924 loss: 4.67087176e-07
Iter: 925 loss: 4.66488757e-07
Iter: 926 loss: 4.67021948e-07
Iter: 927 loss: 4.66156592e-07
Iter: 928 loss: 4.65631189e-07
Iter: 929 loss: 4.65628773e-07
Iter: 930 loss: 4.65264236e-07
Iter: 931 loss: 4.67930562e-07
Iter: 932 loss: 4.65203726e-07
Iter: 933 loss: 4.65002131e-07
Iter: 934 loss: 4.6490959e-07
Iter: 935 loss: 4.64798376e-07
Iter: 936 loss: 4.64325382e-07
Iter: 937 loss: 4.63719886e-07
Iter: 938 loss: 4.63695841e-07
Iter: 939 loss: 4.63076134e-07
Iter: 940 loss: 4.64418633e-07
Iter: 941 loss: 4.62828524e-07
Iter: 942 loss: 4.62057614e-07
Iter: 943 loss: 4.64153516e-07
Iter: 944 loss: 4.6180736e-07
Iter: 945 loss: 4.61127883e-07
Iter: 946 loss: 4.66092843e-07
Iter: 947 loss: 4.61087467e-07
Iter: 948 loss: 4.60550609e-07
Iter: 949 loss: 4.60471512e-07
Iter: 950 loss: 4.6007824e-07
Iter: 951 loss: 4.59375542e-07
Iter: 952 loss: 4.62141202e-07
Iter: 953 loss: 4.59203136e-07
Iter: 954 loss: 4.58527865e-07
Iter: 955 loss: 4.60062381e-07
Iter: 956 loss: 4.58218437e-07
Iter: 957 loss: 4.57657052e-07
Iter: 958 loss: 4.60119452e-07
Iter: 959 loss: 4.57538732e-07
Iter: 960 loss: 4.56891314e-07
Iter: 961 loss: 4.59795217e-07
Iter: 962 loss: 4.56779844e-07
Iter: 963 loss: 4.56444127e-07
Iter: 964 loss: 4.56402631e-07
Iter: 965 loss: 4.56221358e-07
Iter: 966 loss: 4.55949277e-07
Iter: 967 loss: 4.55885186e-07
Iter: 968 loss: 4.55491175e-07
Iter: 969 loss: 4.56288149e-07
Iter: 970 loss: 4.55285601e-07
Iter: 971 loss: 4.54793252e-07
Iter: 972 loss: 4.55006841e-07
Iter: 973 loss: 4.54519324e-07
Iter: 974 loss: 4.53986843e-07
Iter: 975 loss: 4.53686908e-07
Iter: 976 loss: 4.53470221e-07
Iter: 977 loss: 4.52715597e-07
Iter: 978 loss: 4.60403498e-07
Iter: 979 loss: 4.52691239e-07
Iter: 980 loss: 4.5210038e-07
Iter: 981 loss: 4.52601427e-07
Iter: 982 loss: 4.5175824e-07
Iter: 983 loss: 4.50923039e-07
Iter: 984 loss: 4.53415282e-07
Iter: 985 loss: 4.50691289e-07
Iter: 986 loss: 4.50024942e-07
Iter: 987 loss: 4.50902e-07
Iter: 988 loss: 4.49726429e-07
Iter: 989 loss: 4.4926395e-07
Iter: 990 loss: 4.53785702e-07
Iter: 991 loss: 4.49235358e-07
Iter: 992 loss: 4.48776746e-07
Iter: 993 loss: 4.48772369e-07
Iter: 994 loss: 4.48418e-07
Iter: 995 loss: 4.48106334e-07
Iter: 996 loss: 4.48072115e-07
Iter: 997 loss: 4.47746032e-07
Iter: 998 loss: 4.48073308e-07
Iter: 999 loss: 4.47612223e-07
Iter: 1000 loss: 4.47296031e-07
Iter: 1001 loss: 4.47048279e-07
Iter: 1002 loss: 4.46961366e-07
Iter: 1003 loss: 4.46395603e-07
Iter: 1004 loss: 4.48864284e-07
Iter: 1005 loss: 4.46264323e-07
Iter: 1006 loss: 4.4592332e-07
Iter: 1007 loss: 4.45796616e-07
Iter: 1008 loss: 4.45599312e-07
Iter: 1009 loss: 4.45089711e-07
Iter: 1010 loss: 4.46851629e-07
Iter: 1011 loss: 4.44960079e-07
Iter: 1012 loss: 4.44491e-07
Iter: 1013 loss: 4.44286229e-07
Iter: 1014 loss: 4.44015427e-07
Iter: 1015 loss: 4.43314718e-07
Iter: 1016 loss: 4.52405885e-07
Iter: 1017 loss: 4.43270125e-07
Iter: 1018 loss: 4.42791475e-07
Iter: 1019 loss: 4.42325472e-07
Iter: 1020 loss: 4.42202236e-07
Iter: 1021 loss: 4.41655487e-07
Iter: 1022 loss: 4.47653349e-07
Iter: 1023 loss: 4.4165759e-07
Iter: 1024 loss: 4.41164957e-07
Iter: 1025 loss: 4.40864312e-07
Iter: 1026 loss: 4.40598484e-07
Iter: 1027 loss: 4.40139672e-07
Iter: 1028 loss: 4.40154395e-07
Iter: 1029 loss: 4.39923326e-07
Iter: 1030 loss: 4.39896155e-07
Iter: 1031 loss: 4.39672249e-07
Iter: 1032 loss: 4.39196981e-07
Iter: 1033 loss: 4.4562583e-07
Iter: 1034 loss: 4.39184504e-07
Iter: 1035 loss: 4.38713244e-07
Iter: 1036 loss: 4.43001795e-07
Iter: 1037 loss: 4.38702102e-07
Iter: 1038 loss: 4.38363656e-07
Iter: 1039 loss: 4.38592622e-07
Iter: 1040 loss: 4.3818261e-07
Iter: 1041 loss: 4.37839e-07
Iter: 1042 loss: 4.37803408e-07
Iter: 1043 loss: 4.37517713e-07
Iter: 1044 loss: 4.36962637e-07
Iter: 1045 loss: 4.38334325e-07
Iter: 1046 loss: 4.36772e-07
Iter: 1047 loss: 4.36271478e-07
Iter: 1048 loss: 4.39812311e-07
Iter: 1049 loss: 4.36224468e-07
Iter: 1050 loss: 4.35817526e-07
Iter: 1051 loss: 4.3547908e-07
Iter: 1052 loss: 4.35366417e-07
Iter: 1053 loss: 4.34639389e-07
Iter: 1054 loss: 4.41658244e-07
Iter: 1055 loss: 4.34623644e-07
Iter: 1056 loss: 4.34237762e-07
Iter: 1057 loss: 4.33873026e-07
Iter: 1058 loss: 4.33728758e-07
Iter: 1059 loss: 4.33124455e-07
Iter: 1060 loss: 4.36527358e-07
Iter: 1061 loss: 4.32999855e-07
Iter: 1062 loss: 4.32455039e-07
Iter: 1063 loss: 4.32564178e-07
Iter: 1064 loss: 4.32050655e-07
Iter: 1065 loss: 4.32577167e-07
Iter: 1066 loss: 4.31805034e-07
Iter: 1067 loss: 4.31629672e-07
Iter: 1068 loss: 4.31419153e-07
Iter: 1069 loss: 4.31383427e-07
Iter: 1070 loss: 4.31051546e-07
Iter: 1071 loss: 4.31201045e-07
Iter: 1072 loss: 4.30841084e-07
Iter: 1073 loss: 4.30299906e-07
Iter: 1074 loss: 4.32852687e-07
Iter: 1075 loss: 4.3026472e-07
Iter: 1076 loss: 4.29888587e-07
Iter: 1077 loss: 4.29441201e-07
Iter: 1078 loss: 4.29341583e-07
Iter: 1079 loss: 4.28965961e-07
Iter: 1080 loss: 4.35354877e-07
Iter: 1081 loss: 4.28999783e-07
Iter: 1082 loss: 4.28644654e-07
Iter: 1083 loss: 4.28157477e-07
Iter: 1084 loss: 4.28124366e-07
Iter: 1085 loss: 4.27464357e-07
Iter: 1086 loss: 4.33375732e-07
Iter: 1087 loss: 4.27399698e-07
Iter: 1088 loss: 4.27009667e-07
Iter: 1089 loss: 4.26958081e-07
Iter: 1090 loss: 4.26702201e-07
Iter: 1091 loss: 4.26074649e-07
Iter: 1092 loss: 4.2920513e-07
Iter: 1093 loss: 4.25990407e-07
Iter: 1094 loss: 4.25479698e-07
Iter: 1095 loss: 4.27638e-07
Iter: 1096 loss: 4.25360838e-07
Iter: 1097 loss: 4.24854306e-07
Iter: 1098 loss: 4.26213489e-07
Iter: 1099 loss: 4.24694576e-07
Iter: 1100 loss: 4.24529674e-07
Iter: 1101 loss: 4.24467942e-07
Iter: 1102 loss: 4.2424287e-07
Iter: 1103 loss: 4.23726874e-07
Iter: 1104 loss: 4.29512227e-07
Iter: 1105 loss: 4.23681286e-07
Iter: 1106 loss: 4.23306403e-07
Iter: 1107 loss: 4.25120817e-07
Iter: 1108 loss: 4.23224947e-07
Iter: 1109 loss: 4.22776708e-07
Iter: 1110 loss: 4.23820097e-07
Iter: 1111 loss: 4.22607798e-07
Iter: 1112 loss: 4.22138271e-07
Iter: 1113 loss: 4.22619962e-07
Iter: 1114 loss: 4.21880713e-07
Iter: 1115 loss: 4.21402945e-07
Iter: 1116 loss: 4.21597321e-07
Iter: 1117 loss: 4.21108e-07
Iter: 1118 loss: 4.20364557e-07
Iter: 1119 loss: 4.2309091e-07
Iter: 1120 loss: 4.20184477e-07
Iter: 1121 loss: 4.19670897e-07
Iter: 1122 loss: 4.21495713e-07
Iter: 1123 loss: 4.19552435e-07
Iter: 1124 loss: 4.19023905e-07
Iter: 1125 loss: 4.19773073e-07
Iter: 1126 loss: 4.18751341e-07
Iter: 1127 loss: 4.18357047e-07
Iter: 1128 loss: 4.19099592e-07
Iter: 1129 loss: 4.18143202e-07
Iter: 1130 loss: 4.17558937e-07
Iter: 1131 loss: 4.18674688e-07
Iter: 1132 loss: 4.17295468e-07
Iter: 1133 loss: 4.1701017e-07
Iter: 1134 loss: 4.16979447e-07
Iter: 1135 loss: 4.16729677e-07
Iter: 1136 loss: 4.1750522e-07
Iter: 1137 loss: 4.16672e-07
Iter: 1138 loss: 4.16446483e-07
Iter: 1139 loss: 4.15774565e-07
Iter: 1140 loss: 4.23117768e-07
Iter: 1141 loss: 4.15710588e-07
Iter: 1142 loss: 4.15233785e-07
Iter: 1143 loss: 4.20055585e-07
Iter: 1144 loss: 4.15249332e-07
Iter: 1145 loss: 4.14867316e-07
Iter: 1146 loss: 4.1633578e-07
Iter: 1147 loss: 4.14768039e-07
Iter: 1148 loss: 4.14432037e-07
Iter: 1149 loss: 4.14450483e-07
Iter: 1150 loss: 4.14114169e-07
Iter: 1151 loss: 4.13696455e-07
Iter: 1152 loss: 4.13513419e-07
Iter: 1153 loss: 4.13218572e-07
Iter: 1154 loss: 4.12749472e-07
Iter: 1155 loss: 4.19875619e-07
Iter: 1156 loss: 4.12733357e-07
Iter: 1157 loss: 4.12262693e-07
Iter: 1158 loss: 4.11998627e-07
Iter: 1159 loss: 4.11794787e-07
Iter: 1160 loss: 4.1121524e-07
Iter: 1161 loss: 4.15969396e-07
Iter: 1162 loss: 4.11127417e-07
Iter: 1163 loss: 4.10756911e-07
Iter: 1164 loss: 4.11127161e-07
Iter: 1165 loss: 4.10552957e-07
Iter: 1166 loss: 4.10217e-07
Iter: 1167 loss: 4.13607893e-07
Iter: 1168 loss: 4.10187056e-07
Iter: 1169 loss: 4.09924667e-07
Iter: 1170 loss: 4.09915344e-07
Iter: 1171 loss: 4.0968817e-07
Iter: 1172 loss: 4.09307148e-07
Iter: 1173 loss: 4.09309308e-07
Iter: 1174 loss: 4.0891905e-07
Iter: 1175 loss: 4.10225141e-07
Iter: 1176 loss: 4.08829663e-07
Iter: 1177 loss: 4.08504576e-07
Iter: 1178 loss: 4.08253015e-07
Iter: 1179 loss: 4.08142682e-07
Iter: 1180 loss: 4.0760591e-07
Iter: 1181 loss: 4.13846124e-07
Iter: 1182 loss: 4.07626771e-07
Iter: 1183 loss: 4.0736353e-07
Iter: 1184 loss: 4.07314701e-07
Iter: 1185 loss: 4.07175435e-07
Iter: 1186 loss: 4.0676008e-07
Iter: 1187 loss: 4.07058167e-07
Iter: 1188 loss: 4.06494109e-07
Iter: 1189 loss: 4.05949862e-07
Iter: 1190 loss: 4.06163565e-07
Iter: 1191 loss: 4.05558865e-07
Iter: 1192 loss: 4.04987247e-07
Iter: 1193 loss: 4.05002254e-07
Iter: 1194 loss: 4.04698454e-07
Iter: 1195 loss: 4.0433892e-07
Iter: 1196 loss: 4.04323544e-07
Iter: 1197 loss: 4.03698948e-07
Iter: 1198 loss: 4.06396168e-07
Iter: 1199 loss: 4.03589468e-07
Iter: 1200 loss: 4.03204979e-07
Iter: 1201 loss: 4.04325306e-07
Iter: 1202 loss: 4.03100671e-07
Iter: 1203 loss: 4.02848798e-07
Iter: 1204 loss: 4.02774816e-07
Iter: 1205 loss: 4.02621623e-07
Iter: 1206 loss: 4.02438332e-07
Iter: 1207 loss: 4.02386434e-07
Iter: 1208 loss: 4.0216e-07
Iter: 1209 loss: 4.01933846e-07
Iter: 1210 loss: 4.01859268e-07
Iter: 1211 loss: 4.01476484e-07
Iter: 1212 loss: 4.0525e-07
Iter: 1213 loss: 4.01431e-07
Iter: 1214 loss: 4.01170183e-07
Iter: 1215 loss: 4.02807188e-07
Iter: 1216 loss: 4.01153954e-07
Iter: 1217 loss: 4.00868657e-07
Iter: 1218 loss: 4.00367441e-07
Iter: 1219 loss: 4.10978686e-07
Iter: 1220 loss: 4.00395351e-07
Iter: 1221 loss: 3.99926876e-07
Iter: 1222 loss: 4.03655605e-07
Iter: 1223 loss: 3.99858692e-07
Iter: 1224 loss: 3.99470395e-07
Iter: 1225 loss: 4.00439774e-07
Iter: 1226 loss: 3.99359891e-07
Iter: 1227 loss: 3.99010332e-07
Iter: 1228 loss: 3.99901211e-07
Iter: 1229 loss: 3.98872658e-07
Iter: 1230 loss: 3.98439227e-07
Iter: 1231 loss: 3.98952636e-07
Iter: 1232 loss: 3.98164843e-07
Iter: 1233 loss: 3.97729394e-07
Iter: 1234 loss: 3.97945882e-07
Iter: 1235 loss: 3.9741991e-07
Iter: 1236 loss: 3.9712728e-07
Iter: 1237 loss: 3.97043209e-07
Iter: 1238 loss: 3.96737249e-07
Iter: 1239 loss: 3.98044023e-07
Iter: 1240 loss: 3.9666304e-07
Iter: 1241 loss: 3.96489611e-07
Iter: 1242 loss: 3.96152075e-07
Iter: 1243 loss: 4.03642844e-07
Iter: 1244 loss: 3.96165404e-07
Iter: 1245 loss: 3.95691842e-07
Iter: 1246 loss: 3.96968659e-07
Iter: 1247 loss: 3.95546095e-07
Iter: 1248 loss: 3.95227829e-07
Iter: 1249 loss: 3.97273737e-07
Iter: 1250 loss: 3.9517866e-07
Iter: 1251 loss: 3.94858176e-07
Iter: 1252 loss: 3.95699402e-07
Iter: 1253 loss: 3.94736958e-07
Iter: 1254 loss: 3.94412353e-07
Iter: 1255 loss: 3.94711094e-07
Iter: 1256 loss: 3.94236451e-07
Iter: 1257 loss: 3.93893657e-07
Iter: 1258 loss: 3.93455593e-07
Iter: 1259 loss: 3.93404775e-07
Iter: 1260 loss: 3.92863939e-07
Iter: 1261 loss: 4.01129824e-07
Iter: 1262 loss: 3.92863626e-07
Iter: 1263 loss: 3.92510827e-07
Iter: 1264 loss: 3.92912796e-07
Iter: 1265 loss: 3.92329724e-07
Iter: 1266 loss: 3.91929575e-07
Iter: 1267 loss: 3.941866e-07
Iter: 1268 loss: 3.91949015e-07
Iter: 1269 loss: 3.91537355e-07
Iter: 1270 loss: 3.91440778e-07
Iter: 1271 loss: 3.91251234e-07
Iter: 1272 loss: 3.91383907e-07
Iter: 1273 loss: 3.91028948e-07
Iter: 1274 loss: 3.90921343e-07
Iter: 1275 loss: 3.90637126e-07
Iter: 1276 loss: 3.94162726e-07
Iter: 1277 loss: 3.90653156e-07
Iter: 1278 loss: 3.90310674e-07
Iter: 1279 loss: 3.90143555e-07
Iter: 1280 loss: 3.89986724e-07
Iter: 1281 loss: 3.89593652e-07
Iter: 1282 loss: 3.9392566e-07
Iter: 1283 loss: 3.89639411e-07
Iter: 1284 loss: 3.89253842e-07
Iter: 1285 loss: 3.89714273e-07
Iter: 1286 loss: 3.89051024e-07
Iter: 1287 loss: 3.88671396e-07
Iter: 1288 loss: 3.90345377e-07
Iter: 1289 loss: 3.88575415e-07
Iter: 1290 loss: 3.88317432e-07
Iter: 1291 loss: 3.87774975e-07
Iter: 1292 loss: 4.00247444e-07
Iter: 1293 loss: 3.87755705e-07
Iter: 1294 loss: 3.87274923e-07
Iter: 1295 loss: 3.93384568e-07
Iter: 1296 loss: 3.87282569e-07
Iter: 1297 loss: 3.86930822e-07
Iter: 1298 loss: 3.8777705e-07
Iter: 1299 loss: 3.86774133e-07
Iter: 1300 loss: 3.86385068e-07
Iter: 1301 loss: 3.87384432e-07
Iter: 1302 loss: 3.86225679e-07
Iter: 1303 loss: 3.85884022e-07
Iter: 1304 loss: 3.8618154e-07
Iter: 1305 loss: 3.85649315e-07
Iter: 1306 loss: 3.85609951e-07
Iter: 1307 loss: 3.85485748e-07
Iter: 1308 loss: 3.85305128e-07
Iter: 1309 loss: 3.85072383e-07
Iter: 1310 loss: 3.85068972e-07
Iter: 1311 loss: 3.84764178e-07
Iter: 1312 loss: 3.84692697e-07
Iter: 1313 loss: 3.84469502e-07
Iter: 1314 loss: 3.8409982e-07
Iter: 1315 loss: 3.85454314e-07
Iter: 1316 loss: 3.84015266e-07
Iter: 1317 loss: 3.83533546e-07
Iter: 1318 loss: 3.84351608e-07
Iter: 1319 loss: 3.83322856e-07
Iter: 1320 loss: 3.82902499e-07
Iter: 1321 loss: 3.87376701e-07
Iter: 1322 loss: 3.82897923e-07
Iter: 1323 loss: 3.82717644e-07
Iter: 1324 loss: 3.82533926e-07
Iter: 1325 loss: 3.82500986e-07
Iter: 1326 loss: 3.8216649e-07
Iter: 1327 loss: 3.83093152e-07
Iter: 1328 loss: 3.8207844e-07
Iter: 1329 loss: 3.81778477e-07
Iter: 1330 loss: 3.81839072e-07
Iter: 1331 loss: 3.81557186e-07
Iter: 1332 loss: 3.81145185e-07
Iter: 1333 loss: 3.84300392e-07
Iter: 1334 loss: 3.81170423e-07
Iter: 1335 loss: 3.8082635e-07
Iter: 1336 loss: 3.80872478e-07
Iter: 1337 loss: 3.80565808e-07
Iter: 1338 loss: 3.80095173e-07
Iter: 1339 loss: 3.81742609e-07
Iter: 1340 loss: 3.79994106e-07
Iter: 1341 loss: 3.7984438e-07
Iter: 1342 loss: 3.79812406e-07
Iter: 1343 loss: 3.79612061e-07
Iter: 1344 loss: 3.79512926e-07
Iter: 1345 loss: 3.79423511e-07
Iter: 1346 loss: 3.7926435e-07
Iter: 1347 loss: 3.7910948e-07
Iter: 1348 loss: 3.79068894e-07
Iter: 1349 loss: 3.78713338e-07
Iter: 1350 loss: 3.78993718e-07
Iter: 1351 loss: 3.7853431e-07
Iter: 1352 loss: 3.78201747e-07
Iter: 1353 loss: 3.78195296e-07
Iter: 1354 loss: 3.77968519e-07
Iter: 1355 loss: 3.7809491e-07
Iter: 1356 loss: 3.77842525e-07
Iter: 1357 loss: 3.7741944e-07
Iter: 1358 loss: 3.77525794e-07
Iter: 1359 loss: 3.77148183e-07
Iter: 1360 loss: 3.76798937e-07
Iter: 1361 loss: 3.78774274e-07
Iter: 1362 loss: 3.7674522e-07
Iter: 1363 loss: 3.7646862e-07
Iter: 1364 loss: 3.76227149e-07
Iter: 1365 loss: 3.76105959e-07
Iter: 1366 loss: 3.75647289e-07
Iter: 1367 loss: 3.77515818e-07
Iter: 1368 loss: 3.75542356e-07
Iter: 1369 loss: 3.75094515e-07
Iter: 1370 loss: 3.76535411e-07
Iter: 1371 loss: 3.7493254e-07
Iter: 1372 loss: 3.74589376e-07
Iter: 1373 loss: 3.7716174e-07
Iter: 1374 loss: 3.74554645e-07
Iter: 1375 loss: 3.74322781e-07
Iter: 1376 loss: 3.7428731e-07
Iter: 1377 loss: 3.74143269e-07
Iter: 1378 loss: 3.73931641e-07
Iter: 1379 loss: 3.73915213e-07
Iter: 1380 loss: 3.73667206e-07
Iter: 1381 loss: 3.73396915e-07
Iter: 1382 loss: 3.73346495e-07
Iter: 1383 loss: 3.73005264e-07
Iter: 1384 loss: 3.75617503e-07
Iter: 1385 loss: 3.72940974e-07
Iter: 1386 loss: 3.72656274e-07
Iter: 1387 loss: 3.73894949e-07
Iter: 1388 loss: 3.72578313e-07
Iter: 1389 loss: 3.72236457e-07
Iter: 1390 loss: 3.72968259e-07
Iter: 1391 loss: 3.72108616e-07
Iter: 1392 loss: 3.71810614e-07
Iter: 1393 loss: 3.73123328e-07
Iter: 1394 loss: 3.71777844e-07
Iter: 1395 loss: 3.71511248e-07
Iter: 1396 loss: 3.71175474e-07
Iter: 1397 loss: 3.71166664e-07
Iter: 1398 loss: 3.70740423e-07
Iter: 1399 loss: 3.73603143e-07
Iter: 1400 loss: 3.70725559e-07
Iter: 1401 loss: 3.70394929e-07
Iter: 1402 loss: 3.7076822e-07
Iter: 1403 loss: 3.7025444e-07
Iter: 1404 loss: 3.69829337e-07
Iter: 1405 loss: 3.7079667e-07
Iter: 1406 loss: 3.6972483e-07
Iter: 1407 loss: 3.69377688e-07
Iter: 1408 loss: 3.73624971e-07
Iter: 1409 loss: 3.69398492e-07
Iter: 1410 loss: 3.69090884e-07
Iter: 1411 loss: 3.70459816e-07
Iter: 1412 loss: 3.69032193e-07
Iter: 1413 loss: 3.68892188e-07
Iter: 1414 loss: 3.68699546e-07
Iter: 1415 loss: 3.68680617e-07
Iter: 1416 loss: 3.68372127e-07
Iter: 1417 loss: 3.68910804e-07
Iter: 1418 loss: 3.68242496e-07
Iter: 1419 loss: 3.67949838e-07
Iter: 1420 loss: 3.68602116e-07
Iter: 1421 loss: 3.67822736e-07
Iter: 1422 loss: 3.67505521e-07
Iter: 1423 loss: 3.69588236e-07
Iter: 1424 loss: 3.67478151e-07
Iter: 1425 loss: 3.67253392e-07
Iter: 1426 loss: 3.68568379e-07
Iter: 1427 loss: 3.67244411e-07
Iter: 1428 loss: 3.67062796e-07
Iter: 1429 loss: 3.66746463e-07
Iter: 1430 loss: 3.66699226e-07
Iter: 1431 loss: 3.66308143e-07
Iter: 1432 loss: 3.69230122e-07
Iter: 1433 loss: 3.66259883e-07
Iter: 1434 loss: 3.65975779e-07
Iter: 1435 loss: 3.65800361e-07
Iter: 1436 loss: 3.65704096e-07
Iter: 1437 loss: 3.65216e-07
Iter: 1438 loss: 3.67504754e-07
Iter: 1439 loss: 3.65131172e-07
Iter: 1440 loss: 3.6479048e-07
Iter: 1441 loss: 3.65422807e-07
Iter: 1442 loss: 3.64644336e-07
Iter: 1443 loss: 3.64572031e-07
Iter: 1444 loss: 3.644451e-07
Iter: 1445 loss: 3.64278662e-07
Iter: 1446 loss: 3.64017609e-07
Iter: 1447 loss: 3.64030427e-07
Iter: 1448 loss: 3.63783215e-07
Iter: 1449 loss: 3.63724894e-07
Iter: 1450 loss: 3.635854e-07
Iter: 1451 loss: 3.6325639e-07
Iter: 1452 loss: 3.6445897e-07
Iter: 1453 loss: 3.63165668e-07
Iter: 1454 loss: 3.62773562e-07
Iter: 1455 loss: 3.62935623e-07
Iter: 1456 loss: 3.62560968e-07
Iter: 1457 loss: 3.62181538e-07
Iter: 1458 loss: 3.64806567e-07
Iter: 1459 loss: 3.62171306e-07
Iter: 1460 loss: 3.61888027e-07
Iter: 1461 loss: 3.62762165e-07
Iter: 1462 loss: 3.61833855e-07
Iter: 1463 loss: 3.61517948e-07
Iter: 1464 loss: 3.62379808e-07
Iter: 1465 loss: 3.61425464e-07
Iter: 1466 loss: 3.61234299e-07
Iter: 1467 loss: 3.61404631e-07
Iter: 1468 loss: 3.61132749e-07
Iter: 1469 loss: 3.60851118e-07
Iter: 1470 loss: 3.6117612e-07
Iter: 1471 loss: 3.60691871e-07
Iter: 1472 loss: 3.60415044e-07
Iter: 1473 loss: 3.60866807e-07
Iter: 1474 loss: 3.6029428e-07
Iter: 1475 loss: 3.59896944e-07
Iter: 1476 loss: 3.60671748e-07
Iter: 1477 loss: 3.59754722e-07
Iter: 1478 loss: 3.60011597e-07
Iter: 1479 loss: 3.59663773e-07
Iter: 1480 loss: 3.5959431e-07
Iter: 1481 loss: 3.59362048e-07
Iter: 1482 loss: 3.59787521e-07
Iter: 1483 loss: 3.5924225e-07
Iter: 1484 loss: 3.58907187e-07
Iter: 1485 loss: 3.59559323e-07
Iter: 1486 loss: 3.58736372e-07
Iter: 1487 loss: 3.58380873e-07
Iter: 1488 loss: 3.59193223e-07
Iter: 1489 loss: 3.58276338e-07
Iter: 1490 loss: 3.57969668e-07
Iter: 1491 loss: 3.59670622e-07
Iter: 1492 loss: 3.57943321e-07
Iter: 1493 loss: 3.57614624e-07
Iter: 1494 loss: 3.57695598e-07
Iter: 1495 loss: 3.57391116e-07
Iter: 1496 loss: 3.57112697e-07
Iter: 1497 loss: 3.60088251e-07
Iter: 1498 loss: 3.57094677e-07
Iter: 1499 loss: 3.56901182e-07
Iter: 1500 loss: 3.56713969e-07
Iter: 1501 loss: 3.56697086e-07
Iter: 1502 loss: 3.56465705e-07
Iter: 1503 loss: 3.56453256e-07
Iter: 1504 loss: 3.56275905e-07
Iter: 1505 loss: 3.56659825e-07
Iter: 1506 loss: 3.56217527e-07
Iter: 1507 loss: 3.55992881e-07
Iter: 1508 loss: 3.56037503e-07
Iter: 1509 loss: 3.55832583e-07
Iter: 1510 loss: 3.55700763e-07
Iter: 1511 loss: 3.57492183e-07
Iter: 1512 loss: 3.55692407e-07
Iter: 1513 loss: 3.55565305e-07
Iter: 1514 loss: 3.55732482e-07
Iter: 1515 loss: 3.55476345e-07
Iter: 1516 loss: 3.55307918e-07
Iter: 1517 loss: 3.55089412e-07
Iter: 1518 loss: 3.55063321e-07
Iter: 1519 loss: 3.54842825e-07
Iter: 1520 loss: 3.5654665e-07
Iter: 1521 loss: 3.54840864e-07
Iter: 1522 loss: 3.54653793e-07
Iter: 1523 loss: 3.54464532e-07
Iter: 1524 loss: 3.54422525e-07
Iter: 1525 loss: 3.54105111e-07
Iter: 1526 loss: 3.54466351e-07
Iter: 1527 loss: 3.53945268e-07
Iter: 1528 loss: 3.53645021e-07
Iter: 1529 loss: 3.55336624e-07
Iter: 1530 loss: 3.53562797e-07
Iter: 1531 loss: 3.53230917e-07
Iter: 1532 loss: 3.53736937e-07
Iter: 1533 loss: 3.53088296e-07
Iter: 1534 loss: 3.5286871e-07
Iter: 1535 loss: 3.5610816e-07
Iter: 1536 loss: 3.52868938e-07
Iter: 1537 loss: 3.52693462e-07
Iter: 1538 loss: 3.52507129e-07
Iter: 1539 loss: 3.52480498e-07
Iter: 1540 loss: 3.52252982e-07
Iter: 1541 loss: 3.54351243e-07
Iter: 1542 loss: 3.52234537e-07
Iter: 1543 loss: 3.52041e-07
Iter: 1544 loss: 3.52335263e-07
Iter: 1545 loss: 3.51959727e-07
Iter: 1546 loss: 3.51688271e-07
Iter: 1547 loss: 3.53535853e-07
Iter: 1548 loss: 3.5165624e-07
Iter: 1549 loss: 3.51514814e-07
Iter: 1550 loss: 3.53190813e-07
Iter: 1551 loss: 3.51499182e-07
Iter: 1552 loss: 3.51425456e-07
Iter: 1553 loss: 3.51199304e-07
Iter: 1554 loss: 3.54561e-07
Iter: 1555 loss: 3.5121576e-07
Iter: 1556 loss: 3.5092728e-07
Iter: 1557 loss: 3.51123219e-07
Iter: 1558 loss: 3.50778691e-07
Iter: 1559 loss: 3.50541711e-07
Iter: 1560 loss: 3.51347637e-07
Iter: 1561 loss: 3.50461107e-07
Iter: 1562 loss: 3.50173252e-07
Iter: 1563 loss: 3.51548152e-07
Iter: 1564 loss: 3.50135963e-07
Iter: 1565 loss: 3.49934908e-07
Iter: 1566 loss: 3.49743516e-07
Iter: 1567 loss: 3.49673428e-07
Iter: 1568 loss: 3.49412574e-07
Iter: 1569 loss: 3.52719667e-07
Iter: 1570 loss: 3.49411096e-07
Iter: 1571 loss: 3.49217657e-07
Iter: 1572 loss: 3.48971184e-07
Iter: 1573 loss: 3.48943615e-07
Iter: 1574 loss: 3.48735142e-07
Iter: 1575 loss: 3.487188e-07
Iter: 1576 loss: 3.48589708e-07
Iter: 1577 loss: 3.48417984e-07
Iter: 1578 loss: 3.48391666e-07
Iter: 1579 loss: 3.48131891e-07
Iter: 1580 loss: 3.49185427e-07
Iter: 1581 loss: 3.48061747e-07
Iter: 1582 loss: 3.47894058e-07
Iter: 1583 loss: 3.50001699e-07
Iter: 1584 loss: 3.47878483e-07
Iter: 1585 loss: 3.4766515e-07
Iter: 1586 loss: 3.48869946e-07
Iter: 1587 loss: 3.47642867e-07
Iter: 1588 loss: 3.47503942e-07
Iter: 1589 loss: 3.47881155e-07
Iter: 1590 loss: 3.47480068e-07
Iter: 1591 loss: 3.47345093e-07
Iter: 1592 loss: 3.47202757e-07
Iter: 1593 loss: 3.47185733e-07
Iter: 1594 loss: 3.46921411e-07
Iter: 1595 loss: 3.46758043e-07
Iter: 1596 loss: 3.46621334e-07
Iter: 1597 loss: 3.46321514e-07
Iter: 1598 loss: 3.48638935e-07
Iter: 1599 loss: 3.46283286e-07
Iter: 1600 loss: 3.46017146e-07
Iter: 1601 loss: 3.46581913e-07
Iter: 1602 loss: 3.4588524e-07
Iter: 1603 loss: 3.45601762e-07
Iter: 1604 loss: 3.46432699e-07
Iter: 1605 loss: 3.45534488e-07
Iter: 1606 loss: 3.45210196e-07
Iter: 1607 loss: 3.4712869e-07
Iter: 1608 loss: 3.45174129e-07
Iter: 1609 loss: 3.44998966e-07
Iter: 1610 loss: 3.44852282e-07
Iter: 1611 loss: 3.44781483e-07
Iter: 1612 loss: 3.44473762e-07
Iter: 1613 loss: 3.46733088e-07
Iter: 1614 loss: 3.44462933e-07
Iter: 1615 loss: 3.44278618e-07
Iter: 1616 loss: 3.44813657e-07
Iter: 1617 loss: 3.44189658e-07
Iter: 1618 loss: 3.43982038e-07
Iter: 1619 loss: 3.45791477e-07
Iter: 1620 loss: 3.43974648e-07
Iter: 1621 loss: 3.43800764e-07
Iter: 1622 loss: 3.45613387e-07
Iter: 1623 loss: 3.43819806e-07
Iter: 1624 loss: 3.43748667e-07
Iter: 1625 loss: 3.43581917e-07
Iter: 1626 loss: 3.45163443e-07
Iter: 1627 loss: 3.43577398e-07
Iter: 1628 loss: 3.43323876e-07
Iter: 1629 loss: 3.44114596e-07
Iter: 1630 loss: 3.43224826e-07
Iter: 1631 loss: 3.43045315e-07
Iter: 1632 loss: 3.43405873e-07
Iter: 1633 loss: 3.42945498e-07
Iter: 1634 loss: 3.42774342e-07
Iter: 1635 loss: 3.42525226e-07
Iter: 1636 loss: 3.42498026e-07
Iter: 1637 loss: 3.4219272e-07
Iter: 1638 loss: 3.4414046e-07
Iter: 1639 loss: 3.42197097e-07
Iter: 1640 loss: 3.41906912e-07
Iter: 1641 loss: 3.42186581e-07
Iter: 1642 loss: 3.41744283e-07
Iter: 1643 loss: 3.41448441e-07
Iter: 1644 loss: 3.42478785e-07
Iter: 1645 loss: 3.4138705e-07
Iter: 1646 loss: 3.4110559e-07
Iter: 1647 loss: 3.42334857e-07
Iter: 1648 loss: 3.41025896e-07
Iter: 1649 loss: 3.40789398e-07
Iter: 1650 loss: 3.41633552e-07
Iter: 1651 loss: 3.40705924e-07
Iter: 1652 loss: 3.40462407e-07
Iter: 1653 loss: 3.40779536e-07
Iter: 1654 loss: 3.40370775e-07
Iter: 1655 loss: 3.40206668e-07
Iter: 1656 loss: 3.40193196e-07
Iter: 1657 loss: 3.40060922e-07
Iter: 1658 loss: 3.40648967e-07
Iter: 1659 loss: 3.40030908e-07
Iter: 1660 loss: 3.39881524e-07
Iter: 1661 loss: 3.39633942e-07
Iter: 1662 loss: 3.39608e-07
Iter: 1663 loss: 3.39422058e-07
Iter: 1664 loss: 3.40181032e-07
Iter: 1665 loss: 3.39379739e-07
Iter: 1666 loss: 3.39191359e-07
Iter: 1667 loss: 3.38866187e-07
Iter: 1668 loss: 3.38851e-07
Iter: 1669 loss: 3.38610874e-07
Iter: 1670 loss: 3.38605446e-07
Iter: 1671 loss: 3.38401406e-07
Iter: 1672 loss: 3.38958785e-07
Iter: 1673 loss: 3.38312248e-07
Iter: 1674 loss: 3.38081634e-07
Iter: 1675 loss: 3.38434518e-07
Iter: 1676 loss: 3.37994265e-07
Iter: 1677 loss: 3.37789373e-07
Iter: 1678 loss: 3.37528235e-07
Iter: 1679 loss: 3.37506634e-07
Iter: 1680 loss: 3.37225458e-07
Iter: 1681 loss: 3.4068421e-07
Iter: 1682 loss: 3.37224719e-07
Iter: 1683 loss: 3.36999108e-07
Iter: 1684 loss: 3.3695872e-07
Iter: 1685 loss: 3.36803936e-07
Iter: 1686 loss: 3.36526e-07
Iter: 1687 loss: 3.38919676e-07
Iter: 1688 loss: 3.36527876e-07
Iter: 1689 loss: 3.3628848e-07
Iter: 1690 loss: 3.37857131e-07
Iter: 1691 loss: 3.36285211e-07
Iter: 1692 loss: 3.36084327e-07
Iter: 1693 loss: 3.37523289e-07
Iter: 1694 loss: 3.36079694e-07
Iter: 1695 loss: 3.35984168e-07
Iter: 1696 loss: 3.35820346e-07
Iter: 1697 loss: 3.39535177e-07
Iter: 1698 loss: 3.35832112e-07
Iter: 1699 loss: 3.35563072e-07
Iter: 1700 loss: 3.35970242e-07
Iter: 1701 loss: 3.35410391e-07
Iter: 1702 loss: 3.35217038e-07
Iter: 1703 loss: 3.35687673e-07
Iter: 1704 loss: 3.35185121e-07
Iter: 1705 loss: 3.34912528e-07
Iter: 1706 loss: 3.36012192e-07
Iter: 1707 loss: 3.34850654e-07
Iter: 1708 loss: 3.34712524e-07
Iter: 1709 loss: 3.35488778e-07
Iter: 1710 loss: 3.34654885e-07
Iter: 1711 loss: 3.34523691e-07
Iter: 1712 loss: 3.34926426e-07
Iter: 1713 loss: 3.34480092e-07
Iter: 1714 loss: 3.34279491e-07
Iter: 1715 loss: 3.34373681e-07
Iter: 1716 loss: 3.34121864e-07
Iter: 1717 loss: 3.3392584e-07
Iter: 1718 loss: 3.33815137e-07
Iter: 1719 loss: 3.33743e-07
Iter: 1720 loss: 3.33497042e-07
Iter: 1721 loss: 3.36040671e-07
Iter: 1722 loss: 3.3346555e-07
Iter: 1723 loss: 3.33263131e-07
Iter: 1724 loss: 3.33056164e-07
Iter: 1725 loss: 3.33007279e-07
Iter: 1726 loss: 3.33034649e-07
Iter: 1727 loss: 3.32867899e-07
Iter: 1728 loss: 3.32735681e-07
Iter: 1729 loss: 3.32670083e-07
Iter: 1730 loss: 3.32604372e-07
Iter: 1731 loss: 3.32448536e-07
Iter: 1732 loss: 3.32357217e-07
Iter: 1733 loss: 3.32235061e-07
Iter: 1734 loss: 3.32066634e-07
Iter: 1735 loss: 3.32794087e-07
Iter: 1736 loss: 3.32002855e-07
Iter: 1737 loss: 3.31783554e-07
Iter: 1738 loss: 3.31662619e-07
Iter: 1739 loss: 3.31583578e-07
Iter: 1740 loss: 3.31332473e-07
Iter: 1741 loss: 3.3365194e-07
Iter: 1742 loss: 3.31313515e-07
Iter: 1743 loss: 3.31113569e-07
Iter: 1744 loss: 3.31206564e-07
Iter: 1745 loss: 3.30956198e-07
Iter: 1746 loss: 3.30701795e-07
Iter: 1747 loss: 3.31824396e-07
Iter: 1748 loss: 3.30700175e-07
Iter: 1749 loss: 3.30467543e-07
Iter: 1750 loss: 3.31877942e-07
Iter: 1751 loss: 3.30463592e-07
Iter: 1752 loss: 3.3030085e-07
Iter: 1753 loss: 3.3030858e-07
Iter: 1754 loss: 3.30174544e-07
Iter: 1755 loss: 3.29956549e-07
Iter: 1756 loss: 3.29791192e-07
Iter: 1757 loss: 3.2969217e-07
Iter: 1758 loss: 3.29373563e-07
Iter: 1759 loss: 3.32219855e-07
Iter: 1760 loss: 3.29386921e-07
Iter: 1761 loss: 3.29206671e-07
Iter: 1762 loss: 3.31736544e-07
Iter: 1763 loss: 3.29212611e-07
Iter: 1764 loss: 3.29018548e-07
Iter: 1765 loss: 3.29015677e-07
Iter: 1766 loss: 3.28848671e-07
Iter: 1767 loss: 3.2872012e-07
Iter: 1768 loss: 3.28781738e-07
Iter: 1769 loss: 3.28622718e-07
Iter: 1770 loss: 3.28424278e-07
Iter: 1771 loss: 3.2842803e-07
Iter: 1772 loss: 3.28278361e-07
Iter: 1773 loss: 3.27978853e-07
Iter: 1774 loss: 3.2920417e-07
Iter: 1775 loss: 3.27920787e-07
Iter: 1776 loss: 3.2769168e-07
Iter: 1777 loss: 3.28156261e-07
Iter: 1778 loss: 3.27640862e-07
Iter: 1779 loss: 3.27401807e-07
Iter: 1780 loss: 3.28854583e-07
Iter: 1781 loss: 3.27339194e-07
Iter: 1782 loss: 3.27196233e-07
Iter: 1783 loss: 3.27137258e-07
Iter: 1784 loss: 3.27009701e-07
Iter: 1785 loss: 3.2680839e-07
Iter: 1786 loss: 3.29666307e-07
Iter: 1787 loss: 3.26781816e-07
Iter: 1788 loss: 3.2665028e-07
Iter: 1789 loss: 3.27060661e-07
Iter: 1790 loss: 3.26611428e-07
Iter: 1791 loss: 3.26463976e-07
Iter: 1792 loss: 3.26259737e-07
Iter: 1793 loss: 3.26233362e-07
Iter: 1794 loss: 3.25943716e-07
Iter: 1795 loss: 3.26558904e-07
Iter: 1796 loss: 3.25822214e-07
Iter: 1797 loss: 3.25864704e-07
Iter: 1798 loss: 3.25663564e-07
Iter: 1799 loss: 3.25545784e-07
Iter: 1800 loss: 3.25363715e-07
Iter: 1801 loss: 3.253937e-07
Iter: 1802 loss: 3.2526e-07
Iter: 1803 loss: 3.24977918e-07
Iter: 1804 loss: 3.31381926e-07
Iter: 1805 loss: 3.24972859e-07
Iter: 1806 loss: 3.24724766e-07
Iter: 1807 loss: 3.24715614e-07
Iter: 1808 loss: 3.24558727e-07
Iter: 1809 loss: 3.24474286e-07
Iter: 1810 loss: 3.24388026e-07
Iter: 1811 loss: 3.24145333e-07
Iter: 1812 loss: 3.2528169e-07
Iter: 1813 loss: 3.24138171e-07
Iter: 1814 loss: 3.23867596e-07
Iter: 1815 loss: 3.24187198e-07
Iter: 1816 loss: 3.23704967e-07
Iter: 1817 loss: 3.2346378e-07
Iter: 1818 loss: 3.25610102e-07
Iter: 1819 loss: 3.23461933e-07
Iter: 1820 loss: 3.23287878e-07
Iter: 1821 loss: 3.23619417e-07
Iter: 1822 loss: 3.23224356e-07
Iter: 1823 loss: 3.22992207e-07
Iter: 1824 loss: 3.23534181e-07
Iter: 1825 loss: 3.22901769e-07
Iter: 1826 loss: 3.22741869e-07
Iter: 1827 loss: 3.22804965e-07
Iter: 1828 loss: 3.22638357e-07
Iter: 1829 loss: 3.22463109e-07
Iter: 1830 loss: 3.2412197e-07
Iter: 1831 loss: 3.22484368e-07
Iter: 1832 loss: 3.22271688e-07
Iter: 1833 loss: 3.22854333e-07
Iter: 1834 loss: 3.22241902e-07
Iter: 1835 loss: 3.22134781e-07
Iter: 1836 loss: 3.21840275e-07
Iter: 1837 loss: 3.26361402e-07
Iter: 1838 loss: 3.21858721e-07
Iter: 1839 loss: 3.215373e-07
Iter: 1840 loss: 3.22996357e-07
Iter: 1841 loss: 3.21504587e-07
Iter: 1842 loss: 3.21216277e-07
Iter: 1843 loss: 3.21937677e-07
Iter: 1844 loss: 3.21113134e-07
Iter: 1845 loss: 3.20855975e-07
Iter: 1846 loss: 3.22304317e-07
Iter: 1847 loss: 3.20818742e-07
Iter: 1848 loss: 3.20660831e-07
Iter: 1849 loss: 3.20796516e-07
Iter: 1850 loss: 3.20563714e-07
Iter: 1851 loss: 3.20348249e-07
Iter: 1852 loss: 3.21905617e-07
Iter: 1853 loss: 3.20318833e-07
Iter: 1854 loss: 3.20185023e-07
Iter: 1855 loss: 3.20227173e-07
Iter: 1856 loss: 3.20065539e-07
Iter: 1857 loss: 3.19855587e-07
Iter: 1858 loss: 3.21470594e-07
Iter: 1859 loss: 3.19867667e-07
Iter: 1860 loss: 3.19680964e-07
Iter: 1861 loss: 3.19822391e-07
Iter: 1862 loss: 3.19609967e-07
Iter: 1863 loss: 3.19403512e-07
Iter: 1864 loss: 3.19292781e-07
Iter: 1865 loss: 3.19183954e-07
Iter: 1866 loss: 3.19207402e-07
Iter: 1867 loss: 3.19065663e-07
Iter: 1868 loss: 3.18984718e-07
Iter: 1869 loss: 3.18786761e-07
Iter: 1870 loss: 3.18775903e-07
Iter: 1871 loss: 3.18611086e-07
Iter: 1872 loss: 3.18643686e-07
Iter: 1873 loss: 3.18514225e-07
Iter: 1874 loss: 3.18255559e-07
Iter: 1875 loss: 3.18268661e-07
Iter: 1876 loss: 3.18090201e-07
Iter: 1877 loss: 3.17858763e-07
Iter: 1878 loss: 3.17842876e-07
Iter: 1879 loss: 3.17669162e-07
Iter: 1880 loss: 3.17817239e-07
Iter: 1881 loss: 3.17596403e-07
Iter: 1882 loss: 3.17444318e-07
Iter: 1883 loss: 3.17947e-07
Iter: 1884 loss: 3.17352601e-07
Iter: 1885 loss: 3.17136568e-07
Iter: 1886 loss: 3.18056209e-07
Iter: 1887 loss: 3.1713779e-07
Iter: 1888 loss: 3.16946966e-07
Iter: 1889 loss: 3.17196509e-07
Iter: 1890 loss: 3.1686659e-07
Iter: 1891 loss: 3.16667524e-07
Iter: 1892 loss: 3.17705201e-07
Iter: 1893 loss: 3.16651324e-07
Iter: 1894 loss: 3.16464593e-07
Iter: 1895 loss: 3.16184355e-07
Iter: 1896 loss: 3.16159117e-07
Iter: 1897 loss: 3.15998307e-07
Iter: 1898 loss: 3.16002712e-07
Iter: 1899 loss: 3.15880015e-07
Iter: 1900 loss: 3.16815402e-07
Iter: 1901 loss: 3.15831386e-07
Iter: 1902 loss: 3.15744444e-07
Iter: 1903 loss: 3.15527416e-07
Iter: 1904 loss: 3.18400055e-07
Iter: 1905 loss: 3.15506213e-07
Iter: 1906 loss: 3.15330141e-07
Iter: 1907 loss: 3.15798616e-07
Iter: 1908 loss: 3.15210315e-07
Iter: 1909 loss: 3.15018752e-07
Iter: 1910 loss: 3.16028036e-07
Iter: 1911 loss: 3.14957106e-07
Iter: 1912 loss: 3.14739253e-07
Iter: 1913 loss: 3.15359358e-07
Iter: 1914 loss: 3.14683916e-07
Iter: 1915 loss: 3.14449096e-07
Iter: 1916 loss: 3.14712167e-07
Iter: 1917 loss: 3.14310967e-07
Iter: 1918 loss: 3.14093199e-07
Iter: 1919 loss: 3.15096855e-07
Iter: 1920 loss: 3.14052e-07
Iter: 1921 loss: 3.13828764e-07
Iter: 1922 loss: 3.14930389e-07
Iter: 1923 loss: 3.13802e-07
Iter: 1924 loss: 3.13672672e-07
Iter: 1925 loss: 3.1420376e-07
Iter: 1926 loss: 3.13621399e-07
Iter: 1927 loss: 3.13484463e-07
Iter: 1928 loss: 3.13523799e-07
Iter: 1929 loss: 3.13342582e-07
Iter: 1930 loss: 3.13194789e-07
Iter: 1931 loss: 3.13684978e-07
Iter: 1932 loss: 3.13120523e-07
Iter: 1933 loss: 3.13008059e-07
Iter: 1934 loss: 3.13042875e-07
Iter: 1935 loss: 3.129025e-07
Iter: 1936 loss: 3.12705453e-07
Iter: 1937 loss: 3.12703435e-07
Iter: 1938 loss: 3.12543307e-07
Iter: 1939 loss: 3.12373317e-07
Iter: 1940 loss: 3.12330712e-07
Iter: 1941 loss: 3.12076395e-07
Iter: 1942 loss: 3.1385531e-07
Iter: 1943 loss: 3.12054965e-07
Iter: 1944 loss: 3.11839813e-07
Iter: 1945 loss: 3.12534951e-07
Iter: 1946 loss: 3.11818326e-07
Iter: 1947 loss: 3.11629663e-07
Iter: 1948 loss: 3.12789e-07
Iter: 1949 loss: 3.11629606e-07
Iter: 1950 loss: 3.11495455e-07
Iter: 1951 loss: 3.1144981e-07
Iter: 1952 loss: 3.11344877e-07
Iter: 1953 loss: 3.11169032e-07
Iter: 1954 loss: 3.12196676e-07
Iter: 1955 loss: 3.11108352e-07
Iter: 1956 loss: 3.10944671e-07
Iter: 1957 loss: 3.11348174e-07
Iter: 1958 loss: 3.10874896e-07
Iter: 1959 loss: 3.10693707e-07
Iter: 1960 loss: 3.11267e-07
Iter: 1961 loss: 3.10654826e-07
Iter: 1962 loss: 3.10454425e-07
Iter: 1963 loss: 3.10827886e-07
Iter: 1964 loss: 3.10353869e-07
Iter: 1965 loss: 3.10216564e-07
Iter: 1966 loss: 3.1141758e-07
Iter: 1967 loss: 3.1019735e-07
Iter: 1968 loss: 3.10010108e-07
Iter: 1969 loss: 3.10534318e-07
Iter: 1970 loss: 3.09969721e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4
+ date
Thu Oct 22 08:29:34 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/500_500_500_500_1 --function f1 --psi 2 --phi 2.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f881040ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87edeac2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f881045e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8810363d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f881039dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ede228c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87eddb6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87eddd4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87eddd4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87eddfbb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c86496a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c8656620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c8656a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c85b8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c85b8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c8656730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c854d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c851c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c84ebae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c84f3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c85cb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ede92c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87ede54620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c8492378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c8492d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c847d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c8469598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c8446730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c83c62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c839eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c8367620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c8312840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c83122f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c8346510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c83d02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87c8306048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.105287015
test_loss: 0.101711266
train_loss: 0.04650428
test_loss: 0.05397855
train_loss: 0.033227075
test_loss: 0.037013184
train_loss: 0.02759215
test_loss: 0.99006134
train_loss: 0.13906316
test_loss: 0.12021963
train_loss: 0.051398512
test_loss: 0.05689284
train_loss: 0.026726939
test_loss: 0.03418629
train_loss: 0.025052952
test_loss: 0.029687557
train_loss: 0.018036474
test_loss: 0.025326313
train_loss: 0.016337594
test_loss: 0.023623185
train_loss: 0.013871264
test_loss: 0.021158976
train_loss: 0.014786749
test_loss: 0.01896806
train_loss: 0.011956343
test_loss: 0.017028952
train_loss: 0.01115793
test_loss: 0.01681607
train_loss: 0.012073047
test_loss: 0.016039908
train_loss: 0.009764914
test_loss: 0.015444028
train_loss: 0.009696925
test_loss: 0.014690909
train_loss: 0.0092805885
test_loss: 0.014427021
train_loss: 0.010561914
test_loss: 0.013779495
train_loss: 0.00927285
test_loss: 0.013479314
train_loss: 0.008956558
test_loss: 0.014768214
train_loss: 0.009285655
test_loss: 0.012589565
train_loss: 0.009760512
test_loss: 0.014178854
train_loss: 0.008511472
test_loss: 0.012877972
train_loss: 0.007822928
test_loss: 0.0126951765
train_loss: 0.00899122
test_loss: 0.012571939
train_loss: 0.008083651
test_loss: 0.012336713
train_loss: 0.007605441
test_loss: 0.012022993
train_loss: 0.008080534
test_loss: 0.011974725
train_loss: 0.007704844
test_loss: 0.011923028
train_loss: 0.007910893
test_loss: 0.013014127
train_loss: 0.0069310386
test_loss: 0.01121021
train_loss: 0.0065679797
test_loss: 0.011640404
train_loss: 0.0070832935
test_loss: 0.010866782
train_loss: 0.007292779
test_loss: 0.011555895
train_loss: 0.00913659
test_loss: 0.011773912
train_loss: 0.008226417
test_loss: 0.0111461375
train_loss: 0.008870339
test_loss: 0.012110241
train_loss: 0.0064306273
test_loss: 0.011029499
train_loss: 0.007746702
test_loss: 0.010928411
train_loss: 0.0074522207
test_loss: 0.012643676
train_loss: 0.008329003
test_loss: 0.010934917
train_loss: 0.007041352
test_loss: 0.0106802955
train_loss: 0.007144386
test_loss: 0.010706113
train_loss: 0.008624699
test_loss: 0.011519621
train_loss: 0.0068481606
test_loss: 0.011232823
train_loss: 0.0069898935
test_loss: 0.010438382
train_loss: 0.0071622496
test_loss: 0.011568928
train_loss: 0.006617857
test_loss: 0.010252006
train_loss: 0.007348067
test_loss: 0.0108957635
train_loss: 0.0068325633
test_loss: 0.010877054
train_loss: 0.0073531047
test_loss: 0.010300304
train_loss: 0.0069074873
test_loss: 0.010344018
train_loss: 0.005840931
test_loss: 0.010660874
train_loss: 0.0071687712
test_loss: 0.010513834
train_loss: 0.007815656
test_loss: 0.011301751
train_loss: 0.0072107962
test_loss: 0.009999715
train_loss: 0.006895479
test_loss: 0.009999749
train_loss: 0.0064864764
test_loss: 0.0104019465
train_loss: 0.008187391
test_loss: 0.013603112
train_loss: 0.0073754815
test_loss: 0.010507023
train_loss: 0.0070799394
test_loss: 0.01093486
train_loss: 0.008043801
test_loss: 0.010591896
train_loss: 0.006629809
test_loss: 0.009756969
train_loss: 0.0067507983
test_loss: 0.009990297
train_loss: 0.0076503707
test_loss: 0.011576063
train_loss: 0.0064642546
test_loss: 0.009782707
train_loss: 0.0063248044
test_loss: 0.010800475
train_loss: 0.0075504556
test_loss: 0.010871908
train_loss: 0.0066901185
test_loss: 0.010894439
train_loss: 0.0058925385
test_loss: 0.011046339
train_loss: 0.0060727475
test_loss: 0.010364168
train_loss: 0.007170488
test_loss: 0.010530727
train_loss: 0.007274804
test_loss: 0.0104207
train_loss: 0.00695351
test_loss: 0.009993243
train_loss: 0.0055399328
test_loss: 0.0094093755
train_loss: 0.006540861
test_loss: 0.010023908
train_loss: 0.0060248044
test_loss: 0.010406862
train_loss: 0.007186961
test_loss: 0.010168988
train_loss: 0.006218316
test_loss: 0.010604621
train_loss: 0.005788554
test_loss: 0.010502221
train_loss: 0.0067266617
test_loss: 0.010875543
train_loss: 0.0067446525
test_loss: 0.009693706
train_loss: 0.0062438473
test_loss: 0.010293755
train_loss: 0.0055301217
test_loss: 0.010317098
train_loss: 0.007365435
test_loss: 0.010137994
train_loss: 0.006503883
test_loss: 0.011025023
train_loss: 0.007824847
test_loss: 0.0101523
train_loss: 0.006629586
test_loss: 0.010629945
train_loss: 0.007841384
test_loss: 0.009891009
train_loss: 0.005782929
test_loss: 0.010520779
train_loss: 0.006064406
test_loss: 0.010146764
train_loss: 0.005558137
test_loss: 0.00982724
train_loss: 0.005885408
test_loss: 0.009408253
train_loss: 0.008876909
test_loss: 0.010976319
train_loss: 0.00555909
test_loss: 0.010816041
train_loss: 0.0060197483
test_loss: 0.010515141
train_loss: 0.0057574287
test_loss: 0.010015526
train_loss: 0.005334028
test_loss: 0.009478452
train_loss: 0.006752425
test_loss: 0.009886981
train_loss: 0.0066198027
test_loss: 0.009913517
train_loss: 0.006248006
test_loss: 0.009384829
train_loss: 0.0062717022
test_loss: 0.009510952
train_loss: 0.0058680675
test_loss: 0.009285324
train_loss: 0.0064674015
test_loss: 0.010274786
train_loss: 0.0059859576
test_loss: 0.0097747315
train_loss: 0.007734864
test_loss: 0.009555809
train_loss: 0.006133635
test_loss: 0.009556849
train_loss: 0.0063263057
test_loss: 0.009879917
train_loss: 0.006059011
test_loss: 0.009599118
train_loss: 0.005262541
test_loss: 0.009759229
train_loss: 0.0058438303
test_loss: 0.009970448
train_loss: 0.00681788
test_loss: 0.009448699
train_loss: 0.005597795
test_loss: 0.010420309
train_loss: 0.0060883546
test_loss: 0.009673325
train_loss: 0.005487717
test_loss: 0.00925748
train_loss: 0.0061812587
test_loss: 0.009417005
train_loss: 0.005939586
test_loss: 0.009998064
train_loss: 0.0069051078
test_loss: 0.009787335
train_loss: 0.006154169
test_loss: 0.009338187
train_loss: 0.0058615617
test_loss: 0.009781817
train_loss: 0.006340324
test_loss: 0.009370121
train_loss: 0.006466625
test_loss: 0.009403732
train_loss: 0.0062774112
test_loss: 0.009504036
train_loss: 0.005513252
test_loss: 0.009806886
train_loss: 0.0057731792
test_loss: 0.009918614
train_loss: 0.006308535
test_loss: 0.010797058
train_loss: 0.0065717683
test_loss: 0.009328088
train_loss: 0.006306613
test_loss: 0.009614586
train_loss: 0.005821827
test_loss: 0.009488506
train_loss: 0.0055688587
test_loss: 0.009451707
train_loss: 0.006198686
test_loss: 0.010629249
train_loss: 0.006628544
test_loss: 0.009530626
train_loss: 0.007596692
test_loss: 0.011125246
train_loss: 0.007091715
test_loss: 0.009921583
train_loss: 0.0063825757
test_loss: 0.009900217
train_loss: 0.0063194074
test_loss: 0.010305711
train_loss: 0.0065194294
test_loss: 0.010197545
train_loss: 0.0068673184
test_loss: 0.010456219
train_loss: 0.006623029
test_loss: 0.0100998925
train_loss: 0.0063572973
test_loss: 0.009678318
train_loss: 0.006671753
test_loss: 0.010002675
train_loss: 0.005480216
test_loss: 0.009898443
train_loss: 0.006616688
test_loss: 0.009521806
train_loss: 0.0057343026
test_loss: 0.009695331
train_loss: 0.0057423417
test_loss: 0.009499676
train_loss: 0.009169774
test_loss: 0.010800951
train_loss: 0.0062104645
test_loss: 0.010662779
train_loss: 0.006929784
test_loss: 0.010715053
train_loss: 0.0077554607
test_loss: 0.009829927
train_loss: 0.006661155
test_loss: 0.009414289
train_loss: 0.0058535887
test_loss: 0.010264044
train_loss: 0.00795521
test_loss: 0.010061801
train_loss: 0.005752988
test_loss: 0.009744857
train_loss: 0.006068577
test_loss: 0.009488671
train_loss: 0.006092906
test_loss: 0.009919783
train_loss: 0.005661289
test_loss: 0.009564813
train_loss: 0.0051765773
test_loss: 0.0099224495
train_loss: 0.005707411
test_loss: 0.010241849
train_loss: 0.0057930658
test_loss: 0.009878334
train_loss: 0.0066824034
test_loss: 0.010046386
train_loss: 0.006998332
test_loss: 0.01302394
train_loss: 0.0065387446
test_loss: 0.0096076075
train_loss: 0.0069629196
test_loss: 0.009772364
train_loss: 0.0078070476
test_loss: 0.0103543755
train_loss: 0.0056055416
test_loss: 0.009412062
train_loss: 0.006413503
test_loss: 0.009624765
train_loss: 0.006630159
test_loss: 0.009536029
train_loss: 0.005896386
test_loss: 0.0095473435
train_loss: 0.006012541
test_loss: 0.009796167
train_loss: 0.0058011836
test_loss: 0.0098457
train_loss: 0.00673184
test_loss: 0.010250382
train_loss: 0.0067308005
test_loss: 0.00984587
train_loss: 0.006472231
test_loss: 0.009925568
train_loss: 0.006831738
test_loss: 0.010267506
train_loss: 0.005696074
test_loss: 0.009719924
train_loss: 0.006140848
test_loss: 0.0098123355
train_loss: 0.006036872
test_loss: 0.0094662355
train_loss: 0.006680661
test_loss: 0.009852642
train_loss: 0.0064080926
test_loss: 0.009910885
train_loss: 0.006497767
test_loss: 0.0111832265
train_loss: 0.005296199
test_loss: 0.009928237
train_loss: 0.0059091123
test_loss: 0.010237431
train_loss: 0.0067128143
test_loss: 0.009428588
train_loss: 0.0073263813
test_loss: 0.010235218
train_loss: 0.0058054104
test_loss: 0.009388188
train_loss: 0.0067008855
test_loss: 0.009695661
train_loss: 0.005947347
test_loss: 0.00985552
train_loss: 0.0061237095
test_loss: 0.010068738
train_loss: 0.0059154006
test_loss: 0.009942731
train_loss: 0.0063249213
test_loss: 0.00970884
train_loss: 0.0067276885
test_loss: 0.009508012
train_loss: 0.007904906
test_loss: 0.010892411
train_loss: 0.0058501917
test_loss: 0.01014281
train_loss: 0.006916741
test_loss: 0.010388635
train_loss: 0.005867901
test_loss: 0.009395982
train_loss: 0.0073709013
test_loss: 0.009517463
train_loss: 0.006037281
test_loss: 0.01025319
train_loss: 0.006974099
test_loss: 0.010070579
train_loss: 0.00718218
test_loss: 0.010241957
train_loss: 0.007552858
test_loss: 0.010277978
train_loss: 0.0060488936
test_loss: 0.009988549
train_loss: 0.006501281
test_loss: 0.010229632
train_loss: 0.006805667
test_loss: 0.009927975
train_loss: 0.0058013434
test_loss: 0.009986144
train_loss: 0.007286129
test_loss: 0.010070541
train_loss: 0.0071925935
test_loss: 0.010784847
train_loss: 0.0063033733
test_loss: 0.009834993
train_loss: 0.005644642
test_loss: 0.009158805
train_loss: 0.0058224234
test_loss: 0.010490164
train_loss: 0.005525664
test_loss: 0.009562458
train_loss: 0.0070428867
test_loss: 0.009936222
train_loss: 0.0055890395
test_loss: 0.010165329
train_loss: 0.006470998
test_loss: 0.0096351225
train_loss: 0.0052602007
test_loss: 0.009360462
train_loss: 0.0061894446
test_loss: 0.009789284
train_loss: 0.0065136785
test_loss: 0.009946321
train_loss: 0.005821884
test_loss: 0.010817959
train_loss: 0.006393973
test_loss: 0.010612242
train_loss: 0.005827446
test_loss: 0.010195074
train_loss: 0.005981744
test_loss: 0.010215621
train_loss: 0.0074850656
test_loss: 0.012082451
train_loss: 0.0067626378
test_loss: 0.010194002
train_loss: 0.007424987
test_loss: 0.010381147
train_loss: 0.006219214
test_loss: 0.010877172
train_loss: 0.0064327987
test_loss: 0.009588093
train_loss: 0.0060783676
test_loss: 0.010767721
train_loss: 0.0058240034
test_loss: 0.010024983
train_loss: 0.00540385
test_loss: 0.010296129
train_loss: 0.006370567
test_loss: 0.009636003
train_loss: 0.00657339
test_loss: 0.01020907
train_loss: 0.00595088
test_loss: 0.009683787
train_loss: 0.0065744463
test_loss: 0.009705326
train_loss: 0.0060869837
test_loss: 0.012087977
train_loss: 0.0075508356
test_loss: 0.0107963625
train_loss: 0.006706395
test_loss: 0.010065053
train_loss: 0.0076172403
test_loss: 0.010706584
train_loss: 0.0067983037
test_loss: 0.01021147
train_loss: 0.007076094
test_loss: 0.010301398
train_loss: 0.006568266
test_loss: 0.010206032
train_loss: 0.0059293387
test_loss: 0.010281606
train_loss: 0.005407318
test_loss: 0.009607089
train_loss: 0.0070097595
test_loss: 0.010167947
train_loss: 0.0069839796
test_loss: 0.009783459
train_loss: 0.006412302
test_loss: 0.010074651
train_loss: 0.006339643
test_loss: 0.009990835
train_loss: 0.00578371
test_loss: 0.010943922
train_loss: 0.0074042347
test_loss: 0.0108494405
train_loss: 0.0060400334
test_loss: 0.009550933
train_loss: 0.0055877357
test_loss: 0.010484439
train_loss: 0.0063609257
test_loss: 0.010279849
train_loss: 0.0073146196
test_loss: 0.009974885
train_loss: 0.0069839316
test_loss: 0.009695338
train_loss: 0.005892566
test_loss: 0.010332039
train_loss: 0.005997186
test_loss: 0.010296214
train_loss: 0.005521625
test_loss: 0.009596878
train_loss: 0.006586134
test_loss: 0.010122253
train_loss: 0.005473504
test_loss: 0.009688074
train_loss: 0.007625076
test_loss: 0.011078261
train_loss: 0.006594904
test_loss: 0.009742456
train_loss: 0.0064583626
test_loss: 0.01040104
train_loss: 0.007000166
test_loss: 0.009877373
train_loss: 0.005967062
test_loss: 0.011151952
train_loss: 0.0061529763
test_loss: 0.00970056
train_loss: 0.006815097
test_loss: 0.009543238
train_loss: 0.00665707
test_loss: 0.010173381
train_loss: 0.0062129227
test_loss: 0.009695022
train_loss: 0.006198602
test_loss: 0.010102805
train_loss: 0.006319863
test_loss: 0.009963182
train_loss: 0.0073009203
test_loss: 0.011338816
train_loss: 0.008221513
test_loss: 0.011600875
train_loss: 0.0059690913
test_loss: 0.010434384
train_loss: 0.0066325413
test_loss: 0.010837744
train_loss: 0.0063783852
test_loss: 0.010185869
train_loss: 0.006402487
test_loss: 0.009967724
train_loss: 0.0059439144
test_loss: 0.01016409
train_loss: 0.00670472
test_loss: 0.010049328
train_loss: 0.0064813932
test_loss: 0.010487887
train_loss: 0.006178099
test_loss: 0.010179826
train_loss: 0.0064793383
test_loss: 0.009874424
train_loss: 0.0065101385
test_loss: 0.009636038
train_loss: 0.0056830244
test_loss: 0.010487329
train_loss: 0.007352297
test_loss: 0.010578712
train_loss: 0.006665176
test_loss: 0.010084857
train_loss: 0.006344636
test_loss: 0.010340413
train_loss: 0.005518928
test_loss: 0.009610477
train_loss: 0.0060492144
test_loss: 0.010395583
train_loss: 0.0064065354
test_loss: 0.009981795
train_loss: 0.0062939096
test_loss: 0.010498793
train_loss: 0.007892075
test_loss: 0.010716293
train_loss: 0.0059487727
test_loss: 0.0095747
train_loss: 0.007139644
test_loss: 0.010275252
train_loss: 0.005876831
test_loss: 0.009885539
train_loss: 0.006608166
test_loss: 0.009877021
train_loss: 0.0068030735
test_loss: 0.009717891
train_loss: 0.008508508
test_loss: 0.010566759
train_loss: 0.0067080106
test_loss: 0.010273799
train_loss: 0.006941475
test_loss: 0.010515199
train_loss: 0.008094493
test_loss: 0.010738867
train_loss: 0.0069508525
test_loss: 0.010689513
train_loss: 0.005212067
test_loss: 0.009686495
train_loss: 0.005727956
test_loss: 0.009867425
train_loss: 0.0058952966
test_loss: 0.010070391
train_loss: 0.0053358404
test_loss: 0.0094853155
train_loss: 0.0065093013
test_loss: 0.010529158
train_loss: 0.0054191966
test_loss: 0.009827549
train_loss: 0.0066836923
test_loss: 0.010703853
train_loss: 0.0064391587
test_loss: 0.010676682
train_loss: 0.0075995806
test_loss: 0.0116468035
train_loss: 0.006902576
test_loss: 0.011063277
train_loss: 0.0071344348
test_loss: 0.00990329
train_loss: 0.0062409416
test_loss: 0.0105150435
train_loss: 0.0067993365
test_loss: 0.010701262
train_loss: 0.006273465
test_loss: 0.010326094
train_loss: 0.0058528082
test_loss: 0.010102215
train_loss: 0.006127948
test_loss: 0.009828148
train_loss: 0.0055859582
test_loss: 0.0098969955
train_loss: 0.0068998104
test_loss: 0.010589242
train_loss: 0.0059922226
test_loss: 0.009881785
train_loss: 0.0063782763
test_loss: 0.0106506245
train_loss: 0.0072016725
test_loss: 0.009814636
train_loss: 0.0068247067
test_loss: 0.009788422
train_loss: 0.0065579517
test_loss: 0.010252819
train_loss: 0.0066950405
test_loss: 0.010779887
train_loss: 0.006074515
test_loss: 0.009609124
train_loss: 0.0060140835
test_loss: 0.010234853
train_loss: 0.0069371443
test_loss: 0.01066221
train_loss: 0.005759957
test_loss: 0.0101026725
train_loss: 0.006432545
test_loss: 0.009844076
train_loss: 0.0056371824
test_loss: 0.010645116
train_loss: 0.0065398356
test_loss: 0.011101704
train_loss: 0.005950585
test_loss: 0.010925007
train_loss: 0.005784263
test_loss: 0.009744488
train_loss: 0.00669962
test_loss: 0.011165701
train_loss: 0.0061636744
test_loss: 0.01033939
train_loss: 0.00658548
test_loss: 0.010142036
train_loss: 0.0053728633
test_loss: 0.009856964
train_loss: 0.0056174193
test_loss: 0.010123845
train_loss: 0.006536692
test_loss: 0.010636356
train_loss: 0.0071197036
test_loss: 0.01121134
train_loss: 0.0064370316
test_loss: 0.010687712
train_loss: 0.006072808
test_loss: 0.0105689885
train_loss: 0.006088836
test_loss: 0.010044204
train_loss: 0.005999446
test_loss: 0.010698007
train_loss: 0.0070041055
test_loss: 0.010682917
train_loss: 0.0063379537
test_loss: 0.011329165
train_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.006304277
test_loss: 0.010225102
train_loss: 0.007352087
test_loss: 0.011453532
train_loss: 0.0057470202
test_loss: 0.01027811
train_loss: 0.005829485
test_loss: 0.009548723
train_loss: 0.005709918
test_loss: 0.010340077
train_loss: 0.006277482
test_loss: 0.010355859
train_loss: 0.0061822487
test_loss: 0.010807929
train_loss: 0.0073010884
test_loss: 0.011412594
train_loss: 0.0066820565
test_loss: 0.011037691
train_loss: 0.00570862
test_loss: 0.009714759
train_loss: 0.0070166998
test_loss: 0.010300685
train_loss: 0.005848055
test_loss: 0.010277052
train_loss: 0.005971612
test_loss: 0.010201768
train_loss: 0.0064055887
test_loss: 0.010541359
train_loss: 0.006912726
test_loss: 0.010308389
train_loss: 0.0062718494
test_loss: 0.009965478
train_loss: 0.0067860987
test_loss: 0.011640295
train_loss: 0.006192901
test_loss: 0.010349487
train_loss: 0.006299695
test_loss: 0.010525536
train_loss: 0.006172193
test_loss: 0.009976314
train_loss: 0.0061389953
test_loss: 0.0102242995
train_loss: 0.0065375757
test_loss: 0.010734713
train_loss: 0.0069638127
test_loss: 0.010512474
train_loss: 0.006593808
test_loss: 0.0105514275
train_loss: 0.007045746
test_loss: 0.010900046
train_loss: 0.0057845316
test_loss: 0.010155086
train_loss: 0.006669629
test_loss: 0.009918329
train_loss: 0.0069584846
test_loss: 0.011844808
train_loss: 0.005955779
test_loss: 0.010068431
train_loss: 0.006342941
test_loss: 0.011266883
train_loss: 0.0064097866
test_loss: 0.010012182
train_loss: 0.0060947207
test_loss: 0.009891639
train_loss: 0.006283924
test_loss: 0.010305733
train_loss: 0.0061170813
test_loss: 0.010541193
train_loss: 0.006200294
test_loss: 0.009779059
train_loss: 0.005378754
test_loss: 0.01016466
train_loss: 0.006040366
test_loss: 0.009772965
train_loss: 0.0059347157
test_loss: 0.010801647
train_loss: 0.006446125
test_loss: 0.010535949
train_loss: 0.0064929607
test_loss: 0.010466357
train_loss: 0.006390641
test_loss: 0.011258947
train_loss: 0.005444534
test_loss: 0.010257644
train_loss: 0.0076852012
test_loss: 0.011483606
train_loss: 0.006622037
test_loss: 0.010218752
train_loss: 0.006341823
test_loss: 0.010042394
train_loss: 0.0073505105
test_loss: 0.01023634
train_loss: 0.008075181
test_loss: 0.01141189
train_loss: 0.0059446543
test_loss: 0.010361294
train_loss: 0.0061265975
test_loss: 0.01081735
train_loss: 0.0063216686
test_loss: 0.011009328
train_loss: 0.005623615
test_loss: 0.009755462
train_loss: 0.0059428234
test_loss: 0.009950954
train_loss: 0.005472526
test_loss: 0.009841115
train_loss: 0.0058185
test_loss: 0.010516396
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.4 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d04da1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d03af8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d049c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d03f9400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d040c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d040cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d0348620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d02f87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d0348400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d02b1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d02b1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d028c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d02b1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d02cec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d0227840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d01e5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d0198400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d0227730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d01826a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d0198d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d0141950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d0141730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d0090598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d00cb378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d00cb048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d006b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d006b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06d004b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06c7d05620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06c7d05d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06c7cd7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06c7c7d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06c7c7d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06c7c9cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06c7c619d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f06c7c0a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 8.15458916e-05
Iter: 2 loss: 8.90744705e-05
Iter: 3 loss: 5.97774379e-05
Iter: 4 loss: 5.08327576e-05
Iter: 5 loss: 5.04340933e-05
Iter: 6 loss: 4.86035497e-05
Iter: 7 loss: 4.42810378e-05
Iter: 8 loss: 9.37386285e-05
Iter: 9 loss: 4.38887764e-05
Iter: 10 loss: 4.01862744e-05
Iter: 11 loss: 6.4339205e-05
Iter: 12 loss: 3.97921831e-05
Iter: 13 loss: 3.64415391e-05
Iter: 14 loss: 4.1842537e-05
Iter: 15 loss: 3.48894537e-05
Iter: 16 loss: 3.29134527e-05
Iter: 17 loss: 3.37842648e-05
Iter: 18 loss: 3.15682264e-05
Iter: 19 loss: 2.88429837e-05
Iter: 20 loss: 2.44701041e-05
Iter: 21 loss: 2.44350231e-05
Iter: 22 loss: 2.27178862e-05
Iter: 23 loss: 2.23240822e-05
Iter: 24 loss: 2.11547558e-05
Iter: 25 loss: 2.01024668e-05
Iter: 26 loss: 1.9808409e-05
Iter: 27 loss: 1.81388095e-05
Iter: 28 loss: 3.64358e-05
Iter: 29 loss: 1.81015421e-05
Iter: 30 loss: 1.73183507e-05
Iter: 31 loss: 1.78243827e-05
Iter: 32 loss: 1.68226798e-05
Iter: 33 loss: 1.58394196e-05
Iter: 34 loss: 2.37043787e-05
Iter: 35 loss: 1.57732557e-05
Iter: 36 loss: 1.66963837e-05
Iter: 37 loss: 1.55135385e-05
Iter: 38 loss: 1.53692363e-05
Iter: 39 loss: 1.50503238e-05
Iter: 40 loss: 1.96383189e-05
Iter: 41 loss: 1.50350397e-05
Iter: 42 loss: 1.44806218e-05
Iter: 43 loss: 1.57082231e-05
Iter: 44 loss: 1.4266875e-05
Iter: 45 loss: 1.36177632e-05
Iter: 46 loss: 1.71353204e-05
Iter: 47 loss: 1.35213959e-05
Iter: 48 loss: 1.30401513e-05
Iter: 49 loss: 1.44584737e-05
Iter: 50 loss: 1.28929159e-05
Iter: 51 loss: 1.25793285e-05
Iter: 52 loss: 1.20029672e-05
Iter: 53 loss: 2.53750804e-05
Iter: 54 loss: 1.20021723e-05
Iter: 55 loss: 1.15270332e-05
Iter: 56 loss: 1.15246658e-05
Iter: 57 loss: 1.11797499e-05
Iter: 58 loss: 1.06641637e-05
Iter: 59 loss: 1.0652936e-05
Iter: 60 loss: 1.01278956e-05
Iter: 61 loss: 1.01274982e-05
Iter: 62 loss: 9.82703e-06
Iter: 63 loss: 9.85595398e-06
Iter: 64 loss: 9.59458248e-06
Iter: 65 loss: 9.14218435e-06
Iter: 66 loss: 1.30997842e-05
Iter: 67 loss: 9.11927418e-06
Iter: 68 loss: 9.08965376e-06
Iter: 69 loss: 9.04020635e-06
Iter: 70 loss: 8.92633943e-06
Iter: 71 loss: 8.73011595e-06
Iter: 72 loss: 8.72998862e-06
Iter: 73 loss: 8.52857647e-06
Iter: 74 loss: 8.79249546e-06
Iter: 75 loss: 8.42641384e-06
Iter: 76 loss: 8.20445712e-06
Iter: 77 loss: 9.42168299e-06
Iter: 78 loss: 8.17164164e-06
Iter: 79 loss: 8.00348244e-06
Iter: 80 loss: 8.5654583e-06
Iter: 81 loss: 7.95724372e-06
Iter: 82 loss: 7.74876207e-06
Iter: 83 loss: 7.76732577e-06
Iter: 84 loss: 7.58694341e-06
Iter: 85 loss: 7.38706331e-06
Iter: 86 loss: 8.34753428e-06
Iter: 87 loss: 7.35061349e-06
Iter: 88 loss: 7.21031847e-06
Iter: 89 loss: 7.00765e-06
Iter: 90 loss: 7.00117744e-06
Iter: 91 loss: 6.82760265e-06
Iter: 92 loss: 6.81992697e-06
Iter: 93 loss: 6.7173587e-06
Iter: 94 loss: 6.74209423e-06
Iter: 95 loss: 6.64293e-06
Iter: 96 loss: 6.48374908e-06
Iter: 97 loss: 6.66710184e-06
Iter: 98 loss: 6.398398e-06
Iter: 99 loss: 6.26651399e-06
Iter: 100 loss: 7.44854378e-06
Iter: 101 loss: 6.26039127e-06
Iter: 102 loss: 6.3487646e-06
Iter: 103 loss: 6.22643711e-06
Iter: 104 loss: 6.20608262e-06
Iter: 105 loss: 6.14119926e-06
Iter: 106 loss: 6.1867263e-06
Iter: 107 loss: 6.08560413e-06
Iter: 108 loss: 5.99529403e-06
Iter: 109 loss: 6.34154276e-06
Iter: 110 loss: 5.97400413e-06
Iter: 111 loss: 5.88800094e-06
Iter: 112 loss: 6.63329229e-06
Iter: 113 loss: 5.88357898e-06
Iter: 114 loss: 5.81073891e-06
Iter: 115 loss: 5.88090279e-06
Iter: 116 loss: 5.76913044e-06
Iter: 117 loss: 5.68272208e-06
Iter: 118 loss: 5.88340299e-06
Iter: 119 loss: 5.65050959e-06
Iter: 120 loss: 5.55709266e-06
Iter: 121 loss: 5.53153041e-06
Iter: 122 loss: 5.47430136e-06
Iter: 123 loss: 5.37162896e-06
Iter: 124 loss: 6.2174945e-06
Iter: 125 loss: 5.36611742e-06
Iter: 126 loss: 5.30173793e-06
Iter: 127 loss: 5.50872846e-06
Iter: 128 loss: 5.2832379e-06
Iter: 129 loss: 5.19787e-06
Iter: 130 loss: 5.21131915e-06
Iter: 131 loss: 5.13352734e-06
Iter: 132 loss: 5.05414391e-06
Iter: 133 loss: 5.19094192e-06
Iter: 134 loss: 5.01897512e-06
Iter: 135 loss: 4.98063582e-06
Iter: 136 loss: 4.96746179e-06
Iter: 137 loss: 4.90396451e-06
Iter: 138 loss: 4.98959571e-06
Iter: 139 loss: 4.87163925e-06
Iter: 140 loss: 4.84977409e-06
Iter: 141 loss: 4.80079052e-06
Iter: 142 loss: 5.47865693e-06
Iter: 143 loss: 4.7977228e-06
Iter: 144 loss: 4.7299236e-06
Iter: 145 loss: 4.93435437e-06
Iter: 146 loss: 4.70927353e-06
Iter: 147 loss: 4.63580545e-06
Iter: 148 loss: 5.24305779e-06
Iter: 149 loss: 4.63085325e-06
Iter: 150 loss: 4.59712e-06
Iter: 151 loss: 4.64400046e-06
Iter: 152 loss: 4.58058958e-06
Iter: 153 loss: 4.53922712e-06
Iter: 154 loss: 4.53406e-06
Iter: 155 loss: 4.50478365e-06
Iter: 156 loss: 4.4452936e-06
Iter: 157 loss: 4.57773876e-06
Iter: 158 loss: 4.42223791e-06
Iter: 159 loss: 4.36540631e-06
Iter: 160 loss: 4.58841168e-06
Iter: 161 loss: 4.35221637e-06
Iter: 162 loss: 4.30660066e-06
Iter: 163 loss: 4.5426691e-06
Iter: 164 loss: 4.2988122e-06
Iter: 165 loss: 4.25920825e-06
Iter: 166 loss: 4.3002965e-06
Iter: 167 loss: 4.23672509e-06
Iter: 168 loss: 4.18372747e-06
Iter: 169 loss: 4.26819179e-06
Iter: 170 loss: 4.15949853e-06
Iter: 171 loss: 4.1895878e-06
Iter: 172 loss: 4.13299313e-06
Iter: 173 loss: 4.12497138e-06
Iter: 174 loss: 4.09801669e-06
Iter: 175 loss: 4.10657731e-06
Iter: 176 loss: 4.07235029e-06
Iter: 177 loss: 4.0274108e-06
Iter: 178 loss: 4.16881039e-06
Iter: 179 loss: 4.01459e-06
Iter: 180 loss: 3.98789871e-06
Iter: 181 loss: 3.98484e-06
Iter: 182 loss: 3.96710675e-06
Iter: 183 loss: 3.96205087e-06
Iter: 184 loss: 3.95081497e-06
Iter: 185 loss: 3.91867161e-06
Iter: 186 loss: 3.87871432e-06
Iter: 187 loss: 3.87545651e-06
Iter: 188 loss: 3.83009319e-06
Iter: 189 loss: 4.430648e-06
Iter: 190 loss: 3.8301805e-06
Iter: 191 loss: 3.80472079e-06
Iter: 192 loss: 3.7660684e-06
Iter: 193 loss: 3.76551e-06
Iter: 194 loss: 3.71891406e-06
Iter: 195 loss: 4.22622679e-06
Iter: 196 loss: 3.71795068e-06
Iter: 197 loss: 3.68673159e-06
Iter: 198 loss: 3.7949635e-06
Iter: 199 loss: 3.678236e-06
Iter: 200 loss: 3.64651055e-06
Iter: 201 loss: 3.70679413e-06
Iter: 202 loss: 3.63324648e-06
Iter: 203 loss: 3.68458677e-06
Iter: 204 loss: 3.62672836e-06
Iter: 205 loss: 3.62204696e-06
Iter: 206 loss: 3.60598551e-06
Iter: 207 loss: 3.59261094e-06
Iter: 208 loss: 3.5845851e-06
Iter: 209 loss: 3.54887811e-06
Iter: 210 loss: 3.67391681e-06
Iter: 211 loss: 3.53981704e-06
Iter: 212 loss: 3.52160941e-06
Iter: 213 loss: 3.52137022e-06
Iter: 214 loss: 3.5023977e-06
Iter: 215 loss: 3.49275206e-06
Iter: 216 loss: 3.48380922e-06
Iter: 217 loss: 3.45685521e-06
Iter: 218 loss: 3.50083428e-06
Iter: 219 loss: 3.44444106e-06
Iter: 220 loss: 3.41897817e-06
Iter: 221 loss: 3.5052185e-06
Iter: 222 loss: 3.4121706e-06
Iter: 223 loss: 3.3863023e-06
Iter: 224 loss: 3.39413282e-06
Iter: 225 loss: 3.36795574e-06
Iter: 226 loss: 3.34077481e-06
Iter: 227 loss: 3.5258372e-06
Iter: 228 loss: 3.33838398e-06
Iter: 229 loss: 3.31825095e-06
Iter: 230 loss: 3.32449645e-06
Iter: 231 loss: 3.3043334e-06
Iter: 232 loss: 3.274557e-06
Iter: 233 loss: 3.4908926e-06
Iter: 234 loss: 3.27203793e-06
Iter: 235 loss: 3.28016631e-06
Iter: 236 loss: 3.2666776e-06
Iter: 237 loss: 3.25996689e-06
Iter: 238 loss: 3.24348093e-06
Iter: 239 loss: 3.37063852e-06
Iter: 240 loss: 3.24011762e-06
Iter: 241 loss: 3.22234246e-06
Iter: 242 loss: 3.19683022e-06
Iter: 243 loss: 3.19609671e-06
Iter: 244 loss: 3.17551599e-06
Iter: 245 loss: 3.43832676e-06
Iter: 246 loss: 3.17527042e-06
Iter: 247 loss: 3.15695888e-06
Iter: 248 loss: 3.28130272e-06
Iter: 249 loss: 3.15499597e-06
Iter: 250 loss: 3.1404943e-06
Iter: 251 loss: 3.14553745e-06
Iter: 252 loss: 3.13044e-06
Iter: 253 loss: 3.1178788e-06
Iter: 254 loss: 3.13828332e-06
Iter: 255 loss: 3.11233589e-06
Iter: 256 loss: 3.09514871e-06
Iter: 257 loss: 3.101738e-06
Iter: 258 loss: 3.08319295e-06
Iter: 259 loss: 3.06141692e-06
Iter: 260 loss: 3.13176861e-06
Iter: 261 loss: 3.05541539e-06
Iter: 262 loss: 3.03501588e-06
Iter: 263 loss: 3.05086087e-06
Iter: 264 loss: 3.02273065e-06
Iter: 265 loss: 3.00425268e-06
Iter: 266 loss: 3.27749694e-06
Iter: 267 loss: 3.00415604e-06
Iter: 268 loss: 2.9967141e-06
Iter: 269 loss: 2.99650878e-06
Iter: 270 loss: 2.98637588e-06
Iter: 271 loss: 2.98173563e-06
Iter: 272 loss: 2.97662837e-06
Iter: 273 loss: 2.96584153e-06
Iter: 274 loss: 2.94618599e-06
Iter: 275 loss: 3.38804193e-06
Iter: 276 loss: 2.94608776e-06
Iter: 277 loss: 2.93168227e-06
Iter: 278 loss: 2.98570785e-06
Iter: 279 loss: 2.92814798e-06
Iter: 280 loss: 2.91562515e-06
Iter: 281 loss: 3.09150164e-06
Iter: 282 loss: 2.91559104e-06
Iter: 283 loss: 2.90283037e-06
Iter: 284 loss: 2.89750869e-06
Iter: 285 loss: 2.89098693e-06
Iter: 286 loss: 2.87859257e-06
Iter: 287 loss: 2.90237858e-06
Iter: 288 loss: 2.87306239e-06
Iter: 289 loss: 2.85598185e-06
Iter: 290 loss: 2.86584532e-06
Iter: 291 loss: 2.84474481e-06
Iter: 292 loss: 2.82719566e-06
Iter: 293 loss: 2.91551714e-06
Iter: 294 loss: 2.82415044e-06
Iter: 295 loss: 2.80828158e-06
Iter: 296 loss: 2.80622044e-06
Iter: 297 loss: 2.79489745e-06
Iter: 298 loss: 2.77743106e-06
Iter: 299 loss: 2.96152712e-06
Iter: 300 loss: 2.77707022e-06
Iter: 301 loss: 2.77030585e-06
Iter: 302 loss: 2.77020445e-06
Iter: 303 loss: 2.7623862e-06
Iter: 304 loss: 2.79188271e-06
Iter: 305 loss: 2.76054448e-06
Iter: 306 loss: 2.75590764e-06
Iter: 307 loss: 2.74358126e-06
Iter: 308 loss: 2.8469085e-06
Iter: 309 loss: 2.74145987e-06
Iter: 310 loss: 2.72988837e-06
Iter: 311 loss: 2.74393824e-06
Iter: 312 loss: 2.7237802e-06
Iter: 313 loss: 2.70910482e-06
Iter: 314 loss: 2.77769891e-06
Iter: 315 loss: 2.7061983e-06
Iter: 316 loss: 2.69141947e-06
Iter: 317 loss: 2.81671782e-06
Iter: 318 loss: 2.69025713e-06
Iter: 319 loss: 2.68336316e-06
Iter: 320 loss: 2.67714313e-06
Iter: 321 loss: 2.67538826e-06
Iter: 322 loss: 2.6622572e-06
Iter: 323 loss: 2.68130725e-06
Iter: 324 loss: 2.65591416e-06
Iter: 325 loss: 2.6415546e-06
Iter: 326 loss: 2.71958561e-06
Iter: 327 loss: 2.63971879e-06
Iter: 328 loss: 2.63058564e-06
Iter: 329 loss: 2.63223501e-06
Iter: 330 loss: 2.62380422e-06
Iter: 331 loss: 2.61135187e-06
Iter: 332 loss: 2.67197242e-06
Iter: 333 loss: 2.60907927e-06
Iter: 334 loss: 2.6018829e-06
Iter: 335 loss: 2.67687597e-06
Iter: 336 loss: 2.60153138e-06
Iter: 337 loss: 2.59466447e-06
Iter: 338 loss: 2.67802034e-06
Iter: 339 loss: 2.59471267e-06
Iter: 340 loss: 2.59107401e-06
Iter: 341 loss: 2.58157752e-06
Iter: 342 loss: 2.64462278e-06
Iter: 343 loss: 2.57896727e-06
Iter: 344 loss: 2.56892463e-06
Iter: 345 loss: 2.57299712e-06
Iter: 346 loss: 2.56197472e-06
Iter: 347 loss: 2.5483323e-06
Iter: 348 loss: 2.61132027e-06
Iter: 349 loss: 2.54567135e-06
Iter: 350 loss: 2.538422e-06
Iter: 351 loss: 2.53789631e-06
Iter: 352 loss: 2.53333269e-06
Iter: 353 loss: 2.52521227e-06
Iter: 354 loss: 2.71583076e-06
Iter: 355 loss: 2.52540667e-06
Iter: 356 loss: 2.5144559e-06
Iter: 357 loss: 2.53855524e-06
Iter: 358 loss: 2.51031634e-06
Iter: 359 loss: 2.50269295e-06
Iter: 360 loss: 2.57698298e-06
Iter: 361 loss: 2.50235462e-06
Iter: 362 loss: 2.49633104e-06
Iter: 363 loss: 2.48889182e-06
Iter: 364 loss: 2.48791821e-06
Iter: 365 loss: 2.47751018e-06
Iter: 366 loss: 2.55036753e-06
Iter: 367 loss: 2.47636081e-06
Iter: 368 loss: 2.46984382e-06
Iter: 369 loss: 2.52755353e-06
Iter: 370 loss: 2.46961122e-06
Iter: 371 loss: 2.46619447e-06
Iter: 372 loss: 2.46571653e-06
Iter: 373 loss: 2.4633232e-06
Iter: 374 loss: 2.4565652e-06
Iter: 375 loss: 2.4850483e-06
Iter: 376 loss: 2.45363708e-06
Iter: 377 loss: 2.44541593e-06
Iter: 378 loss: 2.45902243e-06
Iter: 379 loss: 2.44149646e-06
Iter: 380 loss: 2.43247632e-06
Iter: 381 loss: 2.44022272e-06
Iter: 382 loss: 2.4275e-06
Iter: 383 loss: 2.42286524e-06
Iter: 384 loss: 2.42117903e-06
Iter: 385 loss: 2.41692419e-06
Iter: 386 loss: 2.40982672e-06
Iter: 387 loss: 2.40988902e-06
Iter: 388 loss: 2.40064355e-06
Iter: 389 loss: 2.41746557e-06
Iter: 390 loss: 2.39701467e-06
Iter: 391 loss: 2.3885209e-06
Iter: 392 loss: 2.45046203e-06
Iter: 393 loss: 2.38785401e-06
Iter: 394 loss: 2.38081225e-06
Iter: 395 loss: 2.37356971e-06
Iter: 396 loss: 2.37195445e-06
Iter: 397 loss: 2.36024744e-06
Iter: 398 loss: 2.44374178e-06
Iter: 399 loss: 2.35914877e-06
Iter: 400 loss: 2.35215521e-06
Iter: 401 loss: 2.37377435e-06
Iter: 402 loss: 2.35005177e-06
Iter: 403 loss: 2.34825211e-06
Iter: 404 loss: 2.34613503e-06
Iter: 405 loss: 2.34318463e-06
Iter: 406 loss: 2.33594528e-06
Iter: 407 loss: 2.38288771e-06
Iter: 408 loss: 2.33425226e-06
Iter: 409 loss: 2.32743105e-06
Iter: 410 loss: 2.34064237e-06
Iter: 411 loss: 2.32484308e-06
Iter: 412 loss: 2.31801073e-06
Iter: 413 loss: 2.32286584e-06
Iter: 414 loss: 2.31362765e-06
Iter: 415 loss: 2.30909927e-06
Iter: 416 loss: 2.30889327e-06
Iter: 417 loss: 2.30390378e-06
Iter: 418 loss: 2.30117166e-06
Iter: 419 loss: 2.2989957e-06
Iter: 420 loss: 2.29304214e-06
Iter: 421 loss: 2.29809029e-06
Iter: 422 loss: 2.28966474e-06
Iter: 423 loss: 2.28275485e-06
Iter: 424 loss: 2.31372974e-06
Iter: 425 loss: 2.28135536e-06
Iter: 426 loss: 2.27501346e-06
Iter: 427 loss: 2.27861119e-06
Iter: 428 loss: 2.2708798e-06
Iter: 429 loss: 2.26316411e-06
Iter: 430 loss: 2.29553848e-06
Iter: 431 loss: 2.26170187e-06
Iter: 432 loss: 2.25584972e-06
Iter: 433 loss: 2.28780891e-06
Iter: 434 loss: 2.25504368e-06
Iter: 435 loss: 2.2544591e-06
Iter: 436 loss: 2.25271083e-06
Iter: 437 loss: 2.25063695e-06
Iter: 438 loss: 2.24543146e-06
Iter: 439 loss: 2.28283875e-06
Iter: 440 loss: 2.24404835e-06
Iter: 441 loss: 2.23880602e-06
Iter: 442 loss: 2.24536643e-06
Iter: 443 loss: 2.23632856e-06
Iter: 444 loss: 2.23023562e-06
Iter: 445 loss: 2.22749327e-06
Iter: 446 loss: 2.2244235e-06
Iter: 447 loss: 2.21705704e-06
Iter: 448 loss: 2.31276272e-06
Iter: 449 loss: 2.21710138e-06
Iter: 450 loss: 2.21092455e-06
Iter: 451 loss: 2.2548229e-06
Iter: 452 loss: 2.21021719e-06
Iter: 453 loss: 2.2064844e-06
Iter: 454 loss: 2.19779895e-06
Iter: 455 loss: 2.33063611e-06
Iter: 456 loss: 2.1975834e-06
Iter: 457 loss: 2.18992454e-06
Iter: 458 loss: 2.25781605e-06
Iter: 459 loss: 2.18971013e-06
Iter: 460 loss: 2.18273135e-06
Iter: 461 loss: 2.19350886e-06
Iter: 462 loss: 2.17930506e-06
Iter: 463 loss: 2.17319871e-06
Iter: 464 loss: 2.19071444e-06
Iter: 465 loss: 2.17139541e-06
Iter: 466 loss: 2.16610192e-06
Iter: 467 loss: 2.1786866e-06
Iter: 468 loss: 2.16403214e-06
Iter: 469 loss: 2.16630519e-06
Iter: 470 loss: 2.16214767e-06
Iter: 471 loss: 2.16009403e-06
Iter: 472 loss: 2.15523323e-06
Iter: 473 loss: 2.20210518e-06
Iter: 474 loss: 2.15466798e-06
Iter: 475 loss: 2.1496694e-06
Iter: 476 loss: 2.15102295e-06
Iter: 477 loss: 2.14607576e-06
Iter: 478 loss: 2.13993735e-06
Iter: 479 loss: 2.14471856e-06
Iter: 480 loss: 2.13645808e-06
Iter: 481 loss: 2.12943542e-06
Iter: 482 loss: 2.14885358e-06
Iter: 483 loss: 2.12710506e-06
Iter: 484 loss: 2.12349505e-06
Iter: 485 loss: 2.12281475e-06
Iter: 486 loss: 2.12000987e-06
Iter: 487 loss: 2.11335328e-06
Iter: 488 loss: 2.2020231e-06
Iter: 489 loss: 2.11286e-06
Iter: 490 loss: 2.10663711e-06
Iter: 491 loss: 2.12024611e-06
Iter: 492 loss: 2.10427652e-06
Iter: 493 loss: 2.0974237e-06
Iter: 494 loss: 2.15800173e-06
Iter: 495 loss: 2.09727477e-06
Iter: 496 loss: 2.09248174e-06
Iter: 497 loss: 2.1056353e-06
Iter: 498 loss: 2.09105679e-06
Iter: 499 loss: 2.08663369e-06
Iter: 500 loss: 2.08779238e-06
Iter: 501 loss: 2.08360166e-06
Iter: 502 loss: 2.08677761e-06
Iter: 503 loss: 2.08183019e-06
Iter: 504 loss: 2.08000301e-06
Iter: 505 loss: 2.07639641e-06
Iter: 506 loss: 2.15969067e-06
Iter: 507 loss: 2.07633229e-06
Iter: 508 loss: 2.07350445e-06
Iter: 509 loss: 2.07087692e-06
Iter: 510 loss: 2.07007952e-06
Iter: 511 loss: 2.06501045e-06
Iter: 512 loss: 2.07321864e-06
Iter: 513 loss: 2.06252116e-06
Iter: 514 loss: 2.05742458e-06
Iter: 515 loss: 2.08495248e-06
Iter: 516 loss: 2.05664765e-06
Iter: 517 loss: 2.05341485e-06
Iter: 518 loss: 2.09596419e-06
Iter: 519 loss: 2.05343804e-06
Iter: 520 loss: 2.050474e-06
Iter: 521 loss: 2.04803177e-06
Iter: 522 loss: 2.04716616e-06
Iter: 523 loss: 2.04296794e-06
Iter: 524 loss: 2.04578237e-06
Iter: 525 loss: 2.04047046e-06
Iter: 526 loss: 2.03546506e-06
Iter: 527 loss: 2.04659909e-06
Iter: 528 loss: 2.03329091e-06
Iter: 529 loss: 2.02765841e-06
Iter: 530 loss: 2.06497543e-06
Iter: 531 loss: 2.02705405e-06
Iter: 532 loss: 2.02232695e-06
Iter: 533 loss: 2.02259253e-06
Iter: 534 loss: 2.01857392e-06
Iter: 535 loss: 2.01746479e-06
Iter: 536 loss: 2.01608077e-06
Iter: 537 loss: 2.01287867e-06
Iter: 538 loss: 2.01264129e-06
Iter: 539 loss: 2.0102932e-06
Iter: 540 loss: 2.00815884e-06
Iter: 541 loss: 2.00416753e-06
Iter: 542 loss: 2.09174732e-06
Iter: 543 loss: 2.00420482e-06
Iter: 544 loss: 1.99860051e-06
Iter: 545 loss: 2.00386603e-06
Iter: 546 loss: 1.99553733e-06
Iter: 547 loss: 1.99128954e-06
Iter: 548 loss: 2.020593e-06
Iter: 549 loss: 1.99074339e-06
Iter: 550 loss: 1.9875547e-06
Iter: 551 loss: 2.0163634e-06
Iter: 552 loss: 1.98748739e-06
Iter: 553 loss: 1.98405132e-06
Iter: 554 loss: 1.98386624e-06
Iter: 555 loss: 1.98142561e-06
Iter: 556 loss: 1.97736063e-06
Iter: 557 loss: 1.97819622e-06
Iter: 558 loss: 1.97438476e-06
Iter: 559 loss: 1.96881797e-06
Iter: 560 loss: 1.97637951e-06
Iter: 561 loss: 1.96582891e-06
Iter: 562 loss: 1.96048904e-06
Iter: 563 loss: 2.02233241e-06
Iter: 564 loss: 1.9604704e-06
Iter: 565 loss: 1.95654e-06
Iter: 566 loss: 1.96060296e-06
Iter: 567 loss: 1.95417169e-06
Iter: 568 loss: 1.95174243e-06
Iter: 569 loss: 1.95172652e-06
Iter: 570 loss: 1.94887252e-06
Iter: 571 loss: 1.95635948e-06
Iter: 572 loss: 1.94792574e-06
Iter: 573 loss: 1.9464328e-06
Iter: 574 loss: 1.94266477e-06
Iter: 575 loss: 1.96264932e-06
Iter: 576 loss: 1.94146423e-06
Iter: 577 loss: 1.9357276e-06
Iter: 578 loss: 1.95315693e-06
Iter: 579 loss: 1.93393544e-06
Iter: 580 loss: 1.92990569e-06
Iter: 581 loss: 1.94700215e-06
Iter: 582 loss: 1.92903781e-06
Iter: 583 loss: 1.92566313e-06
Iter: 584 loss: 1.95218445e-06
Iter: 585 loss: 1.92554808e-06
Iter: 586 loss: 1.922205e-06
Iter: 587 loss: 1.92597781e-06
Iter: 588 loss: 1.92063249e-06
Iter: 589 loss: 1.91721097e-06
Iter: 590 loss: 1.91737035e-06
Iter: 591 loss: 1.91474373e-06
Iter: 592 loss: 1.91049958e-06
Iter: 593 loss: 1.91627e-06
Iter: 594 loss: 1.90839364e-06
Iter: 595 loss: 1.9041604e-06
Iter: 596 loss: 1.94109316e-06
Iter: 597 loss: 1.90399226e-06
Iter: 598 loss: 1.90042351e-06
Iter: 599 loss: 1.90756873e-06
Iter: 600 loss: 1.8990728e-06
Iter: 601 loss: 1.89651314e-06
Iter: 602 loss: 1.89657499e-06
Iter: 603 loss: 1.89405921e-06
Iter: 604 loss: 1.90733249e-06
Iter: 605 loss: 1.89390471e-06
Iter: 606 loss: 1.89280843e-06
Iter: 607 loss: 1.88993465e-06
Iter: 608 loss: 1.90347214e-06
Iter: 609 loss: 1.88895115e-06
Iter: 610 loss: 1.88500985e-06
Iter: 611 loss: 1.90726462e-06
Iter: 612 loss: 1.88443505e-06
Iter: 613 loss: 1.88181366e-06
Iter: 614 loss: 1.88376634e-06
Iter: 615 loss: 1.88014576e-06
Iter: 616 loss: 1.87691762e-06
Iter: 617 loss: 1.90182834e-06
Iter: 618 loss: 1.87658668e-06
Iter: 619 loss: 1.87321098e-06
Iter: 620 loss: 1.88712352e-06
Iter: 621 loss: 1.87243381e-06
Iter: 622 loss: 1.87027888e-06
Iter: 623 loss: 1.86898581e-06
Iter: 624 loss: 1.86835e-06
Iter: 625 loss: 1.86472187e-06
Iter: 626 loss: 1.86607872e-06
Iter: 627 loss: 1.86204215e-06
Iter: 628 loss: 1.85764679e-06
Iter: 629 loss: 1.88842114e-06
Iter: 630 loss: 1.85726196e-06
Iter: 631 loss: 1.85350314e-06
Iter: 632 loss: 1.86302532e-06
Iter: 633 loss: 1.85228578e-06
Iter: 634 loss: 1.84982764e-06
Iter: 635 loss: 1.88552553e-06
Iter: 636 loss: 1.8496529e-06
Iter: 637 loss: 1.84733801e-06
Iter: 638 loss: 1.87292585e-06
Iter: 639 loss: 1.84716623e-06
Iter: 640 loss: 1.84641306e-06
Iter: 641 loss: 1.84346106e-06
Iter: 642 loss: 1.84544183e-06
Iter: 643 loss: 1.84087207e-06
Iter: 644 loss: 1.83595967e-06
Iter: 645 loss: 1.87130013e-06
Iter: 646 loss: 1.8353619e-06
Iter: 647 loss: 1.83224859e-06
Iter: 648 loss: 1.8334722e-06
Iter: 649 loss: 1.82981671e-06
Iter: 650 loss: 1.82562644e-06
Iter: 651 loss: 1.85075521e-06
Iter: 652 loss: 1.82508052e-06
Iter: 653 loss: 1.82104463e-06
Iter: 654 loss: 1.86237867e-06
Iter: 655 loss: 1.82090582e-06
Iter: 656 loss: 1.81895666e-06
Iter: 657 loss: 1.81550502e-06
Iter: 658 loss: 1.81540815e-06
Iter: 659 loss: 1.81103223e-06
Iter: 660 loss: 1.81821542e-06
Iter: 661 loss: 1.80911911e-06
Iter: 662 loss: 1.80557845e-06
Iter: 663 loss: 1.83750308e-06
Iter: 664 loss: 1.80547863e-06
Iter: 665 loss: 1.80253505e-06
Iter: 666 loss: 1.80951656e-06
Iter: 667 loss: 1.80167729e-06
Iter: 668 loss: 1.79920767e-06
Iter: 669 loss: 1.82334145e-06
Iter: 670 loss: 1.79900724e-06
Iter: 671 loss: 1.79757512e-06
Iter: 672 loss: 1.79751646e-06
Iter: 673 loss: 1.7967235e-06
Iter: 674 loss: 1.79448023e-06
Iter: 675 loss: 1.79603671e-06
Iter: 676 loss: 1.79272138e-06
Iter: 677 loss: 1.78910682e-06
Iter: 678 loss: 1.81213363e-06
Iter: 679 loss: 1.7887553e-06
Iter: 680 loss: 1.78608104e-06
Iter: 681 loss: 1.78621269e-06
Iter: 682 loss: 1.78420748e-06
Iter: 683 loss: 1.78045252e-06
Iter: 684 loss: 1.79673384e-06
Iter: 685 loss: 1.77955189e-06
Iter: 686 loss: 1.77834181e-06
Iter: 687 loss: 1.77783897e-06
Iter: 688 loss: 1.7766672e-06
Iter: 689 loss: 1.77379479e-06
Iter: 690 loss: 1.81204973e-06
Iter: 691 loss: 1.77361346e-06
Iter: 692 loss: 1.77004097e-06
Iter: 693 loss: 1.77434379e-06
Iter: 694 loss: 1.76796152e-06
Iter: 695 loss: 1.76478716e-06
Iter: 696 loss: 1.78289429e-06
Iter: 697 loss: 1.76417745e-06
Iter: 698 loss: 1.76101889e-06
Iter: 699 loss: 1.77164839e-06
Iter: 700 loss: 1.76002573e-06
Iter: 701 loss: 1.75725108e-06
Iter: 702 loss: 1.77350648e-06
Iter: 703 loss: 1.75707953e-06
Iter: 704 loss: 1.75634989e-06
Iter: 705 loss: 1.7558815e-06
Iter: 706 loss: 1.7552436e-06
Iter: 707 loss: 1.75348919e-06
Iter: 708 loss: 1.75457058e-06
Iter: 709 loss: 1.75188325e-06
Iter: 710 loss: 1.74940203e-06
Iter: 711 loss: 1.75386708e-06
Iter: 712 loss: 1.74830416e-06
Iter: 713 loss: 1.74464844e-06
Iter: 714 loss: 1.75395303e-06
Iter: 715 loss: 1.74333616e-06
Iter: 716 loss: 1.73994476e-06
Iter: 717 loss: 1.75463163e-06
Iter: 718 loss: 1.73903186e-06
Iter: 719 loss: 1.7373236e-06
Iter: 720 loss: 1.73741421e-06
Iter: 721 loss: 1.73565456e-06
Iter: 722 loss: 1.73299657e-06
Iter: 723 loss: 1.73287822e-06
Iter: 724 loss: 1.72988052e-06
Iter: 725 loss: 1.73325884e-06
Iter: 726 loss: 1.72823991e-06
Iter: 727 loss: 1.72561261e-06
Iter: 728 loss: 1.72853788e-06
Iter: 729 loss: 1.72394232e-06
Iter: 730 loss: 1.72097043e-06
Iter: 731 loss: 1.75342734e-06
Iter: 732 loss: 1.72074328e-06
Iter: 733 loss: 1.71874331e-06
Iter: 734 loss: 1.72772366e-06
Iter: 735 loss: 1.71848478e-06
Iter: 736 loss: 1.71848285e-06
Iter: 737 loss: 1.71756551e-06
Iter: 738 loss: 1.71700651e-06
Iter: 739 loss: 1.71538636e-06
Iter: 740 loss: 1.71607735e-06
Iter: 741 loss: 1.71363149e-06
Iter: 742 loss: 1.71102147e-06
Iter: 743 loss: 1.70981207e-06
Iter: 744 loss: 1.70827343e-06
Iter: 745 loss: 1.70401267e-06
Iter: 746 loss: 1.73081196e-06
Iter: 747 loss: 1.70334386e-06
Iter: 748 loss: 1.70036139e-06
Iter: 749 loss: 1.71634269e-06
Iter: 750 loss: 1.69966165e-06
Iter: 751 loss: 1.69752786e-06
Iter: 752 loss: 1.70451858e-06
Iter: 753 loss: 1.69661075e-06
Iter: 754 loss: 1.69376301e-06
Iter: 755 loss: 1.7094485e-06
Iter: 756 loss: 1.69333703e-06
Iter: 757 loss: 1.69060604e-06
Iter: 758 loss: 1.68961435e-06
Iter: 759 loss: 1.68820634e-06
Iter: 760 loss: 1.68593965e-06
Iter: 761 loss: 1.68527038e-06
Iter: 762 loss: 1.68376084e-06
Iter: 763 loss: 1.68061615e-06
Iter: 764 loss: 1.70542819e-06
Iter: 765 loss: 1.68057591e-06
Iter: 766 loss: 1.67841404e-06
Iter: 767 loss: 1.70737212e-06
Iter: 768 loss: 1.6785466e-06
Iter: 769 loss: 1.67748703e-06
Iter: 770 loss: 1.67742212e-06
Iter: 771 loss: 1.67620635e-06
Iter: 772 loss: 1.6729814e-06
Iter: 773 loss: 1.69354917e-06
Iter: 774 loss: 1.67228757e-06
Iter: 775 loss: 1.67017197e-06
Iter: 776 loss: 1.67037751e-06
Iter: 777 loss: 1.66863413e-06
Iter: 778 loss: 1.66570874e-06
Iter: 779 loss: 1.67287214e-06
Iter: 780 loss: 1.66482528e-06
Iter: 781 loss: 1.66167695e-06
Iter: 782 loss: 1.67586882e-06
Iter: 783 loss: 1.66124823e-06
Iter: 784 loss: 1.65852362e-06
Iter: 785 loss: 1.66295911e-06
Iter: 786 loss: 1.65740175e-06
Iter: 787 loss: 1.65583526e-06
Iter: 788 loss: 1.65561153e-06
Iter: 789 loss: 1.65422512e-06
Iter: 790 loss: 1.65481572e-06
Iter: 791 loss: 1.65333222e-06
Iter: 792 loss: 1.65183951e-06
Iter: 793 loss: 1.64815629e-06
Iter: 794 loss: 1.69367127e-06
Iter: 795 loss: 1.64782455e-06
Iter: 796 loss: 1.64373523e-06
Iter: 797 loss: 1.6683681e-06
Iter: 798 loss: 1.64316111e-06
Iter: 799 loss: 1.64075857e-06
Iter: 800 loss: 1.65807319e-06
Iter: 801 loss: 1.64061169e-06
Iter: 802 loss: 1.639728e-06
Iter: 803 loss: 1.63955576e-06
Iter: 804 loss: 1.63849779e-06
Iter: 805 loss: 1.6378965e-06
Iter: 806 loss: 1.63728669e-06
Iter: 807 loss: 1.63640652e-06
Iter: 808 loss: 1.63435845e-06
Iter: 809 loss: 1.6644484e-06
Iter: 810 loss: 1.63422249e-06
Iter: 811 loss: 1.63137884e-06
Iter: 812 loss: 1.63250797e-06
Iter: 813 loss: 1.62928154e-06
Iter: 814 loss: 1.62660911e-06
Iter: 815 loss: 1.64221603e-06
Iter: 816 loss: 1.62601509e-06
Iter: 817 loss: 1.62350636e-06
Iter: 818 loss: 1.6297447e-06
Iter: 819 loss: 1.62235153e-06
Iter: 820 loss: 1.61993398e-06
Iter: 821 loss: 1.64156052e-06
Iter: 822 loss: 1.61977698e-06
Iter: 823 loss: 1.6176491e-06
Iter: 824 loss: 1.63015613e-06
Iter: 825 loss: 1.61739217e-06
Iter: 826 loss: 1.61570529e-06
Iter: 827 loss: 1.61433013e-06
Iter: 828 loss: 1.61381899e-06
Iter: 829 loss: 1.61150638e-06
Iter: 830 loss: 1.61681328e-06
Iter: 831 loss: 1.61080902e-06
Iter: 832 loss: 1.60834725e-06
Iter: 833 loss: 1.60581385e-06
Iter: 834 loss: 1.60523064e-06
Iter: 835 loss: 1.60924162e-06
Iter: 836 loss: 1.60424815e-06
Iter: 837 loss: 1.60327454e-06
Iter: 838 loss: 1.60478908e-06
Iter: 839 loss: 1.60272361e-06
Iter: 840 loss: 1.60177092e-06
Iter: 841 loss: 1.59944557e-06
Iter: 842 loss: 1.61816661e-06
Iter: 843 loss: 1.59899037e-06
Iter: 844 loss: 1.59565889e-06
Iter: 845 loss: 1.60026559e-06
Iter: 846 loss: 1.59398519e-06
Iter: 847 loss: 1.59127876e-06
Iter: 848 loss: 1.59676733e-06
Iter: 849 loss: 1.59013143e-06
Iter: 850 loss: 1.58678472e-06
Iter: 851 loss: 1.59691172e-06
Iter: 852 loss: 1.58576745e-06
Iter: 853 loss: 1.5836024e-06
Iter: 854 loss: 1.5985778e-06
Iter: 855 loss: 1.58346666e-06
Iter: 856 loss: 1.58180467e-06
Iter: 857 loss: 1.60453828e-06
Iter: 858 loss: 1.58186413e-06
Iter: 859 loss: 1.58056616e-06
Iter: 860 loss: 1.58063608e-06
Iter: 861 loss: 1.57944578e-06
Iter: 862 loss: 1.57812565e-06
Iter: 863 loss: 1.57813565e-06
Iter: 864 loss: 1.57701777e-06
Iter: 865 loss: 1.57502893e-06
Iter: 866 loss: 1.57947716e-06
Iter: 867 loss: 1.57423608e-06
Iter: 868 loss: 1.57219415e-06
Iter: 869 loss: 1.57630438e-06
Iter: 870 loss: 1.57144814e-06
Iter: 871 loss: 1.56923227e-06
Iter: 872 loss: 1.57445675e-06
Iter: 873 loss: 1.56833551e-06
Iter: 874 loss: 1.5717618e-06
Iter: 875 loss: 1.56774854e-06
Iter: 876 loss: 1.56730903e-06
Iter: 877 loss: 1.56587748e-06
Iter: 878 loss: 1.57051545e-06
Iter: 879 loss: 1.56547878e-06
Iter: 880 loss: 1.56372835e-06
Iter: 881 loss: 1.56548128e-06
Iter: 882 loss: 1.56265696e-06
Iter: 883 loss: 1.56040835e-06
Iter: 884 loss: 1.56257988e-06
Iter: 885 loss: 1.55918246e-06
Iter: 886 loss: 1.55716589e-06
Iter: 887 loss: 1.55520956e-06
Iter: 888 loss: 1.55498628e-06
Iter: 889 loss: 1.55254406e-06
Iter: 890 loss: 1.55270027e-06
Iter: 891 loss: 1.55143073e-06
Iter: 892 loss: 1.55718737e-06
Iter: 893 loss: 1.55130147e-06
Iter: 894 loss: 1.54961322e-06
Iter: 895 loss: 1.55267867e-06
Iter: 896 loss: 1.54901795e-06
Iter: 897 loss: 1.54760937e-06
Iter: 898 loss: 1.54675922e-06
Iter: 899 loss: 1.54617737e-06
Iter: 900 loss: 1.54451868e-06
Iter: 901 loss: 1.54414545e-06
Iter: 902 loss: 1.5431782e-06
Iter: 903 loss: 1.54095528e-06
Iter: 904 loss: 1.556553e-06
Iter: 905 loss: 1.54079407e-06
Iter: 906 loss: 1.54232725e-06
Iter: 907 loss: 1.54034171e-06
Iter: 908 loss: 1.53969859e-06
Iter: 909 loss: 1.53848327e-06
Iter: 910 loss: 1.54845861e-06
Iter: 911 loss: 1.5381047e-06
Iter: 912 loss: 1.53632618e-06
Iter: 913 loss: 1.53636745e-06
Iter: 914 loss: 1.53506653e-06
Iter: 915 loss: 1.53265808e-06
Iter: 916 loss: 1.53391898e-06
Iter: 917 loss: 1.53114797e-06
Iter: 918 loss: 1.52849941e-06
Iter: 919 loss: 1.53156884e-06
Iter: 920 loss: 1.5271853e-06
Iter: 921 loss: 1.52457164e-06
Iter: 922 loss: 1.54039265e-06
Iter: 923 loss: 1.52429732e-06
Iter: 924 loss: 1.52177927e-06
Iter: 925 loss: 1.52694247e-06
Iter: 926 loss: 1.52090229e-06
Iter: 927 loss: 1.51926952e-06
Iter: 928 loss: 1.54394536e-06
Iter: 929 loss: 1.51916061e-06
Iter: 930 loss: 1.51772156e-06
Iter: 931 loss: 1.52428379e-06
Iter: 932 loss: 1.51755262e-06
Iter: 933 loss: 1.51653603e-06
Iter: 934 loss: 1.51434438e-06
Iter: 935 loss: 1.55794123e-06
Iter: 936 loss: 1.51429572e-06
Iter: 937 loss: 1.5126011e-06
Iter: 938 loss: 1.52859116e-06
Iter: 939 loss: 1.51242307e-06
Iter: 940 loss: 1.51146628e-06
Iter: 941 loss: 1.51895676e-06
Iter: 942 loss: 1.51147287e-06
Iter: 943 loss: 1.51003348e-06
Iter: 944 loss: 1.51217841e-06
Iter: 945 loss: 1.5093467e-06
Iter: 946 loss: 1.50856954e-06
Iter: 947 loss: 1.50739743e-06
Iter: 948 loss: 1.50743153e-06
Iter: 949 loss: 1.50577932e-06
Iter: 950 loss: 1.50977917e-06
Iter: 951 loss: 1.50547226e-06
Iter: 952 loss: 1.50393441e-06
Iter: 953 loss: 1.5034e-06
Iter: 954 loss: 1.50251947e-06
Iter: 955 loss: 1.50056451e-06
Iter: 956 loss: 1.50758876e-06
Iter: 957 loss: 1.49976802e-06
Iter: 958 loss: 1.49775087e-06
Iter: 959 loss: 1.49878406e-06
Iter: 960 loss: 1.49627022e-06
Iter: 961 loss: 1.49398022e-06
Iter: 962 loss: 1.50859978e-06
Iter: 963 loss: 1.49366338e-06
Iter: 964 loss: 1.49177652e-06
Iter: 965 loss: 1.50963604e-06
Iter: 966 loss: 1.49158291e-06
Iter: 967 loss: 1.49020536e-06
Iter: 968 loss: 1.49934772e-06
Iter: 969 loss: 1.49021525e-06
Iter: 970 loss: 1.48921742e-06
Iter: 971 loss: 1.48687536e-06
Iter: 972 loss: 1.51645736e-06
Iter: 973 loss: 1.48679578e-06
Iter: 974 loss: 1.4847036e-06
Iter: 975 loss: 1.5159128e-06
Iter: 976 loss: 1.48459912e-06
Iter: 977 loss: 1.48329661e-06
Iter: 978 loss: 1.48326581e-06
Iter: 979 loss: 1.48286585e-06
Iter: 980 loss: 1.48164293e-06
Iter: 981 loss: 1.49314008e-06
Iter: 982 loss: 1.48151128e-06
Iter: 983 loss: 1.48013464e-06
Iter: 984 loss: 1.48067988e-06
Iter: 985 loss: 1.47912488e-06
Iter: 986 loss: 1.47737012e-06
Iter: 987 loss: 1.48130346e-06
Iter: 988 loss: 1.47670971e-06
Iter: 989 loss: 1.47514447e-06
Iter: 990 loss: 1.47851051e-06
Iter: 991 loss: 1.47445962e-06
Iter: 992 loss: 1.4727475e-06
Iter: 993 loss: 1.47374681e-06
Iter: 994 loss: 1.47143805e-06
Iter: 995 loss: 1.46913271e-06
Iter: 996 loss: 1.47367189e-06
Iter: 997 loss: 1.46812567e-06
Iter: 998 loss: 1.46624973e-06
Iter: 999 loss: 1.48945333e-06
Iter: 1000 loss: 1.46613866e-06
Iter: 1001 loss: 1.46447542e-06
Iter: 1002 loss: 1.47219782e-06
Iter: 1003 loss: 1.4643781e-06
Iter: 1004 loss: 1.46271634e-06
Iter: 1005 loss: 1.46264506e-06
Iter: 1006 loss: 1.46142656e-06
Iter: 1007 loss: 1.45999445e-06
Iter: 1008 loss: 1.4691451e-06
Iter: 1009 loss: 1.45978322e-06
Iter: 1010 loss: 1.45965146e-06
Iter: 1011 loss: 1.45919603e-06
Iter: 1012 loss: 1.45898775e-06
Iter: 1013 loss: 1.45800993e-06
Iter: 1014 loss: 1.46036052e-06
Iter: 1015 loss: 1.45741944e-06
Iter: 1016 loss: 1.45585068e-06
Iter: 1017 loss: 1.46011541e-06
Iter: 1018 loss: 1.45529043e-06
Iter: 1019 loss: 1.45383785e-06
Iter: 1020 loss: 1.45888225e-06
Iter: 1021 loss: 1.45343506e-06
Iter: 1022 loss: 1.45210834e-06
Iter: 1023 loss: 1.45132833e-06
Iter: 1024 loss: 1.45068395e-06
Iter: 1025 loss: 1.44827879e-06
Iter: 1026 loss: 1.45730098e-06
Iter: 1027 loss: 1.44759429e-06
Iter: 1028 loss: 1.44550631e-06
Iter: 1029 loss: 1.45148613e-06
Iter: 1030 loss: 1.44492174e-06
Iter: 1031 loss: 1.44312696e-06
Iter: 1032 loss: 1.44929186e-06
Iter: 1033 loss: 1.4425874e-06
Iter: 1034 loss: 1.4412858e-06
Iter: 1035 loss: 1.44118712e-06
Iter: 1036 loss: 1.44013336e-06
Iter: 1037 loss: 1.43873035e-06
Iter: 1038 loss: 1.43874968e-06
Iter: 1039 loss: 1.43718466e-06
Iter: 1040 loss: 1.45072954e-06
Iter: 1041 loss: 1.43703869e-06
Iter: 1042 loss: 1.43752982e-06
Iter: 1043 loss: 1.43679381e-06
Iter: 1044 loss: 1.43634156e-06
Iter: 1045 loss: 1.43527336e-06
Iter: 1046 loss: 1.43863781e-06
Iter: 1047 loss: 1.43483385e-06
Iter: 1048 loss: 1.4332702e-06
Iter: 1049 loss: 1.43576244e-06
Iter: 1050 loss: 1.43260229e-06
Iter: 1051 loss: 1.43103637e-06
Iter: 1052 loss: 1.43739157e-06
Iter: 1053 loss: 1.43075533e-06
Iter: 1054 loss: 1.42933595e-06
Iter: 1055 loss: 1.42899307e-06
Iter: 1056 loss: 1.42809e-06
Iter: 1057 loss: 1.42590306e-06
Iter: 1058 loss: 1.43334023e-06
Iter: 1059 loss: 1.42552744e-06
Iter: 1060 loss: 1.42378019e-06
Iter: 1061 loss: 1.42645865e-06
Iter: 1062 loss: 1.42293629e-06
Iter: 1063 loss: 1.42070257e-06
Iter: 1064 loss: 1.4238326e-06
Iter: 1065 loss: 1.41976022e-06
Iter: 1066 loss: 1.41839962e-06
Iter: 1067 loss: 1.41843498e-06
Iter: 1068 loss: 1.41705209e-06
Iter: 1069 loss: 1.41737678e-06
Iter: 1070 loss: 1.4161219e-06
Iter: 1071 loss: 1.41479632e-06
Iter: 1072 loss: 1.41529563e-06
Iter: 1073 loss: 1.41380281e-06
Iter: 1074 loss: 1.41291434e-06
Iter: 1075 loss: 1.41286e-06
Iter: 1076 loss: 1.41149212e-06
Iter: 1077 loss: 1.41326109e-06
Iter: 1078 loss: 1.41095211e-06
Iter: 1079 loss: 1.41028738e-06
Iter: 1080 loss: 1.40897259e-06
Iter: 1081 loss: 1.42766351e-06
Iter: 1082 loss: 1.40899886e-06
Iter: 1083 loss: 1.40762825e-06
Iter: 1084 loss: 1.41331748e-06
Iter: 1085 loss: 1.40740622e-06
Iter: 1086 loss: 1.40576105e-06
Iter: 1087 loss: 1.40728935e-06
Iter: 1088 loss: 1.40487987e-06
Iter: 1089 loss: 1.40372867e-06
Iter: 1090 loss: 1.40615498e-06
Iter: 1091 loss: 1.4031516e-06
Iter: 1092 loss: 1.40159432e-06
Iter: 1093 loss: 1.40207567e-06
Iter: 1094 loss: 1.40048883e-06
Iter: 1095 loss: 1.39864437e-06
Iter: 1096 loss: 1.41346027e-06
Iter: 1097 loss: 1.39855274e-06
Iter: 1098 loss: 1.39763563e-06
Iter: 1099 loss: 1.39798954e-06
Iter: 1100 loss: 1.39689246e-06
Iter: 1101 loss: 1.3962067e-06
Iter: 1102 loss: 1.39604117e-06
Iter: 1103 loss: 1.39529516e-06
Iter: 1104 loss: 1.39451276e-06
Iter: 1105 loss: 1.39438112e-06
Iter: 1106 loss: 1.39387271e-06
Iter: 1107 loss: 1.39377016e-06
Iter: 1108 loss: 1.39335089e-06
Iter: 1109 loss: 1.39538724e-06
Iter: 1110 loss: 1.39301926e-06
Iter: 1111 loss: 1.39288807e-06
Iter: 1112 loss: 1.39239455e-06
Iter: 1113 loss: 1.39246993e-06
Iter: 1114 loss: 1.39169674e-06
Iter: 1115 loss: 1.39063468e-06
Iter: 1116 loss: 1.390639e-06
Iter: 1117 loss: 1.38875043e-06
Iter: 1118 loss: 1.39371423e-06
Iter: 1119 loss: 1.38804558e-06
Iter: 1120 loss: 1.38626365e-06
Iter: 1121 loss: 1.40212296e-06
Iter: 1122 loss: 1.38629741e-06
Iter: 1123 loss: 1.38547966e-06
Iter: 1124 loss: 1.38339988e-06
Iter: 1125 loss: 1.4068014e-06
Iter: 1126 loss: 1.38338419e-06
Iter: 1127 loss: 1.381159e-06
Iter: 1128 loss: 1.40122495e-06
Iter: 1129 loss: 1.38090843e-06
Iter: 1130 loss: 1.37947836e-06
Iter: 1131 loss: 1.38134567e-06
Iter: 1132 loss: 1.37884422e-06
Iter: 1133 loss: 1.37628786e-06
Iter: 1134 loss: 1.37647396e-06
Iter: 1135 loss: 1.37434233e-06
Iter: 1136 loss: 1.37475445e-06
Iter: 1137 loss: 1.37303141e-06
Iter: 1138 loss: 1.37240772e-06
Iter: 1139 loss: 1.37160339e-06
Iter: 1140 loss: 1.37158145e-06
Iter: 1141 loss: 1.37150482e-06
Iter: 1142 loss: 1.371042e-06
Iter: 1143 loss: 1.37048482e-06
Iter: 1144 loss: 1.36886092e-06
Iter: 1145 loss: 1.37528684e-06
Iter: 1146 loss: 1.368158e-06
Iter: 1147 loss: 1.36649396e-06
Iter: 1148 loss: 1.37575125e-06
Iter: 1149 loss: 1.3661313e-06
Iter: 1150 loss: 1.36510016e-06
Iter: 1151 loss: 1.37047095e-06
Iter: 1152 loss: 1.36482868e-06
Iter: 1153 loss: 1.36334324e-06
Iter: 1154 loss: 1.36199242e-06
Iter: 1155 loss: 1.36162089e-06
Iter: 1156 loss: 1.36032781e-06
Iter: 1157 loss: 1.38190637e-06
Iter: 1158 loss: 1.36029144e-06
Iter: 1159 loss: 1.35913638e-06
Iter: 1160 loss: 1.3577037e-06
Iter: 1161 loss: 1.35762718e-06
Iter: 1162 loss: 1.3562634e-06
Iter: 1163 loss: 1.35903724e-06
Iter: 1164 loss: 1.35572554e-06
Iter: 1165 loss: 1.35427274e-06
Iter: 1166 loss: 1.35375558e-06
Iter: 1167 loss: 1.35303367e-06
Iter: 1168 loss: 1.35204755e-06
Iter: 1169 loss: 1.35195796e-06
Iter: 1170 loss: 1.35089931e-06
Iter: 1171 loss: 1.35008486e-06
Iter: 1172 loss: 1.34984396e-06
Iter: 1173 loss: 1.35025789e-06
Iter: 1174 loss: 1.34931781e-06
Iter: 1175 loss: 1.34881975e-06
Iter: 1176 loss: 1.34776201e-06
Iter: 1177 loss: 1.36436586e-06
Iter: 1178 loss: 1.34767981e-06
Iter: 1179 loss: 1.34649099e-06
Iter: 1180 loss: 1.35413052e-06
Iter: 1181 loss: 1.34634547e-06
Iter: 1182 loss: 1.34540937e-06
Iter: 1183 loss: 1.34347738e-06
Iter: 1184 loss: 1.37594475e-06
Iter: 1185 loss: 1.3433646e-06
Iter: 1186 loss: 1.34199104e-06
Iter: 1187 loss: 1.34186485e-06
Iter: 1188 loss: 1.34049935e-06
Iter: 1189 loss: 1.34221295e-06
Iter: 1190 loss: 1.33985736e-06
Iter: 1191 loss: 1.33857384e-06
Iter: 1192 loss: 1.34882703e-06
Iter: 1193 loss: 1.33827587e-06
Iter: 1194 loss: 1.33749177e-06
Iter: 1195 loss: 1.3399449e-06
Iter: 1196 loss: 1.33701337e-06
Iter: 1197 loss: 1.33597769e-06
Iter: 1198 loss: 1.33563549e-06
Iter: 1199 loss: 1.33502022e-06
Iter: 1200 loss: 1.33355616e-06
Iter: 1201 loss: 1.34375455e-06
Iter: 1202 loss: 1.33340143e-06
Iter: 1203 loss: 1.33264837e-06
Iter: 1204 loss: 1.3379755e-06
Iter: 1205 loss: 1.33257959e-06
Iter: 1206 loss: 1.33167919e-06
Iter: 1207 loss: 1.33896901e-06
Iter: 1208 loss: 1.33161188e-06
Iter: 1209 loss: 1.33125161e-06
Iter: 1210 loss: 1.33126287e-06
Iter: 1211 loss: 1.33100582e-06
Iter: 1212 loss: 1.33047502e-06
Iter: 1213 loss: 1.33088247e-06
Iter: 1214 loss: 1.3299416e-06
Iter: 1215 loss: 1.32872788e-06
Iter: 1216 loss: 1.32804803e-06
Iter: 1217 loss: 1.32744356e-06
Iter: 1218 loss: 1.32551804e-06
Iter: 1219 loss: 1.33193521e-06
Iter: 1220 loss: 1.32503988e-06
Iter: 1221 loss: 1.32315927e-06
Iter: 1222 loss: 1.32846708e-06
Iter: 1223 loss: 1.322662e-06
Iter: 1224 loss: 1.32113973e-06
Iter: 1225 loss: 1.33564777e-06
Iter: 1226 loss: 1.32117339e-06
Iter: 1227 loss: 1.31977913e-06
Iter: 1228 loss: 1.32339369e-06
Iter: 1229 loss: 1.31937122e-06
Iter: 1230 loss: 1.31819468e-06
Iter: 1231 loss: 1.31907007e-06
Iter: 1232 loss: 1.31754643e-06
Iter: 1233 loss: 1.31639604e-06
Iter: 1234 loss: 1.32514833e-06
Iter: 1235 loss: 1.31635352e-06
Iter: 1236 loss: 1.31557454e-06
Iter: 1237 loss: 1.31786032e-06
Iter: 1238 loss: 1.31525621e-06
Iter: 1239 loss: 1.31455874e-06
Iter: 1240 loss: 1.31458751e-06
Iter: 1241 loss: 1.31421541e-06
Iter: 1242 loss: 1.31807133e-06
Iter: 1243 loss: 1.31417414e-06
Iter: 1244 loss: 1.31396098e-06
Iter: 1245 loss: 1.31344643e-06
Iter: 1246 loss: 1.31611034e-06
Iter: 1247 loss: 1.3131712e-06
Iter: 1248 loss: 1.31245247e-06
Iter: 1249 loss: 1.31238392e-06
Iter: 1250 loss: 1.31174386e-06
Iter: 1251 loss: 1.3107632e-06
Iter: 1252 loss: 1.31200136e-06
Iter: 1253 loss: 1.31020386e-06
Iter: 1254 loss: 1.30875105e-06
Iter: 1255 loss: 1.3140135e-06
Iter: 1256 loss: 1.30852118e-06
Iter: 1257 loss: 1.30757985e-06
Iter: 1258 loss: 1.31277011e-06
Iter: 1259 loss: 1.30732917e-06
Iter: 1260 loss: 1.30646981e-06
Iter: 1261 loss: 1.30981448e-06
Iter: 1262 loss: 1.3064041e-06
Iter: 1263 loss: 1.30521983e-06
Iter: 1264 loss: 1.30656827e-06
Iter: 1265 loss: 1.30475405e-06
Iter: 1266 loss: 1.30373087e-06
Iter: 1267 loss: 1.3046631e-06
Iter: 1268 loss: 1.30313708e-06
Iter: 1269 loss: 1.30203944e-06
Iter: 1270 loss: 1.30539615e-06
Iter: 1271 loss: 1.30164074e-06
Iter: 1272 loss: 1.30125591e-06
Iter: 1273 loss: 1.30095373e-06
Iter: 1274 loss: 1.30065507e-06
Iter: 1275 loss: 1.30472165e-06
Iter: 1276 loss: 1.3006611e-06
Iter: 1277 loss: 1.30039621e-06
Iter: 1278 loss: 1.29959392e-06
Iter: 1279 loss: 1.30445687e-06
Iter: 1280 loss: 1.2993778e-06
Iter: 1281 loss: 1.29841533e-06
Iter: 1282 loss: 1.29861269e-06
Iter: 1283 loss: 1.29776845e-06
Iter: 1284 loss: 1.29673981e-06
Iter: 1285 loss: 1.29964792e-06
Iter: 1286 loss: 1.29643536e-06
Iter: 1287 loss: 1.29531861e-06
Iter: 1288 loss: 1.29662953e-06
Iter: 1289 loss: 1.29462671e-06
Iter: 1290 loss: 1.29352702e-06
Iter: 1291 loss: 1.29570401e-06
Iter: 1292 loss: 1.29306568e-06
Iter: 1293 loss: 1.29159343e-06
Iter: 1294 loss: 1.29767523e-06
Iter: 1295 loss: 1.29143314e-06
Iter: 1296 loss: 1.29040268e-06
Iter: 1297 loss: 1.29974137e-06
Iter: 1298 loss: 1.29028263e-06
Iter: 1299 loss: 1.28933789e-06
Iter: 1300 loss: 1.28988358e-06
Iter: 1301 loss: 1.28873091e-06
Iter: 1302 loss: 1.28751253e-06
Iter: 1303 loss: 1.28834029e-06
Iter: 1304 loss: 1.28677198e-06
Iter: 1305 loss: 1.28606791e-06
Iter: 1306 loss: 1.28613488e-06
Iter: 1307 loss: 1.28562124e-06
Iter: 1308 loss: 1.28558224e-06
Iter: 1309 loss: 1.28515535e-06
Iter: 1310 loss: 1.28486579e-06
Iter: 1311 loss: 1.28472971e-06
Iter: 1312 loss: 1.28400347e-06
Iter: 1313 loss: 1.28246779e-06
Iter: 1314 loss: 1.3045119e-06
Iter: 1315 loss: 1.28251895e-06
Iter: 1316 loss: 1.2814271e-06
Iter: 1317 loss: 1.28529427e-06
Iter: 1318 loss: 1.28124896e-06
Iter: 1319 loss: 1.27993735e-06
Iter: 1320 loss: 1.28188663e-06
Iter: 1321 loss: 1.2793796e-06
Iter: 1322 loss: 1.27809858e-06
Iter: 1323 loss: 1.28078909e-06
Iter: 1324 loss: 1.27757914e-06
Iter: 1325 loss: 1.27617102e-06
Iter: 1326 loss: 1.28395038e-06
Iter: 1327 loss: 1.27595558e-06
Iter: 1328 loss: 1.27494081e-06
Iter: 1329 loss: 1.27740395e-06
Iter: 1330 loss: 1.27454894e-06
Iter: 1331 loss: 1.27330577e-06
Iter: 1332 loss: 1.27447629e-06
Iter: 1333 loss: 1.27263888e-06
Iter: 1334 loss: 1.27119824e-06
Iter: 1335 loss: 1.27978e-06
Iter: 1336 loss: 1.27085775e-06
Iter: 1337 loss: 1.26977238e-06
Iter: 1338 loss: 1.27813178e-06
Iter: 1339 loss: 1.26973077e-06
Iter: 1340 loss: 1.2692941e-06
Iter: 1341 loss: 1.26916552e-06
Iter: 1342 loss: 1.2687243e-06
Iter: 1343 loss: 1.26780833e-06
Iter: 1344 loss: 1.26783652e-06
Iter: 1345 loss: 1.26711802e-06
Iter: 1346 loss: 1.27022872e-06
Iter: 1347 loss: 1.26688587e-06
Iter: 1348 loss: 1.26620182e-06
Iter: 1349 loss: 1.26679424e-06
Iter: 1350 loss: 1.26591328e-06
Iter: 1351 loss: 1.26536247e-06
Iter: 1352 loss: 1.2640071e-06
Iter: 1353 loss: 1.28730301e-06
Iter: 1354 loss: 1.26385726e-06
Iter: 1355 loss: 1.26233624e-06
Iter: 1356 loss: 1.27217572e-06
Iter: 1357 loss: 1.26218106e-06
Iter: 1358 loss: 1.26106306e-06
Iter: 1359 loss: 1.26251257e-06
Iter: 1360 loss: 1.26054954e-06
Iter: 1361 loss: 1.25933389e-06
Iter: 1362 loss: 1.2647904e-06
Iter: 1363 loss: 1.2590383e-06
Iter: 1364 loss: 1.25779434e-06
Iter: 1365 loss: 1.26013219e-06
Iter: 1366 loss: 1.25727615e-06
Iter: 1367 loss: 1.25614577e-06
Iter: 1368 loss: 1.26129157e-06
Iter: 1369 loss: 1.25583563e-06
Iter: 1370 loss: 1.25491067e-06
Iter: 1371 loss: 1.25835732e-06
Iter: 1372 loss: 1.25465453e-06
Iter: 1373 loss: 1.25455904e-06
Iter: 1374 loss: 1.25419456e-06
Iter: 1375 loss: 1.25370377e-06
Iter: 1376 loss: 1.25389022e-06
Iter: 1377 loss: 1.25336919e-06
Iter: 1378 loss: 1.25296901e-06
Iter: 1379 loss: 1.25257316e-06
Iter: 1380 loss: 1.25242036e-06
Iter: 1381 loss: 1.25173119e-06
Iter: 1382 loss: 1.25725296e-06
Iter: 1383 loss: 1.25164729e-06
Iter: 1384 loss: 1.25121437e-06
Iter: 1385 loss: 1.24996768e-06
Iter: 1386 loss: 1.24996052e-06
Iter: 1387 loss: 1.24901931e-06
Iter: 1388 loss: 1.25308293e-06
Iter: 1389 loss: 1.24871212e-06
Iter: 1390 loss: 1.24794008e-06
Iter: 1391 loss: 1.24850976e-06
Iter: 1392 loss: 1.2474668e-06
Iter: 1393 loss: 1.24626035e-06
Iter: 1394 loss: 1.24776602e-06
Iter: 1395 loss: 1.2458363e-06
Iter: 1396 loss: 1.24459348e-06
Iter: 1397 loss: 1.24674352e-06
Iter: 1398 loss: 1.24419e-06
Iter: 1399 loss: 1.24290295e-06
Iter: 1400 loss: 1.24854455e-06
Iter: 1401 loss: 1.2427538e-06
Iter: 1402 loss: 1.24169992e-06
Iter: 1403 loss: 1.24543726e-06
Iter: 1404 loss: 1.24139422e-06
Iter: 1405 loss: 1.24088865e-06
Iter: 1406 loss: 1.24087751e-06
Iter: 1407 loss: 1.24029339e-06
Iter: 1408 loss: 1.24248686e-06
Iter: 1409 loss: 1.24000121e-06
Iter: 1410 loss: 1.2396315e-06
Iter: 1411 loss: 1.23894017e-06
Iter: 1412 loss: 1.25119379e-06
Iter: 1413 loss: 1.23881262e-06
Iter: 1414 loss: 1.23836571e-06
Iter: 1415 loss: 1.24511189e-06
Iter: 1416 loss: 1.23825691e-06
Iter: 1417 loss: 1.23764482e-06
Iter: 1418 loss: 1.23744371e-06
Iter: 1419 loss: 1.23702057e-06
Iter: 1420 loss: 1.23641053e-06
Iter: 1421 loss: 1.23735936e-06
Iter: 1422 loss: 1.23614177e-06
Iter: 1423 loss: 1.23523762e-06
Iter: 1424 loss: 1.2351469e-06
Iter: 1425 loss: 1.23448876e-06
Iter: 1426 loss: 1.23332984e-06
Iter: 1427 loss: 1.24025883e-06
Iter: 1428 loss: 1.23315453e-06
Iter: 1429 loss: 1.2324216e-06
Iter: 1430 loss: 1.23242489e-06
Iter: 1431 loss: 1.23171571e-06
Iter: 1432 loss: 1.23057259e-06
Iter: 1433 loss: 1.23483414e-06
Iter: 1434 loss: 1.23022357e-06
Iter: 1435 loss: 1.22917072e-06
Iter: 1436 loss: 1.23245627e-06
Iter: 1437 loss: 1.22886286e-06
Iter: 1438 loss: 1.2280932e-06
Iter: 1439 loss: 1.22809013e-06
Iter: 1440 loss: 1.22740857e-06
Iter: 1441 loss: 1.22751749e-06
Iter: 1442 loss: 1.22727e-06
Iter: 1443 loss: 1.22662573e-06
Iter: 1444 loss: 1.23266091e-06
Iter: 1445 loss: 1.2265516e-06
Iter: 1446 loss: 1.22600704e-06
Iter: 1447 loss: 1.22890833e-06
Iter: 1448 loss: 1.22591359e-06
Iter: 1449 loss: 1.22509402e-06
Iter: 1450 loss: 1.2273124e-06
Iter: 1451 loss: 1.2250332e-06
Iter: 1452 loss: 1.22446318e-06
Iter: 1453 loss: 1.22404288e-06
Iter: 1454 loss: 1.22396159e-06
Iter: 1455 loss: 1.22296694e-06
Iter: 1456 loss: 1.22265919e-06
Iter: 1457 loss: 1.22214431e-06
Iter: 1458 loss: 1.22091433e-06
Iter: 1459 loss: 1.23084317e-06
Iter: 1460 loss: 1.22089591e-06
Iter: 1461 loss: 1.21999778e-06
Iter: 1462 loss: 1.21947e-06
Iter: 1463 loss: 1.21903167e-06
Iter: 1464 loss: 1.21783228e-06
Iter: 1465 loss: 1.23200675e-06
Iter: 1466 loss: 1.21784183e-06
Iter: 1467 loss: 1.21690459e-06
Iter: 1468 loss: 1.21649623e-06
Iter: 1469 loss: 1.21603932e-06
Iter: 1470 loss: 1.21528956e-06
Iter: 1471 loss: 1.21528376e-06
Iter: 1472 loss: 1.21501512e-06
Iter: 1473 loss: 1.214843e-06
Iter: 1474 loss: 1.21468429e-06
Iter: 1475 loss: 1.21422113e-06
Iter: 1476 loss: 1.21534072e-06
Iter: 1477 loss: 1.2136768e-06
Iter: 1478 loss: 1.21295011e-06
Iter: 1479 loss: 1.21665425e-06
Iter: 1480 loss: 1.21260621e-06
Iter: 1481 loss: 1.21193216e-06
Iter: 1482 loss: 1.21606763e-06
Iter: 1483 loss: 1.2118885e-06
Iter: 1484 loss: 1.21127118e-06
Iter: 1485 loss: 1.21051926e-06
Iter: 1486 loss: 1.2106234e-06
Iter: 1487 loss: 1.20960794e-06
Iter: 1488 loss: 1.21248695e-06
Iter: 1489 loss: 1.20944765e-06
Iter: 1490 loss: 1.20855316e-06
Iter: 1491 loss: 1.20999994e-06
Iter: 1492 loss: 1.20825393e-06
Iter: 1493 loss: 1.20720006e-06
Iter: 1494 loss: 1.20960021e-06
Iter: 1495 loss: 1.20688537e-06
Iter: 1496 loss: 1.2059561e-06
Iter: 1497 loss: 1.2089688e-06
Iter: 1498 loss: 1.2058714e-06
Iter: 1499 loss: 1.20471634e-06
Iter: 1500 loss: 1.20506752e-06
Iter: 1501 loss: 1.20409345e-06
Iter: 1502 loss: 1.20338052e-06
Iter: 1503 loss: 1.20337722e-06
Iter: 1504 loss: 1.20328332e-06
Iter: 1505 loss: 1.20313427e-06
Iter: 1506 loss: 1.20280572e-06
Iter: 1507 loss: 1.20210007e-06
Iter: 1508 loss: 1.20494315e-06
Iter: 1509 loss: 1.20170728e-06
Iter: 1510 loss: 1.20079335e-06
Iter: 1511 loss: 1.20828713e-06
Iter: 1512 loss: 1.2007215e-06
Iter: 1513 loss: 1.20034395e-06
Iter: 1514 loss: 1.20256982e-06
Iter: 1515 loss: 1.20023731e-06
Iter: 1516 loss: 1.19967876e-06
Iter: 1517 loss: 1.1990378e-06
Iter: 1518 loss: 1.19893411e-06
Iter: 1519 loss: 1.19802417e-06
Iter: 1520 loss: 1.19853235e-06
Iter: 1521 loss: 1.19750553e-06
Iter: 1522 loss: 1.1964562e-06
Iter: 1523 loss: 1.19778042e-06
Iter: 1524 loss: 1.19585729e-06
Iter: 1525 loss: 1.19471838e-06
Iter: 1526 loss: 1.20260393e-06
Iter: 1527 loss: 1.19468393e-06
Iter: 1528 loss: 1.1936628e-06
Iter: 1529 loss: 1.19450783e-06
Iter: 1530 loss: 1.19326216e-06
Iter: 1531 loss: 1.1921353e-06
Iter: 1532 loss: 1.19364518e-06
Iter: 1533 loss: 1.19144067e-06
Iter: 1534 loss: 1.19052265e-06
Iter: 1535 loss: 1.19091101e-06
Iter: 1536 loss: 1.18960577e-06
Iter: 1537 loss: 1.19063725e-06
Iter: 1538 loss: 1.18898993e-06
Iter: 1539 loss: 1.18842377e-06
Iter: 1540 loss: 1.18752757e-06
Iter: 1541 loss: 1.18754883e-06
Iter: 1542 loss: 1.1870884e-06
Iter: 1543 loss: 1.18805269e-06
Iter: 1544 loss: 1.18684829e-06
Iter: 1545 loss: 1.18625326e-06
Iter: 1546 loss: 1.18724597e-06
Iter: 1547 loss: 1.18611899e-06
Iter: 1548 loss: 1.1850899e-06
Iter: 1549 loss: 1.18558728e-06
Iter: 1550 loss: 1.18461253e-06
Iter: 1551 loss: 1.18385833e-06
Iter: 1552 loss: 1.18601656e-06
Iter: 1553 loss: 1.18345201e-06
Iter: 1554 loss: 1.18268849e-06
Iter: 1555 loss: 1.18272828e-06
Iter: 1556 loss: 1.18196226e-06
Iter: 1557 loss: 1.18085677e-06
Iter: 1558 loss: 1.18361424e-06
Iter: 1559 loss: 1.18055209e-06
Iter: 1560 loss: 1.17944546e-06
Iter: 1561 loss: 1.18323703e-06
Iter: 1562 loss: 1.17907325e-06
Iter: 1563 loss: 1.1782563e-06
Iter: 1564 loss: 1.18364665e-06
Iter: 1565 loss: 1.17805212e-06
Iter: 1566 loss: 1.17708646e-06
Iter: 1567 loss: 1.17625473e-06
Iter: 1568 loss: 1.17599279e-06
Iter: 1569 loss: 1.17710806e-06
Iter: 1570 loss: 1.17555896e-06
Iter: 1571 loss: 1.17500304e-06
Iter: 1572 loss: 1.17496666e-06
Iter: 1573 loss: 1.17458308e-06
Iter: 1574 loss: 1.17422019e-06
Iter: 1575 loss: 1.17389163e-06
Iter: 1576 loss: 1.17378079e-06
Iter: 1577 loss: 1.17285106e-06
Iter: 1578 loss: 1.17445268e-06
Iter: 1579 loss: 1.17260117e-06
Iter: 1580 loss: 1.17170202e-06
Iter: 1581 loss: 1.17872185e-06
Iter: 1582 loss: 1.17176944e-06
Iter: 1583 loss: 1.17136744e-06
Iter: 1584 loss: 1.17080458e-06
Iter: 1585 loss: 1.17074467e-06
Iter: 1586 loss: 1.16996023e-06
Iter: 1587 loss: 1.16917204e-06
Iter: 1588 loss: 1.16894489e-06
Iter: 1589 loss: 1.16766023e-06
Iter: 1590 loss: 1.17658476e-06
Iter: 1591 loss: 1.16760577e-06
Iter: 1592 loss: 1.16664091e-06
Iter: 1593 loss: 1.16712158e-06
Iter: 1594 loss: 1.1659713e-06
Iter: 1595 loss: 1.16479509e-06
Iter: 1596 loss: 1.17125933e-06
Iter: 1597 loss: 1.16465822e-06
Iter: 1598 loss: 1.16375497e-06
Iter: 1599 loss: 1.16433057e-06
Iter: 1600 loss: 1.16319575e-06
Iter: 1601 loss: 1.16311662e-06
Iter: 1602 loss: 1.16276715e-06
Iter: 1603 loss: 1.16234014e-06
Iter: 1604 loss: 1.16311207e-06
Iter: 1605 loss: 1.16210128e-06
Iter: 1606 loss: 1.16175784e-06
Iter: 1607 loss: 1.16120214e-06
Iter: 1608 loss: 1.16118258e-06
Iter: 1609 loss: 1.16040587e-06
Iter: 1610 loss: 1.16288891e-06
Iter: 1611 loss: 1.16020237e-06
Iter: 1612 loss: 1.159607e-06
Iter: 1613 loss: 1.16690182e-06
Iter: 1614 loss: 1.15966657e-06
Iter: 1615 loss: 1.1592814e-06
Iter: 1616 loss: 1.15891567e-06
Iter: 1617 loss: 1.15878834e-06
Iter: 1618 loss: 1.15807029e-06
Iter: 1619 loss: 1.15799162e-06
Iter: 1620 loss: 1.15742614e-06
Iter: 1621 loss: 1.15664511e-06
Iter: 1622 loss: 1.16314254e-06
Iter: 1623 loss: 1.15664875e-06
Iter: 1624 loss: 1.15582429e-06
Iter: 1625 loss: 1.15565217e-06
Iter: 1626 loss: 1.1554846e-06
Iter: 1627 loss: 1.15450428e-06
Iter: 1628 loss: 1.15917521e-06
Iter: 1629 loss: 1.15433295e-06
Iter: 1630 loss: 1.1536905e-06
Iter: 1631 loss: 1.15343528e-06
Iter: 1632 loss: 1.15311468e-06
Iter: 1633 loss: 1.15233593e-06
Iter: 1634 loss: 1.15232478e-06
Iter: 1635 loss: 1.15169871e-06
Iter: 1636 loss: 1.15174021e-06
Iter: 1637 loss: 1.15147498e-06
Iter: 1638 loss: 1.15093007e-06
Iter: 1639 loss: 1.15444277e-06
Iter: 1640 loss: 1.15073817e-06
Iter: 1641 loss: 1.14998238e-06
Iter: 1642 loss: 1.15354146e-06
Iter: 1643 loss: 1.14972408e-06
Iter: 1644 loss: 1.14927559e-06
Iter: 1645 loss: 1.15417595e-06
Iter: 1646 loss: 1.14929321e-06
Iter: 1647 loss: 1.14885972e-06
Iter: 1648 loss: 1.14818931e-06
Iter: 1649 loss: 1.14813258e-06
Iter: 1650 loss: 1.1474242e-06
Iter: 1651 loss: 1.14936597e-06
Iter: 1652 loss: 1.14719603e-06
Iter: 1653 loss: 1.14656154e-06
Iter: 1654 loss: 1.14777e-06
Iter: 1655 loss: 1.14643831e-06
Iter: 1656 loss: 1.14551358e-06
Iter: 1657 loss: 1.14528802e-06
Iter: 1658 loss: 1.1447645e-06
Iter: 1659 loss: 1.14375018e-06
Iter: 1660 loss: 1.15464832e-06
Iter: 1661 loss: 1.14373313e-06
Iter: 1662 loss: 1.14298018e-06
Iter: 1663 loss: 1.14272473e-06
Iter: 1664 loss: 1.14222871e-06
Iter: 1665 loss: 1.14144245e-06
Iter: 1666 loss: 1.15100454e-06
Iter: 1667 loss: 1.1414154e-06
Iter: 1668 loss: 1.14153136e-06
Iter: 1669 loss: 1.14110435e-06
Iter: 1670 loss: 1.14097691e-06
Iter: 1671 loss: 1.14051136e-06
Iter: 1672 loss: 1.14089187e-06
Iter: 1673 loss: 1.14010277e-06
Iter: 1674 loss: 1.13949545e-06
Iter: 1675 loss: 1.14576051e-06
Iter: 1676 loss: 1.13948192e-06
Iter: 1677 loss: 1.13905662e-06
Iter: 1678 loss: 1.14444049e-06
Iter: 1679 loss: 1.13910107e-06
Iter: 1680 loss: 1.13862041e-06
Iter: 1681 loss: 1.13827377e-06
Iter: 1682 loss: 1.13831061e-06
Iter: 1683 loss: 1.13773535e-06
Iter: 1684 loss: 1.1386594e-06
Iter: 1685 loss: 1.13752026e-06
Iter: 1686 loss: 1.13694546e-06
Iter: 1687 loss: 1.13678834e-06
Iter: 1688 loss: 1.13637134e-06
Iter: 1689 loss: 1.13530382e-06
Iter: 1690 loss: 1.13787928e-06
Iter: 1691 loss: 1.13502551e-06
Iter: 1692 loss: 1.13424892e-06
Iter: 1693 loss: 1.13712008e-06
Iter: 1694 loss: 1.13406168e-06
Iter: 1695 loss: 1.13321812e-06
Iter: 1696 loss: 1.13475312e-06
Iter: 1697 loss: 1.13272085e-06
Iter: 1698 loss: 1.13180431e-06
Iter: 1699 loss: 1.13497072e-06
Iter: 1700 loss: 1.13160581e-06
Iter: 1701 loss: 1.13245437e-06
Iter: 1702 loss: 1.13141289e-06
Iter: 1703 loss: 1.13112674e-06
Iter: 1704 loss: 1.13056171e-06
Iter: 1705 loss: 1.1316007e-06
Iter: 1706 loss: 1.13019155e-06
Iter: 1707 loss: 1.1293987e-06
Iter: 1708 loss: 1.13369197e-06
Iter: 1709 loss: 1.12920088e-06
Iter: 1710 loss: 1.12857947e-06
Iter: 1711 loss: 1.13235137e-06
Iter: 1712 loss: 1.12849932e-06
Iter: 1713 loss: 1.12779276e-06
Iter: 1714 loss: 1.12820885e-06
Iter: 1715 loss: 1.12728969e-06
Iter: 1716 loss: 1.12657153e-06
Iter: 1717 loss: 1.1274567e-06
Iter: 1718 loss: 1.12628663e-06
Iter: 1719 loss: 1.12557257e-06
Iter: 1720 loss: 1.12733915e-06
Iter: 1721 loss: 1.1253079e-06
Iter: 1722 loss: 1.12457644e-06
Iter: 1723 loss: 1.12642169e-06
Iter: 1724 loss: 1.12433486e-06
Iter: 1725 loss: 1.12369366e-06
Iter: 1726 loss: 1.12278212e-06
Iter: 1727 loss: 1.12277678e-06
Iter: 1728 loss: 1.12153407e-06
Iter: 1729 loss: 1.13466547e-06
Iter: 1730 loss: 1.12140015e-06
Iter: 1731 loss: 1.12090925e-06
Iter: 1732 loss: 1.12360465e-06
Iter: 1733 loss: 1.12088139e-06
Iter: 1734 loss: 1.12071518e-06
Iter: 1735 loss: 1.12059342e-06
Iter: 1736 loss: 1.1202435e-06
Iter: 1737 loss: 1.1196214e-06
Iter: 1738 loss: 1.12777775e-06
Iter: 1739 loss: 1.11958968e-06
Iter: 1740 loss: 1.11895611e-06
Iter: 1741 loss: 1.11951908e-06
Iter: 1742 loss: 1.11874306e-06
Iter: 1743 loss: 1.11819122e-06
Iter: 1744 loss: 1.12076054e-06
Iter: 1745 loss: 1.11809095e-06
Iter: 1746 loss: 1.11748432e-06
Iter: 1747 loss: 1.11897236e-06
Iter: 1748 loss: 1.11729457e-06
Iter: 1749 loss: 1.11680072e-06
Iter: 1750 loss: 1.11706504e-06
Iter: 1751 loss: 1.11641134e-06
Iter: 1752 loss: 1.11584166e-06
Iter: 1753 loss: 1.11628492e-06
Iter: 1754 loss: 1.11542136e-06
Iter: 1755 loss: 1.11458451e-06
Iter: 1756 loss: 1.11807867e-06
Iter: 1757 loss: 1.11451e-06
Iter: 1758 loss: 1.11382076e-06
Iter: 1759 loss: 1.11358065e-06
Iter: 1760 loss: 1.11322493e-06
Iter: 1761 loss: 1.11229872e-06
Iter: 1762 loss: 1.11852717e-06
Iter: 1763 loss: 1.11220993e-06
Iter: 1764 loss: 1.11146232e-06
Iter: 1765 loss: 1.11170948e-06
Iter: 1766 loss: 1.11093209e-06
Iter: 1767 loss: 1.11138638e-06
Iter: 1768 loss: 1.11056443e-06
Iter: 1769 loss: 1.11030727e-06
Iter: 1770 loss: 1.11008899e-06
Iter: 1771 loss: 1.11005238e-06
Iter: 1772 loss: 1.10975884e-06
Iter: 1773 loss: 1.10940778e-06
Iter: 1774 loss: 1.10936367e-06
Iter: 1775 loss: 1.10862038e-06
Iter: 1776 loss: 1.10838675e-06
Iter: 1777 loss: 1.10797282e-06
Iter: 1778 loss: 1.10748101e-06
Iter: 1779 loss: 1.11491079e-06
Iter: 1780 loss: 1.10747965e-06
Iter: 1781 loss: 1.10711221e-06
Iter: 1782 loss: 1.10741917e-06
Iter: 1783 loss: 1.10684368e-06
Iter: 1784 loss: 1.10625513e-06
Iter: 1785 loss: 1.10638427e-06
Iter: 1786 loss: 1.10595965e-06
Iter: 1787 loss: 1.10532096e-06
Iter: 1788 loss: 1.10903977e-06
Iter: 1789 loss: 1.10522387e-06
Iter: 1790 loss: 1.1047739e-06
Iter: 1791 loss: 1.10443636e-06
Iter: 1792 loss: 1.10425981e-06
Iter: 1793 loss: 1.10351857e-06
Iter: 1794 loss: 1.10490066e-06
Iter: 1795 loss: 1.10329586e-06
Iter: 1796 loss: 1.10242854e-06
Iter: 1797 loss: 1.1040687e-06
Iter: 1798 loss: 1.10226097e-06
Iter: 1799 loss: 1.10150631e-06
Iter: 1800 loss: 1.10467397e-06
Iter: 1801 loss: 1.10148153e-06
Iter: 1802 loss: 1.10198209e-06
Iter: 1803 loss: 1.10128985e-06
Iter: 1804 loss: 1.10109465e-06
Iter: 1805 loss: 1.10065548e-06
Iter: 1806 loss: 1.10117435e-06
Iter: 1807 loss: 1.10029703e-06
Iter: 1808 loss: 1.09987286e-06
Iter: 1809 loss: 1.10631629e-06
Iter: 1810 loss: 1.09974371e-06
Iter: 1811 loss: 1.09936423e-06
Iter: 1812 loss: 1.09871962e-06
Iter: 1813 loss: 1.11555198e-06
Iter: 1814 loss: 1.09866562e-06
Iter: 1815 loss: 1.0979245e-06
Iter: 1816 loss: 1.09949201e-06
Iter: 1817 loss: 1.09765915e-06
Iter: 1818 loss: 1.09710675e-06
Iter: 1819 loss: 1.10234259e-06
Iter: 1820 loss: 1.09697714e-06
Iter: 1821 loss: 1.09644247e-06
Iter: 1822 loss: 1.09512575e-06
Iter: 1823 loss: 1.11383508e-06
Iter: 1824 loss: 1.09504731e-06
Iter: 1825 loss: 1.0941543e-06
Iter: 1826 loss: 1.09424627e-06
Iter: 1827 loss: 1.09374275e-06
Iter: 1828 loss: 1.09445909e-06
Iter: 1829 loss: 1.09344251e-06
Iter: 1830 loss: 1.09295513e-06
Iter: 1831 loss: 1.09203347e-06
Iter: 1832 loss: 1.09202335e-06
Iter: 1833 loss: 1.09113068e-06
Iter: 1834 loss: 1.09833991e-06
Iter: 1835 loss: 1.09091116e-06
Iter: 1836 loss: 1.0907554e-06
Iter: 1837 loss: 1.09061693e-06
Iter: 1838 loss: 1.0900493e-06
Iter: 1839 loss: 1.09072903e-06
Iter: 1840 loss: 1.0897669e-06
Iter: 1841 loss: 1.08948598e-06
Iter: 1842 loss: 1.08854306e-06
Iter: 1843 loss: 1.09042207e-06
Iter: 1844 loss: 1.08796166e-06
Iter: 1845 loss: 1.08717097e-06
Iter: 1846 loss: 1.08722065e-06
Iter: 1847 loss: 1.08661925e-06
Iter: 1848 loss: 1.09187329e-06
Iter: 1849 loss: 1.08658151e-06
Iter: 1850 loss: 1.08603649e-06
Iter: 1851 loss: 1.08538916e-06
Iter: 1852 loss: 1.08531094e-06
Iter: 1853 loss: 1.08446136e-06
Iter: 1854 loss: 1.09206542e-06
Iter: 1855 loss: 1.0845439e-06
Iter: 1856 loss: 1.08402446e-06
Iter: 1857 loss: 1.08492816e-06
Iter: 1858 loss: 1.0839608e-06
Iter: 1859 loss: 1.0834583e-06
Iter: 1860 loss: 1.08251049e-06
Iter: 1861 loss: 1.09298173e-06
Iter: 1862 loss: 1.08240079e-06
Iter: 1863 loss: 1.08180723e-06
Iter: 1864 loss: 1.08186032e-06
Iter: 1865 loss: 1.08129666e-06
Iter: 1866 loss: 1.08139261e-06
Iter: 1867 loss: 1.08087966e-06
Iter: 1868 loss: 1.08010863e-06
Iter: 1869 loss: 1.08452878e-06
Iter: 1870 loss: 1.07997937e-06
Iter: 1871 loss: 1.07982987e-06
Iter: 1872 loss: 1.07971096e-06
Iter: 1873 loss: 1.07955498e-06
Iter: 1874 loss: 1.07907817e-06
Iter: 1875 loss: 1.08278255e-06
Iter: 1876 loss: 1.078987e-06
Iter: 1877 loss: 1.07859955e-06
Iter: 1878 loss: 1.07794074e-06
Iter: 1879 loss: 1.07781148e-06
Iter: 1880 loss: 1.07741516e-06
Iter: 1881 loss: 1.07737628e-06
Iter: 1882 loss: 1.07703227e-06
Iter: 1883 loss: 1.07732535e-06
Iter: 1884 loss: 1.07677897e-06
Iter: 1885 loss: 1.0762908e-06
Iter: 1886 loss: 1.0763323e-06
Iter: 1887 loss: 1.07575204e-06
Iter: 1888 loss: 1.07497226e-06
Iter: 1889 loss: 1.07765163e-06
Iter: 1890 loss: 1.0747699e-06
Iter: 1891 loss: 1.07414417e-06
Iter: 1892 loss: 1.07566416e-06
Iter: 1893 loss: 1.07390576e-06
Iter: 1894 loss: 1.07323672e-06
Iter: 1895 loss: 1.07275628e-06
Iter: 1896 loss: 1.07244739e-06
Iter: 1897 loss: 1.07186747e-06
Iter: 1898 loss: 1.08140443e-06
Iter: 1899 loss: 1.07189101e-06
Iter: 1900 loss: 1.07135406e-06
Iter: 1901 loss: 1.07149901e-06
Iter: 1902 loss: 1.0710221e-06
Iter: 1903 loss: 1.07124765e-06
Iter: 1904 loss: 1.07075243e-06
Iter: 1905 loss: 1.07059032e-06
Iter: 1906 loss: 1.07012693e-06
Iter: 1907 loss: 1.07525784e-06
Iter: 1908 loss: 1.07008907e-06
Iter: 1909 loss: 1.06953246e-06
Iter: 1910 loss: 1.06962329e-06
Iter: 1911 loss: 1.06918515e-06
Iter: 1912 loss: 1.06842515e-06
Iter: 1913 loss: 1.07314395e-06
Iter: 1914 loss: 1.06839389e-06
Iter: 1915 loss: 1.067825e-06
Iter: 1916 loss: 1.07309324e-06
Iter: 1917 loss: 1.06792697e-06
Iter: 1918 loss: 1.06757386e-06
Iter: 1919 loss: 1.0668947e-06
Iter: 1920 loss: 1.07292453e-06
Iter: 1921 loss: 1.06680864e-06
Iter: 1922 loss: 1.06592552e-06
Iter: 1923 loss: 1.07447136e-06
Iter: 1924 loss: 1.06597543e-06
Iter: 1925 loss: 1.06553807e-06
Iter: 1926 loss: 1.06610082e-06
Iter: 1927 loss: 1.06534549e-06
Iter: 1928 loss: 1.06464597e-06
Iter: 1929 loss: 1.06415791e-06
Iter: 1930 loss: 1.06395828e-06
Iter: 1931 loss: 1.06333414e-06
Iter: 1932 loss: 1.06518974e-06
Iter: 1933 loss: 1.06307812e-06
Iter: 1934 loss: 1.06242396e-06
Iter: 1935 loss: 1.06357015e-06
Iter: 1936 loss: 1.0620106e-06
Iter: 1937 loss: 1.06269408e-06
Iter: 1938 loss: 1.06186189e-06
Iter: 1939 loss: 1.06173866e-06
Iter: 1940 loss: 1.06148502e-06
Iter: 1941 loss: 1.06152311e-06
Iter: 1942 loss: 1.06120922e-06
Iter: 1943 loss: 1.06048174e-06
Iter: 1944 loss: 1.06439109e-06
Iter: 1945 loss: 1.06025072e-06
Iter: 1946 loss: 1.05970457e-06
Iter: 1947 loss: 1.06568882e-06
Iter: 1948 loss: 1.05969661e-06
Iter: 1949 loss: 1.05932986e-06
Iter: 1950 loss: 1.06054654e-06
Iter: 1951 loss: 1.05916752e-06
Iter: 1952 loss: 1.05871925e-06
Iter: 1953 loss: 1.05951131e-06
Iter: 1954 loss: 1.058512e-06
Iter: 1955 loss: 1.05801701e-06
Iter: 1956 loss: 1.05959725e-06
Iter: 1957 loss: 1.05782101e-06
Iter: 1958 loss: 1.05761171e-06
Iter: 1959 loss: 1.05752906e-06
Iter: 1960 loss: 1.05725849e-06
Iter: 1961 loss: 1.05674417e-06
Iter: 1962 loss: 1.05781601e-06
Iter: 1963 loss: 1.05644131e-06
Iter: 1964 loss: 1.0559977e-06
Iter: 1965 loss: 1.05750064e-06
Iter: 1966 loss: 1.05580739e-06
Iter: 1967 loss: 1.05542961e-06
Iter: 1968 loss: 1.05454751e-06
Iter: 1969 loss: 1.0742732e-06
Iter: 1970 loss: 1.05459583e-06
Iter: 1971 loss: 1.05421145e-06
Iter: 1972 loss: 1.05402671e-06
Iter: 1973 loss: 1.05394474e-06
Iter: 1974 loss: 1.05384879e-06
Iter: 1975 loss: 1.05365177e-06
Iter: 1976 loss: 1.05343338e-06
Iter: 1977 loss: 1.05298477e-06
Iter: 1978 loss: 1.05296988e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.8
+ date
Thu Oct 22 10:41:15 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/500_500_500_500_1 --function f1 --psi 2 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c400158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c412840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c45ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c44de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c33c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c33ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c263730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c2a87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c2a8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c2bbe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c1d2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c1ec8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c1ec400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c21f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90eacd08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90eacc4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90eace4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90eac9f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90eac0b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90eac0be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90eac4eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f910c2c7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c4318730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c432b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c432b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c42bc620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c42749d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c427e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c4366488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c4366d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c422a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c41b7158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c41b71e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c41cab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c42498c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90c4263ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.08472824
test_loss: 0.08646949
train_loss: 0.058538217
test_loss: 0.056643225
train_loss: 0.042075515
test_loss: 0.038269233
train_loss: 0.027337613
test_loss: 0.030679362
train_loss: 0.023590084
test_loss: 0.027996883
train_loss: 0.02330134
test_loss: 0.024150869
train_loss: 0.018417103
test_loss: 0.021148419
train_loss: 0.017416198
test_loss: 0.020115484
train_loss: 0.016917657
test_loss: 0.01862814
train_loss: 0.015744403
test_loss: 0.018543523
train_loss: 0.013912776
test_loss: 0.017238121
train_loss: 0.015221657
test_loss: 0.017753476
train_loss: 0.014969069
test_loss: 0.01717276
train_loss: 0.01480104
test_loss: 0.015703145
train_loss: 0.013706899
test_loss: 0.015813014
train_loss: 0.013336059
test_loss: 0.014797024
train_loss: 0.012936465
test_loss: 0.015375894
train_loss: 0.0117299
test_loss: 0.0155260535
train_loss: 0.011729537
test_loss: 0.015731774
train_loss: 0.01627545
test_loss: 0.019380521
train_loss: 0.012843741
test_loss: 0.015423443
train_loss: 0.011993091
test_loss: 0.014633102
train_loss: 0.01201243
test_loss: 0.015222987
train_loss: 0.012350489
test_loss: 0.015228934
train_loss: 0.010564733
test_loss: 0.014025244
train_loss: 0.011187207
test_loss: 0.014087397
train_loss: 0.010218843
test_loss: 0.013489739
train_loss: 0.011371805
test_loss: 0.013299772
train_loss: 0.010781642
test_loss: 0.013798263
train_loss: 0.010613787
test_loss: 0.014366119
train_loss: 0.010601854
test_loss: 0.012627608
train_loss: 0.010135172
test_loss: 0.01276314
train_loss: 0.0087399185
test_loss: 0.012816101
train_loss: 0.0105745215
test_loss: 0.012990084
train_loss: 0.00946561
test_loss: 0.012987204
train_loss: 0.00946891
test_loss: 0.012391013
train_loss: 0.010144997
test_loss: 0.012919003
train_loss: 0.0102968365
test_loss: 0.013080905
train_loss: 0.011438464
test_loss: 0.012925562
train_loss: 0.010114215
test_loss: 0.012706352
train_loss: 0.0105947405
test_loss: 0.013019632
train_loss: 0.008852255
test_loss: 0.012863674
train_loss: 0.009842234
test_loss: 0.011944717
train_loss: 0.010310422
test_loss: 0.0127814505
train_loss: 0.0095330775
test_loss: 0.012305098
train_loss: 0.008776622
test_loss: 0.012397151
train_loss: 0.0094447285
test_loss: 0.011738273
train_loss: 0.009605772
test_loss: 0.012546528
train_loss: 0.010280846
test_loss: 0.012511013
train_loss: 0.0107944
test_loss: 0.012597667
train_loss: 0.009721436
test_loss: 0.012251159
train_loss: 0.008804688
test_loss: 0.012291701
train_loss: 0.008533709
test_loss: 0.01212277
train_loss: 0.009210158
test_loss: 0.012022676
train_loss: 0.008559046
test_loss: 0.012012327
train_loss: 0.009465328
test_loss: 0.011960456
train_loss: 0.008955209
test_loss: 0.011828815
train_loss: 0.007821943
test_loss: 0.011369395
train_loss: 0.008486467
test_loss: 0.012877134
train_loss: 0.008121359
test_loss: 0.011823807
train_loss: 0.008286727
test_loss: 0.012072225
train_loss: 0.008933706
test_loss: 0.01203102
train_loss: 0.007943009
test_loss: 0.012430021
train_loss: 0.007922217
test_loss: 0.011828877
train_loss: 0.008761812
test_loss: 0.011424574
train_loss: 0.008347537
test_loss: 0.0115387
train_loss: 0.008335841
test_loss: 0.011751311
train_loss: 0.008819531
test_loss: 0.011654616
train_loss: 0.009448605
test_loss: 0.012560431
train_loss: 0.009175934
test_loss: 0.011310519
train_loss: 0.009354589
test_loss: 0.012174435
train_loss: 0.008030886
test_loss: 0.010914741
train_loss: 0.007411926
test_loss: 0.011464648
train_loss: 0.00854248
test_loss: 0.01230995
train_loss: 0.008469419
test_loss: 0.010878045
train_loss: 0.008568758
test_loss: 0.011225452
train_loss: 0.007738469
test_loss: 0.011521249
train_loss: 0.0075445804
test_loss: 0.011287319
train_loss: 0.008935461
test_loss: 0.01206591
train_loss: 0.008163203
test_loss: 0.010859718
train_loss: 0.0074382015
test_loss: 0.010900899
train_loss: 0.0082641905
test_loss: 0.011382608
train_loss: 0.0084165875
test_loss: 0.01134896
train_loss: 0.008913589
test_loss: 0.011747745
train_loss: 0.008686267
test_loss: 0.0111643085
train_loss: 0.008932408
test_loss: 0.01117202
train_loss: 0.007998715
test_loss: 0.011605009
train_loss: 0.010274382
test_loss: 0.01198952
train_loss: 0.008108953
test_loss: 0.0108706765
train_loss: 0.007850088
test_loss: 0.011357574
train_loss: 0.0075603323
test_loss: 0.010681153
train_loss: 0.009456474
test_loss: 0.011283751
train_loss: 0.007954677
test_loss: 0.010931934
train_loss: 0.007501374
test_loss: 0.011140875
train_loss: 0.0072505865
test_loss: 0.011214838
train_loss: 0.0074716317
test_loss: 0.010591709
train_loss: 0.007852111
test_loss: 0.010720146
train_loss: 0.008164847
test_loss: 0.011331466
train_loss: 0.0070130243
test_loss: 0.010057832
train_loss: 0.008239204
test_loss: 0.011136555
train_loss: 0.007748465
test_loss: 0.011315367
train_loss: 0.008020582
test_loss: 0.010693129
train_loss: 0.008069372
test_loss: 0.011463769
train_loss: 0.0070789093
test_loss: 0.010366675
train_loss: 0.008775838
test_loss: 0.011068208
train_loss: 0.008889426
test_loss: 0.011321241
train_loss: 0.007432851
test_loss: 0.010932703
train_loss: 0.0077870903
test_loss: 0.011013813
train_loss: 0.008651723
test_loss: 0.011458757
train_loss: 0.008320827
test_loss: 0.011007306
train_loss: 0.008841721
test_loss: 0.01118039
train_loss: 0.007196373
test_loss: 0.010752861
train_loss: 0.0067948485
test_loss: 0.010663393
train_loss: 0.007141066
test_loss: 0.010650617
train_loss: 0.007968035
test_loss: 0.010417286
train_loss: 0.00806226
test_loss: 0.010712732
train_loss: 0.008739488
test_loss: 0.010844198
train_loss: 0.0073589385
test_loss: 0.011051005
train_loss: 0.0071545187
test_loss: 0.011366348
train_loss: 0.0074724285
test_loss: 0.010373558
train_loss: 0.00755565
test_loss: 0.010941785
train_loss: 0.00844352
test_loss: 0.010759205
train_loss: 0.007572601
test_loss: 0.010389862
train_loss: 0.0069918768
test_loss: 0.010227553
train_loss: 0.0062747933
test_loss: 0.010131468
train_loss: 0.007974954
test_loss: 0.01050573
train_loss: 0.0073392433
test_loss: 0.0107631795
train_loss: 0.0069440855
test_loss: 0.010635576
train_loss: 0.008062152
test_loss: 0.010246926
train_loss: 0.0075533087
test_loss: 0.01058427
train_loss: 0.008208506
test_loss: 0.011748417
train_loss: 0.008280007
test_loss: 0.010849509
train_loss: 0.0082553625
test_loss: 0.011193187
train_loss: 0.006919988
test_loss: 0.010315986
train_loss: 0.006626594
test_loss: 0.0101582315
train_loss: 0.0072842324
test_loss: 0.010110154
train_loss: 0.007379229
test_loss: 0.010491757
train_loss: 0.00800671
test_loss: 0.010859727
train_loss: 0.0067981714
test_loss: 0.010597795
train_loss: 0.007954855
test_loss: 0.010314939
train_loss: 0.0063894102
test_loss: 0.010128191
train_loss: 0.0071956036
test_loss: 0.010264423
train_loss: 0.007507203
test_loss: 0.009804973
train_loss: 0.007212181
test_loss: 0.010384744
train_loss: 0.0071862414
test_loss: 0.010115991
train_loss: 0.0076879947
test_loss: 0.011014429
train_loss: 0.0071190353
test_loss: 0.010515542
train_loss: 0.0076908385
test_loss: 0.010653647
train_loss: 0.0071051223
test_loss: 0.0105038015
train_loss: 0.007717351
test_loss: 0.010076281
train_loss: 0.006651886
test_loss: 0.01020236
train_loss: 0.0071462863
test_loss: 0.010399224
train_loss: 0.007223891
test_loss: 0.010510592
train_loss: 0.007991747
test_loss: 0.011158551
train_loss: 0.007074407
test_loss: 0.010133575
train_loss: 0.0061946157
test_loss: 0.0096718855
train_loss: 0.0067246472
test_loss: 0.010132444
train_loss: 0.0069853533
test_loss: 0.010357463
train_loss: 0.007964348
test_loss: 0.010804515
train_loss: 0.0071218275
test_loss: 0.010332303
train_loss: 0.00641458
test_loss: 0.010465026
train_loss: 0.0071491287
test_loss: 0.010243577
train_loss: 0.006302693
test_loss: 0.010234663
train_loss: 0.00669754
test_loss: 0.009799401
train_loss: 0.0065535707
test_loss: 0.009863225
train_loss: 0.0065105823
test_loss: 0.010340224
train_loss: 0.0068986374
test_loss: 0.010211028
train_loss: 0.0062731104
test_loss: 0.010006622
train_loss: 0.006687984
test_loss: 0.010138781
train_loss: 0.00632743
test_loss: 0.010422515
train_loss: 0.006928035
test_loss: 0.010051043
train_loss: 0.0061098873
test_loss: 0.009505661
train_loss: 0.006624453
test_loss: 0.009677106
train_loss: 0.006546951
test_loss: 0.010090657
train_loss: 0.007044887
test_loss: 0.010295259
train_loss: 0.006410002
test_loss: 0.009706432
train_loss: 0.0063437014
test_loss: 0.009700355
train_loss: 0.0068304255
test_loss: 0.0100109605
train_loss: 0.006709492
test_loss: 0.009990931
train_loss: 0.0060142837
test_loss: 0.009663175
train_loss: 0.00736264
test_loss: 0.009557154
train_loss: 0.0069361534
test_loss: 0.0096145775
train_loss: 0.0067588096
test_loss: 0.009960439
train_loss: 0.006095921
test_loss: 0.009923304
train_loss: 0.0059704604
test_loss: 0.010409984
train_loss: 0.0061006313
test_loss: 0.009405203
train_loss: 0.0059053623
test_loss: 0.009523786
train_loss: 0.006057079
test_loss: 0.009575061
train_loss: 0.006555466
test_loss: 0.009939947
train_loss: 0.0073932777
test_loss: 0.009834362
train_loss: 0.0068491236
test_loss: 0.010323894
train_loss: 0.008376047
test_loss: 0.010030817
train_loss: 0.0062835775
test_loss: 0.009943944
train_loss: 0.0066208746
test_loss: 0.00917999
train_loss: 0.0065670824
test_loss: 0.009663623
train_loss: 0.006124153
test_loss: 0.009564404
train_loss: 0.0066329977
test_loss: 0.009501567
train_loss: 0.0069181747
test_loss: 0.010525577
train_loss: 0.0069091525
test_loss: 0.010664809
train_loss: 0.0066510597
test_loss: 0.010127372
train_loss: 0.0067115882
test_loss: 0.009974924
train_loss: 0.006937676
test_loss: 0.010045902
train_loss: 0.0074095204
test_loss: 0.010420407
train_loss: 0.0065558376
test_loss: 0.009822744
train_loss: 0.0068403566
test_loss: 0.010412297
train_loss: 0.0064044595
test_loss: 0.009821457
train_loss: 0.00622009
test_loss: 0.010116081
train_loss: 0.006063928
test_loss: 0.009536229
train_loss: 0.0078012682
test_loss: 0.01027334
train_loss: 0.005984293
test_loss: 0.009497551
train_loss: 0.0062117195
test_loss: 0.0096523585
train_loss: 0.0065212864
test_loss: 0.009902981
train_loss: 0.007144745
test_loss: 0.010110388
train_loss: 0.007281953
test_loss: 0.010714471
train_loss: 0.006257929
test_loss: 0.010293791
train_loss: 0.0060031167
test_loss: 0.009500129
train_loss: 0.0067865187
test_loss: 0.010042289
train_loss: 0.0063617798
test_loss: 0.009831367
train_loss: 0.0066549336
test_loss: 0.009599082
train_loss: 0.007240752
test_loss: 0.00994442
train_loss: 0.00604482
test_loss: 0.009377999
train_loss: 0.0063636787
test_loss: 0.010196834
train_loss: 0.006874261
test_loss: 0.009858965
train_loss: 0.006635423
test_loss: 0.011329478
train_loss: 0.006951399
test_loss: 0.009938146
train_loss: 0.006152259
test_loss: 0.00999479
train_loss: 0.006297621
test_loss: 0.009988602
train_loss: 0.0066019907
test_loss: 0.009947004
train_loss: 0.005732204
test_loss: 0.009528636
train_loss: 0.0064070476
test_loss: 0.00944204
train_loss: 0.0060375095
test_loss: 0.009842064
train_loss: 0.0052199075
test_loss: 0.00942594
train_loss: 0.0057422565
test_loss: 0.009876164
train_loss: 0.006845083
test_loss: 0.009868029
train_loss: 0.0061132275
test_loss: 0.009489428
train_loss: 0.0055433502
test_loss: 0.009713557
train_loss: 0.0058562076
test_loss: 0.00986665
train_loss: 0.0060399612
test_loss: 0.0100084245
train_loss: 0.006050459
test_loss: 0.0094535295
train_loss: 0.007222992
test_loss: 0.009673481
train_loss: 0.0059337667
test_loss: 0.009292697
train_loss: 0.006713108
test_loss: 0.0105393985
train_loss: 0.006656884
test_loss: 0.0099468995
train_loss: 0.006371389
test_loss: 0.00948713
train_loss: 0.0057477183
test_loss: 0.009887731
train_loss: 0.0061917957
test_loss: 0.009684586
train_loss: 0.0064844256
test_loss: 0.009939842
train_loss: 0.005815913
test_loss: 0.009383793
train_loss: 0.006261717
test_loss: 0.01036926
train_loss: 0.005567181
test_loss: 0.009399876
train_loss: 0.006768156
test_loss: 0.0099297445
train_loss: 0.006079559
test_loss: 0.00932662
train_loss: 0.0065060635
test_loss: 0.009740621
train_loss: 0.0057896757
test_loss: 0.009598643
train_loss: 0.006735936
test_loss: 0.009604545
train_loss: 0.005833002
test_loss: 0.010228056
train_loss: 0.0064425804
test_loss: 0.010052573
train_loss: 0.0056752516
test_loss: 0.009423691
train_loss: 0.006026161
test_loss: 0.010242774
train_loss: 0.0061165453
test_loss: 0.009680068
train_loss: 0.005475631
test_loss: 0.009537922
train_loss: 0.0058212886
test_loss: 0.010026125
train_loss: 0.0058067343
test_loss: 0.009247801
train_loss: 0.0054022935
test_loss: 0.009384691
train_loss: 0.0056294305
test_loss: 0.009728449
train_loss: 0.005060571
test_loss: 0.009706164
train_loss: 0.005581963
test_loss: 0.00948725
train_loss: 0.0067457277
test_loss: 0.009580914
train_loss: 0.0058291876
test_loss: 0.009827253
train_loss: 0.0053805783
test_loss: 0.009540103
train_loss: 0.0066682687
test_loss: 0.0094955675
train_loss: 0.0072317035
test_loss: 0.009343864
train_loss: 0.0061699916
test_loss: 0.010082991
train_loss: 0.005922064
test_loss: 0.009472898
train_loss: 0.0065825796
test_loss: 0.0097504025
train_loss: 0.005799554
test_loss: 0.009382763
train_loss: 0.006245611
test_loss: 0.009894759
train_loss: 0.005102489
test_loss: 0.0092910305
train_loss: 0.0055238074
test_loss: 0.0095462855
train_loss: 0.006439803
test_loss: 0.00945899
train_loss: 0.006424773
test_loss: 0.010114826
train_loss: 0.0063033123
test_loss: 0.009612307
train_loss: 0.0059978203
test_loss: 0.009463156
train_loss: 0.006539087
test_loss: 0.01030622
train_loss: 0.0055498
test_loss: 0.009165498
train_loss: 0.006206007
test_loss: 0.00972277
train_loss: 0.0058825444
test_loss: 0.009430096
train_loss: 0.0060349475
test_loss: 0.009649345
train_loss: 0.005352054
test_loss: 0.00900139
train_loss: 0.0051935287
test_loss: 0.009270452
train_loss: 0.0060724076
test_loss: 0.009607048
train_loss: 0.0068052285
test_loss: 0.010038272
train_loss: 0.005980529
test_loss: 0.009860569
train_loss: 0.0062054195
test_loss: 0.009466408
train_loss: 0.0061817714
test_loss: 0.010014755
train_loss: 0.0065360055
test_loss: 0.009488019
train_loss: 0.005723695
test_loss: 0.009340188
train_loss: 0.005694756
test_loss: 0.009272043
train_loss: 0.0065048654
test_loss: 0.010007721
train_loss: 0.005883583
test_loss: 0.009717835
train_loss: 0.0060210153
test_loss: 0.009204059
train_loss: 0.0064320564
test_loss: 0.0093174
train_loss: 0.005866076
test_loss: 0.009742658
train_loss: 0.0057757525
test_loss: 0.009760307
train_loss: 0.006187137
test_loss: 0.009678846
train_loss: 0.005814173
test_loss: 0.00943559
train_loss: 0.0059269615
test_loss: 0.009455468
train_loss: 0.005906167
test_loss: 0.009632745
train_loss: 0.005547958
test_loss: 0.009914947
train_loss: 0.005558041
test_loss: 0.009996267
train_loss: 0.005518429
test_loss: 0.009471307
train_loss: 0.0065383893
test_loss: 0.009312965
train_loss: 0.006308916
test_loss: 0.00917233
train_loss: 0.005860389
test_loss: 0.009910074
train_loss: 0.0065032933
test_loss: 0.009687286
train_loss: 0.0064142025
test_loss: 0.009716213
train_loss: 0.0064519583
test_loss: 0.009499538
train_loss: 0.0068593547
test_loss: 0.0101549765
train_loss: 0.005511899
test_loss: 0.0098614255
train_loss: 0.005894838
test_loss: 0.009236792
train_loss: 0.005284991
test_loss: 0.009240746
train_loss: 0.005339709
test_loss: 0.009390286
train_loss: 0.0062893596
test_loss: 0.009984996
train_loss: 0.005649001
test_loss: 0.009508417
train_loss: 0.0059004165
test_loss: 0.010122344
train_loss: 0.0058420496
test_loss: 0.009516122
train_loss: 0.005728111
test_loss: 0.008887142
train_loss: 0.006048088
test_loss: 0.009477319
train_loss: 0.005938857
test_loss: 0.009373645
train_loss: 0.005745428
test_loss: 0.009408454
train_loss: 0.0052938466
test_loss: 0.009400585
train_loss: 0.0053004236
test_loss: 0.009514503
train_loss: 0.005256788
test_loss: 0.009978622
train_loss: 0.0054749874
test_loss: 0.009539162
train_loss: 0.0058606975
test_loss: 0.00908243
train_loss: 0.006533492
test_loss: 0.009692794
train_loss: 0.0062328796
test_loss: 0.009609475
train_loss: 0.0057935007
test_loss: 0.009267074
train_loss: 0.0058513484
test_loss: 0.009121192
train_loss: 0.006161147
test_loss: 0.009891401
train_loss: 0.005660336
test_loss: 0.009419455
train_loss: 0.0050271363
test_loss: 0.008963635
train_loss: 0.005342835
test_loss: 0.009370409
train_loss: 0.0059031243
test_loss: 0.009650526
train_loss: 0.0055925027
test_loss: 0.009271401
train_loss: 0.0049409335
test_loss: 0.009221634
train_loss: 0.0051280293
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
test_loss: 0.009360139
train_loss: 0.0055611753
test_loss: 0.00907844
train_loss: 0.0071947444
test_loss: 0.0095034
train_loss: 0.0059679
test_loss: 0.009394068
train_loss: 0.0052671577
test_loss: 0.009568349
train_loss: 0.006012409
test_loss: 0.009558932
train_loss: 0.0050749406
test_loss: 0.009065413
train_loss: 0.00593517
test_loss: 0.009851359
train_loss: 0.0063200416
test_loss: 0.009721841
train_loss: 0.0054161875
test_loss: 0.009794661
train_loss: 0.0071439827
test_loss: 0.009418406
train_loss: 0.0054058577
test_loss: 0.008925766
train_loss: 0.0058570723
test_loss: 0.009529865
train_loss: 0.005569501
test_loss: 0.008973593
train_loss: 0.0054944814
test_loss: 0.00965324
train_loss: 0.005885008
test_loss: 0.009475565
train_loss: 0.007093806
test_loss: 0.009808186
train_loss: 0.00540164
test_loss: 0.009193024
train_loss: 0.00590631
test_loss: 0.00971703
train_loss: 0.0059229806
test_loss: 0.009224119
train_loss: 0.00653089
test_loss: 0.009948532
train_loss: 0.005254628
test_loss: 0.0095151635
train_loss: 0.005722192
test_loss: 0.009244248
train_loss: 0.0054899454
test_loss: 0.009769171
train_loss: 0.0051261196
test_loss: 0.009130477
train_loss: 0.006375128
test_loss: 0.009365247
train_loss: 0.0059014712
test_loss: 0.009785234
train_loss: 0.005954022
test_loss: 0.009939798
train_loss: 0.0064703845
test_loss: 0.009008195
train_loss: 0.0053051393
test_loss: 0.0093464665
train_loss: 0.0058526904
test_loss: 0.009229348
train_loss: 0.005472755
test_loss: 0.008901375
train_loss: 0.00561772
test_loss: 0.009488357
train_loss: 0.0050551854
test_loss: 0.009113328
train_loss: 0.0060539516
test_loss: 0.009503075
train_loss: 0.0057203416
test_loss: 0.00948675
train_loss: 0.005618004
test_loss: 0.009575872
train_loss: 0.004781452
test_loss: 0.00967944
train_loss: 0.0053438805
test_loss: 0.009429699
train_loss: 0.0066116033
test_loss: 0.009270044
train_loss: 0.00566222
test_loss: 0.009580482
train_loss: 0.0048964685
test_loss: 0.008919871
train_loss: 0.005275177
test_loss: 0.009059558
train_loss: 0.0048368564
test_loss: 0.009334291
train_loss: 0.0055955155
test_loss: 0.009444379
train_loss: 0.005341573
test_loss: 0.009317781
train_loss: 0.005832441
test_loss: 0.009689122
train_loss: 0.005340033
test_loss: 0.009014051
train_loss: 0.005029928
test_loss: 0.00927809
train_loss: 0.0056879614
test_loss: 0.0093071135
train_loss: 0.005930249
test_loss: 0.009846584
train_loss: 0.006483691
test_loss: 0.009567512
train_loss: 0.006367638
test_loss: 0.009709227
train_loss: 0.006150202
test_loss: 0.009405471
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.8/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.8 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.8/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bed431e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bed437b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bedb1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87becaa048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bed0cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bec9e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bec3a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bec321e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bec0c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bec0cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bec0c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87beba11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87beb8fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87beb427b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87beae7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87beae77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87beb25488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87beac2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bea96950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87beae7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bea3fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bea96598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87be9a86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87bea151e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87be9b1488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87be9b1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87be9a3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87be93f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87be956488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87be956268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8781c092f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f87be956840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8781c34840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8781c01620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8781babd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8781ba6d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.80876294e-05
Iter: 2 loss: 4.67967911e-05
Iter: 3 loss: 0.000134154514
Iter: 4 loss: 4.59949224e-05
Iter: 5 loss: 4.18953496e-05
Iter: 6 loss: 5.83170549e-05
Iter: 7 loss: 4.09773384e-05
Iter: 8 loss: 3.58314355e-05
Iter: 9 loss: 6.11638097e-05
Iter: 10 loss: 3.49357288e-05
Iter: 11 loss: 3.27303e-05
Iter: 12 loss: 3.33891512e-05
Iter: 13 loss: 3.11523982e-05
Iter: 14 loss: 2.91476354e-05
Iter: 15 loss: 2.59775843e-05
Iter: 16 loss: 2.59452354e-05
Iter: 17 loss: 2.36299602e-05
Iter: 18 loss: 2.35660755e-05
Iter: 19 loss: 2.218796e-05
Iter: 20 loss: 2.0543037e-05
Iter: 21 loss: 2.03711315e-05
Iter: 22 loss: 1.90699975e-05
Iter: 23 loss: 1.89838865e-05
Iter: 24 loss: 1.81198939e-05
Iter: 25 loss: 1.73470107e-05
Iter: 26 loss: 1.71266038e-05
Iter: 27 loss: 1.61520911e-05
Iter: 28 loss: 2.41266571e-05
Iter: 29 loss: 1.60907603e-05
Iter: 30 loss: 1.54237568e-05
Iter: 31 loss: 1.51597342e-05
Iter: 32 loss: 1.48024337e-05
Iter: 33 loss: 1.40486836e-05
Iter: 34 loss: 1.57875038e-05
Iter: 35 loss: 1.37670395e-05
Iter: 36 loss: 1.33219728e-05
Iter: 37 loss: 1.28295324e-05
Iter: 38 loss: 1.27587682e-05
Iter: 39 loss: 1.29982236e-05
Iter: 40 loss: 1.2429291e-05
Iter: 41 loss: 1.22351021e-05
Iter: 42 loss: 1.1951779e-05
Iter: 43 loss: 1.19440365e-05
Iter: 44 loss: 1.15748289e-05
Iter: 45 loss: 1.13047954e-05
Iter: 46 loss: 1.11792497e-05
Iter: 47 loss: 1.08121822e-05
Iter: 48 loss: 1.61279513e-05
Iter: 49 loss: 1.08114182e-05
Iter: 50 loss: 1.04687097e-05
Iter: 51 loss: 1.01998867e-05
Iter: 52 loss: 1.00938832e-05
Iter: 53 loss: 9.83004793e-06
Iter: 54 loss: 9.80540244e-06
Iter: 55 loss: 9.58364e-06
Iter: 56 loss: 9.11490679e-06
Iter: 57 loss: 1.6816055e-05
Iter: 58 loss: 9.10194831e-06
Iter: 59 loss: 8.71161865e-06
Iter: 60 loss: 8.71147131e-06
Iter: 61 loss: 8.53333222e-06
Iter: 62 loss: 8.31029502e-06
Iter: 63 loss: 8.29216606e-06
Iter: 64 loss: 8.1117123e-06
Iter: 65 loss: 8.10466554e-06
Iter: 66 loss: 7.9706524e-06
Iter: 67 loss: 7.86457895e-06
Iter: 68 loss: 7.82362076e-06
Iter: 69 loss: 7.57482849e-06
Iter: 70 loss: 9.87331e-06
Iter: 71 loss: 7.56417467e-06
Iter: 72 loss: 7.50284835e-06
Iter: 73 loss: 7.46900469e-06
Iter: 74 loss: 7.39669667e-06
Iter: 75 loss: 7.22454661e-06
Iter: 76 loss: 9.16861609e-06
Iter: 77 loss: 7.20764592e-06
Iter: 78 loss: 7.01007093e-06
Iter: 79 loss: 7.48633693e-06
Iter: 80 loss: 6.93879701e-06
Iter: 81 loss: 6.76587115e-06
Iter: 82 loss: 7.14539101e-06
Iter: 83 loss: 6.69977317e-06
Iter: 84 loss: 6.55417807e-06
Iter: 85 loss: 7.71571376e-06
Iter: 86 loss: 6.54459291e-06
Iter: 87 loss: 6.44505189e-06
Iter: 88 loss: 6.39241853e-06
Iter: 89 loss: 6.34723756e-06
Iter: 90 loss: 6.26178826e-06
Iter: 91 loss: 6.25979828e-06
Iter: 92 loss: 6.17965497e-06
Iter: 93 loss: 6.07194943e-06
Iter: 94 loss: 6.06622598e-06
Iter: 95 loss: 5.9320455e-06
Iter: 96 loss: 7.5249186e-06
Iter: 97 loss: 5.93010827e-06
Iter: 98 loss: 5.84978807e-06
Iter: 99 loss: 6.04580327e-06
Iter: 100 loss: 5.82148823e-06
Iter: 101 loss: 5.72386807e-06
Iter: 102 loss: 5.8628807e-06
Iter: 103 loss: 5.6760955e-06
Iter: 104 loss: 5.63124286e-06
Iter: 105 loss: 5.63118874e-06
Iter: 106 loss: 5.57339354e-06
Iter: 107 loss: 5.49173183e-06
Iter: 108 loss: 5.488851e-06
Iter: 109 loss: 5.42743237e-06
Iter: 110 loss: 5.75677677e-06
Iter: 111 loss: 5.41816371e-06
Iter: 112 loss: 5.36830612e-06
Iter: 113 loss: 5.27058592e-06
Iter: 114 loss: 7.24563324e-06
Iter: 115 loss: 5.26986787e-06
Iter: 116 loss: 5.17491935e-06
Iter: 117 loss: 6.48002697e-06
Iter: 118 loss: 5.17431272e-06
Iter: 119 loss: 5.09920756e-06
Iter: 120 loss: 5.17566241e-06
Iter: 121 loss: 5.05720527e-06
Iter: 122 loss: 4.98577811e-06
Iter: 123 loss: 5.28293e-06
Iter: 124 loss: 4.97046676e-06
Iter: 125 loss: 4.89247441e-06
Iter: 126 loss: 5.15352076e-06
Iter: 127 loss: 4.87174839e-06
Iter: 128 loss: 4.8195343e-06
Iter: 129 loss: 5.28309465e-06
Iter: 130 loss: 4.81699499e-06
Iter: 131 loss: 4.77507683e-06
Iter: 132 loss: 4.73384125e-06
Iter: 133 loss: 4.72487318e-06
Iter: 134 loss: 4.67154587e-06
Iter: 135 loss: 4.67077825e-06
Iter: 136 loss: 4.6448431e-06
Iter: 137 loss: 4.69157749e-06
Iter: 138 loss: 4.63378274e-06
Iter: 139 loss: 4.59051489e-06
Iter: 140 loss: 4.66351594e-06
Iter: 141 loss: 4.57109581e-06
Iter: 142 loss: 4.54416931e-06
Iter: 143 loss: 4.57057649e-06
Iter: 144 loss: 4.52964878e-06
Iter: 145 loss: 4.49977324e-06
Iter: 146 loss: 4.45317619e-06
Iter: 147 loss: 4.45269188e-06
Iter: 148 loss: 4.39506039e-06
Iter: 149 loss: 4.92324625e-06
Iter: 150 loss: 4.39251289e-06
Iter: 151 loss: 4.34671392e-06
Iter: 152 loss: 4.33809555e-06
Iter: 153 loss: 4.30749151e-06
Iter: 154 loss: 4.25785765e-06
Iter: 155 loss: 4.88189471e-06
Iter: 156 loss: 4.25740109e-06
Iter: 157 loss: 4.21924551e-06
Iter: 158 loss: 4.21593e-06
Iter: 159 loss: 4.1878061e-06
Iter: 160 loss: 4.14532951e-06
Iter: 161 loss: 4.69260794e-06
Iter: 162 loss: 4.14484202e-06
Iter: 163 loss: 4.11244855e-06
Iter: 164 loss: 4.08379401e-06
Iter: 165 loss: 4.07565858e-06
Iter: 166 loss: 4.03004651e-06
Iter: 167 loss: 4.67415657e-06
Iter: 168 loss: 4.029916e-06
Iter: 169 loss: 4.00597219e-06
Iter: 170 loss: 4.17762885e-06
Iter: 171 loss: 4.00370618e-06
Iter: 172 loss: 3.97932399e-06
Iter: 173 loss: 4.03686136e-06
Iter: 174 loss: 3.97023086e-06
Iter: 175 loss: 3.94760627e-06
Iter: 176 loss: 3.92551829e-06
Iter: 177 loss: 3.9205961e-06
Iter: 178 loss: 3.89193792e-06
Iter: 179 loss: 3.8949579e-06
Iter: 180 loss: 3.86971487e-06
Iter: 181 loss: 3.82536109e-06
Iter: 182 loss: 4.05364972e-06
Iter: 183 loss: 3.81820064e-06
Iter: 184 loss: 3.7921277e-06
Iter: 185 loss: 3.83973565e-06
Iter: 186 loss: 3.78119375e-06
Iter: 187 loss: 3.7469797e-06
Iter: 188 loss: 3.83231691e-06
Iter: 189 loss: 3.73522812e-06
Iter: 190 loss: 3.70915677e-06
Iter: 191 loss: 3.82055168e-06
Iter: 192 loss: 3.70403291e-06
Iter: 193 loss: 3.67374105e-06
Iter: 194 loss: 3.70233465e-06
Iter: 195 loss: 3.6566787e-06
Iter: 196 loss: 3.63390427e-06
Iter: 197 loss: 3.63383947e-06
Iter: 198 loss: 3.61528691e-06
Iter: 199 loss: 3.59422393e-06
Iter: 200 loss: 3.59133878e-06
Iter: 201 loss: 3.5669641e-06
Iter: 202 loss: 3.56659e-06
Iter: 203 loss: 3.55157727e-06
Iter: 204 loss: 3.68427663e-06
Iter: 205 loss: 3.55083603e-06
Iter: 206 loss: 3.54110762e-06
Iter: 207 loss: 3.52057077e-06
Iter: 208 loss: 3.8571834e-06
Iter: 209 loss: 3.51967583e-06
Iter: 210 loss: 3.49288757e-06
Iter: 211 loss: 3.52600614e-06
Iter: 212 loss: 3.4786849e-06
Iter: 213 loss: 3.45541139e-06
Iter: 214 loss: 3.49723678e-06
Iter: 215 loss: 3.44490672e-06
Iter: 216 loss: 3.41523491e-06
Iter: 217 loss: 3.53351038e-06
Iter: 218 loss: 3.40819929e-06
Iter: 219 loss: 3.38898894e-06
Iter: 220 loss: 3.41928603e-06
Iter: 221 loss: 3.38019709e-06
Iter: 222 loss: 3.35190316e-06
Iter: 223 loss: 3.40970178e-06
Iter: 224 loss: 3.34022161e-06
Iter: 225 loss: 3.32083391e-06
Iter: 226 loss: 3.43434476e-06
Iter: 227 loss: 3.31799743e-06
Iter: 228 loss: 3.29699287e-06
Iter: 229 loss: 3.32834134e-06
Iter: 230 loss: 3.28650185e-06
Iter: 231 loss: 3.27074486e-06
Iter: 232 loss: 3.51151607e-06
Iter: 233 loss: 3.2706921e-06
Iter: 234 loss: 3.26201348e-06
Iter: 235 loss: 3.27680664e-06
Iter: 236 loss: 3.25782412e-06
Iter: 237 loss: 3.2455871e-06
Iter: 238 loss: 3.31654542e-06
Iter: 239 loss: 3.24391885e-06
Iter: 240 loss: 3.23285121e-06
Iter: 241 loss: 3.22266169e-06
Iter: 242 loss: 3.22028609e-06
Iter: 243 loss: 3.20708182e-06
Iter: 244 loss: 3.21981861e-06
Iter: 245 loss: 3.1995362e-06
Iter: 246 loss: 3.1805564e-06
Iter: 247 loss: 3.19304695e-06
Iter: 248 loss: 3.16865635e-06
Iter: 249 loss: 3.15216403e-06
Iter: 250 loss: 3.24491702e-06
Iter: 251 loss: 3.14995032e-06
Iter: 252 loss: 3.1309014e-06
Iter: 253 loss: 3.13841088e-06
Iter: 254 loss: 3.1179402e-06
Iter: 255 loss: 3.10181031e-06
Iter: 256 loss: 3.22821779e-06
Iter: 257 loss: 3.10081168e-06
Iter: 258 loss: 3.08339531e-06
Iter: 259 loss: 3.06993161e-06
Iter: 260 loss: 3.0647484e-06
Iter: 261 loss: 3.05100411e-06
Iter: 262 loss: 3.05038316e-06
Iter: 263 loss: 3.0384283e-06
Iter: 264 loss: 3.03400975e-06
Iter: 265 loss: 3.02784065e-06
Iter: 266 loss: 3.01313594e-06
Iter: 267 loss: 3.19407877e-06
Iter: 268 loss: 3.01289174e-06
Iter: 269 loss: 3.00519059e-06
Iter: 270 loss: 3.08938388e-06
Iter: 271 loss: 3.00505735e-06
Iter: 272 loss: 2.9982009e-06
Iter: 273 loss: 2.98622626e-06
Iter: 274 loss: 2.98606506e-06
Iter: 275 loss: 2.96958524e-06
Iter: 276 loss: 2.9768803e-06
Iter: 277 loss: 2.95867198e-06
Iter: 278 loss: 2.94404754e-06
Iter: 279 loss: 3.00458782e-06
Iter: 280 loss: 2.94115034e-06
Iter: 281 loss: 2.92594e-06
Iter: 282 loss: 2.93088874e-06
Iter: 283 loss: 2.91493734e-06
Iter: 284 loss: 2.9027924e-06
Iter: 285 loss: 3.05459844e-06
Iter: 286 loss: 2.90261141e-06
Iter: 287 loss: 2.89199534e-06
Iter: 288 loss: 2.88242973e-06
Iter: 289 loss: 2.8798645e-06
Iter: 290 loss: 2.86839395e-06
Iter: 291 loss: 3.04201126e-06
Iter: 292 loss: 2.86819431e-06
Iter: 293 loss: 2.85852593e-06
Iter: 294 loss: 2.84917792e-06
Iter: 295 loss: 2.84689031e-06
Iter: 296 loss: 2.83723944e-06
Iter: 297 loss: 2.83679e-06
Iter: 298 loss: 2.83048985e-06
Iter: 299 loss: 2.83732561e-06
Iter: 300 loss: 2.82668452e-06
Iter: 301 loss: 2.81857865e-06
Iter: 302 loss: 2.89087075e-06
Iter: 303 loss: 2.81850134e-06
Iter: 304 loss: 2.81066332e-06
Iter: 305 loss: 2.81110715e-06
Iter: 306 loss: 2.80477229e-06
Iter: 307 loss: 2.79760275e-06
Iter: 308 loss: 2.79937694e-06
Iter: 309 loss: 2.79249548e-06
Iter: 310 loss: 2.78148218e-06
Iter: 311 loss: 2.77454251e-06
Iter: 312 loss: 2.77002164e-06
Iter: 313 loss: 2.75798084e-06
Iter: 314 loss: 2.88591855e-06
Iter: 315 loss: 2.75771026e-06
Iter: 316 loss: 2.74776266e-06
Iter: 317 loss: 2.74113563e-06
Iter: 318 loss: 2.73734804e-06
Iter: 319 loss: 2.72580928e-06
Iter: 320 loss: 2.88774208e-06
Iter: 321 loss: 2.72557554e-06
Iter: 322 loss: 2.71665022e-06
Iter: 323 loss: 2.70364671e-06
Iter: 324 loss: 2.70326677e-06
Iter: 325 loss: 2.69105294e-06
Iter: 326 loss: 2.69092061e-06
Iter: 327 loss: 2.68380563e-06
Iter: 328 loss: 2.68334497e-06
Iter: 329 loss: 2.67833616e-06
Iter: 330 loss: 2.66728784e-06
Iter: 331 loss: 2.78671268e-06
Iter: 332 loss: 2.66694042e-06
Iter: 333 loss: 2.66245e-06
Iter: 334 loss: 2.66219467e-06
Iter: 335 loss: 2.65885478e-06
Iter: 336 loss: 2.65292692e-06
Iter: 337 loss: 2.65291101e-06
Iter: 338 loss: 2.64326104e-06
Iter: 339 loss: 2.63205789e-06
Iter: 340 loss: 2.63059155e-06
Iter: 341 loss: 2.62128628e-06
Iter: 342 loss: 2.72838452e-06
Iter: 343 loss: 2.62104413e-06
Iter: 344 loss: 2.61276659e-06
Iter: 345 loss: 2.60604406e-06
Iter: 346 loss: 2.60376737e-06
Iter: 347 loss: 2.59434114e-06
Iter: 348 loss: 2.70352166e-06
Iter: 349 loss: 2.59421631e-06
Iter: 350 loss: 2.58625732e-06
Iter: 351 loss: 2.58077648e-06
Iter: 352 loss: 2.57794386e-06
Iter: 353 loss: 2.56827184e-06
Iter: 354 loss: 2.68807207e-06
Iter: 355 loss: 2.56817748e-06
Iter: 356 loss: 2.56111548e-06
Iter: 357 loss: 2.55729651e-06
Iter: 358 loss: 2.55396253e-06
Iter: 359 loss: 2.54452607e-06
Iter: 360 loss: 2.66978418e-06
Iter: 361 loss: 2.54438214e-06
Iter: 362 loss: 2.53919779e-06
Iter: 363 loss: 2.54966108e-06
Iter: 364 loss: 2.53714961e-06
Iter: 365 loss: 2.53019607e-06
Iter: 366 loss: 2.55764326e-06
Iter: 367 loss: 2.52866585e-06
Iter: 368 loss: 2.5217546e-06
Iter: 369 loss: 2.53234589e-06
Iter: 370 loss: 2.51865185e-06
Iter: 371 loss: 2.51351184e-06
Iter: 372 loss: 2.51941015e-06
Iter: 373 loss: 2.51080064e-06
Iter: 374 loss: 2.50461017e-06
Iter: 375 loss: 2.50042331e-06
Iter: 376 loss: 2.4979654e-06
Iter: 377 loss: 2.49212189e-06
Iter: 378 loss: 2.5476902e-06
Iter: 379 loss: 2.4919741e-06
Iter: 380 loss: 2.4856115e-06
Iter: 381 loss: 2.4749188e-06
Iter: 382 loss: 2.47469188e-06
Iter: 383 loss: 2.46830132e-06
Iter: 384 loss: 2.46786612e-06
Iter: 385 loss: 2.46189529e-06
Iter: 386 loss: 2.4544629e-06
Iter: 387 loss: 2.45392584e-06
Iter: 388 loss: 2.44697708e-06
Iter: 389 loss: 2.55943223e-06
Iter: 390 loss: 2.44691091e-06
Iter: 391 loss: 2.44072226e-06
Iter: 392 loss: 2.44021385e-06
Iter: 393 loss: 2.43558452e-06
Iter: 394 loss: 2.42823762e-06
Iter: 395 loss: 2.51903725e-06
Iter: 396 loss: 2.42812257e-06
Iter: 397 loss: 2.42474152e-06
Iter: 398 loss: 2.4246624e-06
Iter: 399 loss: 2.42154783e-06
Iter: 400 loss: 2.42338206e-06
Iter: 401 loss: 2.41967427e-06
Iter: 402 loss: 2.41569251e-06
Iter: 403 loss: 2.41141902e-06
Iter: 404 loss: 2.41065663e-06
Iter: 405 loss: 2.40478221e-06
Iter: 406 loss: 2.43036811e-06
Iter: 407 loss: 2.40372128e-06
Iter: 408 loss: 2.39714291e-06
Iter: 409 loss: 2.39004703e-06
Iter: 410 loss: 2.38884695e-06
Iter: 411 loss: 2.38107737e-06
Iter: 412 loss: 2.4834012e-06
Iter: 413 loss: 2.38105417e-06
Iter: 414 loss: 2.37472887e-06
Iter: 415 loss: 2.36964024e-06
Iter: 416 loss: 2.36744427e-06
Iter: 417 loss: 2.36116966e-06
Iter: 418 loss: 2.44571879e-06
Iter: 419 loss: 2.36114215e-06
Iter: 420 loss: 2.35463222e-06
Iter: 421 loss: 2.34927279e-06
Iter: 422 loss: 2.34760728e-06
Iter: 423 loss: 2.34135632e-06
Iter: 424 loss: 2.43659224e-06
Iter: 425 loss: 2.34149684e-06
Iter: 426 loss: 2.33571564e-06
Iter: 427 loss: 2.3329851e-06
Iter: 428 loss: 2.3301634e-06
Iter: 429 loss: 2.3255468e-06
Iter: 430 loss: 2.32480488e-06
Iter: 431 loss: 2.32162938e-06
Iter: 432 loss: 2.33242054e-06
Iter: 433 loss: 2.32080265e-06
Iter: 434 loss: 2.31845752e-06
Iter: 435 loss: 2.31631475e-06
Iter: 436 loss: 2.31551167e-06
Iter: 437 loss: 2.31066406e-06
Iter: 438 loss: 2.31121658e-06
Iter: 439 loss: 2.3070852e-06
Iter: 440 loss: 2.30184173e-06
Iter: 441 loss: 2.33951e-06
Iter: 442 loss: 2.30131468e-06
Iter: 443 loss: 2.29735906e-06
Iter: 444 loss: 2.29279681e-06
Iter: 445 loss: 2.29236275e-06
Iter: 446 loss: 2.286788e-06
Iter: 447 loss: 2.36806591e-06
Iter: 448 loss: 2.28690487e-06
Iter: 449 loss: 2.28317685e-06
Iter: 450 loss: 2.27653459e-06
Iter: 451 loss: 2.43850695e-06
Iter: 452 loss: 2.2767731e-06
Iter: 453 loss: 2.27111445e-06
Iter: 454 loss: 2.27093255e-06
Iter: 455 loss: 2.26750399e-06
Iter: 456 loss: 2.2644424e-06
Iter: 457 loss: 2.2636566e-06
Iter: 458 loss: 2.25721556e-06
Iter: 459 loss: 2.29817783e-06
Iter: 460 loss: 2.25641543e-06
Iter: 461 loss: 2.25381177e-06
Iter: 462 loss: 2.25345229e-06
Iter: 463 loss: 2.25089457e-06
Iter: 464 loss: 2.25611e-06
Iter: 465 loss: 2.24971791e-06
Iter: 466 loss: 2.24648738e-06
Iter: 467 loss: 2.24201e-06
Iter: 468 loss: 2.24186715e-06
Iter: 469 loss: 2.2370773e-06
Iter: 470 loss: 2.26192878e-06
Iter: 471 loss: 2.23619759e-06
Iter: 472 loss: 2.231606e-06
Iter: 473 loss: 2.22921153e-06
Iter: 474 loss: 2.22700282e-06
Iter: 475 loss: 2.22040899e-06
Iter: 476 loss: 2.25987333e-06
Iter: 477 loss: 2.21974915e-06
Iter: 478 loss: 2.21538767e-06
Iter: 479 loss: 2.2132665e-06
Iter: 480 loss: 2.21137816e-06
Iter: 481 loss: 2.20464722e-06
Iter: 482 loss: 2.25745885e-06
Iter: 483 loss: 2.20419383e-06
Iter: 484 loss: 2.20004108e-06
Iter: 485 loss: 2.19687786e-06
Iter: 486 loss: 2.19545427e-06
Iter: 487 loss: 2.18923674e-06
Iter: 488 loss: 2.27238957e-06
Iter: 489 loss: 2.18921741e-06
Iter: 490 loss: 2.18558e-06
Iter: 491 loss: 2.19008029e-06
Iter: 492 loss: 2.18376317e-06
Iter: 493 loss: 2.17873912e-06
Iter: 494 loss: 2.20407605e-06
Iter: 495 loss: 2.17804927e-06
Iter: 496 loss: 2.17393017e-06
Iter: 497 loss: 2.2117747e-06
Iter: 498 loss: 2.17385877e-06
Iter: 499 loss: 2.17133e-06
Iter: 500 loss: 2.17317256e-06
Iter: 501 loss: 2.16981016e-06
Iter: 502 loss: 2.1671849e-06
Iter: 503 loss: 2.16289754e-06
Iter: 504 loss: 2.16287276e-06
Iter: 505 loss: 2.15772434e-06
Iter: 506 loss: 2.20684137e-06
Iter: 507 loss: 2.15749901e-06
Iter: 508 loss: 2.15448563e-06
Iter: 509 loss: 2.15330238e-06
Iter: 510 loss: 2.15173395e-06
Iter: 511 loss: 2.14615307e-06
Iter: 512 loss: 2.1561998e-06
Iter: 513 loss: 2.14371084e-06
Iter: 514 loss: 2.13998101e-06
Iter: 515 loss: 2.16146873e-06
Iter: 516 loss: 2.13943667e-06
Iter: 517 loss: 2.13503245e-06
Iter: 518 loss: 2.13122803e-06
Iter: 519 loss: 2.13021644e-06
Iter: 520 loss: 2.12574651e-06
Iter: 521 loss: 2.19061712e-06
Iter: 522 loss: 2.12587361e-06
Iter: 523 loss: 2.12181021e-06
Iter: 524 loss: 2.12008717e-06
Iter: 525 loss: 2.11786437e-06
Iter: 526 loss: 2.11531233e-06
Iter: 527 loss: 2.11469205e-06
Iter: 528 loss: 2.11255474e-06
Iter: 529 loss: 2.12853297e-06
Iter: 530 loss: 2.11231395e-06
Iter: 531 loss: 2.11034649e-06
Iter: 532 loss: 2.10760936e-06
Iter: 533 loss: 2.10741382e-06
Iter: 534 loss: 2.10345115e-06
Iter: 535 loss: 2.10851522e-06
Iter: 536 loss: 2.10150188e-06
Iter: 537 loss: 2.09792847e-06
Iter: 538 loss: 2.10399912e-06
Iter: 539 loss: 2.09628956e-06
Iter: 540 loss: 2.0910104e-06
Iter: 541 loss: 2.09256268e-06
Iter: 542 loss: 2.0870425e-06
Iter: 543 loss: 2.08238976e-06
Iter: 544 loss: 2.11376664e-06
Iter: 545 loss: 2.0820894e-06
Iter: 546 loss: 2.0778e-06
Iter: 547 loss: 2.07664652e-06
Iter: 548 loss: 2.07396215e-06
Iter: 549 loss: 2.06899449e-06
Iter: 550 loss: 2.11291035e-06
Iter: 551 loss: 2.06875825e-06
Iter: 552 loss: 2.06434697e-06
Iter: 553 loss: 2.06290224e-06
Iter: 554 loss: 2.06027698e-06
Iter: 555 loss: 2.05641277e-06
Iter: 556 loss: 2.05625406e-06
Iter: 557 loss: 2.05294236e-06
Iter: 558 loss: 2.05056722e-06
Iter: 559 loss: 2.04941853e-06
Iter: 560 loss: 2.04558842e-06
Iter: 561 loss: 2.0454072e-06
Iter: 562 loss: 2.04293383e-06
Iter: 563 loss: 2.04645403e-06
Iter: 564 loss: 2.04194794e-06
Iter: 565 loss: 2.04024423e-06
Iter: 566 loss: 2.03892091e-06
Iter: 567 loss: 2.03830223e-06
Iter: 568 loss: 2.03422383e-06
Iter: 569 loss: 2.03204468e-06
Iter: 570 loss: 2.03032369e-06
Iter: 571 loss: 2.02673141e-06
Iter: 572 loss: 2.02669821e-06
Iter: 573 loss: 2.02401134e-06
Iter: 574 loss: 2.01999228e-06
Iter: 575 loss: 2.01986131e-06
Iter: 576 loss: 2.01629246e-06
Iter: 577 loss: 2.06338177e-06
Iter: 578 loss: 2.01621629e-06
Iter: 579 loss: 2.01321041e-06
Iter: 580 loss: 2.00843488e-06
Iter: 581 loss: 2.0084949e-06
Iter: 582 loss: 2.00481486e-06
Iter: 583 loss: 2.00476052e-06
Iter: 584 loss: 2.00195063e-06
Iter: 585 loss: 2.00096565e-06
Iter: 586 loss: 1.9994377e-06
Iter: 587 loss: 1.99542615e-06
Iter: 588 loss: 2.02438468e-06
Iter: 589 loss: 1.99507849e-06
Iter: 590 loss: 1.99388114e-06
Iter: 591 loss: 1.99355691e-06
Iter: 592 loss: 1.99199917e-06
Iter: 593 loss: 1.98981638e-06
Iter: 594 loss: 1.9895017e-06
Iter: 595 loss: 1.98663724e-06
Iter: 596 loss: 1.99142551e-06
Iter: 597 loss: 1.98536964e-06
Iter: 598 loss: 1.98283828e-06
Iter: 599 loss: 1.98751604e-06
Iter: 600 loss: 1.98176485e-06
Iter: 601 loss: 1.97828922e-06
Iter: 602 loss: 1.97619465e-06
Iter: 603 loss: 1.97464033e-06
Iter: 604 loss: 1.97120744e-06
Iter: 605 loss: 2.02639012e-06
Iter: 606 loss: 1.97118106e-06
Iter: 607 loss: 1.96871747e-06
Iter: 608 loss: 1.96343535e-06
Iter: 609 loss: 2.04279854e-06
Iter: 610 loss: 1.96314886e-06
Iter: 611 loss: 1.96010137e-06
Iter: 612 loss: 1.95958319e-06
Iter: 613 loss: 1.95737357e-06
Iter: 614 loss: 1.95345274e-06
Iter: 615 loss: 1.95353687e-06
Iter: 616 loss: 1.94989866e-06
Iter: 617 loss: 2.00035697e-06
Iter: 618 loss: 1.94997233e-06
Iter: 619 loss: 1.94732706e-06
Iter: 620 loss: 1.94502513e-06
Iter: 621 loss: 1.94441418e-06
Iter: 622 loss: 1.94078325e-06
Iter: 623 loss: 1.97101281e-06
Iter: 624 loss: 1.9405677e-06
Iter: 625 loss: 1.94049517e-06
Iter: 626 loss: 1.93907772e-06
Iter: 627 loss: 1.93846449e-06
Iter: 628 loss: 1.9363888e-06
Iter: 629 loss: 1.93629694e-06
Iter: 630 loss: 1.93427627e-06
Iter: 631 loss: 1.93074402e-06
Iter: 632 loss: 1.93073788e-06
Iter: 633 loss: 1.92858442e-06
Iter: 634 loss: 1.92927473e-06
Iter: 635 loss: 1.92711195e-06
Iter: 636 loss: 1.92398261e-06
Iter: 637 loss: 1.92360767e-06
Iter: 638 loss: 1.92149969e-06
Iter: 639 loss: 1.91848267e-06
Iter: 640 loss: 1.94113727e-06
Iter: 641 loss: 1.91838171e-06
Iter: 642 loss: 1.91529193e-06
Iter: 643 loss: 1.91760864e-06
Iter: 644 loss: 1.91357663e-06
Iter: 645 loss: 1.91040726e-06
Iter: 646 loss: 1.91122763e-06
Iter: 647 loss: 1.90800165e-06
Iter: 648 loss: 1.9039262e-06
Iter: 649 loss: 1.94101017e-06
Iter: 650 loss: 1.90365313e-06
Iter: 651 loss: 1.90056096e-06
Iter: 652 loss: 1.89751927e-06
Iter: 653 loss: 1.89667207e-06
Iter: 654 loss: 1.89213551e-06
Iter: 655 loss: 1.95885514e-06
Iter: 656 loss: 1.89217621e-06
Iter: 657 loss: 1.89027207e-06
Iter: 658 loss: 1.91615436e-06
Iter: 659 loss: 1.89024649e-06
Iter: 660 loss: 1.8874797e-06
Iter: 661 loss: 1.88454021e-06
Iter: 662 loss: 1.88422553e-06
Iter: 663 loss: 1.88183071e-06
Iter: 664 loss: 1.88822116e-06
Iter: 665 loss: 1.88128536e-06
Iter: 666 loss: 1.87859951e-06
Iter: 667 loss: 1.87731985e-06
Iter: 668 loss: 1.87617854e-06
Iter: 669 loss: 1.87241631e-06
Iter: 670 loss: 1.90183846e-06
Iter: 671 loss: 1.87214027e-06
Iter: 672 loss: 1.86980969e-06
Iter: 673 loss: 1.86714919e-06
Iter: 674 loss: 1.86692159e-06
Iter: 675 loss: 1.86270893e-06
Iter: 676 loss: 1.90685523e-06
Iter: 677 loss: 1.86256148e-06
Iter: 678 loss: 1.8603713e-06
Iter: 679 loss: 1.86327111e-06
Iter: 680 loss: 1.85931185e-06
Iter: 681 loss: 1.85618842e-06
Iter: 682 loss: 1.86160503e-06
Iter: 683 loss: 1.85469401e-06
Iter: 684 loss: 1.85186195e-06
Iter: 685 loss: 1.85758029e-06
Iter: 686 loss: 1.85085389e-06
Iter: 687 loss: 1.84782971e-06
Iter: 688 loss: 1.86096429e-06
Iter: 689 loss: 1.84726105e-06
Iter: 690 loss: 1.84526471e-06
Iter: 691 loss: 1.85156841e-06
Iter: 692 loss: 1.84447026e-06
Iter: 693 loss: 1.84129613e-06
Iter: 694 loss: 1.8584027e-06
Iter: 695 loss: 1.8407884e-06
Iter: 696 loss: 1.83874e-06
Iter: 697 loss: 1.83818815e-06
Iter: 698 loss: 1.83707334e-06
Iter: 699 loss: 1.83477869e-06
Iter: 700 loss: 1.83286352e-06
Iter: 701 loss: 1.83213479e-06
Iter: 702 loss: 1.82840654e-06
Iter: 703 loss: 1.85967224e-06
Iter: 704 loss: 1.82810243e-06
Iter: 705 loss: 1.82581903e-06
Iter: 706 loss: 1.825857e-06
Iter: 707 loss: 1.82407553e-06
Iter: 708 loss: 1.82025974e-06
Iter: 709 loss: 1.83643203e-06
Iter: 710 loss: 1.81960456e-06
Iter: 711 loss: 1.81763448e-06
Iter: 712 loss: 1.82093368e-06
Iter: 713 loss: 1.816647e-06
Iter: 714 loss: 1.81362168e-06
Iter: 715 loss: 1.81606583e-06
Iter: 716 loss: 1.81164251e-06
Iter: 717 loss: 1.80933353e-06
Iter: 718 loss: 1.82650126e-06
Iter: 719 loss: 1.80921677e-06
Iter: 720 loss: 1.80678944e-06
Iter: 721 loss: 1.80806524e-06
Iter: 722 loss: 1.8052408e-06
Iter: 723 loss: 1.80311645e-06
Iter: 724 loss: 1.82455096e-06
Iter: 725 loss: 1.80296115e-06
Iter: 726 loss: 1.80206018e-06
Iter: 727 loss: 1.80204825e-06
Iter: 728 loss: 1.8007953e-06
Iter: 729 loss: 1.79873041e-06
Iter: 730 loss: 1.79864037e-06
Iter: 731 loss: 1.7969478e-06
Iter: 732 loss: 1.79925507e-06
Iter: 733 loss: 1.79610117e-06
Iter: 734 loss: 1.79375979e-06
Iter: 735 loss: 1.79206791e-06
Iter: 736 loss: 1.79142603e-06
Iter: 737 loss: 1.78859625e-06
Iter: 738 loss: 1.78852167e-06
Iter: 739 loss: 1.78677283e-06
Iter: 740 loss: 1.78415348e-06
Iter: 741 loss: 1.78405367e-06
Iter: 742 loss: 1.78163657e-06
Iter: 743 loss: 1.78171797e-06
Iter: 744 loss: 1.7799332e-06
Iter: 745 loss: 1.77788138e-06
Iter: 746 loss: 1.77779339e-06
Iter: 747 loss: 1.77484037e-06
Iter: 748 loss: 1.80546954e-06
Iter: 749 loss: 1.77498612e-06
Iter: 750 loss: 1.77290758e-06
Iter: 751 loss: 1.77047286e-06
Iter: 752 loss: 1.77027357e-06
Iter: 753 loss: 1.76646461e-06
Iter: 754 loss: 1.78373102e-06
Iter: 755 loss: 1.76583694e-06
Iter: 756 loss: 1.76408491e-06
Iter: 757 loss: 1.76411504e-06
Iter: 758 loss: 1.76281208e-06
Iter: 759 loss: 1.76028266e-06
Iter: 760 loss: 1.78205357e-06
Iter: 761 loss: 1.76019648e-06
Iter: 762 loss: 1.75878404e-06
Iter: 763 loss: 1.75883565e-06
Iter: 764 loss: 1.75801733e-06
Iter: 765 loss: 1.75651394e-06
Iter: 766 loss: 1.78021389e-06
Iter: 767 loss: 1.75654316e-06
Iter: 768 loss: 1.75448884e-06
Iter: 769 loss: 1.75472292e-06
Iter: 770 loss: 1.75309867e-06
Iter: 771 loss: 1.75096352e-06
Iter: 772 loss: 1.77187655e-06
Iter: 773 loss: 1.751019e-06
Iter: 774 loss: 1.74934007e-06
Iter: 775 loss: 1.74646982e-06
Iter: 776 loss: 1.74645186e-06
Iter: 777 loss: 1.74443528e-06
Iter: 778 loss: 1.74438537e-06
Iter: 779 loss: 1.74263403e-06
Iter: 780 loss: 1.74144486e-06
Iter: 781 loss: 1.74104207e-06
Iter: 782 loss: 1.73832268e-06
Iter: 783 loss: 1.7571524e-06
Iter: 784 loss: 1.7380703e-06
Iter: 785 loss: 1.7360328e-06
Iter: 786 loss: 1.7344563e-06
Iter: 787 loss: 1.7337e-06
Iter: 788 loss: 1.73109413e-06
Iter: 789 loss: 1.77379729e-06
Iter: 790 loss: 1.73106082e-06
Iter: 791 loss: 1.72967214e-06
Iter: 792 loss: 1.73290266e-06
Iter: 793 loss: 1.72911382e-06
Iter: 794 loss: 1.7275745e-06
Iter: 795 loss: 1.74054162e-06
Iter: 796 loss: 1.72738987e-06
Iter: 797 loss: 1.72551972e-06
Iter: 798 loss: 1.72739396e-06
Iter: 799 loss: 1.72450655e-06
Iter: 800 loss: 1.72336513e-06
Iter: 801 loss: 1.72233376e-06
Iter: 802 loss: 1.7222219e-06
Iter: 803 loss: 1.71965905e-06
Iter: 804 loss: 1.72216119e-06
Iter: 805 loss: 1.71826082e-06
Iter: 806 loss: 1.71651868e-06
Iter: 807 loss: 1.73803187e-06
Iter: 808 loss: 1.71650288e-06
Iter: 809 loss: 1.71510487e-06
Iter: 810 loss: 1.71379213e-06
Iter: 811 loss: 1.71347574e-06
Iter: 812 loss: 1.71105717e-06
Iter: 813 loss: 1.72868067e-06
Iter: 814 loss: 1.71087765e-06
Iter: 815 loss: 1.70952967e-06
Iter: 816 loss: 1.70890132e-06
Iter: 817 loss: 1.70821681e-06
Iter: 818 loss: 1.70554108e-06
Iter: 819 loss: 1.71853685e-06
Iter: 820 loss: 1.704959e-06
Iter: 821 loss: 1.7035735e-06
Iter: 822 loss: 1.71101306e-06
Iter: 823 loss: 1.70345925e-06
Iter: 824 loss: 1.70197347e-06
Iter: 825 loss: 1.70041221e-06
Iter: 826 loss: 1.70014278e-06
Iter: 827 loss: 1.69842303e-06
Iter: 828 loss: 1.69833118e-06
Iter: 829 loss: 1.69709733e-06
Iter: 830 loss: 1.70821568e-06
Iter: 831 loss: 1.69703742e-06
Iter: 832 loss: 1.69633358e-06
Iter: 833 loss: 1.69435725e-06
Iter: 834 loss: 1.70090493e-06
Iter: 835 loss: 1.69363091e-06
Iter: 836 loss: 1.69099258e-06
Iter: 837 loss: 1.72028638e-06
Iter: 838 loss: 1.69098109e-06
Iter: 839 loss: 1.68950021e-06
Iter: 840 loss: 1.68809675e-06
Iter: 841 loss: 1.68760653e-06
Iter: 842 loss: 1.68465772e-06
Iter: 843 loss: 1.70792759e-06
Iter: 844 loss: 1.68448094e-06
Iter: 845 loss: 1.6829481e-06
Iter: 846 loss: 1.68783913e-06
Iter: 847 loss: 1.68255531e-06
Iter: 848 loss: 1.68077781e-06
Iter: 849 loss: 1.6802353e-06
Iter: 850 loss: 1.6792668e-06
Iter: 851 loss: 1.67761266e-06
Iter: 852 loss: 1.67759299e-06
Iter: 853 loss: 1.6764252e-06
Iter: 854 loss: 1.6761262e-06
Iter: 855 loss: 1.67547091e-06
Iter: 856 loss: 1.67347525e-06
Iter: 857 loss: 1.6859492e-06
Iter: 858 loss: 1.67307439e-06
Iter: 859 loss: 1.67201733e-06
Iter: 860 loss: 1.67434609e-06
Iter: 861 loss: 1.67157805e-06
Iter: 862 loss: 1.67010626e-06
Iter: 863 loss: 1.68047143e-06
Iter: 864 loss: 1.67003975e-06
Iter: 865 loss: 1.6688864e-06
Iter: 866 loss: 1.66724476e-06
Iter: 867 loss: 1.66716916e-06
Iter: 868 loss: 1.66571431e-06
Iter: 869 loss: 1.66846348e-06
Iter: 870 loss: 1.66523182e-06
Iter: 871 loss: 1.66335894e-06
Iter: 872 loss: 1.66605651e-06
Iter: 873 loss: 1.66262589e-06
Iter: 874 loss: 1.66102177e-06
Iter: 875 loss: 1.66899974e-06
Iter: 876 loss: 1.66073619e-06
Iter: 877 loss: 1.65888753e-06
Iter: 878 loss: 1.65882409e-06
Iter: 879 loss: 1.65744405e-06
Iter: 880 loss: 1.65557685e-06
Iter: 881 loss: 1.67248209e-06
Iter: 882 loss: 1.655392e-06
Iter: 883 loss: 1.65397796e-06
Iter: 884 loss: 1.65420761e-06
Iter: 885 loss: 1.65300844e-06
Iter: 886 loss: 1.65088795e-06
Iter: 887 loss: 1.66204632e-06
Iter: 888 loss: 1.65042047e-06
Iter: 889 loss: 1.64922608e-06
Iter: 890 loss: 1.65388803e-06
Iter: 891 loss: 1.64880748e-06
Iter: 892 loss: 1.64724236e-06
Iter: 893 loss: 1.65330482e-06
Iter: 894 loss: 1.64693358e-06
Iter: 895 loss: 1.64594326e-06
Iter: 896 loss: 1.66029008e-06
Iter: 897 loss: 1.64589392e-06
Iter: 898 loss: 1.64520043e-06
Iter: 899 loss: 1.64480014e-06
Iter: 900 loss: 1.6444219e-06
Iter: 901 loss: 1.64351832e-06
Iter: 902 loss: 1.64143228e-06
Iter: 903 loss: 1.67732605e-06
Iter: 904 loss: 1.64127e-06
Iter: 905 loss: 1.63981144e-06
Iter: 906 loss: 1.6396649e-06
Iter: 907 loss: 1.63849495e-06
Iter: 908 loss: 1.6372303e-06
Iter: 909 loss: 1.63696006e-06
Iter: 910 loss: 1.63524055e-06
Iter: 911 loss: 1.66048926e-06
Iter: 912 loss: 1.63515779e-06
Iter: 913 loss: 1.63425966e-06
Iter: 914 loss: 1.63375569e-06
Iter: 915 loss: 1.63307448e-06
Iter: 916 loss: 1.63160814e-06
Iter: 917 loss: 1.63273626e-06
Iter: 918 loss: 1.6305853e-06
Iter: 919 loss: 1.62938443e-06
Iter: 920 loss: 1.63325092e-06
Iter: 921 loss: 1.62921015e-06
Iter: 922 loss: 1.62774018e-06
Iter: 923 loss: 1.62860727e-06
Iter: 924 loss: 1.62686558e-06
Iter: 925 loss: 1.62592539e-06
Iter: 926 loss: 1.62585582e-06
Iter: 927 loss: 1.62506126e-06
Iter: 928 loss: 1.62814263e-06
Iter: 929 loss: 1.62487697e-06
Iter: 930 loss: 1.62397407e-06
Iter: 931 loss: 1.62301694e-06
Iter: 932 loss: 1.62289962e-06
Iter: 933 loss: 1.62129822e-06
Iter: 934 loss: 1.62346123e-06
Iter: 935 loss: 1.62067784e-06
Iter: 936 loss: 1.6193294e-06
Iter: 937 loss: 1.61938078e-06
Iter: 938 loss: 1.61830053e-06
Iter: 939 loss: 1.61640321e-06
Iter: 940 loss: 1.6252219e-06
Iter: 941 loss: 1.61580579e-06
Iter: 942 loss: 1.61437879e-06
Iter: 943 loss: 1.61686319e-06
Iter: 944 loss: 1.61368928e-06
Iter: 945 loss: 1.61186881e-06
Iter: 946 loss: 1.61985054e-06
Iter: 947 loss: 1.61143487e-06
Iter: 948 loss: 1.61014225e-06
Iter: 949 loss: 1.61645698e-06
Iter: 950 loss: 1.61001822e-06
Iter: 951 loss: 1.60883872e-06
Iter: 952 loss: 1.60806201e-06
Iter: 953 loss: 1.60754553e-06
Iter: 954 loss: 1.60633977e-06
Iter: 955 loss: 1.62033564e-06
Iter: 956 loss: 1.60638945e-06
Iter: 957 loss: 1.60534762e-06
Iter: 958 loss: 1.60453283e-06
Iter: 959 loss: 1.60418244e-06
Iter: 960 loss: 1.60315233e-06
Iter: 961 loss: 1.60297157e-06
Iter: 962 loss: 1.60230206e-06
Iter: 963 loss: 1.60298441e-06
Iter: 964 loss: 1.60197681e-06
Iter: 965 loss: 1.60114939e-06
Iter: 966 loss: 1.60052423e-06
Iter: 967 loss: 1.60038076e-06
Iter: 968 loss: 1.59875628e-06
Iter: 969 loss: 1.59842114e-06
Iter: 970 loss: 1.59751676e-06
Iter: 971 loss: 1.59615104e-06
Iter: 972 loss: 1.6132883e-06
Iter: 973 loss: 1.59628075e-06
Iter: 974 loss: 1.59489286e-06
Iter: 975 loss: 1.59345632e-06
Iter: 976 loss: 1.59335423e-06
Iter: 977 loss: 1.59153251e-06
Iter: 978 loss: 1.61064122e-06
Iter: 979 loss: 1.59146191e-06
Iter: 980 loss: 1.59009505e-06
Iter: 981 loss: 1.5919718e-06
Iter: 982 loss: 1.5893213e-06
Iter: 983 loss: 1.58759576e-06
Iter: 984 loss: 1.59783531e-06
Iter: 985 loss: 1.5874291e-06
Iter: 986 loss: 1.58638545e-06
Iter: 987 loss: 1.58655666e-06
Iter: 988 loss: 1.5854348e-06
Iter: 989 loss: 1.58409023e-06
Iter: 990 loss: 1.58784974e-06
Iter: 991 loss: 1.58357784e-06
Iter: 992 loss: 1.58242801e-06
Iter: 993 loss: 1.59258821e-06
Iter: 994 loss: 1.58239197e-06
Iter: 995 loss: 1.58120338e-06
Iter: 996 loss: 1.58558805e-06
Iter: 997 loss: 1.58096555e-06
Iter: 998 loss: 1.58011039e-06
Iter: 999 loss: 1.58010073e-06
Iter: 1000 loss: 1.57938211e-06
Iter: 1001 loss: 1.57853935e-06
Iter: 1002 loss: 1.57789714e-06
Iter: 1003 loss: 1.57763111e-06
Iter: 1004 loss: 1.57588954e-06
Iter: 1005 loss: 1.57935028e-06
Iter: 1006 loss: 1.5750943e-06
Iter: 1007 loss: 1.57390082e-06
Iter: 1008 loss: 1.58304408e-06
Iter: 1009 loss: 1.57386137e-06
Iter: 1010 loss: 1.57268812e-06
Iter: 1011 loss: 1.57198087e-06
Iter: 1012 loss: 1.57155023e-06
Iter: 1013 loss: 1.57011732e-06
Iter: 1014 loss: 1.58183889e-06
Iter: 1015 loss: 1.56988244e-06
Iter: 1016 loss: 1.56839235e-06
Iter: 1017 loss: 1.56826138e-06
Iter: 1018 loss: 1.56702208e-06
Iter: 1019 loss: 1.56491478e-06
Iter: 1020 loss: 1.58475973e-06
Iter: 1021 loss: 1.5647413e-06
Iter: 1022 loss: 1.56367969e-06
Iter: 1023 loss: 1.56287456e-06
Iter: 1024 loss: 1.56257477e-06
Iter: 1025 loss: 1.56030683e-06
Iter: 1026 loss: 1.56742044e-06
Iter: 1027 loss: 1.55968053e-06
Iter: 1028 loss: 1.55981661e-06
Iter: 1029 loss: 1.55889552e-06
Iter: 1030 loss: 1.55842486e-06
Iter: 1031 loss: 1.55717782e-06
Iter: 1032 loss: 1.56967985e-06
Iter: 1033 loss: 1.55691748e-06
Iter: 1034 loss: 1.55519638e-06
Iter: 1035 loss: 1.5560463e-06
Iter: 1036 loss: 1.5539481e-06
Iter: 1037 loss: 1.55213115e-06
Iter: 1038 loss: 1.56751344e-06
Iter: 1039 loss: 1.55199223e-06
Iter: 1040 loss: 1.55056603e-06
Iter: 1041 loss: 1.55047974e-06
Iter: 1042 loss: 1.54937368e-06
Iter: 1043 loss: 1.54759323e-06
Iter: 1044 loss: 1.56352758e-06
Iter: 1045 loss: 1.54755708e-06
Iter: 1046 loss: 1.54622876e-06
Iter: 1047 loss: 1.5450139e-06
Iter: 1048 loss: 1.54457337e-06
Iter: 1049 loss: 1.54310396e-06
Iter: 1050 loss: 1.54318423e-06
Iter: 1051 loss: 1.54226552e-06
Iter: 1052 loss: 1.54295424e-06
Iter: 1053 loss: 1.54166844e-06
Iter: 1054 loss: 1.54026452e-06
Iter: 1055 loss: 1.54214717e-06
Iter: 1056 loss: 1.53952783e-06
Iter: 1057 loss: 1.53824487e-06
Iter: 1058 loss: 1.54274267e-06
Iter: 1059 loss: 1.53794372e-06
Iter: 1060 loss: 1.53685721e-06
Iter: 1061 loss: 1.54986071e-06
Iter: 1062 loss: 1.53685482e-06
Iter: 1063 loss: 1.53580027e-06
Iter: 1064 loss: 1.53524138e-06
Iter: 1065 loss: 1.53478368e-06
Iter: 1066 loss: 1.53397127e-06
Iter: 1067 loss: 1.53514509e-06
Iter: 1068 loss: 1.53353426e-06
Iter: 1069 loss: 1.53236169e-06
Iter: 1070 loss: 1.53128735e-06
Iter: 1071 loss: 1.53102542e-06
Iter: 1072 loss: 1.52949e-06
Iter: 1073 loss: 1.55181669e-06
Iter: 1074 loss: 1.52944426e-06
Iter: 1075 loss: 1.52843359e-06
Iter: 1076 loss: 1.52777307e-06
Iter: 1077 loss: 1.52738221e-06
Iter: 1078 loss: 1.52543362e-06
Iter: 1079 loss: 1.53271662e-06
Iter: 1080 loss: 1.52498876e-06
Iter: 1081 loss: 1.52365965e-06
Iter: 1082 loss: 1.5284096e-06
Iter: 1083 loss: 1.52339737e-06
Iter: 1084 loss: 1.52193286e-06
Iter: 1085 loss: 1.52439634e-06
Iter: 1086 loss: 1.52116218e-06
Iter: 1087 loss: 1.52008249e-06
Iter: 1088 loss: 1.52711323e-06
Iter: 1089 loss: 1.51987024e-06
Iter: 1090 loss: 1.51881216e-06
Iter: 1091 loss: 1.5175408e-06
Iter: 1092 loss: 1.51741858e-06
Iter: 1093 loss: 1.51781137e-06
Iter: 1094 loss: 1.51681786e-06
Iter: 1095 loss: 1.51608936e-06
Iter: 1096 loss: 1.51580616e-06
Iter: 1097 loss: 1.51552831e-06
Iter: 1098 loss: 1.51458971e-06
Iter: 1099 loss: 1.51312724e-06
Iter: 1100 loss: 1.5130596e-06
Iter: 1101 loss: 1.51175334e-06
Iter: 1102 loss: 1.52314612e-06
Iter: 1103 loss: 1.51167342e-06
Iter: 1104 loss: 1.51057543e-06
Iter: 1105 loss: 1.51028803e-06
Iter: 1106 loss: 1.50967139e-06
Iter: 1107 loss: 1.50806818e-06
Iter: 1108 loss: 1.52187795e-06
Iter: 1109 loss: 1.5080218e-06
Iter: 1110 loss: 1.50697065e-06
Iter: 1111 loss: 1.50585367e-06
Iter: 1112 loss: 1.50569167e-06
Iter: 1113 loss: 1.50346636e-06
Iter: 1114 loss: 1.51496829e-06
Iter: 1115 loss: 1.50327628e-06
Iter: 1116 loss: 1.50215919e-06
Iter: 1117 loss: 1.5066795e-06
Iter: 1118 loss: 1.50194614e-06
Iter: 1119 loss: 1.50046844e-06
Iter: 1120 loss: 1.50029291e-06
Iter: 1121 loss: 1.49932919e-06
Iter: 1122 loss: 1.49803759e-06
Iter: 1123 loss: 1.51830977e-06
Iter: 1124 loss: 1.49793789e-06
Iter: 1125 loss: 1.49708353e-06
Iter: 1126 loss: 1.49596758e-06
Iter: 1127 loss: 1.49598316e-06
Iter: 1128 loss: 1.49514449e-06
Iter: 1129 loss: 1.49492212e-06
Iter: 1130 loss: 1.49454536e-06
Iter: 1131 loss: 1.49370567e-06
Iter: 1132 loss: 1.50537289e-06
Iter: 1133 loss: 1.49360949e-06
Iter: 1134 loss: 1.49249627e-06
Iter: 1135 loss: 1.49076595e-06
Iter: 1136 loss: 1.49075959e-06
Iter: 1137 loss: 1.4895237e-06
Iter: 1138 loss: 1.48929416e-06
Iter: 1139 loss: 1.48839558e-06
Iter: 1140 loss: 1.48598451e-06
Iter: 1141 loss: 1.51979191e-06
Iter: 1142 loss: 1.48580648e-06
Iter: 1143 loss: 1.48468109e-06
Iter: 1144 loss: 1.48453387e-06
Iter: 1145 loss: 1.48344884e-06
Iter: 1146 loss: 1.48175104e-06
Iter: 1147 loss: 1.48164668e-06
Iter: 1148 loss: 1.48011191e-06
Iter: 1149 loss: 1.49088851e-06
Iter: 1150 loss: 1.47998503e-06
Iter: 1151 loss: 1.47851745e-06
Iter: 1152 loss: 1.4817482e-06
Iter: 1153 loss: 1.47792662e-06
Iter: 1154 loss: 1.4766058e-06
Iter: 1155 loss: 1.4825489e-06
Iter: 1156 loss: 1.47646858e-06
Iter: 1157 loss: 1.47497781e-06
Iter: 1158 loss: 1.47722449e-06
Iter: 1159 loss: 1.47432559e-06
Iter: 1160 loss: 1.47409355e-06
Iter: 1161 loss: 1.47371293e-06
Iter: 1162 loss: 1.47309026e-06
Iter: 1163 loss: 1.47243372e-06
Iter: 1164 loss: 1.4725299e-06
Iter: 1165 loss: 1.47145283e-06
Iter: 1166 loss: 1.46961588e-06
Iter: 1167 loss: 1.4695886e-06
Iter: 1168 loss: 1.46817547e-06
Iter: 1169 loss: 1.48522702e-06
Iter: 1170 loss: 1.4681118e-06
Iter: 1171 loss: 1.46677621e-06
Iter: 1172 loss: 1.4652328e-06
Iter: 1173 loss: 1.46514617e-06
Iter: 1174 loss: 1.46397315e-06
Iter: 1175 loss: 1.46387799e-06
Iter: 1176 loss: 1.46268371e-06
Iter: 1177 loss: 1.46131242e-06
Iter: 1178 loss: 1.46121727e-06
Iter: 1179 loss: 1.45970114e-06
Iter: 1180 loss: 1.47305013e-06
Iter: 1181 loss: 1.45970057e-06
Iter: 1182 loss: 1.45860656e-06
Iter: 1183 loss: 1.45703598e-06
Iter: 1184 loss: 1.45690171e-06
Iter: 1185 loss: 1.45527656e-06
Iter: 1186 loss: 1.47310209e-06
Iter: 1187 loss: 1.45531021e-06
Iter: 1188 loss: 1.45397053e-06
Iter: 1189 loss: 1.45989122e-06
Iter: 1190 loss: 1.45373224e-06
Iter: 1191 loss: 1.45275703e-06
Iter: 1192 loss: 1.46286709e-06
Iter: 1193 loss: 1.45279523e-06
Iter: 1194 loss: 1.4521363e-06
Iter: 1195 loss: 1.45316199e-06
Iter: 1196 loss: 1.45169508e-06
Iter: 1197 loss: 1.45096442e-06
Iter: 1198 loss: 1.45012439e-06
Iter: 1199 loss: 1.45006788e-06
Iter: 1200 loss: 1.44881062e-06
Iter: 1201 loss: 1.4530001e-06
Iter: 1202 loss: 1.44853857e-06
Iter: 1203 loss: 1.44763032e-06
Iter: 1204 loss: 1.44711544e-06
Iter: 1205 loss: 1.44666024e-06
Iter: 1206 loss: 1.44512023e-06
Iter: 1207 loss: 1.45722993e-06
Iter: 1208 loss: 1.44506168e-06
Iter: 1209 loss: 1.44411331e-06
Iter: 1210 loss: 1.44610965e-06
Iter: 1211 loss: 1.44387377e-06
Iter: 1212 loss: 1.44272235e-06
Iter: 1213 loss: 1.44290232e-06
Iter: 1214 loss: 1.44160424e-06
Iter: 1215 loss: 1.44031083e-06
Iter: 1216 loss: 1.44510375e-06
Iter: 1217 loss: 1.43998068e-06
Iter: 1218 loss: 1.43820876e-06
Iter: 1219 loss: 1.43649186e-06
Iter: 1220 loss: 1.43616739e-06
Iter: 1221 loss: 1.43484499e-06
Iter: 1222 loss: 1.4347811e-06
Iter: 1223 loss: 1.43377872e-06
Iter: 1224 loss: 1.43723423e-06
Iter: 1225 loss: 1.43347415e-06
Iter: 1226 loss: 1.43210355e-06
Iter: 1227 loss: 1.43890952e-06
Iter: 1228 loss: 1.43189322e-06
Iter: 1229 loss: 1.43106638e-06
Iter: 1230 loss: 1.43078159e-06
Iter: 1231 loss: 1.43033094e-06
Iter: 1232 loss: 1.42889803e-06
Iter: 1233 loss: 1.42772433e-06
Iter: 1234 loss: 1.42738645e-06
Iter: 1235 loss: 1.42549095e-06
Iter: 1236 loss: 1.4373627e-06
Iter: 1237 loss: 1.42542694e-06
Iter: 1238 loss: 1.42359943e-06
Iter: 1239 loss: 1.42502e-06
Iter: 1240 loss: 1.4227337e-06
Iter: 1241 loss: 1.42137765e-06
Iter: 1242 loss: 1.43516468e-06
Iter: 1243 loss: 1.42119393e-06
Iter: 1244 loss: 1.4200134e-06
Iter: 1245 loss: 1.42027739e-06
Iter: 1246 loss: 1.41914938e-06
Iter: 1247 loss: 1.41717464e-06
Iter: 1248 loss: 1.42594217e-06
Iter: 1249 loss: 1.4168952e-06
Iter: 1250 loss: 1.4155703e-06
Iter: 1251 loss: 1.41466353e-06
Iter: 1252 loss: 1.41426824e-06
Iter: 1253 loss: 1.41194801e-06
Iter: 1254 loss: 1.42749218e-06
Iter: 1255 loss: 1.41178907e-06
Iter: 1256 loss: 1.41053772e-06
Iter: 1257 loss: 1.41580222e-06
Iter: 1258 loss: 1.41030523e-06
Iter: 1259 loss: 1.40880775e-06
Iter: 1260 loss: 1.41713565e-06
Iter: 1261 loss: 1.4084718e-06
Iter: 1262 loss: 1.40747488e-06
Iter: 1263 loss: 1.40748637e-06
Iter: 1264 loss: 1.40649445e-06
Iter: 1265 loss: 1.40555017e-06
Iter: 1266 loss: 1.40647978e-06
Iter: 1267 loss: 1.40506916e-06
Iter: 1268 loss: 1.40361271e-06
Iter: 1269 loss: 1.40203872e-06
Iter: 1270 loss: 1.4017769e-06
Iter: 1271 loss: 1.40021496e-06
Iter: 1272 loss: 1.41443775e-06
Iter: 1273 loss: 1.40027942e-06
Iter: 1274 loss: 1.39883502e-06
Iter: 1275 loss: 1.39764302e-06
Iter: 1276 loss: 1.39717463e-06
Iter: 1277 loss: 1.39553958e-06
Iter: 1278 loss: 1.39560871e-06
Iter: 1279 loss: 1.39408235e-06
Iter: 1280 loss: 1.39387748e-06
Iter: 1281 loss: 1.39300687e-06
Iter: 1282 loss: 1.39133363e-06
Iter: 1283 loss: 1.40221744e-06
Iter: 1284 loss: 1.39116264e-06
Iter: 1285 loss: 1.39006261e-06
Iter: 1286 loss: 1.38770179e-06
Iter: 1287 loss: 1.43137663e-06
Iter: 1288 loss: 1.38763914e-06
Iter: 1289 loss: 1.38599808e-06
Iter: 1290 loss: 1.38597909e-06
Iter: 1291 loss: 1.38473638e-06
Iter: 1292 loss: 1.39465146e-06
Iter: 1293 loss: 1.38462133e-06
Iter: 1294 loss: 1.38321366e-06
Iter: 1295 loss: 1.39059023e-06
Iter: 1296 loss: 1.3829964e-06
Iter: 1297 loss: 1.38242308e-06
Iter: 1298 loss: 1.38254813e-06
Iter: 1299 loss: 1.38209043e-06
Iter: 1300 loss: 1.38131782e-06
Iter: 1301 loss: 1.38023358e-06
Iter: 1302 loss: 1.38016298e-06
Iter: 1303 loss: 1.37899144e-06
Iter: 1304 loss: 1.39550434e-06
Iter: 1305 loss: 1.3788565e-06
Iter: 1306 loss: 1.37817e-06
Iter: 1307 loss: 1.3769004e-06
Iter: 1308 loss: 1.37690699e-06
Iter: 1309 loss: 1.37544305e-06
Iter: 1310 loss: 1.39114468e-06
Iter: 1311 loss: 1.37526195e-06
Iter: 1312 loss: 1.37432346e-06
Iter: 1313 loss: 1.37614893e-06
Iter: 1314 loss: 1.37390793e-06
Iter: 1315 loss: 1.37260531e-06
Iter: 1316 loss: 1.37450058e-06
Iter: 1317 loss: 1.37205075e-06
Iter: 1318 loss: 1.37117638e-06
Iter: 1319 loss: 1.37264897e-06
Iter: 1320 loss: 1.37080815e-06
Iter: 1321 loss: 1.3694721e-06
Iter: 1322 loss: 1.36871552e-06
Iter: 1323 loss: 1.36826247e-06
Iter: 1324 loss: 1.36674294e-06
Iter: 1325 loss: 1.3775915e-06
Iter: 1326 loss: 1.36663903e-06
Iter: 1327 loss: 1.36591098e-06
Iter: 1328 loss: 1.36582e-06
Iter: 1329 loss: 1.36501762e-06
Iter: 1330 loss: 1.36498875e-06
Iter: 1331 loss: 1.36449125e-06
Iter: 1332 loss: 1.36390179e-06
Iter: 1333 loss: 1.36333142e-06
Iter: 1334 loss: 1.36327367e-06
Iter: 1335 loss: 1.36201379e-06
Iter: 1336 loss: 1.36318135e-06
Iter: 1337 loss: 1.36123424e-06
Iter: 1338 loss: 1.36040217e-06
Iter: 1339 loss: 1.36669848e-06
Iter: 1340 loss: 1.36032622e-06
Iter: 1341 loss: 1.35945083e-06
Iter: 1342 loss: 1.35754e-06
Iter: 1343 loss: 1.39314398e-06
Iter: 1344 loss: 1.35754385e-06
Iter: 1345 loss: 1.3567826e-06
Iter: 1346 loss: 1.35652306e-06
Iter: 1347 loss: 1.35575965e-06
Iter: 1348 loss: 1.35473681e-06
Iter: 1349 loss: 1.3547758e-06
Iter: 1350 loss: 1.35333062e-06
Iter: 1351 loss: 1.35334881e-06
Iter: 1352 loss: 1.35248968e-06
Iter: 1353 loss: 1.35080143e-06
Iter: 1354 loss: 1.38906444e-06
Iter: 1355 loss: 1.35081973e-06
Iter: 1356 loss: 1.34861421e-06
Iter: 1357 loss: 1.35609969e-06
Iter: 1358 loss: 1.34793584e-06
Iter: 1359 loss: 1.34685138e-06
Iter: 1360 loss: 1.36016081e-06
Iter: 1361 loss: 1.34689481e-06
Iter: 1362 loss: 1.34557342e-06
Iter: 1363 loss: 1.34937909e-06
Iter: 1364 loss: 1.34510424e-06
Iter: 1365 loss: 1.34404172e-06
Iter: 1366 loss: 1.34366314e-06
Iter: 1367 loss: 1.34316929e-06
Iter: 1368 loss: 1.34227218e-06
Iter: 1369 loss: 1.34365359e-06
Iter: 1370 loss: 1.3418969e-06
Iter: 1371 loss: 1.34059633e-06
Iter: 1372 loss: 1.34121865e-06
Iter: 1373 loss: 1.33976312e-06
Iter: 1374 loss: 1.3388069e-06
Iter: 1375 loss: 1.34500897e-06
Iter: 1376 loss: 1.33858543e-06
Iter: 1377 loss: 1.33742651e-06
Iter: 1378 loss: 1.33722688e-06
Iter: 1379 loss: 1.33654544e-06
Iter: 1380 loss: 1.33545211e-06
Iter: 1381 loss: 1.34361744e-06
Iter: 1382 loss: 1.33530307e-06
Iter: 1383 loss: 1.3341687e-06
Iter: 1384 loss: 1.33275284e-06
Iter: 1385 loss: 1.33259732e-06
Iter: 1386 loss: 1.3313379e-06
Iter: 1387 loss: 1.33133176e-06
Iter: 1388 loss: 1.33032233e-06
Iter: 1389 loss: 1.32914329e-06
Iter: 1390 loss: 1.32896196e-06
Iter: 1391 loss: 1.32738535e-06
Iter: 1392 loss: 1.34445418e-06
Iter: 1393 loss: 1.32737614e-06
Iter: 1394 loss: 1.32685841e-06
Iter: 1395 loss: 1.32682078e-06
Iter: 1396 loss: 1.3263525e-06
Iter: 1397 loss: 1.32506523e-06
Iter: 1398 loss: 1.33513379e-06
Iter: 1399 loss: 1.3248241e-06
Iter: 1400 loss: 1.3236056e-06
Iter: 1401 loss: 1.33299875e-06
Iter: 1402 loss: 1.32348623e-06
Iter: 1403 loss: 1.32270782e-06
Iter: 1404 loss: 1.32160164e-06
Iter: 1405 loss: 1.32156765e-06
Iter: 1406 loss: 1.31971763e-06
Iter: 1407 loss: 1.33123831e-06
Iter: 1408 loss: 1.31952993e-06
Iter: 1409 loss: 1.31844274e-06
Iter: 1410 loss: 1.31774482e-06
Iter: 1411 loss: 1.31733373e-06
Iter: 1412 loss: 1.3154629e-06
Iter: 1413 loss: 1.32565583e-06
Iter: 1414 loss: 1.31532397e-06
Iter: 1415 loss: 1.31379204e-06
Iter: 1416 loss: 1.31361855e-06
Iter: 1417 loss: 1.3124702e-06
Iter: 1418 loss: 1.31074626e-06
Iter: 1419 loss: 1.33206254e-06
Iter: 1420 loss: 1.31068236e-06
Iter: 1421 loss: 1.30984029e-06
Iter: 1422 loss: 1.31377112e-06
Iter: 1423 loss: 1.30970784e-06
Iter: 1424 loss: 1.30859667e-06
Iter: 1425 loss: 1.30949752e-06
Iter: 1426 loss: 1.30790397e-06
Iter: 1427 loss: 1.30768683e-06
Iter: 1428 loss: 1.30733429e-06
Iter: 1429 loss: 1.30689637e-06
Iter: 1430 loss: 1.30645276e-06
Iter: 1431 loss: 1.30632361e-06
Iter: 1432 loss: 1.30558124e-06
Iter: 1433 loss: 1.30424269e-06
Iter: 1434 loss: 1.33420838e-06
Iter: 1435 loss: 1.30429635e-06
Iter: 1436 loss: 1.30283638e-06
Iter: 1437 loss: 1.31928846e-06
Iter: 1438 loss: 1.30272133e-06
Iter: 1439 loss: 1.3019885e-06
Iter: 1440 loss: 1.30172293e-06
Iter: 1441 loss: 1.30107173e-06
Iter: 1442 loss: 1.29978048e-06
Iter: 1443 loss: 1.30734895e-06
Iter: 1444 loss: 1.2995e-06
Iter: 1445 loss: 1.29882255e-06
Iter: 1446 loss: 1.2975222e-06
Iter: 1447 loss: 1.29754312e-06
Iter: 1448 loss: 1.29664522e-06
Iter: 1449 loss: 1.31297929e-06
Iter: 1450 loss: 1.29662897e-06
Iter: 1451 loss: 1.29572231e-06
Iter: 1452 loss: 1.29588614e-06
Iter: 1453 loss: 1.2950436e-06
Iter: 1454 loss: 1.29417651e-06
Iter: 1455 loss: 1.29850662e-06
Iter: 1456 loss: 1.29390264e-06
Iter: 1457 loss: 1.29327873e-06
Iter: 1458 loss: 1.29254727e-06
Iter: 1459 loss: 1.29234786e-06
Iter: 1460 loss: 1.29130967e-06
Iter: 1461 loss: 1.29127352e-06
Iter: 1462 loss: 1.29067087e-06
Iter: 1463 loss: 1.29064131e-06
Iter: 1464 loss: 1.29020418e-06
Iter: 1465 loss: 1.28940042e-06
Iter: 1466 loss: 1.28930878e-06
Iter: 1467 loss: 1.28840315e-06
Iter: 1468 loss: 1.29015962e-06
Iter: 1469 loss: 1.28785769e-06
Iter: 1470 loss: 1.28706347e-06
Iter: 1471 loss: 1.28750651e-06
Iter: 1472 loss: 1.28660872e-06
Iter: 1473 loss: 1.28528222e-06
Iter: 1474 loss: 1.28943304e-06
Iter: 1475 loss: 1.28490399e-06
Iter: 1476 loss: 1.28424358e-06
Iter: 1477 loss: 1.28761008e-06
Iter: 1478 loss: 1.28408442e-06
Iter: 1479 loss: 1.2832063e-06
Iter: 1480 loss: 1.28274519e-06
Iter: 1481 loss: 1.28230567e-06
Iter: 1482 loss: 1.28141028e-06
Iter: 1483 loss: 1.28368458e-06
Iter: 1484 loss: 1.28112083e-06
Iter: 1485 loss: 1.27988869e-06
Iter: 1486 loss: 1.2834405e-06
Iter: 1487 loss: 1.27943281e-06
Iter: 1488 loss: 1.2787383e-06
Iter: 1489 loss: 1.28067359e-06
Iter: 1490 loss: 1.27845055e-06
Iter: 1491 loss: 1.27749422e-06
Iter: 1492 loss: 1.28102806e-06
Iter: 1493 loss: 1.27722456e-06
Iter: 1494 loss: 1.27723183e-06
Iter: 1495 loss: 1.276891e-06
Iter: 1496 loss: 1.2766194e-06
Iter: 1497 loss: 1.27617363e-06
Iter: 1498 loss: 1.27618409e-06
Iter: 1499 loss: 1.27551982e-06
Iter: 1500 loss: 1.27566204e-06
Iter: 1501 loss: 1.27499879e-06
Iter: 1502 loss: 1.27433509e-06
Iter: 1503 loss: 1.27432077e-06
Iter: 1504 loss: 1.27390331e-06
Iter: 1505 loss: 1.27283488e-06
Iter: 1506 loss: 1.27731903e-06
Iter: 1507 loss: 1.27260364e-06
Iter: 1508 loss: 1.27194244e-06
Iter: 1509 loss: 1.27327917e-06
Iter: 1510 loss: 1.27153214e-06
Iter: 1511 loss: 1.2705716e-06
Iter: 1512 loss: 1.27238309e-06
Iter: 1513 loss: 1.27001283e-06
Iter: 1514 loss: 1.26915756e-06
Iter: 1515 loss: 1.26885232e-06
Iter: 1516 loss: 1.26829673e-06
Iter: 1517 loss: 1.26702344e-06
Iter: 1518 loss: 1.27851149e-06
Iter: 1519 loss: 1.26689497e-06
Iter: 1520 loss: 1.26612065e-06
Iter: 1521 loss: 1.26594409e-06
Iter: 1522 loss: 1.26537634e-06
Iter: 1523 loss: 1.26417831e-06
Iter: 1524 loss: 1.26954342e-06
Iter: 1525 loss: 1.26391944e-06
Iter: 1526 loss: 1.26316195e-06
Iter: 1527 loss: 1.26405155e-06
Iter: 1528 loss: 1.26279861e-06
Iter: 1529 loss: 1.26186148e-06
Iter: 1530 loss: 1.26186296e-06
Iter: 1531 loss: 1.2611406e-06
Iter: 1532 loss: 1.26036707e-06
Iter: 1533 loss: 1.26024656e-06
Iter: 1534 loss: 1.25941381e-06
Iter: 1535 loss: 1.25952511e-06
Iter: 1536 loss: 1.25875681e-06
Iter: 1537 loss: 1.25725524e-06
Iter: 1538 loss: 1.25827933e-06
Iter: 1539 loss: 1.25645147e-06
Iter: 1540 loss: 1.25536394e-06
Iter: 1541 loss: 1.27111457e-06
Iter: 1542 loss: 1.25537849e-06
Iter: 1543 loss: 1.25460588e-06
Iter: 1544 loss: 1.25378506e-06
Iter: 1545 loss: 1.2536882e-06
Iter: 1546 loss: 1.25237034e-06
Iter: 1547 loss: 1.26592261e-06
Iter: 1548 loss: 1.25241013e-06
Iter: 1549 loss: 1.25148699e-06
Iter: 1550 loss: 1.25008455e-06
Iter: 1551 loss: 1.28705301e-06
Iter: 1552 loss: 1.25004533e-06
Iter: 1553 loss: 1.24898372e-06
Iter: 1554 loss: 1.24893177e-06
Iter: 1555 loss: 1.24805183e-06
Iter: 1556 loss: 1.24675796e-06
Iter: 1557 loss: 1.24672806e-06
Iter: 1558 loss: 1.24548114e-06
Iter: 1559 loss: 1.25933116e-06
Iter: 1560 loss: 1.24559506e-06
Iter: 1561 loss: 1.24492726e-06
Iter: 1562 loss: 1.24486712e-06
Iter: 1563 loss: 1.24421e-06
Iter: 1564 loss: 1.24508085e-06
Iter: 1565 loss: 1.24384133e-06
Iter: 1566 loss: 1.24341784e-06
Iter: 1567 loss: 1.24315181e-06
Iter: 1568 loss: 1.24296571e-06
Iter: 1569 loss: 1.24207259e-06
Iter: 1570 loss: 1.24272617e-06
Iter: 1571 loss: 1.24164058e-06
Iter: 1572 loss: 1.24098574e-06
Iter: 1573 loss: 1.24687131e-06
Iter: 1574 loss: 1.24098801e-06
Iter: 1575 loss: 1.24042697e-06
Iter: 1576 loss: 1.24000474e-06
Iter: 1577 loss: 1.23980067e-06
Iter: 1578 loss: 1.23896314e-06
Iter: 1579 loss: 1.24640349e-06
Iter: 1580 loss: 1.23893255e-06
Iter: 1581 loss: 1.23822849e-06
Iter: 1582 loss: 1.23726488e-06
Iter: 1583 loss: 1.23720588e-06
Iter: 1584 loss: 1.236191e-06
Iter: 1585 loss: 1.24634619e-06
Iter: 1586 loss: 1.23614836e-06
Iter: 1587 loss: 1.23531777e-06
Iter: 1588 loss: 1.23457562e-06
Iter: 1589 loss: 1.23420079e-06
Iter: 1590 loss: 1.23336486e-06
Iter: 1591 loss: 1.24461394e-06
Iter: 1592 loss: 1.23340408e-06
Iter: 1593 loss: 1.23275981e-06
Iter: 1594 loss: 1.23172742e-06
Iter: 1595 loss: 1.25803149e-06
Iter: 1596 loss: 1.2316915e-06
Iter: 1597 loss: 1.2316832e-06
Iter: 1598 loss: 1.23107975e-06
Iter: 1599 loss: 1.23073085e-06
Iter: 1600 loss: 1.22989638e-06
Iter: 1601 loss: 1.24437156e-06
Iter: 1602 loss: 1.22997631e-06
Iter: 1603 loss: 1.22902e-06
Iter: 1604 loss: 1.22839947e-06
Iter: 1605 loss: 1.22817028e-06
Iter: 1606 loss: 1.2271073e-06
Iter: 1607 loss: 1.23949189e-06
Iter: 1608 loss: 1.22705978e-06
Iter: 1609 loss: 1.22605411e-06
Iter: 1610 loss: 1.22651272e-06
Iter: 1611 loss: 1.22542031e-06
Iter: 1612 loss: 1.22446295e-06
Iter: 1613 loss: 1.22972642e-06
Iter: 1614 loss: 1.22435006e-06
Iter: 1615 loss: 1.2234218e-06
Iter: 1616 loss: 1.22394135e-06
Iter: 1617 loss: 1.22289714e-06
Iter: 1618 loss: 1.22194956e-06
Iter: 1619 loss: 1.23154177e-06
Iter: 1620 loss: 1.22191125e-06
Iter: 1621 loss: 1.22131382e-06
Iter: 1622 loss: 1.22039148e-06
Iter: 1623 loss: 1.22034089e-06
Iter: 1624 loss: 1.21914786e-06
Iter: 1625 loss: 1.23650466e-06
Iter: 1626 loss: 1.21911785e-06
Iter: 1627 loss: 1.21860057e-06
Iter: 1628 loss: 1.22352435e-06
Iter: 1629 loss: 1.2185576e-06
Iter: 1630 loss: 1.21817641e-06
Iter: 1631 loss: 1.2232033e-06
Iter: 1632 loss: 1.21822586e-06
Iter: 1633 loss: 1.21797086e-06
Iter: 1634 loss: 1.21761821e-06
Iter: 1635 loss: 1.21755966e-06
Iter: 1636 loss: 1.21716448e-06
Iter: 1637 loss: 1.2162343e-06
Iter: 1638 loss: 1.23191785e-06
Iter: 1639 loss: 1.2162925e-06
Iter: 1640 loss: 1.2155358e-06
Iter: 1641 loss: 1.21550931e-06
Iter: 1642 loss: 1.21481321e-06
Iter: 1643 loss: 1.21461574e-06
Iter: 1644 loss: 1.21410267e-06
Iter: 1645 loss: 1.21321136e-06
Iter: 1646 loss: 1.22114068e-06
Iter: 1647 loss: 1.21314156e-06
Iter: 1648 loss: 1.2125322e-06
Iter: 1649 loss: 1.21211156e-06
Iter: 1650 loss: 1.21194159e-06
Iter: 1651 loss: 1.21127e-06
Iter: 1652 loss: 1.21950484e-06
Iter: 1653 loss: 1.21130643e-06
Iter: 1654 loss: 1.21081086e-06
Iter: 1655 loss: 1.21015e-06
Iter: 1656 loss: 1.21001563e-06
Iter: 1657 loss: 1.20910693e-06
Iter: 1658 loss: 1.21970243e-06
Iter: 1659 loss: 1.20915604e-06
Iter: 1660 loss: 1.20856362e-06
Iter: 1661 loss: 1.2102829e-06
Iter: 1662 loss: 1.20844675e-06
Iter: 1663 loss: 1.20804066e-06
Iter: 1664 loss: 1.20817037e-06
Iter: 1665 loss: 1.20780362e-06
Iter: 1666 loss: 1.2070127e-06
Iter: 1667 loss: 1.21428161e-06
Iter: 1668 loss: 1.20688605e-06
Iter: 1669 loss: 1.2058108e-06
Iter: 1670 loss: 1.20581967e-06
Iter: 1671 loss: 1.20491006e-06
Iter: 1672 loss: 1.20408208e-06
Iter: 1673 loss: 1.21133144e-06
Iter: 1674 loss: 1.20404081e-06
Iter: 1675 loss: 1.20329389e-06
Iter: 1676 loss: 1.20381605e-06
Iter: 1677 loss: 1.20287405e-06
Iter: 1678 loss: 1.20216816e-06
Iter: 1679 loss: 1.2108892e-06
Iter: 1680 loss: 1.20212621e-06
Iter: 1681 loss: 1.20159814e-06
Iter: 1682 loss: 1.20110712e-06
Iter: 1683 loss: 1.2010247e-06
Iter: 1684 loss: 1.20022492e-06
Iter: 1685 loss: 1.20741629e-06
Iter: 1686 loss: 1.2002547e-06
Iter: 1687 loss: 1.19963306e-06
Iter: 1688 loss: 1.19909782e-06
Iter: 1689 loss: 1.19894e-06
Iter: 1690 loss: 1.19809351e-06
Iter: 1691 loss: 1.20891207e-06
Iter: 1692 loss: 1.19810807e-06
Iter: 1693 loss: 1.19750507e-06
Iter: 1694 loss: 1.19750939e-06
Iter: 1695 loss: 1.19702088e-06
Iter: 1696 loss: 1.19666288e-06
Iter: 1697 loss: 1.19656738e-06
Iter: 1698 loss: 1.19612753e-06
Iter: 1699 loss: 1.19619654e-06
Iter: 1700 loss: 1.19570473e-06
Iter: 1701 loss: 1.19525498e-06
Iter: 1702 loss: 1.19463016e-06
Iter: 1703 loss: 1.19469121e-06
Iter: 1704 loss: 1.19353581e-06
Iter: 1705 loss: 1.1958366e-06
Iter: 1706 loss: 1.19300773e-06
Iter: 1707 loss: 1.19201354e-06
Iter: 1708 loss: 1.19719061e-06
Iter: 1709 loss: 1.19190622e-06
Iter: 1710 loss: 1.19139941e-06
Iter: 1711 loss: 1.19305037e-06
Iter: 1712 loss: 1.19138633e-06
Iter: 1713 loss: 1.19076844e-06
Iter: 1714 loss: 1.1905131e-06
Iter: 1715 loss: 1.19014e-06
Iter: 1716 loss: 1.18952823e-06
Iter: 1717 loss: 1.19367212e-06
Iter: 1718 loss: 1.18947628e-06
Iter: 1719 loss: 1.18881326e-06
Iter: 1720 loss: 1.18871458e-06
Iter: 1721 loss: 1.18814785e-06
Iter: 1722 loss: 1.1875228e-06
Iter: 1723 loss: 1.19224478e-06
Iter: 1724 loss: 1.18741298e-06
Iter: 1725 loss: 1.18668822e-06
Iter: 1726 loss: 1.1866623e-06
Iter: 1727 loss: 1.18618846e-06
Iter: 1728 loss: 1.18587081e-06
Iter: 1729 loss: 1.18562139e-06
Iter: 1730 loss: 1.18525088e-06
Iter: 1731 loss: 1.18679304e-06
Iter: 1732 loss: 1.18505761e-06
Iter: 1733 loss: 1.18479215e-06
Iter: 1734 loss: 1.18399635e-06
Iter: 1735 loss: 1.18924527e-06
Iter: 1736 loss: 1.18387732e-06
Iter: 1737 loss: 1.18299192e-06
Iter: 1738 loss: 1.18830621e-06
Iter: 1739 loss: 1.18288654e-06
Iter: 1740 loss: 1.18226501e-06
Iter: 1741 loss: 1.18311482e-06
Iter: 1742 loss: 1.18187768e-06
Iter: 1743 loss: 1.18129378e-06
Iter: 1744 loss: 1.18417483e-06
Iter: 1745 loss: 1.18109097e-06
Iter: 1746 loss: 1.18043545e-06
Iter: 1747 loss: 1.18374828e-06
Iter: 1748 loss: 1.18023343e-06
Iter: 1749 loss: 1.17981722e-06
Iter: 1750 loss: 1.18166713e-06
Iter: 1751 loss: 1.17975412e-06
Iter: 1752 loss: 1.17917364e-06
Iter: 1753 loss: 1.17902312e-06
Iter: 1754 loss: 1.1786193e-06
Iter: 1755 loss: 1.1781126e-06
Iter: 1756 loss: 1.17983586e-06
Iter: 1757 loss: 1.17787067e-06
Iter: 1758 loss: 1.17728518e-06
Iter: 1759 loss: 1.17962986e-06
Iter: 1760 loss: 1.17703303e-06
Iter: 1761 loss: 1.17665741e-06
Iter: 1762 loss: 1.17934178e-06
Iter: 1763 loss: 1.17666491e-06
Iter: 1764 loss: 1.17635022e-06
Iter: 1765 loss: 1.18012986e-06
Iter: 1766 loss: 1.17630248e-06
Iter: 1767 loss: 1.17613627e-06
Iter: 1768 loss: 1.17567868e-06
Iter: 1769 loss: 1.18301591e-06
Iter: 1770 loss: 1.17570812e-06
Iter: 1771 loss: 1.17535399e-06
Iter: 1772 loss: 1.1766482e-06
Iter: 1773 loss: 1.17528589e-06
Iter: 1774 loss: 1.17486638e-06
Iter: 1775 loss: 1.17475986e-06
Iter: 1776 loss: 1.1744462e-06
Iter: 1777 loss: 1.17402374e-06
Iter: 1778 loss: 1.17628952e-06
Iter: 1779 loss: 1.1739819e-06
Iter: 1780 loss: 1.17341392e-06
Iter: 1781 loss: 1.17436025e-06
Iter: 1782 loss: 1.17322622e-06
Iter: 1783 loss: 1.17288982e-06
Iter: 1784 loss: 1.17506238e-06
Iter: 1785 loss: 1.17279342e-06
Iter: 1786 loss: 1.17234492e-06
Iter: 1787 loss: 1.17273248e-06
Iter: 1788 loss: 1.17201648e-06
Iter: 1789 loss: 1.17156276e-06
Iter: 1790 loss: 1.17336049e-06
Iter: 1791 loss: 1.17140962e-06
Iter: 1792 loss: 1.17080128e-06
Iter: 1793 loss: 1.17155719e-06
Iter: 1794 loss: 1.17062973e-06
Iter: 1795 loss: 1.170225e-06
Iter: 1796 loss: 1.17181662e-06
Iter: 1797 loss: 1.1701552e-06
Iter: 1798 loss: 1.16974502e-06
Iter: 1799 loss: 1.17077263e-06
Iter: 1800 loss: 1.16959518e-06
Iter: 1801 loss: 1.16952458e-06
Iter: 1802 loss: 1.16934893e-06
Iter: 1803 loss: 1.1692357e-06
Iter: 1804 loss: 1.16894944e-06
Iter: 1805 loss: 1.17072045e-06
Iter: 1806 loss: 1.16894421e-06
Iter: 1807 loss: 1.16853766e-06
Iter: 1808 loss: 1.16811339e-06
Iter: 1809 loss: 1.16789988e-06
Iter: 1810 loss: 1.16754654e-06
Iter: 1811 loss: 1.16750402e-06
Iter: 1812 loss: 1.16728779e-06
Iter: 1813 loss: 1.16724129e-06
Iter: 1814 loss: 1.16697902e-06
Iter: 1815 loss: 1.16660408e-06
Iter: 1816 loss: 1.16874548e-06
Iter: 1817 loss: 1.16655383e-06
Iter: 1818 loss: 1.1661366e-06
Iter: 1819 loss: 1.16634465e-06
Iter: 1820 loss: 1.16586079e-06
Iter: 1821 loss: 1.16522551e-06
Iter: 1822 loss: 1.16869023e-06
Iter: 1823 loss: 1.165193e-06
Iter: 1824 loss: 1.16494084e-06
Iter: 1825 loss: 1.16553497e-06
Iter: 1826 loss: 1.16480192e-06
Iter: 1827 loss: 1.16439969e-06
Iter: 1828 loss: 1.16503668e-06
Iter: 1829 loss: 1.16420142e-06
Iter: 1830 loss: 1.16361468e-06
Iter: 1831 loss: 1.1647295e-06
Iter: 1832 loss: 1.16341255e-06
Iter: 1833 loss: 1.16295087e-06
Iter: 1834 loss: 1.16553e-06
Iter: 1835 loss: 1.16289027e-06
Iter: 1836 loss: 1.1625948e-06
Iter: 1837 loss: 1.16261833e-06
Iter: 1838 loss: 1.16233048e-06
Iter: 1839 loss: 1.16173305e-06
Iter: 1840 loss: 1.1699035e-06
Iter: 1841 loss: 1.16163437e-06
Iter: 1842 loss: 1.16126955e-06
Iter: 1843 loss: 1.16286742e-06
Iter: 1844 loss: 1.1610623e-06
Iter: 1845 loss: 1.16073147e-06
Iter: 1846 loss: 1.16051319e-06
Iter: 1847 loss: 1.16031765e-06
Iter: 1848 loss: 1.1599102e-06
Iter: 1849 loss: 1.16460421e-06
Iter: 1850 loss: 1.15992975e-06
Iter: 1851 loss: 1.1595223e-06
Iter: 1852 loss: 1.15944124e-06
Iter: 1853 loss: 1.15922853e-06
Iter: 1854 loss: 1.15867351e-06
Iter: 1855 loss: 1.16156843e-06
Iter: 1856 loss: 1.15867692e-06
Iter: 1857 loss: 1.15832609e-06
Iter: 1858 loss: 1.15853334e-06
Iter: 1859 loss: 1.15810064e-06
Iter: 1860 loss: 1.15749299e-06
Iter: 1861 loss: 1.15994033e-06
Iter: 1862 loss: 1.15744251e-06
Iter: 1863 loss: 1.15700936e-06
Iter: 1864 loss: 1.15836554e-06
Iter: 1865 loss: 1.1567995e-06
Iter: 1866 loss: 1.1563169e-06
Iter: 1867 loss: 1.15718967e-06
Iter: 1868 loss: 1.15611147e-06
Iter: 1869 loss: 1.15551711e-06
Iter: 1870 loss: 1.158685e-06
Iter: 1871 loss: 1.15538865e-06
Iter: 1872 loss: 1.1551906e-06
Iter: 1873 loss: 1.15512898e-06
Iter: 1874 loss: 1.15496084e-06
Iter: 1875 loss: 1.15471198e-06
Iter: 1876 loss: 1.15652836e-06
Iter: 1877 loss: 1.15457738e-06
Iter: 1878 loss: 1.15412661e-06
Iter: 1879 loss: 1.15395642e-06
Iter: 1880 loss: 1.15358034e-06
Iter: 1881 loss: 1.15322223e-06
Iter: 1882 loss: 1.15761077e-06
Iter: 1883 loss: 1.15326782e-06
Iter: 1884 loss: 1.15270529e-06
Iter: 1885 loss: 1.15294756e-06
Iter: 1886 loss: 1.15252499e-06
Iter: 1887 loss: 1.15197236e-06
Iter: 1888 loss: 1.15443038e-06
Iter: 1889 loss: 1.15194348e-06
Iter: 1890 loss: 1.15149214e-06
Iter: 1891 loss: 1.15160231e-06
Iter: 1892 loss: 1.15114585e-06
Iter: 1893 loss: 1.15057117e-06
Iter: 1894 loss: 1.15336695e-06
Iter: 1895 loss: 1.15045566e-06
Iter: 1896 loss: 1.1499211e-06
Iter: 1897 loss: 1.15008595e-06
Iter: 1898 loss: 1.14951683e-06
Iter: 1899 loss: 1.1487889e-06
Iter: 1900 loss: 1.1513049e-06
Iter: 1901 loss: 1.14858358e-06
Iter: 1902 loss: 1.14794989e-06
Iter: 1903 loss: 1.15088767e-06
Iter: 1904 loss: 1.14788349e-06
Iter: 1905 loss: 1.147636e-06
Iter: 1906 loss: 1.14764862e-06
Iter: 1907 loss: 1.14739794e-06
Iter: 1908 loss: 1.14708382e-06
Iter: 1909 loss: 1.14709792e-06
Iter: 1910 loss: 1.14662714e-06
Iter: 1911 loss: 1.14642853e-06
Iter: 1912 loss: 1.14618024e-06
Iter: 1913 loss: 1.14575698e-06
Iter: 1914 loss: 1.14692853e-06
Iter: 1915 loss: 1.14558702e-06
Iter: 1916 loss: 1.14493662e-06
Iter: 1917 loss: 1.1458211e-06
Iter: 1918 loss: 1.14457612e-06
Iter: 1919 loss: 1.14409249e-06
Iter: 1920 loss: 1.14617819e-06
Iter: 1921 loss: 1.14402292e-06
Iter: 1922 loss: 1.14349552e-06
Iter: 1923 loss: 1.14393845e-06
Iter: 1924 loss: 1.1431539e-06
Iter: 1925 loss: 1.1425476e-06
Iter: 1926 loss: 1.14587783e-06
Iter: 1927 loss: 1.14239378e-06
Iter: 1928 loss: 1.14197485e-06
Iter: 1929 loss: 1.1421771e-06
Iter: 1930 loss: 1.14176294e-06
Iter: 1931 loss: 1.14107741e-06
Iter: 1932 loss: 1.14553632e-06
Iter: 1933 loss: 1.14100226e-06
Iter: 1934 loss: 1.14052239e-06
Iter: 1935 loss: 1.14110082e-06
Iter: 1936 loss: 1.14040893e-06
Iter: 1937 loss: 1.14002762e-06
Iter: 1938 loss: 1.14309137e-06
Iter: 1939 loss: 1.13984117e-06
Iter: 1940 loss: 1.13960346e-06
Iter: 1941 loss: 1.14428224e-06
Iter: 1942 loss: 1.13956514e-06
Iter: 1943 loss: 1.13930855e-06
Iter: 1944 loss: 1.13890394e-06
Iter: 1945 loss: 1.14556019e-06
Iter: 1946 loss: 1.13885869e-06
Iter: 1947 loss: 1.13860369e-06
Iter: 1948 loss: 1.13896363e-06
Iter: 1949 loss: 1.13832516e-06
Iter: 1950 loss: 1.13774468e-06
Iter: 1951 loss: 1.13972e-06
Iter: 1952 loss: 1.13761507e-06
Iter: 1953 loss: 1.13716578e-06
Iter: 1954 loss: 1.13766248e-06
Iter: 1955 loss: 1.13694159e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/500_500_500_500_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi3
+ date
Thu Oct 22 12:52:16 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/500_500_500_500_1 --function f1 --psi 2 --phi 3 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3/ --save_name 500_500_500_500_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5e0b2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5e2bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5e2bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5da8d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5d71510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5d058c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5c54158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5c86510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5d71b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5d719d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5c229d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5bc38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5bc3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5b6a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5cc9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5cba048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5cba2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5b221e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5ac2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5b6a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5b43950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5a6e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9b5a7f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9515b9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9515b9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe951588620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe95149b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9514a1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe95152c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe951529f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe92c550510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe92c57e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe92c57e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe92c586598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe92c4d8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe951512510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.021275725
test_loss: 0.020842426
train_loss: 0.011267587
test_loss: 0.012370709
train_loss: 0.0072510634
test_loss: 0.009972201
train_loss: 0.007436293
test_loss: 0.009401438
train_loss: 0.006492368
test_loss: 0.009593663
train_loss: 0.005978459
test_loss: 0.009057888
train_loss: 0.0063542193
test_loss: 0.008535062
train_loss: 0.0062070074
test_loss: 0.008767892
train_loss: 0.0055554938
test_loss: 0.008748496
train_loss: 0.006111074
test_loss: 0.009050936
train_loss: 0.005991207
test_loss: 0.009431764
train_loss: 0.005219331
test_loss: 0.008282232
train_loss: 0.0053211763
test_loss: 0.008251335
train_loss: 0.0050654616
test_loss: 0.009074797
train_loss: 0.0060019037
test_loss: 0.008171064
train_loss: 0.0055290475
test_loss: 0.008682189
train_loss: 0.005416582
test_loss: 0.008432781
train_loss: 0.005257762
test_loss: 0.008138275
train_loss: 0.005364665
test_loss: 0.008994279
train_loss: 0.004657692
test_loss: 0.008345144
train_loss: 0.0053358986
test_loss: 0.007838609
train_loss: 0.0045805345
test_loss: 0.008067921
train_loss: 0.005382186
test_loss: 0.008220796
train_loss: 0.0054519284
test_loss: 0.0082114255
train_loss: 0.0051769894
test_loss: 0.008252722
train_loss: 0.004618494
test_loss: 0.008245091
train_loss: 0.0052494947
test_loss: 0.0080451565
train_loss: 0.0056816195
test_loss: 0.008394078
train_loss: 0.005193145
test_loss: 0.008977766
train_loss: 0.0050267633
test_loss: 0.008042195
train_loss: 0.005817443
test_loss: 0.008119879
train_loss: 0.0050482047
test_loss: 0.008178174
train_loss: 0.0056508156
test_loss: 0.008468443
train_loss: 0.0051207603
test_loss: 0.007946922
train_loss: 0.004648348
test_loss: 0.00801948
train_loss: 0.0045888126
test_loss: 0.007971904
train_loss: 0.0049507027
test_loss: 0.0077839918
train_loss: 0.0052630696
test_loss: 0.00830408
train_loss: 0.0047476385
test_loss: 0.008333243
train_loss: 0.005072218
test_loss: 0.007774564
train_loss: 0.0049259523
test_loss: 0.007907022
train_loss: 0.0052184267
test_loss: 0.00789364
train_loss: 0.004939419
test_loss: 0.007779275
train_loss: 0.004845484
test_loss: 0.00766439
train_loss: 0.0047826404
test_loss: 0.0076216245
train_loss: 0.004627706
test_loss: 0.007704243
train_loss: 0.00482578
test_loss: 0.00810525
train_loss: 0.004944919
test_loss: 0.007906003
train_loss: 0.005303475
test_loss: 0.0079442505
train_loss: 0.005211729
test_loss: 0.007916789
train_loss: 0.004444691
test_loss: 0.00793963
train_loss: 0.0047589354
test_loss: 0.008286458
train_loss: 0.0046935417
test_loss: 0.008221128
train_loss: 0.0046340385
test_loss: 0.007802296
train_loss: 0.004693092
test_loss: 0.008007552
train_loss: 0.00458824
test_loss: 0.007841641
train_loss: 0.005537307
test_loss: 0.0074800253
train_loss: 0.004144787
test_loss: 0.008122365
train_loss: 0.0045876224
test_loss: 0.007879322
train_loss: 0.0048755333
test_loss: 0.008124572
train_loss: 0.004881558
test_loss: 0.00764234
train_loss: 0.0047139544
test_loss: 0.0077702357
train_loss: 0.0046047284
test_loss: 0.0078039127
train_loss: 0.0049885013
test_loss: 0.0079811895
train_loss: 0.0045696716
test_loss: 0.0077670743
train_loss: 0.0042363843
test_loss: 0.0077810427
train_loss: 0.005311247
test_loss: 0.008002888
train_loss: 0.0046196505
test_loss: 0.007833733
train_loss: 0.004608709
test_loss: 0.007787342
train_loss: 0.00472557
test_loss: 0.008100711
train_loss: 0.0044573545
test_loss: 0.0077148126
train_loss: 0.0045199096
test_loss: 0.007841558
train_loss: 0.0049542775
test_loss: 0.008199574
train_loss: 0.00468303
test_loss: 0.0076848054
train_loss: 0.0050114035
test_loss: 0.007915786
train_loss: 0.0051405686
test_loss: 0.007488484
train_loss: 0.004858784
test_loss: 0.008099116
train_loss: 0.004377956
test_loss: 0.007947399
train_loss: 0.0051578903
test_loss: 0.0077469065
train_loss: 0.004380544
test_loss: 0.007819127
train_loss: 0.0047857165
test_loss: 0.007923232
train_loss: 0.0048418697
test_loss: 0.008264618
train_loss: 0.0046384814
test_loss: 0.008381095
train_loss: 0.00523053
test_loss: 0.008304426
train_loss: 0.004514106
test_loss: 0.0078119384
train_loss: 0.0049940934
test_loss: 0.007643704
train_loss: 0.0044379374
test_loss: 0.007824749
train_loss: 0.004583621
test_loss: 0.0074772355
train_loss: 0.0046354383
test_loss: 0.007766622
train_loss: 0.005515442
test_loss: 0.007962341
train_loss: 0.00418215
test_loss: 0.007884137
train_loss: 0.005101936
test_loss: 0.0074295322
train_loss: 0.004951815
test_loss: 0.00797057
train_loss: 0.0049292464
test_loss: 0.007954864
train_loss: 0.0049441946
test_loss: 0.007660545
train_loss: 0.004795682
test_loss: 0.008465459
train_loss: 0.0054079965
test_loss: 0.007487667
train_loss: 0.0045141894
test_loss: 0.0081526255
train_loss: 0.0043456242
test_loss: 0.0076736524
train_loss: 0.0046500387
test_loss: 0.008097307
train_loss: 0.0051605487
test_loss: 0.007914259
train_loss: 0.0045685167
test_loss: 0.007870079
train_loss: 0.0043322314
test_loss: 0.007948742
train_loss: 0.004169609
test_loss: 0.007559027
train_loss: 0.0044696885
test_loss: 0.007647469
train_loss: 0.0047997846
test_loss: 0.0077891047
train_loss: 0.0042631617
test_loss: 0.007549209
train_loss: 0.004550234
test_loss: 0.007531466
train_loss: 0.0044312603
test_loss: 0.008003354
train_loss: 0.0044099363
test_loss: 0.007647893
train_loss: 0.004184093
test_loss: 0.007407101
train_loss: 0.004341075
test_loss: 0.007514825
train_loss: 0.0049168603
test_loss: 0.0078091165
train_loss: 0.004625629
test_loss: 0.008015116
train_loss: 0.005070606
test_loss: 0.008086057
train_loss: 0.0049289404
test_loss: 0.00792111
train_loss: 0.0043892907
test_loss: 0.008033371
train_loss: 0.004574004
test_loss: 0.0075121904
train_loss: 0.0044645607
test_loss: 0.007692908
train_loss: 0.004047252
test_loss: 0.007817674
train_loss: 0.0045290557
test_loss: 0.0076599126
train_loss: 0.0043229708
test_loss: 0.007476018
train_loss: 0.0045829816
test_loss: 0.0074391086
train_loss: 0.004014535
test_loss: 0.007847353
train_loss: 0.004060863
test_loss: 0.0077083334
train_loss: 0.0038050318
test_loss: 0.007390562
train_loss: 0.004944966
test_loss: 0.0077778036
train_loss: 0.00507081
test_loss: 0.0077760546
train_loss: 0.004777919
test_loss: 0.00752827
train_loss: 0.0045495206
test_loss: 0.007833865
train_loss: 0.004405792
test_loss: 0.007421936
train_loss: 0.0049175676
test_loss: 0.00763578
train_loss: 0.00464809
test_loss: 0.008230954
train_loss: 0.005521918
test_loss: 0.007824346
train_loss: 0.0047264113
test_loss: 0.007659079
train_loss: 0.004307792
test_loss: 0.0075940234
train_loss: 0.0044424906
test_loss: 0.008060374
train_loss: 0.0047023273
test_loss: 0.0077222874
train_loss: 0.0048397356
test_loss: 0.0076247696
train_loss: 0.0038242615
test_loss: 0.007773849
train_loss: 0.00431732
test_loss: 0.00750008
train_loss: 0.0044438797
test_loss: 0.007634566
train_loss: 0.00461284
test_loss: 0.007886647
train_loss: 0.0048736925
test_loss: 0.0077511836
train_loss: 0.0044933595
test_loss: 0.0077079274
train_loss: 0.005051988
test_loss: 0.007971782
train_loss: 0.004481996
test_loss: 0.008103511
train_loss: 0.005196208
test_loss: 0.0075926613
train_loss: 0.0038075233
test_loss: 0.0073107793
train_loss: 0.0038050672
test_loss: 0.007506028
train_loss: 0.00404964
test_loss: 0.007686483
train_loss: 0.0036263638
test_loss: 0.007400619
train_loss: 0.0051264134
test_loss: 0.0077908314
train_loss: 0.0045727417
test_loss: 0.00797645
train_loss: 0.00430668
test_loss: 0.0074893776
train_loss: 0.0041199676
test_loss: 0.007831198
train_loss: 0.004371285
test_loss: 0.0074743675
train_loss: 0.004650246
test_loss: 0.0077520246
train_loss: 0.004245271
test_loss: 0.007573412
train_loss: 0.0045861723
test_loss: 0.007766523
train_loss: 0.0043364987
test_loss: 0.007574626
train_loss: 0.00447659
test_loss: 0.007875479
train_loss: 0.0044356203
test_loss: 0.0074253622
train_loss: 0.0048141386
test_loss: 0.007880186
train_loss: 0.0044113137
test_loss: 0.0074629197
train_loss: 0.0038580487
test_loss: 0.007963311
train_loss: 0.0044070114
test_loss: 0.007978092
train_loss: 0.0042344993
test_loss: 0.0077341455
train_loss: 0.0048274556
test_loss: 0.0073945234
train_loss: 0.0040957546
test_loss: 0.0073678647
train_loss: 0.0051361797
test_loss: 0.0076300683
train_loss: 0.003988184
test_loss: 0.0075337696
train_loss: 0.0046648895
test_loss: 0.0077745747
train_loss: 0.004357451
test_loss: 0.007749228
train_loss: 0.0043794527
test_loss: 0.007722643
train_loss: 0.0048836344
test_loss: 0.00792186
train_loss: 0.004324519
test_loss: 0.007550144
train_loss: 0.0047643585
test_loss: 0.00798783
train_loss: 0.005429557
test_loss: 0.0078010852
train_loss: 0.004332223
test_loss: 0.007509613
train_loss: 0.004408799
test_loss: 0.0076843044
train_loss: 0.004372793
test_loss: 0.007854622
train_loss: 0.004107853
test_loss: 0.0075099203
train_loss: 0.0047282204
test_loss: 0.007679139
train_loss: 0.004295753
test_loss: 0.0075782132
train_loss: 0.004396354
test_loss: 0.0077041234
train_loss: 0.0043095537
test_loss: 0.007465149
train_loss: 0.004776556
test_loss: 0.0075481893
train_loss: 0.003963385
test_loss: 0.0074628806
train_loss: 0.003917337
test_loss: 0.007587766
train_loss: 0.004494537
test_loss: 0.00766579
train_loss: 0.004043367
test_loss: 0.007471227
train_loss: 0.004643452
test_loss: 0.0080161765
train_loss: 0.00441428
test_loss: 0.007769872
train_loss: 0.0045912797
test_loss: 0.0076335343
train_loss: 0.0043422566
test_loss: 0.0076260814
train_loss: 0.004360145
test_loss: 0.007613179
train_loss: 0.0041125324
test_loss: 0.0075176847
train_loss: 0.004283697
test_loss: 0.00741839
train_loss: 0.003908535
test_loss: 0.0074659
train_loss: 0.0041714194
test_loss: 0.0076100626
train_loss: 0.0049246126
test_loss: 0.008064354
train_loss: 0.004280708
test_loss: 0.00793548
train_loss: 0.0044125193
test_loss: 0.007987995
train_loss: 0.004434225
test_loss: 0.00770939
train_loss: 0.004165209
test_loss: 0.0072998344
train_loss: 0.004149733
test_loss: 0.007395611
train_loss: 0.00391969
test_loss: 0.0075408826
train_loss: 0.004256622
test_loss: 0.007628751
train_loss: 0.004093022
test_loss: 0.0075383643
train_loss: 0.004260942
test_loss: 0.008008868
train_loss: 0.004442611
test_loss: 0.0078038527
train_loss: 0.0045416746
test_loss: 0.007489823
train_loss: 0.0046195234
test_loss: 0.0073654205
train_loss: 0.0048632077
test_loss: 0.008668023
train_loss: 0.0044034803
test_loss: 0.0075082383
train_loss: 0.004276068
test_loss: 0.0074344734
train_loss: 0.004507967
test_loss: 0.007566042
train_loss: 0.0046386346
test_loss: 0.007971618
train_loss: 0.004225199
test_loss: 0.0076027084
train_loss: 0.004003877
test_loss: 0.0070657814
train_loss: 0.004340241
test_loss: 0.0074675903
train_loss: 0.0044271857
test_loss: 0.007274084
train_loss: 0.004725196
test_loss: 0.008004864
train_loss: 0.0046054586
test_loss: 0.00786205
train_loss: 0.0042156233
test_loss: 0.0077737398
train_loss: 0.0044120075
test_loss: 0.0074145254
train_loss: 0.0042435536
test_loss: 0.007291137
train_loss: 0.00388648
test_loss: 0.0073301783
train_loss: 0.004427763
test_loss: 0.007451818
train_loss: 0.004654264
test_loss: 0.007567418
train_loss: 0.0043666805
test_loss: 0.007392758
train_loss: 0.0043640486
test_loss: 0.007864926
train_loss: 0.003948145
test_loss: 0.0077012973
train_loss: 0.004525344
test_loss: 0.007841584
train_loss: 0.004143113
test_loss: 0.0076148203
train_loss: 0.0044543818
test_loss: 0.007707482
train_loss: 0.004008351
test_loss: 0.0074949753
train_loss: 0.004421815
test_loss: 0.0074863774
train_loss: 0.0045051714
test_loss: 0.0076668337
train_loss: 0.004338283
test_loss: 0.007789551
train_loss: 0.0037956275
test_loss: 0.0073555517
train_loss: 0.0036094896
test_loss: 0.007298524
train_loss: 0.0042550713
test_loss: 0.007648816
train_loss: 0.004913243
test_loss: 0.0081584975
train_loss: 0.0047809593
test_loss: 0.0073415614
train_loss: 0.004047973
test_loss: 0.0073548807
train_loss: 0.004056432
test_loss: 0.007419385
train_loss: 0.0041173063
test_loss: 0.0077515026
train_loss: 0.003907547
test_loss: 0.0073251007
train_loss: 0.004517077
test_loss: 0.0072781355
train_loss: 0.003894147
test_loss: 0.007881157
train_loss: 0.0041648643
test_loss: 0.007391548
train_loss: 0.0040511815
test_loss: 0.007786377
train_loss: 0.0041395565
test_loss: 0.007781656
train_loss: 0.004450273
test_loss: 0.007530601
train_loss: 0.004510497
test_loss: 0.0075354767
train_loss: 0.004248522
test_loss: 0.0077523333
train_loss: 0.004069536
test_loss: 0.007456513
train_loss: 0.0043880153
test_loss: 0.0075790044
train_loss: 0.0040533585
test_loss: 0.007729759
train_loss: 0.0042382213
test_loss: 0.00789344
train_loss: 0.004388179
test_loss: 0.0075039566
train_loss: 0.004143925
test_loss: 0.007521444
train_loss: 0.0039366875
test_loss: 0.0075843795
train_loss: 0.0037498937
test_loss: 0.0073935827
train_loss: 0.004337919
test_loss: 0.0072846115
train_loss: 0.0039123595
test_loss: 0.007131583
train_loss: 0.004664287
test_loss: 0.0075298795
train_loss: 0.0045500444
test_loss: 0.007953279
train_loss: 0.004255057
test_loss: 0.007600851
train_loss: 0.0034932438
test_loss: 0.0076257475
train_loss: 0.003954166
test_loss: 0.007301623
train_loss: 0.003852271
test_loss: 0.007247151
train_loss: 0.003864915
test_loss: 0.007490284
train_loss: 0.0040649734
test_loss: 0.0073092617
train_loss: 0.0050457367
test_loss: 0.008077544
train_loss: 0.0039937175
test_loss: 0.00773403
train_loss: 0.0044517256
test_loss: 0.0071355226
train_loss: 0.004112214
test_loss: 0.0074174614
train_loss: 0.004315772
test_loss: 0.007407787
train_loss: 0.0039874143
test_loss: 0.0074237795
train_loss: 0.0039259456
test_loss: 0.007972517
train_loss: 0.004125653
test_loss: 0.007689391
train_loss: 0.0037325288
test_loss: 0.0074760527
train_loss: 0.0038349133
test_loss: 0.0072639184
train_loss: 0.0040041464
test_loss: 0.007859737
train_loss: 0.0038779324
test_loss: 0.0073241415
train_loss: 0.0039781323
test_loss: 0.007106773
train_loss: 0.00407308
test_loss: 0.008114321
train_loss: 0.0047486955
test_loss: 0.007837582
train_loss: 0.0039700787
test_loss: 0.0075924927
train_loss: 0.0035860348
test_loss: 0.007362624
train_loss: 0.003555017
test_loss: 0.00741173
train_loss: 0.0040266365
test_loss: 0.008004355
train_loss: 0.0040192488
test_loss: 0.0075869733
train_loss: 0.0045008236
test_loss: 0.007664019
train_loss: 0.0040495424
test_loss: 0.007796717
train_loss: 0.0039581917
test_loss: 0.007329355
train_loss: 0.004520918
test_loss: 0.0074803764
train_loss: 0.003968597
test_loss: 0.0073650675
train_loss: 0.004459668
test_loss: 0.0076031727
train_loss: 0.0037469908
test_loss: 0.007762841
train_loss: 0.0045372644
test_loss: 0.0076465486
train_loss: 0.004414272
test_loss: 0.007328741
train_loss: 0.0047056684
test_loss: 0.007554833
train_loss: 0.0040038736
test_loss: 0.007686764
train_loss: 0.0040672095
test_loss: 0.007473895
train_loss: 0.003754689
test_loss: 0.007470493
train_loss: 0.004102276
test_loss: 0.007359017
train_loss: 0.0044455756
test_loss: 0.007761641
train_loss: 0.0038487327
test_loss: 0.007653308
train_loss: 0.0039143255
test_loss: 0.007573771
train_loss: 0.0046243155
test_loss: 0.008016716
train_loss: 0.0041762902
test_loss: 0.007661925
train_loss: 0.0038806517
test_loss: 0.007596635
train_loss: 0.0040483535
test_loss: 0.007292009
train_loss: 0.0037879306
test_loss: 0.0077926363
train_loss: 0.004220979
test_loss: 0.007712906
train_loss: 0.004225559
test_loss: 0.007517292
train_loss: 0.0038998248
test_loss: 0.007305469
train_loss: 0.004645507
test_loss: 0.00806026
train_loss: 0.004357798
test_loss: 0.007932156
train_loss: 0.004075791
test_loss: 0.0073781842
train_loss: 0.0041591283
test_loss: 0.007272357
train_loss: 0.0036557522
test_loss: 0.007516989
train_loss: 0.0038695545
test_loss: 0.007475323
train_loss: 0.0041677584
test_loss: 0.007519713
train_loss: 0.0039504077
test_loss: 0.007780035
train_loss: 0.00411591
test_loss: 0.00790638
train_loss: 0.004237123
test_loss: 0.0074890573
train_loss: 0.004029984
test_loss: 0.00758922
train_loss: 0.004105656
test_loss: 0.007795999
train_loss: 0.0038777585
test_loss: 0.007564499
train_loss: 0.0040377784
test_loss: 0.007269929
train_loss: 0.003870904
test_loss: 0.007225298
train_loss: 0.0044637136
test_loss: 0.0076130517
train_loss: 0.0037431882
test_loss: 0.0072336653
train_loss: 0.0038208321
test_loss: 0.007457692
train_loss: 0.0036048838
test_loss: 0.0073219263
train_loss: 0.0036052878
test_loss: 0.0072464827
train_loss: 0.0039254297
test_loss: 0.0076758517
train_loss: 0.0038907852
test_loss: 0.0076873866
train_loss: 0.0038094842/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0073753265
train_loss: 0.004064122
test_loss: 0.0073511573
train_loss: 0.003792736
test_loss: 0.0073299855
train_loss: 0.004063726
test_loss: 0.007900601
train_loss: 0.003906418
test_loss: 0.007508274
train_loss: 0.004620724
test_loss: 0.0073177163
train_loss: 0.0039585135
test_loss: 0.007685471
train_loss: 0.0040910137
test_loss: 0.0074099433
train_loss: 0.0040938947
test_loss: 0.0074686124
train_loss: 0.0040417756
test_loss: 0.007644687
train_loss: 0.0044361856
test_loss: 0.007493059
train_loss: 0.004182584
test_loss: 0.007656723
train_loss: 0.003823061
test_loss: 0.007277098
train_loss: 0.003869352
test_loss: 0.00796638
train_loss: 0.0043707564
test_loss: 0.0073212716
train_loss: 0.0041375696
test_loss: 0.007810032
train_loss: 0.0047101215
test_loss: 0.0077369316
train_loss: 0.003912828
test_loss: 0.007528707
train_loss: 0.004096967
test_loss: 0.007834426
train_loss: 0.0044313325
test_loss: 0.0074380985
train_loss: 0.0038487012
test_loss: 0.0072872494
train_loss: 0.0043591987
test_loss: 0.0078156805
train_loss: 0.003968198
test_loss: 0.007432425
train_loss: 0.0037268442
test_loss: 0.007128354
train_loss: 0.0037801133
test_loss: 0.007216993
train_loss: 0.003997606
test_loss: 0.007270932
train_loss: 0.0037073675
test_loss: 0.007305713
train_loss: 0.003876458
test_loss: 0.007906309
train_loss: 0.004242709
test_loss: 0.00761314
train_loss: 0.004653546
test_loss: 0.008211487
train_loss: 0.0043901666
test_loss: 0.007350274
train_loss: 0.0037047165
test_loss: 0.0073893173
train_loss: 0.004322271
test_loss: 0.0075916215
train_loss: 0.004181628
test_loss: 0.007239982
train_loss: 0.0045433934
test_loss: 0.007429792
train_loss: 0.0041867103
test_loss: 0.0074121873
train_loss: 0.0039977767
test_loss: 0.00768283
train_loss: 0.0036760834
test_loss: 0.007736661
train_loss: 0.003958131
test_loss: 0.0073170704
train_loss: 0.0038442307
test_loss: 0.007593625
train_loss: 0.0038499008
test_loss: 0.007324113
train_loss: 0.0039799907
test_loss: 0.007472819
train_loss: 0.0033837357
test_loss: 0.0073315166
train_loss: 0.0040661697
test_loss: 0.007266804
train_loss: 0.0038188961
test_loss: 0.0074518183
train_loss: 0.0040641185
test_loss: 0.0074025337
train_loss: 0.0037343134
test_loss: 0.007846425
train_loss: 0.0037321914
test_loss: 0.007377746
train_loss: 0.0038950057
test_loss: 0.007470039
train_loss: 0.0037174514
test_loss: 0.007077274
train_loss: 0.0036852155
test_loss: 0.0073831226
train_loss: 0.0038339389
test_loss: 0.0071677924
train_loss: 0.0042809523
test_loss: 0.0072363447
train_loss: 0.0036961245
test_loss: 0.0075332583
train_loss: 0.003624316
test_loss: 0.0071497243
train_loss: 0.0041409973
test_loss: 0.007776917
train_loss: 0.004062703
test_loss: 0.0076722847
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi3/500_500_500_500_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3/500_500_500_500_1 --optimizer lbfgs --function f1 --psi 2 --phi 3 --layers 500_500_500_500_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi3/ --save_name 500_500_500_500_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 500_500_500_500_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ee35840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ee9fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12eda2158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12edcfd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12edc6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ed3f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12edcff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ecc6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ecc6620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ecf3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ec3f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ec6c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ec6cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ec0e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ebcf950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ebd3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ebd3158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12eb847b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12eb60510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12ebd3b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12eb60378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12eb32d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa12eaf4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1118cb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1118cb048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa11188e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa111851840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa111872620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa11186d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1118200d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1117d2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1117dcae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1117dcb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa1117b9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa11176bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa111707268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.75715717e-05
Iter: 2 loss: 3.01068212e-05
Iter: 3 loss: 3.00302454e-05
Iter: 4 loss: 2.62876856e-05
Iter: 5 loss: 4.63463512e-05
Iter: 6 loss: 2.5710624e-05
Iter: 7 loss: 2.44304101e-05
Iter: 8 loss: 2.09473565e-05
Iter: 9 loss: 4.2488522e-05
Iter: 10 loss: 2.00362101e-05
Iter: 11 loss: 1.78913233e-05
Iter: 12 loss: 1.78362061e-05
Iter: 13 loss: 1.62118722e-05
Iter: 14 loss: 1.90089177e-05
Iter: 15 loss: 1.54897971e-05
Iter: 16 loss: 1.43499974e-05
Iter: 17 loss: 2.52237769e-05
Iter: 18 loss: 1.43054622e-05
Iter: 19 loss: 1.33990025e-05
Iter: 20 loss: 1.24255657e-05
Iter: 21 loss: 1.22705569e-05
Iter: 22 loss: 1.09429357e-05
Iter: 23 loss: 2.57357278e-05
Iter: 24 loss: 1.09171397e-05
Iter: 25 loss: 1.03607072e-05
Iter: 26 loss: 1.30998706e-05
Iter: 27 loss: 1.02655285e-05
Iter: 28 loss: 9.78449134e-06
Iter: 29 loss: 9.36238939e-06
Iter: 30 loss: 9.23626067e-06
Iter: 31 loss: 8.76430931e-06
Iter: 32 loss: 9.85222e-06
Iter: 33 loss: 8.58915064e-06
Iter: 34 loss: 8.00847e-06
Iter: 35 loss: 9.15429518e-06
Iter: 36 loss: 7.76816523e-06
Iter: 37 loss: 7.38673907e-06
Iter: 38 loss: 8.78366063e-06
Iter: 39 loss: 7.29202657e-06
Iter: 40 loss: 6.92120921e-06
Iter: 41 loss: 9.41228427e-06
Iter: 42 loss: 6.88368482e-06
Iter: 43 loss: 6.78166725e-06
Iter: 44 loss: 6.72708802e-06
Iter: 45 loss: 6.64553272e-06
Iter: 46 loss: 6.39401242e-06
Iter: 47 loss: 6.94473465e-06
Iter: 48 loss: 6.24246695e-06
Iter: 49 loss: 6.00309886e-06
Iter: 50 loss: 6.00275644e-06
Iter: 51 loss: 5.81823315e-06
Iter: 52 loss: 5.71455257e-06
Iter: 53 loss: 5.63439517e-06
Iter: 54 loss: 5.44174873e-06
Iter: 55 loss: 5.44077284e-06
Iter: 56 loss: 5.32848253e-06
Iter: 57 loss: 5.27612792e-06
Iter: 58 loss: 5.22077426e-06
Iter: 59 loss: 5.01911927e-06
Iter: 60 loss: 5.7188372e-06
Iter: 61 loss: 4.96601797e-06
Iter: 62 loss: 4.83793201e-06
Iter: 63 loss: 5.18512843e-06
Iter: 64 loss: 4.79574601e-06
Iter: 65 loss: 4.64220193e-06
Iter: 66 loss: 5.11640974e-06
Iter: 67 loss: 4.59729335e-06
Iter: 68 loss: 4.49357049e-06
Iter: 69 loss: 4.70096074e-06
Iter: 70 loss: 4.45116e-06
Iter: 71 loss: 4.31338049e-06
Iter: 72 loss: 4.38290544e-06
Iter: 73 loss: 4.2210213e-06
Iter: 74 loss: 4.20803553e-06
Iter: 75 loss: 4.17804449e-06
Iter: 76 loss: 4.12472173e-06
Iter: 77 loss: 4.04205639e-06
Iter: 78 loss: 4.04103048e-06
Iter: 79 loss: 3.9432507e-06
Iter: 80 loss: 4.09116637e-06
Iter: 81 loss: 3.89686375e-06
Iter: 82 loss: 3.82923e-06
Iter: 83 loss: 3.73871808e-06
Iter: 84 loss: 3.73358921e-06
Iter: 85 loss: 3.62615128e-06
Iter: 86 loss: 3.62551e-06
Iter: 87 loss: 3.56893133e-06
Iter: 88 loss: 3.56788314e-06
Iter: 89 loss: 3.52386087e-06
Iter: 90 loss: 3.42957969e-06
Iter: 91 loss: 3.76716798e-06
Iter: 92 loss: 3.40533461e-06
Iter: 93 loss: 3.33996604e-06
Iter: 94 loss: 3.44752107e-06
Iter: 95 loss: 3.31000911e-06
Iter: 96 loss: 3.22232745e-06
Iter: 97 loss: 3.45523904e-06
Iter: 98 loss: 3.19303285e-06
Iter: 99 loss: 3.13599412e-06
Iter: 100 loss: 3.62779792e-06
Iter: 101 loss: 3.13290957e-06
Iter: 102 loss: 3.07935807e-06
Iter: 103 loss: 3.03404022e-06
Iter: 104 loss: 3.0192557e-06
Iter: 105 loss: 2.96992675e-06
Iter: 106 loss: 3.21758284e-06
Iter: 107 loss: 2.96161988e-06
Iter: 108 loss: 2.92501545e-06
Iter: 109 loss: 2.92492859e-06
Iter: 110 loss: 2.88781439e-06
Iter: 111 loss: 2.93552011e-06
Iter: 112 loss: 2.86858813e-06
Iter: 113 loss: 2.84963221e-06
Iter: 114 loss: 2.79585606e-06
Iter: 115 loss: 3.06140919e-06
Iter: 116 loss: 2.77774916e-06
Iter: 117 loss: 2.7455585e-06
Iter: 118 loss: 2.73726528e-06
Iter: 119 loss: 2.71302588e-06
Iter: 120 loss: 2.67510131e-06
Iter: 121 loss: 2.67452651e-06
Iter: 122 loss: 2.63169659e-06
Iter: 123 loss: 2.63156608e-06
Iter: 124 loss: 2.60181446e-06
Iter: 125 loss: 2.56753174e-06
Iter: 126 loss: 2.56327394e-06
Iter: 127 loss: 2.52196e-06
Iter: 128 loss: 2.52179461e-06
Iter: 129 loss: 2.49722234e-06
Iter: 130 loss: 2.46958894e-06
Iter: 131 loss: 2.46586956e-06
Iter: 132 loss: 2.43758973e-06
Iter: 133 loss: 2.43581098e-06
Iter: 134 loss: 2.41235193e-06
Iter: 135 loss: 2.41786665e-06
Iter: 136 loss: 2.39512201e-06
Iter: 137 loss: 2.3677303e-06
Iter: 138 loss: 2.62881895e-06
Iter: 139 loss: 2.3665707e-06
Iter: 140 loss: 2.35374137e-06
Iter: 141 loss: 2.35379093e-06
Iter: 142 loss: 2.34303798e-06
Iter: 143 loss: 2.31674767e-06
Iter: 144 loss: 2.58716227e-06
Iter: 145 loss: 2.3137809e-06
Iter: 146 loss: 2.28395e-06
Iter: 147 loss: 2.3795119e-06
Iter: 148 loss: 2.27540431e-06
Iter: 149 loss: 2.25544863e-06
Iter: 150 loss: 2.23704251e-06
Iter: 151 loss: 2.23221673e-06
Iter: 152 loss: 2.20491756e-06
Iter: 153 loss: 2.55912983e-06
Iter: 154 loss: 2.20484344e-06
Iter: 155 loss: 2.17972934e-06
Iter: 156 loss: 2.18027481e-06
Iter: 157 loss: 2.16013268e-06
Iter: 158 loss: 2.13847261e-06
Iter: 159 loss: 2.13787689e-06
Iter: 160 loss: 2.12376654e-06
Iter: 161 loss: 2.1002038e-06
Iter: 162 loss: 2.10015651e-06
Iter: 163 loss: 2.08537062e-06
Iter: 164 loss: 2.08415508e-06
Iter: 165 loss: 2.07292987e-06
Iter: 166 loss: 2.04815501e-06
Iter: 167 loss: 2.41770158e-06
Iter: 168 loss: 2.04705475e-06
Iter: 169 loss: 2.02870524e-06
Iter: 170 loss: 2.02748015e-06
Iter: 171 loss: 2.01508033e-06
Iter: 172 loss: 2.02037813e-06
Iter: 173 loss: 2.00658383e-06
Iter: 174 loss: 1.98975522e-06
Iter: 175 loss: 2.19494223e-06
Iter: 176 loss: 1.9895524e-06
Iter: 177 loss: 1.97962527e-06
Iter: 178 loss: 1.96569295e-06
Iter: 179 loss: 1.96512724e-06
Iter: 180 loss: 1.94935365e-06
Iter: 181 loss: 1.92501284e-06
Iter: 182 loss: 1.92467292e-06
Iter: 183 loss: 1.90987794e-06
Iter: 184 loss: 1.9093527e-06
Iter: 185 loss: 1.89468642e-06
Iter: 186 loss: 1.87641285e-06
Iter: 187 loss: 1.87506123e-06
Iter: 188 loss: 1.85334522e-06
Iter: 189 loss: 1.92926564e-06
Iter: 190 loss: 1.84775536e-06
Iter: 191 loss: 1.8261826e-06
Iter: 192 loss: 2.03105719e-06
Iter: 193 loss: 1.82521183e-06
Iter: 194 loss: 1.8127281e-06
Iter: 195 loss: 1.83294935e-06
Iter: 196 loss: 1.80674635e-06
Iter: 197 loss: 1.78932862e-06
Iter: 198 loss: 1.85989848e-06
Iter: 199 loss: 1.78547657e-06
Iter: 200 loss: 1.7772104e-06
Iter: 201 loss: 1.79021993e-06
Iter: 202 loss: 1.77334823e-06
Iter: 203 loss: 1.76085928e-06
Iter: 204 loss: 1.771764e-06
Iter: 205 loss: 1.75338801e-06
Iter: 206 loss: 1.7409368e-06
Iter: 207 loss: 1.81227324e-06
Iter: 208 loss: 1.7391751e-06
Iter: 209 loss: 1.7275396e-06
Iter: 210 loss: 1.78645962e-06
Iter: 211 loss: 1.72542468e-06
Iter: 212 loss: 1.71292675e-06
Iter: 213 loss: 1.79914457e-06
Iter: 214 loss: 1.71170711e-06
Iter: 215 loss: 1.7073196e-06
Iter: 216 loss: 1.70010151e-06
Iter: 217 loss: 1.70008593e-06
Iter: 218 loss: 1.68744782e-06
Iter: 219 loss: 1.666152e-06
Iter: 220 loss: 1.66606674e-06
Iter: 221 loss: 1.65313372e-06
Iter: 222 loss: 1.7631462e-06
Iter: 223 loss: 1.65246297e-06
Iter: 224 loss: 1.6391956e-06
Iter: 225 loss: 1.67354688e-06
Iter: 226 loss: 1.63466882e-06
Iter: 227 loss: 1.62280207e-06
Iter: 228 loss: 1.61229036e-06
Iter: 229 loss: 1.60948809e-06
Iter: 230 loss: 1.59982142e-06
Iter: 231 loss: 1.59901924e-06
Iter: 232 loss: 1.58951048e-06
Iter: 233 loss: 1.5840908e-06
Iter: 234 loss: 1.58000125e-06
Iter: 235 loss: 1.56933936e-06
Iter: 236 loss: 1.62193226e-06
Iter: 237 loss: 1.56748342e-06
Iter: 238 loss: 1.55591465e-06
Iter: 239 loss: 1.62804531e-06
Iter: 240 loss: 1.55463158e-06
Iter: 241 loss: 1.54865575e-06
Iter: 242 loss: 1.56251076e-06
Iter: 243 loss: 1.54643294e-06
Iter: 244 loss: 1.53839915e-06
Iter: 245 loss: 1.53103861e-06
Iter: 246 loss: 1.52923326e-06
Iter: 247 loss: 1.53282508e-06
Iter: 248 loss: 1.52349094e-06
Iter: 249 loss: 1.52063603e-06
Iter: 250 loss: 1.51573408e-06
Iter: 251 loss: 1.51576683e-06
Iter: 252 loss: 1.50848621e-06
Iter: 253 loss: 1.50174719e-06
Iter: 254 loss: 1.50022743e-06
Iter: 255 loss: 1.49051289e-06
Iter: 256 loss: 1.55435418e-06
Iter: 257 loss: 1.48953973e-06
Iter: 258 loss: 1.48174945e-06
Iter: 259 loss: 1.48108484e-06
Iter: 260 loss: 1.47504784e-06
Iter: 261 loss: 1.46780849e-06
Iter: 262 loss: 1.51822792e-06
Iter: 263 loss: 1.46710533e-06
Iter: 264 loss: 1.45882609e-06
Iter: 265 loss: 1.45817103e-06
Iter: 266 loss: 1.45206866e-06
Iter: 267 loss: 1.44610385e-06
Iter: 268 loss: 1.44599085e-06
Iter: 269 loss: 1.440706e-06
Iter: 270 loss: 1.42868817e-06
Iter: 271 loss: 1.59515844e-06
Iter: 272 loss: 1.42788588e-06
Iter: 273 loss: 1.42274803e-06
Iter: 274 loss: 1.42116335e-06
Iter: 275 loss: 1.41575606e-06
Iter: 276 loss: 1.41912381e-06
Iter: 277 loss: 1.41215185e-06
Iter: 278 loss: 1.40651207e-06
Iter: 279 loss: 1.46665207e-06
Iter: 280 loss: 1.40633392e-06
Iter: 281 loss: 1.40330303e-06
Iter: 282 loss: 1.40327779e-06
Iter: 283 loss: 1.40069551e-06
Iter: 284 loss: 1.3927546e-06
Iter: 285 loss: 1.42881254e-06
Iter: 286 loss: 1.38988162e-06
Iter: 287 loss: 1.38449798e-06
Iter: 288 loss: 1.3846178e-06
Iter: 289 loss: 1.37961342e-06
Iter: 290 loss: 1.36968742e-06
Iter: 291 loss: 1.55606108e-06
Iter: 292 loss: 1.36961614e-06
Iter: 293 loss: 1.36354697e-06
Iter: 294 loss: 1.36349092e-06
Iter: 295 loss: 1.35820221e-06
Iter: 296 loss: 1.34752065e-06
Iter: 297 loss: 1.54439135e-06
Iter: 298 loss: 1.34738661e-06
Iter: 299 loss: 1.34035645e-06
Iter: 300 loss: 1.43400655e-06
Iter: 301 loss: 1.34031484e-06
Iter: 302 loss: 1.33340041e-06
Iter: 303 loss: 1.34808124e-06
Iter: 304 loss: 1.33065987e-06
Iter: 305 loss: 1.32510331e-06
Iter: 306 loss: 1.3302157e-06
Iter: 307 loss: 1.32211335e-06
Iter: 308 loss: 1.31669799e-06
Iter: 309 loss: 1.37578979e-06
Iter: 310 loss: 1.31664785e-06
Iter: 311 loss: 1.31243701e-06
Iter: 312 loss: 1.31523825e-06
Iter: 313 loss: 1.30994977e-06
Iter: 314 loss: 1.30480544e-06
Iter: 315 loss: 1.36684685e-06
Iter: 316 loss: 1.30478475e-06
Iter: 317 loss: 1.30105366e-06
Iter: 318 loss: 1.32193225e-06
Iter: 319 loss: 1.30051592e-06
Iter: 320 loss: 1.29858381e-06
Iter: 321 loss: 1.29439445e-06
Iter: 322 loss: 1.3426943e-06
Iter: 323 loss: 1.2940684e-06
Iter: 324 loss: 1.2883296e-06
Iter: 325 loss: 1.31294314e-06
Iter: 326 loss: 1.28714646e-06
Iter: 327 loss: 1.28336706e-06
Iter: 328 loss: 1.30154433e-06
Iter: 329 loss: 1.28269335e-06
Iter: 330 loss: 1.27827457e-06
Iter: 331 loss: 1.27397925e-06
Iter: 332 loss: 1.27318378e-06
Iter: 333 loss: 1.26898465e-06
Iter: 334 loss: 1.31991987e-06
Iter: 335 loss: 1.26885345e-06
Iter: 336 loss: 1.26514601e-06
Iter: 337 loss: 1.26210398e-06
Iter: 338 loss: 1.26095654e-06
Iter: 339 loss: 1.25613235e-06
Iter: 340 loss: 1.28700617e-06
Iter: 341 loss: 1.25567954e-06
Iter: 342 loss: 1.25018084e-06
Iter: 343 loss: 1.25662802e-06
Iter: 344 loss: 1.24713733e-06
Iter: 345 loss: 1.24212443e-06
Iter: 346 loss: 1.24231394e-06
Iter: 347 loss: 1.23809536e-06
Iter: 348 loss: 1.23169605e-06
Iter: 349 loss: 1.3235782e-06
Iter: 350 loss: 1.23167331e-06
Iter: 351 loss: 1.22882284e-06
Iter: 352 loss: 1.22791653e-06
Iter: 353 loss: 1.22614142e-06
Iter: 354 loss: 1.22265737e-06
Iter: 355 loss: 1.22248628e-06
Iter: 356 loss: 1.22007225e-06
Iter: 357 loss: 1.22611141e-06
Iter: 358 loss: 1.21922631e-06
Iter: 359 loss: 1.21769608e-06
Iter: 360 loss: 1.21385347e-06
Iter: 361 loss: 1.25563929e-06
Iter: 362 loss: 1.21353128e-06
Iter: 363 loss: 1.20931429e-06
Iter: 364 loss: 1.24957614e-06
Iter: 365 loss: 1.20901427e-06
Iter: 366 loss: 1.20621553e-06
Iter: 367 loss: 1.20613231e-06
Iter: 368 loss: 1.20375159e-06
Iter: 369 loss: 1.19898527e-06
Iter: 370 loss: 1.21930191e-06
Iter: 371 loss: 1.19782658e-06
Iter: 372 loss: 1.19485412e-06
Iter: 373 loss: 1.19192441e-06
Iter: 374 loss: 1.19132426e-06
Iter: 375 loss: 1.18518051e-06
Iter: 376 loss: 1.22216818e-06
Iter: 377 loss: 1.18433309e-06
Iter: 378 loss: 1.181416e-06
Iter: 379 loss: 1.18331138e-06
Iter: 380 loss: 1.17949276e-06
Iter: 381 loss: 1.17532522e-06
Iter: 382 loss: 1.20051038e-06
Iter: 383 loss: 1.17481034e-06
Iter: 384 loss: 1.17203342e-06
Iter: 385 loss: 1.16977867e-06
Iter: 386 loss: 1.16910724e-06
Iter: 387 loss: 1.16563285e-06
Iter: 388 loss: 1.16552462e-06
Iter: 389 loss: 1.16308945e-06
Iter: 390 loss: 1.19072797e-06
Iter: 391 loss: 1.16302931e-06
Iter: 392 loss: 1.16191927e-06
Iter: 393 loss: 1.1593977e-06
Iter: 394 loss: 1.20179925e-06
Iter: 395 loss: 1.15933176e-06
Iter: 396 loss: 1.1558293e-06
Iter: 397 loss: 1.16387127e-06
Iter: 398 loss: 1.15455327e-06
Iter: 399 loss: 1.15189437e-06
Iter: 400 loss: 1.15424632e-06
Iter: 401 loss: 1.15025114e-06
Iter: 402 loss: 1.14635941e-06
Iter: 403 loss: 1.15694252e-06
Iter: 404 loss: 1.14505769e-06
Iter: 405 loss: 1.142271e-06
Iter: 406 loss: 1.16544652e-06
Iter: 407 loss: 1.14213185e-06
Iter: 408 loss: 1.1393887e-06
Iter: 409 loss: 1.13428666e-06
Iter: 410 loss: 1.25118231e-06
Iter: 411 loss: 1.13424176e-06
Iter: 412 loss: 1.13071621e-06
Iter: 413 loss: 1.17373088e-06
Iter: 414 loss: 1.13080387e-06
Iter: 415 loss: 1.127102e-06
Iter: 416 loss: 1.12761063e-06
Iter: 417 loss: 1.1243601e-06
Iter: 418 loss: 1.12141e-06
Iter: 419 loss: 1.13069632e-06
Iter: 420 loss: 1.12045745e-06
Iter: 421 loss: 1.11672693e-06
Iter: 422 loss: 1.13524663e-06
Iter: 423 loss: 1.11612167e-06
Iter: 424 loss: 1.11537713e-06
Iter: 425 loss: 1.11488362e-06
Iter: 426 loss: 1.11353143e-06
Iter: 427 loss: 1.11041265e-06
Iter: 428 loss: 1.15495664e-06
Iter: 429 loss: 1.11031136e-06
Iter: 430 loss: 1.10797919e-06
Iter: 431 loss: 1.1269658e-06
Iter: 432 loss: 1.10786914e-06
Iter: 433 loss: 1.10611461e-06
Iter: 434 loss: 1.10583665e-06
Iter: 435 loss: 1.10462986e-06
Iter: 436 loss: 1.10168367e-06
Iter: 437 loss: 1.10597784e-06
Iter: 438 loss: 1.10020164e-06
Iter: 439 loss: 1.09817756e-06
Iter: 440 loss: 1.10515407e-06
Iter: 441 loss: 1.09755831e-06
Iter: 442 loss: 1.09444647e-06
Iter: 443 loss: 1.09280199e-06
Iter: 444 loss: 1.09135658e-06
Iter: 445 loss: 1.08789141e-06
Iter: 446 loss: 1.09384393e-06
Iter: 447 loss: 1.08642894e-06
Iter: 448 loss: 1.08245172e-06
Iter: 449 loss: 1.11066856e-06
Iter: 450 loss: 1.08210816e-06
Iter: 451 loss: 1.07948017e-06
Iter: 452 loss: 1.0785393e-06
Iter: 453 loss: 1.07722576e-06
Iter: 454 loss: 1.07367623e-06
Iter: 455 loss: 1.11101201e-06
Iter: 456 loss: 1.07354538e-06
Iter: 457 loss: 1.07126914e-06
Iter: 458 loss: 1.07528911e-06
Iter: 459 loss: 1.07038397e-06
Iter: 460 loss: 1.06715754e-06
Iter: 461 loss: 1.09831615e-06
Iter: 462 loss: 1.06705511e-06
Iter: 463 loss: 1.06568314e-06
Iter: 464 loss: 1.06496884e-06
Iter: 465 loss: 1.06432458e-06
Iter: 466 loss: 1.06286177e-06
Iter: 467 loss: 1.06137225e-06
Iter: 468 loss: 1.0611e-06
Iter: 469 loss: 1.05743038e-06
Iter: 470 loss: 1.06967684e-06
Iter: 471 loss: 1.05644813e-06
Iter: 472 loss: 1.05460288e-06
Iter: 473 loss: 1.05697325e-06
Iter: 474 loss: 1.05379411e-06
Iter: 475 loss: 1.05064032e-06
Iter: 476 loss: 1.05274421e-06
Iter: 477 loss: 1.04888545e-06
Iter: 478 loss: 1.04668288e-06
Iter: 479 loss: 1.07672247e-06
Iter: 480 loss: 1.04662547e-06
Iter: 481 loss: 1.04457922e-06
Iter: 482 loss: 1.04184062e-06
Iter: 483 loss: 1.04171409e-06
Iter: 484 loss: 1.03894672e-06
Iter: 485 loss: 1.05147842e-06
Iter: 486 loss: 1.0384299e-06
Iter: 487 loss: 1.03519608e-06
Iter: 488 loss: 1.04091669e-06
Iter: 489 loss: 1.03381751e-06
Iter: 490 loss: 1.03103162e-06
Iter: 491 loss: 1.03063587e-06
Iter: 492 loss: 1.0286858e-06
Iter: 493 loss: 1.03368791e-06
Iter: 494 loss: 1.02746139e-06
Iter: 495 loss: 1.02663012e-06
Iter: 496 loss: 1.02533522e-06
Iter: 497 loss: 1.02536967e-06
Iter: 498 loss: 1.02379511e-06
Iter: 499 loss: 1.0207159e-06
Iter: 500 loss: 1.07909705e-06
Iter: 501 loss: 1.02066906e-06
Iter: 502 loss: 1.01939952e-06
Iter: 503 loss: 1.01900332e-06
Iter: 504 loss: 1.0178087e-06
Iter: 505 loss: 1.01593537e-06
Iter: 506 loss: 1.01595333e-06
Iter: 507 loss: 1.01261219e-06
Iter: 508 loss: 1.01846888e-06
Iter: 509 loss: 1.01107946e-06
Iter: 510 loss: 1.0090057e-06
Iter: 511 loss: 1.02486399e-06
Iter: 512 loss: 1.00886587e-06
Iter: 513 loss: 1.0068585e-06
Iter: 514 loss: 1.00862258e-06
Iter: 515 loss: 1.00570901e-06
Iter: 516 loss: 1.0038234e-06
Iter: 517 loss: 1.01406795e-06
Iter: 518 loss: 1.00353634e-06
Iter: 519 loss: 1.00133025e-06
Iter: 520 loss: 9.99140184e-07
Iter: 521 loss: 9.98672135e-07
Iter: 522 loss: 9.96265157e-07
Iter: 523 loss: 1.02646891e-06
Iter: 524 loss: 9.96301424e-07
Iter: 525 loss: 9.94250854e-07
Iter: 526 loss: 9.916198e-07
Iter: 527 loss: 9.91428124e-07
Iter: 528 loss: 9.88989768e-07
Iter: 529 loss: 1.01529452e-06
Iter: 530 loss: 9.8886926e-07
Iter: 531 loss: 9.87754e-07
Iter: 532 loss: 9.87552312e-07
Iter: 533 loss: 9.86544251e-07
Iter: 534 loss: 9.8467774e-07
Iter: 535 loss: 1.0306386e-06
Iter: 536 loss: 9.84701728e-07
Iter: 537 loss: 9.83113296e-07
Iter: 538 loss: 9.80251571e-07
Iter: 539 loss: 1.04138508e-06
Iter: 540 loss: 9.80281925e-07
Iter: 541 loss: 9.78403591e-07
Iter: 542 loss: 9.77981927e-07
Iter: 543 loss: 9.76638603e-07
Iter: 544 loss: 9.7631073e-07
Iter: 545 loss: 9.75503e-07
Iter: 546 loss: 9.73397505e-07
Iter: 547 loss: 9.78978278e-07
Iter: 548 loss: 9.72704242e-07
Iter: 549 loss: 9.70653559e-07
Iter: 550 loss: 9.68675e-07
Iter: 551 loss: 9.68338782e-07
Iter: 552 loss: 9.66521611e-07
Iter: 553 loss: 9.66359e-07
Iter: 554 loss: 9.64917263e-07
Iter: 555 loss: 9.65443633e-07
Iter: 556 loss: 9.63962293e-07
Iter: 557 loss: 9.61781438e-07
Iter: 558 loss: 9.63754132e-07
Iter: 559 loss: 9.60525654e-07
Iter: 560 loss: 9.59007593e-07
Iter: 561 loss: 9.70070914e-07
Iter: 562 loss: 9.5895291e-07
Iter: 563 loss: 9.57632437e-07
Iter: 564 loss: 9.65077675e-07
Iter: 565 loss: 9.57466909e-07
Iter: 566 loss: 9.55799578e-07
Iter: 567 loss: 9.57161205e-07
Iter: 568 loss: 9.5491248e-07
Iter: 569 loss: 9.53806534e-07
Iter: 570 loss: 9.51808488e-07
Iter: 571 loss: 9.96507e-07
Iter: 572 loss: 9.51855554e-07
Iter: 573 loss: 9.49729e-07
Iter: 574 loss: 9.68238282e-07
Iter: 575 loss: 9.49560615e-07
Iter: 576 loss: 9.4806245e-07
Iter: 577 loss: 9.4876134e-07
Iter: 578 loss: 9.47036142e-07
Iter: 579 loss: 9.44543956e-07
Iter: 580 loss: 9.58017608e-07
Iter: 581 loss: 9.44152418e-07
Iter: 582 loss: 9.42890836e-07
Iter: 583 loss: 9.42736563e-07
Iter: 584 loss: 9.41781536e-07
Iter: 585 loss: 9.39682195e-07
Iter: 586 loss: 9.47578826e-07
Iter: 587 loss: 9.39157076e-07
Iter: 588 loss: 9.37456662e-07
Iter: 589 loss: 9.36500953e-07
Iter: 590 loss: 9.35702e-07
Iter: 591 loss: 9.332378e-07
Iter: 592 loss: 9.64413744e-07
Iter: 593 loss: 9.33195167e-07
Iter: 594 loss: 9.3185065e-07
Iter: 595 loss: 9.35816786e-07
Iter: 596 loss: 9.31424438e-07
Iter: 597 loss: 9.30023816e-07
Iter: 598 loss: 9.34562763e-07
Iter: 599 loss: 9.29549856e-07
Iter: 600 loss: 9.28051122e-07
Iter: 601 loss: 9.44070734e-07
Iter: 602 loss: 9.27980807e-07
Iter: 603 loss: 9.27306587e-07
Iter: 604 loss: 9.25877657e-07
Iter: 605 loss: 9.40228347e-07
Iter: 606 loss: 9.25597078e-07
Iter: 607 loss: 9.23928724e-07
Iter: 608 loss: 9.33462275e-07
Iter: 609 loss: 9.2366713e-07
Iter: 610 loss: 9.22311528e-07
Iter: 611 loss: 9.20612706e-07
Iter: 612 loss: 9.2041978e-07
Iter: 613 loss: 9.18951912e-07
Iter: 614 loss: 9.18725277e-07
Iter: 615 loss: 9.17766272e-07
Iter: 616 loss: 9.16807e-07
Iter: 617 loss: 9.16624e-07
Iter: 618 loss: 9.1458827e-07
Iter: 619 loss: 9.24337712e-07
Iter: 620 loss: 9.1439864e-07
Iter: 621 loss: 9.12972041e-07
Iter: 622 loss: 9.12772293e-07
Iter: 623 loss: 9.11780546e-07
Iter: 624 loss: 9.09820528e-07
Iter: 625 loss: 9.33800038e-07
Iter: 626 loss: 9.09827179e-07
Iter: 627 loss: 9.08432753e-07
Iter: 628 loss: 9.15689839e-07
Iter: 629 loss: 9.08143932e-07
Iter: 630 loss: 9.06949822e-07
Iter: 631 loss: 9.07972662e-07
Iter: 632 loss: 9.06274693e-07
Iter: 633 loss: 9.0558774e-07
Iter: 634 loss: 9.05311822e-07
Iter: 635 loss: 9.04748106e-07
Iter: 636 loss: 9.03306216e-07
Iter: 637 loss: 9.1778054e-07
Iter: 638 loss: 9.03121872e-07
Iter: 639 loss: 9.01705448e-07
Iter: 640 loss: 9.0447304e-07
Iter: 641 loss: 9.01088868e-07
Iter: 642 loss: 8.99737472e-07
Iter: 643 loss: 8.99838142e-07
Iter: 644 loss: 8.9859418e-07
Iter: 645 loss: 8.97355676e-07
Iter: 646 loss: 8.97294456e-07
Iter: 647 loss: 8.96090285e-07
Iter: 648 loss: 8.9448713e-07
Iter: 649 loss: 8.9442e-07
Iter: 650 loss: 8.92150695e-07
Iter: 651 loss: 9.13175143e-07
Iter: 652 loss: 8.91954642e-07
Iter: 653 loss: 8.90738647e-07
Iter: 654 loss: 8.90960678e-07
Iter: 655 loss: 8.89712567e-07
Iter: 656 loss: 8.88240606e-07
Iter: 657 loss: 8.97472432e-07
Iter: 658 loss: 8.880927e-07
Iter: 659 loss: 8.86762336e-07
Iter: 660 loss: 8.89864964e-07
Iter: 661 loss: 8.86338569e-07
Iter: 662 loss: 8.84784129e-07
Iter: 663 loss: 8.88130899e-07
Iter: 664 loss: 8.84176814e-07
Iter: 665 loss: 8.83405392e-07
Iter: 666 loss: 8.83453822e-07
Iter: 667 loss: 8.82523523e-07
Iter: 668 loss: 8.82569282e-07
Iter: 669 loss: 8.81751703e-07
Iter: 670 loss: 8.80872051e-07
Iter: 671 loss: 8.79669301e-07
Iter: 672 loss: 8.79683967e-07
Iter: 673 loss: 8.779636e-07
Iter: 674 loss: 8.8128013e-07
Iter: 675 loss: 8.77202183e-07
Iter: 676 loss: 8.75917635e-07
Iter: 677 loss: 8.84481494e-07
Iter: 678 loss: 8.75764329e-07
Iter: 679 loss: 8.74456873e-07
Iter: 680 loss: 8.75829642e-07
Iter: 681 loss: 8.73682325e-07
Iter: 682 loss: 8.72562509e-07
Iter: 683 loss: 8.90235242e-07
Iter: 684 loss: 8.72554e-07
Iter: 685 loss: 8.71698603e-07
Iter: 686 loss: 8.69891039e-07
Iter: 687 loss: 8.98582414e-07
Iter: 688 loss: 8.69850226e-07
Iter: 689 loss: 8.68436e-07
Iter: 690 loss: 8.68413167e-07
Iter: 691 loss: 8.67359688e-07
Iter: 692 loss: 8.68704205e-07
Iter: 693 loss: 8.66812911e-07
Iter: 694 loss: 8.65588106e-07
Iter: 695 loss: 8.71417853e-07
Iter: 696 loss: 8.65377388e-07
Iter: 697 loss: 8.64347669e-07
Iter: 698 loss: 8.66472192e-07
Iter: 699 loss: 8.6395238e-07
Iter: 700 loss: 8.62919705e-07
Iter: 701 loss: 8.62911179e-07
Iter: 702 loss: 8.62225136e-07
Iter: 703 loss: 8.61208719e-07
Iter: 704 loss: 8.84278847e-07
Iter: 705 loss: 8.61168132e-07
Iter: 706 loss: 8.6e-07
Iter: 707 loss: 8.59897398e-07
Iter: 708 loss: 8.59001318e-07
Iter: 709 loss: 8.57353939e-07
Iter: 710 loss: 8.66631297e-07
Iter: 711 loss: 8.57195801e-07
Iter: 712 loss: 8.55672624e-07
Iter: 713 loss: 8.56393115e-07
Iter: 714 loss: 8.54595385e-07
Iter: 715 loss: 8.53431061e-07
Iter: 716 loss: 8.72299e-07
Iter: 717 loss: 8.53429299e-07
Iter: 718 loss: 8.52333073e-07
Iter: 719 loss: 8.53441e-07
Iter: 720 loss: 8.51620939e-07
Iter: 721 loss: 8.50704225e-07
Iter: 722 loss: 8.55907729e-07
Iter: 723 loss: 8.50599349e-07
Iter: 724 loss: 8.49742321e-07
Iter: 725 loss: 8.48700267e-07
Iter: 726 loss: 8.4860983e-07
Iter: 727 loss: 8.47354784e-07
Iter: 728 loss: 8.65287177e-07
Iter: 729 loss: 8.47297542e-07
Iter: 730 loss: 8.46442276e-07
Iter: 731 loss: 8.45155e-07
Iter: 732 loss: 8.45153465e-07
Iter: 733 loss: 8.44696444e-07
Iter: 734 loss: 8.44351803e-07
Iter: 735 loss: 8.43580381e-07
Iter: 736 loss: 8.45438251e-07
Iter: 737 loss: 8.43414739e-07
Iter: 738 loss: 8.42899e-07
Iter: 739 loss: 8.41672545e-07
Iter: 740 loss: 8.54445773e-07
Iter: 741 loss: 8.41450913e-07
Iter: 742 loss: 8.40144367e-07
Iter: 743 loss: 8.44744704e-07
Iter: 744 loss: 8.39849122e-07
Iter: 745 loss: 8.38414394e-07
Iter: 746 loss: 8.44234364e-07
Iter: 747 loss: 8.38095218e-07
Iter: 748 loss: 8.37022696e-07
Iter: 749 loss: 8.39408585e-07
Iter: 750 loss: 8.36584e-07
Iter: 751 loss: 8.35272317e-07
Iter: 752 loss: 8.39464235e-07
Iter: 753 loss: 8.34881e-07
Iter: 754 loss: 8.33748913e-07
Iter: 755 loss: 8.41778e-07
Iter: 756 loss: 8.33774436e-07
Iter: 757 loss: 8.32777687e-07
Iter: 758 loss: 8.33504828e-07
Iter: 759 loss: 8.3233266e-07
Iter: 760 loss: 8.31277248e-07
Iter: 761 loss: 8.34095715e-07
Iter: 762 loss: 8.3099826e-07
Iter: 763 loss: 8.29811825e-07
Iter: 764 loss: 8.29454052e-07
Iter: 765 loss: 8.28708494e-07
Iter: 766 loss: 8.27860902e-07
Iter: 767 loss: 8.27832253e-07
Iter: 768 loss: 8.27259555e-07
Iter: 769 loss: 8.2730088e-07
Iter: 770 loss: 8.26858525e-07
Iter: 771 loss: 8.25593929e-07
Iter: 772 loss: 8.3425482e-07
Iter: 773 loss: 8.25371444e-07
Iter: 774 loss: 8.24153688e-07
Iter: 775 loss: 8.29361284e-07
Iter: 776 loss: 8.24042104e-07
Iter: 777 loss: 8.22903417e-07
Iter: 778 loss: 8.23140965e-07
Iter: 779 loss: 8.22104141e-07
Iter: 780 loss: 8.20759055e-07
Iter: 781 loss: 8.2486639e-07
Iter: 782 loss: 8.20396735e-07
Iter: 783 loss: 8.18973319e-07
Iter: 784 loss: 8.2452857e-07
Iter: 785 loss: 8.18683588e-07
Iter: 786 loss: 8.17593502e-07
Iter: 787 loss: 8.21389278e-07
Iter: 788 loss: 8.17386194e-07
Iter: 789 loss: 8.16382567e-07
Iter: 790 loss: 8.20914693e-07
Iter: 791 loss: 8.16203e-07
Iter: 792 loss: 8.15406565e-07
Iter: 793 loss: 8.1838914e-07
Iter: 794 loss: 8.15226144e-07
Iter: 795 loss: 8.14516625e-07
Iter: 796 loss: 8.14134751e-07
Iter: 797 loss: 8.13702911e-07
Iter: 798 loss: 8.12864869e-07
Iter: 799 loss: 8.17168541e-07
Iter: 800 loss: 8.12681378e-07
Iter: 801 loss: 8.1194338e-07
Iter: 802 loss: 8.11938889e-07
Iter: 803 loss: 8.11271093e-07
Iter: 804 loss: 8.11223799e-07
Iter: 805 loss: 8.10650249e-07
Iter: 806 loss: 8.10045663e-07
Iter: 807 loss: 8.08843765e-07
Iter: 808 loss: 8.29319958e-07
Iter: 809 loss: 8.08859681e-07
Iter: 810 loss: 8.07272556e-07
Iter: 811 loss: 8.18642889e-07
Iter: 812 loss: 8.07293645e-07
Iter: 813 loss: 8.06419905e-07
Iter: 814 loss: 8.09013102e-07
Iter: 815 loss: 8.06134778e-07
Iter: 816 loss: 8.04997057e-07
Iter: 817 loss: 8.04991942e-07
Iter: 818 loss: 8.04061301e-07
Iter: 819 loss: 8.02736281e-07
Iter: 820 loss: 8.06420076e-07
Iter: 821 loss: 8.02366287e-07
Iter: 822 loss: 8.00905241e-07
Iter: 823 loss: 8.0652103e-07
Iter: 824 loss: 8.00632336e-07
Iter: 825 loss: 7.99741883e-07
Iter: 826 loss: 8.03301759e-07
Iter: 827 loss: 7.99522695e-07
Iter: 828 loss: 7.98403903e-07
Iter: 829 loss: 8.01386079e-07
Iter: 830 loss: 7.98112069e-07
Iter: 831 loss: 7.97047107e-07
Iter: 832 loss: 7.99059762e-07
Iter: 833 loss: 7.96646077e-07
Iter: 834 loss: 7.95672406e-07
Iter: 835 loss: 7.96234531e-07
Iter: 836 loss: 7.94901439e-07
Iter: 837 loss: 7.95564233e-07
Iter: 838 loss: 7.9454162e-07
Iter: 839 loss: 7.94185269e-07
Iter: 840 loss: 7.93279469e-07
Iter: 841 loss: 8.00119778e-07
Iter: 842 loss: 7.93144636e-07
Iter: 843 loss: 7.92275728e-07
Iter: 844 loss: 7.94787923e-07
Iter: 845 loss: 7.9191534e-07
Iter: 846 loss: 7.91083721e-07
Iter: 847 loss: 7.90468903e-07
Iter: 848 loss: 7.90103741e-07
Iter: 849 loss: 7.88872683e-07
Iter: 850 loss: 8.06339244e-07
Iter: 851 loss: 7.88864838e-07
Iter: 852 loss: 7.88124225e-07
Iter: 853 loss: 7.88202271e-07
Iter: 854 loss: 7.87516569e-07
Iter: 855 loss: 7.86434157e-07
Iter: 856 loss: 7.93645654e-07
Iter: 857 loss: 7.8640312e-07
Iter: 858 loss: 7.85427574e-07
Iter: 859 loss: 7.856612e-07
Iter: 860 loss: 7.84804342e-07
Iter: 861 loss: 7.83532926e-07
Iter: 862 loss: 7.89641e-07
Iter: 863 loss: 7.83299e-07
Iter: 864 loss: 7.82497523e-07
Iter: 865 loss: 7.84110341e-07
Iter: 866 loss: 7.82071595e-07
Iter: 867 loss: 7.80980486e-07
Iter: 868 loss: 7.80677738e-07
Iter: 869 loss: 7.8003734e-07
Iter: 870 loss: 7.79384e-07
Iter: 871 loss: 7.79286324e-07
Iter: 872 loss: 7.78541562e-07
Iter: 873 loss: 7.8223718e-07
Iter: 874 loss: 7.78379217e-07
Iter: 875 loss: 7.77848243e-07
Iter: 876 loss: 7.76662489e-07
Iter: 877 loss: 7.94579421e-07
Iter: 878 loss: 7.76673232e-07
Iter: 879 loss: 7.75661192e-07
Iter: 880 loss: 7.77802825e-07
Iter: 881 loss: 7.7529694e-07
Iter: 882 loss: 7.74398188e-07
Iter: 883 loss: 7.81250492e-07
Iter: 884 loss: 7.74300247e-07
Iter: 885 loss: 7.73503132e-07
Iter: 886 loss: 7.72338922e-07
Iter: 887 loss: 7.72358533e-07
Iter: 888 loss: 7.71233204e-07
Iter: 889 loss: 7.71167549e-07
Iter: 890 loss: 7.70429551e-07
Iter: 891 loss: 7.70088775e-07
Iter: 892 loss: 7.69649091e-07
Iter: 893 loss: 7.68505117e-07
Iter: 894 loss: 7.76496961e-07
Iter: 895 loss: 7.68359939e-07
Iter: 896 loss: 7.67645076e-07
Iter: 897 loss: 7.66688117e-07
Iter: 898 loss: 7.66643666e-07
Iter: 899 loss: 7.65635e-07
Iter: 900 loss: 7.65654136e-07
Iter: 901 loss: 7.65101618e-07
Iter: 902 loss: 7.6483559e-07
Iter: 903 loss: 7.64514539e-07
Iter: 904 loss: 7.63708613e-07
Iter: 905 loss: 7.63698324e-07
Iter: 906 loss: 7.62991647e-07
Iter: 907 loss: 7.66029245e-07
Iter: 908 loss: 7.62854881e-07
Iter: 909 loss: 7.62527634e-07
Iter: 910 loss: 7.61712954e-07
Iter: 911 loss: 7.6440142e-07
Iter: 912 loss: 7.61224726e-07
Iter: 913 loss: 7.6049281e-07
Iter: 914 loss: 7.60426929e-07
Iter: 915 loss: 7.59807e-07
Iter: 916 loss: 7.58348506e-07
Iter: 917 loss: 7.77049422e-07
Iter: 918 loss: 7.58306271e-07
Iter: 919 loss: 7.57299915e-07
Iter: 920 loss: 7.57271891e-07
Iter: 921 loss: 7.56510133e-07
Iter: 922 loss: 7.55400265e-07
Iter: 923 loss: 7.55346548e-07
Iter: 924 loss: 7.54142775e-07
Iter: 925 loss: 7.69210885e-07
Iter: 926 loss: 7.54143116e-07
Iter: 927 loss: 7.53193717e-07
Iter: 928 loss: 7.54423695e-07
Iter: 929 loss: 7.52722e-07
Iter: 930 loss: 7.51751088e-07
Iter: 931 loss: 7.5326e-07
Iter: 932 loss: 7.51368248e-07
Iter: 933 loss: 7.50406286e-07
Iter: 934 loss: 7.581109e-07
Iter: 935 loss: 7.50263e-07
Iter: 936 loss: 7.49824721e-07
Iter: 937 loss: 7.54882137e-07
Iter: 938 loss: 7.49828814e-07
Iter: 939 loss: 7.49423066e-07
Iter: 940 loss: 7.50010486e-07
Iter: 941 loss: 7.49128674e-07
Iter: 942 loss: 7.48585308e-07
Iter: 943 loss: 7.48162677e-07
Iter: 944 loss: 7.47962929e-07
Iter: 945 loss: 7.47387162e-07
Iter: 946 loss: 7.47183833e-07
Iter: 947 loss: 7.46852436e-07
Iter: 948 loss: 7.4586552e-07
Iter: 949 loss: 7.5355365e-07
Iter: 950 loss: 7.45782472e-07
Iter: 951 loss: 7.45186526e-07
Iter: 952 loss: 7.46591411e-07
Iter: 953 loss: 7.44904128e-07
Iter: 954 loss: 7.44027375e-07
Iter: 955 loss: 7.44293061e-07
Iter: 956 loss: 7.43373e-07
Iter: 957 loss: 7.42757265e-07
Iter: 958 loss: 7.49389244e-07
Iter: 959 loss: 7.42730549e-07
Iter: 960 loss: 7.42153702e-07
Iter: 961 loss: 7.42400289e-07
Iter: 962 loss: 7.41679116e-07
Iter: 963 loss: 7.41100052e-07
Iter: 964 loss: 7.45092734e-07
Iter: 965 loss: 7.41037638e-07
Iter: 966 loss: 7.40441635e-07
Iter: 967 loss: 7.39408222e-07
Iter: 968 loss: 7.39437155e-07
Iter: 969 loss: 7.39160498e-07
Iter: 970 loss: 7.38971835e-07
Iter: 971 loss: 7.38576205e-07
Iter: 972 loss: 7.41169345e-07
Iter: 973 loss: 7.38610652e-07
Iter: 974 loss: 7.38174322e-07
Iter: 975 loss: 7.37627033e-07
Iter: 976 loss: 7.37657501e-07
Iter: 977 loss: 7.3706218e-07
Iter: 978 loss: 7.38071435e-07
Iter: 979 loss: 7.36895174e-07
Iter: 980 loss: 7.36257e-07
Iter: 981 loss: 7.36781203e-07
Iter: 982 loss: 7.35911215e-07
Iter: 983 loss: 7.35304e-07
Iter: 984 loss: 7.38971e-07
Iter: 985 loss: 7.35192486e-07
Iter: 986 loss: 7.34504624e-07
Iter: 987 loss: 7.34676519e-07
Iter: 988 loss: 7.34013952e-07
Iter: 989 loss: 7.33410502e-07
Iter: 990 loss: 7.37280686e-07
Iter: 991 loss: 7.33259299e-07
Iter: 992 loss: 7.32460308e-07
Iter: 993 loss: 7.31302862e-07
Iter: 994 loss: 7.31314913e-07
Iter: 995 loss: 7.30636259e-07
Iter: 996 loss: 7.3051433e-07
Iter: 997 loss: 7.29826922e-07
Iter: 998 loss: 7.28679822e-07
Iter: 999 loss: 7.53750783e-07
Iter: 1000 loss: 7.28665725e-07
Iter: 1001 loss: 7.27624e-07
Iter: 1002 loss: 7.27681e-07
Iter: 1003 loss: 7.27184158e-07
Iter: 1004 loss: 7.27130896e-07
Iter: 1005 loss: 7.26793587e-07
Iter: 1006 loss: 7.27087809e-07
Iter: 1007 loss: 7.26459e-07
Iter: 1008 loss: 7.26166149e-07
Iter: 1009 loss: 7.25932637e-07
Iter: 1010 loss: 7.25797406e-07
Iter: 1011 loss: 7.25224368e-07
Iter: 1012 loss: 7.24749384e-07
Iter: 1013 loss: 7.24626602e-07
Iter: 1014 loss: 7.2395261e-07
Iter: 1015 loss: 7.30728061e-07
Iter: 1016 loss: 7.2390867e-07
Iter: 1017 loss: 7.23416861e-07
Iter: 1018 loss: 7.24890924e-07
Iter: 1019 loss: 7.23167773e-07
Iter: 1020 loss: 7.22752134e-07
Iter: 1021 loss: 7.25041218e-07
Iter: 1022 loss: 7.22653056e-07
Iter: 1023 loss: 7.22193e-07
Iter: 1024 loss: 7.21596848e-07
Iter: 1025 loss: 7.21560241e-07
Iter: 1026 loss: 7.2099084e-07
Iter: 1027 loss: 7.29816691e-07
Iter: 1028 loss: 7.20977596e-07
Iter: 1029 loss: 7.2037335e-07
Iter: 1030 loss: 7.19847662e-07
Iter: 1031 loss: 7.19744151e-07
Iter: 1032 loss: 7.19025365e-07
Iter: 1033 loss: 7.26982e-07
Iter: 1034 loss: 7.19038383e-07
Iter: 1035 loss: 7.18451815e-07
Iter: 1036 loss: 7.1864315e-07
Iter: 1037 loss: 7.18144804e-07
Iter: 1038 loss: 7.17628836e-07
Iter: 1039 loss: 7.17520606e-07
Iter: 1040 loss: 7.17257876e-07
Iter: 1041 loss: 7.17083481e-07
Iter: 1042 loss: 7.17019475e-07
Iter: 1043 loss: 7.16597299e-07
Iter: 1044 loss: 7.15986403e-07
Iter: 1045 loss: 7.15957924e-07
Iter: 1046 loss: 7.15277224e-07
Iter: 1047 loss: 7.2054371e-07
Iter: 1048 loss: 7.15158762e-07
Iter: 1049 loss: 7.14569182e-07
Iter: 1050 loss: 7.144439e-07
Iter: 1051 loss: 7.14002397e-07
Iter: 1052 loss: 7.13495e-07
Iter: 1053 loss: 7.13481882e-07
Iter: 1054 loss: 7.12967108e-07
Iter: 1055 loss: 7.12296071e-07
Iter: 1056 loss: 7.1224872e-07
Iter: 1057 loss: 7.11556254e-07
Iter: 1058 loss: 7.17086778e-07
Iter: 1059 loss: 7.11488838e-07
Iter: 1060 loss: 7.10864697e-07
Iter: 1061 loss: 7.10758e-07
Iter: 1062 loss: 7.10297513e-07
Iter: 1063 loss: 7.09620736e-07
Iter: 1064 loss: 7.10902327e-07
Iter: 1065 loss: 7.0936818e-07
Iter: 1066 loss: 7.08542075e-07
Iter: 1067 loss: 7.09248638e-07
Iter: 1068 loss: 7.0809233e-07
Iter: 1069 loss: 7.07992513e-07
Iter: 1070 loss: 7.07834488e-07
Iter: 1071 loss: 7.075339e-07
Iter: 1072 loss: 7.07586082e-07
Iter: 1073 loss: 7.07341712e-07
Iter: 1074 loss: 7.06883498e-07
Iter: 1075 loss: 7.06048581e-07
Iter: 1076 loss: 7.21276308e-07
Iter: 1077 loss: 7.05972184e-07
Iter: 1078 loss: 7.05451725e-07
Iter: 1079 loss: 7.12173971e-07
Iter: 1080 loss: 7.05409093e-07
Iter: 1081 loss: 7.04899207e-07
Iter: 1082 loss: 7.0467e-07
Iter: 1083 loss: 7.04426725e-07
Iter: 1084 loss: 7.03887849e-07
Iter: 1085 loss: 7.09060487e-07
Iter: 1086 loss: 7.03970386e-07
Iter: 1087 loss: 7.03375065e-07
Iter: 1088 loss: 7.03658145e-07
Iter: 1089 loss: 7.03090677e-07
Iter: 1090 loss: 7.02502234e-07
Iter: 1091 loss: 7.06700234e-07
Iter: 1092 loss: 7.02413672e-07
Iter: 1093 loss: 7.02083184e-07
Iter: 1094 loss: 7.01762758e-07
Iter: 1095 loss: 7.01707108e-07
Iter: 1096 loss: 7.0109013e-07
Iter: 1097 loss: 7.0215242e-07
Iter: 1098 loss: 7.00854571e-07
Iter: 1099 loss: 7.00233841e-07
Iter: 1100 loss: 7.02499506e-07
Iter: 1101 loss: 7.00089572e-07
Iter: 1102 loss: 6.99528471e-07
Iter: 1103 loss: 7.00643284e-07
Iter: 1104 loss: 6.99270856e-07
Iter: 1105 loss: 6.98985389e-07
Iter: 1106 loss: 6.9898357e-07
Iter: 1107 loss: 6.9868554e-07
Iter: 1108 loss: 6.97917699e-07
Iter: 1109 loss: 7.01170165e-07
Iter: 1110 loss: 6.97644e-07
Iter: 1111 loss: 6.96908273e-07
Iter: 1112 loss: 7.05605203e-07
Iter: 1113 loss: 6.96916345e-07
Iter: 1114 loss: 6.96259633e-07
Iter: 1115 loss: 6.95199105e-07
Iter: 1116 loss: 6.95237247e-07
Iter: 1117 loss: 6.9417672e-07
Iter: 1118 loss: 7.1069519e-07
Iter: 1119 loss: 6.94210428e-07
Iter: 1120 loss: 6.93576794e-07
Iter: 1121 loss: 6.94105267e-07
Iter: 1122 loss: 6.9324858e-07
Iter: 1123 loss: 6.92393542e-07
Iter: 1124 loss: 6.96565962e-07
Iter: 1125 loss: 6.92248761e-07
Iter: 1126 loss: 6.91727223e-07
Iter: 1127 loss: 6.93152856e-07
Iter: 1128 loss: 6.91503601e-07
Iter: 1129 loss: 6.90914931e-07
Iter: 1130 loss: 6.91203809e-07
Iter: 1131 loss: 6.90523507e-07
Iter: 1132 loss: 6.89827402e-07
Iter: 1133 loss: 6.94670462e-07
Iter: 1134 loss: 6.89744752e-07
Iter: 1135 loss: 6.88962132e-07
Iter: 1136 loss: 6.88334808e-07
Iter: 1137 loss: 6.88166324e-07
Iter: 1138 loss: 6.87422244e-07
Iter: 1139 loss: 6.93629772e-07
Iter: 1140 loss: 6.87349598e-07
Iter: 1141 loss: 6.86708916e-07
Iter: 1142 loss: 6.9160194e-07
Iter: 1143 loss: 6.86709939e-07
Iter: 1144 loss: 6.85983309e-07
Iter: 1145 loss: 6.87576289e-07
Iter: 1146 loss: 6.85689884e-07
Iter: 1147 loss: 6.85411294e-07
Iter: 1148 loss: 6.85000828e-07
Iter: 1149 loss: 6.85001112e-07
Iter: 1150 loss: 6.84146926e-07
Iter: 1151 loss: 6.84290058e-07
Iter: 1152 loss: 6.83578662e-07
Iter: 1153 loss: 6.82908421e-07
Iter: 1154 loss: 6.90618663e-07
Iter: 1155 loss: 6.82882558e-07
Iter: 1156 loss: 6.82403652e-07
Iter: 1157 loss: 6.82057362e-07
Iter: 1158 loss: 6.81821746e-07
Iter: 1159 loss: 6.81265135e-07
Iter: 1160 loss: 6.90490424e-07
Iter: 1161 loss: 6.81230119e-07
Iter: 1162 loss: 6.80789867e-07
Iter: 1163 loss: 6.80241669e-07
Iter: 1164 loss: 6.80177436e-07
Iter: 1165 loss: 6.79401523e-07
Iter: 1166 loss: 6.85738542e-07
Iter: 1167 loss: 6.79328082e-07
Iter: 1168 loss: 6.78678816e-07
Iter: 1169 loss: 6.78343781e-07
Iter: 1170 loss: 6.78125161e-07
Iter: 1171 loss: 6.77732771e-07
Iter: 1172 loss: 6.7766814e-07
Iter: 1173 loss: 6.7738074e-07
Iter: 1174 loss: 6.77854871e-07
Iter: 1175 loss: 6.77291951e-07
Iter: 1176 loss: 6.76687819e-07
Iter: 1177 loss: 6.78448657e-07
Iter: 1178 loss: 6.76594823e-07
Iter: 1179 loss: 6.76116883e-07
Iter: 1180 loss: 6.76025707e-07
Iter: 1181 loss: 6.75685328e-07
Iter: 1182 loss: 6.7539537e-07
Iter: 1183 loss: 6.75563513e-07
Iter: 1184 loss: 6.75111039e-07
Iter: 1185 loss: 6.7465129e-07
Iter: 1186 loss: 6.76996194e-07
Iter: 1187 loss: 6.74636567e-07
Iter: 1188 loss: 6.74229454e-07
Iter: 1189 loss: 6.73489808e-07
Iter: 1190 loss: 6.88357318e-07
Iter: 1191 loss: 6.73463489e-07
Iter: 1192 loss: 6.72786769e-07
Iter: 1193 loss: 6.7278495e-07
Iter: 1194 loss: 6.72321335e-07
Iter: 1195 loss: 6.72615158e-07
Iter: 1196 loss: 6.71998578e-07
Iter: 1197 loss: 6.71520638e-07
Iter: 1198 loss: 6.75903777e-07
Iter: 1199 loss: 6.71463908e-07
Iter: 1200 loss: 6.7116116e-07
Iter: 1201 loss: 6.70762574e-07
Iter: 1202 loss: 6.707333e-07
Iter: 1203 loss: 6.70076361e-07
Iter: 1204 loss: 6.7181287e-07
Iter: 1205 loss: 6.69932888e-07
Iter: 1206 loss: 6.69461542e-07
Iter: 1207 loss: 6.70354041e-07
Iter: 1208 loss: 6.69301528e-07
Iter: 1209 loss: 6.68990879e-07
Iter: 1210 loss: 6.68965811e-07
Iter: 1211 loss: 6.68653911e-07
Iter: 1212 loss: 6.68306257e-07
Iter: 1213 loss: 6.6829449e-07
Iter: 1214 loss: 6.67888571e-07
Iter: 1215 loss: 6.67627432e-07
Iter: 1216 loss: 6.67419499e-07
Iter: 1217 loss: 6.667874e-07
Iter: 1218 loss: 6.70820668e-07
Iter: 1219 loss: 6.66755682e-07
Iter: 1220 loss: 6.66279448e-07
Iter: 1221 loss: 6.65702885e-07
Iter: 1222 loss: 6.65687878e-07
Iter: 1223 loss: 6.64849381e-07
Iter: 1224 loss: 6.72882834e-07
Iter: 1225 loss: 6.64791173e-07
Iter: 1226 loss: 6.64277422e-07
Iter: 1227 loss: 6.63781634e-07
Iter: 1228 loss: 6.63624462e-07
Iter: 1229 loss: 6.63033518e-07
Iter: 1230 loss: 6.63004869e-07
Iter: 1231 loss: 6.62644936e-07
Iter: 1232 loss: 6.625786e-07
Iter: 1233 loss: 6.62261755e-07
Iter: 1234 loss: 6.6153234e-07
Iter: 1235 loss: 6.62196328e-07
Iter: 1236 loss: 6.61123408e-07
Iter: 1237 loss: 6.6042054e-07
Iter: 1238 loss: 6.61631816e-07
Iter: 1239 loss: 6.60176511e-07
Iter: 1240 loss: 6.59829254e-07
Iter: 1241 loss: 6.59730915e-07
Iter: 1242 loss: 6.59407647e-07
Iter: 1243 loss: 6.59510818e-07
Iter: 1244 loss: 6.59147872e-07
Iter: 1245 loss: 6.588931e-07
Iter: 1246 loss: 6.58443e-07
Iter: 1247 loss: 6.584429e-07
Iter: 1248 loss: 6.57800399e-07
Iter: 1249 loss: 6.62115781e-07
Iter: 1250 loss: 6.57793692e-07
Iter: 1251 loss: 6.57343321e-07
Iter: 1252 loss: 6.57352302e-07
Iter: 1253 loss: 6.57003e-07
Iter: 1254 loss: 6.56247323e-07
Iter: 1255 loss: 6.58139641e-07
Iter: 1256 loss: 6.55983968e-07
Iter: 1257 loss: 6.55287408e-07
Iter: 1258 loss: 6.54866938e-07
Iter: 1259 loss: 6.54561518e-07
Iter: 1260 loss: 6.53481777e-07
Iter: 1261 loss: 6.64159131e-07
Iter: 1262 loss: 6.53408506e-07
Iter: 1263 loss: 6.52747758e-07
Iter: 1264 loss: 6.54388259e-07
Iter: 1265 loss: 6.52567621e-07
Iter: 1266 loss: 6.5183e-07
Iter: 1267 loss: 6.52556537e-07
Iter: 1268 loss: 6.51275059e-07
Iter: 1269 loss: 6.5060749e-07
Iter: 1270 loss: 6.50081574e-07
Iter: 1271 loss: 6.49870231e-07
Iter: 1272 loss: 6.49226195e-07
Iter: 1273 loss: 6.4922591e-07
Iter: 1274 loss: 6.48998537e-07
Iter: 1275 loss: 6.48913272e-07
Iter: 1276 loss: 6.48709033e-07
Iter: 1277 loss: 6.48009632e-07
Iter: 1278 loss: 6.48654805e-07
Iter: 1279 loss: 6.47553861e-07
Iter: 1280 loss: 6.46673925e-07
Iter: 1281 loss: 6.55387964e-07
Iter: 1282 loss: 6.46594401e-07
Iter: 1283 loss: 6.46056719e-07
Iter: 1284 loss: 6.45845034e-07
Iter: 1285 loss: 6.45505395e-07
Iter: 1286 loss: 6.44955321e-07
Iter: 1287 loss: 6.44956401e-07
Iter: 1288 loss: 6.44650811e-07
Iter: 1289 loss: 6.44382e-07
Iter: 1290 loss: 6.44309864e-07
Iter: 1291 loss: 6.43771841e-07
Iter: 1292 loss: 6.43748194e-07
Iter: 1293 loss: 6.43381156e-07
Iter: 1294 loss: 6.42832219e-07
Iter: 1295 loss: 6.44183956e-07
Iter: 1296 loss: 6.42679765e-07
Iter: 1297 loss: 6.42071711e-07
Iter: 1298 loss: 6.45007844e-07
Iter: 1299 loss: 6.41935856e-07
Iter: 1300 loss: 6.41491226e-07
Iter: 1301 loss: 6.41003226e-07
Iter: 1302 loss: 6.40937799e-07
Iter: 1303 loss: 6.40226745e-07
Iter: 1304 loss: 6.49383651e-07
Iter: 1305 loss: 6.4023709e-07
Iter: 1306 loss: 6.39800305e-07
Iter: 1307 loss: 6.39162124e-07
Iter: 1308 loss: 6.39101131e-07
Iter: 1309 loss: 6.38646384e-07
Iter: 1310 loss: 6.38630695e-07
Iter: 1311 loss: 6.3827855e-07
Iter: 1312 loss: 6.38313509e-07
Iter: 1313 loss: 6.38119e-07
Iter: 1314 loss: 6.37619621e-07
Iter: 1315 loss: 6.41127713e-07
Iter: 1316 loss: 6.37368885e-07
Iter: 1317 loss: 6.370135e-07
Iter: 1318 loss: 6.39427185e-07
Iter: 1319 loss: 6.36910045e-07
Iter: 1320 loss: 6.36569496e-07
Iter: 1321 loss: 6.36482e-07
Iter: 1322 loss: 6.3620746e-07
Iter: 1323 loss: 6.35579568e-07
Iter: 1324 loss: 6.38053734e-07
Iter: 1325 loss: 6.35515903e-07
Iter: 1326 loss: 6.35089123e-07
Iter: 1327 loss: 6.35587924e-07
Iter: 1328 loss: 6.34864e-07
Iter: 1329 loss: 6.34374203e-07
Iter: 1330 loss: 6.36097354e-07
Iter: 1331 loss: 6.34153366e-07
Iter: 1332 loss: 6.3370419e-07
Iter: 1333 loss: 6.3373659e-07
Iter: 1334 loss: 6.3334528e-07
Iter: 1335 loss: 6.3266026e-07
Iter: 1336 loss: 6.37457333e-07
Iter: 1337 loss: 6.32584204e-07
Iter: 1338 loss: 6.32132299e-07
Iter: 1339 loss: 6.32170213e-07
Iter: 1340 loss: 6.31799367e-07
Iter: 1341 loss: 6.31006117e-07
Iter: 1342 loss: 6.32305159e-07
Iter: 1343 loss: 6.30654256e-07
Iter: 1344 loss: 6.29964063e-07
Iter: 1345 loss: 6.32669412e-07
Iter: 1346 loss: 6.29900342e-07
Iter: 1347 loss: 6.29374881e-07
Iter: 1348 loss: 6.29373289e-07
Iter: 1349 loss: 6.29076055e-07
Iter: 1350 loss: 6.28842372e-07
Iter: 1351 loss: 6.28787461e-07
Iter: 1352 loss: 6.28524504e-07
Iter: 1353 loss: 6.27766099e-07
Iter: 1354 loss: 6.35726337e-07
Iter: 1355 loss: 6.27753366e-07
Iter: 1356 loss: 6.27105123e-07
Iter: 1357 loss: 6.27138093e-07
Iter: 1358 loss: 6.26562894e-07
Iter: 1359 loss: 6.26008e-07
Iter: 1360 loss: 6.25895836e-07
Iter: 1361 loss: 6.25071323e-07
Iter: 1362 loss: 6.2890058e-07
Iter: 1363 loss: 6.24899883e-07
Iter: 1364 loss: 6.24115444e-07
Iter: 1365 loss: 6.2942047e-07
Iter: 1366 loss: 6.24018242e-07
Iter: 1367 loss: 6.23547123e-07
Iter: 1368 loss: 6.25576888e-07
Iter: 1369 loss: 6.23446567e-07
Iter: 1370 loss: 6.2298767e-07
Iter: 1371 loss: 6.22526954e-07
Iter: 1372 loss: 6.22305606e-07
Iter: 1373 loss: 6.21858817e-07
Iter: 1374 loss: 6.26052497e-07
Iter: 1375 loss: 6.21832783e-07
Iter: 1376 loss: 6.21441927e-07
Iter: 1377 loss: 6.21838126e-07
Iter: 1378 loss: 6.21151912e-07
Iter: 1379 loss: 6.20639753e-07
Iter: 1380 loss: 6.24540121e-07
Iter: 1381 loss: 6.20628612e-07
Iter: 1382 loss: 6.20301137e-07
Iter: 1383 loss: 6.21250706e-07
Iter: 1384 loss: 6.2017341e-07
Iter: 1385 loss: 6.19615207e-07
Iter: 1386 loss: 6.21768947e-07
Iter: 1387 loss: 6.19493051e-07
Iter: 1388 loss: 6.19147272e-07
Iter: 1389 loss: 6.190877e-07
Iter: 1390 loss: 6.18870899e-07
Iter: 1391 loss: 6.18574575e-07
Iter: 1392 loss: 6.17856244e-07
Iter: 1393 loss: 6.26682e-07
Iter: 1394 loss: 6.17799174e-07
Iter: 1395 loss: 6.17220849e-07
Iter: 1396 loss: 6.17267631e-07
Iter: 1397 loss: 6.1672722e-07
Iter: 1398 loss: 6.16945727e-07
Iter: 1399 loss: 6.16382806e-07
Iter: 1400 loss: 6.1594875e-07
Iter: 1401 loss: 6.18908928e-07
Iter: 1402 loss: 6.15888553e-07
Iter: 1403 loss: 6.15383e-07
Iter: 1404 loss: 6.1505267e-07
Iter: 1405 loss: 6.14862813e-07
Iter: 1406 loss: 6.1443086e-07
Iter: 1407 loss: 6.20622245e-07
Iter: 1408 loss: 6.14425517e-07
Iter: 1409 loss: 6.14113674e-07
Iter: 1410 loss: 6.13686211e-07
Iter: 1411 loss: 6.13613565e-07
Iter: 1412 loss: 6.13085831e-07
Iter: 1413 loss: 6.17532464e-07
Iter: 1414 loss: 6.13039276e-07
Iter: 1415 loss: 6.12765064e-07
Iter: 1416 loss: 6.12735e-07
Iter: 1417 loss: 6.1255264e-07
Iter: 1418 loss: 6.11986479e-07
Iter: 1419 loss: 6.14141754e-07
Iter: 1420 loss: 6.11872e-07
Iter: 1421 loss: 6.11580276e-07
Iter: 1422 loss: 6.11575956e-07
Iter: 1423 loss: 6.11444875e-07
Iter: 1424 loss: 6.11086136e-07
Iter: 1425 loss: 6.1551151e-07
Iter: 1426 loss: 6.11069254e-07
Iter: 1427 loss: 6.1067135e-07
Iter: 1428 loss: 6.10125312e-07
Iter: 1429 loss: 6.10115535e-07
Iter: 1430 loss: 6.09455356e-07
Iter: 1431 loss: 6.16089721e-07
Iter: 1432 loss: 6.09456379e-07
Iter: 1433 loss: 6.0904631e-07
Iter: 1434 loss: 6.09025847e-07
Iter: 1435 loss: 6.08655796e-07
Iter: 1436 loss: 6.08193773e-07
Iter: 1437 loss: 6.1027265e-07
Iter: 1438 loss: 6.08104e-07
Iter: 1439 loss: 6.07523248e-07
Iter: 1440 loss: 6.09150845e-07
Iter: 1441 loss: 6.07388472e-07
Iter: 1442 loss: 6.06967149e-07
Iter: 1443 loss: 6.08196387e-07
Iter: 1444 loss: 6.06894901e-07
Iter: 1445 loss: 6.06398601e-07
Iter: 1446 loss: 6.07192192e-07
Iter: 1447 loss: 6.06229037e-07
Iter: 1448 loss: 6.0581e-07
Iter: 1449 loss: 6.08246182e-07
Iter: 1450 loss: 6.05702894e-07
Iter: 1451 loss: 6.05342109e-07
Iter: 1452 loss: 6.04758043e-07
Iter: 1453 loss: 6.04725528e-07
Iter: 1454 loss: 6.05107175e-07
Iter: 1455 loss: 6.04542e-07
Iter: 1456 loss: 6.04402e-07
Iter: 1457 loss: 6.04318245e-07
Iter: 1458 loss: 6.04325066e-07
Iter: 1459 loss: 6.04048239e-07
Iter: 1460 loss: 6.03772548e-07
Iter: 1461 loss: 6.03687909e-07
Iter: 1462 loss: 6.03295575e-07
Iter: 1463 loss: 6.05377181e-07
Iter: 1464 loss: 6.03286139e-07
Iter: 1465 loss: 6.02971113e-07
Iter: 1466 loss: 6.0259049e-07
Iter: 1467 loss: 6.02570822e-07
Iter: 1468 loss: 6.02062073e-07
Iter: 1469 loss: 6.02879709e-07
Iter: 1470 loss: 6.01845159e-07
Iter: 1471 loss: 6.01345789e-07
Iter: 1472 loss: 6.06840274e-07
Iter: 1473 loss: 6.01246541e-07
Iter: 1474 loss: 6.00998362e-07
Iter: 1475 loss: 6.00789292e-07
Iter: 1476 loss: 6.00701583e-07
Iter: 1477 loss: 6.00148326e-07
Iter: 1478 loss: 6.03005617e-07
Iter: 1479 loss: 6.00130079e-07
Iter: 1480 loss: 5.99793395e-07
Iter: 1481 loss: 5.99981149e-07
Iter: 1482 loss: 5.99600867e-07
Iter: 1483 loss: 5.99003556e-07
Iter: 1484 loss: 6.00710564e-07
Iter: 1485 loss: 5.98892655e-07
Iter: 1486 loss: 5.98473548e-07
Iter: 1487 loss: 6.01090733e-07
Iter: 1488 loss: 5.98473321e-07
Iter: 1489 loss: 5.98172221e-07
Iter: 1490 loss: 5.9820286e-07
Iter: 1491 loss: 5.97981796e-07
Iter: 1492 loss: 5.97724352e-07
Iter: 1493 loss: 5.9768405e-07
Iter: 1494 loss: 5.97421433e-07
Iter: 1495 loss: 5.97550581e-07
Iter: 1496 loss: 5.97211852e-07
Iter: 1497 loss: 5.96706514e-07
Iter: 1498 loss: 5.97441499e-07
Iter: 1499 loss: 5.96445375e-07
Iter: 1500 loss: 5.96097834e-07
Iter: 1501 loss: 5.97112262e-07
Iter: 1502 loss: 5.96025927e-07
Iter: 1503 loss: 5.95502797e-07
Iter: 1504 loss: 5.95642064e-07
Iter: 1505 loss: 5.9515844e-07
Iter: 1506 loss: 5.94781e-07
Iter: 1507 loss: 6.00593125e-07
Iter: 1508 loss: 5.9476821e-07
Iter: 1509 loss: 5.94481776e-07
Iter: 1510 loss: 5.93890547e-07
Iter: 1511 loss: 5.93944947e-07
Iter: 1512 loss: 5.93340928e-07
Iter: 1513 loss: 6.00029921e-07
Iter: 1514 loss: 5.9335963e-07
Iter: 1515 loss: 5.92925971e-07
Iter: 1516 loss: 5.92812626e-07
Iter: 1517 loss: 5.92538527e-07
Iter: 1518 loss: 5.92171034e-07
Iter: 1519 loss: 5.96074813e-07
Iter: 1520 loss: 5.92149831e-07
Iter: 1521 loss: 5.91841172e-07
Iter: 1522 loss: 5.96254097e-07
Iter: 1523 loss: 5.91809339e-07
Iter: 1524 loss: 5.91440937e-07
Iter: 1525 loss: 5.91895173e-07
Iter: 1526 loss: 5.9131014e-07
Iter: 1527 loss: 5.91114144e-07
Iter: 1528 loss: 5.90972888e-07
Iter: 1529 loss: 5.90908542e-07
Iter: 1530 loss: 5.90552133e-07
Iter: 1531 loss: 5.90841751e-07
Iter: 1532 loss: 5.90299805e-07
Iter: 1533 loss: 5.8993021e-07
Iter: 1534 loss: 5.94269409e-07
Iter: 1535 loss: 5.89939759e-07
Iter: 1536 loss: 5.89629963e-07
Iter: 1537 loss: 5.89160493e-07
Iter: 1538 loss: 5.89149863e-07
Iter: 1539 loss: 5.88668627e-07
Iter: 1540 loss: 5.92225149e-07
Iter: 1541 loss: 5.88636e-07
Iter: 1542 loss: 5.88218e-07
Iter: 1543 loss: 5.88096214e-07
Iter: 1544 loss: 5.87843033e-07
Iter: 1545 loss: 5.87428588e-07
Iter: 1546 loss: 5.87440923e-07
Iter: 1547 loss: 5.87142779e-07
Iter: 1548 loss: 5.86661884e-07
Iter: 1549 loss: 5.98611621e-07
Iter: 1550 loss: 5.86664441e-07
Iter: 1551 loss: 5.86290582e-07
Iter: 1552 loss: 5.89360411e-07
Iter: 1553 loss: 5.8621788e-07
Iter: 1554 loss: 5.85929229e-07
Iter: 1555 loss: 5.85650469e-07
Iter: 1556 loss: 5.85548548e-07
Iter: 1557 loss: 5.85190946e-07
Iter: 1558 loss: 5.88998887e-07
Iter: 1559 loss: 5.8512228e-07
Iter: 1560 loss: 5.84868474e-07
Iter: 1561 loss: 5.87707746e-07
Iter: 1562 loss: 5.84835561e-07
Iter: 1563 loss: 5.84492625e-07
Iter: 1564 loss: 5.85522e-07
Iter: 1565 loss: 5.8446318e-07
Iter: 1566 loss: 5.84350801e-07
Iter: 1567 loss: 5.84115e-07
Iter: 1568 loss: 5.84101144e-07
Iter: 1569 loss: 5.83726603e-07
Iter: 1570 loss: 5.84606e-07
Iter: 1571 loss: 5.8366038e-07
Iter: 1572 loss: 5.8334183e-07
Iter: 1573 loss: 5.86163083e-07
Iter: 1574 loss: 5.83300846e-07
Iter: 1575 loss: 5.83036e-07
Iter: 1576 loss: 5.83037036e-07
Iter: 1577 loss: 5.82871166e-07
Iter: 1578 loss: 5.82570635e-07
Iter: 1579 loss: 5.84410031e-07
Iter: 1580 loss: 5.82534312e-07
Iter: 1581 loss: 5.82274765e-07
Iter: 1582 loss: 5.81780341e-07
Iter: 1583 loss: 5.92766867e-07
Iter: 1584 loss: 5.81771e-07
Iter: 1585 loss: 5.81360553e-07
Iter: 1586 loss: 5.81396534e-07
Iter: 1587 loss: 5.81020686e-07
Iter: 1588 loss: 5.81205654e-07
Iter: 1589 loss: 5.8075068e-07
Iter: 1590 loss: 5.80414394e-07
Iter: 1591 loss: 5.82640212e-07
Iter: 1592 loss: 5.80389496e-07
Iter: 1593 loss: 5.79931054e-07
Iter: 1594 loss: 5.80192363e-07
Iter: 1595 loss: 5.79639845e-07
Iter: 1596 loss: 5.79582547e-07
Iter: 1597 loss: 5.79378252e-07
Iter: 1598 loss: 5.79269795e-07
Iter: 1599 loss: 5.79066864e-07
Iter: 1600 loss: 5.82090479e-07
Iter: 1601 loss: 5.7902048e-07
Iter: 1602 loss: 5.78597906e-07
Iter: 1603 loss: 5.78306413e-07
Iter: 1604 loss: 5.78237575e-07
Iter: 1605 loss: 5.77807896e-07
Iter: 1606 loss: 5.79141897e-07
Iter: 1607 loss: 5.7772e-07
Iter: 1608 loss: 5.77306196e-07
Iter: 1609 loss: 5.77401067e-07
Iter: 1610 loss: 5.77050514e-07
Iter: 1611 loss: 5.76642947e-07
Iter: 1612 loss: 5.79946573e-07
Iter: 1613 loss: 5.76627428e-07
Iter: 1614 loss: 5.76193145e-07
Iter: 1615 loss: 5.76119589e-07
Iter: 1616 loss: 5.7591626e-07
Iter: 1617 loss: 5.75428714e-07
Iter: 1618 loss: 5.75801437e-07
Iter: 1619 loss: 5.75145123e-07
Iter: 1620 loss: 5.74615513e-07
Iter: 1621 loss: 5.81655399e-07
Iter: 1622 loss: 5.74575495e-07
Iter: 1623 loss: 5.74291448e-07
Iter: 1624 loss: 5.7411205e-07
Iter: 1625 loss: 5.73977161e-07
Iter: 1626 loss: 5.73472619e-07
Iter: 1627 loss: 5.78296067e-07
Iter: 1628 loss: 5.73454088e-07
Iter: 1629 loss: 5.73197156e-07
Iter: 1630 loss: 5.74618696e-07
Iter: 1631 loss: 5.73222792e-07
Iter: 1632 loss: 5.72946078e-07
Iter: 1633 loss: 5.74146952e-07
Iter: 1634 loss: 5.72887529e-07
Iter: 1635 loss: 5.72622412e-07
Iter: 1636 loss: 5.72311365e-07
Iter: 1637 loss: 5.72302952e-07
Iter: 1638 loss: 5.71928e-07
Iter: 1639 loss: 5.72630029e-07
Iter: 1640 loss: 5.71810347e-07
Iter: 1641 loss: 5.71272835e-07
Iter: 1642 loss: 5.72600584e-07
Iter: 1643 loss: 5.71081159e-07
Iter: 1644 loss: 5.70664724e-07
Iter: 1645 loss: 5.71891746e-07
Iter: 1646 loss: 5.70521706e-07
Iter: 1647 loss: 5.69965039e-07
Iter: 1648 loss: 5.70446559e-07
Iter: 1649 loss: 5.69741587e-07
Iter: 1650 loss: 5.69352e-07
Iter: 1651 loss: 5.73354441e-07
Iter: 1652 loss: 5.6930412e-07
Iter: 1653 loss: 5.69032e-07
Iter: 1654 loss: 5.68545147e-07
Iter: 1655 loss: 5.68528435e-07
Iter: 1656 loss: 5.68211e-07
Iter: 1657 loss: 5.68204086e-07
Iter: 1658 loss: 5.67898894e-07
Iter: 1659 loss: 5.67609732e-07
Iter: 1660 loss: 5.67482687e-07
Iter: 1661 loss: 5.67157144e-07
Iter: 1662 loss: 5.67133384e-07
Iter: 1663 loss: 5.66933636e-07
Iter: 1664 loss: 5.680144e-07
Iter: 1665 loss: 5.66882591e-07
Iter: 1666 loss: 5.66533174e-07
Iter: 1667 loss: 5.66584959e-07
Iter: 1668 loss: 5.66306937e-07
Iter: 1669 loss: 5.65980372e-07
Iter: 1670 loss: 5.66251913e-07
Iter: 1671 loss: 5.65900734e-07
Iter: 1672 loss: 5.65462756e-07
Iter: 1673 loss: 5.65476284e-07
Iter: 1674 loss: 5.65187065e-07
Iter: 1675 loss: 5.64766253e-07
Iter: 1676 loss: 5.70494308e-07
Iter: 1677 loss: 5.64846346e-07
Iter: 1678 loss: 5.64530296e-07
Iter: 1679 loss: 5.64173888e-07
Iter: 1680 loss: 5.6421527e-07
Iter: 1681 loss: 5.63759897e-07
Iter: 1682 loss: 5.66951485e-07
Iter: 1683 loss: 5.63680715e-07
Iter: 1684 loss: 5.63406616e-07
Iter: 1685 loss: 5.63285766e-07
Iter: 1686 loss: 5.63114156e-07
Iter: 1687 loss: 5.62730349e-07
Iter: 1688 loss: 5.65935181e-07
Iter: 1689 loss: 5.62669584e-07
Iter: 1690 loss: 5.62391961e-07
Iter: 1691 loss: 5.61975753e-07
Iter: 1692 loss: 5.6194051e-07
Iter: 1693 loss: 5.61599222e-07
Iter: 1694 loss: 5.67814595e-07
Iter: 1695 loss: 5.61571e-07
Iter: 1696 loss: 5.61354341e-07
Iter: 1697 loss: 5.63003653e-07
Iter: 1698 loss: 5.61360196e-07
Iter: 1699 loss: 5.61112756e-07
Iter: 1700 loss: 5.61824436e-07
Iter: 1701 loss: 5.61032095e-07
Iter: 1702 loss: 5.60915964e-07
Iter: 1703 loss: 5.60703427e-07
Iter: 1704 loss: 5.65275343e-07
Iter: 1705 loss: 5.60691831e-07
Iter: 1706 loss: 5.60392323e-07
Iter: 1707 loss: 5.60078831e-07
Iter: 1708 loss: 5.60024091e-07
Iter: 1709 loss: 5.59737941e-07
Iter: 1710 loss: 5.59733508e-07
Iter: 1711 loss: 5.59551154e-07
Iter: 1712 loss: 5.59242892e-07
Iter: 1713 loss: 5.59289333e-07
Iter: 1714 loss: 5.58908e-07
Iter: 1715 loss: 5.62898435e-07
Iter: 1716 loss: 5.58869715e-07
Iter: 1717 loss: 5.5864831e-07
Iter: 1718 loss: 5.58401666e-07
Iter: 1719 loss: 5.58383306e-07
Iter: 1720 loss: 5.57913097e-07
Iter: 1721 loss: 5.61790785e-07
Iter: 1722 loss: 5.57965336e-07
Iter: 1723 loss: 5.57563681e-07
Iter: 1724 loss: 5.571016e-07
Iter: 1725 loss: 5.57093131e-07
Iter: 1726 loss: 5.56506166e-07
Iter: 1727 loss: 5.62147761e-07
Iter: 1728 loss: 5.56484167e-07
Iter: 1729 loss: 5.55947736e-07
Iter: 1730 loss: 5.56269e-07
Iter: 1731 loss: 5.55580755e-07
Iter: 1732 loss: 5.56062332e-07
Iter: 1733 loss: 5.5540329e-07
Iter: 1734 loss: 5.55304723e-07
Iter: 1735 loss: 5.54944904e-07
Iter: 1736 loss: 5.55675797e-07
Iter: 1737 loss: 5.54809844e-07
Iter: 1738 loss: 5.54062581e-07
Iter: 1739 loss: 5.56845237e-07
Iter: 1740 loss: 5.53918937e-07
Iter: 1741 loss: 5.53345046e-07
Iter: 1742 loss: 5.55941256e-07
Iter: 1743 loss: 5.53201289e-07
Iter: 1744 loss: 5.52677875e-07
Iter: 1745 loss: 5.55098097e-07
Iter: 1746 loss: 5.52577148e-07
Iter: 1747 loss: 5.52088522e-07
Iter: 1748 loss: 5.55831e-07
Iter: 1749 loss: 5.520684e-07
Iter: 1750 loss: 5.51720348e-07
Iter: 1751 loss: 5.52412757e-07
Iter: 1752 loss: 5.51539244e-07
Iter: 1753 loss: 5.51225867e-07
Iter: 1754 loss: 5.51087851e-07
Iter: 1755 loss: 5.50904474e-07
Iter: 1756 loss: 5.50500033e-07
Iter: 1757 loss: 5.5627e-07
Iter: 1758 loss: 5.50473942e-07
Iter: 1759 loss: 5.50292498e-07
Iter: 1760 loss: 5.50478831e-07
Iter: 1761 loss: 5.50165737e-07
Iter: 1762 loss: 5.49872368e-07
Iter: 1763 loss: 5.50665391e-07
Iter: 1764 loss: 5.4984389e-07
Iter: 1765 loss: 5.49620268e-07
Iter: 1766 loss: 5.4935515e-07
Iter: 1767 loss: 5.49273295e-07
Iter: 1768 loss: 5.49104413e-07
Iter: 1769 loss: 5.49060815e-07
Iter: 1770 loss: 5.48887158e-07
Iter: 1771 loss: 5.50867753e-07
Iter: 1772 loss: 5.48849926e-07
Iter: 1773 loss: 5.48775461e-07
Iter: 1774 loss: 5.484236e-07
Iter: 1775 loss: 5.49559104e-07
Iter: 1776 loss: 5.48340836e-07
Iter: 1777 loss: 5.48017283e-07
Iter: 1778 loss: 5.50662207e-07
Iter: 1779 loss: 5.47996081e-07
Iter: 1780 loss: 5.47725222e-07
Iter: 1781 loss: 5.48370053e-07
Iter: 1782 loss: 5.47667e-07
Iter: 1783 loss: 5.47320951e-07
Iter: 1784 loss: 5.49056722e-07
Iter: 1785 loss: 5.47358241e-07
Iter: 1786 loss: 5.47091759e-07
Iter: 1787 loss: 5.4720573e-07
Iter: 1788 loss: 5.46903664e-07
Iter: 1789 loss: 5.46647925e-07
Iter: 1790 loss: 5.47862896e-07
Iter: 1791 loss: 5.46639399e-07
Iter: 1792 loss: 5.46420438e-07
Iter: 1793 loss: 5.46480237e-07
Iter: 1794 loss: 5.46236379e-07
Iter: 1795 loss: 5.45956652e-07
Iter: 1796 loss: 5.48411322e-07
Iter: 1797 loss: 5.45946591e-07
Iter: 1798 loss: 5.45710805e-07
Iter: 1799 loss: 5.45375e-07
Iter: 1800 loss: 5.45356272e-07
Iter: 1801 loss: 5.44963484e-07
Iter: 1802 loss: 5.44965758e-07
Iter: 1803 loss: 5.44882141e-07
Iter: 1804 loss: 5.44834677e-07
Iter: 1805 loss: 5.44770273e-07
Iter: 1806 loss: 5.44391128e-07
Iter: 1807 loss: 5.46811805e-07
Iter: 1808 loss: 5.44305067e-07
Iter: 1809 loss: 5.43979183e-07
Iter: 1810 loss: 5.46271508e-07
Iter: 1811 loss: 5.43944736e-07
Iter: 1812 loss: 5.43693375e-07
Iter: 1813 loss: 5.43618512e-07
Iter: 1814 loss: 5.43470321e-07
Iter: 1815 loss: 5.431869e-07
Iter: 1816 loss: 5.47108243e-07
Iter: 1817 loss: 5.43224928e-07
Iter: 1818 loss: 5.42904331e-07
Iter: 1819 loss: 5.44207353e-07
Iter: 1820 loss: 5.42888927e-07
Iter: 1821 loss: 5.42739713e-07
Iter: 1822 loss: 5.43196336e-07
Iter: 1823 loss: 5.42567363e-07
Iter: 1824 loss: 5.42412465e-07
Iter: 1825 loss: 5.42098803e-07
Iter: 1826 loss: 5.42023713e-07
Iter: 1827 loss: 5.41673103e-07
Iter: 1828 loss: 5.45548687e-07
Iter: 1829 loss: 5.41663724e-07
Iter: 1830 loss: 5.41394115e-07
Iter: 1831 loss: 5.41725797e-07
Iter: 1832 loss: 5.41234215e-07
Iter: 1833 loss: 5.40998599e-07
Iter: 1834 loss: 5.43047804e-07
Iter: 1835 loss: 5.4097319e-07
Iter: 1836 loss: 5.40662768e-07
Iter: 1837 loss: 5.40191877e-07
Iter: 1838 loss: 5.40215126e-07
Iter: 1839 loss: 5.40043118e-07
Iter: 1840 loss: 5.39979396e-07
Iter: 1841 loss: 5.39858206e-07
Iter: 1842 loss: 5.39883558e-07
Iter: 1843 loss: 5.39716268e-07
Iter: 1844 loss: 5.39465418e-07
Iter: 1845 loss: 5.42572593e-07
Iter: 1846 loss: 5.39483437e-07
Iter: 1847 loss: 5.39247822e-07
Iter: 1848 loss: 5.39738835e-07
Iter: 1849 loss: 5.39139421e-07
Iter: 1850 loss: 5.38843551e-07
Iter: 1851 loss: 5.39574216e-07
Iter: 1852 loss: 5.38784604e-07
Iter: 1853 loss: 5.38556264e-07
Iter: 1854 loss: 5.40626161e-07
Iter: 1855 loss: 5.38584e-07
Iter: 1856 loss: 5.3835646e-07
Iter: 1857 loss: 5.38172287e-07
Iter: 1858 loss: 5.38213158e-07
Iter: 1859 loss: 5.37898757e-07
Iter: 1860 loss: 5.39679718e-07
Iter: 1861 loss: 5.37796154e-07
Iter: 1862 loss: 5.37589358e-07
Iter: 1863 loss: 5.37004e-07
Iter: 1864 loss: 5.49416313e-07
Iter: 1865 loss: 5.37019673e-07
Iter: 1866 loss: 5.36652351e-07
Iter: 1867 loss: 5.36658547e-07
Iter: 1868 loss: 5.36239e-07
Iter: 1869 loss: 5.35857794e-07
Iter: 1870 loss: 5.35837444e-07
Iter: 1871 loss: 5.35676577e-07
Iter: 1872 loss: 5.35590345e-07
Iter: 1873 loss: 5.35425386e-07
Iter: 1874 loss: 5.36477e-07
Iter: 1875 loss: 5.35403444e-07
Iter: 1876 loss: 5.35230186e-07
Iter: 1877 loss: 5.35023048e-07
Iter: 1878 loss: 5.35034815e-07
Iter: 1879 loss: 5.34692788e-07
Iter: 1880 loss: 5.34785727e-07
Iter: 1881 loss: 5.34496507e-07
Iter: 1882 loss: 5.3420365e-07
Iter: 1883 loss: 5.34405387e-07
Iter: 1884 loss: 5.34065464e-07
Iter: 1885 loss: 5.33675575e-07
Iter: 1886 loss: 5.38199345e-07
Iter: 1887 loss: 5.33647267e-07
Iter: 1888 loss: 5.33415232e-07
Iter: 1889 loss: 5.33301773e-07
Iter: 1890 loss: 5.33182117e-07
Iter: 1891 loss: 5.32784611e-07
Iter: 1892 loss: 5.35664697e-07
Iter: 1893 loss: 5.3277256e-07
Iter: 1894 loss: 5.3249039e-07
Iter: 1895 loss: 5.32619538e-07
Iter: 1896 loss: 5.32328272e-07
Iter: 1897 loss: 5.32000797e-07
Iter: 1898 loss: 5.32707247e-07
Iter: 1899 loss: 5.31834417e-07
Iter: 1900 loss: 5.31578451e-07
Iter: 1901 loss: 5.31585329e-07
Iter: 1902 loss: 5.31383762e-07
Iter: 1903 loss: 5.31123248e-07
Iter: 1904 loss: 5.31114779e-07
Iter: 1905 loss: 5.30971647e-07
Iter: 1906 loss: 5.30985744e-07
Iter: 1907 loss: 5.30856255e-07
Iter: 1908 loss: 5.30581758e-07
Iter: 1909 loss: 5.32840943e-07
Iter: 1910 loss: 5.30587101e-07
Iter: 1911 loss: 5.30454827e-07
Iter: 1912 loss: 5.30445959e-07
Iter: 1913 loss: 5.3037428e-07
Iter: 1914 loss: 5.30218e-07
Iter: 1915 loss: 5.29825172e-07
Iter: 1916 loss: 5.35149354e-07
Iter: 1917 loss: 5.29802037e-07
Iter: 1918 loss: 5.29543058e-07
Iter: 1919 loss: 5.29568183e-07
Iter: 1920 loss: 5.29322108e-07
Iter: 1921 loss: 5.29557e-07
Iter: 1922 loss: 5.29203e-07
Iter: 1923 loss: 5.28904081e-07
Iter: 1924 loss: 5.29510316e-07
Iter: 1925 loss: 5.28840417e-07
Iter: 1926 loss: 5.28537157e-07
Iter: 1927 loss: 5.28444048e-07
Iter: 1928 loss: 5.28291082e-07
Iter: 1929 loss: 5.27814223e-07
Iter: 1930 loss: 5.30665659e-07
Iter: 1931 loss: 5.2773305e-07
Iter: 1932 loss: 5.27490442e-07
Iter: 1933 loss: 5.27554448e-07
Iter: 1934 loss: 5.27321617e-07
Iter: 1935 loss: 5.26927238e-07
Iter: 1936 loss: 5.27941211e-07
Iter: 1937 loss: 5.26728968e-07
Iter: 1938 loss: 5.2641542e-07
Iter: 1939 loss: 5.272002e-07
Iter: 1940 loss: 5.26322879e-07
Iter: 1941 loss: 5.26064355e-07
Iter: 1942 loss: 5.28973601e-07
Iter: 1943 loss: 5.26027748e-07
Iter: 1944 loss: 5.25846644e-07
Iter: 1945 loss: 5.26009615e-07
Iter: 1946 loss: 5.25713631e-07
Iter: 1947 loss: 5.25580163e-07
Iter: 1948 loss: 5.25336816e-07
Iter: 1949 loss: 5.25367795e-07
Iter: 1950 loss: 5.25086648e-07
Iter: 1951 loss: 5.26004328e-07
Iter: 1952 loss: 5.25062e-07
Iter: 1953 loss: 5.24784753e-07
Iter: 1954 loss: 5.24862571e-07
Iter: 1955 loss: 5.24593645e-07
Iter: 1956 loss: 5.24188124e-07
Iter: 1957 loss: 5.26676445e-07
Iter: 1958 loss: 5.24151915e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3/500_500_500_500_1
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi3_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi3_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi3_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi3_phi0
+ date
Thu Oct 22 15:05:30 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi3_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 3 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi3_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf433a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf42d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf3dc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf462e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf3cc268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf3cc8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf38a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf38a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf3cce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf3217b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf38a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf245d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf22a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf22abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf27cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf1936a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf193b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf1469d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf1afb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf10de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf10d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf10dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27adbad840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27adbc3d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27adbc4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27adbc48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27adb227b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27adadac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27adada620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f278823fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f278823f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27881ec2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27881ecbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27cf0c2488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f27881698c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2788169510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
