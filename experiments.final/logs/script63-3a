+ RUN=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='2 3'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1 --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc3bc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc3bc1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc2f6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc25bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc255950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc166510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc1bcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc189ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc189730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc118b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0d3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0dd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0a1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efbffea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc003b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc02f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5e3c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d8f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d8ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d53268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d0ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5cc5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5cd36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5cd37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5dfc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5dfc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5c83ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5c09510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5c29f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc03f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5b8c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5b8c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5b8c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5bf5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5bd7d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.019547626
test_loss: 0.019671382
train_loss: 0.0090725655
test_loss: 0.00898119
train_loss: 0.006269957
test_loss: 0.0066933804
train_loss: 0.005477133
test_loss: 0.0056914627
train_loss: 0.0053307153
test_loss: 0.005301539
train_loss: 0.005177132
test_loss: 0.0051679346
train_loss: 0.004584152
test_loss: 0.0050352486
train_loss: 0.004709363
test_loss: 0.0047426084
train_loss: 0.0045332955
test_loss: 0.004968934
train_loss: 0.004450911
test_loss: 0.004846005
train_loss: 0.0042535053
test_loss: 0.004670961
train_loss: 0.004462671
test_loss: 0.004772661
train_loss: 0.0041610645
test_loss: 0.0046513537
train_loss: 0.004110224
test_loss: 0.004519015
train_loss: 0.0041226554
test_loss: 0.0048108515
train_loss: 0.0041026324
test_loss: 0.0044956687
train_loss: 0.0039682505
test_loss: 0.004372526
train_loss: 0.004067778
test_loss: 0.004321856
train_loss: 0.0039699683
test_loss: 0.0043749553
train_loss: 0.003813988
test_loss: 0.0041737743
train_loss: 0.0040311324
test_loss: 0.0042153453
train_loss: 0.0039775805
test_loss: 0.0042263716
train_loss: 0.0038991314
test_loss: 0.004353083
train_loss: 0.004083889
test_loss: 0.0041781124
train_loss: 0.0038717086
test_loss: 0.004348058
train_loss: 0.004047476
test_loss: 0.0041527497
train_loss: 0.004040085
test_loss: 0.0044506085
train_loss: 0.0037939711
test_loss: 0.0041256505
train_loss: 0.0038523804
test_loss: 0.0042982325
train_loss: 0.003646867
test_loss: 0.004183979
train_loss: 0.003758031
test_loss: 0.004175936
train_loss: 0.0034912326
test_loss: 0.0041251816
train_loss: 0.0035836175
test_loss: 0.0042388123
train_loss: 0.003477535
test_loss: 0.003987771
train_loss: 0.0038018895
test_loss: 0.003973167
train_loss: 0.0035137008
test_loss: 0.003928891
train_loss: 0.00344377
test_loss: 0.0039027405
train_loss: 0.0034626015
test_loss: 0.0038776102
train_loss: 0.0036062435
test_loss: 0.0039891796
train_loss: 0.0034179867
test_loss: 0.0040519987
train_loss: 0.0034938576
test_loss: 0.003945753
train_loss: 0.0034357607
test_loss: 0.003971588
train_loss: 0.0036368994
test_loss: 0.0040100412
train_loss: 0.0035285421
test_loss: 0.0038912948
train_loss: 0.0034754837
test_loss: 0.004097174
train_loss: 0.0035042984
test_loss: 0.0039125034
train_loss: 0.0034002087
test_loss: 0.0037513967
train_loss: 0.0033820462
test_loss: 0.0039441707
train_loss: 0.0032757826
test_loss: 0.003755735
train_loss: 0.0035601624
test_loss: 0.0037517531
train_loss: 0.003427818
test_loss: 0.0037525482
train_loss: 0.0033806544
test_loss: 0.0038233057
train_loss: 0.003265006
test_loss: 0.0036814178
train_loss: 0.003451409
test_loss: 0.0037147857
train_loss: 0.0033702694
test_loss: 0.0038042273
train_loss: 0.0034005356
test_loss: 0.0040830285
train_loss: 0.0035511795
test_loss: 0.004015981
train_loss: 0.0032240718
test_loss: 0.0035995601
train_loss: 0.0033367996
test_loss: 0.0036661767
train_loss: 0.0032932581
test_loss: 0.0037487417
train_loss: 0.003391197
test_loss: 0.0038559055
train_loss: 0.0032253219
test_loss: 0.0037323972
train_loss: 0.0031579952
test_loss: 0.0035365745
train_loss: 0.0032422296
test_loss: 0.0035806636
train_loss: 0.0032111001
test_loss: 0.0038482659
train_loss: 0.003291798
test_loss: 0.0036947862
train_loss: 0.003109728
test_loss: 0.0036800758
train_loss: 0.0032820161
test_loss: 0.0036755698
train_loss: 0.003481783
test_loss: 0.0036724657
train_loss: 0.0029738753
test_loss: 0.0034515145
train_loss: 0.003355905
test_loss: 0.0037474388
train_loss: 0.003100234
test_loss: 0.0036929282
train_loss: 0.0031402444
test_loss: 0.0036745954
train_loss: 0.003534715
test_loss: 0.0035946153
train_loss: 0.003177234
test_loss: 0.0037799387
train_loss: 0.00330197
test_loss: 0.0037961465
train_loss: 0.002825023
test_loss: 0.0034027852
train_loss: 0.003039434
test_loss: 0.0035345107
train_loss: 0.0030412555
test_loss: 0.0036148957
train_loss: 0.0031319219
test_loss: 0.003370568
train_loss: 0.0031488682
test_loss: 0.003540186
train_loss: 0.0028720784
test_loss: 0.0035075166
train_loss: 0.0030095554
test_loss: 0.0036241552
train_loss: 0.0030054187
test_loss: 0.0036150832
train_loss: 0.0031567193
test_loss: 0.0035198943
train_loss: 0.0031195716
test_loss: 0.0035026195
train_loss: 0.003205
test_loss: 0.0035274802
train_loss: 0.0029213028
test_loss: 0.0035053417
train_loss: 0.0029996864
test_loss: 0.0035015382
train_loss: 0.0031255074
test_loss: 0.0037172693
train_loss: 0.0030857595
test_loss: 0.0036228434
train_loss: 0.0030892007
test_loss: 0.0035819702
train_loss: 0.0030290368
test_loss: 0.0035269032
train_loss: 0.0030759373
test_loss: 0.0035402868
train_loss: 0.0033104937
test_loss: 0.0037094196
train_loss: 0.0030366126
test_loss: 0.0037057057
train_loss: 0.0030122912
test_loss: 0.0036834236
train_loss: 0.0029721374
test_loss: 0.0035096689
train_loss: 0.0032497342
test_loss: 0.0034526417
train_loss: 0.0029798911
test_loss: 0.00347377
train_loss: 0.0027542568
test_loss: 0.003471824
train_loss: 0.0029923492
test_loss: 0.0035209518
train_loss: 0.0031998272
test_loss: 0.00355104
train_loss: 0.0032948111
test_loss: 0.003453693
train_loss: 0.0031483178
test_loss: 0.003475432
train_loss: 0.0033348862
test_loss: 0.0036273901
train_loss: 0.0030804984
test_loss: 0.0036315022
train_loss: 0.0032577193
test_loss: 0.0036114228
train_loss: 0.0031659636
test_loss: 0.003802141
train_loss: 0.0030901646
test_loss: 0.0035083375
train_loss: 0.0029157498
test_loss: 0.0034951034
train_loss: 0.0028182624
test_loss: 0.0033984596
train_loss: 0.0028133206
test_loss: 0.003326088
train_loss: 0.0029177275
test_loss: 0.0032958766
train_loss: 0.0029230635
test_loss: 0.0034473774
train_loss: 0.0030684215
test_loss: 0.0035568478
train_loss: 0.0033718667
test_loss: 0.0035185178
train_loss: 0.0029587739
test_loss: 0.0035073983
train_loss: 0.002879326
test_loss: 0.0034426556
train_loss: 0.002886285
test_loss: 0.003563428
train_loss: 0.0028952968
test_loss: 0.0034910107
train_loss: 0.0027492498
test_loss: 0.0035135318
train_loss: 0.0028979182
test_loss: 0.003454611
train_loss: 0.0030358643
test_loss: 0.0035228077
train_loss: 0.002753323
test_loss: 0.0033736704
train_loss: 0.0030144933
test_loss: 0.0034357475
train_loss: 0.0027485294
test_loss: 0.0035915799
train_loss: 0.0029275483
test_loss: 0.003461708
train_loss: 0.0026932012
test_loss: 0.0035387974
train_loss: 0.0029065844
test_loss: 0.003393748
train_loss: 0.002970633
test_loss: 0.0034050585
train_loss: 0.0031778896
test_loss: 0.0035654195
train_loss: 0.003054358
test_loss: 0.0034266133
train_loss: 0.0028854788
test_loss: 0.003346553
train_loss: 0.0028133374
test_loss: 0.0033250407
train_loss: 0.002825835
test_loss: 0.0033496371
train_loss: 0.002874393
test_loss: 0.0033193908
train_loss: 0.002926884
test_loss: 0.003607652
train_loss: 0.003126409
test_loss: 0.003431352
train_loss: 0.0030203417
test_loss: 0.0033926833
train_loss: 0.0028834217
test_loss: 0.003385762
train_loss: 0.0028411853
test_loss: 0.0033201354
train_loss: 0.0031297058
test_loss: 0.003518323
train_loss: 0.0028077648
test_loss: 0.0034006243
train_loss: 0.003007865
test_loss: 0.0033842642
train_loss: 0.0028573652
test_loss: 0.0034196775
train_loss: 0.002902547
test_loss: 0.0034724858
train_loss: 0.002824613
test_loss: 0.0032689595
train_loss: 0.002866327
test_loss: 0.003336344
train_loss: 0.0026882575
test_loss: 0.0032234036
train_loss: 0.0029545038
test_loss: 0.0033085416
train_loss: 0.0029573021
test_loss: 0.00350564
train_loss: 0.002799062
test_loss: 0.0034960934
train_loss: 0.0027994132
test_loss: 0.003284992
train_loss: 0.002702122
test_loss: 0.0033569399
train_loss: 0.0028348344
test_loss: 0.0033491221
train_loss: 0.0029710871
test_loss: 0.003295566
train_loss: 0.003029543
test_loss: 0.0035612963
train_loss: 0.0029697614
test_loss: 0.0034142302
train_loss: 0.002884876
test_loss: 0.003381833
train_loss: 0.0028450328
test_loss: 0.0033320785
train_loss: 0.0027937444
test_loss: 0.003335728
train_loss: 0.002887777
test_loss: 0.0032338463
train_loss: 0.002654045
test_loss: 0.0032966551
train_loss: 0.0029149055
test_loss: 0.0033503133
train_loss: 0.0026438679
test_loss: 0.003198512
train_loss: 0.0026022391
test_loss: 0.0032339573
train_loss: 0.0029360927
test_loss: 0.0031914106
train_loss: 0.0027526254
test_loss: 0.0033213145
train_loss: 0.0026618934
test_loss: 0.0032308914
train_loss: 0.0027489283
test_loss: 0.0033388557
train_loss: 0.003222722
test_loss: 0.0034210582
train_loss: 0.0034299411
test_loss: 0.0035947682
train_loss: 0.002843319
test_loss: 0.0035041643
train_loss: 0.0027061112
test_loss: 0.0033784953
train_loss: 0.003024421
test_loss: 0.0031818687
train_loss: 0.0030919171
test_loss: 0.0035386032
train_loss: 0.0028586753
test_loss: 0.0032383043
train_loss: 0.0027233376
test_loss: 0.0032418056
train_loss: 0.003011302
test_loss: 0.0033342505
train_loss: 0.0027760505
test_loss: 0.003261263
train_loss: 0.0030223336
test_loss: 0.0032019895
train_loss: 0.0026941323
test_loss: 0.0033408583
train_loss: 0.0028003599
test_loss: 0.0031526724
train_loss: 0.0029007941
test_loss: 0.0034161375
train_loss: 0.002626876
test_loss: 0.0033419884
train_loss: 0.002694023
test_loss: 0.003144593
train_loss: 0.00271307
test_loss: 0.003214994
train_loss: 0.0027537274
test_loss: 0.0031517926
train_loss: 0.0028093643
test_loss: 0.0031971983
train_loss: 0.0028385757
test_loss: 0.0032446086
train_loss: 0.0027733813
test_loss: 0.0034759967
train_loss: 0.0028015967
test_loss: 0.0034257513
train_loss: 0.002635348
test_loss: 0.0031476042
train_loss: 0.0023957689
test_loss: 0.0030443403
train_loss: 0.0026956652
test_loss: 0.0031948178
train_loss: 0.0027950779
test_loss: 0.0031661435
train_loss: 0.002696092
test_loss: 0.0032547778
train_loss: 0.0027798621
test_loss: 0.0032376836
train_loss: 0.002688308
test_loss: 0.003332539
train_loss: 0.0028173276
test_loss: 0.0031972623
train_loss: 0.0028132463
test_loss: 0.0033499089
train_loss: 0.0031623738
test_loss: 0.0033269152
train_loss: 0.002786164
test_loss: 0.0033834744
train_loss: 0.0031135888
test_loss: 0.0033840556
train_loss: 0.0026651553
test_loss: 0.0032382691
train_loss: 0.0026890999
test_loss: 0.003319003
train_loss: 0.002792087
test_loss: 0.0032163612
train_loss: 0.0026116234
test_loss: 0.0033806683
train_loss: 0.0026158623
test_loss: 0.0031239614
train_loss: 0.0026314107
test_loss: 0.0032441325
train_loss: 0.0026469356
test_loss: 0.0030629989
train_loss: 0.0025724557
test_loss: 0.00327408
train_loss: 0.0027236014
test_loss: 0.0032804494
train_loss: 0.0026818425
test_loss: 0.0032046246
train_loss: 0.0027574685
test_loss: 0.0032881144
train_loss: 0.0027022338
test_loss: 0.003108547
train_loss: 0.002743667
test_loss: 0.0033077
train_loss: 0.002777342
test_loss: 0.0033761356
train_loss: 0.002554835
test_loss: 0.0032199097
train_loss: 0.0026825909
test_loss: 0.0032697592
train_loss: 0.0026999621
test_loss: 0.0031754095
train_loss: 0.0024217698
test_loss: 0.0033690229
train_loss: 0.0025917653
test_loss: 0.0031233663
train_loss: 0.0028114829
test_loss: 0.0032382866
train_loss: 0.0027476624
test_loss: 0.003409831
train_loss: 0.0028319359
test_loss: 0.003225152
train_loss: 0.0025178567
test_loss: 0.003197765
train_loss: 0.0027305246
test_loss: 0.0032894956
train_loss: 0.002803234
test_loss: 0.0034056862
train_loss: 0.0028627482
test_loss: 0.0032638193
train_loss: 0.0029985865
test_loss: 0.0033122015
train_loss: 0.0028155248
test_loss: 0.0033387826
train_loss: 0.0027036667
test_loss: 0.0032714938
train_loss: 0.0028630737
test_loss: 0.0032813158
train_loss: 0.0026126604
test_loss: 0.0031988195
train_loss: 0.0026209902
test_loss: 0.0032417884
train_loss: 0.0027234002
test_loss: 0.0033010526
train_loss: 0.0026880282
test_loss: 0.0032584174
train_loss: 0.002762279
test_loss: 0.003310852
train_loss: 0.002726587
test_loss: 0.0032861647
train_loss: 0.0026257313
test_loss: 0.003142028
train_loss: 0.0025778664
test_loss: 0.0030626336
train_loss: 0.0025943327
test_loss: 0.0031601323
train_loss: 0.0026659602
test_loss: 0.0032112047
train_loss: 0.0025574851
test_loss: 0.0032067134
train_loss: 0.00271732
test_loss: 0.0032462399
train_loss: 0.0025556935
test_loss: 0.003166267
train_loss: 0.0025719004
test_loss: 0.0032335941
train_loss: 0.0027727224
test_loss: 0.0033520472
train_loss: 0.0026562877
test_loss: 0.0031408158
train_loss: 0.0026923446
test_loss: 0.003292029
train_loss: 0.0025160247
test_loss: 0.0031453564
train_loss: 0.0024189523
test_loss: 0.003115839
train_loss: 0.0026687554
test_loss: 0.0031823325
train_loss: 0.002682491
test_loss: 0.0031937123
train_loss: 0.0025997362
test_loss: 0.003151958
train_loss: 0.0027012008
test_loss: 0.0031147015
train_loss: 0.0025992217
test_loss: 0.003533864
train_loss: 0.0025204343
test_loss: 0.0032185288
train_loss: 0.0027215588
test_loss: 0.0032751097
train_loss: 0.0025348132
test_loss: 0.0031375815
train_loss: 0.0028095096
test_loss: 0.0032115935
train_loss: 0.0025713623
test_loss: 0.0032993476
train_loss: 0.002621413
test_loss: 0.0031954846
train_loss: 0.0024814971
test_loss: 0.0030397547
train_loss: 0.0026260803
test_loss: 0.0031995226
train_loss: 0.0025979038
test_loss: 0.0031914676
train_loss: 0.0025971734
test_loss: 0.003248692
train_loss: 0.0030938676
test_loss: 0.0034732234
train_loss: 0.0026483994
test_loss: 0.0032684952
train_loss: 0.0027586455
test_loss: 0.0033304596
train_loss: 0.0028772866
test_loss: 0.003260725
train_loss: 0.0027161753
test_loss: 0.0031898678
train_loss: 0.002536872
test_loss: 0.0031462116
train_loss: 0.002630613
test_loss: 0.0031388162
train_loss: 0.0025586563
test_loss: 0.003173081
train_loss: 0.0028531898
test_loss: 0.0031996535
train_loss: 0.0027549951
test_loss: 0.0033838912
train_loss: 0.0025846928
test_loss: 0.003104193
train_loss: 0.0024986614
test_loss: 0.0033008936
train_loss: 0.002775758
test_loss: 0.0032895943
train_loss: 0.002772307
test_loss: 0.0033783403
train_loss: 0.002718167
test_loss: 0.0032204606
train_loss: 0.0027224112
test_loss: 0.0030071212
train_loss: 0.0024644127
test_loss: 0.0031549106
train_loss: 0.002532889
test_loss: 0.0030257497
train_loss: 0.0024496533
test_loss: 0.0031925093
train_loss: 0.0023963158
test_loss: 0.0029779105
train_loss: 0.0024233332
test_loss: 0.0030250254
train_loss: 0.002536714
test_loss: 0.003176634
train_loss: 0.0027666967
test_loss: 0.0032432538
train_loss: 0.0024197388
test_loss: 0.003253465
train_loss: 0.0029169878
test_loss: 0.0034578792
train_loss: 0.0027120921
test_loss: 0.0031953193
train_loss: 0.0025132964
test_loss: 0.0030944238
train_loss: 0.002676434
test_loss: 0.0030667614
train_loss: 0.0023834049
test_loss: 0.00330171
train_loss: 0.0024319836
test_loss: 0.003156773
train_loss: 0.0024584215
test_loss: 0.0030539038
train_loss: 0.0027637458
test_loss: 0.0031473078
train_loss: 0.002597585
test_loss: 0.0032146797
train_loss: 0.0026314098
test_loss: 0.0032535386
train_loss: 0.0028311112
test_loss: 0.0031532736
train_loss: 0.0027307014
test_loss: 0.003292303
train_loss: 0.00267726
test_loss: 0.003286467
train_loss: 0.0026124779
test_loss: 0.0033061751
train_loss: 0.0025667772
test_loss: 0.0031463292
train_loss: 0.0025587752
test_loss: 0.0031405352
train_loss: 0.0024749776
test_loss: 0.0030493222
train_loss: 0.0025178455
test_loss: 0.0031446076
train_loss: 0.0024154168
test_loss: 0.0031210517
train_loss: 0.002609696
test_loss: 0.003080377
train_loss: 0.0026693458
test_loss: 0.0031875062
train_loss: 0.0025300256
test_loss: 0.0031683906
train_loss: 0.0024807556
test_loss: 0.003110037
train_loss: 0.0024542338
test_loss: 0.0030096434
train_loss: 0.002450978
test_loss: 0.0029865035
train_loss: 0.0026240128
test_loss: 0.0030784276
train_loss: 0.0025440054
test_loss: 0.0032461074
train_loss: 0.002582653
test_loss: 0.0030958874
train_loss: 0.002752099
test_loss: 0.0031691738
train_loss: 0.00269428
test_loss: 0.0030995032
train_loss: 0.0025932903
test_loss: 0.003092225
train_loss: 0.002448188
test_loss: 0.00313418
train_loss: 0.0025063078
test_loss: 0.0029509466
train_loss: 0.0024693797
test_loss: 0.0029468816
train_loss: 0.00262089
test_loss: 0.003243929
train_loss: 0.002646273
test_loss: 0.003057043
train_loss: 0.0025041103
test_loss: 0.0030433333
train_loss: 0.0029375395
test_loss: 0.0035068288
train_loss: 0.0024414447
test_loss: 0.0029233298
train_loss: 0.002566438
test_loss: 0.0032200178
train_loss: 0.0025737358
test_loss: 0.0033183282
train_loss: 0.0024729944
test_loss: 0.0032712352
train_loss: 0.0027658953
test_loss: 0.003351998
train_loss: 0.0025987923
test_loss: 0.0031846971
train_loss: 0.0026269965
test_loss: 0.0032000847
train_loss: 0.0024411494/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0032126354
train_loss: 0.002560078
test_loss: 0.003026019
train_loss: 0.002644809
test_loss: 0.003232324
train_loss: 0.0023800246
test_loss: 0.0029141563
train_loss: 0.0025498027
test_loss: 0.0029841291
train_loss: 0.0026895276
test_loss: 0.0031211667
train_loss: 0.0027047307
test_loss: 0.0032030554
train_loss: 0.0024833623
test_loss: 0.0031813611
train_loss: 0.0025495898
test_loss: 0.0030673563
train_loss: 0.0027053724
test_loss: 0.0033673234
train_loss: 0.002633778
test_loss: 0.0030812847
train_loss: 0.0025631562
test_loss: 0.0031251484
train_loss: 0.0024574527
test_loss: 0.003230523
train_loss: 0.002821933
test_loss: 0.003120549
train_loss: 0.0025756587
test_loss: 0.0031189083
train_loss: 0.00249699
test_loss: 0.0031774729
train_loss: 0.0026065325
test_loss: 0.0030327952
train_loss: 0.0025449756
test_loss: 0.0031637473
train_loss: 0.0026330429
test_loss: 0.003210316
train_loss: 0.0025113681
test_loss: 0.0030163305
train_loss: 0.002590535
test_loss: 0.003071573
train_loss: 0.0024417685
test_loss: 0.0031393494
train_loss: 0.0023713158
test_loss: 0.0029934049
train_loss: 0.0025528274
test_loss: 0.0029964452
train_loss: 0.0026006075
test_loss: 0.0032278167
train_loss: 0.0024897803
test_loss: 0.0030757496
train_loss: 0.002413476
test_loss: 0.0031063438
train_loss: 0.0025376377
test_loss: 0.0030348764
train_loss: 0.0024660714
test_loss: 0.0030175198
train_loss: 0.002412226
test_loss: 0.0030282724
train_loss: 0.00232203
test_loss: 0.002991953
train_loss: 0.0024197064
test_loss: 0.0031470598
train_loss: 0.0025783249
test_loss: 0.0031458843
train_loss: 0.0024773595
test_loss: 0.0030919118
train_loss: 0.0024919591
test_loss: 0.003150029
train_loss: 0.0025356773
test_loss: 0.0031160566
train_loss: 0.0025923483
test_loss: 0.0031655896
train_loss: 0.0024821889
test_loss: 0.0031022725
train_loss: 0.002557021
test_loss: 0.0029873028
train_loss: 0.002798224
test_loss: 0.0032589764
train_loss: 0.0027342234
test_loss: 0.003073332
train_loss: 0.0024486268
test_loss: 0.0030973046
train_loss: 0.002436045
test_loss: 0.0030808318
train_loss: 0.0029918118
test_loss: 0.0031587463
train_loss: 0.0025038186
test_loss: 0.00307083
train_loss: 0.0024309482
test_loss: 0.002891278
train_loss: 0.0023055123
test_loss: 0.0028691937
train_loss: 0.0022931718
test_loss: 0.0029486648
train_loss: 0.0024666006
test_loss: 0.0031697948
train_loss: 0.002434828
test_loss: 0.0031721594
train_loss: 0.002684794
test_loss: 0.0031193423
train_loss: 0.0025489626
test_loss: 0.0031713264
train_loss: 0.0026219536
test_loss: 0.0031988425
train_loss: 0.0027065915
test_loss: 0.003382226
train_loss: 0.0026149082
test_loss: 0.0031611882
train_loss: 0.002571876
test_loss: 0.003026249
train_loss: 0.0025392056
test_loss: 0.0030589036
train_loss: 0.0024663433
test_loss: 0.0030268452
train_loss: 0.0025131316
test_loss: 0.0030573246
train_loss: 0.0024927997
test_loss: 0.0030460132
train_loss: 0.0024919226
test_loss: 0.0030590286
train_loss: 0.002466178
test_loss: 0.0032317145
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f301d82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f301d8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f302ad048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f301e0e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30215378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f3016ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30157840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300fd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300fdd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300fd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300c4048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30098620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300c46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30056840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300560d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10094488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10094158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10050400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10012378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10039f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e647cf950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e647cfc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e647a8158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64751268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64751ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646fe6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646cc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646ccd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646f6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646cce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e6464d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64672950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646727b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64635378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e645d32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64579268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.20475843e-05
Iter: 2 loss: 1.07282967e-05
Iter: 3 loss: 9.60075886e-06
Iter: 4 loss: 8.76355e-06
Iter: 5 loss: 8.44306851e-06
Iter: 6 loss: 7.98728706e-06
Iter: 7 loss: 7.4197319e-06
Iter: 8 loss: 9.58292458e-06
Iter: 9 loss: 7.28441273e-06
Iter: 10 loss: 6.83092367e-06
Iter: 11 loss: 1.06186853e-05
Iter: 12 loss: 6.80403264e-06
Iter: 13 loss: 6.48051855e-06
Iter: 14 loss: 6.70939426e-06
Iter: 15 loss: 6.27997815e-06
Iter: 16 loss: 5.95777192e-06
Iter: 17 loss: 6.47154593e-06
Iter: 18 loss: 5.80829874e-06
Iter: 19 loss: 5.47589116e-06
Iter: 20 loss: 8.82174572e-06
Iter: 21 loss: 5.46545198e-06
Iter: 22 loss: 5.24184543e-06
Iter: 23 loss: 4.84430711e-06
Iter: 24 loss: 1.47764376e-05
Iter: 25 loss: 4.84428438e-06
Iter: 26 loss: 4.74110402e-06
Iter: 27 loss: 4.66936945e-06
Iter: 28 loss: 4.50187054e-06
Iter: 29 loss: 4.385377e-06
Iter: 30 loss: 4.32525258e-06
Iter: 31 loss: 4.14454962e-06
Iter: 32 loss: 4.28057683e-06
Iter: 33 loss: 4.03414197e-06
Iter: 34 loss: 3.81890459e-06
Iter: 35 loss: 3.86415877e-06
Iter: 36 loss: 3.65972119e-06
Iter: 37 loss: 3.63894878e-06
Iter: 38 loss: 3.55120392e-06
Iter: 39 loss: 3.43443344e-06
Iter: 40 loss: 3.33964408e-06
Iter: 41 loss: 3.30505873e-06
Iter: 42 loss: 3.15753778e-06
Iter: 43 loss: 3.27017437e-06
Iter: 44 loss: 3.06776656e-06
Iter: 45 loss: 3.03495631e-06
Iter: 46 loss: 2.99806379e-06
Iter: 47 loss: 2.95365726e-06
Iter: 48 loss: 2.87242142e-06
Iter: 49 loss: 4.77785488e-06
Iter: 50 loss: 2.87232206e-06
Iter: 51 loss: 2.78791458e-06
Iter: 52 loss: 3.51804e-06
Iter: 53 loss: 2.78344305e-06
Iter: 54 loss: 2.72244324e-06
Iter: 55 loss: 2.91235574e-06
Iter: 56 loss: 2.70460851e-06
Iter: 57 loss: 2.63651282e-06
Iter: 58 loss: 2.62994104e-06
Iter: 59 loss: 2.57990655e-06
Iter: 60 loss: 2.51113829e-06
Iter: 61 loss: 2.82366591e-06
Iter: 62 loss: 2.49793084e-06
Iter: 63 loss: 2.4140179e-06
Iter: 64 loss: 2.75533148e-06
Iter: 65 loss: 2.39547717e-06
Iter: 66 loss: 2.35556877e-06
Iter: 67 loss: 2.31339232e-06
Iter: 68 loss: 2.30621163e-06
Iter: 69 loss: 2.24544442e-06
Iter: 70 loss: 2.40341865e-06
Iter: 71 loss: 2.22477183e-06
Iter: 72 loss: 2.2070526e-06
Iter: 73 loss: 2.19802564e-06
Iter: 74 loss: 2.16815056e-06
Iter: 75 loss: 2.11784618e-06
Iter: 76 loss: 2.1176902e-06
Iter: 77 loss: 2.07874973e-06
Iter: 78 loss: 2.15191017e-06
Iter: 79 loss: 2.06220489e-06
Iter: 80 loss: 2.02366618e-06
Iter: 81 loss: 2.6221287e-06
Iter: 82 loss: 2.02366255e-06
Iter: 83 loss: 1.99591295e-06
Iter: 84 loss: 1.97023201e-06
Iter: 85 loss: 1.96364158e-06
Iter: 86 loss: 1.93588767e-06
Iter: 87 loss: 2.24007727e-06
Iter: 88 loss: 1.93526239e-06
Iter: 89 loss: 1.91058234e-06
Iter: 90 loss: 1.93829646e-06
Iter: 91 loss: 1.89719231e-06
Iter: 92 loss: 1.87156309e-06
Iter: 93 loss: 1.92643438e-06
Iter: 94 loss: 1.86156149e-06
Iter: 95 loss: 1.83688212e-06
Iter: 96 loss: 1.9255026e-06
Iter: 97 loss: 1.83061388e-06
Iter: 98 loss: 1.80538109e-06
Iter: 99 loss: 1.96951805e-06
Iter: 100 loss: 1.80270717e-06
Iter: 101 loss: 1.78597838e-06
Iter: 102 loss: 1.75176069e-06
Iter: 103 loss: 2.35989933e-06
Iter: 104 loss: 1.75104299e-06
Iter: 105 loss: 1.71490774e-06
Iter: 106 loss: 1.82133545e-06
Iter: 107 loss: 1.70372925e-06
Iter: 108 loss: 1.69595592e-06
Iter: 109 loss: 1.68801103e-06
Iter: 110 loss: 1.67285816e-06
Iter: 111 loss: 1.66232689e-06
Iter: 112 loss: 1.65688994e-06
Iter: 113 loss: 1.64021253e-06
Iter: 114 loss: 1.6363706e-06
Iter: 115 loss: 1.62562787e-06
Iter: 116 loss: 1.61658863e-06
Iter: 117 loss: 1.61360481e-06
Iter: 118 loss: 1.60305103e-06
Iter: 119 loss: 1.58066302e-06
Iter: 120 loss: 1.94111522e-06
Iter: 121 loss: 1.57991894e-06
Iter: 122 loss: 1.56510225e-06
Iter: 123 loss: 1.753883e-06
Iter: 124 loss: 1.56503688e-06
Iter: 125 loss: 1.54793599e-06
Iter: 126 loss: 1.53968915e-06
Iter: 127 loss: 1.53146607e-06
Iter: 128 loss: 1.51436882e-06
Iter: 129 loss: 1.63492257e-06
Iter: 130 loss: 1.5128785e-06
Iter: 131 loss: 1.50066421e-06
Iter: 132 loss: 1.55946077e-06
Iter: 133 loss: 1.49848347e-06
Iter: 134 loss: 1.48522565e-06
Iter: 135 loss: 1.50683718e-06
Iter: 136 loss: 1.47913545e-06
Iter: 137 loss: 1.46716707e-06
Iter: 138 loss: 1.4712864e-06
Iter: 139 loss: 1.45866852e-06
Iter: 140 loss: 1.44668047e-06
Iter: 141 loss: 1.43987006e-06
Iter: 142 loss: 1.43469867e-06
Iter: 143 loss: 1.43766715e-06
Iter: 144 loss: 1.42668432e-06
Iter: 145 loss: 1.42073657e-06
Iter: 146 loss: 1.40547809e-06
Iter: 147 loss: 1.53088683e-06
Iter: 148 loss: 1.40277632e-06
Iter: 149 loss: 1.38673431e-06
Iter: 150 loss: 1.4671441e-06
Iter: 151 loss: 1.38406631e-06
Iter: 152 loss: 1.37535835e-06
Iter: 153 loss: 1.37499774e-06
Iter: 154 loss: 1.36811445e-06
Iter: 155 loss: 1.35532036e-06
Iter: 156 loss: 1.63853383e-06
Iter: 157 loss: 1.3552816e-06
Iter: 158 loss: 1.34745403e-06
Iter: 159 loss: 1.44916498e-06
Iter: 160 loss: 1.34741845e-06
Iter: 161 loss: 1.33922572e-06
Iter: 162 loss: 1.3501882e-06
Iter: 163 loss: 1.33510173e-06
Iter: 164 loss: 1.32906678e-06
Iter: 165 loss: 1.32855871e-06
Iter: 166 loss: 1.32411787e-06
Iter: 167 loss: 1.31238835e-06
Iter: 168 loss: 1.36544054e-06
Iter: 169 loss: 1.31012712e-06
Iter: 170 loss: 1.2997973e-06
Iter: 171 loss: 1.32306889e-06
Iter: 172 loss: 1.2958867e-06
Iter: 173 loss: 1.28872193e-06
Iter: 174 loss: 1.29362309e-06
Iter: 175 loss: 1.28430418e-06
Iter: 176 loss: 1.27556814e-06
Iter: 177 loss: 1.28879969e-06
Iter: 178 loss: 1.27144017e-06
Iter: 179 loss: 1.27102862e-06
Iter: 180 loss: 1.26737245e-06
Iter: 181 loss: 1.26496093e-06
Iter: 182 loss: 1.25816371e-06
Iter: 183 loss: 1.2896171e-06
Iter: 184 loss: 1.25569e-06
Iter: 185 loss: 1.2480632e-06
Iter: 186 loss: 1.28360261e-06
Iter: 187 loss: 1.24657504e-06
Iter: 188 loss: 1.2391564e-06
Iter: 189 loss: 1.33258254e-06
Iter: 190 loss: 1.23910354e-06
Iter: 191 loss: 1.23403618e-06
Iter: 192 loss: 1.22424376e-06
Iter: 193 loss: 1.43131433e-06
Iter: 194 loss: 1.22418305e-06
Iter: 195 loss: 1.21736889e-06
Iter: 196 loss: 1.21738663e-06
Iter: 197 loss: 1.2118885e-06
Iter: 198 loss: 1.2372343e-06
Iter: 199 loss: 1.21080677e-06
Iter: 200 loss: 1.20751577e-06
Iter: 201 loss: 1.20309801e-06
Iter: 202 loss: 1.20285802e-06
Iter: 203 loss: 1.19759238e-06
Iter: 204 loss: 1.19753395e-06
Iter: 205 loss: 1.19457241e-06
Iter: 206 loss: 1.19063975e-06
Iter: 207 loss: 1.19039487e-06
Iter: 208 loss: 1.18433707e-06
Iter: 209 loss: 1.19288939e-06
Iter: 210 loss: 1.18139167e-06
Iter: 211 loss: 1.17410252e-06
Iter: 212 loss: 1.19245806e-06
Iter: 213 loss: 1.17158902e-06
Iter: 214 loss: 1.16682452e-06
Iter: 215 loss: 1.16640706e-06
Iter: 216 loss: 1.16383706e-06
Iter: 217 loss: 1.15817068e-06
Iter: 218 loss: 1.24125063e-06
Iter: 219 loss: 1.15792045e-06
Iter: 220 loss: 1.153021e-06
Iter: 221 loss: 1.18459025e-06
Iter: 222 loss: 1.15247281e-06
Iter: 223 loss: 1.14899831e-06
Iter: 224 loss: 1.14901673e-06
Iter: 225 loss: 1.14680677e-06
Iter: 226 loss: 1.14119121e-06
Iter: 227 loss: 1.18980392e-06
Iter: 228 loss: 1.14029308e-06
Iter: 229 loss: 1.13488841e-06
Iter: 230 loss: 1.20012896e-06
Iter: 231 loss: 1.13481724e-06
Iter: 232 loss: 1.12963914e-06
Iter: 233 loss: 1.14993168e-06
Iter: 234 loss: 1.12850535e-06
Iter: 235 loss: 1.12510497e-06
Iter: 236 loss: 1.12259909e-06
Iter: 237 loss: 1.12143118e-06
Iter: 238 loss: 1.11742122e-06
Iter: 239 loss: 1.11732288e-06
Iter: 240 loss: 1.11504505e-06
Iter: 241 loss: 1.11217935e-06
Iter: 242 loss: 1.11196937e-06
Iter: 243 loss: 1.10903034e-06
Iter: 244 loss: 1.11743475e-06
Iter: 245 loss: 1.10817564e-06
Iter: 246 loss: 1.10476765e-06
Iter: 247 loss: 1.11935583e-06
Iter: 248 loss: 1.10402993e-06
Iter: 249 loss: 1.10120106e-06
Iter: 250 loss: 1.13610395e-06
Iter: 251 loss: 1.10118481e-06
Iter: 252 loss: 1.09937264e-06
Iter: 253 loss: 1.09437758e-06
Iter: 254 loss: 1.12036093e-06
Iter: 255 loss: 1.09275402e-06
Iter: 256 loss: 1.08749214e-06
Iter: 257 loss: 1.15216324e-06
Iter: 258 loss: 1.08741472e-06
Iter: 259 loss: 1.08416725e-06
Iter: 260 loss: 1.13504871e-06
Iter: 261 loss: 1.0841718e-06
Iter: 262 loss: 1.08199515e-06
Iter: 263 loss: 1.07757057e-06
Iter: 264 loss: 1.15942134e-06
Iter: 265 loss: 1.07748133e-06
Iter: 266 loss: 1.07489404e-06
Iter: 267 loss: 1.10307724e-06
Iter: 268 loss: 1.07484777e-06
Iter: 269 loss: 1.07211781e-06
Iter: 270 loss: 1.08216e-06
Iter: 271 loss: 1.07144217e-06
Iter: 272 loss: 1.06949744e-06
Iter: 273 loss: 1.0674446e-06
Iter: 274 loss: 1.06706091e-06
Iter: 275 loss: 1.0640648e-06
Iter: 276 loss: 1.06407674e-06
Iter: 277 loss: 1.06195876e-06
Iter: 278 loss: 1.05840888e-06
Iter: 279 loss: 1.058409e-06
Iter: 280 loss: 1.05409549e-06
Iter: 281 loss: 1.05436379e-06
Iter: 282 loss: 1.05064498e-06
Iter: 283 loss: 1.05076776e-06
Iter: 284 loss: 1.04845958e-06
Iter: 285 loss: 1.04695312e-06
Iter: 286 loss: 1.05032268e-06
Iter: 287 loss: 1.04634546e-06
Iter: 288 loss: 1.04475464e-06
Iter: 289 loss: 1.04200194e-06
Iter: 290 loss: 1.04201672e-06
Iter: 291 loss: 1.03918035e-06
Iter: 292 loss: 1.05110621e-06
Iter: 293 loss: 1.03858952e-06
Iter: 294 loss: 1.036409e-06
Iter: 295 loss: 1.03641401e-06
Iter: 296 loss: 1.03492653e-06
Iter: 297 loss: 1.03164211e-06
Iter: 298 loss: 1.08116103e-06
Iter: 299 loss: 1.03153377e-06
Iter: 300 loss: 1.02828812e-06
Iter: 301 loss: 1.0423056e-06
Iter: 302 loss: 1.02760168e-06
Iter: 303 loss: 1.02464878e-06
Iter: 304 loss: 1.06251821e-06
Iter: 305 loss: 1.024624e-06
Iter: 306 loss: 1.02299077e-06
Iter: 307 loss: 1.02118599e-06
Iter: 308 loss: 1.0209335e-06
Iter: 309 loss: 1.01933176e-06
Iter: 310 loss: 1.01923308e-06
Iter: 311 loss: 1.01785849e-06
Iter: 312 loss: 1.01501041e-06
Iter: 313 loss: 1.06120854e-06
Iter: 314 loss: 1.01494095e-06
Iter: 315 loss: 1.01266767e-06
Iter: 316 loss: 1.02043202e-06
Iter: 317 loss: 1.01208389e-06
Iter: 318 loss: 1.01018236e-06
Iter: 319 loss: 1.03604202e-06
Iter: 320 loss: 1.01018134e-06
Iter: 321 loss: 1.00811917e-06
Iter: 322 loss: 1.00815328e-06
Iter: 323 loss: 1.00646434e-06
Iter: 324 loss: 1.00431521e-06
Iter: 325 loss: 1.00688862e-06
Iter: 326 loss: 1.00315833e-06
Iter: 327 loss: 1.00069826e-06
Iter: 328 loss: 1.0058211e-06
Iter: 329 loss: 9.99724193e-07
Iter: 330 loss: 9.98282644e-07
Iter: 331 loss: 9.98156338e-07
Iter: 332 loss: 9.97059374e-07
Iter: 333 loss: 9.94639322e-07
Iter: 334 loss: 1.02983267e-06
Iter: 335 loss: 9.94525e-07
Iter: 336 loss: 9.9239935e-07
Iter: 337 loss: 1.0041515e-06
Iter: 338 loss: 9.92098e-07
Iter: 339 loss: 9.90394255e-07
Iter: 340 loss: 1.01170406e-06
Iter: 341 loss: 9.9039562e-07
Iter: 342 loss: 9.88985903e-07
Iter: 343 loss: 9.86529585e-07
Iter: 344 loss: 9.86527311e-07
Iter: 345 loss: 9.84743224e-07
Iter: 346 loss: 9.84674671e-07
Iter: 347 loss: 9.82872621e-07
Iter: 348 loss: 9.79462698e-07
Iter: 349 loss: 1.05569461e-06
Iter: 350 loss: 9.79477818e-07
Iter: 351 loss: 9.77143e-07
Iter: 352 loss: 9.84179451e-07
Iter: 353 loss: 9.76425554e-07
Iter: 354 loss: 9.75107e-07
Iter: 355 loss: 9.74920795e-07
Iter: 356 loss: 9.73625674e-07
Iter: 357 loss: 9.72241423e-07
Iter: 358 loss: 9.72034741e-07
Iter: 359 loss: 9.70503379e-07
Iter: 360 loss: 9.72481757e-07
Iter: 361 loss: 9.69742587e-07
Iter: 362 loss: 9.67872438e-07
Iter: 363 loss: 9.76287e-07
Iter: 364 loss: 9.67468509e-07
Iter: 365 loss: 9.66048333e-07
Iter: 366 loss: 9.83036216e-07
Iter: 367 loss: 9.66036851e-07
Iter: 368 loss: 9.65064146e-07
Iter: 369 loss: 9.62922172e-07
Iter: 370 loss: 9.96670678e-07
Iter: 371 loss: 9.62870331e-07
Iter: 372 loss: 9.60935381e-07
Iter: 373 loss: 9.73162e-07
Iter: 374 loss: 9.60731541e-07
Iter: 375 loss: 9.59026806e-07
Iter: 376 loss: 9.76691581e-07
Iter: 377 loss: 9.58983492e-07
Iter: 378 loss: 9.58104238e-07
Iter: 379 loss: 9.57691555e-07
Iter: 380 loss: 9.57237262e-07
Iter: 381 loss: 9.56138138e-07
Iter: 382 loss: 9.69541702e-07
Iter: 383 loss: 9.56112444e-07
Iter: 384 loss: 9.5504015e-07
Iter: 385 loss: 9.53076494e-07
Iter: 386 loss: 9.98296e-07
Iter: 387 loss: 9.5307314e-07
Iter: 388 loss: 9.51441962e-07
Iter: 389 loss: 9.54883717e-07
Iter: 390 loss: 9.50838626e-07
Iter: 391 loss: 9.4944761e-07
Iter: 392 loss: 9.49391733e-07
Iter: 393 loss: 9.48482352e-07
Iter: 394 loss: 9.46378236e-07
Iter: 395 loss: 9.72584075e-07
Iter: 396 loss: 9.46225e-07
Iter: 397 loss: 9.44400767e-07
Iter: 398 loss: 9.58626742e-07
Iter: 399 loss: 9.44276053e-07
Iter: 400 loss: 9.42875886e-07
Iter: 401 loss: 9.5145225e-07
Iter: 402 loss: 9.42692964e-07
Iter: 403 loss: 9.41530857e-07
Iter: 404 loss: 9.45919851e-07
Iter: 405 loss: 9.41229644e-07
Iter: 406 loss: 9.40205382e-07
Iter: 407 loss: 9.3888076e-07
Iter: 408 loss: 9.3883034e-07
Iter: 409 loss: 9.36788922e-07
Iter: 410 loss: 9.40335e-07
Iter: 411 loss: 9.35840831e-07
Iter: 412 loss: 9.34671903e-07
Iter: 413 loss: 9.34489947e-07
Iter: 414 loss: 9.33620413e-07
Iter: 415 loss: 9.31887143e-07
Iter: 416 loss: 9.64431592e-07
Iter: 417 loss: 9.31864065e-07
Iter: 418 loss: 9.30779834e-07
Iter: 419 loss: 9.30615101e-07
Iter: 420 loss: 9.29722262e-07
Iter: 421 loss: 9.28192321e-07
Iter: 422 loss: 9.28185614e-07
Iter: 423 loss: 9.27033113e-07
Iter: 424 loss: 9.3496908e-07
Iter: 425 loss: 9.26901691e-07
Iter: 426 loss: 9.25723612e-07
Iter: 427 loss: 9.34999889e-07
Iter: 428 loss: 9.25642837e-07
Iter: 429 loss: 9.24928713e-07
Iter: 430 loss: 9.23368589e-07
Iter: 431 loss: 9.47222532e-07
Iter: 432 loss: 9.23295602e-07
Iter: 433 loss: 9.21843309e-07
Iter: 434 loss: 9.29808095e-07
Iter: 435 loss: 9.21674427e-07
Iter: 436 loss: 9.20089576e-07
Iter: 437 loss: 9.28043903e-07
Iter: 438 loss: 9.19841398e-07
Iter: 439 loss: 9.18676733e-07
Iter: 440 loss: 9.21139531e-07
Iter: 441 loss: 9.18234264e-07
Iter: 442 loss: 9.17117632e-07
Iter: 443 loss: 9.16168e-07
Iter: 444 loss: 9.15793635e-07
Iter: 445 loss: 9.14305588e-07
Iter: 446 loss: 9.27926578e-07
Iter: 447 loss: 9.1427404e-07
Iter: 448 loss: 9.13285589e-07
Iter: 449 loss: 9.24435085e-07
Iter: 450 loss: 9.13268e-07
Iter: 451 loss: 9.1255805e-07
Iter: 452 loss: 9.10904362e-07
Iter: 453 loss: 9.32707394e-07
Iter: 454 loss: 9.10812503e-07
Iter: 455 loss: 9.09750838e-07
Iter: 456 loss: 9.0956928e-07
Iter: 457 loss: 9.08849131e-07
Iter: 458 loss: 9.07273659e-07
Iter: 459 loss: 9.33617e-07
Iter: 460 loss: 9.07263029e-07
Iter: 461 loss: 9.06135767e-07
Iter: 462 loss: 9.06131334e-07
Iter: 463 loss: 9.04861622e-07
Iter: 464 loss: 9.06246e-07
Iter: 465 loss: 9.04200363e-07
Iter: 466 loss: 9.03314799e-07
Iter: 467 loss: 9.02596867e-07
Iter: 468 loss: 9.02321744e-07
Iter: 469 loss: 9.01238081e-07
Iter: 470 loss: 9.06204605e-07
Iter: 471 loss: 9.01024634e-07
Iter: 472 loss: 8.99725364e-07
Iter: 473 loss: 9.08971799e-07
Iter: 474 loss: 8.99615941e-07
Iter: 475 loss: 8.98804387e-07
Iter: 476 loss: 8.98108397e-07
Iter: 477 loss: 8.97889095e-07
Iter: 478 loss: 8.96440781e-07
Iter: 479 loss: 8.98965141e-07
Iter: 480 loss: 8.95753033e-07
Iter: 481 loss: 8.94562163e-07
Iter: 482 loss: 9.02938041e-07
Iter: 483 loss: 8.94429036e-07
Iter: 484 loss: 8.93242941e-07
Iter: 485 loss: 8.98788244e-07
Iter: 486 loss: 8.93023298e-07
Iter: 487 loss: 8.92223113e-07
Iter: 488 loss: 8.92562923e-07
Iter: 489 loss: 8.91691286e-07
Iter: 490 loss: 8.90903e-07
Iter: 491 loss: 9.02205215e-07
Iter: 492 loss: 8.90884621e-07
Iter: 493 loss: 8.90337105e-07
Iter: 494 loss: 8.89031696e-07
Iter: 495 loss: 9.07025878e-07
Iter: 496 loss: 8.88950694e-07
Iter: 497 loss: 8.88086504e-07
Iter: 498 loss: 8.88044042e-07
Iter: 499 loss: 8.87113515e-07
Iter: 500 loss: 8.88767204e-07
Iter: 501 loss: 8.86705834e-07
Iter: 502 loss: 8.85926966e-07
Iter: 503 loss: 8.83931421e-07
Iter: 504 loss: 8.96882625e-07
Iter: 505 loss: 8.83434723e-07
Iter: 506 loss: 8.82917e-07
Iter: 507 loss: 8.82254824e-07
Iter: 508 loss: 8.81350331e-07
Iter: 509 loss: 8.85073518e-07
Iter: 510 loss: 8.81151664e-07
Iter: 511 loss: 8.80575612e-07
Iter: 512 loss: 8.79671347e-07
Iter: 513 loss: 8.79650543e-07
Iter: 514 loss: 8.78607295e-07
Iter: 515 loss: 8.85073518e-07
Iter: 516 loss: 8.78478772e-07
Iter: 517 loss: 8.77481739e-07
Iter: 518 loss: 8.80662242e-07
Iter: 519 loss: 8.77203547e-07
Iter: 520 loss: 8.76102035e-07
Iter: 521 loss: 8.8191166e-07
Iter: 522 loss: 8.7592025e-07
Iter: 523 loss: 8.75141836e-07
Iter: 524 loss: 8.74489217e-07
Iter: 525 loss: 8.74277305e-07
Iter: 526 loss: 8.72937562e-07
Iter: 527 loss: 8.86430371e-07
Iter: 528 loss: 8.72884812e-07
Iter: 529 loss: 8.72220085e-07
Iter: 530 loss: 8.70939118e-07
Iter: 531 loss: 8.98156259e-07
Iter: 532 loss: 8.70945e-07
Iter: 533 loss: 8.70626309e-07
Iter: 534 loss: 8.70313215e-07
Iter: 535 loss: 8.69672249e-07
Iter: 536 loss: 8.69330904e-07
Iter: 537 loss: 8.69121266e-07
Iter: 538 loss: 8.68396569e-07
Iter: 539 loss: 8.67656581e-07
Iter: 540 loss: 8.67529138e-07
Iter: 541 loss: 8.66751066e-07
Iter: 542 loss: 8.66729863e-07
Iter: 543 loss: 8.66200594e-07
Iter: 544 loss: 8.65175593e-07
Iter: 545 loss: 8.86459418e-07
Iter: 546 loss: 8.65172296e-07
Iter: 547 loss: 8.63836362e-07
Iter: 548 loss: 8.65780407e-07
Iter: 549 loss: 8.63198e-07
Iter: 550 loss: 8.62027548e-07
Iter: 551 loss: 8.65568438e-07
Iter: 552 loss: 8.61632486e-07
Iter: 553 loss: 8.60752607e-07
Iter: 554 loss: 8.73235251e-07
Iter: 555 loss: 8.6075022e-07
Iter: 556 loss: 8.59844704e-07
Iter: 557 loss: 8.61714454e-07
Iter: 558 loss: 8.59445038e-07
Iter: 559 loss: 8.58873591e-07
Iter: 560 loss: 8.60174623e-07
Iter: 561 loss: 8.58654346e-07
Iter: 562 loss: 8.57818634e-07
Iter: 563 loss: 8.59308784e-07
Iter: 564 loss: 8.57440114e-07
Iter: 565 loss: 8.56547103e-07
Iter: 566 loss: 8.56162956e-07
Iter: 567 loss: 8.55708322e-07
Iter: 568 loss: 8.54969471e-07
Iter: 569 loss: 8.54952248e-07
Iter: 570 loss: 8.54389043e-07
Iter: 571 loss: 8.56728093e-07
Iter: 572 loss: 8.54253415e-07
Iter: 573 loss: 8.53773713e-07
Iter: 574 loss: 8.53023323e-07
Iter: 575 loss: 8.53001e-07
Iter: 576 loss: 8.52147082e-07
Iter: 577 loss: 8.53725339e-07
Iter: 578 loss: 8.51801133e-07
Iter: 579 loss: 8.51137088e-07
Iter: 580 loss: 8.51098946e-07
Iter: 581 loss: 8.5058889e-07
Iter: 582 loss: 8.49456512e-07
Iter: 583 loss: 8.62537149e-07
Iter: 584 loss: 8.49349931e-07
Iter: 585 loss: 8.48234e-07
Iter: 586 loss: 8.52005542e-07
Iter: 587 loss: 8.47896672e-07
Iter: 588 loss: 8.46767193e-07
Iter: 589 loss: 8.5059412e-07
Iter: 590 loss: 8.4649696e-07
Iter: 591 loss: 8.45991053e-07
Iter: 592 loss: 8.45890952e-07
Iter: 593 loss: 8.45397608e-07
Iter: 594 loss: 8.44536032e-07
Iter: 595 loss: 8.44548538e-07
Iter: 596 loss: 8.43997611e-07
Iter: 597 loss: 8.43989255e-07
Iter: 598 loss: 8.43429063e-07
Iter: 599 loss: 8.42460736e-07
Iter: 600 loss: 8.42476425e-07
Iter: 601 loss: 8.41549877e-07
Iter: 602 loss: 8.45297564e-07
Iter: 603 loss: 8.41367864e-07
Iter: 604 loss: 8.40289886e-07
Iter: 605 loss: 8.47998422e-07
Iter: 606 loss: 8.40223493e-07
Iter: 607 loss: 8.39517497e-07
Iter: 608 loss: 8.38600158e-07
Iter: 609 loss: 8.38527626e-07
Iter: 610 loss: 8.37586754e-07
Iter: 611 loss: 8.44689851e-07
Iter: 612 loss: 8.37515756e-07
Iter: 613 loss: 8.36798563e-07
Iter: 614 loss: 8.40188534e-07
Iter: 615 loss: 8.36690901e-07
Iter: 616 loss: 8.35836886e-07
Iter: 617 loss: 8.36806862e-07
Iter: 618 loss: 8.35384071e-07
Iter: 619 loss: 8.34814841e-07
Iter: 620 loss: 8.33676381e-07
Iter: 621 loss: 8.55742087e-07
Iter: 622 loss: 8.33667286e-07
Iter: 623 loss: 8.32488809e-07
Iter: 624 loss: 8.39207246e-07
Iter: 625 loss: 8.32328055e-07
Iter: 626 loss: 8.31689647e-07
Iter: 627 loss: 8.31618195e-07
Iter: 628 loss: 8.31007185e-07
Iter: 629 loss: 8.30376166e-07
Iter: 630 loss: 8.30304657e-07
Iter: 631 loss: 8.29509645e-07
Iter: 632 loss: 8.34257833e-07
Iter: 633 loss: 8.2938061e-07
Iter: 634 loss: 8.28540919e-07
Iter: 635 loss: 8.30037436e-07
Iter: 636 loss: 8.28162456e-07
Iter: 637 loss: 8.27589929e-07
Iter: 638 loss: 8.27927352e-07
Iter: 639 loss: 8.27223346e-07
Iter: 640 loss: 8.26513e-07
Iter: 641 loss: 8.26507289e-07
Iter: 642 loss: 8.2614838e-07
Iter: 643 loss: 8.25328414e-07
Iter: 644 loss: 8.35152036e-07
Iter: 645 loss: 8.2526833e-07
Iter: 646 loss: 8.24248787e-07
Iter: 647 loss: 8.28283248e-07
Iter: 648 loss: 8.24051938e-07
Iter: 649 loss: 8.23384767e-07
Iter: 650 loss: 8.23383232e-07
Iter: 651 loss: 8.22875279e-07
Iter: 652 loss: 8.228958e-07
Iter: 653 loss: 8.22484935e-07
Iter: 654 loss: 8.21790593e-07
Iter: 655 loss: 8.21685376e-07
Iter: 656 loss: 8.21185267e-07
Iter: 657 loss: 8.20417483e-07
Iter: 658 loss: 8.22158199e-07
Iter: 659 loss: 8.20100126e-07
Iter: 660 loss: 8.19356273e-07
Iter: 661 loss: 8.21448396e-07
Iter: 662 loss: 8.19138165e-07
Iter: 663 loss: 8.18442743e-07
Iter: 664 loss: 8.18427225e-07
Iter: 665 loss: 8.18000558e-07
Iter: 666 loss: 8.17034106e-07
Iter: 667 loss: 8.29179385e-07
Iter: 668 loss: 8.16960664e-07
Iter: 669 loss: 8.16267743e-07
Iter: 670 loss: 8.16224428e-07
Iter: 671 loss: 8.15675094e-07
Iter: 672 loss: 8.15259398e-07
Iter: 673 loss: 8.15058115e-07
Iter: 674 loss: 8.14615419e-07
Iter: 675 loss: 8.14596945e-07
Iter: 676 loss: 8.14174427e-07
Iter: 677 loss: 8.13272038e-07
Iter: 678 loss: 8.25364054e-07
Iter: 679 loss: 8.13192457e-07
Iter: 680 loss: 8.12569965e-07
Iter: 681 loss: 8.19832394e-07
Iter: 682 loss: 8.12542623e-07
Iter: 683 loss: 8.11963787e-07
Iter: 684 loss: 8.17965656e-07
Iter: 685 loss: 8.11958557e-07
Iter: 686 loss: 8.11520067e-07
Iter: 687 loss: 8.10924121e-07
Iter: 688 loss: 8.10893312e-07
Iter: 689 loss: 8.10035772e-07
Iter: 690 loss: 8.10240408e-07
Iter: 691 loss: 8.09423625e-07
Iter: 692 loss: 8.0837026e-07
Iter: 693 loss: 8.12490043e-07
Iter: 694 loss: 8.08134246e-07
Iter: 695 loss: 8.0732724e-07
Iter: 696 loss: 8.14849727e-07
Iter: 697 loss: 8.07252e-07
Iter: 698 loss: 8.06422861e-07
Iter: 699 loss: 8.10100914e-07
Iter: 700 loss: 8.06249432e-07
Iter: 701 loss: 8.05854029e-07
Iter: 702 loss: 8.0537643e-07
Iter: 703 loss: 8.05324817e-07
Iter: 704 loss: 8.04598358e-07
Iter: 705 loss: 8.13325869e-07
Iter: 706 loss: 8.04587501e-07
Iter: 707 loss: 8.04177e-07
Iter: 708 loss: 8.04485808e-07
Iter: 709 loss: 8.03935166e-07
Iter: 710 loss: 8.03355192e-07
Iter: 711 loss: 8.06749028e-07
Iter: 712 loss: 8.03292835e-07
Iter: 713 loss: 8.02827e-07
Iter: 714 loss: 8.01715828e-07
Iter: 715 loss: 8.14698467e-07
Iter: 716 loss: 8.01623344e-07
Iter: 717 loss: 8.01170813e-07
Iter: 718 loss: 8.01011652e-07
Iter: 719 loss: 8.00429916e-07
Iter: 720 loss: 7.9998108e-07
Iter: 721 loss: 7.99803274e-07
Iter: 722 loss: 7.99060558e-07
Iter: 723 loss: 8.00663713e-07
Iter: 724 loss: 7.98796e-07
Iter: 725 loss: 7.98046187e-07
Iter: 726 loss: 7.99547934e-07
Iter: 727 loss: 7.97765324e-07
Iter: 728 loss: 7.97001064e-07
Iter: 729 loss: 7.9725794e-07
Iter: 730 loss: 7.96481e-07
Iter: 731 loss: 7.96095378e-07
Iter: 732 loss: 7.95972085e-07
Iter: 733 loss: 7.95521657e-07
Iter: 734 loss: 7.94826406e-07
Iter: 735 loss: 7.94827372e-07
Iter: 736 loss: 7.94162361e-07
Iter: 737 loss: 7.98191422e-07
Iter: 738 loss: 7.94069706e-07
Iter: 739 loss: 7.93349727e-07
Iter: 740 loss: 7.95502103e-07
Iter: 741 loss: 7.93126787e-07
Iter: 742 loss: 7.92737069e-07
Iter: 743 loss: 7.96075e-07
Iter: 744 loss: 7.92708533e-07
Iter: 745 loss: 7.92315745e-07
Iter: 746 loss: 7.92277547e-07
Iter: 747 loss: 7.9199981e-07
Iter: 748 loss: 7.91533807e-07
Iter: 749 loss: 7.90948775e-07
Iter: 750 loss: 7.90883291e-07
Iter: 751 loss: 7.90144441e-07
Iter: 752 loss: 7.90136482e-07
Iter: 753 loss: 7.89758076e-07
Iter: 754 loss: 7.89070214e-07
Iter: 755 loss: 7.89073681e-07
Iter: 756 loss: 7.88218586e-07
Iter: 757 loss: 7.89541218e-07
Iter: 758 loss: 7.87832448e-07
Iter: 759 loss: 7.86881515e-07
Iter: 760 loss: 7.9203312e-07
Iter: 761 loss: 7.86749752e-07
Iter: 762 loss: 7.86121745e-07
Iter: 763 loss: 7.87361955e-07
Iter: 764 loss: 7.85843895e-07
Iter: 765 loss: 7.85435816e-07
Iter: 766 loss: 7.85394207e-07
Iter: 767 loss: 7.85099246e-07
Iter: 768 loss: 7.84389044e-07
Iter: 769 loss: 7.93985805e-07
Iter: 770 loss: 7.84310373e-07
Iter: 771 loss: 7.83687e-07
Iter: 772 loss: 7.92321259e-07
Iter: 773 loss: 7.83680207e-07
Iter: 774 loss: 7.83063797e-07
Iter: 775 loss: 7.83704195e-07
Iter: 776 loss: 7.82719894e-07
Iter: 777 loss: 7.82212226e-07
Iter: 778 loss: 7.8576e-07
Iter: 779 loss: 7.82158963e-07
Iter: 780 loss: 7.81534084e-07
Iter: 781 loss: 7.81158064e-07
Iter: 782 loss: 7.80920857e-07
Iter: 783 loss: 7.8036112e-07
Iter: 784 loss: 7.80531366e-07
Iter: 785 loss: 7.7998925e-07
Iter: 786 loss: 7.79565653e-07
Iter: 787 loss: 7.79473794e-07
Iter: 788 loss: 7.79264496e-07
Iter: 789 loss: 7.7867918e-07
Iter: 790 loss: 7.84992e-07
Iter: 791 loss: 7.78621313e-07
Iter: 792 loss: 7.77943569e-07
Iter: 793 loss: 7.78918775e-07
Iter: 794 loss: 7.77616151e-07
Iter: 795 loss: 7.76870309e-07
Iter: 796 loss: 7.81926e-07
Iter: 797 loss: 7.76811703e-07
Iter: 798 loss: 7.76072852e-07
Iter: 799 loss: 7.78379444e-07
Iter: 800 loss: 7.75872195e-07
Iter: 801 loss: 7.75144e-07
Iter: 802 loss: 7.80307232e-07
Iter: 803 loss: 7.75088665e-07
Iter: 804 loss: 7.7464847e-07
Iter: 805 loss: 7.74045e-07
Iter: 806 loss: 7.74019099e-07
Iter: 807 loss: 7.73476074e-07
Iter: 808 loss: 7.73475961e-07
Iter: 809 loss: 7.72928388e-07
Iter: 810 loss: 7.72864041e-07
Iter: 811 loss: 7.72482736e-07
Iter: 812 loss: 7.72105636e-07
Iter: 813 loss: 7.72103249e-07
Iter: 814 loss: 7.71850409e-07
Iter: 815 loss: 7.71184318e-07
Iter: 816 loss: 7.76198306e-07
Iter: 817 loss: 7.71066e-07
Iter: 818 loss: 7.70295458e-07
Iter: 819 loss: 7.72281226e-07
Iter: 820 loss: 7.70042504e-07
Iter: 821 loss: 7.69490498e-07
Iter: 822 loss: 7.69467647e-07
Iter: 823 loss: 7.6904746e-07
Iter: 824 loss: 7.68531493e-07
Iter: 825 loss: 7.68484938e-07
Iter: 826 loss: 7.67812e-07
Iter: 827 loss: 7.67576807e-07
Iter: 828 loss: 7.6721426e-07
Iter: 829 loss: 7.66559083e-07
Iter: 830 loss: 7.75023523e-07
Iter: 831 loss: 7.66534299e-07
Iter: 832 loss: 7.6602953e-07
Iter: 833 loss: 7.70227246e-07
Iter: 834 loss: 7.65986215e-07
Iter: 835 loss: 7.65551704e-07
Iter: 836 loss: 7.66756045e-07
Iter: 837 loss: 7.654005e-07
Iter: 838 loss: 7.65022435e-07
Iter: 839 loss: 7.64584797e-07
Iter: 840 loss: 7.64525339e-07
Iter: 841 loss: 7.63949856e-07
Iter: 842 loss: 7.71859845e-07
Iter: 843 loss: 7.6395105e-07
Iter: 844 loss: 7.63461344e-07
Iter: 845 loss: 7.64461447e-07
Iter: 846 loss: 7.63277399e-07
Iter: 847 loss: 7.62867671e-07
Iter: 848 loss: 7.65323307e-07
Iter: 849 loss: 7.62843399e-07
Iter: 850 loss: 7.62451123e-07
Iter: 851 loss: 7.61768774e-07
Iter: 852 loss: 7.78507456e-07
Iter: 853 loss: 7.61766955e-07
Iter: 854 loss: 7.61249794e-07
Iter: 855 loss: 7.6202582e-07
Iter: 856 loss: 7.61002525e-07
Iter: 857 loss: 7.60291641e-07
Iter: 858 loss: 7.6143408e-07
Iter: 859 loss: 7.60008447e-07
Iter: 860 loss: 7.5966318e-07
Iter: 861 loss: 7.5958792e-07
Iter: 862 loss: 7.59147497e-07
Iter: 863 loss: 7.58480212e-07
Iter: 864 loss: 7.58486863e-07
Iter: 865 loss: 7.57837938e-07
Iter: 866 loss: 7.57466751e-07
Iter: 867 loss: 7.57183784e-07
Iter: 868 loss: 7.5667765e-07
Iter: 869 loss: 7.56629163e-07
Iter: 870 loss: 7.56164582e-07
Iter: 871 loss: 7.58860665e-07
Iter: 872 loss: 7.5609978e-07
Iter: 873 loss: 7.55761562e-07
Iter: 874 loss: 7.55472456e-07
Iter: 875 loss: 7.55368092e-07
Iter: 876 loss: 7.54963935e-07
Iter: 877 loss: 7.58856913e-07
Iter: 878 loss: 7.54916243e-07
Iter: 879 loss: 7.54536757e-07
Iter: 880 loss: 7.55785209e-07
Iter: 881 loss: 7.54407097e-07
Iter: 882 loss: 7.54006635e-07
Iter: 883 loss: 7.54449047e-07
Iter: 884 loss: 7.53811491e-07
Iter: 885 loss: 7.53377549e-07
Iter: 886 loss: 7.5612337e-07
Iter: 887 loss: 7.53354925e-07
Iter: 888 loss: 7.53020572e-07
Iter: 889 loss: 7.52241e-07
Iter: 890 loss: 7.62030709e-07
Iter: 891 loss: 7.52197479e-07
Iter: 892 loss: 7.51506718e-07
Iter: 893 loss: 7.56591248e-07
Iter: 894 loss: 7.51427706e-07
Iter: 895 loss: 7.50965796e-07
Iter: 896 loss: 7.51984317e-07
Iter: 897 loss: 7.50743e-07
Iter: 898 loss: 7.50461879e-07
Iter: 899 loss: 7.50445054e-07
Iter: 900 loss: 7.50158e-07
Iter: 901 loss: 7.49654248e-07
Iter: 902 loss: 7.49649871e-07
Iter: 903 loss: 7.49185915e-07
Iter: 904 loss: 7.48891921e-07
Iter: 905 loss: 7.48747652e-07
Iter: 906 loss: 7.48253115e-07
Iter: 907 loss: 7.48211619e-07
Iter: 908 loss: 7.47757213e-07
Iter: 909 loss: 7.47432068e-07
Iter: 910 loss: 7.47305648e-07
Iter: 911 loss: 7.46811907e-07
Iter: 912 loss: 7.46586693e-07
Iter: 913 loss: 7.46337321e-07
Iter: 914 loss: 7.46074306e-07
Iter: 915 loss: 7.459887e-07
Iter: 916 loss: 7.45643661e-07
Iter: 917 loss: 7.45969487e-07
Iter: 918 loss: 7.4544414e-07
Iter: 919 loss: 7.45081365e-07
Iter: 920 loss: 7.45795091e-07
Iter: 921 loss: 7.44918907e-07
Iter: 922 loss: 7.44442332e-07
Iter: 923 loss: 7.4541822e-07
Iter: 924 loss: 7.44266629e-07
Iter: 925 loss: 7.43760324e-07
Iter: 926 loss: 7.44115823e-07
Iter: 927 loss: 7.43457e-07
Iter: 928 loss: 7.42970315e-07
Iter: 929 loss: 7.42389034e-07
Iter: 930 loss: 7.42316786e-07
Iter: 931 loss: 7.41956569e-07
Iter: 932 loss: 7.41865847e-07
Iter: 933 loss: 7.41445376e-07
Iter: 934 loss: 7.418476e-07
Iter: 935 loss: 7.41178155e-07
Iter: 936 loss: 7.40777182e-07
Iter: 937 loss: 7.40576354e-07
Iter: 938 loss: 7.40391e-07
Iter: 939 loss: 7.39869392e-07
Iter: 940 loss: 7.4142423e-07
Iter: 941 loss: 7.39676182e-07
Iter: 942 loss: 7.39147e-07
Iter: 943 loss: 7.39153904e-07
Iter: 944 loss: 7.38829954e-07
Iter: 945 loss: 7.38036874e-07
Iter: 946 loss: 7.44008446e-07
Iter: 947 loss: 7.37879361e-07
Iter: 948 loss: 7.37254709e-07
Iter: 949 loss: 7.45745069e-07
Iter: 950 loss: 7.37247888e-07
Iter: 951 loss: 7.36894322e-07
Iter: 952 loss: 7.36891707e-07
Iter: 953 loss: 7.36585434e-07
Iter: 954 loss: 7.36085326e-07
Iter: 955 loss: 7.47949457e-07
Iter: 956 loss: 7.36080096e-07
Iter: 957 loss: 7.35736648e-07
Iter: 958 loss: 7.3574256e-07
Iter: 959 loss: 7.35449589e-07
Iter: 960 loss: 7.34837045e-07
Iter: 961 loss: 7.4385531e-07
Iter: 962 loss: 7.34825562e-07
Iter: 963 loss: 7.34183175e-07
Iter: 964 loss: 7.39350696e-07
Iter: 965 loss: 7.3412474e-07
Iter: 966 loss: 7.3362321e-07
Iter: 967 loss: 7.33514923e-07
Iter: 968 loss: 7.33199442e-07
Iter: 969 loss: 7.32732644e-07
Iter: 970 loss: 7.32712351e-07
Iter: 971 loss: 7.32307058e-07
Iter: 972 loss: 7.32154888e-07
Iter: 973 loss: 7.31939053e-07
Iter: 974 loss: 7.31517616e-07
Iter: 975 loss: 7.31394152e-07
Iter: 976 loss: 7.31143189e-07
Iter: 977 loss: 7.30972943e-07
Iter: 978 loss: 7.30862439e-07
Iter: 979 loss: 7.30588852e-07
Iter: 980 loss: 7.29974488e-07
Iter: 981 loss: 7.40363248e-07
Iter: 982 loss: 7.2996761e-07
Iter: 983 loss: 7.29337444e-07
Iter: 984 loss: 7.30294e-07
Iter: 985 loss: 7.29076e-07
Iter: 986 loss: 7.28722057e-07
Iter: 987 loss: 7.28663338e-07
Iter: 988 loss: 7.28269299e-07
Iter: 989 loss: 7.27666759e-07
Iter: 990 loss: 7.2762839e-07
Iter: 991 loss: 7.27215706e-07
Iter: 992 loss: 7.33098545e-07
Iter: 993 loss: 7.27203087e-07
Iter: 994 loss: 7.26755957e-07
Iter: 995 loss: 7.26584801e-07
Iter: 996 loss: 7.26368398e-07
Iter: 997 loss: 7.25988855e-07
Iter: 998 loss: 7.26936776e-07
Iter: 999 loss: 7.25832365e-07
Iter: 1000 loss: 7.25410189e-07
Iter: 1001 loss: 7.26664609e-07
Iter: 1002 loss: 7.25284735e-07
Iter: 1003 loss: 7.24882057e-07
Iter: 1004 loss: 7.27706492e-07
Iter: 1005 loss: 7.24843801e-07
Iter: 1006 loss: 7.2442225e-07
Iter: 1007 loss: 7.24467895e-07
Iter: 1008 loss: 7.24075619e-07
Iter: 1009 loss: 7.23553285e-07
Iter: 1010 loss: 7.23222968e-07
Iter: 1011 loss: 7.23025e-07
Iter: 1012 loss: 7.22614686e-07
Iter: 1013 loss: 7.22596212e-07
Iter: 1014 loss: 7.22134587e-07
Iter: 1015 loss: 7.22082518e-07
Iter: 1016 loss: 7.21727474e-07
Iter: 1017 loss: 7.21257948e-07
Iter: 1018 loss: 7.21025458e-07
Iter: 1019 loss: 7.20768469e-07
Iter: 1020 loss: 7.20203843e-07
Iter: 1021 loss: 7.22166419e-07
Iter: 1022 loss: 7.2007424e-07
Iter: 1023 loss: 7.19663944e-07
Iter: 1024 loss: 7.19637e-07
Iter: 1025 loss: 7.19300488e-07
Iter: 1026 loss: 7.18609272e-07
Iter: 1027 loss: 7.30278941e-07
Iter: 1028 loss: 7.18604156e-07
Iter: 1029 loss: 7.18059198e-07
Iter: 1030 loss: 7.22227071e-07
Iter: 1031 loss: 7.18004e-07
Iter: 1032 loss: 7.17421926e-07
Iter: 1033 loss: 7.18598358e-07
Iter: 1034 loss: 7.17169428e-07
Iter: 1035 loss: 7.16743671e-07
Iter: 1036 loss: 7.16289719e-07
Iter: 1037 loss: 7.16208774e-07
Iter: 1038 loss: 7.1577017e-07
Iter: 1039 loss: 7.20018704e-07
Iter: 1040 loss: 7.1575414e-07
Iter: 1041 loss: 7.15274837e-07
Iter: 1042 loss: 7.182731e-07
Iter: 1043 loss: 7.15235103e-07
Iter: 1044 loss: 7.14948101e-07
Iter: 1045 loss: 7.14699468e-07
Iter: 1046 loss: 7.14647854e-07
Iter: 1047 loss: 7.14061628e-07
Iter: 1048 loss: 7.14459475e-07
Iter: 1049 loss: 7.13710961e-07
Iter: 1050 loss: 7.13046916e-07
Iter: 1051 loss: 7.16053648e-07
Iter: 1052 loss: 7.12932888e-07
Iter: 1053 loss: 7.12480755e-07
Iter: 1054 loss: 7.12491e-07
Iter: 1055 loss: 7.12045392e-07
Iter: 1056 loss: 7.11158577e-07
Iter: 1057 loss: 7.26368967e-07
Iter: 1058 loss: 7.11141809e-07
Iter: 1059 loss: 7.10749305e-07
Iter: 1060 loss: 7.10733e-07
Iter: 1061 loss: 7.10321331e-07
Iter: 1062 loss: 7.11565e-07
Iter: 1063 loss: 7.10205313e-07
Iter: 1064 loss: 7.09934284e-07
Iter: 1065 loss: 7.093671e-07
Iter: 1066 loss: 7.19991931e-07
Iter: 1067 loss: 7.09365054e-07
Iter: 1068 loss: 7.08958169e-07
Iter: 1069 loss: 7.08946118e-07
Iter: 1070 loss: 7.08568223e-07
Iter: 1071 loss: 7.08473749e-07
Iter: 1072 loss: 7.08232051e-07
Iter: 1073 loss: 7.07791969e-07
Iter: 1074 loss: 7.0704084e-07
Iter: 1075 loss: 7.07039931e-07
Iter: 1076 loss: 7.06994342e-07
Iter: 1077 loss: 7.06724904e-07
Iter: 1078 loss: 7.0636861e-07
Iter: 1079 loss: 7.06356445e-07
Iter: 1080 loss: 7.06112701e-07
Iter: 1081 loss: 7.05733044e-07
Iter: 1082 loss: 7.05623506e-07
Iter: 1083 loss: 7.05390505e-07
Iter: 1084 loss: 7.0487414e-07
Iter: 1085 loss: 7.04725494e-07
Iter: 1086 loss: 7.04399156e-07
Iter: 1087 loss: 7.03906835e-07
Iter: 1088 loss: 7.0386136e-07
Iter: 1089 loss: 7.03427304e-07
Iter: 1090 loss: 7.04635909e-07
Iter: 1091 loss: 7.03279284e-07
Iter: 1092 loss: 7.02840168e-07
Iter: 1093 loss: 7.02823968e-07
Iter: 1094 loss: 7.02430043e-07
Iter: 1095 loss: 7.02030093e-07
Iter: 1096 loss: 7.04046386e-07
Iter: 1097 loss: 7.01922943e-07
Iter: 1098 loss: 7.01509748e-07
Iter: 1099 loss: 7.05451725e-07
Iter: 1100 loss: 7.01482463e-07
Iter: 1101 loss: 7.01195063e-07
Iter: 1102 loss: 7.00630778e-07
Iter: 1103 loss: 7.11176654e-07
Iter: 1104 loss: 7.0062e-07
Iter: 1105 loss: 7.00060923e-07
Iter: 1106 loss: 7.01721717e-07
Iter: 1107 loss: 6.99897839e-07
Iter: 1108 loss: 6.99489419e-07
Iter: 1109 loss: 6.99456507e-07
Iter: 1110 loss: 6.99189e-07
Iter: 1111 loss: 6.98514555e-07
Iter: 1112 loss: 7.06496166e-07
Iter: 1113 loss: 6.98473514e-07
Iter: 1114 loss: 6.97750579e-07
Iter: 1115 loss: 7.02357283e-07
Iter: 1116 loss: 6.97680889e-07
Iter: 1117 loss: 6.97377345e-07
Iter: 1118 loss: 6.97330279e-07
Iter: 1119 loss: 6.97086762e-07
Iter: 1120 loss: 6.96548909e-07
Iter: 1121 loss: 7.02655427e-07
Iter: 1122 loss: 6.96481038e-07
Iter: 1123 loss: 6.95881226e-07
Iter: 1124 loss: 6.97032647e-07
Iter: 1125 loss: 6.95619747e-07
Iter: 1126 loss: 6.95016297e-07
Iter: 1127 loss: 6.96834832e-07
Iter: 1128 loss: 6.94825815e-07
Iter: 1129 loss: 6.94472533e-07
Iter: 1130 loss: 6.9440506e-07
Iter: 1131 loss: 6.94162054e-07
Iter: 1132 loss: 6.93588845e-07
Iter: 1133 loss: 6.99761301e-07
Iter: 1134 loss: 6.93507673e-07
Iter: 1135 loss: 6.9317997e-07
Iter: 1136 loss: 6.93141146e-07
Iter: 1137 loss: 6.92848744e-07
Iter: 1138 loss: 6.93476181e-07
Iter: 1139 loss: 6.92697313e-07
Iter: 1140 loss: 6.92387232e-07
Iter: 1141 loss: 6.92138656e-07
Iter: 1142 loss: 6.9204566e-07
Iter: 1143 loss: 6.9159762e-07
Iter: 1144 loss: 6.93548259e-07
Iter: 1145 loss: 6.91497576e-07
Iter: 1146 loss: 6.91143839e-07
Iter: 1147 loss: 6.94212929e-07
Iter: 1148 loss: 6.9111951e-07
Iter: 1149 loss: 6.90767706e-07
Iter: 1150 loss: 6.90106731e-07
Iter: 1151 loss: 7.03649334e-07
Iter: 1152 loss: 6.90103207e-07
Iter: 1153 loss: 6.89617423e-07
Iter: 1154 loss: 6.96551638e-07
Iter: 1155 loss: 6.89611909e-07
Iter: 1156 loss: 6.89101569e-07
Iter: 1157 loss: 6.90541469e-07
Iter: 1158 loss: 6.88933198e-07
Iter: 1159 loss: 6.88631303e-07
Iter: 1160 loss: 6.88047066e-07
Iter: 1161 loss: 7.0153726e-07
Iter: 1162 loss: 6.88050875e-07
Iter: 1163 loss: 6.87382681e-07
Iter: 1164 loss: 6.90725415e-07
Iter: 1165 loss: 6.87247962e-07
Iter: 1166 loss: 6.87014563e-07
Iter: 1167 loss: 6.87006946e-07
Iter: 1168 loss: 6.86677083e-07
Iter: 1169 loss: 6.86144517e-07
Iter: 1170 loss: 6.992218e-07
Iter: 1171 loss: 6.86145427e-07
Iter: 1172 loss: 6.8559757e-07
Iter: 1173 loss: 6.87229715e-07
Iter: 1174 loss: 6.85434429e-07
Iter: 1175 loss: 6.85001851e-07
Iter: 1176 loss: 6.84996166e-07
Iter: 1177 loss: 6.84709221e-07
Iter: 1178 loss: 6.84121e-07
Iter: 1179 loss: 6.95935455e-07
Iter: 1180 loss: 6.84123677e-07
Iter: 1181 loss: 6.8358e-07
Iter: 1182 loss: 6.85133955e-07
Iter: 1183 loss: 6.83405915e-07
Iter: 1184 loss: 6.82838049e-07
Iter: 1185 loss: 6.87179408e-07
Iter: 1186 loss: 6.82794905e-07
Iter: 1187 loss: 6.82434461e-07
Iter: 1188 loss: 6.84801876e-07
Iter: 1189 loss: 6.82382961e-07
Iter: 1190 loss: 6.82109089e-07
Iter: 1191 loss: 6.81638426e-07
Iter: 1192 loss: 6.81623135e-07
Iter: 1193 loss: 6.81233871e-07
Iter: 1194 loss: 6.81238134e-07
Iter: 1195 loss: 6.80802486e-07
Iter: 1196 loss: 6.8035115e-07
Iter: 1197 loss: 6.80262644e-07
Iter: 1198 loss: 6.7972104e-07
Iter: 1199 loss: 6.80632184e-07
Iter: 1200 loss: 6.79461436e-07
Iter: 1201 loss: 6.7904773e-07
Iter: 1202 loss: 6.84436486e-07
Iter: 1203 loss: 6.79060236e-07
Iter: 1204 loss: 6.78616175e-07
Iter: 1205 loss: 6.79163406e-07
Iter: 1206 loss: 6.78394713e-07
Iter: 1207 loss: 6.78118283e-07
Iter: 1208 loss: 6.77931666e-07
Iter: 1209 loss: 6.77837647e-07
Iter: 1210 loss: 6.77442927e-07
Iter: 1211 loss: 6.77438038e-07
Iter: 1212 loss: 6.77163143e-07
Iter: 1213 loss: 6.76579873e-07
Iter: 1214 loss: 6.85532882e-07
Iter: 1215 loss: 6.76534626e-07
Iter: 1216 loss: 6.75922308e-07
Iter: 1217 loss: 6.77098228e-07
Iter: 1218 loss: 6.75671572e-07
Iter: 1219 loss: 6.75049591e-07
Iter: 1220 loss: 6.8063423e-07
Iter: 1221 loss: 6.75037427e-07
Iter: 1222 loss: 6.74537887e-07
Iter: 1223 loss: 6.77359594e-07
Iter: 1224 loss: 6.74499574e-07
Iter: 1225 loss: 6.74151238e-07
Iter: 1226 loss: 6.7493886e-07
Iter: 1227 loss: 6.74052671e-07
Iter: 1228 loss: 6.73740658e-07
Iter: 1229 loss: 6.74417549e-07
Iter: 1230 loss: 6.73637487e-07
Iter: 1231 loss: 6.73240152e-07
Iter: 1232 loss: 6.74383e-07
Iter: 1233 loss: 6.73127204e-07
Iter: 1234 loss: 6.72768465e-07
Iter: 1235 loss: 6.72313945e-07
Iter: 1236 loss: 6.72285523e-07
Iter: 1237 loss: 6.71651094e-07
Iter: 1238 loss: 6.72526141e-07
Iter: 1239 loss: 6.7135187e-07
Iter: 1240 loss: 6.71056341e-07
Iter: 1241 loss: 6.70928785e-07
Iter: 1242 loss: 6.70605573e-07
Iter: 1243 loss: 6.70206191e-07
Iter: 1244 loss: 6.7016208e-07
Iter: 1245 loss: 6.69850579e-07
Iter: 1246 loss: 6.73217869e-07
Iter: 1247 loss: 6.6984262e-07
Iter: 1248 loss: 6.69482574e-07
Iter: 1249 loss: 6.69968472e-07
Iter: 1250 loss: 6.69287e-07
Iter: 1251 loss: 6.6904056e-07
Iter: 1252 loss: 6.68504526e-07
Iter: 1253 loss: 6.75555839e-07
Iter: 1254 loss: 6.68451889e-07
Iter: 1255 loss: 6.67738846e-07
Iter: 1256 loss: 6.71401267e-07
Iter: 1257 loss: 6.67622487e-07
Iter: 1258 loss: 6.67220775e-07
Iter: 1259 loss: 6.67218046e-07
Iter: 1260 loss: 6.66827191e-07
Iter: 1261 loss: 6.66332426e-07
Iter: 1262 loss: 6.66278595e-07
Iter: 1263 loss: 6.6595851e-07
Iter: 1264 loss: 6.65949415e-07
Iter: 1265 loss: 6.65608e-07
Iter: 1266 loss: 6.6537757e-07
Iter: 1267 loss: 6.65256835e-07
Iter: 1268 loss: 6.64914921e-07
Iter: 1269 loss: 6.66377559e-07
Iter: 1270 loss: 6.64836421e-07
Iter: 1271 loss: 6.64467052e-07
Iter: 1272 loss: 6.64105301e-07
Iter: 1273 loss: 6.64048116e-07
Iter: 1274 loss: 6.63717969e-07
Iter: 1275 loss: 6.63693413e-07
Iter: 1276 loss: 6.6333223e-07
Iter: 1277 loss: 6.63025389e-07
Iter: 1278 loss: 6.62963771e-07
Iter: 1279 loss: 6.62483842e-07
Iter: 1280 loss: 6.62727871e-07
Iter: 1281 loss: 6.62180469e-07
Iter: 1282 loss: 6.61928311e-07
Iter: 1283 loss: 6.61870502e-07
Iter: 1284 loss: 6.61594243e-07
Iter: 1285 loss: 6.60987666e-07
Iter: 1286 loss: 6.72918645e-07
Iter: 1287 loss: 6.60999149e-07
Iter: 1288 loss: 6.60544288e-07
Iter: 1289 loss: 6.60914679e-07
Iter: 1290 loss: 6.603006e-07
Iter: 1291 loss: 6.59819193e-07
Iter: 1292 loss: 6.59833177e-07
Iter: 1293 loss: 6.5940327e-07
Iter: 1294 loss: 6.59720058e-07
Iter: 1295 loss: 6.59158786e-07
Iter: 1296 loss: 6.58751333e-07
Iter: 1297 loss: 6.58779754e-07
Iter: 1298 loss: 6.58452336e-07
Iter: 1299 loss: 6.58022827e-07
Iter: 1300 loss: 6.58026e-07
Iter: 1301 loss: 6.57724968e-07
Iter: 1302 loss: 6.5709321e-07
Iter: 1303 loss: 6.64956588e-07
Iter: 1304 loss: 6.57031705e-07
Iter: 1305 loss: 6.56543534e-07
Iter: 1306 loss: 6.62495097e-07
Iter: 1307 loss: 6.56547911e-07
Iter: 1308 loss: 6.56127327e-07
Iter: 1309 loss: 6.5674044e-07
Iter: 1310 loss: 6.55942e-07
Iter: 1311 loss: 6.55619885e-07
Iter: 1312 loss: 6.55616191e-07
Iter: 1313 loss: 6.55340557e-07
Iter: 1314 loss: 6.54904454e-07
Iter: 1315 loss: 6.54904738e-07
Iter: 1316 loss: 6.54428277e-07
Iter: 1317 loss: 6.542287e-07
Iter: 1318 loss: 6.53975235e-07
Iter: 1319 loss: 6.53416805e-07
Iter: 1320 loss: 6.5824355e-07
Iter: 1321 loss: 6.53412087e-07
Iter: 1322 loss: 6.52946255e-07
Iter: 1323 loss: 6.58701822e-07
Iter: 1324 loss: 6.52936365e-07
Iter: 1325 loss: 6.52637823e-07
Iter: 1326 loss: 6.52077858e-07
Iter: 1327 loss: 6.62670175e-07
Iter: 1328 loss: 6.52061317e-07
Iter: 1329 loss: 6.51654318e-07
Iter: 1330 loss: 6.54817086e-07
Iter: 1331 loss: 6.51618279e-07
Iter: 1332 loss: 6.51247092e-07
Iter: 1333 loss: 6.54261441e-07
Iter: 1334 loss: 6.51197e-07
Iter: 1335 loss: 6.50939569e-07
Iter: 1336 loss: 6.50623178e-07
Iter: 1337 loss: 6.50598508e-07
Iter: 1338 loss: 6.50284164e-07
Iter: 1339 loss: 6.50268362e-07
Iter: 1340 loss: 6.50012453e-07
Iter: 1341 loss: 6.49495746e-07
Iter: 1342 loss: 6.59935e-07
Iter: 1343 loss: 6.49459707e-07
Iter: 1344 loss: 6.48910145e-07
Iter: 1345 loss: 6.49251433e-07
Iter: 1346 loss: 6.48550099e-07
Iter: 1347 loss: 6.48363937e-07
Iter: 1348 loss: 6.48231378e-07
Iter: 1349 loss: 6.47972058e-07
Iter: 1350 loss: 6.48233e-07
Iter: 1351 loss: 6.47848765e-07
Iter: 1352 loss: 6.47494289e-07
Iter: 1353 loss: 6.47297384e-07
Iter: 1354 loss: 6.47159766e-07
Iter: 1355 loss: 6.46708941e-07
Iter: 1356 loss: 6.48605749e-07
Iter: 1357 loss: 6.46610715e-07
Iter: 1358 loss: 6.46331614e-07
Iter: 1359 loss: 6.46328203e-07
Iter: 1360 loss: 6.46000501e-07
Iter: 1361 loss: 6.45392561e-07
Iter: 1362 loss: 6.56550128e-07
Iter: 1363 loss: 6.45392504e-07
Iter: 1364 loss: 6.4488097e-07
Iter: 1365 loss: 6.4644e-07
Iter: 1366 loss: 6.44739032e-07
Iter: 1367 loss: 6.44209251e-07
Iter: 1368 loss: 6.49675599e-07
Iter: 1369 loss: 6.44213344e-07
Iter: 1370 loss: 6.43959083e-07
Iter: 1371 loss: 6.43854946e-07
Iter: 1372 loss: 6.4368578e-07
Iter: 1373 loss: 6.43350745e-07
Iter: 1374 loss: 6.47753211e-07
Iter: 1375 loss: 6.43333408e-07
Iter: 1376 loss: 6.43124736e-07
Iter: 1377 loss: 6.42653276e-07
Iter: 1378 loss: 6.51051096e-07
Iter: 1379 loss: 6.42669875e-07
Iter: 1380 loss: 6.42184602e-07
Iter: 1381 loss: 6.44725105e-07
Iter: 1382 loss: 6.42144471e-07
Iter: 1383 loss: 6.41757424e-07
Iter: 1384 loss: 6.4544065e-07
Iter: 1385 loss: 6.41741963e-07
Iter: 1386 loss: 6.41462577e-07
Iter: 1387 loss: 6.41140559e-07
Iter: 1388 loss: 6.41089855e-07
Iter: 1389 loss: 6.40516305e-07
Iter: 1390 loss: 6.41217355e-07
Iter: 1391 loss: 6.40225323e-07
Iter: 1392 loss: 6.39748805e-07
Iter: 1393 loss: 6.42771624e-07
Iter: 1394 loss: 6.39679342e-07
Iter: 1395 loss: 6.39359598e-07
Iter: 1396 loss: 6.39356472e-07
Iter: 1397 loss: 6.39193956e-07
Iter: 1398 loss: 6.38732047e-07
Iter: 1399 loss: 6.44236422e-07
Iter: 1400 loss: 6.38713232e-07
Iter: 1401 loss: 6.38374104e-07
Iter: 1402 loss: 6.42452505e-07
Iter: 1403 loss: 6.38348922e-07
Iter: 1404 loss: 6.3797296e-07
Iter: 1405 loss: 6.38652182e-07
Iter: 1406 loss: 6.37839321e-07
Iter: 1407 loss: 6.37434596e-07
Iter: 1408 loss: 6.37567155e-07
Iter: 1409 loss: 6.37180392e-07
Iter: 1410 loss: 6.36663572e-07
Iter: 1411 loss: 6.41458939e-07
Iter: 1412 loss: 6.36635832e-07
Iter: 1413 loss: 6.36394589e-07
Iter: 1414 loss: 6.35850085e-07
Iter: 1415 loss: 6.44526949e-07
Iter: 1416 loss: 6.35828314e-07
Iter: 1417 loss: 6.35484753e-07
Iter: 1418 loss: 6.35478727e-07
Iter: 1419 loss: 6.35110837e-07
Iter: 1420 loss: 6.35888284e-07
Iter: 1421 loss: 6.34991807e-07
Iter: 1422 loss: 6.3472271e-07
Iter: 1423 loss: 6.34715661e-07
Iter: 1424 loss: 6.34460662e-07
Iter: 1425 loss: 6.34085268e-07
Iter: 1426 loss: 6.35393121e-07
Iter: 1427 loss: 6.33970558e-07
Iter: 1428 loss: 6.33641434e-07
Iter: 1429 loss: 6.36148798e-07
Iter: 1430 loss: 6.33628304e-07
Iter: 1431 loss: 6.33207719e-07
Iter: 1432 loss: 6.33160425e-07
Iter: 1433 loss: 6.32871263e-07
Iter: 1434 loss: 6.32494789e-07
Iter: 1435 loss: 6.32467618e-07
Iter: 1436 loss: 6.32186811e-07
Iter: 1437 loss: 6.31912485e-07
Iter: 1438 loss: 6.31906175e-07
Iter: 1439 loss: 6.31650607e-07
Iter: 1440 loss: 6.31566252e-07
Iter: 1441 loss: 6.31443356e-07
Iter: 1442 loss: 6.31166472e-07
Iter: 1443 loss: 6.33643936e-07
Iter: 1444 loss: 6.31153114e-07
Iter: 1445 loss: 6.30867646e-07
Iter: 1446 loss: 6.30713259e-07
Iter: 1447 loss: 6.30577802e-07
Iter: 1448 loss: 6.3029529e-07
Iter: 1449 loss: 6.30188651e-07
Iter: 1450 loss: 6.30010902e-07
Iter: 1451 loss: 6.29707188e-07
Iter: 1452 loss: 6.29680471e-07
Iter: 1453 loss: 6.29429564e-07
Iter: 1454 loss: 6.2895856e-07
Iter: 1455 loss: 6.28965836e-07
Iter: 1456 loss: 6.28492e-07
Iter: 1457 loss: 6.30171144e-07
Iter: 1458 loss: 6.28380519e-07
Iter: 1459 loss: 6.27981535e-07
Iter: 1460 loss: 6.32183401e-07
Iter: 1461 loss: 6.27980057e-07
Iter: 1462 loss: 6.27664861e-07
Iter: 1463 loss: 6.29244482e-07
Iter: 1464 loss: 6.27595909e-07
Iter: 1465 loss: 6.27339773e-07
Iter: 1466 loss: 6.27008262e-07
Iter: 1467 loss: 6.26995416e-07
Iter: 1468 loss: 6.26693748e-07
Iter: 1469 loss: 6.29912506e-07
Iter: 1470 loss: 6.26675e-07
Iter: 1471 loss: 6.26359e-07
Iter: 1472 loss: 6.26504516e-07
Iter: 1473 loss: 6.26124574e-07
Iter: 1474 loss: 6.25749408e-07
Iter: 1475 loss: 6.28030818e-07
Iter: 1476 loss: 6.25703137e-07
Iter: 1477 loss: 6.25354176e-07
Iter: 1478 loss: 6.25743155e-07
Iter: 1479 loss: 6.25157668e-07
Iter: 1480 loss: 6.24812628e-07
Iter: 1481 loss: 6.24479412e-07
Iter: 1482 loss: 6.2440165e-07
Iter: 1483 loss: 6.24110726e-07
Iter: 1484 loss: 6.24109e-07
Iter: 1485 loss: 6.23798599e-07
Iter: 1486 loss: 6.24142899e-07
Iter: 1487 loss: 6.23616813e-07
Iter: 1488 loss: 6.23375797e-07
Iter: 1489 loss: 6.23012795e-07
Iter: 1490 loss: 6.22982725e-07
Iter: 1491 loss: 6.22505922e-07
Iter: 1492 loss: 6.2605136e-07
Iter: 1493 loss: 6.22462323e-07
Iter: 1494 loss: 6.22126208e-07
Iter: 1495 loss: 6.26258384e-07
Iter: 1496 loss: 6.22143375e-07
Iter: 1497 loss: 6.21770937e-07
Iter: 1498 loss: 6.21385539e-07
Iter: 1499 loss: 6.21355071e-07
Iter: 1500 loss: 6.20887192e-07
Iter: 1501 loss: 6.21139293e-07
Iter: 1502 loss: 6.20602236e-07
Iter: 1503 loss: 6.20175683e-07
Iter: 1504 loss: 6.20160904e-07
Iter: 1505 loss: 6.19893e-07
Iter: 1506 loss: 6.1994092e-07
Iter: 1507 loss: 6.19684556e-07
Iter: 1508 loss: 6.1941364e-07
Iter: 1509 loss: 6.22340053e-07
Iter: 1510 loss: 6.19405512e-07
Iter: 1511 loss: 6.19138177e-07
Iter: 1512 loss: 6.18814909e-07
Iter: 1513 loss: 6.18789272e-07
Iter: 1514 loss: 6.1841871e-07
Iter: 1515 loss: 6.18457e-07
Iter: 1516 loss: 6.18118747e-07
Iter: 1517 loss: 6.17831859e-07
Iter: 1518 loss: 6.17798264e-07
Iter: 1519 loss: 6.17561e-07
Iter: 1520 loss: 6.17150477e-07
Iter: 1521 loss: 6.27151621e-07
Iter: 1522 loss: 6.17153034e-07
Iter: 1523 loss: 6.16654518e-07
Iter: 1524 loss: 6.16588125e-07
Iter: 1525 loss: 6.16222792e-07
Iter: 1526 loss: 6.15869567e-07
Iter: 1527 loss: 6.15844215e-07
Iter: 1528 loss: 6.15504746e-07
Iter: 1529 loss: 6.17095e-07
Iter: 1530 loss: 6.15448812e-07
Iter: 1531 loss: 6.15169483e-07
Iter: 1532 loss: 6.15144074e-07
Iter: 1533 loss: 6.14941428e-07
Iter: 1534 loss: 6.14614123e-07
Iter: 1535 loss: 6.14246801e-07
Iter: 1536 loss: 6.1421639e-07
Iter: 1537 loss: 6.13968382e-07
Iter: 1538 loss: 6.13872146e-07
Iter: 1539 loss: 6.13638804e-07
Iter: 1540 loss: 6.13344241e-07
Iter: 1541 loss: 6.13334805e-07
Iter: 1542 loss: 6.12956967e-07
Iter: 1543 loss: 6.17764726e-07
Iter: 1544 loss: 6.12951567e-07
Iter: 1545 loss: 6.12692531e-07
Iter: 1546 loss: 6.12464191e-07
Iter: 1547 loss: 6.1239291e-07
Iter: 1548 loss: 6.12064071e-07
Iter: 1549 loss: 6.11947883e-07
Iter: 1550 loss: 6.11731934e-07
Iter: 1551 loss: 6.11677e-07
Iter: 1552 loss: 6.11541225e-07
Iter: 1553 loss: 6.11326755e-07
Iter: 1554 loss: 6.1104231e-07
Iter: 1555 loss: 6.1102088e-07
Iter: 1556 loss: 6.10645e-07
Iter: 1557 loss: 6.10853135e-07
Iter: 1558 loss: 6.10382244e-07
Iter: 1559 loss: 6.1017e-07
Iter: 1560 loss: 6.10139068e-07
Iter: 1561 loss: 6.09834558e-07
Iter: 1562 loss: 6.09578763e-07
Iter: 1563 loss: 6.0951777e-07
Iter: 1564 loss: 6.09192625e-07
Iter: 1565 loss: 6.09786582e-07
Iter: 1566 loss: 6.09083827e-07
Iter: 1567 loss: 6.08767664e-07
Iter: 1568 loss: 6.10590291e-07
Iter: 1569 loss: 6.08714117e-07
Iter: 1570 loss: 6.08483731e-07
Iter: 1571 loss: 6.10028735e-07
Iter: 1572 loss: 6.08461278e-07
Iter: 1573 loss: 6.0822191e-07
Iter: 1574 loss: 6.08062e-07
Iter: 1575 loss: 6.07987e-07
Iter: 1576 loss: 6.07655352e-07
Iter: 1577 loss: 6.11013661e-07
Iter: 1578 loss: 6.07672121e-07
Iter: 1579 loss: 6.07411494e-07
Iter: 1580 loss: 6.07219704e-07
Iter: 1581 loss: 6.07152288e-07
Iter: 1582 loss: 6.06800654e-07
Iter: 1583 loss: 6.06514845e-07
Iter: 1584 loss: 6.06401898e-07
Iter: 1585 loss: 6.0586774e-07
Iter: 1586 loss: 6.08041205e-07
Iter: 1587 loss: 6.05746152e-07
Iter: 1588 loss: 6.05505193e-07
Iter: 1589 loss: 6.05496552e-07
Iter: 1590 loss: 6.05227683e-07
Iter: 1591 loss: 6.05557887e-07
Iter: 1592 loss: 6.0508421e-07
Iter: 1593 loss: 6.0487946e-07
Iter: 1594 loss: 6.04786237e-07
Iter: 1595 loss: 6.04684942e-07
Iter: 1596 loss: 6.04278796e-07
Iter: 1597 loss: 6.07159336e-07
Iter: 1598 loss: 6.04228774e-07
Iter: 1599 loss: 6.03999752e-07
Iter: 1600 loss: 6.03660624e-07
Iter: 1601 loss: 6.03669093e-07
Iter: 1602 loss: 6.03218837e-07
Iter: 1603 loss: 6.05277251e-07
Iter: 1604 loss: 6.03136527e-07
Iter: 1605 loss: 6.02787111e-07
Iter: 1606 loss: 6.06294691e-07
Iter: 1607 loss: 6.02791602e-07
Iter: 1608 loss: 6.02564e-07
Iter: 1609 loss: 6.02928367e-07
Iter: 1610 loss: 6.02457703e-07
Iter: 1611 loss: 6.02205432e-07
Iter: 1612 loss: 6.02782848e-07
Iter: 1613 loss: 6.0211471e-07
Iter: 1614 loss: 6.01825263e-07
Iter: 1615 loss: 6.02712873e-07
Iter: 1616 loss: 6.01759e-07
Iter: 1617 loss: 6.01523766e-07
Iter: 1618 loss: 6.01093461e-07
Iter: 1619 loss: 6.10187499e-07
Iter: 1620 loss: 6.01101647e-07
Iter: 1621 loss: 6.00623878e-07
Iter: 1622 loss: 6.0178138e-07
Iter: 1623 loss: 6.00450676e-07
Iter: 1624 loss: 5.99935163e-07
Iter: 1625 loss: 6.02779892e-07
Iter: 1626 loss: 5.99874056e-07
Iter: 1627 loss: 5.9971137e-07
Iter: 1628 loss: 5.99644068e-07
Iter: 1629 loss: 5.99463192e-07
Iter: 1630 loss: 5.99110308e-07
Iter: 1631 loss: 6.06976528e-07
Iter: 1632 loss: 5.99094506e-07
Iter: 1633 loss: 5.98965585e-07
Iter: 1634 loss: 5.98946144e-07
Iter: 1635 loss: 5.98771294e-07
Iter: 1636 loss: 5.98481051e-07
Iter: 1637 loss: 6.04561365e-07
Iter: 1638 loss: 5.98481e-07
Iter: 1639 loss: 5.98177166e-07
Iter: 1640 loss: 5.98568306e-07
Iter: 1641 loss: 5.98031875e-07
Iter: 1642 loss: 5.97730377e-07
Iter: 1643 loss: 6.00379906e-07
Iter: 1644 loss: 5.97693884e-07
Iter: 1645 loss: 5.97404835e-07
Iter: 1646 loss: 5.97903806e-07
Iter: 1647 loss: 5.97257497e-07
Iter: 1648 loss: 5.96949803e-07
Iter: 1649 loss: 5.97534608e-07
Iter: 1650 loss: 5.96830489e-07
Iter: 1651 loss: 5.96542691e-07
Iter: 1652 loss: 5.98772772e-07
Iter: 1653 loss: 5.9652416e-07
Iter: 1654 loss: 5.96302755e-07
Iter: 1655 loss: 5.96348514e-07
Iter: 1656 loss: 5.9616923e-07
Iter: 1657 loss: 5.95929691e-07
Iter: 1658 loss: 5.95722724e-07
Iter: 1659 loss: 5.9563348e-07
Iter: 1660 loss: 5.95260872e-07
Iter: 1661 loss: 5.96413486e-07
Iter: 1662 loss: 5.95141501e-07
Iter: 1663 loss: 5.94763719e-07
Iter: 1664 loss: 5.96446114e-07
Iter: 1665 loss: 5.94714038e-07
Iter: 1666 loss: 5.94232461e-07
Iter: 1667 loss: 5.97326675e-07
Iter: 1668 loss: 5.94189373e-07
Iter: 1669 loss: 5.93988148e-07
Iter: 1670 loss: 5.93995082e-07
Iter: 1671 loss: 5.93829e-07
Iter: 1672 loss: 5.93484629e-07
Iter: 1673 loss: 5.96245854e-07
Iter: 1674 loss: 5.9349793e-07
Iter: 1675 loss: 5.93286813e-07
Iter: 1676 loss: 5.92798131e-07
Iter: 1677 loss: 5.98822567e-07
Iter: 1678 loss: 5.92726053e-07
Iter: 1679 loss: 5.92353e-07
Iter: 1680 loss: 5.94279754e-07
Iter: 1681 loss: 5.92286767e-07
Iter: 1682 loss: 5.91939397e-07
Iter: 1683 loss: 5.93170739e-07
Iter: 1684 loss: 5.91847652e-07
Iter: 1685 loss: 5.91588673e-07
Iter: 1686 loss: 5.95145195e-07
Iter: 1687 loss: 5.91578896e-07
Iter: 1688 loss: 5.91314461e-07
Iter: 1689 loss: 5.91323897e-07
Iter: 1690 loss: 5.91142225e-07
Iter: 1691 loss: 5.90815262e-07
Iter: 1692 loss: 5.92457582e-07
Iter: 1693 loss: 5.90781099e-07
Iter: 1694 loss: 5.90526156e-07
Iter: 1695 loss: 5.91794731e-07
Iter: 1696 loss: 5.90479772e-07
Iter: 1697 loss: 5.90246486e-07
Iter: 1698 loss: 5.90206923e-07
Iter: 1699 loss: 5.90019e-07
Iter: 1700 loss: 5.89792194e-07
Iter: 1701 loss: 5.89829369e-07
Iter: 1702 loss: 5.89572778e-07
Iter: 1703 loss: 5.89289584e-07
Iter: 1704 loss: 5.92712809e-07
Iter: 1705 loss: 5.89286628e-07
Iter: 1706 loss: 5.88966259e-07
Iter: 1707 loss: 5.89928845e-07
Iter: 1708 loss: 5.88852686e-07
Iter: 1709 loss: 5.88628779e-07
Iter: 1710 loss: 5.88448415e-07
Iter: 1711 loss: 5.88397e-07
Iter: 1712 loss: 5.87960699e-07
Iter: 1713 loss: 5.91444518e-07
Iter: 1714 loss: 5.87921193e-07
Iter: 1715 loss: 5.87730767e-07
Iter: 1716 loss: 5.87273519e-07
Iter: 1717 loss: 5.91382388e-07
Iter: 1718 loss: 5.87197178e-07
Iter: 1719 loss: 5.86751923e-07
Iter: 1720 loss: 5.92018523e-07
Iter: 1721 loss: 5.86741123e-07
Iter: 1722 loss: 5.86517046e-07
Iter: 1723 loss: 5.88509806e-07
Iter: 1724 loss: 5.8650852e-07
Iter: 1725 loss: 5.86243857e-07
Iter: 1726 loss: 5.86154158e-07
Iter: 1727 loss: 5.85995508e-07
Iter: 1728 loss: 5.85716464e-07
Iter: 1729 loss: 5.8826646e-07
Iter: 1730 loss: 5.85697592e-07
Iter: 1731 loss: 5.85448106e-07
Iter: 1732 loss: 5.85525754e-07
Iter: 1733 loss: 5.85286102e-07
Iter: 1734 loss: 5.84934128e-07
Iter: 1735 loss: 5.85382168e-07
Iter: 1736 loss: 5.84743589e-07
Iter: 1737 loss: 5.84374447e-07
Iter: 1738 loss: 5.84291968e-07
Iter: 1739 loss: 5.84070676e-07
Iter: 1740 loss: 5.8399803e-07
Iter: 1741 loss: 5.83872747e-07
Iter: 1742 loss: 5.83686187e-07
Iter: 1743 loss: 5.83579379e-07
Iter: 1744 loss: 5.83505312e-07
Iter: 1745 loss: 5.83292831e-07
Iter: 1746 loss: 5.84341e-07
Iter: 1747 loss: 5.83258156e-07
Iter: 1748 loss: 5.83011399e-07
Iter: 1749 loss: 5.83056078e-07
Iter: 1750 loss: 5.82800851e-07
Iter: 1751 loss: 5.82522944e-07
Iter: 1752 loss: 5.82140046e-07
Iter: 1753 loss: 5.82139933e-07
Iter: 1754 loss: 5.81646532e-07
Iter: 1755 loss: 5.83796407e-07
Iter: 1756 loss: 5.81553252e-07
Iter: 1757 loss: 5.81377662e-07
Iter: 1758 loss: 5.81317522e-07
Iter: 1759 loss: 5.81128688e-07
Iter: 1760 loss: 5.80860331e-07
Iter: 1761 loss: 5.80861297e-07
Iter: 1762 loss: 5.8061903e-07
Iter: 1763 loss: 5.84439476e-07
Iter: 1764 loss: 5.80617041e-07
Iter: 1765 loss: 5.80436051e-07
Iter: 1766 loss: 5.80518417e-07
Iter: 1767 loss: 5.80279732e-07
Iter: 1768 loss: 5.80066853e-07
Iter: 1769 loss: 5.79830839e-07
Iter: 1770 loss: 5.79795312e-07
Iter: 1771 loss: 5.79336074e-07
Iter: 1772 loss: 5.81818881e-07
Iter: 1773 loss: 5.79281959e-07
Iter: 1774 loss: 5.78976255e-07
Iter: 1775 loss: 5.80177471e-07
Iter: 1776 loss: 5.78902814e-07
Iter: 1777 loss: 5.78540039e-07
Iter: 1778 loss: 5.80073333e-07
Iter: 1779 loss: 5.78447271e-07
Iter: 1780 loss: 5.78258e-07
Iter: 1781 loss: 5.78004403e-07
Iter: 1782 loss: 5.77976436e-07
Iter: 1783 loss: 5.77724791e-07
Iter: 1784 loss: 5.7770535e-07
Iter: 1785 loss: 5.77592232e-07
Iter: 1786 loss: 5.77320179e-07
Iter: 1787 loss: 5.80191113e-07
Iter: 1788 loss: 5.77273681e-07
Iter: 1789 loss: 5.76929153e-07
Iter: 1790 loss: 5.79390701e-07
Iter: 1791 loss: 5.76896e-07
Iter: 1792 loss: 5.76637035e-07
Iter: 1793 loss: 5.7930913e-07
Iter: 1794 loss: 5.7662362e-07
Iter: 1795 loss: 5.76474577e-07
Iter: 1796 loss: 5.76071272e-07
Iter: 1797 loss: 5.82487e-07
Iter: 1798 loss: 5.76053537e-07
Iter: 1799 loss: 5.7565984e-07
Iter: 1800 loss: 5.75661034e-07
Iter: 1801 loss: 5.75460717e-07
Iter: 1802 loss: 5.75344245e-07
Iter: 1803 loss: 5.7524926e-07
Iter: 1804 loss: 5.74943556e-07
Iter: 1805 loss: 5.75441e-07
Iter: 1806 loss: 5.7482157e-07
Iter: 1807 loss: 5.74544401e-07
Iter: 1808 loss: 5.76221112e-07
Iter: 1809 loss: 5.74504384e-07
Iter: 1810 loss: 5.74342835e-07
Iter: 1811 loss: 5.76678076e-07
Iter: 1812 loss: 5.74335957e-07
Iter: 1813 loss: 5.74127512e-07
Iter: 1814 loss: 5.73797934e-07
Iter: 1815 loss: 5.73784575e-07
Iter: 1816 loss: 5.73500643e-07
Iter: 1817 loss: 5.74474598e-07
Iter: 1818 loss: 5.73429361e-07
Iter: 1819 loss: 5.73073805e-07
Iter: 1820 loss: 5.75139211e-07
Iter: 1821 loss: 5.73028274e-07
Iter: 1822 loss: 5.72803401e-07
Iter: 1823 loss: 5.72388899e-07
Iter: 1824 loss: 5.80952474e-07
Iter: 1825 loss: 5.72393901e-07
Iter: 1826 loss: 5.72062902e-07
Iter: 1827 loss: 5.72067279e-07
Iter: 1828 loss: 5.71806368e-07
Iter: 1829 loss: 5.72723764e-07
Iter: 1830 loss: 5.71730538e-07
Iter: 1831 loss: 5.71548355e-07
Iter: 1832 loss: 5.71591386e-07
Iter: 1833 loss: 5.71438761e-07
Iter: 1834 loss: 5.71170062e-07
Iter: 1835 loss: 5.72653562e-07
Iter: 1836 loss: 5.71127202e-07
Iter: 1837 loss: 5.70963948e-07
Iter: 1838 loss: 5.70637326e-07
Iter: 1839 loss: 5.77107e-07
Iter: 1840 loss: 5.7064824e-07
Iter: 1841 loss: 5.70264888e-07
Iter: 1842 loss: 5.7250088e-07
Iter: 1843 loss: 5.70201166e-07
Iter: 1844 loss: 5.69829922e-07
Iter: 1845 loss: 5.70311386e-07
Iter: 1846 loss: 5.69628355e-07
Iter: 1847 loss: 5.69393478e-07
Iter: 1848 loss: 5.69372673e-07
Iter: 1849 loss: 5.69159965e-07
Iter: 1850 loss: 5.69022e-07
Iter: 1851 loss: 5.68937708e-07
Iter: 1852 loss: 5.68729433e-07
Iter: 1853 loss: 5.69253189e-07
Iter: 1854 loss: 5.68636267e-07
Iter: 1855 loss: 5.68332723e-07
Iter: 1856 loss: 5.69887106e-07
Iter: 1857 loss: 5.68294809e-07
Iter: 1858 loss: 5.68131384e-07
Iter: 1859 loss: 5.67763777e-07
Iter: 1860 loss: 5.73778493e-07
Iter: 1861 loss: 5.67756445e-07
Iter: 1862 loss: 5.67529185e-07
Iter: 1863 loss: 5.6750406e-07
Iter: 1864 loss: 5.67259349e-07
Iter: 1865 loss: 5.67227175e-07
Iter: 1866 loss: 5.67044424e-07
Iter: 1867 loss: 5.66752874e-07
Iter: 1868 loss: 5.66548692e-07
Iter: 1869 loss: 5.66458198e-07
Iter: 1870 loss: 5.66268341e-07
Iter: 1871 loss: 5.66227811e-07
Iter: 1872 loss: 5.66015274e-07
Iter: 1873 loss: 5.65701271e-07
Iter: 1874 loss: 5.65687e-07
Iter: 1875 loss: 5.65382038e-07
Iter: 1876 loss: 5.6587362e-07
Iter: 1877 loss: 5.65215885e-07
Iter: 1878 loss: 5.649066e-07
Iter: 1879 loss: 5.67416578e-07
Iter: 1880 loss: 5.64912227e-07
Iter: 1881 loss: 5.64654101e-07
Iter: 1882 loss: 5.6606109e-07
Iter: 1883 loss: 5.64613913e-07
Iter: 1884 loss: 5.64384322e-07
Iter: 1885 loss: 5.64269271e-07
Iter: 1886 loss: 5.6412506e-07
Iter: 1887 loss: 5.63852268e-07
Iter: 1888 loss: 5.64221409e-07
Iter: 1889 loss: 5.63705271e-07
Iter: 1890 loss: 5.63470735e-07
Iter: 1891 loss: 5.63468461e-07
Iter: 1892 loss: 5.63292645e-07
Iter: 1893 loss: 5.63061803e-07
Iter: 1894 loss: 5.63043955e-07
Iter: 1895 loss: 5.62783612e-07
Iter: 1896 loss: 5.62652815e-07
Iter: 1897 loss: 5.62525e-07
Iter: 1898 loss: 5.6240583e-07
Iter: 1899 loss: 5.62303455e-07
Iter: 1900 loss: 5.62156856e-07
Iter: 1901 loss: 5.61775e-07
Iter: 1902 loss: 5.65298e-07
Iter: 1903 loss: 5.61732634e-07
Iter: 1904 loss: 5.61372417e-07
Iter: 1905 loss: 5.65507662e-07
Iter: 1906 loss: 5.61352806e-07
Iter: 1907 loss: 5.61023569e-07
Iter: 1908 loss: 5.62045898e-07
Iter: 1909 loss: 5.60899252e-07
Iter: 1910 loss: 5.60693763e-07
Iter: 1911 loss: 5.60366e-07
Iter: 1912 loss: 5.60351054e-07
Iter: 1913 loss: 5.6000863e-07
Iter: 1914 loss: 5.61775778e-07
Iter: 1915 loss: 5.59932801e-07
Iter: 1916 loss: 5.59733905e-07
Iter: 1917 loss: 5.59700482e-07
Iter: 1918 loss: 5.5960129e-07
Iter: 1919 loss: 5.593227e-07
Iter: 1920 loss: 5.61924821e-07
Iter: 1921 loss: 5.59285127e-07
Iter: 1922 loss: 5.58887848e-07
Iter: 1923 loss: 5.60824049e-07
Iter: 1924 loss: 5.58857664e-07
Iter: 1925 loss: 5.58558668e-07
Iter: 1926 loss: 5.61896854e-07
Iter: 1927 loss: 5.58550425e-07
Iter: 1928 loss: 5.58348461e-07
Iter: 1929 loss: 5.58134388e-07
Iter: 1930 loss: 5.58081865e-07
Iter: 1931 loss: 5.57696126e-07
Iter: 1932 loss: 5.57789861e-07
Iter: 1933 loss: 5.5741657e-07
Iter: 1934 loss: 5.57062e-07
Iter: 1935 loss: 5.58173156e-07
Iter: 1936 loss: 5.56948919e-07
Iter: 1937 loss: 5.56787882e-07
Iter: 1938 loss: 5.56747295e-07
Iter: 1939 loss: 5.56577561e-07
Iter: 1940 loss: 5.56271743e-07
Iter: 1941 loss: 5.56268787e-07
Iter: 1942 loss: 5.5598656e-07
Iter: 1943 loss: 5.57393207e-07
Iter: 1944 loss: 5.55951715e-07
Iter: 1945 loss: 5.55608153e-07
Iter: 1946 loss: 5.56406235e-07
Iter: 1947 loss: 5.5547514e-07
Iter: 1948 loss: 5.55249528e-07
Iter: 1949 loss: 5.55191491e-07
Iter: 1950 loss: 5.55047905e-07
Iter: 1951 loss: 5.54900794e-07
Iter: 1952 loss: 5.54851511e-07
Iter: 1953 loss: 5.54707242e-07
Iter: 1954 loss: 5.54353733e-07
Iter: 1955 loss: 5.57930434e-07
Iter: 1956 loss: 5.54293479e-07
Iter: 1957 loss: 5.53968334e-07
Iter: 1958 loss: 5.55653855e-07
Iter: 1959 loss: 5.53914617e-07
Iter: 1960 loss: 5.53730331e-07
Iter: 1961 loss: 5.53721236e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6
+ date
Wed Oct 21 11:12:28 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee813df158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81429f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81429268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812f4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812ef598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812ef840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812796a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81215598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81215ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81297598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee8129d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d7c1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d7c1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee811b9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee811b9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d77c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d77c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d6fd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d6c5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d6aed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee811d99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d626ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d656620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d761268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d761a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d5a7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d58a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d59b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d52a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d552ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d60a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4ed6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4ed730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4f0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4da8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d673268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.025194433
test_loss: 0.023110636
train_loss: 0.010310841
test_loss: 0.010788235
train_loss: 0.0063897427
test_loss: 0.0069764024
train_loss: 0.0051481547
test_loss: 0.0055416953
train_loss: 0.004544401
test_loss: 0.004927233
train_loss: 0.0041004224
test_loss: 0.0046658306
train_loss: 0.004025884
test_loss: 0.0046371454
train_loss: 0.0040539065
test_loss: 0.0048913504
train_loss: 0.0037005583
test_loss: 0.004320377
train_loss: 0.0038519832
test_loss: 0.004373812
train_loss: 0.0038969778
test_loss: 0.004351088
train_loss: 0.0034851735
test_loss: 0.0041042506
train_loss: 0.0036536297
test_loss: 0.003953703
train_loss: 0.0032386193
test_loss: 0.004076515
train_loss: 0.0036976095
test_loss: 0.0042083035
train_loss: 0.003200599
test_loss: 0.0038717333
train_loss: 0.003669896
test_loss: 0.0039634537
train_loss: 0.0036293578
test_loss: 0.0041042273
train_loss: 0.0035273824
test_loss: 0.00402706
train_loss: 0.0034000925
test_loss: 0.0039601573
train_loss: 0.0032257442
test_loss: 0.0039587985
train_loss: 0.0031617791
test_loss: 0.0041256854
train_loss: 0.0032776278
test_loss: 0.004033732
train_loss: 0.003489577
test_loss: 0.0039704284
train_loss: 0.0032581529
test_loss: 0.0038812568
train_loss: 0.003302399
test_loss: 0.0038640324
train_loss: 0.0031706307
test_loss: 0.0038777892
train_loss: 0.0032950114
test_loss: 0.003842349
train_loss: 0.0035696058
test_loss: 0.003902768
train_loss: 0.0030589092
test_loss: 0.003805045
train_loss: 0.0032939606
test_loss: 0.003905695
train_loss: 0.0032471388
test_loss: 0.0039141746
train_loss: 0.0032466738
test_loss: 0.003913614
train_loss: 0.0033660373
test_loss: 0.0039042698
train_loss: 0.0031094453
test_loss: 0.003895641
train_loss: 0.0033132443
test_loss: 0.0039754156
train_loss: 0.0030440064
test_loss: 0.0038598785
train_loss: 0.003345397
test_loss: 0.0037141573
train_loss: 0.0031742766
test_loss: 0.0037122394
train_loss: 0.002975363
test_loss: 0.0038592075
train_loss: 0.0033824132
test_loss: 0.0038594436
train_loss: 0.0031651778
test_loss: 0.003784422
train_loss: 0.0029288712
test_loss: 0.0037089777
train_loss: 0.0028629424
test_loss: 0.003744931
train_loss: 0.0030417724
test_loss: 0.0038431708
train_loss: 0.0032339122
test_loss: 0.00375414
train_loss: 0.0032499414
test_loss: 0.0038089429
train_loss: 0.0033793258
test_loss: 0.0040466976
train_loss: 0.0033395006
test_loss: 0.0042521097
train_loss: 0.0033296042
test_loss: 0.003989337
train_loss: 0.0030479324
test_loss: 0.0037267958
train_loss: 0.003040888
test_loss: 0.0038123918
train_loss: 0.002938806
test_loss: 0.00356653
train_loss: 0.0032026
test_loss: 0.0036538597
train_loss: 0.0030390634
test_loss: 0.003685207
train_loss: 0.0032275766
test_loss: 0.0037612417
train_loss: 0.0029279138
test_loss: 0.0037155706
train_loss: 0.0031224648
test_loss: 0.0037791994
train_loss: 0.0031286594
test_loss: 0.0038725052
train_loss: 0.0031681855
test_loss: 0.0037410152
train_loss: 0.0031848645
test_loss: 0.0038372234
train_loss: 0.00307099
test_loss: 0.0036107765
train_loss: 0.002901545
test_loss: 0.003613769
train_loss: 0.0029660857
test_loss: 0.0035885198
train_loss: 0.0028820797
test_loss: 0.0038036003
train_loss: 0.0030736523
test_loss: 0.0035439965
train_loss: 0.0030246992
test_loss: 0.0037498293
train_loss: 0.0032962984
test_loss: 0.004160189
train_loss: 0.003458164
test_loss: 0.0039314656
train_loss: 0.0030575683
test_loss: 0.0037646634
train_loss: 0.003350905
test_loss: 0.0037582116
train_loss: 0.0031277672
test_loss: 0.0036485742
train_loss: 0.002863029
test_loss: 0.0036082736
train_loss: 0.0029652487
test_loss: 0.0036596362
train_loss: 0.0030650645
test_loss: 0.0036598444
train_loss: 0.0029050077
test_loss: 0.0038145753
train_loss: 0.003032151
test_loss: 0.0036894337
train_loss: 0.002973441
test_loss: 0.0036701972
train_loss: 0.002881941
test_loss: 0.0036679185
train_loss: 0.003000666
test_loss: 0.003624764
train_loss: 0.0033266996
test_loss: 0.0036752517
train_loss: 0.0029354435
test_loss: 0.0035241474
train_loss: 0.0028345035
test_loss: 0.0036741
train_loss: 0.0030226128
test_loss: 0.0036759083
train_loss: 0.0028988435
test_loss: 0.0038330383
train_loss: 0.0031913763
test_loss: 0.0036865815
train_loss: 0.003189894
test_loss: 0.003715343
train_loss: 0.0027654222
test_loss: 0.0035954476
train_loss: 0.003150758
test_loss: 0.0036764215
train_loss: 0.0030954543
test_loss: 0.0037503575
train_loss: 0.003275795
test_loss: 0.003607842
train_loss: 0.0035222836
test_loss: 0.0039272304
train_loss: 0.0033486062
test_loss: 0.0039387015
train_loss: 0.0029770704
test_loss: 0.0035907377
train_loss: 0.0030225338
test_loss: 0.0036193426
train_loss: 0.0031252555
test_loss: 0.0037949462
train_loss: 0.002927986
test_loss: 0.003545851
train_loss: 0.0031897714
test_loss: 0.0036758287
train_loss: 0.00304558
test_loss: 0.0035952167
train_loss: 0.0028907005
test_loss: 0.0035703809
train_loss: 0.0028551607
test_loss: 0.003632332
train_loss: 0.0028625177
test_loss: 0.0037777887
train_loss: 0.003197608
test_loss: 0.0036930216
train_loss: 0.003063746
test_loss: 0.0036270597
train_loss: 0.0029527394
test_loss: 0.003598627
train_loss: 0.0030734306
test_loss: 0.0035080684
train_loss: 0.0031148943
test_loss: 0.0036569056
train_loss: 0.0031162426
test_loss: 0.0036965352
train_loss: 0.002793735
test_loss: 0.0035429797
train_loss: 0.0030805566
test_loss: 0.0035965508
train_loss: 0.0029374138
test_loss: 0.0035569181
train_loss: 0.0028767865
test_loss: 0.0035153716
train_loss: 0.0027027573
test_loss: 0.003510368
train_loss: 0.0029938358
test_loss: 0.0036478892
train_loss: 0.002877255
test_loss: 0.0035644085
train_loss: 0.0029315865
test_loss: 0.0035883836
train_loss: 0.003148341
test_loss: 0.0036027245
train_loss: 0.0028622858
test_loss: 0.0037333136
train_loss: 0.002933751
test_loss: 0.003489398
train_loss: 0.0028089767
test_loss: 0.003548242
train_loss: 0.003005623
test_loss: 0.003625554
train_loss: 0.0028554625
test_loss: 0.0035614474
train_loss: 0.0030609185
test_loss: 0.0037922242
train_loss: 0.002963445
test_loss: 0.0034892834
train_loss: 0.0030901483
test_loss: 0.0036154513
train_loss: 0.0031753043
test_loss: 0.0037633495
train_loss: 0.0033824865
test_loss: 0.0036542956
train_loss: 0.0033060913
test_loss: 0.003656802
train_loss: 0.0032626255
test_loss: 0.0036742769
train_loss: 0.002852629
test_loss: 0.0037284396
train_loss: 0.0032796473
test_loss: 0.0036807293
train_loss: 0.00271149
test_loss: 0.003553167
train_loss: 0.0026753661
test_loss: 0.00345967
train_loss: 0.0026685856
test_loss: 0.0035688998
train_loss: 0.0031835968
test_loss: 0.0036233529
train_loss: 0.0029525298
test_loss: 0.0036478597
train_loss: 0.0029213473
test_loss: 0.0037102895
train_loss: 0.002891866
test_loss: 0.003683831
train_loss: 0.0029333204
test_loss: 0.0035722926
train_loss: 0.0029431519
test_loss: 0.0038019489
train_loss: 0.003229297
test_loss: 0.003790273
train_loss: 0.0029963767
test_loss: 0.0035170175
train_loss: 0.0030789382
test_loss: 0.0035909554
train_loss: 0.00312832
test_loss: 0.0034677628
train_loss: 0.0031166554
test_loss: 0.0036411923
train_loss: 0.0029026808
test_loss: 0.0036306987
train_loss: 0.0029749214
test_loss: 0.0036327266
train_loss: 0.0028737045
test_loss: 0.0038808207
train_loss: 0.0028568497
test_loss: 0.0035378635
train_loss: 0.002744075
test_loss: 0.0035089094
train_loss: 0.0029269224
test_loss: 0.0035761988
train_loss: 0.0031937184
test_loss: 0.0036028004
train_loss: 0.003247986
test_loss: 0.0036943448
train_loss: 0.0027181203
test_loss: 0.0036904525
train_loss: 0.0027926848
test_loss: 0.003474862
train_loss: 0.0026403589
test_loss: 0.0035099678
train_loss: 0.0030719098
test_loss: 0.003603512
train_loss: 0.0027578715
test_loss: 0.0035831982
train_loss: 0.0029677262
test_loss: 0.0035835824
train_loss: 0.0028936062
test_loss: 0.0035495725
train_loss: 0.0028784804
test_loss: 0.0036186944
train_loss: 0.002779987
test_loss: 0.0035799758
train_loss: 0.0031392942
test_loss: 0.0036591557
train_loss: 0.0028588283
test_loss: 0.003376408
train_loss: 0.002852135
test_loss: 0.0035133874
train_loss: 0.0027411848
test_loss: 0.003571226
train_loss: 0.0028520087
test_loss: 0.0035053839
train_loss: 0.0029995476
test_loss: 0.0035636427
train_loss: 0.002719915
test_loss: 0.003424328
train_loss: 0.0029842432
test_loss: 0.0036014493
train_loss: 0.0026673824
test_loss: 0.0036272835
train_loss: 0.0029081032
test_loss: 0.0035063066
train_loss: 0.0027544403
test_loss: 0.0034714208
train_loss: 0.0026343542
test_loss: 0.0035003081
train_loss: 0.0028731618
test_loss: 0.0033882926
train_loss: 0.0027693326
test_loss: 0.0035169336
train_loss: 0.0028663522
test_loss: 0.0034112083
train_loss: 0.002747799
test_loss: 0.003552817
train_loss: 0.002837382
test_loss: 0.0034938997
train_loss: 0.0029429472
test_loss: 0.0035976171
train_loss: 0.0027898212
test_loss: 0.0034525385
train_loss: 0.0027208833
test_loss: 0.0034761978
train_loss: 0.002852776
test_loss: 0.003529151
train_loss: 0.002935764
test_loss: 0.0036161134
train_loss: 0.0027843132
test_loss: 0.0034322976
train_loss: 0.0027609617
test_loss: 0.0035088062
train_loss: 0.0026829098
test_loss: 0.0036042384
train_loss: 0.0032645655
test_loss: 0.0035259088
train_loss: 0.0029549436
test_loss: 0.0036564092
train_loss: 0.0029499857
test_loss: 0.003631931
train_loss: 0.002750425
test_loss: 0.003483084
train_loss: 0.0028624614
test_loss: 0.0036002004
train_loss: 0.0030468656
test_loss: 0.0036573964
train_loss: 0.0028833176
test_loss: 0.0034793476
train_loss: 0.0028541868
test_loss: 0.0034817238
train_loss: 0.0028768494
test_loss: 0.0036347879
train_loss: 0.0031572252
test_loss: 0.0037191287
train_loss: 0.002836349
test_loss: 0.0036435435
train_loss: 0.0029057432
test_loss: 0.003509931
train_loss: 0.0029617993
test_loss: 0.0035664958
train_loss: 0.0026869
test_loss: 0.0034349007
train_loss: 0.002738635
test_loss: 0.0035018462
train_loss: 0.002650505
test_loss: 0.003415637
train_loss: 0.002967069
test_loss: 0.0037233087
train_loss: 0.0029728513
test_loss: 0.0036340826
train_loss: 0.0029142955
test_loss: 0.003645205
train_loss: 0.0026007365
test_loss: 0.0035181392
train_loss: 0.0027762263
test_loss: 0.003365362
train_loss: 0.0025828886
test_loss: 0.0034622103
train_loss: 0.0026776171
test_loss: 0.003530883
train_loss: 0.0028911894
test_loss: 0.003568275
train_loss: 0.0031026264
test_loss: 0.0036909545
train_loss: 0.0033109118
test_loss: 0.0036241328
train_loss: 0.0029827282
test_loss: 0.0038153597
train_loss: 0.0027518698
test_loss: 0.0035452298
train_loss: 0.0026707705
test_loss: 0.0034615532
train_loss: 0.0027483148
test_loss: 0.0034717747
train_loss: 0.0029308568
test_loss: 0.003680856
train_loss: 0.003044969
test_loss: 0.0034636213
train_loss: 0.002953954
test_loss: 0.0037542987
train_loss: 0.003003291
test_loss: 0.003823113
train_loss: 0.0031732689
test_loss: 0.003720542
train_loss: 0.003035988
test_loss: 0.0036456601
train_loss: 0.0027841162
test_loss: 0.0038518365
train_loss: 0.0030715931
test_loss: 0.0036400694
train_loss: 0.0032187318
test_loss: 0.0037891697
train_loss: 0.0028598248
test_loss: 0.003537194
train_loss: 0.0027437173
test_loss: 0.0033845971
train_loss: 0.0027679687
test_loss: 0.0036416869
train_loss: 0.003135328
test_loss: 0.0040364508
train_loss: 0.0027697433
test_loss: 0.0035608374
train_loss: 0.0030193096
test_loss: 0.003545773
train_loss: 0.002925136
test_loss: 0.0035090137
train_loss: 0.0028583899
test_loss: 0.003519357
train_loss: 0.0029421297
test_loss: 0.0035916783
train_loss: 0.002747239
test_loss: 0.003560001
train_loss: 0.0027080122
test_loss: 0.0033873178
train_loss: 0.0027677165
test_loss: 0.0034754013
train_loss: 0.0031422921
test_loss: 0.0039772647
train_loss: 0.0028983592
test_loss: 0.0037232114
train_loss: 0.0028942048
test_loss: 0.003769643
train_loss: 0.0027787765
test_loss: 0.0035911496
train_loss: 0.0029230262
test_loss: 0.003793312
train_loss: 0.0027649752
test_loss: 0.0036232912
train_loss: 0.0028759588
test_loss: 0.0035029189
train_loss: 0.0026381866
test_loss: 0.0034675244
train_loss: 0.0026246926
test_loss: 0.0034703454
train_loss: 0.0027601318
test_loss: 0.00341854
train_loss: 0.0030260868
test_loss: 0.0035969706
train_loss: 0.0028003943
test_loss: 0.0035250217
train_loss: 0.0027795385
test_loss: 0.0035082148
train_loss: 0.0027664346
test_loss: 0.0034375447
train_loss: 0.0028067513
test_loss: 0.003605116
train_loss: 0.0029763903
test_loss: 0.003496069
train_loss: 0.0026995775
test_loss: 0.0033905134
train_loss: 0.002756687
test_loss: 0.0033996417
train_loss: 0.0026535066
test_loss: 0.0034755028
train_loss: 0.0029052568
test_loss: 0.0034978641
train_loss: 0.0028094107
test_loss: 0.003555657
train_loss: 0.0030502258
test_loss: 0.00364097
train_loss: 0.0028081965
test_loss: 0.0036562171
train_loss: 0.0028560408
test_loss: 0.0035784517
train_loss: 0.0029341998
test_loss: 0.0034889274
train_loss: 0.0027865516
test_loss: 0.0034714502
train_loss: 0.002664049
test_loss: 0.0033623676
train_loss: 0.0028058197
test_loss: 0.0034668718
train_loss: 0.0028034814
test_loss: 0.003758159
train_loss: 0.00271322
test_loss: 0.0035413338
train_loss: 0.002708406
test_loss: 0.0035605552
train_loss: 0.0028300916
test_loss: 0.0035655801
train_loss: 0.002685506
test_loss: 0.0035234015
train_loss: 0.0030162027
test_loss: 0.003438278
train_loss: 0.0029662475
test_loss: 0.003619555
train_loss: 0.0027148128
test_loss: 0.0035323696
train_loss: 0.0026829836
test_loss: 0.0034789871
train_loss: 0.002907959
test_loss: 0.0038307533
train_loss: 0.0027909388
test_loss: 0.0035472647
train_loss: 0.0028285459
test_loss: 0.0035651668
train_loss: 0.002766843
test_loss: 0.0036169433
train_loss: 0.0028579938
test_loss: 0.003465784
train_loss: 0.0028299459
test_loss: 0.0035696423
train_loss: 0.0030124635
test_loss: 0.0035184496
train_loss: 0.0027219562
test_loss: 0.0033826723
train_loss: 0.002661201
test_loss: 0.0033943895
train_loss: 0.0026849462
test_loss: 0.0036153747
train_loss: 0.002888586
test_loss: 0.0035569558
train_loss: 0.0030185392
test_loss: 0.003604468
train_loss: 0.0027685617
test_loss: 0.0036190783
train_loss: 0.002805593
test_loss: 0.00341782
train_loss: 0.0025422175
test_loss: 0.003394879
train_loss: 0.002932062
test_loss: 0.0036453574
train_loss: 0.0028611978
test_loss: 0.003536034
train_loss: 0.0028123066
test_loss: 0.0034506128
train_loss: 0.00291736
test_loss: 0.0035616788
train_loss: 0.00291675
test_loss: 0.0036218492
train_loss: 0.0028547787
test_loss: 0.0037921644
train_loss: 0.0027452265
test_loss: 0.0035464007
train_loss: 0.0027861726
test_loss: 0.0036519743
train_loss: 0.002749468
test_loss: 0.0034882275
train_loss: 0.0027963126
test_loss: 0.003562525
train_loss: 0.0028435031
test_loss: 0.003572203
train_loss: 0.002677355
test_loss: 0.0035496447
train_loss: 0.0027444197
test_loss: 0.0034572629
train_loss: 0.0027560666
test_loss: 0.003414849
train_loss: 0.0027662767
test_loss: 0.003385235
train_loss: 0.002556981
test_loss: 0.003335892
train_loss: 0.002625689
test_loss: 0.0033425698
train_loss: 0.002791469
test_loss: 0.0034735631
train_loss: 0.0028822634
test_loss: 0.0034684893
train_loss: 0.0026253588
test_loss: 0.0033473806
train_loss: 0.0025504394
test_loss: 0.003384721
train_loss: 0.0028322095
test_loss: 0.0034621053
train_loss: 0.0027670169
test_loss: 0.0034692176
train_loss: 0.0027754628
test_loss: 0.0034996413
train_loss: 0.002820869
test_loss: 0.0035934863
train_loss: 0.0030362855
test_loss: 0.003546401
train_loss: 0.0030920883
test_loss: 0.003591036
train_loss: 0.0027525814
test_loss: 0.0035640462
train_loss: 0.0027084043
test_loss: 0.0034442917
train_loss: 0.002838762
test_loss: 0.0035925875
train_loss: 0.0026209902
test_loss: 0.0035216324
train_loss: 0.0027898934
test_loss: 0.0034359181
train_loss: 0.0029634715
test_loss: 0.003556626
train_loss: 0.0027387897
test_loss: 0.0035732682
train_loss: 0.002908708
test_loss: 0.0036236658
train_loss: 0.0030117137
test_loss: 0.0035683836
train_loss: 0.0025604859
test_loss: 0.003514366
train_loss: 0.002510483
test_loss: 0.0033008154
train_loss: 0.002830075
test_loss: 0.003543705
train_loss: 0.0029278002
test_loss: 0.0036412177
train_loss: 0.0028854022
test_loss: 0.0035969247
train_loss: 0.002672387
test_loss: 0.0035165
train_loss: 0.0026935318
test_loss: 0.0034021817
train_loss: 0.0028068204
test_loss: 0.0034845236
train_loss: 0.0026685237
test_loss: 0.0034821932
train_loss: 0.0028340076
test_loss: 0.0034771326
train_loss: 0.0026680012
test_loss: 0.0033820367
train_loss: 0.0027026166
test_loss: 0.0033927374
train_loss: 0.002621938
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.0033718268
train_loss: 0.0028831642
test_loss: 0.0036163565
train_loss: 0.0028631624
test_loss: 0.003777802
train_loss: 0.002583895
test_loss: 0.0033533284
train_loss: 0.0027490614
test_loss: 0.003468036
train_loss: 0.0027345512
test_loss: 0.003370081
train_loss: 0.002703107
test_loss: 0.0034602731
train_loss: 0.0026799769
test_loss: 0.0033333814
train_loss: 0.0025981641
test_loss: 0.0035516394
train_loss: 0.0029105826
test_loss: 0.0035868532
train_loss: 0.002624886
test_loss: 0.003348778
train_loss: 0.0027439469
test_loss: 0.0035794894
train_loss: 0.0029758476
test_loss: 0.0036566532
train_loss: 0.002848931
test_loss: 0.0035978635
train_loss: 0.0026759403
test_loss: 0.0034384218
train_loss: 0.0028853333
test_loss: 0.0035702263
train_loss: 0.0028339908
test_loss: 0.0034181934
train_loss: 0.0029688873
test_loss: 0.0035700086
train_loss: 0.0028755302
test_loss: 0.003391671
train_loss: 0.0026547275
test_loss: 0.0035549547
train_loss: 0.0026906854
test_loss: 0.0034214174
train_loss: 0.0026764479
test_loss: 0.0035095455
train_loss: 0.0027557183
test_loss: 0.0035445194
train_loss: 0.0025431612
test_loss: 0.003316021
train_loss: 0.0025826297
test_loss: 0.0035065247
train_loss: 0.0027827797
test_loss: 0.0034740416
train_loss: 0.0029307697
test_loss: 0.0034419857
train_loss: 0.0026256875
test_loss: 0.0033539638
train_loss: 0.0025865885
test_loss: 0.00334915
train_loss: 0.0026383854
test_loss: 0.0034623016
train_loss: 0.0026779487
test_loss: 0.0033359984
train_loss: 0.0027340276
test_loss: 0.003374681
train_loss: 0.0026981414
test_loss: 0.0034982287
train_loss: 0.0026017786
test_loss: 0.003395537
train_loss: 0.0026805154
test_loss: 0.0036029115
train_loss: 0.0029659495
test_loss: 0.0036738839
train_loss: 0.0029576651
test_loss: 0.0035396868
train_loss: 0.0027964418
test_loss: 0.0034693992
train_loss: 0.0030361924
test_loss: 0.003419796
train_loss: 0.002599189
test_loss: 0.0036259864
train_loss: 0.0026632852
test_loss: 0.003484903
train_loss: 0.0027521858
test_loss: 0.0035516552
train_loss: 0.0030925511
test_loss: 0.0034198042
train_loss: 0.0030966988
test_loss: 0.0037768583
train_loss: 0.0029135533
test_loss: 0.0035740172
train_loss: 0.0029283806
test_loss: 0.0036423337
train_loss: 0.0026666597
test_loss: 0.003600704
train_loss: 0.002763742
test_loss: 0.0034406537
train_loss: 0.0026708941
test_loss: 0.00335256
train_loss: 0.0027775895
test_loss: 0.0033719984
train_loss: 0.0027703005
test_loss: 0.003506965
train_loss: 0.002602586
test_loss: 0.0035044774
train_loss: 0.0025832744
test_loss: 0.003363342
train_loss: 0.0028076593
test_loss: 0.0036327387
train_loss: 0.0028053906
test_loss: 0.0034676034
train_loss: 0.0026996483
test_loss: 0.0035479185
train_loss: 0.0026676415
test_loss: 0.003548929
train_loss: 0.0026622093
test_loss: 0.0034754404
train_loss: 0.0027137618
test_loss: 0.003487335
train_loss: 0.0025696382
test_loss: 0.0034455708
train_loss: 0.00280922
test_loss: 0.003439209
train_loss: 0.0026961837
test_loss: 0.0035228934
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e95f32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e95f3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e965c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e9562b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e94ec1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e95017b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e94b29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e947e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e947e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e947e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e9476950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e978d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e940c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93d4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93d4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93a1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93a0400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e93359d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92e7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e9310f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92b4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92b4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d48906a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92ba158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7e92b4400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d4859488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d4806598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d482a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d47b7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d47e0d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d477d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d477da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d477dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d47547b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d477d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7d46c5048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.32208734e-05
Iter: 2 loss: 1.09611901e-05
Iter: 3 loss: 1.07958094e-05
Iter: 4 loss: 9.96090785e-06
Iter: 5 loss: 1.14697614e-05
Iter: 6 loss: 9.59993395e-06
Iter: 7 loss: 8.78153332e-06
Iter: 8 loss: 1.47096916e-05
Iter: 9 loss: 8.71147586e-06
Iter: 10 loss: 8.30887802e-06
Iter: 11 loss: 8.07907691e-06
Iter: 12 loss: 7.90594277e-06
Iter: 13 loss: 7.35863296e-06
Iter: 14 loss: 8.42571444e-06
Iter: 15 loss: 7.13198961e-06
Iter: 16 loss: 6.63300398e-06
Iter: 17 loss: 1.38318337e-05
Iter: 18 loss: 6.63202536e-06
Iter: 19 loss: 6.37626772e-06
Iter: 20 loss: 6.2019908e-06
Iter: 21 loss: 6.10775623e-06
Iter: 22 loss: 5.68936048e-06
Iter: 23 loss: 6.19517732e-06
Iter: 24 loss: 5.46993488e-06
Iter: 25 loss: 5.36160223e-06
Iter: 26 loss: 5.30999023e-06
Iter: 27 loss: 5.16716545e-06
Iter: 28 loss: 4.89863623e-06
Iter: 29 loss: 1.08347658e-05
Iter: 30 loss: 4.89791455e-06
Iter: 31 loss: 4.61728587e-06
Iter: 32 loss: 5.67663346e-06
Iter: 33 loss: 4.54993415e-06
Iter: 34 loss: 4.32851175e-06
Iter: 35 loss: 4.29438569e-06
Iter: 36 loss: 4.14077158e-06
Iter: 37 loss: 3.84357872e-06
Iter: 38 loss: 5.31209844e-06
Iter: 39 loss: 3.79297626e-06
Iter: 40 loss: 3.57782915e-06
Iter: 41 loss: 5.10667496e-06
Iter: 42 loss: 3.55861289e-06
Iter: 43 loss: 3.39978556e-06
Iter: 44 loss: 5.11470353e-06
Iter: 45 loss: 3.39612e-06
Iter: 46 loss: 3.28133092e-06
Iter: 47 loss: 3.41964369e-06
Iter: 48 loss: 3.22107576e-06
Iter: 49 loss: 3.12973043e-06
Iter: 50 loss: 3.31936553e-06
Iter: 51 loss: 3.093337e-06
Iter: 52 loss: 3.00165675e-06
Iter: 53 loss: 4.24167501e-06
Iter: 54 loss: 3.00118541e-06
Iter: 55 loss: 2.94764322e-06
Iter: 56 loss: 2.96327e-06
Iter: 57 loss: 2.90899561e-06
Iter: 58 loss: 2.82303699e-06
Iter: 59 loss: 3.22491223e-06
Iter: 60 loss: 2.80706445e-06
Iter: 61 loss: 2.75553703e-06
Iter: 62 loss: 2.74355398e-06
Iter: 63 loss: 2.71044064e-06
Iter: 64 loss: 2.6327757e-06
Iter: 65 loss: 2.6175926e-06
Iter: 66 loss: 2.56590556e-06
Iter: 67 loss: 2.51287156e-06
Iter: 68 loss: 2.5035838e-06
Iter: 69 loss: 2.46320838e-06
Iter: 70 loss: 2.4090964e-06
Iter: 71 loss: 2.40621898e-06
Iter: 72 loss: 2.3430855e-06
Iter: 73 loss: 2.47880325e-06
Iter: 74 loss: 2.31860759e-06
Iter: 75 loss: 2.25322765e-06
Iter: 76 loss: 2.26344832e-06
Iter: 77 loss: 2.20386642e-06
Iter: 78 loss: 2.15033378e-06
Iter: 79 loss: 2.70899773e-06
Iter: 80 loss: 2.14893566e-06
Iter: 81 loss: 2.10100779e-06
Iter: 82 loss: 2.27160353e-06
Iter: 83 loss: 2.08869596e-06
Iter: 84 loss: 2.05159427e-06
Iter: 85 loss: 2.55137593e-06
Iter: 86 loss: 2.05139213e-06
Iter: 87 loss: 2.02834235e-06
Iter: 88 loss: 1.99461761e-06
Iter: 89 loss: 1.99366195e-06
Iter: 90 loss: 1.97211716e-06
Iter: 91 loss: 1.9698773e-06
Iter: 92 loss: 1.94647714e-06
Iter: 93 loss: 1.9267577e-06
Iter: 94 loss: 1.9203394e-06
Iter: 95 loss: 1.89493971e-06
Iter: 96 loss: 1.89478521e-06
Iter: 97 loss: 1.8747462e-06
Iter: 98 loss: 1.85834074e-06
Iter: 99 loss: 1.85248359e-06
Iter: 100 loss: 1.82979193e-06
Iter: 101 loss: 1.81854557e-06
Iter: 102 loss: 1.80761344e-06
Iter: 103 loss: 1.77651145e-06
Iter: 104 loss: 2.12115197e-06
Iter: 105 loss: 1.77588072e-06
Iter: 106 loss: 1.76005983e-06
Iter: 107 loss: 1.92702055e-06
Iter: 108 loss: 1.75967295e-06
Iter: 109 loss: 1.74611569e-06
Iter: 110 loss: 1.71861336e-06
Iter: 111 loss: 2.22029303e-06
Iter: 112 loss: 1.71811735e-06
Iter: 113 loss: 1.68880024e-06
Iter: 114 loss: 1.84478517e-06
Iter: 115 loss: 1.68422889e-06
Iter: 116 loss: 1.66238624e-06
Iter: 117 loss: 1.65788867e-06
Iter: 118 loss: 1.64346829e-06
Iter: 119 loss: 1.62506285e-06
Iter: 120 loss: 1.62468314e-06
Iter: 121 loss: 1.606821e-06
Iter: 122 loss: 1.6522331e-06
Iter: 123 loss: 1.60067873e-06
Iter: 124 loss: 1.58914645e-06
Iter: 125 loss: 1.59261378e-06
Iter: 126 loss: 1.58087641e-06
Iter: 127 loss: 1.56063413e-06
Iter: 128 loss: 1.69223404e-06
Iter: 129 loss: 1.55847681e-06
Iter: 130 loss: 1.54799079e-06
Iter: 131 loss: 1.58904299e-06
Iter: 132 loss: 1.54559791e-06
Iter: 133 loss: 1.53250619e-06
Iter: 134 loss: 1.52899702e-06
Iter: 135 loss: 1.52086261e-06
Iter: 136 loss: 1.5078034e-06
Iter: 137 loss: 1.49593029e-06
Iter: 138 loss: 1.49262269e-06
Iter: 139 loss: 1.47207902e-06
Iter: 140 loss: 1.60128684e-06
Iter: 141 loss: 1.46975503e-06
Iter: 142 loss: 1.45778199e-06
Iter: 143 loss: 1.45775709e-06
Iter: 144 loss: 1.448376e-06
Iter: 145 loss: 1.43408602e-06
Iter: 146 loss: 1.43386239e-06
Iter: 147 loss: 1.41958162e-06
Iter: 148 loss: 1.51175141e-06
Iter: 149 loss: 1.41798296e-06
Iter: 150 loss: 1.40533689e-06
Iter: 151 loss: 1.40044506e-06
Iter: 152 loss: 1.39356291e-06
Iter: 153 loss: 1.38081089e-06
Iter: 154 loss: 1.46525917e-06
Iter: 155 loss: 1.37955419e-06
Iter: 156 loss: 1.36930669e-06
Iter: 157 loss: 1.47549326e-06
Iter: 158 loss: 1.36900735e-06
Iter: 159 loss: 1.36108338e-06
Iter: 160 loss: 1.37655479e-06
Iter: 161 loss: 1.35785149e-06
Iter: 162 loss: 1.35268738e-06
Iter: 163 loss: 1.41098349e-06
Iter: 164 loss: 1.35261234e-06
Iter: 165 loss: 1.34695551e-06
Iter: 166 loss: 1.33899789e-06
Iter: 167 loss: 1.33866945e-06
Iter: 168 loss: 1.33185404e-06
Iter: 169 loss: 1.33178696e-06
Iter: 170 loss: 1.32752552e-06
Iter: 171 loss: 1.31566446e-06
Iter: 172 loss: 1.3813858e-06
Iter: 173 loss: 1.31219213e-06
Iter: 174 loss: 1.29876821e-06
Iter: 175 loss: 1.42010686e-06
Iter: 176 loss: 1.29809223e-06
Iter: 177 loss: 1.29049909e-06
Iter: 178 loss: 1.34012078e-06
Iter: 179 loss: 1.28963268e-06
Iter: 180 loss: 1.28088936e-06
Iter: 181 loss: 1.30044839e-06
Iter: 182 loss: 1.27751605e-06
Iter: 183 loss: 1.27233579e-06
Iter: 184 loss: 1.27208978e-06
Iter: 185 loss: 1.26810028e-06
Iter: 186 loss: 1.25966949e-06
Iter: 187 loss: 1.2618566e-06
Iter: 188 loss: 1.2534781e-06
Iter: 189 loss: 1.24698977e-06
Iter: 190 loss: 1.24693111e-06
Iter: 191 loss: 1.24145913e-06
Iter: 192 loss: 1.23995483e-06
Iter: 193 loss: 1.23664631e-06
Iter: 194 loss: 1.22767869e-06
Iter: 195 loss: 1.30580315e-06
Iter: 196 loss: 1.22718484e-06
Iter: 197 loss: 1.22282e-06
Iter: 198 loss: 1.25000247e-06
Iter: 199 loss: 1.22232063e-06
Iter: 200 loss: 1.21747939e-06
Iter: 201 loss: 1.21666335e-06
Iter: 202 loss: 1.21340622e-06
Iter: 203 loss: 1.20841514e-06
Iter: 204 loss: 1.24588951e-06
Iter: 205 loss: 1.20801747e-06
Iter: 206 loss: 1.20335949e-06
Iter: 207 loss: 1.1990852e-06
Iter: 208 loss: 1.1979987e-06
Iter: 209 loss: 1.1923654e-06
Iter: 210 loss: 1.19620336e-06
Iter: 211 loss: 1.18884896e-06
Iter: 212 loss: 1.18207265e-06
Iter: 213 loss: 1.20570348e-06
Iter: 214 loss: 1.18034495e-06
Iter: 215 loss: 1.17336606e-06
Iter: 216 loss: 1.24799487e-06
Iter: 217 loss: 1.17322134e-06
Iter: 218 loss: 1.16907654e-06
Iter: 219 loss: 1.16276942e-06
Iter: 220 loss: 1.16264562e-06
Iter: 221 loss: 1.15549506e-06
Iter: 222 loss: 1.1650875e-06
Iter: 223 loss: 1.15194348e-06
Iter: 224 loss: 1.14316822e-06
Iter: 225 loss: 1.19189076e-06
Iter: 226 loss: 1.14193631e-06
Iter: 227 loss: 1.13706244e-06
Iter: 228 loss: 1.1895014e-06
Iter: 229 loss: 1.13694909e-06
Iter: 230 loss: 1.13208944e-06
Iter: 231 loss: 1.13935516e-06
Iter: 232 loss: 1.12970793e-06
Iter: 233 loss: 1.12590783e-06
Iter: 234 loss: 1.18215007e-06
Iter: 235 loss: 1.12592863e-06
Iter: 236 loss: 1.12344514e-06
Iter: 237 loss: 1.12526072e-06
Iter: 238 loss: 1.12196722e-06
Iter: 239 loss: 1.11884799e-06
Iter: 240 loss: 1.1220784e-06
Iter: 241 loss: 1.11716406e-06
Iter: 242 loss: 1.11322026e-06
Iter: 243 loss: 1.12585178e-06
Iter: 244 loss: 1.11209465e-06
Iter: 245 loss: 1.10858423e-06
Iter: 246 loss: 1.1018459e-06
Iter: 247 loss: 1.24692019e-06
Iter: 248 loss: 1.10187943e-06
Iter: 249 loss: 1.09669088e-06
Iter: 250 loss: 1.14539341e-06
Iter: 251 loss: 1.09647863e-06
Iter: 252 loss: 1.09168468e-06
Iter: 253 loss: 1.12823489e-06
Iter: 254 loss: 1.09135192e-06
Iter: 255 loss: 1.0876297e-06
Iter: 256 loss: 1.08930692e-06
Iter: 257 loss: 1.08505947e-06
Iter: 258 loss: 1.0816043e-06
Iter: 259 loss: 1.07742517e-06
Iter: 260 loss: 1.0769952e-06
Iter: 261 loss: 1.07095866e-06
Iter: 262 loss: 1.10840858e-06
Iter: 263 loss: 1.07027665e-06
Iter: 264 loss: 1.06507264e-06
Iter: 265 loss: 1.08440463e-06
Iter: 266 loss: 1.06381958e-06
Iter: 267 loss: 1.05965228e-06
Iter: 268 loss: 1.05961988e-06
Iter: 269 loss: 1.05742379e-06
Iter: 270 loss: 1.06357106e-06
Iter: 271 loss: 1.05670574e-06
Iter: 272 loss: 1.05389358e-06
Iter: 273 loss: 1.05229947e-06
Iter: 274 loss: 1.05100253e-06
Iter: 275 loss: 1.04803485e-06
Iter: 276 loss: 1.07597077e-06
Iter: 277 loss: 1.04790706e-06
Iter: 278 loss: 1.04565891e-06
Iter: 279 loss: 1.04750688e-06
Iter: 280 loss: 1.04435514e-06
Iter: 281 loss: 1.04153014e-06
Iter: 282 loss: 1.03846901e-06
Iter: 283 loss: 1.03799368e-06
Iter: 284 loss: 1.03440505e-06
Iter: 285 loss: 1.07766937e-06
Iter: 286 loss: 1.03437378e-06
Iter: 287 loss: 1.03182595e-06
Iter: 288 loss: 1.03805746e-06
Iter: 289 loss: 1.0309177e-06
Iter: 290 loss: 1.02739318e-06
Iter: 291 loss: 1.03633352e-06
Iter: 292 loss: 1.02611023e-06
Iter: 293 loss: 1.02344165e-06
Iter: 294 loss: 1.01890851e-06
Iter: 295 loss: 1.01890373e-06
Iter: 296 loss: 1.0143915e-06
Iter: 297 loss: 1.04039293e-06
Iter: 298 loss: 1.01379555e-06
Iter: 299 loss: 1.01028729e-06
Iter: 300 loss: 1.02865579e-06
Iter: 301 loss: 1.00972716e-06
Iter: 302 loss: 1.00741045e-06
Iter: 303 loss: 1.0073785e-06
Iter: 304 loss: 1.00598891e-06
Iter: 305 loss: 1.00757245e-06
Iter: 306 loss: 1.00525699e-06
Iter: 307 loss: 1.00355169e-06
Iter: 308 loss: 1.00707371e-06
Iter: 309 loss: 1.00283103e-06
Iter: 310 loss: 1.00113675e-06
Iter: 311 loss: 1.00027307e-06
Iter: 312 loss: 9.99496e-07
Iter: 313 loss: 9.96480821e-07
Iter: 314 loss: 1.01509022e-06
Iter: 315 loss: 9.96097469e-07
Iter: 316 loss: 9.94271545e-07
Iter: 317 loss: 9.90219633e-07
Iter: 318 loss: 1.04900244e-06
Iter: 319 loss: 9.90051149e-07
Iter: 320 loss: 9.86195118e-07
Iter: 321 loss: 9.86191935e-07
Iter: 322 loss: 9.84414214e-07
Iter: 323 loss: 1.0016912e-06
Iter: 324 loss: 9.84367148e-07
Iter: 325 loss: 9.82682309e-07
Iter: 326 loss: 9.82427764e-07
Iter: 327 loss: 9.81248604e-07
Iter: 328 loss: 9.79077186e-07
Iter: 329 loss: 9.8299131e-07
Iter: 330 loss: 9.78148364e-07
Iter: 331 loss: 9.75793e-07
Iter: 332 loss: 9.75068815e-07
Iter: 333 loss: 9.73688543e-07
Iter: 334 loss: 9.70673113e-07
Iter: 335 loss: 9.79123797e-07
Iter: 336 loss: 9.69719622e-07
Iter: 337 loss: 9.68532277e-07
Iter: 338 loss: 9.67832e-07
Iter: 339 loss: 9.66566176e-07
Iter: 340 loss: 9.65246159e-07
Iter: 341 loss: 9.65027084e-07
Iter: 342 loss: 9.63038929e-07
Iter: 343 loss: 9.76647584e-07
Iter: 344 loss: 9.62873287e-07
Iter: 345 loss: 9.61238243e-07
Iter: 346 loss: 9.5946541e-07
Iter: 347 loss: 9.59227918e-07
Iter: 348 loss: 9.57402e-07
Iter: 349 loss: 9.57396196e-07
Iter: 350 loss: 9.56174745e-07
Iter: 351 loss: 9.54263896e-07
Iter: 352 loss: 9.54200232e-07
Iter: 353 loss: 9.51857487e-07
Iter: 354 loss: 9.57783e-07
Iter: 355 loss: 9.51066227e-07
Iter: 356 loss: 9.48698926e-07
Iter: 357 loss: 9.57716225e-07
Iter: 358 loss: 9.48143452e-07
Iter: 359 loss: 9.45680313e-07
Iter: 360 loss: 9.67553092e-07
Iter: 361 loss: 9.45572708e-07
Iter: 362 loss: 9.44364956e-07
Iter: 363 loss: 9.41646e-07
Iter: 364 loss: 9.79780111e-07
Iter: 365 loss: 9.41510848e-07
Iter: 366 loss: 9.39199424e-07
Iter: 367 loss: 9.71714826e-07
Iter: 368 loss: 9.39192e-07
Iter: 369 loss: 9.3734559e-07
Iter: 370 loss: 9.38332164e-07
Iter: 371 loss: 9.36151594e-07
Iter: 372 loss: 9.35228115e-07
Iter: 373 loss: 9.34934292e-07
Iter: 374 loss: 9.33854494e-07
Iter: 375 loss: 9.33877402e-07
Iter: 376 loss: 9.33003435e-07
Iter: 377 loss: 9.31792215e-07
Iter: 378 loss: 9.32928856e-07
Iter: 379 loss: 9.31069053e-07
Iter: 380 loss: 9.29033e-07
Iter: 381 loss: 9.31335308e-07
Iter: 382 loss: 9.2789378e-07
Iter: 383 loss: 9.2665465e-07
Iter: 384 loss: 9.31342868e-07
Iter: 385 loss: 9.26346274e-07
Iter: 386 loss: 9.24540871e-07
Iter: 387 loss: 9.2320289e-07
Iter: 388 loss: 9.22627123e-07
Iter: 389 loss: 9.20734067e-07
Iter: 390 loss: 9.22189201e-07
Iter: 391 loss: 9.1953973e-07
Iter: 392 loss: 9.17030377e-07
Iter: 393 loss: 9.22659069e-07
Iter: 394 loss: 9.16088084e-07
Iter: 395 loss: 9.13667805e-07
Iter: 396 loss: 9.26709561e-07
Iter: 397 loss: 9.13311794e-07
Iter: 398 loss: 9.11972108e-07
Iter: 399 loss: 9.11949428e-07
Iter: 400 loss: 9.10675681e-07
Iter: 401 loss: 9.08113066e-07
Iter: 402 loss: 9.55786732e-07
Iter: 403 loss: 9.08065658e-07
Iter: 404 loss: 9.05644583e-07
Iter: 405 loss: 9.12351e-07
Iter: 406 loss: 9.04869864e-07
Iter: 407 loss: 9.0272539e-07
Iter: 408 loss: 9.13573e-07
Iter: 409 loss: 9.02372108e-07
Iter: 410 loss: 9.01149178e-07
Iter: 411 loss: 9.01130079e-07
Iter: 412 loss: 8.99770043e-07
Iter: 413 loss: 9.00573923e-07
Iter: 414 loss: 8.98939561e-07
Iter: 415 loss: 8.97969699e-07
Iter: 416 loss: 8.98073949e-07
Iter: 417 loss: 8.97249379e-07
Iter: 418 loss: 8.95679534e-07
Iter: 419 loss: 9.03133127e-07
Iter: 420 loss: 8.95375251e-07
Iter: 421 loss: 8.93920514e-07
Iter: 422 loss: 8.9763995e-07
Iter: 423 loss: 8.93395395e-07
Iter: 424 loss: 8.9248681e-07
Iter: 425 loss: 8.9269696e-07
Iter: 426 loss: 8.91789512e-07
Iter: 427 loss: 8.90278102e-07
Iter: 428 loss: 8.96776385e-07
Iter: 429 loss: 8.89958301e-07
Iter: 430 loss: 8.88402099e-07
Iter: 431 loss: 8.88121122e-07
Iter: 432 loss: 8.87059059e-07
Iter: 433 loss: 8.8533136e-07
Iter: 434 loss: 8.8770787e-07
Iter: 435 loss: 8.84520489e-07
Iter: 436 loss: 8.83e-07
Iter: 437 loss: 8.91826403e-07
Iter: 438 loss: 8.82825418e-07
Iter: 439 loss: 8.81263702e-07
Iter: 440 loss: 8.93621348e-07
Iter: 441 loss: 8.81150697e-07
Iter: 442 loss: 8.80062657e-07
Iter: 443 loss: 8.80345567e-07
Iter: 444 loss: 8.79307606e-07
Iter: 445 loss: 8.78158346e-07
Iter: 446 loss: 8.76202819e-07
Iter: 447 loss: 8.76200033e-07
Iter: 448 loss: 8.76848276e-07
Iter: 449 loss: 8.75380351e-07
Iter: 450 loss: 8.74533725e-07
Iter: 451 loss: 8.74095406e-07
Iter: 452 loss: 8.73672889e-07
Iter: 453 loss: 8.72518513e-07
Iter: 454 loss: 8.70944291e-07
Iter: 455 loss: 8.70850272e-07
Iter: 456 loss: 8.69944301e-07
Iter: 457 loss: 8.69872281e-07
Iter: 458 loss: 8.68772759e-07
Iter: 459 loss: 8.68363202e-07
Iter: 460 loss: 8.67767369e-07
Iter: 461 loss: 8.66674156e-07
Iter: 462 loss: 8.7013791e-07
Iter: 463 loss: 8.66318373e-07
Iter: 464 loss: 8.64949925e-07
Iter: 465 loss: 8.65885852e-07
Iter: 466 loss: 8.64117055e-07
Iter: 467 loss: 8.6259729e-07
Iter: 468 loss: 8.78257424e-07
Iter: 469 loss: 8.62572904e-07
Iter: 470 loss: 8.61732e-07
Iter: 471 loss: 8.60188322e-07
Iter: 472 loss: 8.95568746e-07
Iter: 473 loss: 8.60173429e-07
Iter: 474 loss: 8.58461647e-07
Iter: 475 loss: 8.72134535e-07
Iter: 476 loss: 8.58337103e-07
Iter: 477 loss: 8.56826773e-07
Iter: 478 loss: 8.6970249e-07
Iter: 479 loss: 8.56758675e-07
Iter: 480 loss: 8.55935411e-07
Iter: 481 loss: 8.54855898e-07
Iter: 482 loss: 8.54788254e-07
Iter: 483 loss: 8.53388e-07
Iter: 484 loss: 8.5905657e-07
Iter: 485 loss: 8.53071469e-07
Iter: 486 loss: 8.52603193e-07
Iter: 487 loss: 8.52401911e-07
Iter: 488 loss: 8.51894697e-07
Iter: 489 loss: 8.50503966e-07
Iter: 490 loss: 8.59166789e-07
Iter: 491 loss: 8.50152446e-07
Iter: 492 loss: 8.48729599e-07
Iter: 493 loss: 8.5312729e-07
Iter: 494 loss: 8.48298e-07
Iter: 495 loss: 8.47107117e-07
Iter: 496 loss: 8.47113711e-07
Iter: 497 loss: 8.46293347e-07
Iter: 498 loss: 8.45683928e-07
Iter: 499 loss: 8.45394197e-07
Iter: 500 loss: 8.4437437e-07
Iter: 501 loss: 8.44571787e-07
Iter: 502 loss: 8.43661383e-07
Iter: 503 loss: 8.4215128e-07
Iter: 504 loss: 8.56788915e-07
Iter: 505 loss: 8.42095176e-07
Iter: 506 loss: 8.41205349e-07
Iter: 507 loss: 8.4246318e-07
Iter: 508 loss: 8.40791756e-07
Iter: 509 loss: 8.39801601e-07
Iter: 510 loss: 8.39143866e-07
Iter: 511 loss: 8.38742153e-07
Iter: 512 loss: 8.3727457e-07
Iter: 513 loss: 8.52333301e-07
Iter: 514 loss: 8.37216419e-07
Iter: 515 loss: 8.36211711e-07
Iter: 516 loss: 8.43435828e-07
Iter: 517 loss: 8.36141567e-07
Iter: 518 loss: 8.35307219e-07
Iter: 519 loss: 8.33936e-07
Iter: 520 loss: 8.33962758e-07
Iter: 521 loss: 8.32939577e-07
Iter: 522 loss: 8.47222111e-07
Iter: 523 loss: 8.32934177e-07
Iter: 524 loss: 8.31843863e-07
Iter: 525 loss: 8.36435106e-07
Iter: 526 loss: 8.31616148e-07
Iter: 527 loss: 8.31091029e-07
Iter: 528 loss: 8.30063925e-07
Iter: 529 loss: 8.52870471e-07
Iter: 530 loss: 8.30053295e-07
Iter: 531 loss: 8.28677344e-07
Iter: 532 loss: 8.30738827e-07
Iter: 533 loss: 8.28002044e-07
Iter: 534 loss: 8.27462372e-07
Iter: 535 loss: 8.27179633e-07
Iter: 536 loss: 8.26752967e-07
Iter: 537 loss: 8.2548388e-07
Iter: 538 loss: 8.30683121e-07
Iter: 539 loss: 8.24998892e-07
Iter: 540 loss: 8.23770051e-07
Iter: 541 loss: 8.43333453e-07
Iter: 542 loss: 8.23780567e-07
Iter: 543 loss: 8.22836171e-07
Iter: 544 loss: 8.27873123e-07
Iter: 545 loss: 8.22681784e-07
Iter: 546 loss: 8.21704646e-07
Iter: 547 loss: 8.20933906e-07
Iter: 548 loss: 8.20618425e-07
Iter: 549 loss: 8.19471097e-07
Iter: 550 loss: 8.22944855e-07
Iter: 551 loss: 8.19163233e-07
Iter: 552 loss: 8.1801079e-07
Iter: 553 loss: 8.32037699e-07
Iter: 554 loss: 8.17977707e-07
Iter: 555 loss: 8.17254545e-07
Iter: 556 loss: 8.1733458e-07
Iter: 557 loss: 8.16696456e-07
Iter: 558 loss: 8.15623423e-07
Iter: 559 loss: 8.16272404e-07
Iter: 560 loss: 8.14923112e-07
Iter: 561 loss: 8.14397822e-07
Iter: 562 loss: 8.14187558e-07
Iter: 563 loss: 8.13683471e-07
Iter: 564 loss: 8.12540293e-07
Iter: 565 loss: 8.29744351e-07
Iter: 566 loss: 8.12461963e-07
Iter: 567 loss: 8.11373752e-07
Iter: 568 loss: 8.12024155e-07
Iter: 569 loss: 8.1064286e-07
Iter: 570 loss: 8.09379912e-07
Iter: 571 loss: 8.12744e-07
Iter: 572 loss: 8.08948812e-07
Iter: 573 loss: 8.08348659e-07
Iter: 574 loss: 8.08196546e-07
Iter: 575 loss: 8.07568824e-07
Iter: 576 loss: 8.06934963e-07
Iter: 577 loss: 8.06838784e-07
Iter: 578 loss: 8.05885861e-07
Iter: 579 loss: 8.05634102e-07
Iter: 580 loss: 8.05053901e-07
Iter: 581 loss: 8.04081424e-07
Iter: 582 loss: 8.04064086e-07
Iter: 583 loss: 8.03297667e-07
Iter: 584 loss: 8.04052888e-07
Iter: 585 loss: 8.02844397e-07
Iter: 586 loss: 8.02020168e-07
Iter: 587 loss: 8.01240105e-07
Iter: 588 loss: 8.01085321e-07
Iter: 589 loss: 8.00187e-07
Iter: 590 loss: 8.00073281e-07
Iter: 591 loss: 7.99449e-07
Iter: 592 loss: 7.9871711e-07
Iter: 593 loss: 7.98600126e-07
Iter: 594 loss: 7.97634357e-07
Iter: 595 loss: 8.04420779e-07
Iter: 596 loss: 7.97546e-07
Iter: 597 loss: 7.96567747e-07
Iter: 598 loss: 8.02610202e-07
Iter: 599 loss: 7.96456561e-07
Iter: 600 loss: 7.95903702e-07
Iter: 601 loss: 7.94923494e-07
Iter: 602 loss: 7.94917e-07
Iter: 603 loss: 7.93652646e-07
Iter: 604 loss: 7.96728045e-07
Iter: 605 loss: 7.93155891e-07
Iter: 606 loss: 7.91978948e-07
Iter: 607 loss: 7.92164542e-07
Iter: 608 loss: 7.91035745e-07
Iter: 609 loss: 7.90932745e-07
Iter: 610 loss: 7.90455886e-07
Iter: 611 loss: 7.89866704e-07
Iter: 612 loss: 7.88559532e-07
Iter: 613 loss: 8.08437903e-07
Iter: 614 loss: 7.88508544e-07
Iter: 615 loss: 7.8736349e-07
Iter: 616 loss: 7.94200503e-07
Iter: 617 loss: 7.87206147e-07
Iter: 618 loss: 7.8644922e-07
Iter: 619 loss: 7.91778689e-07
Iter: 620 loss: 7.86334851e-07
Iter: 621 loss: 7.85515567e-07
Iter: 622 loss: 7.85682403e-07
Iter: 623 loss: 7.8489029e-07
Iter: 624 loss: 7.84045369e-07
Iter: 625 loss: 7.8405418e-07
Iter: 626 loss: 7.83365465e-07
Iter: 627 loss: 7.82253437e-07
Iter: 628 loss: 7.822523e-07
Iter: 629 loss: 7.81765152e-07
Iter: 630 loss: 7.8078051e-07
Iter: 631 loss: 8.00715725e-07
Iter: 632 loss: 7.80773462e-07
Iter: 633 loss: 7.8008128e-07
Iter: 634 loss: 7.80026653e-07
Iter: 635 loss: 7.79352376e-07
Iter: 636 loss: 7.79564e-07
Iter: 637 loss: 7.78904791e-07
Iter: 638 loss: 7.78365916e-07
Iter: 639 loss: 7.77271907e-07
Iter: 640 loss: 7.97065e-07
Iter: 641 loss: 7.77258265e-07
Iter: 642 loss: 7.76186482e-07
Iter: 643 loss: 7.91953312e-07
Iter: 644 loss: 7.76195293e-07
Iter: 645 loss: 7.75403805e-07
Iter: 646 loss: 7.78865058e-07
Iter: 647 loss: 7.75255899e-07
Iter: 648 loss: 7.74272735e-07
Iter: 649 loss: 7.74216232e-07
Iter: 650 loss: 7.73503473e-07
Iter: 651 loss: 7.7259233e-07
Iter: 652 loss: 7.73409454e-07
Iter: 653 loss: 7.72047e-07
Iter: 654 loss: 7.71088366e-07
Iter: 655 loss: 7.7732841e-07
Iter: 656 loss: 7.70964959e-07
Iter: 657 loss: 7.7007445e-07
Iter: 658 loss: 7.74981913e-07
Iter: 659 loss: 7.69936662e-07
Iter: 660 loss: 7.69278813e-07
Iter: 661 loss: 7.68757104e-07
Iter: 662 loss: 7.6855514e-07
Iter: 663 loss: 7.67754045e-07
Iter: 664 loss: 7.79093284e-07
Iter: 665 loss: 7.6774711e-07
Iter: 666 loss: 7.6697313e-07
Iter: 667 loss: 7.67938332e-07
Iter: 668 loss: 7.66584776e-07
Iter: 669 loss: 7.6616459e-07
Iter: 670 loss: 7.71202622e-07
Iter: 671 loss: 7.66160952e-07
Iter: 672 loss: 7.65669142e-07
Iter: 673 loss: 7.64588e-07
Iter: 674 loss: 7.8125646e-07
Iter: 675 loss: 7.64550293e-07
Iter: 676 loss: 7.63460093e-07
Iter: 677 loss: 7.6677577e-07
Iter: 678 loss: 7.63190428e-07
Iter: 679 loss: 7.62342779e-07
Iter: 680 loss: 7.63573951e-07
Iter: 681 loss: 7.61943681e-07
Iter: 682 loss: 7.61051183e-07
Iter: 683 loss: 7.75217302e-07
Iter: 684 loss: 7.61058743e-07
Iter: 685 loss: 7.60582566e-07
Iter: 686 loss: 7.6030426e-07
Iter: 687 loss: 7.60091439e-07
Iter: 688 loss: 7.59315583e-07
Iter: 689 loss: 7.59816089e-07
Iter: 690 loss: 7.58846454e-07
Iter: 691 loss: 7.57917178e-07
Iter: 692 loss: 7.6121745e-07
Iter: 693 loss: 7.57703901e-07
Iter: 694 loss: 7.56829706e-07
Iter: 695 loss: 7.63845435e-07
Iter: 696 loss: 7.56749671e-07
Iter: 697 loss: 7.56068857e-07
Iter: 698 loss: 7.56254508e-07
Iter: 699 loss: 7.55590804e-07
Iter: 700 loss: 7.54887196e-07
Iter: 701 loss: 7.56885242e-07
Iter: 702 loss: 7.54667212e-07
Iter: 703 loss: 7.53678478e-07
Iter: 704 loss: 7.56984775e-07
Iter: 705 loss: 7.53421205e-07
Iter: 706 loss: 7.52957703e-07
Iter: 707 loss: 7.59902264e-07
Iter: 708 loss: 7.52958613e-07
Iter: 709 loss: 7.52540473e-07
Iter: 710 loss: 7.51786956e-07
Iter: 711 loss: 7.51782522e-07
Iter: 712 loss: 7.5088667e-07
Iter: 713 loss: 7.50739e-07
Iter: 714 loss: 7.50134859e-07
Iter: 715 loss: 7.4925606e-07
Iter: 716 loss: 7.52716062e-07
Iter: 717 loss: 7.49067908e-07
Iter: 718 loss: 7.48267e-07
Iter: 719 loss: 7.48279945e-07
Iter: 720 loss: 7.4778211e-07
Iter: 721 loss: 7.46902856e-07
Iter: 722 loss: 7.67893255e-07
Iter: 723 loss: 7.46901094e-07
Iter: 724 loss: 7.4595431e-07
Iter: 725 loss: 7.51266214e-07
Iter: 726 loss: 7.45817317e-07
Iter: 727 loss: 7.45077955e-07
Iter: 728 loss: 7.47281206e-07
Iter: 729 loss: 7.44890258e-07
Iter: 730 loss: 7.44135491e-07
Iter: 731 loss: 7.48586899e-07
Iter: 732 loss: 7.44051e-07
Iter: 733 loss: 7.43425858e-07
Iter: 734 loss: 7.44607689e-07
Iter: 735 loss: 7.43210535e-07
Iter: 736 loss: 7.42653128e-07
Iter: 737 loss: 7.44146746e-07
Iter: 738 loss: 7.42467876e-07
Iter: 739 loss: 7.41872668e-07
Iter: 740 loss: 7.46273884e-07
Iter: 741 loss: 7.41804115e-07
Iter: 742 loss: 7.41425424e-07
Iter: 743 loss: 7.42715315e-07
Iter: 744 loss: 7.41323902e-07
Iter: 745 loss: 7.40825953e-07
Iter: 746 loss: 7.40433563e-07
Iter: 747 loss: 7.40257803e-07
Iter: 748 loss: 7.39647419e-07
Iter: 749 loss: 7.3890476e-07
Iter: 750 loss: 7.38827453e-07
Iter: 751 loss: 7.37666653e-07
Iter: 752 loss: 7.42724865e-07
Iter: 753 loss: 7.37456389e-07
Iter: 754 loss: 7.36782169e-07
Iter: 755 loss: 7.36757499e-07
Iter: 756 loss: 7.36202651e-07
Iter: 757 loss: 7.3574239e-07
Iter: 758 loss: 7.35571234e-07
Iter: 759 loss: 7.34949e-07
Iter: 760 loss: 7.3447444e-07
Iter: 761 loss: 7.34254968e-07
Iter: 762 loss: 7.33469051e-07
Iter: 763 loss: 7.46246656e-07
Iter: 764 loss: 7.3347411e-07
Iter: 765 loss: 7.32871626e-07
Iter: 766 loss: 7.34744901e-07
Iter: 767 loss: 7.32687454e-07
Iter: 768 loss: 7.31940531e-07
Iter: 769 loss: 7.32963827e-07
Iter: 770 loss: 7.3158418e-07
Iter: 771 loss: 7.31002331e-07
Iter: 772 loss: 7.3429203e-07
Iter: 773 loss: 7.30941565e-07
Iter: 774 loss: 7.30286615e-07
Iter: 775 loss: 7.32294779e-07
Iter: 776 loss: 7.3012535e-07
Iter: 777 loss: 7.2948751e-07
Iter: 778 loss: 7.31447813e-07
Iter: 779 loss: 7.29304e-07
Iter: 780 loss: 7.28735131e-07
Iter: 781 loss: 7.30021952e-07
Iter: 782 loss: 7.28545501e-07
Iter: 783 loss: 7.28043744e-07
Iter: 784 loss: 7.27054555e-07
Iter: 785 loss: 7.44983822e-07
Iter: 786 loss: 7.27038355e-07
Iter: 787 loss: 7.26161716e-07
Iter: 788 loss: 7.335085e-07
Iter: 789 loss: 7.261159e-07
Iter: 790 loss: 7.25380175e-07
Iter: 791 loss: 7.2867374e-07
Iter: 792 loss: 7.25262169e-07
Iter: 793 loss: 7.24336246e-07
Iter: 794 loss: 7.26569e-07
Iter: 795 loss: 7.23999506e-07
Iter: 796 loss: 7.23479388e-07
Iter: 797 loss: 7.22792663e-07
Iter: 798 loss: 7.22756909e-07
Iter: 799 loss: 7.21910283e-07
Iter: 800 loss: 7.27325528e-07
Iter: 801 loss: 7.21807226e-07
Iter: 802 loss: 7.21257607e-07
Iter: 803 loss: 7.23864844e-07
Iter: 804 loss: 7.21163701e-07
Iter: 805 loss: 7.20402056e-07
Iter: 806 loss: 7.22139134e-07
Iter: 807 loss: 7.201063e-07
Iter: 808 loss: 7.19547245e-07
Iter: 809 loss: 7.20399328e-07
Iter: 810 loss: 7.19305433e-07
Iter: 811 loss: 7.18500928e-07
Iter: 812 loss: 7.22408117e-07
Iter: 813 loss: 7.18359843e-07
Iter: 814 loss: 7.17758269e-07
Iter: 815 loss: 7.19295826e-07
Iter: 816 loss: 7.17536068e-07
Iter: 817 loss: 7.16956663e-07
Iter: 818 loss: 7.1846e-07
Iter: 819 loss: 7.16765612e-07
Iter: 820 loss: 7.1623424e-07
Iter: 821 loss: 7.1558793e-07
Iter: 822 loss: 7.15519377e-07
Iter: 823 loss: 7.1468304e-07
Iter: 824 loss: 7.19302079e-07
Iter: 825 loss: 7.14520183e-07
Iter: 826 loss: 7.13880922e-07
Iter: 827 loss: 7.15679732e-07
Iter: 828 loss: 7.13677082e-07
Iter: 829 loss: 7.13152701e-07
Iter: 830 loss: 7.13140366e-07
Iter: 831 loss: 7.12731946e-07
Iter: 832 loss: 7.1184968e-07
Iter: 833 loss: 7.26830535e-07
Iter: 834 loss: 7.11813868e-07
Iter: 835 loss: 7.10981169e-07
Iter: 836 loss: 7.12761903e-07
Iter: 837 loss: 7.10657673e-07
Iter: 838 loss: 7.0982e-07
Iter: 839 loss: 7.14968394e-07
Iter: 840 loss: 7.09676669e-07
Iter: 841 loss: 7.08921561e-07
Iter: 842 loss: 7.15065255e-07
Iter: 843 loss: 7.08874381e-07
Iter: 844 loss: 7.08308221e-07
Iter: 845 loss: 7.08941116e-07
Iter: 846 loss: 7.08056803e-07
Iter: 847 loss: 7.07674644e-07
Iter: 848 loss: 7.07638492e-07
Iter: 849 loss: 7.07418167e-07
Iter: 850 loss: 7.06902142e-07
Iter: 851 loss: 7.16909312e-07
Iter: 852 loss: 7.06909077e-07
Iter: 853 loss: 7.06222636e-07
Iter: 854 loss: 7.0892429e-07
Iter: 855 loss: 7.06041192e-07
Iter: 856 loss: 7.05606055e-07
Iter: 857 loss: 7.05182401e-07
Iter: 858 loss: 7.05093498e-07
Iter: 859 loss: 7.04276488e-07
Iter: 860 loss: 7.07192839e-07
Iter: 861 loss: 7.04082595e-07
Iter: 862 loss: 7.03378532e-07
Iter: 863 loss: 7.05329512e-07
Iter: 864 loss: 7.03134333e-07
Iter: 865 loss: 7.02379452e-07
Iter: 866 loss: 7.09492269e-07
Iter: 867 loss: 7.02363366e-07
Iter: 868 loss: 7.01900888e-07
Iter: 869 loss: 7.01768101e-07
Iter: 870 loss: 7.01482918e-07
Iter: 871 loss: 7.00803412e-07
Iter: 872 loss: 6.99981115e-07
Iter: 873 loss: 6.99914949e-07
Iter: 874 loss: 6.99439056e-07
Iter: 875 loss: 6.99358452e-07
Iter: 876 loss: 6.98910071e-07
Iter: 877 loss: 7.00948704e-07
Iter: 878 loss: 6.98799056e-07
Iter: 879 loss: 6.98344877e-07
Iter: 880 loss: 6.98572705e-07
Iter: 881 loss: 6.98056567e-07
Iter: 882 loss: 6.97406335e-07
Iter: 883 loss: 7.0213008e-07
Iter: 884 loss: 6.97367341e-07
Iter: 885 loss: 6.97108874e-07
Iter: 886 loss: 6.96991e-07
Iter: 887 loss: 6.96822667e-07
Iter: 888 loss: 6.9619216e-07
Iter: 889 loss: 6.96249344e-07
Iter: 890 loss: 6.95697963e-07
Iter: 891 loss: 6.9492927e-07
Iter: 892 loss: 6.95120093e-07
Iter: 893 loss: 6.9441694e-07
Iter: 894 loss: 6.9356804e-07
Iter: 895 loss: 6.99338045e-07
Iter: 896 loss: 6.93490904e-07
Iter: 897 loss: 6.93022798e-07
Iter: 898 loss: 6.98804456e-07
Iter: 899 loss: 6.93020525e-07
Iter: 900 loss: 6.92647177e-07
Iter: 901 loss: 6.92985395e-07
Iter: 902 loss: 6.92411334e-07
Iter: 903 loss: 6.91893604e-07
Iter: 904 loss: 6.91535263e-07
Iter: 905 loss: 6.91362857e-07
Iter: 906 loss: 6.90599165e-07
Iter: 907 loss: 6.90988941e-07
Iter: 908 loss: 6.90159254e-07
Iter: 909 loss: 6.89496e-07
Iter: 910 loss: 6.99784209e-07
Iter: 911 loss: 6.89495835e-07
Iter: 912 loss: 6.8889949e-07
Iter: 913 loss: 6.91769628e-07
Iter: 914 loss: 6.88788532e-07
Iter: 915 loss: 6.88304397e-07
Iter: 916 loss: 6.90205809e-07
Iter: 917 loss: 6.88197304e-07
Iter: 918 loss: 6.87631029e-07
Iter: 919 loss: 6.88156717e-07
Iter: 920 loss: 6.87303213e-07
Iter: 921 loss: 6.86853e-07
Iter: 922 loss: 6.87546446e-07
Iter: 923 loss: 6.86629164e-07
Iter: 924 loss: 6.85990926e-07
Iter: 925 loss: 6.87594365e-07
Iter: 926 loss: 6.85772591e-07
Iter: 927 loss: 6.8528152e-07
Iter: 928 loss: 6.84614236e-07
Iter: 929 loss: 6.84578538e-07
Iter: 930 loss: 6.83835935e-07
Iter: 931 loss: 6.90713705e-07
Iter: 932 loss: 6.83802455e-07
Iter: 933 loss: 6.83176e-07
Iter: 934 loss: 6.88519e-07
Iter: 935 loss: 6.83143867e-07
Iter: 936 loss: 6.82694917e-07
Iter: 937 loss: 6.82857547e-07
Iter: 938 loss: 6.82388759e-07
Iter: 939 loss: 6.81812253e-07
Iter: 940 loss: 6.816515e-07
Iter: 941 loss: 6.81308e-07
Iter: 942 loss: 6.80507924e-07
Iter: 943 loss: 6.83578833e-07
Iter: 944 loss: 6.8030721e-07
Iter: 945 loss: 6.79705295e-07
Iter: 946 loss: 6.83205201e-07
Iter: 947 loss: 6.79641403e-07
Iter: 948 loss: 6.79020332e-07
Iter: 949 loss: 6.82998291e-07
Iter: 950 loss: 6.78958713e-07
Iter: 951 loss: 6.78597189e-07
Iter: 952 loss: 6.8060524e-07
Iter: 953 loss: 6.78542278e-07
Iter: 954 loss: 6.78128686e-07
Iter: 955 loss: 6.77226637e-07
Iter: 956 loss: 6.89718149e-07
Iter: 957 loss: 6.77185e-07
Iter: 958 loss: 6.76750687e-07
Iter: 959 loss: 6.76711352e-07
Iter: 960 loss: 6.76337663e-07
Iter: 961 loss: 6.75995352e-07
Iter: 962 loss: 6.75910258e-07
Iter: 963 loss: 6.75292029e-07
Iter: 964 loss: 6.75647129e-07
Iter: 965 loss: 6.74874855e-07
Iter: 966 loss: 6.74299883e-07
Iter: 967 loss: 6.78282731e-07
Iter: 968 loss: 6.74213652e-07
Iter: 969 loss: 6.73542161e-07
Iter: 970 loss: 6.76495404e-07
Iter: 971 loss: 6.73415173e-07
Iter: 972 loss: 6.72948772e-07
Iter: 973 loss: 6.730628e-07
Iter: 974 loss: 6.72573492e-07
Iter: 975 loss: 6.72008468e-07
Iter: 976 loss: 6.72658643e-07
Iter: 977 loss: 6.71743805e-07
Iter: 978 loss: 6.71045655e-07
Iter: 979 loss: 6.73518798e-07
Iter: 980 loss: 6.70856195e-07
Iter: 981 loss: 6.70339375e-07
Iter: 982 loss: 6.73894874e-07
Iter: 983 loss: 6.70304075e-07
Iter: 984 loss: 6.6965913e-07
Iter: 985 loss: 6.71068051e-07
Iter: 986 loss: 6.69446763e-07
Iter: 987 loss: 6.69026747e-07
Iter: 988 loss: 6.73890099e-07
Iter: 989 loss: 6.69042208e-07
Iter: 990 loss: 6.68809264e-07
Iter: 991 loss: 6.68395842e-07
Iter: 992 loss: 6.68393909e-07
Iter: 993 loss: 6.67888e-07
Iter: 994 loss: 6.70340967e-07
Iter: 995 loss: 6.67779091e-07
Iter: 996 loss: 6.67183826e-07
Iter: 997 loss: 6.67183144e-07
Iter: 998 loss: 6.66721e-07
Iter: 999 loss: 6.6610346e-07
Iter: 1000 loss: 6.67098561e-07
Iter: 1001 loss: 6.65797643e-07
Iter: 1002 loss: 6.65284347e-07
Iter: 1003 loss: 6.6912844e-07
Iter: 1004 loss: 6.65248876e-07
Iter: 1005 loss: 6.64647473e-07
Iter: 1006 loss: 6.66090955e-07
Iter: 1007 loss: 6.64450226e-07
Iter: 1008 loss: 6.64039646e-07
Iter: 1009 loss: 6.63579954e-07
Iter: 1010 loss: 6.63507763e-07
Iter: 1011 loss: 6.62776756e-07
Iter: 1012 loss: 6.67116922e-07
Iter: 1013 loss: 6.62672619e-07
Iter: 1014 loss: 6.62108732e-07
Iter: 1015 loss: 6.62587e-07
Iter: 1016 loss: 6.61720605e-07
Iter: 1017 loss: 6.61317245e-07
Iter: 1018 loss: 6.61260515e-07
Iter: 1019 loss: 6.60910189e-07
Iter: 1020 loss: 6.61508807e-07
Iter: 1021 loss: 6.60729938e-07
Iter: 1022 loss: 6.60362502e-07
Iter: 1023 loss: 6.61396484e-07
Iter: 1024 loss: 6.60263936e-07
Iter: 1025 loss: 6.59900138e-07
Iter: 1026 loss: 6.59543389e-07
Iter: 1027 loss: 6.59454145e-07
Iter: 1028 loss: 6.59042541e-07
Iter: 1029 loss: 6.63097353e-07
Iter: 1030 loss: 6.59009629e-07
Iter: 1031 loss: 6.58571707e-07
Iter: 1032 loss: 6.58028398e-07
Iter: 1033 loss: 6.57954672e-07
Iter: 1034 loss: 6.57327575e-07
Iter: 1035 loss: 6.59866373e-07
Iter: 1036 loss: 6.57203486e-07
Iter: 1037 loss: 6.56754196e-07
Iter: 1038 loss: 6.62976561e-07
Iter: 1039 loss: 6.56759084e-07
Iter: 1040 loss: 6.56321617e-07
Iter: 1041 loss: 6.56223165e-07
Iter: 1042 loss: 6.55963e-07
Iter: 1043 loss: 6.55476072e-07
Iter: 1044 loss: 6.54805604e-07
Iter: 1045 loss: 6.54742053e-07
Iter: 1046 loss: 6.54197265e-07
Iter: 1047 loss: 6.54217274e-07
Iter: 1048 loss: 6.53730808e-07
Iter: 1049 loss: 6.54011899e-07
Iter: 1050 loss: 6.53410439e-07
Iter: 1051 loss: 6.52868096e-07
Iter: 1052 loss: 6.52881454e-07
Iter: 1053 loss: 6.52549488e-07
Iter: 1054 loss: 6.52698418e-07
Iter: 1055 loss: 6.52304152e-07
Iter: 1056 loss: 6.51883454e-07
Iter: 1057 loss: 6.5274935e-07
Iter: 1058 loss: 6.51675919e-07
Iter: 1059 loss: 6.51223e-07
Iter: 1060 loss: 6.50936101e-07
Iter: 1061 loss: 6.50733114e-07
Iter: 1062 loss: 6.50326115e-07
Iter: 1063 loss: 6.50334187e-07
Iter: 1064 loss: 6.49978801e-07
Iter: 1065 loss: 6.49612275e-07
Iter: 1066 loss: 6.49566971e-07
Iter: 1067 loss: 6.49011895e-07
Iter: 1068 loss: 6.49885578e-07
Iter: 1069 loss: 6.48766161e-07
Iter: 1070 loss: 6.48239279e-07
Iter: 1071 loss: 6.48233936e-07
Iter: 1072 loss: 6.47817103e-07
Iter: 1073 loss: 6.4725441e-07
Iter: 1074 loss: 6.47204388e-07
Iter: 1075 loss: 6.46637e-07
Iter: 1076 loss: 6.46966782e-07
Iter: 1077 loss: 6.46272156e-07
Iter: 1078 loss: 6.4557338e-07
Iter: 1079 loss: 6.50054744e-07
Iter: 1080 loss: 6.45528473e-07
Iter: 1081 loss: 6.45213106e-07
Iter: 1082 loss: 6.45225043e-07
Iter: 1083 loss: 6.44912802e-07
Iter: 1084 loss: 6.45602199e-07
Iter: 1085 loss: 6.44787576e-07
Iter: 1086 loss: 6.44495117e-07
Iter: 1087 loss: 6.44758302e-07
Iter: 1088 loss: 6.44294232e-07
Iter: 1089 loss: 6.43893259e-07
Iter: 1090 loss: 6.44460499e-07
Iter: 1091 loss: 6.43689532e-07
Iter: 1092 loss: 6.43258772e-07
Iter: 1093 loss: 6.43854321e-07
Iter: 1094 loss: 6.43059252e-07
Iter: 1095 loss: 6.42631448e-07
Iter: 1096 loss: 6.44932527e-07
Iter: 1097 loss: 6.42559144e-07
Iter: 1098 loss: 6.42107523e-07
Iter: 1099 loss: 6.42236046e-07
Iter: 1100 loss: 6.41768167e-07
Iter: 1101 loss: 6.4125453e-07
Iter: 1102 loss: 6.40937e-07
Iter: 1103 loss: 6.4074e-07
Iter: 1104 loss: 6.40402391e-07
Iter: 1105 loss: 6.4026608e-07
Iter: 1106 loss: 6.39997779e-07
Iter: 1107 loss: 6.39456971e-07
Iter: 1108 loss: 6.48684534e-07
Iter: 1109 loss: 6.39429174e-07
Iter: 1110 loss: 6.3882834e-07
Iter: 1111 loss: 6.41241513e-07
Iter: 1112 loss: 6.38694758e-07
Iter: 1113 loss: 6.38183508e-07
Iter: 1114 loss: 6.38217557e-07
Iter: 1115 loss: 6.37760536e-07
Iter: 1116 loss: 6.37576136e-07
Iter: 1117 loss: 6.37412e-07
Iter: 1118 loss: 6.37066e-07
Iter: 1119 loss: 6.37178687e-07
Iter: 1120 loss: 6.36841321e-07
Iter: 1121 loss: 6.36410391e-07
Iter: 1122 loss: 6.36574555e-07
Iter: 1123 loss: 6.36121911e-07
Iter: 1124 loss: 6.35572633e-07
Iter: 1125 loss: 6.37630308e-07
Iter: 1126 loss: 6.35436663e-07
Iter: 1127 loss: 6.34994649e-07
Iter: 1128 loss: 6.34852e-07
Iter: 1129 loss: 6.3459089e-07
Iter: 1130 loss: 6.33929631e-07
Iter: 1131 loss: 6.41697511e-07
Iter: 1132 loss: 6.33930426e-07
Iter: 1133 loss: 6.33570153e-07
Iter: 1134 loss: 6.33792922e-07
Iter: 1135 loss: 6.33326067e-07
Iter: 1136 loss: 6.329592e-07
Iter: 1137 loss: 6.33248931e-07
Iter: 1138 loss: 6.32708634e-07
Iter: 1139 loss: 6.32325566e-07
Iter: 1140 loss: 6.32319825e-07
Iter: 1141 loss: 6.32106776e-07
Iter: 1142 loss: 6.31548346e-07
Iter: 1143 loss: 6.35664946e-07
Iter: 1144 loss: 6.3144671e-07
Iter: 1145 loss: 6.30663408e-07
Iter: 1146 loss: 6.32962326e-07
Iter: 1147 loss: 6.30411591e-07
Iter: 1148 loss: 6.29688429e-07
Iter: 1149 loss: 6.32177944e-07
Iter: 1150 loss: 6.29515284e-07
Iter: 1151 loss: 6.29256e-07
Iter: 1152 loss: 6.29126362e-07
Iter: 1153 loss: 6.28908936e-07
Iter: 1154 loss: 6.28571797e-07
Iter: 1155 loss: 6.28582143e-07
Iter: 1156 loss: 6.28128305e-07
Iter: 1157 loss: 6.30034e-07
Iter: 1158 loss: 6.28059411e-07
Iter: 1159 loss: 6.27607278e-07
Iter: 1160 loss: 6.27464772e-07
Iter: 1161 loss: 6.27181407e-07
Iter: 1162 loss: 6.2662042e-07
Iter: 1163 loss: 6.28084194e-07
Iter: 1164 loss: 6.26408337e-07
Iter: 1165 loss: 6.2582933e-07
Iter: 1166 loss: 6.29668136e-07
Iter: 1167 loss: 6.25771747e-07
Iter: 1168 loss: 6.25298071e-07
Iter: 1169 loss: 6.26424e-07
Iter: 1170 loss: 6.25133225e-07
Iter: 1171 loss: 6.24718837e-07
Iter: 1172 loss: 6.24849577e-07
Iter: 1173 loss: 6.24420863e-07
Iter: 1174 loss: 6.23974756e-07
Iter: 1175 loss: 6.3077573e-07
Iter: 1176 loss: 6.23968731e-07
Iter: 1177 loss: 6.23569463e-07
Iter: 1178 loss: 6.23182302e-07
Iter: 1179 loss: 6.23078222e-07
Iter: 1180 loss: 6.22605683e-07
Iter: 1181 loss: 6.23176959e-07
Iter: 1182 loss: 6.22358073e-07
Iter: 1183 loss: 6.21907e-07
Iter: 1184 loss: 6.24765164e-07
Iter: 1185 loss: 6.21839035e-07
Iter: 1186 loss: 6.21554364e-07
Iter: 1187 loss: 6.21539357e-07
Iter: 1188 loss: 6.21316872e-07
Iter: 1189 loss: 6.20819151e-07
Iter: 1190 loss: 6.24223674e-07
Iter: 1191 loss: 6.20695857e-07
Iter: 1192 loss: 6.20302501e-07
Iter: 1193 loss: 6.20260039e-07
Iter: 1194 loss: 6.19987645e-07
Iter: 1195 loss: 6.19926482e-07
Iter: 1196 loss: 6.19729917e-07
Iter: 1197 loss: 6.19288926e-07
Iter: 1198 loss: 6.19151933e-07
Iter: 1199 loss: 6.18899151e-07
Iter: 1200 loss: 6.18450485e-07
Iter: 1201 loss: 6.18437355e-07
Iter: 1202 loss: 6.1815274e-07
Iter: 1203 loss: 6.17889896e-07
Iter: 1204 loss: 6.17826686e-07
Iter: 1205 loss: 6.17278715e-07
Iter: 1206 loss: 6.19232196e-07
Iter: 1207 loss: 6.1718913e-07
Iter: 1208 loss: 6.16734326e-07
Iter: 1209 loss: 6.20890717e-07
Iter: 1210 loss: 6.16724037e-07
Iter: 1211 loss: 6.16320676e-07
Iter: 1212 loss: 6.1594011e-07
Iter: 1213 loss: 6.15894919e-07
Iter: 1214 loss: 6.15372585e-07
Iter: 1215 loss: 6.15137367e-07
Iter: 1216 loss: 6.14858209e-07
Iter: 1217 loss: 6.14409885e-07
Iter: 1218 loss: 6.14360772e-07
Iter: 1219 loss: 6.139623e-07
Iter: 1220 loss: 6.16229727e-07
Iter: 1221 loss: 6.13879081e-07
Iter: 1222 loss: 6.13666884e-07
Iter: 1223 loss: 6.13166321e-07
Iter: 1224 loss: 6.1939329e-07
Iter: 1225 loss: 6.13113457e-07
Iter: 1226 loss: 6.12778194e-07
Iter: 1227 loss: 6.12724e-07
Iter: 1228 loss: 6.12519443e-07
Iter: 1229 loss: 6.12231474e-07
Iter: 1230 loss: 6.12210329e-07
Iter: 1231 loss: 6.11848236e-07
Iter: 1232 loss: 6.14035571e-07
Iter: 1233 loss: 6.11800715e-07
Iter: 1234 loss: 6.11453515e-07
Iter: 1235 loss: 6.12597887e-07
Iter: 1236 loss: 6.11375526e-07
Iter: 1237 loss: 6.11018436e-07
Iter: 1238 loss: 6.10623829e-07
Iter: 1239 loss: 6.10536233e-07
Iter: 1240 loss: 6.10076427e-07
Iter: 1241 loss: 6.15427e-07
Iter: 1242 loss: 6.10071766e-07
Iter: 1243 loss: 6.09675567e-07
Iter: 1244 loss: 6.11361429e-07
Iter: 1245 loss: 6.09607241e-07
Iter: 1246 loss: 6.09237077e-07
Iter: 1247 loss: 6.0880086e-07
Iter: 1248 loss: 6.08750611e-07
Iter: 1249 loss: 6.08267783e-07
Iter: 1250 loss: 6.10069e-07
Iter: 1251 loss: 6.08139544e-07
Iter: 1252 loss: 6.07937864e-07
Iter: 1253 loss: 6.07900688e-07
Iter: 1254 loss: 6.07653e-07
Iter: 1255 loss: 6.07198274e-07
Iter: 1256 loss: 6.16222223e-07
Iter: 1257 loss: 6.07169227e-07
Iter: 1258 loss: 6.06730168e-07
Iter: 1259 loss: 6.08234927e-07
Iter: 1260 loss: 6.06586468e-07
Iter: 1261 loss: 6.06155709e-07
Iter: 1262 loss: 6.10163511e-07
Iter: 1263 loss: 6.06160825e-07
Iter: 1264 loss: 6.05858759e-07
Iter: 1265 loss: 6.05337732e-07
Iter: 1266 loss: 6.05334662e-07
Iter: 1267 loss: 6.04859e-07
Iter: 1268 loss: 6.10540098e-07
Iter: 1269 loss: 6.04867637e-07
Iter: 1270 loss: 6.04433637e-07
Iter: 1271 loss: 6.04858e-07
Iter: 1272 loss: 6.04213142e-07
Iter: 1273 loss: 6.0374856e-07
Iter: 1274 loss: 6.04593481e-07
Iter: 1275 loss: 6.03536307e-07
Iter: 1276 loss: 6.03130616e-07
Iter: 1277 loss: 6.06499043e-07
Iter: 1278 loss: 6.0310316e-07
Iter: 1279 loss: 6.02762725e-07
Iter: 1280 loss: 6.03675687e-07
Iter: 1281 loss: 6.02631076e-07
Iter: 1282 loss: 6.02310195e-07
Iter: 1283 loss: 6.02134264e-07
Iter: 1284 loss: 6.01979878e-07
Iter: 1285 loss: 6.0156367e-07
Iter: 1286 loss: 6.031064e-07
Iter: 1287 loss: 6.01453962e-07
Iter: 1288 loss: 6.01081126e-07
Iter: 1289 loss: 6.01087095e-07
Iter: 1290 loss: 6.0086893e-07
Iter: 1291 loss: 6.00361545e-07
Iter: 1292 loss: 6.05802256e-07
Iter: 1293 loss: 6.00282874e-07
Iter: 1294 loss: 5.99713e-07
Iter: 1295 loss: 6.02746809e-07
Iter: 1296 loss: 5.99644522e-07
Iter: 1297 loss: 5.99212228e-07
Iter: 1298 loss: 6.04732e-07
Iter: 1299 loss: 5.992311e-07
Iter: 1300 loss: 5.98976726e-07
Iter: 1301 loss: 5.98463316e-07
Iter: 1302 loss: 6.06399567e-07
Iter: 1303 loss: 5.98423753e-07
Iter: 1304 loss: 5.98100087e-07
Iter: 1305 loss: 5.98073484e-07
Iter: 1306 loss: 5.97799044e-07
Iter: 1307 loss: 5.97622261e-07
Iter: 1308 loss: 5.97502321e-07
Iter: 1309 loss: 5.97092594e-07
Iter: 1310 loss: 5.98254246e-07
Iter: 1311 loss: 5.96945711e-07
Iter: 1312 loss: 5.96610789e-07
Iter: 1313 loss: 6.00426347e-07
Iter: 1314 loss: 5.9661636e-07
Iter: 1315 loss: 5.963376e-07
Iter: 1316 loss: 5.96184179e-07
Iter: 1317 loss: 5.96047244e-07
Iter: 1318 loss: 5.9564718e-07
Iter: 1319 loss: 5.96280756e-07
Iter: 1320 loss: 5.9544027e-07
Iter: 1321 loss: 5.95058054e-07
Iter: 1322 loss: 5.98325073e-07
Iter: 1323 loss: 5.95052938e-07
Iter: 1324 loss: 5.94661174e-07
Iter: 1325 loss: 5.95810093e-07
Iter: 1326 loss: 5.94556e-07
Iter: 1327 loss: 5.94354e-07
Iter: 1328 loss: 5.93938239e-07
Iter: 1329 loss: 6.01127169e-07
Iter: 1330 loss: 5.93930736e-07
Iter: 1331 loss: 5.93345874e-07
Iter: 1332 loss: 5.96804114e-07
Iter: 1333 loss: 5.9329119e-07
Iter: 1334 loss: 5.92810579e-07
Iter: 1335 loss: 5.96411724e-07
Iter: 1336 loss: 5.92766071e-07
Iter: 1337 loss: 5.92531933e-07
Iter: 1338 loss: 5.92029323e-07
Iter: 1339 loss: 6.02746638e-07
Iter: 1340 loss: 5.92025344e-07
Iter: 1341 loss: 5.91625e-07
Iter: 1342 loss: 5.91604476e-07
Iter: 1343 loss: 5.91260687e-07
Iter: 1344 loss: 5.91020694e-07
Iter: 1345 loss: 5.90910076e-07
Iter: 1346 loss: 5.90505579e-07
Iter: 1347 loss: 5.9386997e-07
Iter: 1348 loss: 5.90494e-07
Iter: 1349 loss: 5.90177592e-07
Iter: 1350 loss: 5.9120913e-07
Iter: 1351 loss: 5.90062314e-07
Iter: 1352 loss: 5.89708463e-07
Iter: 1353 loss: 5.89792194e-07
Iter: 1354 loss: 5.89424189e-07
Iter: 1355 loss: 5.89099841e-07
Iter: 1356 loss: 5.89823287e-07
Iter: 1357 loss: 5.889666e-07
Iter: 1358 loss: 5.88719331e-07
Iter: 1359 loss: 5.88729222e-07
Iter: 1360 loss: 5.88488774e-07
Iter: 1361 loss: 5.88101614e-07
Iter: 1362 loss: 5.88104967e-07
Iter: 1363 loss: 5.8773287e-07
Iter: 1364 loss: 5.87997192e-07
Iter: 1365 loss: 5.87490604e-07
Iter: 1366 loss: 5.87189334e-07
Iter: 1367 loss: 5.8719047e-07
Iter: 1368 loss: 5.86893e-07
Iter: 1369 loss: 5.8660936e-07
Iter: 1370 loss: 5.86562919e-07
Iter: 1371 loss: 5.86080091e-07
Iter: 1372 loss: 5.86815759e-07
Iter: 1373 loss: 5.85859e-07
Iter: 1374 loss: 5.85453677e-07
Iter: 1375 loss: 5.90081868e-07
Iter: 1376 loss: 5.85444582e-07
Iter: 1377 loss: 5.85126827e-07
Iter: 1378 loss: 5.8525211e-07
Iter: 1379 loss: 5.8489826e-07
Iter: 1380 loss: 5.84615464e-07
Iter: 1381 loss: 5.86271426e-07
Iter: 1382 loss: 5.84570728e-07
Iter: 1383 loss: 5.84202894e-07
Iter: 1384 loss: 5.84453801e-07
Iter: 1385 loss: 5.83984558e-07
Iter: 1386 loss: 5.83609449e-07
Iter: 1387 loss: 5.84418e-07
Iter: 1388 loss: 5.83502072e-07
Iter: 1389 loss: 5.83102519e-07
Iter: 1390 loss: 5.82792381e-07
Iter: 1391 loss: 5.82683867e-07
Iter: 1392 loss: 5.82777602e-07
Iter: 1393 loss: 5.8241551e-07
Iter: 1394 loss: 5.82253506e-07
Iter: 1395 loss: 5.81894824e-07
Iter: 1396 loss: 5.87813759e-07
Iter: 1397 loss: 5.81846791e-07
Iter: 1398 loss: 5.814112e-07
Iter: 1399 loss: 5.81478844e-07
Iter: 1400 loss: 5.81108168e-07
Iter: 1401 loss: 5.80703954e-07
Iter: 1402 loss: 5.84189081e-07
Iter: 1403 loss: 5.80668484e-07
Iter: 1404 loss: 5.8020521e-07
Iter: 1405 loss: 5.80997778e-07
Iter: 1406 loss: 5.79981247e-07
Iter: 1407 loss: 5.7971755e-07
Iter: 1408 loss: 5.7977428e-07
Iter: 1409 loss: 5.79531843e-07
Iter: 1410 loss: 5.79180437e-07
Iter: 1411 loss: 5.82317796e-07
Iter: 1412 loss: 5.79152e-07
Iter: 1413 loss: 5.78829599e-07
Iter: 1414 loss: 5.79151845e-07
Iter: 1415 loss: 5.786315e-07
Iter: 1416 loss: 5.78349272e-07
Iter: 1417 loss: 5.79268033e-07
Iter: 1418 loss: 5.78273671e-07
Iter: 1419 loss: 5.77866444e-07
Iter: 1420 loss: 5.78788445e-07
Iter: 1421 loss: 5.77717856e-07
Iter: 1422 loss: 5.77426874e-07
Iter: 1423 loss: 5.77113838e-07
Iter: 1424 loss: 5.77028516e-07
Iter: 1425 loss: 5.7652619e-07
Iter: 1426 loss: 5.79864263e-07
Iter: 1427 loss: 5.76501634e-07
Iter: 1428 loss: 5.76161483e-07
Iter: 1429 loss: 5.76167508e-07
Iter: 1430 loss: 5.75945592e-07
Iter: 1431 loss: 5.75449519e-07
Iter: 1432 loss: 5.82583368e-07
Iter: 1433 loss: 5.75439e-07
Iter: 1434 loss: 5.74868523e-07
Iter: 1435 loss: 5.76048706e-07
Iter: 1436 loss: 5.74611818e-07
Iter: 1437 loss: 5.74171395e-07
Iter: 1438 loss: 5.79095854e-07
Iter: 1439 loss: 5.74143826e-07
Iter: 1440 loss: 5.73749332e-07
Iter: 1441 loss: 5.75234651e-07
Iter: 1442 loss: 5.73648663e-07
Iter: 1443 loss: 5.73391958e-07
Iter: 1444 loss: 5.73122634e-07
Iter: 1445 loss: 5.73059708e-07
Iter: 1446 loss: 5.7262946e-07
Iter: 1447 loss: 5.75008585e-07
Iter: 1448 loss: 5.72556132e-07
Iter: 1449 loss: 5.72129295e-07
Iter: 1450 loss: 5.74622788e-07
Iter: 1451 loss: 5.72084e-07
Iter: 1452 loss: 5.71822397e-07
Iter: 1453 loss: 5.71846215e-07
Iter: 1454 loss: 5.71609576e-07
Iter: 1455 loss: 5.71185865e-07
Iter: 1456 loss: 5.73978753e-07
Iter: 1457 loss: 5.71151475e-07
Iter: 1458 loss: 5.70877887e-07
Iter: 1459 loss: 5.70358168e-07
Iter: 1460 loss: 5.82415055e-07
Iter: 1461 loss: 5.70371469e-07
Iter: 1462 loss: 5.70113968e-07
Iter: 1463 loss: 5.70079123e-07
Iter: 1464 loss: 5.69762562e-07
Iter: 1465 loss: 5.69778194e-07
Iter: 1466 loss: 5.69474537e-07
Iter: 1467 loss: 5.69195493e-07
Iter: 1468 loss: 5.69775466e-07
Iter: 1469 loss: 5.69083e-07
Iter: 1470 loss: 5.68799805e-07
Iter: 1471 loss: 5.68634334e-07
Iter: 1472 loss: 5.6851917e-07
Iter: 1473 loss: 5.68135533e-07
Iter: 1474 loss: 5.68156338e-07
Iter: 1475 loss: 5.67870416e-07
Iter: 1476 loss: 5.67707559e-07
Iter: 1477 loss: 5.67585914e-07
Iter: 1478 loss: 5.67156917e-07
Iter: 1479 loss: 5.67209781e-07
Iter: 1480 loss: 5.66837912e-07
Iter: 1481 loss: 5.66479457e-07
Iter: 1482 loss: 5.71177736e-07
Iter: 1483 loss: 5.66484232e-07
Iter: 1484 loss: 5.66103949e-07
Iter: 1485 loss: 5.67055622e-07
Iter: 1486 loss: 5.65970424e-07
Iter: 1487 loss: 5.65749815e-07
Iter: 1488 loss: 5.66857352e-07
Iter: 1489 loss: 5.65706387e-07
Iter: 1490 loss: 5.65451046e-07
Iter: 1491 loss: 5.65600544e-07
Iter: 1492 loss: 5.65276423e-07
Iter: 1493 loss: 5.64938091e-07
Iter: 1494 loss: 5.64553034e-07
Iter: 1495 loss: 5.64499942e-07
Iter: 1496 loss: 5.64481752e-07
Iter: 1497 loss: 5.64303491e-07
Iter: 1498 loss: 5.6408868e-07
Iter: 1499 loss: 5.63602157e-07
Iter: 1500 loss: 5.71240548e-07
Iter: 1501 loss: 5.635859e-07
Iter: 1502 loss: 5.63125809e-07
Iter: 1503 loss: 5.63913375e-07
Iter: 1504 loss: 5.62936634e-07
Iter: 1505 loss: 5.62487116e-07
Iter: 1506 loss: 5.65409721e-07
Iter: 1507 loss: 5.62448122e-07
Iter: 1508 loss: 5.62188291e-07
Iter: 1509 loss: 5.64394e-07
Iter: 1510 loss: 5.62153218e-07
Iter: 1511 loss: 5.61840864e-07
Iter: 1512 loss: 5.61736272e-07
Iter: 1513 loss: 5.61570971e-07
Iter: 1514 loss: 5.6119535e-07
Iter: 1515 loss: 5.61589104e-07
Iter: 1516 loss: 5.60977128e-07
Iter: 1517 loss: 5.60573881e-07
Iter: 1518 loss: 5.61557044e-07
Iter: 1519 loss: 5.60417561e-07
Iter: 1520 loss: 5.60037108e-07
Iter: 1521 loss: 5.65625271e-07
Iter: 1522 loss: 5.6003455e-07
Iter: 1523 loss: 5.59777391e-07
Iter: 1524 loss: 5.59649493e-07
Iter: 1525 loss: 5.5951341e-07
Iter: 1526 loss: 5.59131081e-07
Iter: 1527 loss: 5.62546234e-07
Iter: 1528 loss: 5.59117154e-07
Iter: 1529 loss: 5.58816964e-07
Iter: 1530 loss: 5.58310262e-07
Iter: 1531 loss: 5.58306283e-07
Iter: 1532 loss: 5.5800848e-07
Iter: 1533 loss: 5.58001489e-07
Iter: 1534 loss: 5.57703174e-07
Iter: 1535 loss: 5.59060652e-07
Iter: 1536 loss: 5.5763843e-07
Iter: 1537 loss: 5.57487965e-07
Iter: 1538 loss: 5.57101373e-07
Iter: 1539 loss: 5.59675698e-07
Iter: 1540 loss: 5.56965631e-07
Iter: 1541 loss: 5.56463306e-07
Iter: 1542 loss: 5.58012289e-07
Iter: 1543 loss: 5.56317104e-07
Iter: 1544 loss: 5.56105533e-07
Iter: 1545 loss: 5.560251e-07
Iter: 1546 loss: 5.55781639e-07
Iter: 1547 loss: 5.55712404e-07
Iter: 1548 loss: 5.5557939e-07
Iter: 1549 loss: 5.55203769e-07
Iter: 1550 loss: 5.55309498e-07
Iter: 1551 loss: 5.54906705e-07
Iter: 1552 loss: 5.54525286e-07
Iter: 1553 loss: 5.56854729e-07
Iter: 1554 loss: 5.54478e-07
Iter: 1555 loss: 5.541267e-07
Iter: 1556 loss: 5.55560291e-07
Iter: 1557 loss: 5.54039161e-07
Iter: 1558 loss: 5.53715722e-07
Iter: 1559 loss: 5.55173926e-07
Iter: 1560 loss: 5.53639e-07
Iter: 1561 loss: 5.53446853e-07
Iter: 1562 loss: 5.54107032e-07
Iter: 1563 loss: 5.53393875e-07
Iter: 1564 loss: 5.53128132e-07
Iter: 1565 loss: 5.52983352e-07
Iter: 1566 loss: 5.52873e-07
Iter: 1567 loss: 5.52697486e-07
Iter: 1568 loss: 5.52681627e-07
Iter: 1569 loss: 5.52477616e-07
Iter: 1570 loss: 5.52106485e-07
Iter: 1571 loss: 5.52117228e-07
Iter: 1572 loss: 5.51743938e-07
Iter: 1573 loss: 5.51710968e-07
Iter: 1574 loss: 5.51451478e-07
Iter: 1575 loss: 5.50894356e-07
Iter: 1576 loss: 5.52028439e-07
Iter: 1577 loss: 5.50719733e-07
Iter: 1578 loss: 5.5028454e-07
Iter: 1579 loss: 5.53048e-07
Iter: 1580 loss: 5.5027192e-07
Iter: 1581 loss: 5.49954166e-07
Iter: 1582 loss: 5.49941319e-07
Iter: 1583 loss: 5.49777042e-07
Iter: 1584 loss: 5.49423874e-07
Iter: 1585 loss: 5.55991051e-07
Iter: 1586 loss: 5.49412391e-07
Iter: 1587 loss: 5.4899e-07
Iter: 1588 loss: 5.50139134e-07
Iter: 1589 loss: 5.48856235e-07
Iter: 1590 loss: 5.48547177e-07
Iter: 1591 loss: 5.48543653e-07
Iter: 1592 loss: 5.48251705e-07
Iter: 1593 loss: 5.48497212e-07
Iter: 1594 loss: 5.4807623e-07
Iter: 1595 loss: 5.47818331e-07
Iter: 1596 loss: 5.4875818e-07
Iter: 1597 loss: 5.47742161e-07
Iter: 1598 loss: 5.47437e-07
Iter: 1599 loss: 5.4832924e-07
Iter: 1600 loss: 5.47329705e-07
Iter: 1601 loss: 5.47104207e-07
Iter: 1602 loss: 5.48894377e-07
Iter: 1603 loss: 5.47080901e-07
Iter: 1604 loss: 5.46845513e-07
Iter: 1605 loss: 5.47035199e-07
Iter: 1606 loss: 5.46726255e-07
Iter: 1607 loss: 5.46485239e-07
Iter: 1608 loss: 5.46244451e-07
Iter: 1609 loss: 5.46195906e-07
Iter: 1610 loss: 5.4577481e-07
Iter: 1611 loss: 5.46460342e-07
Iter: 1612 loss: 5.45600244e-07
Iter: 1613 loss: 5.45135379e-07
Iter: 1614 loss: 5.45299258e-07
Iter: 1615 loss: 5.44802845e-07
Iter: 1616 loss: 5.44213265e-07
Iter: 1617 loss: 5.458744e-07
Iter: 1618 loss: 5.44031309e-07
Iter: 1619 loss: 5.43650117e-07
Iter: 1620 loss: 5.43646e-07
Iter: 1621 loss: 5.43418196e-07
Iter: 1622 loss: 5.45393618e-07
Iter: 1623 loss: 5.43387841e-07
Iter: 1624 loss: 5.43166266e-07
Iter: 1625 loss: 5.42706402e-07
Iter: 1626 loss: 5.50681193e-07
Iter: 1627 loss: 5.42674343e-07
Iter: 1628 loss: 5.42300029e-07
Iter: 1629 loss: 5.44011527e-07
Iter: 1630 loss: 5.42230282e-07
Iter: 1631 loss: 5.41955615e-07
Iter: 1632 loss: 5.4195317e-07
Iter: 1633 loss: 5.41699876e-07
Iter: 1634 loss: 5.41506324e-07
Iter: 1635 loss: 5.41429074e-07
Iter: 1636 loss: 5.41183738e-07
Iter: 1637 loss: 5.41185784e-07
Iter: 1638 loss: 5.40960855e-07
Iter: 1639 loss: 5.40702217e-07
Iter: 1640 loss: 5.40684709e-07
Iter: 1641 loss: 5.40399355e-07
Iter: 1642 loss: 5.41884845e-07
Iter: 1643 loss: 5.40359e-07
Iter: 1644 loss: 5.40070914e-07
Iter: 1645 loss: 5.40166127e-07
Iter: 1646 loss: 5.39883445e-07
Iter: 1647 loss: 5.39598886e-07
Iter: 1648 loss: 5.40761675e-07
Iter: 1649 loss: 5.39527264e-07
Iter: 1650 loss: 5.3922372e-07
Iter: 1651 loss: 5.38999871e-07
Iter: 1652 loss: 5.38915174e-07
Iter: 1653 loss: 5.38530571e-07
Iter: 1654 loss: 5.41319537e-07
Iter: 1655 loss: 5.38511927e-07
Iter: 1656 loss: 5.38164045e-07
Iter: 1657 loss: 5.40689371e-07
Iter: 1658 loss: 5.38146537e-07
Iter: 1659 loss: 5.37933715e-07
Iter: 1660 loss: 5.37744256e-07
Iter: 1661 loss: 5.37669052e-07
Iter: 1662 loss: 5.37327196e-07
Iter: 1663 loss: 5.37517622e-07
Iter: 1664 loss: 5.37124095e-07
Iter: 1665 loss: 5.36852212e-07
Iter: 1666 loss: 5.36847892e-07
Iter: 1667 loss: 5.3656936e-07
Iter: 1668 loss: 5.36648031e-07
Iter: 1669 loss: 5.36377797e-07
Iter: 1670 loss: 5.36199252e-07
Iter: 1671 loss: 5.36190612e-07
Iter: 1672 loss: 5.36017296e-07
Iter: 1673 loss: 5.3567544e-07
Iter: 1674 loss: 5.42974362e-07
Iter: 1675 loss: 5.35644119e-07
Iter: 1676 loss: 5.35366212e-07
Iter: 1677 loss: 5.37419623e-07
Iter: 1678 loss: 5.35338415e-07
Iter: 1679 loss: 5.35083132e-07
Iter: 1680 loss: 5.35835397e-07
Iter: 1681 loss: 5.35004119e-07
Iter: 1682 loss: 5.34659762e-07
Iter: 1683 loss: 5.34315461e-07
Iter: 1684 loss: 5.34248898e-07
Iter: 1685 loss: 5.33884531e-07
Iter: 1686 loss: 5.3593817e-07
Iter: 1687 loss: 5.33833941e-07
Iter: 1688 loss: 5.33466789e-07
Iter: 1689 loss: 5.35669471e-07
Iter: 1690 loss: 5.33424782e-07
Iter: 1691 loss: 5.33174898e-07
Iter: 1692 loss: 5.35199945e-07
Iter: 1693 loss: 5.33150342e-07
Iter: 1694 loss: 5.32953322e-07
Iter: 1695 loss: 5.32644208e-07
Iter: 1696 loss: 5.3263193e-07
Iter: 1697 loss: 5.32301954e-07
Iter: 1698 loss: 5.33601337e-07
Iter: 1699 loss: 5.3222584e-07
Iter: 1700 loss: 5.31919341e-07
Iter: 1701 loss: 5.33476282e-07
Iter: 1702 loss: 5.31868864e-07
Iter: 1703 loss: 5.31479373e-07
Iter: 1704 loss: 5.32323838e-07
Iter: 1705 loss: 5.31340788e-07
Iter: 1706 loss: 5.31151954e-07
Iter: 1707 loss: 5.34001344e-07
Iter: 1708 loss: 5.31151e-07
Iter: 1709 loss: 5.30991485e-07
Iter: 1710 loss: 5.30619502e-07
Iter: 1711 loss: 5.35526453e-07
Iter: 1712 loss: 5.30601369e-07
Iter: 1713 loss: 5.30228e-07
Iter: 1714 loss: 5.31127284e-07
Iter: 1715 loss: 5.30087505e-07
Iter: 1716 loss: 5.29745137e-07
Iter: 1717 loss: 5.33658294e-07
Iter: 1718 loss: 5.29732347e-07
Iter: 1719 loss: 5.29458475e-07
Iter: 1720 loss: 5.2960462e-07
Iter: 1721 loss: 5.29274303e-07
Iter: 1722 loss: 5.28972294e-07
Iter: 1723 loss: 5.28585929e-07
Iter: 1724 loss: 5.28546593e-07
Iter: 1725 loss: 5.28337182e-07
Iter: 1726 loss: 5.28304838e-07
Iter: 1727 loss: 5.28023634e-07
Iter: 1728 loss: 5.28186661e-07
Iter: 1729 loss: 5.27837358e-07
Iter: 1730 loss: 5.27471968e-07
Iter: 1731 loss: 5.27378688e-07
Iter: 1732 loss: 5.27160694e-07
Iter: 1733 loss: 5.26773e-07
Iter: 1734 loss: 5.2776727e-07
Iter: 1735 loss: 5.2662125e-07
Iter: 1736 loss: 5.26388249e-07
Iter: 1737 loss: 5.26350163e-07
Iter: 1738 loss: 5.26184408e-07
Iter: 1739 loss: 5.26307417e-07
Iter: 1740 loss: 5.26077883e-07
Iter: 1741 loss: 5.25812538e-07
Iter: 1742 loss: 5.2662773e-07
Iter: 1743 loss: 5.25735e-07
Iter: 1744 loss: 5.25481596e-07
Iter: 1745 loss: 5.25479436e-07
Iter: 1746 loss: 5.2526957e-07
Iter: 1747 loss: 5.24988195e-07
Iter: 1748 loss: 5.25022074e-07
Iter: 1749 loss: 5.24779807e-07
Iter: 1750 loss: 5.24508096e-07
Iter: 1751 loss: 5.27625673e-07
Iter: 1752 loss: 5.24493146e-07
Iter: 1753 loss: 5.24225868e-07
Iter: 1754 loss: 5.24614052e-07
Iter: 1755 loss: 5.24086e-07
Iter: 1756 loss: 5.2381472e-07
Iter: 1757 loss: 5.23674316e-07
Iter: 1758 loss: 5.23552615e-07
Iter: 1759 loss: 5.23230085e-07
Iter: 1760 loss: 5.25724886e-07
Iter: 1761 loss: 5.23168467e-07
Iter: 1762 loss: 5.22886353e-07
Iter: 1763 loss: 5.25650535e-07
Iter: 1764 loss: 5.22875553e-07
Iter: 1765 loss: 5.22699224e-07
Iter: 1766 loss: 5.22412961e-07
Iter: 1767 loss: 5.22399091e-07
Iter: 1768 loss: 5.22043251e-07
Iter: 1769 loss: 5.22796199e-07
Iter: 1770 loss: 5.21920697e-07
Iter: 1771 loss: 5.21729362e-07
Iter: 1772 loss: 5.21689458e-07
Iter: 1773 loss: 5.21559173e-07
Iter: 1774 loss: 5.2137716e-07
Iter: 1775 loss: 5.21359425e-07
Iter: 1776 loss: 5.21020866e-07
Iter: 1777 loss: 5.22413416e-07
Iter: 1778 loss: 5.20955155e-07
Iter: 1779 loss: 5.20722892e-07
Iter: 1780 loss: 5.2035449e-07
Iter: 1781 loss: 5.29818294e-07
Iter: 1782 loss: 5.20342269e-07
Iter: 1783 loss: 5.20001777e-07
Iter: 1784 loss: 5.24035613e-07
Iter: 1785 loss: 5.19994e-07
Iter: 1786 loss: 5.19753542e-07
Iter: 1787 loss: 5.20445042e-07
Iter: 1788 loss: 5.19664297e-07
Iter: 1789 loss: 5.19393552e-07
Iter: 1790 loss: 5.20202491e-07
Iter: 1791 loss: 5.19332048e-07
Iter: 1792 loss: 5.19072671e-07
Iter: 1793 loss: 5.18950799e-07
Iter: 1794 loss: 5.1880113e-07
Iter: 1795 loss: 5.18451543e-07
Iter: 1796 loss: 5.19784351e-07
Iter: 1797 loss: 5.18369632e-07
Iter: 1798 loss: 5.18125205e-07
Iter: 1799 loss: 5.18125148e-07
Iter: 1800 loss: 5.17947228e-07
Iter: 1801 loss: 5.17499416e-07
Iter: 1802 loss: 5.23594167e-07
Iter: 1803 loss: 5.17453657e-07
Iter: 1804 loss: 5.17104752e-07
Iter: 1805 loss: 5.20553442e-07
Iter: 1806 loss: 5.17096964e-07
Iter: 1807 loss: 5.16892669e-07
Iter: 1808 loss: 5.16867487e-07
Iter: 1809 loss: 5.16707701e-07
Iter: 1810 loss: 5.16507953e-07
Iter: 1811 loss: 5.16491355e-07
Iter: 1812 loss: 5.16278362e-07
Iter: 1813 loss: 5.19214268e-07
Iter: 1814 loss: 5.16260627e-07
Iter: 1815 loss: 5.16109708e-07
Iter: 1816 loss: 5.15717147e-07
Iter: 1817 loss: 5.19587331e-07
Iter: 1818 loss: 5.15659792e-07
Iter: 1819 loss: 5.15221359e-07
Iter: 1820 loss: 5.15563158e-07
Iter: 1821 loss: 5.1497e-07
Iter: 1822 loss: 5.14713804e-07
Iter: 1823 loss: 5.14662815e-07
Iter: 1824 loss: 5.14396561e-07
Iter: 1825 loss: 5.14454314e-07
Iter: 1826 loss: 5.14188514e-07
Iter: 1827 loss: 5.13877239e-07
Iter: 1828 loss: 5.14199769e-07
Iter: 1829 loss: 5.13714781e-07
Iter: 1830 loss: 5.13388102e-07
Iter: 1831 loss: 5.15243073e-07
Iter: 1832 loss: 5.1333825e-07
Iter: 1833 loss: 5.13019e-07
Iter: 1834 loss: 5.14816293e-07
Iter: 1835 loss: 5.12964618e-07
Iter: 1836 loss: 5.12763336e-07
Iter: 1837 loss: 5.12892655e-07
Iter: 1838 loss: 5.1263396e-07
Iter: 1839 loss: 5.12353381e-07
Iter: 1840 loss: 5.12329734e-07
Iter: 1841 loss: 5.12111797e-07
Iter: 1842 loss: 5.12005101e-07
Iter: 1843 loss: 5.11906251e-07
Iter: 1844 loss: 5.11773e-07
Iter: 1845 loss: 5.1149118e-07
Iter: 1846 loss: 5.17353726e-07
Iter: 1847 loss: 5.11493681e-07
Iter: 1848 loss: 5.11225721e-07
Iter: 1849 loss: 5.14122064e-07
Iter: 1850 loss: 5.11215717e-07
Iter: 1851 loss: 5.11001815e-07
Iter: 1852 loss: 5.10994e-07
Iter: 1853 loss: 5.10804625e-07
Iter: 1854 loss: 5.10568839e-07
Iter: 1855 loss: 5.1027763e-07
Iter: 1856 loss: 5.10249833e-07
Iter: 1857 loss: 5.09853692e-07
Iter: 1858 loss: 5.11447809e-07
Iter: 1859 loss: 5.09768142e-07
Iter: 1860 loss: 5.09391e-07
Iter: 1861 loss: 5.11027338e-07
Iter: 1862 loss: 5.09317715e-07
Iter: 1863 loss: 5.09056292e-07
Iter: 1864 loss: 5.12682e-07
Iter: 1865 loss: 5.0904714e-07
Iter: 1866 loss: 5.08811e-07
Iter: 1867 loss: 5.08598305e-07
Iter: 1868 loss: 5.08533788e-07
Iter: 1869 loss: 5.08196877e-07
Iter: 1870 loss: 5.09189135e-07
Iter: 1871 loss: 5.08069e-07
Iter: 1872 loss: 5.07745483e-07
Iter: 1873 loss: 5.11767439e-07
Iter: 1874 loss: 5.07723712e-07
Iter: 1875 loss: 5.07505206e-07
Iter: 1876 loss: 5.07273e-07
Iter: 1877 loss: 5.07234745e-07
Iter: 1878 loss: 5.06951721e-07
Iter: 1879 loss: 5.06946378e-07
Iter: 1880 loss: 5.066629e-07
Iter: 1881 loss: 5.07114123e-07
Iter: 1882 loss: 5.06576896e-07
Iter: 1883 loss: 5.06351682e-07
Iter: 1884 loss: 5.06368906e-07
Iter: 1885 loss: 5.06198774e-07
Iter: 1886 loss: 5.05972878e-07
Iter: 1887 loss: 5.07136122e-07
Iter: 1888 loss: 5.05932917e-07
Iter: 1889 loss: 5.05661035e-07
Iter: 1890 loss: 5.05993455e-07
Iter: 1891 loss: 5.05544449e-07
Iter: 1892 loss: 5.05312187e-07
Iter: 1893 loss: 5.0499284e-07
Iter: 1894 loss: 5.04986588e-07
Iter: 1895 loss: 5.04550485e-07
Iter: 1896 loss: 5.05107153e-07
Iter: 1897 loss: 5.04313164e-07
Iter: 1898 loss: 5.0393794e-07
Iter: 1899 loss: 5.09197946e-07
Iter: 1900 loss: 5.03909632e-07
Iter: 1901 loss: 5.03630417e-07
Iter: 1902 loss: 5.04040145e-07
Iter: 1903 loss: 5.03505248e-07
Iter: 1904 loss: 5.03197327e-07
Iter: 1905 loss: 5.06782214e-07
Iter: 1906 loss: 5.0320125e-07
Iter: 1907 loss: 5.02970806e-07
Iter: 1908 loss: 5.02549e-07
Iter: 1909 loss: 5.11893575e-07
Iter: 1910 loss: 5.02554826e-07
Iter: 1911 loss: 5.02501848e-07
Iter: 1912 loss: 5.02352918e-07
Iter: 1913 loss: 5.02218e-07
Iter: 1914 loss: 5.0219154e-07
Iter: 1915 loss: 5.02088e-07
Iter: 1916 loss: 5.01867078e-07
Iter: 1917 loss: 5.02946136e-07
Iter: 1918 loss: 5.01835757e-07
Iter: 1919 loss: 5.016459e-07
Iter: 1920 loss: 5.01510954e-07
Iter: 1921 loss: 5.01431373e-07
Iter: 1922 loss: 5.01157558e-07
Iter: 1923 loss: 5.01232876e-07
Iter: 1924 loss: 5.00979695e-07
Iter: 1925 loss: 5.00580086e-07
Iter: 1926 loss: 5.02008788e-07
Iter: 1927 loss: 5.00478563e-07
Iter: 1928 loss: 5.00337364e-07
Iter: 1929 loss: 5.00309e-07
Iter: 1930 loss: 5.00178e-07
Iter: 1931 loss: 4.99844873e-07
Iter: 1932 loss: 5.03605861e-07
Iter: 1933 loss: 4.99831458e-07
Iter: 1934 loss: 4.99468911e-07
Iter: 1935 loss: 4.99968451e-07
Iter: 1936 loss: 4.99290195e-07
Iter: 1937 loss: 4.99023031e-07
Iter: 1938 loss: 4.99028602e-07
Iter: 1939 loss: 4.98785084e-07
Iter: 1940 loss: 4.99575322e-07
Iter: 1941 loss: 4.98735687e-07
Iter: 1942 loss: 4.98491488e-07
Iter: 1943 loss: 4.98629106e-07
Iter: 1944 loss: 4.98355121e-07
Iter: 1945 loss: 4.98216536e-07
Iter: 1946 loss: 4.98222334e-07
Iter: 1947 loss: 4.98061127e-07
Iter: 1948 loss: 4.9786604e-07
Iter: 1949 loss: 4.97864562e-07
Iter: 1950 loss: 4.97630083e-07
Iter: 1951 loss: 5.00328213e-07
Iter: 1952 loss: 4.97618e-07
Iter: 1953 loss: 4.97481153e-07
Iter: 1954 loss: 4.9712105e-07
Iter: 1955 loss: 4.99620796e-07
Iter: 1956 loss: 4.97053065e-07
Iter: 1957 loss: 4.96673124e-07
Iter: 1958 loss: 5.00051101e-07
Iter: 1959 loss: 4.96645271e-07
Iter: 1960 loss: 4.96387258e-07
Iter: 1961 loss: 4.99196517e-07
Iter: 1962 loss: 4.96387145e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2
+ date
Wed Oct 21 12:50:13 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1 --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c1c9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c0d6488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c1fe9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c1fec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c11b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c07e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bff0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bfc2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bfc2488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c03be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c051840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf8f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf8fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf1e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c0a3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3c0a3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf8fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e169fdbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3beed950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e3bf04f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e16a2f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e16a2f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e169578c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e169b1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e16986488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e16986d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd00a87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd00b0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd00b0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8df0055950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd0017ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d487bc510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d487bc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d487e8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8dd0080a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8e169180d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.04727656
test_loss: 0.04593319
train_loss: 0.020359008
test_loss: 0.020845871
train_loss: 0.012422589
test_loss: 0.012919019
train_loss: 0.008681728
test_loss: 0.010014267
train_loss: 0.007508901
test_loss: 0.008326084
train_loss: 0.0070775202
test_loss: 0.007591439
train_loss: 0.0068660257
test_loss: 0.007374335
train_loss: 0.006080945
test_loss: 0.007047101
train_loss: 0.0059288573
test_loss: 0.006896944
train_loss: 0.0056730565
test_loss: 0.0066436795
train_loss: 0.0054848674
test_loss: 0.0066064075
train_loss: 0.0055770627
test_loss: 0.006447815
train_loss: 0.005030862
test_loss: 0.0062142513
train_loss: 0.0053278534
test_loss: 0.006308455
train_loss: 0.005508627
test_loss: 0.006254092
train_loss: 0.0052335486
test_loss: 0.0059769065
train_loss: 0.0052238847
test_loss: 0.0058520995
train_loss: 0.0052503035
test_loss: 0.0060572126
train_loss: 0.0049848347
test_loss: 0.0058310544
train_loss: 0.0050358595
test_loss: 0.005838017
train_loss: 0.0048251357
test_loss: 0.0057813325
train_loss: 0.0045970464
test_loss: 0.005680734
train_loss: 0.005254079
test_loss: 0.005992147
train_loss: 0.0051401136
test_loss: 0.005638298
train_loss: 0.0049516526
test_loss: 0.006040196
train_loss: 0.0049243188
test_loss: 0.0058531957
train_loss: 0.0049594105
test_loss: 0.0056235217
train_loss: 0.0045231264
test_loss: 0.00546072
train_loss: 0.0043745926
test_loss: 0.005543978
train_loss: 0.004616551
test_loss: 0.005645988
train_loss: 0.004455081
test_loss: 0.0053735687
train_loss: 0.004432856
test_loss: 0.005245917
train_loss: 0.0042045265
test_loss: 0.005195771
train_loss: 0.00458317
test_loss: 0.005483595
train_loss: 0.0048983945
test_loss: 0.0055129845
train_loss: 0.004615143
test_loss: 0.0053911014
train_loss: 0.004250705
test_loss: 0.005238079
train_loss: 0.0045403917
test_loss: 0.0054373527
train_loss: 0.004365779
test_loss: 0.0054899617
train_loss: 0.0039407113
test_loss: 0.0051720697
train_loss: 0.0043285526
test_loss: 0.0054733674
train_loss: 0.0043695047
test_loss: 0.00541137
train_loss: 0.0043933936
test_loss: 0.005496816
train_loss: 0.0043144957
test_loss: 0.0054034204
train_loss: 0.004173503
test_loss: 0.0053329626
train_loss: 0.0044706417
test_loss: 0.005421031
train_loss: 0.004827565
test_loss: 0.005529195
train_loss: 0.0044879937
test_loss: 0.0054503772
train_loss: 0.004344031
test_loss: 0.0052826703
train_loss: 0.003995328
test_loss: 0.005146161
train_loss: 0.0039687185
test_loss: 0.0050902455
train_loss: 0.004396828
test_loss: 0.005498315
train_loss: 0.0043527707
test_loss: 0.005422782
train_loss: 0.004341125
test_loss: 0.005199777
train_loss: 0.0044846903
test_loss: 0.005458369
train_loss: 0.004154704
test_loss: 0.0050623333
train_loss: 0.004050733
test_loss: 0.0050403574
train_loss: 0.004188638
test_loss: 0.0052478765
train_loss: 0.004090078
test_loss: 0.0050378595
train_loss: 0.0040287785
test_loss: 0.005103703
train_loss: 0.004470502
test_loss: 0.005316942
train_loss: 0.004007516
test_loss: 0.0049864785
train_loss: 0.0040696487
test_loss: 0.005383338
train_loss: 0.0042900415
test_loss: 0.0051232157
train_loss: 0.004276801
test_loss: 0.005227621
train_loss: 0.004370775
test_loss: 0.005151008
train_loss: 0.004278271
test_loss: 0.0049110292
train_loss: 0.0041910363
test_loss: 0.0052007353
train_loss: 0.0040061213
test_loss: 0.0050654765
train_loss: 0.004230417
test_loss: 0.0052400604
train_loss: 0.0040801372
test_loss: 0.005054953
train_loss: 0.0041322256
test_loss: 0.0054430296
train_loss: 0.0039695287
test_loss: 0.0054028886
train_loss: 0.004188831
test_loss: 0.005048675
train_loss: 0.004211356
test_loss: 0.005226396
train_loss: 0.0040747295
test_loss: 0.0050390055
train_loss: 0.0040901573
test_loss: 0.0050086533
train_loss: 0.004527921
test_loss: 0.0051036985
train_loss: 0.0042344187
test_loss: 0.0053009433
train_loss: 0.0041879914
test_loss: 0.005729495
train_loss: 0.003962545
test_loss: 0.0051319753
train_loss: 0.00396231
test_loss: 0.0053949202
train_loss: 0.00406551
test_loss: 0.0050234813
train_loss: 0.004281554
test_loss: 0.0049675265
train_loss: 0.003840463
test_loss: 0.005004796
train_loss: 0.003861918
test_loss: 0.0048124064
train_loss: 0.004077878
test_loss: 0.0050758366
train_loss: 0.004348949
test_loss: 0.005189467
train_loss: 0.0042964127
test_loss: 0.005293627
train_loss: 0.004622271
test_loss: 0.0050062817
train_loss: 0.0036814963
test_loss: 0.004964522
train_loss: 0.0040515554
test_loss: 0.0048893
train_loss: 0.004003311
test_loss: 0.0050535263
train_loss: 0.0038656262
test_loss: 0.0052211843
train_loss: 0.004233581
test_loss: 0.0052128984
train_loss: 0.004244972
test_loss: 0.0050733113
train_loss: 0.003734096
test_loss: 0.0049749585
train_loss: 0.0039005298
test_loss: 0.0049818265
train_loss: 0.003852158
test_loss: 0.004963888
train_loss: 0.003809529
test_loss: 0.00485149
train_loss: 0.0038083717
test_loss: 0.0047299247
train_loss: 0.0038670884
test_loss: 0.004858309
train_loss: 0.004141531
test_loss: 0.0051075784
train_loss: 0.003854332
test_loss: 0.0051533887
train_loss: 0.003903283
test_loss: 0.0049368357
train_loss: 0.0035809367
test_loss: 0.004748202
train_loss: 0.0038458654
test_loss: 0.0049080527
train_loss: 0.0038158007
test_loss: 0.00491799
train_loss: 0.004049514
test_loss: 0.0050756894
train_loss: 0.003860538
test_loss: 0.0051745637
train_loss: 0.003849937
test_loss: 0.00490927
train_loss: 0.004208629
test_loss: 0.0052628107
train_loss: 0.004120264
test_loss: 0.005456044
train_loss: 0.003965639
test_loss: 0.004940358
train_loss: 0.004514213
test_loss: 0.0059202267
train_loss: 0.004546942
test_loss: 0.0053944564
train_loss: 0.0037996555
test_loss: 0.0050111846
train_loss: 0.0038276783
test_loss: 0.0052018655
train_loss: 0.0039272117
test_loss: 0.005350185
train_loss: 0.0044833925
test_loss: 0.0051778895
train_loss: 0.0037072478
test_loss: 0.004636128
train_loss: 0.004448153
test_loss: 0.0050402265
train_loss: 0.003690899
test_loss: 0.0049431333
train_loss: 0.0039519984
test_loss: 0.0050689736
train_loss: 0.0038204936
test_loss: 0.0049312045
train_loss: 0.003975883
test_loss: 0.0050026695
train_loss: 0.0039585894
test_loss: 0.0048604137
train_loss: 0.0039747055
test_loss: 0.0050108917
train_loss: 0.004116197
test_loss: 0.004873778
train_loss: 0.003840178
test_loss: 0.0050073643
train_loss: 0.0041151163
test_loss: 0.0051851054
train_loss: 0.0040529515
test_loss: 0.005340728
train_loss: 0.0042377613
test_loss: 0.0048807976
train_loss: 0.003853942
test_loss: 0.0047380463
train_loss: 0.0038662502
test_loss: 0.0050563496
train_loss: 0.003977609
test_loss: 0.0052877567
train_loss: 0.004229832
test_loss: 0.0052416623
train_loss: 0.004124572
test_loss: 0.004931567
train_loss: 0.0038881025
test_loss: 0.005167265
train_loss: 0.0041002464
test_loss: 0.005011109
train_loss: 0.003926753
test_loss: 0.0049422313
train_loss: 0.0038399552
test_loss: 0.004860274
train_loss: 0.0039294674
test_loss: 0.004936772
train_loss: 0.0043076114
test_loss: 0.005151269
train_loss: 0.0038964455
test_loss: 0.0051749563
train_loss: 0.0042674905
test_loss: 0.0055120415
train_loss: 0.004371381
test_loss: 0.0052845865
train_loss: 0.003917507
test_loss: 0.0053507416
train_loss: 0.0040365397
test_loss: 0.004937497
train_loss: 0.004112854
test_loss: 0.0051582004
train_loss: 0.00378283
test_loss: 0.005273264
train_loss: 0.0040811463
test_loss: 0.004974869
train_loss: 0.0037222137
test_loss: 0.0050213193
train_loss: 0.0037600438
test_loss: 0.0051019955
train_loss: 0.0040761176
test_loss: 0.005103604
train_loss: 0.0039141383
test_loss: 0.0049815676
train_loss: 0.003943599
test_loss: 0.004933741
train_loss: 0.0036051688
test_loss: 0.004821793
train_loss: 0.003996802
test_loss: 0.0049254415
train_loss: 0.0041669896
test_loss: 0.0051585496
train_loss: 0.003814748
test_loss: 0.0047874707
train_loss: 0.0036339108
test_loss: 0.004842961
train_loss: 0.0032806636
test_loss: 0.004777748
train_loss: 0.0036772594
test_loss: 0.0049174116
train_loss: 0.0035595447
test_loss: 0.0049722474
train_loss: 0.0039821295
test_loss: 0.0055728722
train_loss: 0.003976546
test_loss: 0.005032206
train_loss: 0.0037200756
test_loss: 0.005048874
train_loss: 0.003965466
test_loss: 0.00508684
train_loss: 0.0041211266
test_loss: 0.004866873
train_loss: 0.0040455298
test_loss: 0.0049285213
train_loss: 0.0037072406
test_loss: 0.0050595365
train_loss: 0.0037576396
test_loss: 0.0051021506
train_loss: 0.00394328
test_loss: 0.0050316104
train_loss: 0.004056524
test_loss: 0.0046314932
train_loss: 0.003911481
test_loss: 0.0051122336
train_loss: 0.00390288
test_loss: 0.00489292
train_loss: 0.0037824865
test_loss: 0.004989925
train_loss: 0.0035473034
test_loss: 0.004863419
train_loss: 0.0037664764
test_loss: 0.004689192
train_loss: 0.0037540945
test_loss: 0.0047975522
train_loss: 0.003950988
test_loss: 0.004949582
train_loss: 0.003888343
test_loss: 0.0051081227
train_loss: 0.0039047392
test_loss: 0.0051911385
train_loss: 0.00377404
test_loss: 0.004968389
train_loss: 0.0039344695
test_loss: 0.005116324
train_loss: 0.0035391035
test_loss: 0.0048437733
train_loss: 0.0037415815
test_loss: 0.004794464
train_loss: 0.0039209407
test_loss: 0.004901974
train_loss: 0.0037607695
test_loss: 0.0050636707
train_loss: 0.0038862037
test_loss: 0.0052259755
train_loss: 0.0042873253
test_loss: 0.0049492144
train_loss: 0.0038079913
test_loss: 0.004918185
train_loss: 0.0037005213
test_loss: 0.005089249
train_loss: 0.003431982
test_loss: 0.004877839
train_loss: 0.0036086128
test_loss: 0.0047208937
train_loss: 0.0037006603
test_loss: 0.004921338
train_loss: 0.0036298716
test_loss: 0.0049652276
train_loss: 0.0037296498
test_loss: 0.0048173307
train_loss: 0.0036027643
test_loss: 0.004906307
train_loss: 0.0035160019
test_loss: 0.0047667264
train_loss: 0.0038320138
test_loss: 0.0047839954
train_loss: 0.0037321148
test_loss: 0.004803929
train_loss: 0.0036232378
test_loss: 0.0049608443
train_loss: 0.0036300302
test_loss: 0.0047470443
train_loss: 0.0037102774
test_loss: 0.0049362793
train_loss: 0.0040042573
test_loss: 0.0052024634
train_loss: 0.0041158707
test_loss: 0.0049546733
train_loss: 0.0041469866
test_loss: 0.004946996
train_loss: 0.0035444396
test_loss: 0.0049022315
train_loss: 0.0037661796
test_loss: 0.004641427
train_loss: 0.0038128847
test_loss: 0.004902811
train_loss: 0.0038745464
test_loss: 0.0049332287
train_loss: 0.0041110376
test_loss: 0.0050099217
train_loss: 0.0039720386
test_loss: 0.0051286533
train_loss: 0.0037678874
test_loss: 0.004886622
train_loss: 0.004087669
test_loss: 0.0051859133
train_loss: 0.0038763434
test_loss: 0.005125674
train_loss: 0.0035955287
test_loss: 0.004824592
train_loss: 0.0034081298
test_loss: 0.0047683967
train_loss: 0.0037911674
test_loss: 0.0048964852
train_loss: 0.0037684948
test_loss: 0.0047038835
train_loss: 0.0041774055
test_loss: 0.005015044
train_loss: 0.004058008
test_loss: 0.0048378007
train_loss: 0.0044563645
test_loss: 0.0054318244
train_loss: 0.0038768742
test_loss: 0.004977419
train_loss: 0.00397171
test_loss: 0.0048505454
train_loss: 0.0035615708
test_loss: 0.004926521
train_loss: 0.0037269536
test_loss: 0.0049402616
train_loss: 0.0039808885
test_loss: 0.0048241336
train_loss: 0.0036824988
test_loss: 0.005066928
train_loss: 0.0040922295
test_loss: 0.004890088
train_loss: 0.0037516146
test_loss: 0.004964741
train_loss: 0.00399173
test_loss: 0.0049610077
train_loss: 0.0038581018
test_loss: 0.0048372685
train_loss: 0.0038447117
test_loss: 0.0046807523
train_loss: 0.003529645
test_loss: 0.0048054433
train_loss: 0.003696106
test_loss: 0.0048628133
train_loss: 0.0037786337
test_loss: 0.0048463847
train_loss: 0.00389648
test_loss: 0.0049441196
train_loss: 0.003771523
test_loss: 0.004878116
train_loss: 0.003561622
test_loss: 0.0049103065
train_loss: 0.003283627
test_loss: 0.0046903347
train_loss: 0.0038562922
test_loss: 0.004814605
train_loss: 0.0037946096
test_loss: 0.004786944
train_loss: 0.003511081
test_loss: 0.0047318027
train_loss: 0.0039523984
test_loss: 0.004821574
train_loss: 0.0034490344
test_loss: 0.00475605
train_loss: 0.0037097821
test_loss: 0.0049733724
train_loss: 0.0038622105
test_loss: 0.004852749
train_loss: 0.0033625509
test_loss: 0.0047495333
train_loss: 0.0039992076
test_loss: 0.004892646
train_loss: 0.0037324708
test_loss: 0.0051407684
train_loss: 0.003599293
test_loss: 0.0049426183
train_loss: 0.0036131798
test_loss: 0.0048286193
train_loss: 0.0035765797
test_loss: 0.004919613
train_loss: 0.003663085
test_loss: 0.004808716
train_loss: 0.0039158044
test_loss: 0.005048465
train_loss: 0.003322386
test_loss: 0.0046977303
train_loss: 0.0035637286
test_loss: 0.0048091314
train_loss: 0.003702079
test_loss: 0.0050267107
train_loss: 0.003567373
test_loss: 0.0048376545
train_loss: 0.0037503275
test_loss: 0.0049219183
train_loss: 0.0035771704
test_loss: 0.0048175137
train_loss: 0.0038435033
test_loss: 0.0047257356
train_loss: 0.0036536953
test_loss: 0.0049859425
train_loss: 0.0039665895
test_loss: 0.004996082
train_loss: 0.0035752114
test_loss: 0.0050314125
train_loss: 0.003708268
test_loss: 0.004878426
train_loss: 0.003476856
test_loss: 0.0046565323
train_loss: 0.0038073263
test_loss: 0.0052709184
train_loss: 0.0037491038
test_loss: 0.005157239
train_loss: 0.003892885
test_loss: 0.004988088
train_loss: 0.003675954
test_loss: 0.0048428318
train_loss: 0.0036040666
test_loss: 0.0047763838
train_loss: 0.0035724523
test_loss: 0.004797819
train_loss: 0.003851192
test_loss: 0.004779588
train_loss: 0.0036379024
test_loss: 0.0051172874
train_loss: 0.0036419386
test_loss: 0.0048529683
train_loss: 0.0036641068
test_loss: 0.0050730407
train_loss: 0.0033207324
test_loss: 0.004816549
train_loss: 0.0036593867
test_loss: 0.004765007
train_loss: 0.003601563
test_loss: 0.0048208255
train_loss: 0.0040498935
test_loss: 0.0052548354
train_loss: 0.004216535
test_loss: 0.005115986
train_loss: 0.0034445403
test_loss: 0.004891754
train_loss: 0.0039063683
test_loss: 0.0050516496
train_loss: 0.003740353
test_loss: 0.0048036575
train_loss: 0.00455743
test_loss: 0.0049278084
train_loss: 0.003604249
test_loss: 0.00480437
train_loss: 0.0036901748
test_loss: 0.004742387
train_loss: 0.0038011344
test_loss: 0.0048483755
train_loss: 0.0039541656
test_loss: 0.004903208
train_loss: 0.0037479568
test_loss: 0.004778453
train_loss: 0.0037362783
test_loss: 0.00487643
train_loss: 0.003569607
test_loss: 0.0048808493
train_loss: 0.0035378523
test_loss: 0.004611264
train_loss: 0.0032953059
test_loss: 0.0048598917
train_loss: 0.0034989503
test_loss: 0.0048073838
train_loss: 0.0038382118
test_loss: 0.004934508
train_loss: 0.0036582276
test_loss: 0.0048824293
train_loss: 0.00396282
test_loss: 0.004711601
train_loss: 0.0037907823
test_loss: 0.004908465
train_loss: 0.0037003588
test_loss: 0.004931736
train_loss: 0.003879197
test_loss: 0.0048509496
train_loss: 0.003970066
test_loss: 0.005178622
train_loss: 0.0040620547
test_loss: 0.004807851
train_loss: 0.0034926604
test_loss: 0.0045962287
train_loss: 0.003913221
test_loss: 0.00489393
train_loss: 0.0036599962
test_loss: 0.004917236
train_loss: 0.0039206184
test_loss: 0.0048973374
train_loss: 0.0037983404
test_loss: 0.0048753195
train_loss: 0.0037766013
test_loss: 0.004811995
train_loss: 0.0035362511
test_loss: 0.004782578
train_loss: 0.004312228
test_loss: 0.0049737548
train_loss: 0.0037195615
test_loss: 0.0048246905
train_loss: 0.0035572296
test_loss: 0.00490188
train_loss: 0.0036038803
test_loss: 0.0049216617
train_loss: 0.0037860763
test_loss: 0.0051057674
train_loss: 0.0038883863
test_loss: 0.0049812035
train_loss: 0.0036427223
test_loss: 0.0047516758
train_loss: 0.003776729
test_loss: 0.004851938
train_loss: 0.0039195246
test_loss: 0.0048897346
train_loss: 0.0038902499
test_loss: 0.004900598
train_loss: 0.0037145508
test_loss: 0.004710713
train_loss: 0.003312184
test_loss: 0.0046347575
train_loss: 0.0033938938
test_loss: 0.0048259455
train_loss: 0.003648201
test_loss: 0.0047019273
train_loss: 0.0037612522
test_loss: 0.00520391
train_loss: 0.003672793
test_loss: 0.004749397
train_loss: 0.0038904839
test_loss: 0.004914551
train_loss: 0.0035522443
test_loss: 0.005242437
train_loss: 0.0043468107
test_loss: 0.0049796416
train_loss: 0.0038637673
test_loss: 0.0049981177
train_loss: 0.004089058
test_loss: 0.004828428
train_loss: 0.0033134588
test_loss: 0.004659542
train_loss: 0.0036833468
test_loss: 0.004897387
train_loss: 0.003526947
test_loss: 0.0046550254
train_loss: 0.0036846504
test_loss: 0.0048910007
train_loss: 0.0032947916
test_loss: 0.004656339
train_loss: 0.0037518921
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.00491748
train_loss: 0.0037773207
test_loss: 0.0047932677
train_loss: 0.0036939448
test_loss: 0.0049175336
train_loss: 0.003745668
test_loss: 0.004673335
train_loss: 0.0037015053
test_loss: 0.004611061
train_loss: 0.003472522
test_loss: 0.004939607
train_loss: 0.0033002102
test_loss: 0.0046812017
train_loss: 0.003697203
test_loss: 0.0046533695
train_loss: 0.004150767
test_loss: 0.0050054826
train_loss: 0.0038694139
test_loss: 0.0049577495
train_loss: 0.0034809562
test_loss: 0.0048969663
train_loss: 0.0031691806
test_loss: 0.0048483075
train_loss: 0.003703667
test_loss: 0.0046880506
train_loss: 0.0034918059
test_loss: 0.0049812114
train_loss: 0.0038345787
test_loss: 0.0048520383
train_loss: 0.0038087736
test_loss: 0.0050681983
train_loss: 0.0036893548
test_loss: 0.004852346
train_loss: 0.0034846244
test_loss: 0.004621957
train_loss: 0.0034227418
test_loss: 0.004827002
train_loss: 0.0035287263
test_loss: 0.0049856976
train_loss: 0.0032755858
test_loss: 0.0047136103
train_loss: 0.0033541848
test_loss: 0.00471906
train_loss: 0.0038389787
test_loss: 0.0046294057
train_loss: 0.0036608658
test_loss: 0.0048499075
train_loss: 0.0037107347
test_loss: 0.0049177846
train_loss: 0.0037006314
test_loss: 0.004704923
train_loss: 0.0035241367
test_loss: 0.005058254
train_loss: 0.0033925944
test_loss: 0.004629879
train_loss: 0.003570295
test_loss: 0.004769428
train_loss: 0.003619008
test_loss: 0.0047128852
train_loss: 0.0033712944
test_loss: 0.004765036
train_loss: 0.0033661881
test_loss: 0.0046601538
train_loss: 0.0036110342
test_loss: 0.0049228705
train_loss: 0.0036711479
test_loss: 0.0048413556
train_loss: 0.0039656498
test_loss: 0.0050903563
train_loss: 0.0037513014
test_loss: 0.004807814
train_loss: 0.00345937
test_loss: 0.0047422014
train_loss: 0.003302209
test_loss: 0.004665612
train_loss: 0.00325434
test_loss: 0.0046447716
train_loss: 0.003593314
test_loss: 0.0048221
train_loss: 0.003548547
test_loss: 0.004798765
train_loss: 0.0036046212
test_loss: 0.0048847585
train_loss: 0.0037756732
test_loss: 0.0049653156
train_loss: 0.003723559
test_loss: 0.0048192535
train_loss: 0.0036715092
test_loss: 0.004856458
train_loss: 0.0034368187
test_loss: 0.005138211
train_loss: 0.003440099
test_loss: 0.0049758987
train_loss: 0.003595437
test_loss: 0.004681973
train_loss: 0.0033854526
test_loss: 0.0046673454
train_loss: 0.003344398
test_loss: 0.004659061
train_loss: 0.0035955035
test_loss: 0.004625402
train_loss: 0.003473172
test_loss: 0.0048095877
train_loss: 0.0036737993
test_loss: 0.004776191
train_loss: 0.0033062717
test_loss: 0.0047915187
train_loss: 0.0036162413
test_loss: 0.0048447866
train_loss: 0.0037277567
test_loss: 0.0046075084
train_loss: 0.003240052
test_loss: 0.004502551
train_loss: 0.003366332
test_loss: 0.00475155
train_loss: 0.0031512477
test_loss: 0.0047788117
train_loss: 0.0033541385
test_loss: 0.0046235244
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d9f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7deec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d9ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d29bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d43510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d43950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7c90bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7d430d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7c3e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7c3ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7c10b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7bbfd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7bb5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b56840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b27840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b27bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b452f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7b0ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7ac89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a66950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a668c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a20598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a55730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d7a00620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d79e4598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1c15ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1be2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1bfc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1bfc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1bc5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1b66950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1b8a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1b8ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1b34620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1af2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f16d1af28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.37979984e-05
Iter: 2 loss: 2.00734066e-05
Iter: 3 loss: 1.98604357e-05
Iter: 4 loss: 1.85861591e-05
Iter: 5 loss: 2.95694463e-05
Iter: 6 loss: 1.85166373e-05
Iter: 7 loss: 1.75345758e-05
Iter: 8 loss: 1.85106328e-05
Iter: 9 loss: 1.69821069e-05
Iter: 10 loss: 1.56597744e-05
Iter: 11 loss: 1.50086435e-05
Iter: 12 loss: 1.43753141e-05
Iter: 13 loss: 1.30340341e-05
Iter: 14 loss: 2.10407288e-05
Iter: 15 loss: 1.2864487e-05
Iter: 16 loss: 1.18328735e-05
Iter: 17 loss: 1.29643668e-05
Iter: 18 loss: 1.12712914e-05
Iter: 19 loss: 1.04086903e-05
Iter: 20 loss: 1.47030551e-05
Iter: 21 loss: 1.02621125e-05
Iter: 22 loss: 9.6544145e-06
Iter: 23 loss: 1.1926596e-05
Iter: 24 loss: 9.50703179e-06
Iter: 25 loss: 9.01402382e-06
Iter: 26 loss: 1.34773563e-05
Iter: 27 loss: 8.99142e-06
Iter: 28 loss: 8.68823099e-06
Iter: 29 loss: 8.29024793e-06
Iter: 30 loss: 8.26540054e-06
Iter: 31 loss: 7.66986159e-06
Iter: 32 loss: 1.02300583e-05
Iter: 33 loss: 7.5471562e-06
Iter: 34 loss: 7.0924707e-06
Iter: 35 loss: 7.60932335e-06
Iter: 36 loss: 6.84757651e-06
Iter: 37 loss: 6.44236843e-06
Iter: 38 loss: 9.5400892e-06
Iter: 39 loss: 6.41196266e-06
Iter: 40 loss: 6.33998707e-06
Iter: 41 loss: 6.27226427e-06
Iter: 42 loss: 6.13927523e-06
Iter: 43 loss: 5.97515555e-06
Iter: 44 loss: 5.96080281e-06
Iter: 45 loss: 5.7410607e-06
Iter: 46 loss: 7.1040331e-06
Iter: 47 loss: 5.71521332e-06
Iter: 48 loss: 5.58852753e-06
Iter: 49 loss: 5.52369602e-06
Iter: 50 loss: 5.46462661e-06
Iter: 51 loss: 5.26378744e-06
Iter: 52 loss: 5.86902752e-06
Iter: 53 loss: 5.203417e-06
Iter: 54 loss: 5.00308306e-06
Iter: 55 loss: 5.54604503e-06
Iter: 56 loss: 4.93731795e-06
Iter: 57 loss: 4.79714708e-06
Iter: 58 loss: 5.2442515e-06
Iter: 59 loss: 4.75701108e-06
Iter: 60 loss: 4.61417949e-06
Iter: 61 loss: 5.0772087e-06
Iter: 62 loss: 4.57410624e-06
Iter: 63 loss: 4.41454631e-06
Iter: 64 loss: 4.86618546e-06
Iter: 65 loss: 4.36421942e-06
Iter: 66 loss: 4.24622476e-06
Iter: 67 loss: 4.30293858e-06
Iter: 68 loss: 4.16674447e-06
Iter: 69 loss: 4.01927673e-06
Iter: 70 loss: 4.68120379e-06
Iter: 71 loss: 3.99051305e-06
Iter: 72 loss: 3.88383842e-06
Iter: 73 loss: 3.89412025e-06
Iter: 74 loss: 3.80156825e-06
Iter: 75 loss: 3.7089e-06
Iter: 76 loss: 3.70554471e-06
Iter: 77 loss: 3.63468962e-06
Iter: 78 loss: 4.45414207e-06
Iter: 79 loss: 3.63375761e-06
Iter: 80 loss: 3.59953106e-06
Iter: 81 loss: 3.52800407e-06
Iter: 82 loss: 4.741054e-06
Iter: 83 loss: 3.5263015e-06
Iter: 84 loss: 3.44726163e-06
Iter: 85 loss: 4.33360492e-06
Iter: 86 loss: 3.44572982e-06
Iter: 87 loss: 3.40523752e-06
Iter: 88 loss: 3.3436902e-06
Iter: 89 loss: 3.34276092e-06
Iter: 90 loss: 3.26602776e-06
Iter: 91 loss: 3.83028191e-06
Iter: 92 loss: 3.25964311e-06
Iter: 93 loss: 3.20700497e-06
Iter: 94 loss: 3.3958936e-06
Iter: 95 loss: 3.19345327e-06
Iter: 96 loss: 3.1326415e-06
Iter: 97 loss: 3.07977461e-06
Iter: 98 loss: 3.06346465e-06
Iter: 99 loss: 3.02397029e-06
Iter: 100 loss: 3.01965883e-06
Iter: 101 loss: 2.97814518e-06
Iter: 102 loss: 2.91228457e-06
Iter: 103 loss: 2.91168817e-06
Iter: 104 loss: 2.85222086e-06
Iter: 105 loss: 3.38087625e-06
Iter: 106 loss: 2.84926182e-06
Iter: 107 loss: 2.79919504e-06
Iter: 108 loss: 2.75318621e-06
Iter: 109 loss: 2.74100967e-06
Iter: 110 loss: 2.70164674e-06
Iter: 111 loss: 2.69908833e-06
Iter: 112 loss: 2.67061796e-06
Iter: 113 loss: 3.10302494e-06
Iter: 114 loss: 2.67066707e-06
Iter: 115 loss: 2.65094627e-06
Iter: 116 loss: 2.61486434e-06
Iter: 117 loss: 3.4523764e-06
Iter: 118 loss: 2.61485911e-06
Iter: 119 loss: 2.58815498e-06
Iter: 120 loss: 2.82357928e-06
Iter: 121 loss: 2.58684349e-06
Iter: 122 loss: 2.5590125e-06
Iter: 123 loss: 2.55460759e-06
Iter: 124 loss: 2.53534449e-06
Iter: 125 loss: 2.49855611e-06
Iter: 126 loss: 2.48258652e-06
Iter: 127 loss: 2.46367949e-06
Iter: 128 loss: 2.42752435e-06
Iter: 129 loss: 2.97072484e-06
Iter: 130 loss: 2.42749297e-06
Iter: 131 loss: 2.39622295e-06
Iter: 132 loss: 2.42907163e-06
Iter: 133 loss: 2.37885888e-06
Iter: 134 loss: 2.34845902e-06
Iter: 135 loss: 2.4816004e-06
Iter: 136 loss: 2.34227582e-06
Iter: 137 loss: 2.32174693e-06
Iter: 138 loss: 2.4337437e-06
Iter: 139 loss: 2.31865624e-06
Iter: 140 loss: 2.2934621e-06
Iter: 141 loss: 2.26942188e-06
Iter: 142 loss: 2.26368775e-06
Iter: 143 loss: 2.23628217e-06
Iter: 144 loss: 2.29141e-06
Iter: 145 loss: 2.22513063e-06
Iter: 146 loss: 2.1907922e-06
Iter: 147 loss: 2.35683819e-06
Iter: 148 loss: 2.1847186e-06
Iter: 149 loss: 2.18257856e-06
Iter: 150 loss: 2.17609886e-06
Iter: 151 loss: 2.16651961e-06
Iter: 152 loss: 2.14154784e-06
Iter: 153 loss: 2.33164383e-06
Iter: 154 loss: 2.13662179e-06
Iter: 155 loss: 2.10879034e-06
Iter: 156 loss: 2.24620635e-06
Iter: 157 loss: 2.10406051e-06
Iter: 158 loss: 2.08938536e-06
Iter: 159 loss: 2.31699664e-06
Iter: 160 loss: 2.08941287e-06
Iter: 161 loss: 2.07863582e-06
Iter: 162 loss: 2.06056757e-06
Iter: 163 loss: 2.06059849e-06
Iter: 164 loss: 2.03603736e-06
Iter: 165 loss: 2.09811878e-06
Iter: 166 loss: 2.02742012e-06
Iter: 167 loss: 2.00914815e-06
Iter: 168 loss: 2.09004611e-06
Iter: 169 loss: 2.00552404e-06
Iter: 170 loss: 1.98710677e-06
Iter: 171 loss: 2.08887445e-06
Iter: 172 loss: 1.98444332e-06
Iter: 173 loss: 1.97053214e-06
Iter: 174 loss: 1.95895427e-06
Iter: 175 loss: 1.95498546e-06
Iter: 176 loss: 1.93637743e-06
Iter: 177 loss: 1.93645792e-06
Iter: 178 loss: 1.92511197e-06
Iter: 179 loss: 1.93043229e-06
Iter: 180 loss: 1.91750314e-06
Iter: 181 loss: 1.90388835e-06
Iter: 182 loss: 1.90020887e-06
Iter: 183 loss: 1.89184493e-06
Iter: 184 loss: 1.87406852e-06
Iter: 185 loss: 1.95358939e-06
Iter: 186 loss: 1.87069236e-06
Iter: 187 loss: 1.86319278e-06
Iter: 188 loss: 1.86018929e-06
Iter: 189 loss: 1.85395243e-06
Iter: 190 loss: 1.8435785e-06
Iter: 191 loss: 1.84345822e-06
Iter: 192 loss: 1.83271322e-06
Iter: 193 loss: 1.83118118e-06
Iter: 194 loss: 1.82361555e-06
Iter: 195 loss: 1.8096398e-06
Iter: 196 loss: 2.00871864e-06
Iter: 197 loss: 1.80962866e-06
Iter: 198 loss: 1.8005876e-06
Iter: 199 loss: 1.8011101e-06
Iter: 200 loss: 1.79351434e-06
Iter: 201 loss: 1.78378195e-06
Iter: 202 loss: 1.79597635e-06
Iter: 203 loss: 1.77872653e-06
Iter: 204 loss: 1.76509434e-06
Iter: 205 loss: 1.7826992e-06
Iter: 206 loss: 1.75813454e-06
Iter: 207 loss: 1.7487223e-06
Iter: 208 loss: 1.74858008e-06
Iter: 209 loss: 1.74140541e-06
Iter: 210 loss: 1.72845773e-06
Iter: 211 loss: 2.04459593e-06
Iter: 212 loss: 1.72850173e-06
Iter: 213 loss: 1.71826639e-06
Iter: 214 loss: 1.71798251e-06
Iter: 215 loss: 1.71107081e-06
Iter: 216 loss: 1.71465945e-06
Iter: 217 loss: 1.70647138e-06
Iter: 218 loss: 1.69792474e-06
Iter: 219 loss: 1.71066858e-06
Iter: 220 loss: 1.69376767e-06
Iter: 221 loss: 1.6895068e-06
Iter: 222 loss: 1.68883832e-06
Iter: 223 loss: 1.68290399e-06
Iter: 224 loss: 1.66937707e-06
Iter: 225 loss: 1.85794727e-06
Iter: 226 loss: 1.66875816e-06
Iter: 227 loss: 1.66072869e-06
Iter: 228 loss: 1.69329735e-06
Iter: 229 loss: 1.65892482e-06
Iter: 230 loss: 1.65091024e-06
Iter: 231 loss: 1.67881558e-06
Iter: 232 loss: 1.64888684e-06
Iter: 233 loss: 1.63940422e-06
Iter: 234 loss: 1.66836628e-06
Iter: 235 loss: 1.636663e-06
Iter: 236 loss: 1.63025743e-06
Iter: 237 loss: 1.62546689e-06
Iter: 238 loss: 1.62344736e-06
Iter: 239 loss: 1.61341563e-06
Iter: 240 loss: 1.67690018e-06
Iter: 241 loss: 1.61229536e-06
Iter: 242 loss: 1.60530772e-06
Iter: 243 loss: 1.61361925e-06
Iter: 244 loss: 1.60169589e-06
Iter: 245 loss: 1.59155525e-06
Iter: 246 loss: 1.64066728e-06
Iter: 247 loss: 1.58982857e-06
Iter: 248 loss: 1.5840377e-06
Iter: 249 loss: 1.6030159e-06
Iter: 250 loss: 1.58239584e-06
Iter: 251 loss: 1.57606894e-06
Iter: 252 loss: 1.58158173e-06
Iter: 253 loss: 1.5724205e-06
Iter: 254 loss: 1.56270698e-06
Iter: 255 loss: 1.58252851e-06
Iter: 256 loss: 1.55883686e-06
Iter: 257 loss: 1.55463908e-06
Iter: 258 loss: 1.55466864e-06
Iter: 259 loss: 1.55040948e-06
Iter: 260 loss: 1.5612203e-06
Iter: 261 loss: 1.54900727e-06
Iter: 262 loss: 1.54548025e-06
Iter: 263 loss: 1.53664837e-06
Iter: 264 loss: 1.61109733e-06
Iter: 265 loss: 1.53514486e-06
Iter: 266 loss: 1.52678092e-06
Iter: 267 loss: 1.58492662e-06
Iter: 268 loss: 1.52602945e-06
Iter: 269 loss: 1.52030645e-06
Iter: 270 loss: 1.60345974e-06
Iter: 271 loss: 1.52026871e-06
Iter: 272 loss: 1.51529105e-06
Iter: 273 loss: 1.51127915e-06
Iter: 274 loss: 1.50975939e-06
Iter: 275 loss: 1.50332e-06
Iter: 276 loss: 1.51848542e-06
Iter: 277 loss: 1.50093229e-06
Iter: 278 loss: 1.49428524e-06
Iter: 279 loss: 1.51142603e-06
Iter: 280 loss: 1.49198559e-06
Iter: 281 loss: 1.48635331e-06
Iter: 282 loss: 1.51792358e-06
Iter: 283 loss: 1.48556819e-06
Iter: 284 loss: 1.47928449e-06
Iter: 285 loss: 1.48634024e-06
Iter: 286 loss: 1.47589321e-06
Iter: 287 loss: 1.47008802e-06
Iter: 288 loss: 1.50990843e-06
Iter: 289 loss: 1.46948696e-06
Iter: 290 loss: 1.46543312e-06
Iter: 291 loss: 1.4695097e-06
Iter: 292 loss: 1.4631014e-06
Iter: 293 loss: 1.45664899e-06
Iter: 294 loss: 1.46882041e-06
Iter: 295 loss: 1.45387116e-06
Iter: 296 loss: 1.45324225e-06
Iter: 297 loss: 1.45134504e-06
Iter: 298 loss: 1.44987166e-06
Iter: 299 loss: 1.44670912e-06
Iter: 300 loss: 1.50218898e-06
Iter: 301 loss: 1.44663829e-06
Iter: 302 loss: 1.44265482e-06
Iter: 303 loss: 1.43730563e-06
Iter: 304 loss: 1.43700447e-06
Iter: 305 loss: 1.43150351e-06
Iter: 306 loss: 1.46281263e-06
Iter: 307 loss: 1.43076704e-06
Iter: 308 loss: 1.42451393e-06
Iter: 309 loss: 1.46732214e-06
Iter: 310 loss: 1.42384738e-06
Iter: 311 loss: 1.42092824e-06
Iter: 312 loss: 1.41545797e-06
Iter: 313 loss: 1.5415759e-06
Iter: 314 loss: 1.41544501e-06
Iter: 315 loss: 1.40801046e-06
Iter: 316 loss: 1.45114655e-06
Iter: 317 loss: 1.40713e-06
Iter: 318 loss: 1.40235943e-06
Iter: 319 loss: 1.41501482e-06
Iter: 320 loss: 1.40079362e-06
Iter: 321 loss: 1.39560711e-06
Iter: 322 loss: 1.42624901e-06
Iter: 323 loss: 1.39497502e-06
Iter: 324 loss: 1.39055987e-06
Iter: 325 loss: 1.39394456e-06
Iter: 326 loss: 1.38780706e-06
Iter: 327 loss: 1.38298162e-06
Iter: 328 loss: 1.4095981e-06
Iter: 329 loss: 1.3821857e-06
Iter: 330 loss: 1.37860707e-06
Iter: 331 loss: 1.39425015e-06
Iter: 332 loss: 1.37786083e-06
Iter: 333 loss: 1.3747765e-06
Iter: 334 loss: 1.4036259e-06
Iter: 335 loss: 1.37461598e-06
Iter: 336 loss: 1.37141251e-06
Iter: 337 loss: 1.3679462e-06
Iter: 338 loss: 1.36745018e-06
Iter: 339 loss: 1.36339315e-06
Iter: 340 loss: 1.3647666e-06
Iter: 341 loss: 1.36047424e-06
Iter: 342 loss: 1.3565591e-06
Iter: 343 loss: 1.3685285e-06
Iter: 344 loss: 1.35538505e-06
Iter: 345 loss: 1.35118921e-06
Iter: 346 loss: 1.37292079e-06
Iter: 347 loss: 1.35047537e-06
Iter: 348 loss: 1.34579068e-06
Iter: 349 loss: 1.35928951e-06
Iter: 350 loss: 1.34423203e-06
Iter: 351 loss: 1.34189213e-06
Iter: 352 loss: 1.33714718e-06
Iter: 353 loss: 1.4304793e-06
Iter: 354 loss: 1.33707715e-06
Iter: 355 loss: 1.33158551e-06
Iter: 356 loss: 1.3925503e-06
Iter: 357 loss: 1.33147046e-06
Iter: 358 loss: 1.32819264e-06
Iter: 359 loss: 1.33952676e-06
Iter: 360 loss: 1.32728951e-06
Iter: 361 loss: 1.32336447e-06
Iter: 362 loss: 1.33437106e-06
Iter: 363 loss: 1.32214268e-06
Iter: 364 loss: 1.31889215e-06
Iter: 365 loss: 1.32351443e-06
Iter: 366 loss: 1.31732202e-06
Iter: 367 loss: 1.31349043e-06
Iter: 368 loss: 1.33715218e-06
Iter: 369 loss: 1.31311742e-06
Iter: 370 loss: 1.31070124e-06
Iter: 371 loss: 1.34851211e-06
Iter: 372 loss: 1.31074341e-06
Iter: 373 loss: 1.30888884e-06
Iter: 374 loss: 1.30648186e-06
Iter: 375 loss: 1.30639194e-06
Iter: 376 loss: 1.30261219e-06
Iter: 377 loss: 1.30057833e-06
Iter: 378 loss: 1.29888497e-06
Iter: 379 loss: 1.2952836e-06
Iter: 380 loss: 1.31140109e-06
Iter: 381 loss: 1.29460216e-06
Iter: 382 loss: 1.29106684e-06
Iter: 383 loss: 1.3053849e-06
Iter: 384 loss: 1.29027444e-06
Iter: 385 loss: 1.28683371e-06
Iter: 386 loss: 1.31085415e-06
Iter: 387 loss: 1.28647912e-06
Iter: 388 loss: 1.28427678e-06
Iter: 389 loss: 1.2808199e-06
Iter: 390 loss: 1.28084832e-06
Iter: 391 loss: 1.27706778e-06
Iter: 392 loss: 1.30461444e-06
Iter: 393 loss: 1.27672911e-06
Iter: 394 loss: 1.27374699e-06
Iter: 395 loss: 1.27363217e-06
Iter: 396 loss: 1.27140697e-06
Iter: 397 loss: 1.26777741e-06
Iter: 398 loss: 1.26775808e-06
Iter: 399 loss: 1.2657373e-06
Iter: 400 loss: 1.26895861e-06
Iter: 401 loss: 1.26480063e-06
Iter: 402 loss: 1.26256987e-06
Iter: 403 loss: 1.26851205e-06
Iter: 404 loss: 1.2618309e-06
Iter: 405 loss: 1.25880717e-06
Iter: 406 loss: 1.27452176e-06
Iter: 407 loss: 1.25840199e-06
Iter: 408 loss: 1.25633392e-06
Iter: 409 loss: 1.25894621e-06
Iter: 410 loss: 1.2552473e-06
Iter: 411 loss: 1.25350732e-06
Iter: 412 loss: 1.2517653e-06
Iter: 413 loss: 1.25140537e-06
Iter: 414 loss: 1.24800897e-06
Iter: 415 loss: 1.25483916e-06
Iter: 416 loss: 1.2466603e-06
Iter: 417 loss: 1.24352482e-06
Iter: 418 loss: 1.26050134e-06
Iter: 419 loss: 1.24301221e-06
Iter: 420 loss: 1.24084659e-06
Iter: 421 loss: 1.26413556e-06
Iter: 422 loss: 1.2407861e-06
Iter: 423 loss: 1.23870552e-06
Iter: 424 loss: 1.23473819e-06
Iter: 425 loss: 1.31865772e-06
Iter: 426 loss: 1.23473205e-06
Iter: 427 loss: 1.23122038e-06
Iter: 428 loss: 1.24270809e-06
Iter: 429 loss: 1.23030895e-06
Iter: 430 loss: 1.22652887e-06
Iter: 431 loss: 1.24442477e-06
Iter: 432 loss: 1.2258896e-06
Iter: 433 loss: 1.22390202e-06
Iter: 434 loss: 1.24162693e-06
Iter: 435 loss: 1.2238072e-06
Iter: 436 loss: 1.22157826e-06
Iter: 437 loss: 1.22062795e-06
Iter: 438 loss: 1.21945993e-06
Iter: 439 loss: 1.21849621e-06
Iter: 440 loss: 1.21817891e-06
Iter: 441 loss: 1.21693608e-06
Iter: 442 loss: 1.21672872e-06
Iter: 443 loss: 1.2158647e-06
Iter: 444 loss: 1.21401456e-06
Iter: 445 loss: 1.21284245e-06
Iter: 446 loss: 1.21207938e-06
Iter: 447 loss: 1.20928405e-06
Iter: 448 loss: 1.21848575e-06
Iter: 449 loss: 1.20847551e-06
Iter: 450 loss: 1.20601248e-06
Iter: 451 loss: 1.20475238e-06
Iter: 452 loss: 1.20364052e-06
Iter: 453 loss: 1.20092932e-06
Iter: 454 loss: 1.24107942e-06
Iter: 455 loss: 1.20094955e-06
Iter: 456 loss: 1.19902347e-06
Iter: 457 loss: 1.2065866e-06
Iter: 458 loss: 1.19863989e-06
Iter: 459 loss: 1.19638821e-06
Iter: 460 loss: 1.19720812e-06
Iter: 461 loss: 1.19476965e-06
Iter: 462 loss: 1.19305514e-06
Iter: 463 loss: 1.19207391e-06
Iter: 464 loss: 1.19127196e-06
Iter: 465 loss: 1.18831258e-06
Iter: 466 loss: 1.20560821e-06
Iter: 467 loss: 1.18790126e-06
Iter: 468 loss: 1.18557216e-06
Iter: 469 loss: 1.19123638e-06
Iter: 470 loss: 1.1847319e-06
Iter: 471 loss: 1.18202206e-06
Iter: 472 loss: 1.20247648e-06
Iter: 473 loss: 1.18182663e-06
Iter: 474 loss: 1.18070807e-06
Iter: 475 loss: 1.19422282e-06
Iter: 476 loss: 1.18064747e-06
Iter: 477 loss: 1.17940476e-06
Iter: 478 loss: 1.17825425e-06
Iter: 479 loss: 1.17795173e-06
Iter: 480 loss: 1.17619561e-06
Iter: 481 loss: 1.17741331e-06
Iter: 482 loss: 1.17514548e-06
Iter: 483 loss: 1.17296133e-06
Iter: 484 loss: 1.17949503e-06
Iter: 485 loss: 1.17225363e-06
Iter: 486 loss: 1.17015793e-06
Iter: 487 loss: 1.16984438e-06
Iter: 488 loss: 1.16834974e-06
Iter: 489 loss: 1.16554327e-06
Iter: 490 loss: 1.17953857e-06
Iter: 491 loss: 1.16508954e-06
Iter: 492 loss: 1.16350395e-06
Iter: 493 loss: 1.1634919e-06
Iter: 494 loss: 1.16225976e-06
Iter: 495 loss: 1.16117099e-06
Iter: 496 loss: 1.16080651e-06
Iter: 497 loss: 1.15872115e-06
Iter: 498 loss: 1.16198044e-06
Iter: 499 loss: 1.15769183e-06
Iter: 500 loss: 1.15525256e-06
Iter: 501 loss: 1.15527337e-06
Iter: 502 loss: 1.1532934e-06
Iter: 503 loss: 1.15121679e-06
Iter: 504 loss: 1.15119246e-06
Iter: 505 loss: 1.14993111e-06
Iter: 506 loss: 1.15845887e-06
Iter: 507 loss: 1.14979707e-06
Iter: 508 loss: 1.14860609e-06
Iter: 509 loss: 1.15192984e-06
Iter: 510 loss: 1.14816362e-06
Iter: 511 loss: 1.14635316e-06
Iter: 512 loss: 1.14786235e-06
Iter: 513 loss: 1.14527575e-06
Iter: 514 loss: 1.14375212e-06
Iter: 515 loss: 1.14237616e-06
Iter: 516 loss: 1.14198383e-06
Iter: 517 loss: 1.13968986e-06
Iter: 518 loss: 1.15437842e-06
Iter: 519 loss: 1.13945816e-06
Iter: 520 loss: 1.13750866e-06
Iter: 521 loss: 1.14106945e-06
Iter: 522 loss: 1.13669864e-06
Iter: 523 loss: 1.13477086e-06
Iter: 524 loss: 1.13523311e-06
Iter: 525 loss: 1.13336694e-06
Iter: 526 loss: 1.13174917e-06
Iter: 527 loss: 1.13176225e-06
Iter: 528 loss: 1.13013209e-06
Iter: 529 loss: 1.13046565e-06
Iter: 530 loss: 1.12893986e-06
Iter: 531 loss: 1.12670591e-06
Iter: 532 loss: 1.12982491e-06
Iter: 533 loss: 1.12558348e-06
Iter: 534 loss: 1.12373766e-06
Iter: 535 loss: 1.12344105e-06
Iter: 536 loss: 1.12223029e-06
Iter: 537 loss: 1.11992631e-06
Iter: 538 loss: 1.14793772e-06
Iter: 539 loss: 1.11983422e-06
Iter: 540 loss: 1.11837255e-06
Iter: 541 loss: 1.12038811e-06
Iter: 542 loss: 1.11764757e-06
Iter: 543 loss: 1.11576753e-06
Iter: 544 loss: 1.13891906e-06
Iter: 545 loss: 1.11574468e-06
Iter: 546 loss: 1.11472036e-06
Iter: 547 loss: 1.11845839e-06
Iter: 548 loss: 1.11445752e-06
Iter: 549 loss: 1.11376812e-06
Iter: 550 loss: 1.11221652e-06
Iter: 551 loss: 1.13543672e-06
Iter: 552 loss: 1.11216582e-06
Iter: 553 loss: 1.11004772e-06
Iter: 554 loss: 1.11583086e-06
Iter: 555 loss: 1.1094063e-06
Iter: 556 loss: 1.10772737e-06
Iter: 557 loss: 1.12446241e-06
Iter: 558 loss: 1.10766121e-06
Iter: 559 loss: 1.10633709e-06
Iter: 560 loss: 1.10403e-06
Iter: 561 loss: 1.1040031e-06
Iter: 562 loss: 1.10149745e-06
Iter: 563 loss: 1.12389762e-06
Iter: 564 loss: 1.10135966e-06
Iter: 565 loss: 1.09981374e-06
Iter: 566 loss: 1.1231366e-06
Iter: 567 loss: 1.09978077e-06
Iter: 568 loss: 1.09875987e-06
Iter: 569 loss: 1.09679331e-06
Iter: 570 loss: 1.14012403e-06
Iter: 571 loss: 1.09681548e-06
Iter: 572 loss: 1.09451378e-06
Iter: 573 loss: 1.10607425e-06
Iter: 574 loss: 1.09413384e-06
Iter: 575 loss: 1.09259872e-06
Iter: 576 loss: 1.09321456e-06
Iter: 577 loss: 1.09150767e-06
Iter: 578 loss: 1.08979839e-06
Iter: 579 loss: 1.11294219e-06
Iter: 580 loss: 1.0897752e-06
Iter: 581 loss: 1.08875702e-06
Iter: 582 loss: 1.08878157e-06
Iter: 583 loss: 1.08791687e-06
Iter: 584 loss: 1.08647464e-06
Iter: 585 loss: 1.08645656e-06
Iter: 586 loss: 1.08464758e-06
Iter: 587 loss: 1.09172925e-06
Iter: 588 loss: 1.08416646e-06
Iter: 589 loss: 1.08302538e-06
Iter: 590 loss: 1.08187328e-06
Iter: 591 loss: 1.081645e-06
Iter: 592 loss: 1.07943583e-06
Iter: 593 loss: 1.09265955e-06
Iter: 594 loss: 1.07911865e-06
Iter: 595 loss: 1.07752544e-06
Iter: 596 loss: 1.08401457e-06
Iter: 597 loss: 1.07716357e-06
Iter: 598 loss: 1.07583162e-06
Iter: 599 loss: 1.07665073e-06
Iter: 600 loss: 1.07500023e-06
Iter: 601 loss: 1.07395181e-06
Iter: 602 loss: 1.07395272e-06
Iter: 603 loss: 1.07297012e-06
Iter: 604 loss: 1.07101243e-06
Iter: 605 loss: 1.10794826e-06
Iter: 606 loss: 1.07101391e-06
Iter: 607 loss: 1.06915434e-06
Iter: 608 loss: 1.08391691e-06
Iter: 609 loss: 1.0689663e-06
Iter: 610 loss: 1.06770904e-06
Iter: 611 loss: 1.06651589e-06
Iter: 612 loss: 1.06620882e-06
Iter: 613 loss: 1.06405878e-06
Iter: 614 loss: 1.08557151e-06
Iter: 615 loss: 1.06397977e-06
Iter: 616 loss: 1.06353991e-06
Iter: 617 loss: 1.06318851e-06
Iter: 618 loss: 1.06269613e-06
Iter: 619 loss: 1.06162247e-06
Iter: 620 loss: 1.08025358e-06
Iter: 621 loss: 1.06156835e-06
Iter: 622 loss: 1.0601201e-06
Iter: 623 loss: 1.0623038e-06
Iter: 624 loss: 1.05942649e-06
Iter: 625 loss: 1.05797744e-06
Iter: 626 loss: 1.06335858e-06
Iter: 627 loss: 1.05761512e-06
Iter: 628 loss: 1.0564288e-06
Iter: 629 loss: 1.05701235e-06
Iter: 630 loss: 1.05568529e-06
Iter: 631 loss: 1.05417416e-06
Iter: 632 loss: 1.06560788e-06
Iter: 633 loss: 1.05406116e-06
Iter: 634 loss: 1.05291076e-06
Iter: 635 loss: 1.05307527e-06
Iter: 636 loss: 1.05202139e-06
Iter: 637 loss: 1.05079437e-06
Iter: 638 loss: 1.06537107e-06
Iter: 639 loss: 1.05078323e-06
Iter: 640 loss: 1.04965102e-06
Iter: 641 loss: 1.04929404e-06
Iter: 642 loss: 1.04865717e-06
Iter: 643 loss: 1.04729077e-06
Iter: 644 loss: 1.04887681e-06
Iter: 645 loss: 1.04656488e-06
Iter: 646 loss: 1.04498019e-06
Iter: 647 loss: 1.04707965e-06
Iter: 648 loss: 1.04405319e-06
Iter: 649 loss: 1.04264086e-06
Iter: 650 loss: 1.05535605e-06
Iter: 651 loss: 1.04259789e-06
Iter: 652 loss: 1.04122807e-06
Iter: 653 loss: 1.0550234e-06
Iter: 654 loss: 1.04120068e-06
Iter: 655 loss: 1.04041828e-06
Iter: 656 loss: 1.03956791e-06
Iter: 657 loss: 1.03946434e-06
Iter: 658 loss: 1.03836862e-06
Iter: 659 loss: 1.04329047e-06
Iter: 660 loss: 1.03811556e-06
Iter: 661 loss: 1.03713091e-06
Iter: 662 loss: 1.03831076e-06
Iter: 663 loss: 1.03656907e-06
Iter: 664 loss: 1.0355252e-06
Iter: 665 loss: 1.03557545e-06
Iter: 666 loss: 1.03469233e-06
Iter: 667 loss: 1.03266302e-06
Iter: 668 loss: 1.03755076e-06
Iter: 669 loss: 1.03194657e-06
Iter: 670 loss: 1.03049081e-06
Iter: 671 loss: 1.0463699e-06
Iter: 672 loss: 1.03046602e-06
Iter: 673 loss: 1.0296053e-06
Iter: 674 loss: 1.02976651e-06
Iter: 675 loss: 1.02890988e-06
Iter: 676 loss: 1.02724425e-06
Iter: 677 loss: 1.03112313e-06
Iter: 678 loss: 1.02661818e-06
Iter: 679 loss: 1.02566332e-06
Iter: 680 loss: 1.02619856e-06
Iter: 681 loss: 1.0250119e-06
Iter: 682 loss: 1.02342744e-06
Iter: 683 loss: 1.02178967e-06
Iter: 684 loss: 1.02149352e-06
Iter: 685 loss: 1.02110312e-06
Iter: 686 loss: 1.02059403e-06
Iter: 687 loss: 1.01961348e-06
Iter: 688 loss: 1.02158674e-06
Iter: 689 loss: 1.01921466e-06
Iter: 690 loss: 1.01851083e-06
Iter: 691 loss: 1.01812248e-06
Iter: 692 loss: 1.01783871e-06
Iter: 693 loss: 1.01676733e-06
Iter: 694 loss: 1.02040417e-06
Iter: 695 loss: 1.01646719e-06
Iter: 696 loss: 1.01538285e-06
Iter: 697 loss: 1.01679097e-06
Iter: 698 loss: 1.01476962e-06
Iter: 699 loss: 1.01370631e-06
Iter: 700 loss: 1.01604985e-06
Iter: 701 loss: 1.01326827e-06
Iter: 702 loss: 1.01199066e-06
Iter: 703 loss: 1.01431e-06
Iter: 704 loss: 1.01139381e-06
Iter: 705 loss: 1.00983357e-06
Iter: 706 loss: 1.01657372e-06
Iter: 707 loss: 1.00947523e-06
Iter: 708 loss: 1.00832244e-06
Iter: 709 loss: 1.01304829e-06
Iter: 710 loss: 1.00810621e-06
Iter: 711 loss: 1.00711657e-06
Iter: 712 loss: 1.01233297e-06
Iter: 713 loss: 1.00691159e-06
Iter: 714 loss: 1.00610339e-06
Iter: 715 loss: 1.00513796e-06
Iter: 716 loss: 1.00502473e-06
Iter: 717 loss: 1.00350485e-06
Iter: 718 loss: 1.00474188e-06
Iter: 719 loss: 1.00253931e-06
Iter: 720 loss: 1.00103136e-06
Iter: 721 loss: 1.02280444e-06
Iter: 722 loss: 1.00103432e-06
Iter: 723 loss: 1.00005104e-06
Iter: 724 loss: 1.00005161e-06
Iter: 725 loss: 9.99652229e-07
Iter: 726 loss: 9.98604492e-07
Iter: 727 loss: 1.00524812e-06
Iter: 728 loss: 9.9831891e-07
Iter: 729 loss: 9.97117354e-07
Iter: 730 loss: 1.00572242e-06
Iter: 731 loss: 9.96984681e-07
Iter: 732 loss: 9.96027325e-07
Iter: 733 loss: 1.0020583e-06
Iter: 734 loss: 9.95846449e-07
Iter: 735 loss: 9.95064852e-07
Iter: 736 loss: 9.93863523e-07
Iter: 737 loss: 9.93840558e-07
Iter: 738 loss: 9.9254e-07
Iter: 739 loss: 1.00677573e-06
Iter: 740 loss: 9.92536343e-07
Iter: 741 loss: 9.91444495e-07
Iter: 742 loss: 9.92467676e-07
Iter: 743 loss: 9.90787385e-07
Iter: 744 loss: 9.89893579e-07
Iter: 745 loss: 1.00299417e-06
Iter: 746 loss: 9.89896193e-07
Iter: 747 loss: 9.89378918e-07
Iter: 748 loss: 9.89797286e-07
Iter: 749 loss: 9.89060482e-07
Iter: 750 loss: 9.88051397e-07
Iter: 751 loss: 9.86086661e-07
Iter: 752 loss: 1.02772526e-06
Iter: 753 loss: 9.8608848e-07
Iter: 754 loss: 9.84190365e-07
Iter: 755 loss: 9.99396207e-07
Iter: 756 loss: 9.84078156e-07
Iter: 757 loss: 9.82987217e-07
Iter: 758 loss: 9.92278274e-07
Iter: 759 loss: 9.8290036e-07
Iter: 760 loss: 9.81946528e-07
Iter: 761 loss: 9.90124818e-07
Iter: 762 loss: 9.81903895e-07
Iter: 763 loss: 9.81522589e-07
Iter: 764 loss: 9.80479854e-07
Iter: 765 loss: 9.85855081e-07
Iter: 766 loss: 9.80161076e-07
Iter: 767 loss: 9.78585831e-07
Iter: 768 loss: 9.87139856e-07
Iter: 769 loss: 9.78336629e-07
Iter: 770 loss: 9.77212e-07
Iter: 771 loss: 9.88053785e-07
Iter: 772 loss: 9.77225909e-07
Iter: 773 loss: 9.76321417e-07
Iter: 774 loss: 9.75418288e-07
Iter: 775 loss: 9.75200237e-07
Iter: 776 loss: 9.74172e-07
Iter: 777 loss: 9.81425728e-07
Iter: 778 loss: 9.74065415e-07
Iter: 779 loss: 9.73057e-07
Iter: 780 loss: 9.75733428e-07
Iter: 781 loss: 9.72703674e-07
Iter: 782 loss: 9.71798841e-07
Iter: 783 loss: 9.76345746e-07
Iter: 784 loss: 9.71597842e-07
Iter: 785 loss: 9.70675501e-07
Iter: 786 loss: 9.71179702e-07
Iter: 787 loss: 9.70089104e-07
Iter: 788 loss: 9.68800123e-07
Iter: 789 loss: 9.7358668e-07
Iter: 790 loss: 9.68524e-07
Iter: 791 loss: 9.67770234e-07
Iter: 792 loss: 9.68671429e-07
Iter: 793 loss: 9.67357664e-07
Iter: 794 loss: 9.66739321e-07
Iter: 795 loss: 9.66761e-07
Iter: 796 loss: 9.66062089e-07
Iter: 797 loss: 9.65333356e-07
Iter: 798 loss: 9.65177378e-07
Iter: 799 loss: 9.64350306e-07
Iter: 800 loss: 9.63729576e-07
Iter: 801 loss: 9.63454454e-07
Iter: 802 loss: 9.61962428e-07
Iter: 803 loss: 9.6591566e-07
Iter: 804 loss: 9.61439582e-07
Iter: 805 loss: 9.60087846e-07
Iter: 806 loss: 9.76364618e-07
Iter: 807 loss: 9.60027137e-07
Iter: 808 loss: 9.59294539e-07
Iter: 809 loss: 9.59544195e-07
Iter: 810 loss: 9.58772716e-07
Iter: 811 loss: 9.5778887e-07
Iter: 812 loss: 9.58527835e-07
Iter: 813 loss: 9.57167572e-07
Iter: 814 loss: 9.56175427e-07
Iter: 815 loss: 9.70472684e-07
Iter: 816 loss: 9.56205099e-07
Iter: 817 loss: 9.55456e-07
Iter: 818 loss: 9.55749442e-07
Iter: 819 loss: 9.54946131e-07
Iter: 820 loss: 9.53925564e-07
Iter: 821 loss: 9.5861742e-07
Iter: 822 loss: 9.53783456e-07
Iter: 823 loss: 9.53028291e-07
Iter: 824 loss: 9.54321536e-07
Iter: 825 loss: 9.52748792e-07
Iter: 826 loss: 9.51911431e-07
Iter: 827 loss: 9.51543313e-07
Iter: 828 loss: 9.51167522e-07
Iter: 829 loss: 9.50883759e-07
Iter: 830 loss: 9.50509616e-07
Iter: 831 loss: 9.49992341e-07
Iter: 832 loss: 9.49495757e-07
Iter: 833 loss: 9.49391733e-07
Iter: 834 loss: 9.48704098e-07
Iter: 835 loss: 9.46843556e-07
Iter: 836 loss: 9.59018507e-07
Iter: 837 loss: 9.46423256e-07
Iter: 838 loss: 9.46211e-07
Iter: 839 loss: 9.45571514e-07
Iter: 840 loss: 9.44876547e-07
Iter: 841 loss: 9.4545328e-07
Iter: 842 loss: 9.44477392e-07
Iter: 843 loss: 9.43412488e-07
Iter: 844 loss: 9.43901114e-07
Iter: 845 loss: 9.42711836e-07
Iter: 846 loss: 9.41465e-07
Iter: 847 loss: 9.42471672e-07
Iter: 848 loss: 9.40744656e-07
Iter: 849 loss: 9.39845904e-07
Iter: 850 loss: 9.39808842e-07
Iter: 851 loss: 9.39204199e-07
Iter: 852 loss: 9.41525343e-07
Iter: 853 loss: 9.38975859e-07
Iter: 854 loss: 9.38263781e-07
Iter: 855 loss: 9.38092228e-07
Iter: 856 loss: 9.37574896e-07
Iter: 857 loss: 9.3653432e-07
Iter: 858 loss: 9.42004476e-07
Iter: 859 loss: 9.36353e-07
Iter: 860 loss: 9.35564231e-07
Iter: 861 loss: 9.35028197e-07
Iter: 862 loss: 9.34733123e-07
Iter: 863 loss: 9.34211073e-07
Iter: 864 loss: 9.34058221e-07
Iter: 865 loss: 9.33456477e-07
Iter: 866 loss: 9.33768433e-07
Iter: 867 loss: 9.33034073e-07
Iter: 868 loss: 9.32418402e-07
Iter: 869 loss: 9.31246177e-07
Iter: 870 loss: 9.52927451e-07
Iter: 871 loss: 9.3122037e-07
Iter: 872 loss: 9.29961118e-07
Iter: 873 loss: 9.35605158e-07
Iter: 874 loss: 9.29715213e-07
Iter: 875 loss: 9.2870107e-07
Iter: 876 loss: 9.32773617e-07
Iter: 877 loss: 9.28497172e-07
Iter: 878 loss: 9.27456256e-07
Iter: 879 loss: 9.34409172e-07
Iter: 880 loss: 9.27347912e-07
Iter: 881 loss: 9.26655503e-07
Iter: 882 loss: 9.27406063e-07
Iter: 883 loss: 9.26286532e-07
Iter: 884 loss: 9.25531822e-07
Iter: 885 loss: 9.25670633e-07
Iter: 886 loss: 9.24982601e-07
Iter: 887 loss: 9.24086351e-07
Iter: 888 loss: 9.35194521e-07
Iter: 889 loss: 9.24053e-07
Iter: 890 loss: 9.23389e-07
Iter: 891 loss: 9.25462132e-07
Iter: 892 loss: 9.23227844e-07
Iter: 893 loss: 9.22593927e-07
Iter: 894 loss: 9.2308062e-07
Iter: 895 loss: 9.22213e-07
Iter: 896 loss: 9.21304547e-07
Iter: 897 loss: 9.22134404e-07
Iter: 898 loss: 9.20759419e-07
Iter: 899 loss: 9.19937236e-07
Iter: 900 loss: 9.2945254e-07
Iter: 901 loss: 9.19917738e-07
Iter: 902 loss: 9.19049967e-07
Iter: 903 loss: 9.21144363e-07
Iter: 904 loss: 9.1874756e-07
Iter: 905 loss: 9.18211811e-07
Iter: 906 loss: 9.17568e-07
Iter: 907 loss: 9.17450052e-07
Iter: 908 loss: 9.16558e-07
Iter: 909 loss: 9.16548515e-07
Iter: 910 loss: 9.15857299e-07
Iter: 911 loss: 9.14607313e-07
Iter: 912 loss: 9.22222966e-07
Iter: 913 loss: 9.1445e-07
Iter: 914 loss: 9.13411839e-07
Iter: 915 loss: 9.1556609e-07
Iter: 916 loss: 9.12947e-07
Iter: 917 loss: 9.11788788e-07
Iter: 918 loss: 9.21969615e-07
Iter: 919 loss: 9.11708185e-07
Iter: 920 loss: 9.1111076e-07
Iter: 921 loss: 9.10478832e-07
Iter: 922 loss: 9.10420567e-07
Iter: 923 loss: 9.09451273e-07
Iter: 924 loss: 9.16924137e-07
Iter: 925 loss: 9.09370556e-07
Iter: 926 loss: 9.08853394e-07
Iter: 927 loss: 9.15114924e-07
Iter: 928 loss: 9.08849643e-07
Iter: 929 loss: 9.08392735e-07
Iter: 930 loss: 9.07526e-07
Iter: 931 loss: 9.26125608e-07
Iter: 932 loss: 9.07545882e-07
Iter: 933 loss: 9.06583864e-07
Iter: 934 loss: 9.18086357e-07
Iter: 935 loss: 9.0657943e-07
Iter: 936 loss: 9.06008495e-07
Iter: 937 loss: 9.07474885e-07
Iter: 938 loss: 9.05827676e-07
Iter: 939 loss: 9.05083425e-07
Iter: 940 loss: 9.07471872e-07
Iter: 941 loss: 9.04868443e-07
Iter: 942 loss: 9.04355602e-07
Iter: 943 loss: 9.03810587e-07
Iter: 944 loss: 9.03727653e-07
Iter: 945 loss: 9.02970555e-07
Iter: 946 loss: 9.03747434e-07
Iter: 947 loss: 9.02505576e-07
Iter: 948 loss: 9.01483304e-07
Iter: 949 loss: 9.02316572e-07
Iter: 950 loss: 9.00935675e-07
Iter: 951 loss: 8.99919257e-07
Iter: 952 loss: 9.11051643e-07
Iter: 953 loss: 8.99894189e-07
Iter: 954 loss: 8.9909787e-07
Iter: 955 loss: 9.01695898e-07
Iter: 956 loss: 8.98893518e-07
Iter: 957 loss: 8.98065423e-07
Iter: 958 loss: 8.99098666e-07
Iter: 959 loss: 8.97593452e-07
Iter: 960 loss: 8.96935717e-07
Iter: 961 loss: 8.97405812e-07
Iter: 962 loss: 8.96525933e-07
Iter: 963 loss: 8.95435278e-07
Iter: 964 loss: 9.01752799e-07
Iter: 965 loss: 8.95336029e-07
Iter: 966 loss: 8.94509469e-07
Iter: 967 loss: 8.96854203e-07
Iter: 968 loss: 8.94227469e-07
Iter: 969 loss: 8.93670403e-07
Iter: 970 loss: 8.94842628e-07
Iter: 971 loss: 8.93439619e-07
Iter: 972 loss: 8.92719186e-07
Iter: 973 loss: 8.96838912e-07
Iter: 974 loss: 8.92631931e-07
Iter: 975 loss: 8.91952595e-07
Iter: 976 loss: 8.95450967e-07
Iter: 977 loss: 8.9180827e-07
Iter: 978 loss: 8.9143623e-07
Iter: 979 loss: 8.90444539e-07
Iter: 980 loss: 8.97908365e-07
Iter: 981 loss: 8.90203523e-07
Iter: 982 loss: 8.89092291e-07
Iter: 983 loss: 9.00409304e-07
Iter: 984 loss: 8.89071259e-07
Iter: 985 loss: 8.88319221e-07
Iter: 986 loss: 8.88256693e-07
Iter: 987 loss: 8.87746296e-07
Iter: 988 loss: 8.86893758e-07
Iter: 989 loss: 8.94321886e-07
Iter: 990 loss: 8.86830946e-07
Iter: 991 loss: 8.8616531e-07
Iter: 992 loss: 8.89634464e-07
Iter: 993 loss: 8.86003e-07
Iter: 994 loss: 8.85236432e-07
Iter: 995 loss: 8.85319821e-07
Iter: 996 loss: 8.84702672e-07
Iter: 997 loss: 8.83718315e-07
Iter: 998 loss: 8.84605e-07
Iter: 999 loss: 8.8312504e-07
Iter: 1000 loss: 8.8234475e-07
Iter: 1001 loss: 8.82333552e-07
Iter: 1002 loss: 8.81720666e-07
Iter: 1003 loss: 8.83665166e-07
Iter: 1004 loss: 8.81494771e-07
Iter: 1005 loss: 8.81040705e-07
Iter: 1006 loss: 8.80565551e-07
Iter: 1007 loss: 8.8041287e-07
Iter: 1008 loss: 8.79821641e-07
Iter: 1009 loss: 8.79773722e-07
Iter: 1010 loss: 8.79289132e-07
Iter: 1011 loss: 8.80311291e-07
Iter: 1012 loss: 8.79096604e-07
Iter: 1013 loss: 8.78687842e-07
Iter: 1014 loss: 8.7804483e-07
Iter: 1015 loss: 8.78032324e-07
Iter: 1016 loss: 8.77127491e-07
Iter: 1017 loss: 8.77483615e-07
Iter: 1018 loss: 8.7649687e-07
Iter: 1019 loss: 8.75476e-07
Iter: 1020 loss: 8.82067752e-07
Iter: 1021 loss: 8.75350622e-07
Iter: 1022 loss: 8.74536624e-07
Iter: 1023 loss: 8.75670764e-07
Iter: 1024 loss: 8.74157081e-07
Iter: 1025 loss: 8.73262593e-07
Iter: 1026 loss: 8.78424032e-07
Iter: 1027 loss: 8.73109457e-07
Iter: 1028 loss: 8.72349915e-07
Iter: 1029 loss: 8.76735896e-07
Iter: 1030 loss: 8.72271357e-07
Iter: 1031 loss: 8.71595375e-07
Iter: 1032 loss: 8.71397447e-07
Iter: 1033 loss: 8.71034217e-07
Iter: 1034 loss: 8.70421331e-07
Iter: 1035 loss: 8.74783609e-07
Iter: 1036 loss: 8.70383246e-07
Iter: 1037 loss: 8.69654968e-07
Iter: 1038 loss: 8.70863573e-07
Iter: 1039 loss: 8.69337327e-07
Iter: 1040 loss: 8.68571078e-07
Iter: 1041 loss: 8.70260692e-07
Iter: 1042 loss: 8.68266909e-07
Iter: 1043 loss: 8.67939036e-07
Iter: 1044 loss: 8.67958249e-07
Iter: 1045 loss: 8.67574272e-07
Iter: 1046 loss: 8.66825758e-07
Iter: 1047 loss: 8.81243068e-07
Iter: 1048 loss: 8.66782671e-07
Iter: 1049 loss: 8.65928e-07
Iter: 1050 loss: 8.70291444e-07
Iter: 1051 loss: 8.65838274e-07
Iter: 1052 loss: 8.65274615e-07
Iter: 1053 loss: 8.64616482e-07
Iter: 1054 loss: 8.6454e-07
Iter: 1055 loss: 8.63713694e-07
Iter: 1056 loss: 8.68736038e-07
Iter: 1057 loss: 8.63634114e-07
Iter: 1058 loss: 8.6279249e-07
Iter: 1059 loss: 8.62635318e-07
Iter: 1060 loss: 8.62087745e-07
Iter: 1061 loss: 8.61221793e-07
Iter: 1062 loss: 8.72409544e-07
Iter: 1063 loss: 8.61193769e-07
Iter: 1064 loss: 8.60586738e-07
Iter: 1065 loss: 8.61840476e-07
Iter: 1066 loss: 8.60338616e-07
Iter: 1067 loss: 8.59456861e-07
Iter: 1068 loss: 8.61301601e-07
Iter: 1069 loss: 8.59082661e-07
Iter: 1070 loss: 8.58392241e-07
Iter: 1071 loss: 8.58936403e-07
Iter: 1072 loss: 8.57979899e-07
Iter: 1073 loss: 8.57285e-07
Iter: 1074 loss: 8.64072604e-07
Iter: 1075 loss: 8.57274642e-07
Iter: 1076 loss: 8.56696715e-07
Iter: 1077 loss: 8.58718522e-07
Iter: 1078 loss: 8.56511e-07
Iter: 1079 loss: 8.56082352e-07
Iter: 1080 loss: 8.57442046e-07
Iter: 1081 loss: 8.55935241e-07
Iter: 1082 loss: 8.55432859e-07
Iter: 1083 loss: 8.58673275e-07
Iter: 1084 loss: 8.55367603e-07
Iter: 1085 loss: 8.55027338e-07
Iter: 1086 loss: 8.54377618e-07
Iter: 1087 loss: 8.69920598e-07
Iter: 1088 loss: 8.54395466e-07
Iter: 1089 loss: 8.53747736e-07
Iter: 1090 loss: 8.58255476e-07
Iter: 1091 loss: 8.53654342e-07
Iter: 1092 loss: 8.53148606e-07
Iter: 1093 loss: 8.52229846e-07
Iter: 1094 loss: 8.5221609e-07
Iter: 1095 loss: 8.51225252e-07
Iter: 1096 loss: 8.58102567e-07
Iter: 1097 loss: 8.51148343e-07
Iter: 1098 loss: 8.5039e-07
Iter: 1099 loss: 8.50879928e-07
Iter: 1100 loss: 8.49853564e-07
Iter: 1101 loss: 8.48933e-07
Iter: 1102 loss: 8.59969532e-07
Iter: 1103 loss: 8.48956347e-07
Iter: 1104 loss: 8.48412583e-07
Iter: 1105 loss: 8.50755384e-07
Iter: 1106 loss: 8.48330671e-07
Iter: 1107 loss: 8.4779839e-07
Iter: 1108 loss: 8.47231888e-07
Iter: 1109 loss: 8.471294e-07
Iter: 1110 loss: 8.4638657e-07
Iter: 1111 loss: 8.54515e-07
Iter: 1112 loss: 8.46360479e-07
Iter: 1113 loss: 8.45805687e-07
Iter: 1114 loss: 8.48724824e-07
Iter: 1115 loss: 8.45692625e-07
Iter: 1116 loss: 8.4516887e-07
Iter: 1117 loss: 8.4642727e-07
Iter: 1118 loss: 8.44992371e-07
Iter: 1119 loss: 8.44450426e-07
Iter: 1120 loss: 8.48331183e-07
Iter: 1121 loss: 8.44407623e-07
Iter: 1122 loss: 8.44105841e-07
Iter: 1123 loss: 8.43428722e-07
Iter: 1124 loss: 8.52023618e-07
Iter: 1125 loss: 8.43405815e-07
Iter: 1126 loss: 8.42675377e-07
Iter: 1127 loss: 8.4918463e-07
Iter: 1128 loss: 8.42633483e-07
Iter: 1129 loss: 8.42041914e-07
Iter: 1130 loss: 8.41892586e-07
Iter: 1131 loss: 8.41559654e-07
Iter: 1132 loss: 8.40748612e-07
Iter: 1133 loss: 8.43601299e-07
Iter: 1134 loss: 8.40532607e-07
Iter: 1135 loss: 8.39845086e-07
Iter: 1136 loss: 8.39635391e-07
Iter: 1137 loss: 8.39246354e-07
Iter: 1138 loss: 8.38446226e-07
Iter: 1139 loss: 8.38444635e-07
Iter: 1140 loss: 8.37889161e-07
Iter: 1141 loss: 8.40203938e-07
Iter: 1142 loss: 8.3780526e-07
Iter: 1143 loss: 8.37243078e-07
Iter: 1144 loss: 8.36913159e-07
Iter: 1145 loss: 8.36702725e-07
Iter: 1146 loss: 8.35907144e-07
Iter: 1147 loss: 8.38955259e-07
Iter: 1148 loss: 8.35763103e-07
Iter: 1149 loss: 8.3522383e-07
Iter: 1150 loss: 8.35222522e-07
Iter: 1151 loss: 8.34852244e-07
Iter: 1152 loss: 8.35679145e-07
Iter: 1153 loss: 8.34718776e-07
Iter: 1154 loss: 8.34244872e-07
Iter: 1155 loss: 8.3507507e-07
Iter: 1156 loss: 8.34007039e-07
Iter: 1157 loss: 8.33537797e-07
Iter: 1158 loss: 8.33062927e-07
Iter: 1159 loss: 8.32945148e-07
Iter: 1160 loss: 8.32217438e-07
Iter: 1161 loss: 8.33127672e-07
Iter: 1162 loss: 8.31861655e-07
Iter: 1163 loss: 8.30988142e-07
Iter: 1164 loss: 8.38678204e-07
Iter: 1165 loss: 8.30969213e-07
Iter: 1166 loss: 8.30501563e-07
Iter: 1167 loss: 8.29892258e-07
Iter: 1168 loss: 8.29858209e-07
Iter: 1169 loss: 8.28935299e-07
Iter: 1170 loss: 8.33655236e-07
Iter: 1171 loss: 8.28738848e-07
Iter: 1172 loss: 8.28071336e-07
Iter: 1173 loss: 8.29499129e-07
Iter: 1174 loss: 8.2782509e-07
Iter: 1175 loss: 8.26953283e-07
Iter: 1176 loss: 8.32878186e-07
Iter: 1177 loss: 8.2690525e-07
Iter: 1178 loss: 8.26441124e-07
Iter: 1179 loss: 8.27380632e-07
Iter: 1180 loss: 8.26197379e-07
Iter: 1181 loss: 8.25741211e-07
Iter: 1182 loss: 8.26199539e-07
Iter: 1183 loss: 8.25439201e-07
Iter: 1184 loss: 8.249865e-07
Iter: 1185 loss: 8.24967458e-07
Iter: 1186 loss: 8.24628273e-07
Iter: 1187 loss: 8.25519066e-07
Iter: 1188 loss: 8.24503957e-07
Iter: 1189 loss: 8.24041308e-07
Iter: 1190 loss: 8.24435176e-07
Iter: 1191 loss: 8.23751634e-07
Iter: 1192 loss: 8.23363962e-07
Iter: 1193 loss: 8.23087703e-07
Iter: 1194 loss: 8.22928087e-07
Iter: 1195 loss: 8.22248523e-07
Iter: 1196 loss: 8.22575146e-07
Iter: 1197 loss: 8.21782578e-07
Iter: 1198 loss: 8.20895821e-07
Iter: 1199 loss: 8.2857639e-07
Iter: 1200 loss: 8.20860407e-07
Iter: 1201 loss: 8.20232913e-07
Iter: 1202 loss: 8.20954313e-07
Iter: 1203 loss: 8.19867182e-07
Iter: 1204 loss: 8.19319268e-07
Iter: 1205 loss: 8.1986e-07
Iter: 1206 loss: 8.18951776e-07
Iter: 1207 loss: 8.1817393e-07
Iter: 1208 loss: 8.19178581e-07
Iter: 1209 loss: 8.17789328e-07
Iter: 1210 loss: 8.16992042e-07
Iter: 1211 loss: 8.26021505e-07
Iter: 1212 loss: 8.16984141e-07
Iter: 1213 loss: 8.1637711e-07
Iter: 1214 loss: 8.18538865e-07
Iter: 1215 loss: 8.1620226e-07
Iter: 1216 loss: 8.15729436e-07
Iter: 1217 loss: 8.15686235e-07
Iter: 1218 loss: 8.15231033e-07
Iter: 1219 loss: 8.14637701e-07
Iter: 1220 loss: 8.21607728e-07
Iter: 1221 loss: 8.14587338e-07
Iter: 1222 loss: 8.14062673e-07
Iter: 1223 loss: 8.17092939e-07
Iter: 1224 loss: 8.13979739e-07
Iter: 1225 loss: 8.13607471e-07
Iter: 1226 loss: 8.14955683e-07
Iter: 1227 loss: 8.13503e-07
Iter: 1228 loss: 8.13134704e-07
Iter: 1229 loss: 8.12497547e-07
Iter: 1230 loss: 8.25655491e-07
Iter: 1231 loss: 8.12503345e-07
Iter: 1232 loss: 8.11718905e-07
Iter: 1233 loss: 8.14447105e-07
Iter: 1234 loss: 8.11476752e-07
Iter: 1235 loss: 8.10842778e-07
Iter: 1236 loss: 8.1188665e-07
Iter: 1237 loss: 8.10522408e-07
Iter: 1238 loss: 8.09831363e-07
Iter: 1239 loss: 8.157308e-07
Iter: 1240 loss: 8.09817607e-07
Iter: 1241 loss: 8.09355925e-07
Iter: 1242 loss: 8.09374114e-07
Iter: 1243 loss: 8.09078074e-07
Iter: 1244 loss: 8.08335e-07
Iter: 1245 loss: 8.08933e-07
Iter: 1246 loss: 8.07890387e-07
Iter: 1247 loss: 8.07296772e-07
Iter: 1248 loss: 8.11403879e-07
Iter: 1249 loss: 8.07220886e-07
Iter: 1250 loss: 8.06543767e-07
Iter: 1251 loss: 8.08372306e-07
Iter: 1252 loss: 8.06300818e-07
Iter: 1253 loss: 8.05724483e-07
Iter: 1254 loss: 8.06674336e-07
Iter: 1255 loss: 8.05462946e-07
Iter: 1256 loss: 8.04962099e-07
Iter: 1257 loss: 8.11520465e-07
Iter: 1258 loss: 8.04947e-07
Iter: 1259 loss: 8.04507351e-07
Iter: 1260 loss: 8.05708964e-07
Iter: 1261 loss: 8.04407e-07
Iter: 1262 loss: 8.04010142e-07
Iter: 1263 loss: 8.04683737e-07
Iter: 1264 loss: 8.0385712e-07
Iter: 1265 loss: 8.03400269e-07
Iter: 1266 loss: 8.02915167e-07
Iter: 1267 loss: 8.02807449e-07
Iter: 1268 loss: 8.02217698e-07
Iter: 1269 loss: 8.03423291e-07
Iter: 1270 loss: 8.01983219e-07
Iter: 1271 loss: 8.01280237e-07
Iter: 1272 loss: 8.03210241e-07
Iter: 1273 loss: 8.01033252e-07
Iter: 1274 loss: 8.00482894e-07
Iter: 1275 loss: 8.04053855e-07
Iter: 1276 loss: 8.00441285e-07
Iter: 1277 loss: 7.99889676e-07
Iter: 1278 loss: 8.00854764e-07
Iter: 1279 loss: 7.99624161e-07
Iter: 1280 loss: 7.99103532e-07
Iter: 1281 loss: 7.98702274e-07
Iter: 1282 loss: 7.98504743e-07
Iter: 1283 loss: 7.97609289e-07
Iter: 1284 loss: 8.0215375e-07
Iter: 1285 loss: 7.97467578e-07
Iter: 1286 loss: 7.9681331e-07
Iter: 1287 loss: 8.01009321e-07
Iter: 1288 loss: 7.96748054e-07
Iter: 1289 loss: 7.96193149e-07
Iter: 1290 loss: 7.99827e-07
Iter: 1291 loss: 7.96140512e-07
Iter: 1292 loss: 7.95761309e-07
Iter: 1293 loss: 7.96006134e-07
Iter: 1294 loss: 7.9560823e-07
Iter: 1295 loss: 7.95145695e-07
Iter: 1296 loss: 8.00713963e-07
Iter: 1297 loss: 7.95151664e-07
Iter: 1298 loss: 7.94746938e-07
Iter: 1299 loss: 7.94254447e-07
Iter: 1300 loss: 7.94199764e-07
Iter: 1301 loss: 7.93510935e-07
Iter: 1302 loss: 7.98085807e-07
Iter: 1303 loss: 7.93462107e-07
Iter: 1304 loss: 7.93051754e-07
Iter: 1305 loss: 7.9259064e-07
Iter: 1306 loss: 7.92518222e-07
Iter: 1307 loss: 7.91845764e-07
Iter: 1308 loss: 7.94605171e-07
Iter: 1309 loss: 7.91739808e-07
Iter: 1310 loss: 7.91152956e-07
Iter: 1311 loss: 7.92137257e-07
Iter: 1312 loss: 7.90897843e-07
Iter: 1313 loss: 7.90253353e-07
Iter: 1314 loss: 7.93460345e-07
Iter: 1315 loss: 7.90162858e-07
Iter: 1316 loss: 7.8960079e-07
Iter: 1317 loss: 7.91113337e-07
Iter: 1318 loss: 7.8941423e-07
Iter: 1319 loss: 7.88813736e-07
Iter: 1320 loss: 7.89202659e-07
Iter: 1321 loss: 7.88438342e-07
Iter: 1322 loss: 7.87834097e-07
Iter: 1323 loss: 7.88959937e-07
Iter: 1324 loss: 7.87520321e-07
Iter: 1325 loss: 7.87077738e-07
Iter: 1326 loss: 7.870334e-07
Iter: 1327 loss: 7.86535452e-07
Iter: 1328 loss: 7.86462408e-07
Iter: 1329 loss: 7.86150736e-07
Iter: 1330 loss: 7.85825591e-07
Iter: 1331 loss: 7.85780571e-07
Iter: 1332 loss: 7.85470434e-07
Iter: 1333 loss: 7.85113116e-07
Iter: 1334 loss: 7.85107488e-07
Iter: 1335 loss: 7.84641713e-07
Iter: 1336 loss: 7.86704561e-07
Iter: 1337 loss: 7.84579e-07
Iter: 1338 loss: 7.8418077e-07
Iter: 1339 loss: 7.840938e-07
Iter: 1340 loss: 7.83850453e-07
Iter: 1341 loss: 7.83306291e-07
Iter: 1342 loss: 7.8284927e-07
Iter: 1343 loss: 7.82708071e-07
Iter: 1344 loss: 7.82105417e-07
Iter: 1345 loss: 7.91235607e-07
Iter: 1346 loss: 7.82062784e-07
Iter: 1347 loss: 7.81585868e-07
Iter: 1348 loss: 7.81446261e-07
Iter: 1349 loss: 7.81145673e-07
Iter: 1350 loss: 7.80403e-07
Iter: 1351 loss: 7.86978148e-07
Iter: 1352 loss: 7.80386245e-07
Iter: 1353 loss: 7.79883521e-07
Iter: 1354 loss: 7.80534037e-07
Iter: 1355 loss: 7.79637503e-07
Iter: 1356 loss: 7.7902024e-07
Iter: 1357 loss: 7.79048833e-07
Iter: 1358 loss: 7.78542e-07
Iter: 1359 loss: 7.77796515e-07
Iter: 1360 loss: 7.81716608e-07
Iter: 1361 loss: 7.77633829e-07
Iter: 1362 loss: 7.77095806e-07
Iter: 1363 loss: 7.84275699e-07
Iter: 1364 loss: 7.77086314e-07
Iter: 1365 loss: 7.76729507e-07
Iter: 1366 loss: 7.78452886e-07
Iter: 1367 loss: 7.76662603e-07
Iter: 1368 loss: 7.76342404e-07
Iter: 1369 loss: 7.77790092e-07
Iter: 1370 loss: 7.76276693e-07
Iter: 1371 loss: 7.76052843e-07
Iter: 1372 loss: 7.75715307e-07
Iter: 1373 loss: 7.75680405e-07
Iter: 1374 loss: 7.75216e-07
Iter: 1375 loss: 7.78651781e-07
Iter: 1376 loss: 7.75190301e-07
Iter: 1377 loss: 7.74838554e-07
Iter: 1378 loss: 7.74288651e-07
Iter: 1379 loss: 7.74286775e-07
Iter: 1380 loss: 7.73571855e-07
Iter: 1381 loss: 7.74871921e-07
Iter: 1382 loss: 7.73272291e-07
Iter: 1383 loss: 7.72462897e-07
Iter: 1384 loss: 7.75460649e-07
Iter: 1385 loss: 7.72274859e-07
Iter: 1386 loss: 7.71689884e-07
Iter: 1387 loss: 7.77772811e-07
Iter: 1388 loss: 7.71672831e-07
Iter: 1389 loss: 7.71238206e-07
Iter: 1390 loss: 7.71525322e-07
Iter: 1391 loss: 7.70968654e-07
Iter: 1392 loss: 7.70383963e-07
Iter: 1393 loss: 7.71820623e-07
Iter: 1394 loss: 7.70226848e-07
Iter: 1395 loss: 7.69655628e-07
Iter: 1396 loss: 7.70572342e-07
Iter: 1397 loss: 7.69424275e-07
Iter: 1398 loss: 7.68921382e-07
Iter: 1399 loss: 7.71586201e-07
Iter: 1400 loss: 7.6884146e-07
Iter: 1401 loss: 7.68294399e-07
Iter: 1402 loss: 7.71391569e-07
Iter: 1403 loss: 7.68246537e-07
Iter: 1404 loss: 7.67828055e-07
Iter: 1405 loss: 7.71469445e-07
Iter: 1406 loss: 7.67840049e-07
Iter: 1407 loss: 7.67603069e-07
Iter: 1408 loss: 7.67277356e-07
Iter: 1409 loss: 7.67258143e-07
Iter: 1410 loss: 7.66812605e-07
Iter: 1411 loss: 7.67668212e-07
Iter: 1412 loss: 7.66639801e-07
Iter: 1413 loss: 7.66060339e-07
Iter: 1414 loss: 7.68384439e-07
Iter: 1415 loss: 7.65932043e-07
Iter: 1416 loss: 7.65542609e-07
Iter: 1417 loss: 7.64962124e-07
Iter: 1418 loss: 7.64925289e-07
Iter: 1419 loss: 7.64258857e-07
Iter: 1420 loss: 7.68460097e-07
Iter: 1421 loss: 7.64171e-07
Iter: 1422 loss: 7.63617209e-07
Iter: 1423 loss: 7.64917218e-07
Iter: 1424 loss: 7.63452192e-07
Iter: 1425 loss: 7.628459e-07
Iter: 1426 loss: 7.67626886e-07
Iter: 1427 loss: 7.62848686e-07
Iter: 1428 loss: 7.62431114e-07
Iter: 1429 loss: 7.6287472e-07
Iter: 1430 loss: 7.62240745e-07
Iter: 1431 loss: 7.61722845e-07
Iter: 1432 loss: 7.62143145e-07
Iter: 1433 loss: 7.61417084e-07
Iter: 1434 loss: 7.60704665e-07
Iter: 1435 loss: 7.61959029e-07
Iter: 1436 loss: 7.60365424e-07
Iter: 1437 loss: 7.60034254e-07
Iter: 1438 loss: 7.5995e-07
Iter: 1439 loss: 7.59609634e-07
Iter: 1440 loss: 7.6011986e-07
Iter: 1441 loss: 7.59428701e-07
Iter: 1442 loss: 7.58958095e-07
Iter: 1443 loss: 7.59115665e-07
Iter: 1444 loss: 7.58673252e-07
Iter: 1445 loss: 7.58233739e-07
Iter: 1446 loss: 7.59124191e-07
Iter: 1447 loss: 7.58054171e-07
Iter: 1448 loss: 7.57752389e-07
Iter: 1449 loss: 7.59784257e-07
Iter: 1450 loss: 7.57691396e-07
Iter: 1451 loss: 7.57336e-07
Iter: 1452 loss: 7.57250689e-07
Iter: 1453 loss: 7.57013368e-07
Iter: 1454 loss: 7.56500242e-07
Iter: 1455 loss: 7.56236204e-07
Iter: 1456 loss: 7.5600849e-07
Iter: 1457 loss: 7.55338363e-07
Iter: 1458 loss: 7.59327747e-07
Iter: 1459 loss: 7.55242e-07
Iter: 1460 loss: 7.54675398e-07
Iter: 1461 loss: 7.56114503e-07
Iter: 1462 loss: 7.54468374e-07
Iter: 1463 loss: 7.53994073e-07
Iter: 1464 loss: 7.58961278e-07
Iter: 1465 loss: 7.53981681e-07
Iter: 1466 loss: 7.53544498e-07
Iter: 1467 loss: 7.53007157e-07
Iter: 1468 loss: 7.52982146e-07
Iter: 1469 loss: 7.5218793e-07
Iter: 1470 loss: 7.57336124e-07
Iter: 1471 loss: 7.52117217e-07
Iter: 1472 loss: 7.51737957e-07
Iter: 1473 loss: 7.52436847e-07
Iter: 1474 loss: 7.51550829e-07
Iter: 1475 loss: 7.5130896e-07
Iter: 1476 loss: 7.51260586e-07
Iter: 1477 loss: 7.50997742e-07
Iter: 1478 loss: 7.51075959e-07
Iter: 1479 loss: 7.50813456e-07
Iter: 1480 loss: 7.50528841e-07
Iter: 1481 loss: 7.50837216e-07
Iter: 1482 loss: 7.50358822e-07
Iter: 1483 loss: 7.49932042e-07
Iter: 1484 loss: 7.49816195e-07
Iter: 1485 loss: 7.49576316e-07
Iter: 1486 loss: 7.49028e-07
Iter: 1487 loss: 7.52072424e-07
Iter: 1488 loss: 7.48977072e-07
Iter: 1489 loss: 7.48448088e-07
Iter: 1490 loss: 7.49468711e-07
Iter: 1491 loss: 7.48251e-07
Iter: 1492 loss: 7.47820707e-07
Iter: 1493 loss: 7.47717934e-07
Iter: 1494 loss: 7.47416e-07
Iter: 1495 loss: 7.46868579e-07
Iter: 1496 loss: 7.49227809e-07
Iter: 1497 loss: 7.46724481e-07
Iter: 1498 loss: 7.46221872e-07
Iter: 1499 loss: 7.46516889e-07
Iter: 1500 loss: 7.45839941e-07
Iter: 1501 loss: 7.45214095e-07
Iter: 1502 loss: 7.5145806e-07
Iter: 1503 loss: 7.45201e-07
Iter: 1504 loss: 7.44763e-07
Iter: 1505 loss: 7.4584068e-07
Iter: 1506 loss: 7.44559713e-07
Iter: 1507 loss: 7.44056763e-07
Iter: 1508 loss: 7.44171473e-07
Iter: 1509 loss: 7.43674264e-07
Iter: 1510 loss: 7.43208375e-07
Iter: 1511 loss: 7.46041e-07
Iter: 1512 loss: 7.43106057e-07
Iter: 1513 loss: 7.42742543e-07
Iter: 1514 loss: 7.46054411e-07
Iter: 1515 loss: 7.42728218e-07
Iter: 1516 loss: 7.4232878e-07
Iter: 1517 loss: 7.44320289e-07
Iter: 1518 loss: 7.42267105e-07
Iter: 1519 loss: 7.4195583e-07
Iter: 1520 loss: 7.41714871e-07
Iter: 1521 loss: 7.41575832e-07
Iter: 1522 loss: 7.40986138e-07
Iter: 1523 loss: 7.42011878e-07
Iter: 1524 loss: 7.4069203e-07
Iter: 1525 loss: 7.40331586e-07
Iter: 1526 loss: 7.42168652e-07
Iter: 1527 loss: 7.40286907e-07
Iter: 1528 loss: 7.39885763e-07
Iter: 1529 loss: 7.39946756e-07
Iter: 1530 loss: 7.39550728e-07
Iter: 1531 loss: 7.39008726e-07
Iter: 1532 loss: 7.3970773e-07
Iter: 1533 loss: 7.38731728e-07
Iter: 1534 loss: 7.38144706e-07
Iter: 1535 loss: 7.38446886e-07
Iter: 1536 loss: 7.37776077e-07
Iter: 1537 loss: 7.37113737e-07
Iter: 1538 loss: 7.40912697e-07
Iter: 1539 loss: 7.36998231e-07
Iter: 1540 loss: 7.36468678e-07
Iter: 1541 loss: 7.39496841e-07
Iter: 1542 loss: 7.36392792e-07
Iter: 1543 loss: 7.35978801e-07
Iter: 1544 loss: 7.38032554e-07
Iter: 1545 loss: 7.35904e-07
Iter: 1546 loss: 7.3559147e-07
Iter: 1547 loss: 7.35423214e-07
Iter: 1548 loss: 7.35229662e-07
Iter: 1549 loss: 7.34642867e-07
Iter: 1550 loss: 7.35721528e-07
Iter: 1551 loss: 7.34432888e-07
Iter: 1552 loss: 7.34176069e-07
Iter: 1553 loss: 7.34065907e-07
Iter: 1554 loss: 7.33815625e-07
Iter: 1555 loss: 7.33798572e-07
Iter: 1556 loss: 7.33610818e-07
Iter: 1557 loss: 7.33287e-07
Iter: 1558 loss: 7.338042e-07
Iter: 1559 loss: 7.33141633e-07
Iter: 1560 loss: 7.327871e-07
Iter: 1561 loss: 7.33022e-07
Iter: 1562 loss: 7.32548756e-07
Iter: 1563 loss: 7.32189392e-07
Iter: 1564 loss: 7.34083926e-07
Iter: 1565 loss: 7.32109e-07
Iter: 1566 loss: 7.31697583e-07
Iter: 1567 loss: 7.32139597e-07
Iter: 1568 loss: 7.31459238e-07
Iter: 1569 loss: 7.3099136e-07
Iter: 1570 loss: 7.31062187e-07
Iter: 1571 loss: 7.30654961e-07
Iter: 1572 loss: 7.30070781e-07
Iter: 1573 loss: 7.30843908e-07
Iter: 1574 loss: 7.29744897e-07
Iter: 1575 loss: 7.29166e-07
Iter: 1576 loss: 7.33337401e-07
Iter: 1577 loss: 7.2908864e-07
Iter: 1578 loss: 7.28625309e-07
Iter: 1579 loss: 7.29832323e-07
Iter: 1580 loss: 7.28484906e-07
Iter: 1581 loss: 7.27983718e-07
Iter: 1582 loss: 7.30834699e-07
Iter: 1583 loss: 7.27953534e-07
Iter: 1584 loss: 7.27570125e-07
Iter: 1585 loss: 7.27186148e-07
Iter: 1586 loss: 7.27117936e-07
Iter: 1587 loss: 7.27173244e-07
Iter: 1588 loss: 7.26844178e-07
Iter: 1589 loss: 7.26645e-07
Iter: 1590 loss: 7.26609358e-07
Iter: 1591 loss: 7.26486e-07
Iter: 1592 loss: 7.26251699e-07
Iter: 1593 loss: 7.26440703e-07
Iter: 1594 loss: 7.26097142e-07
Iter: 1595 loss: 7.25700602e-07
Iter: 1596 loss: 7.25842142e-07
Iter: 1597 loss: 7.25432699e-07
Iter: 1598 loss: 7.24993129e-07
Iter: 1599 loss: 7.26181668e-07
Iter: 1600 loss: 7.24813049e-07
Iter: 1601 loss: 7.24336871e-07
Iter: 1602 loss: 7.26857e-07
Iter: 1603 loss: 7.24251208e-07
Iter: 1604 loss: 7.23838752e-07
Iter: 1605 loss: 7.23520145e-07
Iter: 1606 loss: 7.23380595e-07
Iter: 1607 loss: 7.22757079e-07
Iter: 1608 loss: 7.25213795e-07
Iter: 1609 loss: 7.22617642e-07
Iter: 1610 loss: 7.22163293e-07
Iter: 1611 loss: 7.22805339e-07
Iter: 1612 loss: 7.21941433e-07
Iter: 1613 loss: 7.2134867e-07
Iter: 1614 loss: 7.22828304e-07
Iter: 1615 loss: 7.21149092e-07
Iter: 1616 loss: 7.20601918e-07
Iter: 1617 loss: 7.23760195e-07
Iter: 1618 loss: 7.20522905e-07
Iter: 1619 loss: 7.20036326e-07
Iter: 1620 loss: 7.22516518e-07
Iter: 1621 loss: 7.19924458e-07
Iter: 1622 loss: 7.1962279e-07
Iter: 1623 loss: 7.21505444e-07
Iter: 1624 loss: 7.19596926e-07
Iter: 1625 loss: 7.19178047e-07
Iter: 1626 loss: 7.19988293e-07
Iter: 1627 loss: 7.19039917e-07
Iter: 1628 loss: 7.18741035e-07
Iter: 1629 loss: 7.18598926e-07
Iter: 1630 loss: 7.18449655e-07
Iter: 1631 loss: 7.17942726e-07
Iter: 1632 loss: 7.19415084e-07
Iter: 1633 loss: 7.17735588e-07
Iter: 1634 loss: 7.17224566e-07
Iter: 1635 loss: 7.18000138e-07
Iter: 1636 loss: 7.1697923e-07
Iter: 1637 loss: 7.16577802e-07
Iter: 1638 loss: 7.20528192e-07
Iter: 1639 loss: 7.16549e-07
Iter: 1640 loss: 7.16178533e-07
Iter: 1641 loss: 7.15868168e-07
Iter: 1642 loss: 7.15784267e-07
Iter: 1643 loss: 7.15276883e-07
Iter: 1644 loss: 7.17541411e-07
Iter: 1645 loss: 7.15169676e-07
Iter: 1646 loss: 7.1469367e-07
Iter: 1647 loss: 7.14488692e-07
Iter: 1648 loss: 7.1427e-07
Iter: 1649 loss: 7.13614e-07
Iter: 1650 loss: 7.1901087e-07
Iter: 1651 loss: 7.1358852e-07
Iter: 1652 loss: 7.13156282e-07
Iter: 1653 loss: 7.13448344e-07
Iter: 1654 loss: 7.12947553e-07
Iter: 1655 loss: 7.1244915e-07
Iter: 1656 loss: 7.16758734e-07
Iter: 1657 loss: 7.12415044e-07
Iter: 1658 loss: 7.12030442e-07
Iter: 1659 loss: 7.13850454e-07
Iter: 1660 loss: 7.1193665e-07
Iter: 1661 loss: 7.1161071e-07
Iter: 1662 loss: 7.15535407e-07
Iter: 1663 loss: 7.11611619e-07
Iter: 1664 loss: 7.11432222e-07
Iter: 1665 loss: 7.1097395e-07
Iter: 1666 loss: 7.15148076e-07
Iter: 1667 loss: 7.10916368e-07
Iter: 1668 loss: 7.10414611e-07
Iter: 1669 loss: 7.13566692e-07
Iter: 1670 loss: 7.10379879e-07
Iter: 1671 loss: 7.09926098e-07
Iter: 1672 loss: 7.12190683e-07
Iter: 1673 loss: 7.09851747e-07
Iter: 1674 loss: 7.09513188e-07
Iter: 1675 loss: 7.09334813e-07
Iter: 1676 loss: 7.09136543e-07
Iter: 1677 loss: 7.08741368e-07
Iter: 1678 loss: 7.1330112e-07
Iter: 1679 loss: 7.08755806e-07
Iter: 1680 loss: 7.08404457e-07
Iter: 1681 loss: 7.08375183e-07
Iter: 1682 loss: 7.08112225e-07
Iter: 1683 loss: 7.07587276e-07
Iter: 1684 loss: 7.08143375e-07
Iter: 1685 loss: 7.07310164e-07
Iter: 1686 loss: 7.06882133e-07
Iter: 1687 loss: 7.08457151e-07
Iter: 1688 loss: 7.06767082e-07
Iter: 1689 loss: 7.06370088e-07
Iter: 1690 loss: 7.07504228e-07
Iter: 1691 loss: 7.06242872e-07
Iter: 1692 loss: 7.05759362e-07
Iter: 1693 loss: 7.05617e-07
Iter: 1694 loss: 7.05324851e-07
Iter: 1695 loss: 7.05019033e-07
Iter: 1696 loss: 7.04954e-07
Iter: 1697 loss: 7.04687295e-07
Iter: 1698 loss: 7.07085348e-07
Iter: 1699 loss: 7.04685704e-07
Iter: 1700 loss: 7.04485672e-07
Iter: 1701 loss: 7.04217712e-07
Iter: 1702 loss: 7.04189461e-07
Iter: 1703 loss: 7.03850787e-07
Iter: 1704 loss: 7.04370109e-07
Iter: 1705 loss: 7.03650812e-07
Iter: 1706 loss: 7.03245746e-07
Iter: 1707 loss: 7.02858301e-07
Iter: 1708 loss: 7.02753e-07
Iter: 1709 loss: 7.02097282e-07
Iter: 1710 loss: 7.07181471e-07
Iter: 1711 loss: 7.02018269e-07
Iter: 1712 loss: 7.01582621e-07
Iter: 1713 loss: 7.05891921e-07
Iter: 1714 loss: 7.01549652e-07
Iter: 1715 loss: 7.01174713e-07
Iter: 1716 loss: 7.01557383e-07
Iter: 1717 loss: 7.0099e-07
Iter: 1718 loss: 7.00598e-07
Iter: 1719 loss: 7.00752651e-07
Iter: 1720 loss: 7.00370379e-07
Iter: 1721 loss: 6.99968382e-07
Iter: 1722 loss: 7.04487e-07
Iter: 1723 loss: 6.99942575e-07
Iter: 1724 loss: 6.99530801e-07
Iter: 1725 loss: 6.99553709e-07
Iter: 1726 loss: 6.99243742e-07
Iter: 1727 loss: 6.98773249e-07
Iter: 1728 loss: 7.00226678e-07
Iter: 1729 loss: 6.98670362e-07
Iter: 1730 loss: 6.98265922e-07
Iter: 1731 loss: 6.98307304e-07
Iter: 1732 loss: 6.97958114e-07
Iter: 1733 loss: 6.98104714e-07
Iter: 1734 loss: 6.97744895e-07
Iter: 1735 loss: 6.97600285e-07
Iter: 1736 loss: 6.97349321e-07
Iter: 1737 loss: 7.02991656e-07
Iter: 1738 loss: 6.97335508e-07
Iter: 1739 loss: 6.9697478e-07
Iter: 1740 loss: 6.96840743e-07
Iter: 1741 loss: 6.9665964e-07
Iter: 1742 loss: 6.96092229e-07
Iter: 1743 loss: 6.99150291e-07
Iter: 1744 loss: 6.96017594e-07
Iter: 1745 loss: 6.95560573e-07
Iter: 1746 loss: 6.96130485e-07
Iter: 1747 loss: 6.95285166e-07
Iter: 1748 loss: 6.9489397e-07
Iter: 1749 loss: 6.9695534e-07
Iter: 1750 loss: 6.9481024e-07
Iter: 1751 loss: 6.94449e-07
Iter: 1752 loss: 6.96491497e-07
Iter: 1753 loss: 6.94405344e-07
Iter: 1754 loss: 6.94057121e-07
Iter: 1755 loss: 6.94021821e-07
Iter: 1756 loss: 6.93823267e-07
Iter: 1757 loss: 6.9338904e-07
Iter: 1758 loss: 6.93906372e-07
Iter: 1759 loss: 6.93195773e-07
Iter: 1760 loss: 6.92797e-07
Iter: 1761 loss: 6.92801109e-07
Iter: 1762 loss: 6.92517801e-07
Iter: 1763 loss: 6.92564868e-07
Iter: 1764 loss: 6.92256606e-07
Iter: 1765 loss: 6.91871264e-07
Iter: 1766 loss: 6.92001208e-07
Iter: 1767 loss: 6.91614787e-07
Iter: 1768 loss: 6.91104049e-07
Iter: 1769 loss: 6.9268242e-07
Iter: 1770 loss: 6.90965066e-07
Iter: 1771 loss: 6.90872525e-07
Iter: 1772 loss: 6.90706315e-07
Iter: 1773 loss: 6.9053408e-07
Iter: 1774 loss: 6.90238721e-07
Iter: 1775 loss: 6.97560324e-07
Iter: 1776 loss: 6.90230252e-07
Iter: 1777 loss: 6.89881176e-07
Iter: 1778 loss: 6.89929834e-07
Iter: 1779 loss: 6.89637091e-07
Iter: 1780 loss: 6.89190301e-07
Iter: 1781 loss: 6.92743811e-07
Iter: 1782 loss: 6.89171031e-07
Iter: 1783 loss: 6.88836735e-07
Iter: 1784 loss: 6.88748742e-07
Iter: 1785 loss: 6.88589353e-07
Iter: 1786 loss: 6.88078103e-07
Iter: 1787 loss: 6.9002823e-07
Iter: 1788 loss: 6.87979423e-07
Iter: 1789 loss: 6.87596582e-07
Iter: 1790 loss: 6.91350692e-07
Iter: 1791 loss: 6.87564579e-07
Iter: 1792 loss: 6.87267516e-07
Iter: 1793 loss: 6.87042302e-07
Iter: 1794 loss: 6.86952148e-07
Iter: 1795 loss: 6.86550493e-07
Iter: 1796 loss: 6.87208967e-07
Iter: 1797 loss: 6.86389967e-07
Iter: 1798 loss: 6.85962902e-07
Iter: 1799 loss: 6.9118289e-07
Iter: 1800 loss: 6.85951761e-07
Iter: 1801 loss: 6.85595296e-07
Iter: 1802 loss: 6.85809198e-07
Iter: 1803 loss: 6.85359794e-07
Iter: 1804 loss: 6.85028e-07
Iter: 1805 loss: 6.84993495e-07
Iter: 1806 loss: 6.84713427e-07
Iter: 1807 loss: 6.84301938e-07
Iter: 1808 loss: 6.86706869e-07
Iter: 1809 loss: 6.84251688e-07
Iter: 1810 loss: 6.83914664e-07
Iter: 1811 loss: 6.8713689e-07
Iter: 1812 loss: 6.83916483e-07
Iter: 1813 loss: 6.83623341e-07
Iter: 1814 loss: 6.84255554e-07
Iter: 1815 loss: 6.83451219e-07
Iter: 1816 loss: 6.83253688e-07
Iter: 1817 loss: 6.83033477e-07
Iter: 1818 loss: 6.82998461e-07
Iter: 1819 loss: 6.82611869e-07
Iter: 1820 loss: 6.83622034e-07
Iter: 1821 loss: 6.82512166e-07
Iter: 1822 loss: 6.82104314e-07
Iter: 1823 loss: 6.82875452e-07
Iter: 1824 loss: 6.81933273e-07
Iter: 1825 loss: 6.81557538e-07
Iter: 1826 loss: 6.82058328e-07
Iter: 1827 loss: 6.81350969e-07
Iter: 1828 loss: 6.80878202e-07
Iter: 1829 loss: 6.80901621e-07
Iter: 1830 loss: 6.80495646e-07
Iter: 1831 loss: 6.80260769e-07
Iter: 1832 loss: 6.80154e-07
Iter: 1833 loss: 6.79902087e-07
Iter: 1834 loss: 6.79781465e-07
Iter: 1835 loss: 6.79650725e-07
Iter: 1836 loss: 6.79287155e-07
Iter: 1837 loss: 6.79918571e-07
Iter: 1838 loss: 6.79150276e-07
Iter: 1839 loss: 6.78788126e-07
Iter: 1840 loss: 6.8120687e-07
Iter: 1841 loss: 6.78754816e-07
Iter: 1842 loss: 6.78426204e-07
Iter: 1843 loss: 6.79091215e-07
Iter: 1844 loss: 6.78260051e-07
Iter: 1845 loss: 6.77955427e-07
Iter: 1846 loss: 6.78107085e-07
Iter: 1847 loss: 6.77738342e-07
Iter: 1848 loss: 6.77561388e-07
Iter: 1849 loss: 6.77528703e-07
Iter: 1850 loss: 6.77284447e-07
Iter: 1851 loss: 6.76812761e-07
Iter: 1852 loss: 6.81352162e-07
Iter: 1853 loss: 6.76743866e-07
Iter: 1854 loss: 6.76289e-07
Iter: 1855 loss: 6.78761126e-07
Iter: 1856 loss: 6.76230343e-07
Iter: 1857 loss: 6.758616e-07
Iter: 1858 loss: 6.76458342e-07
Iter: 1859 loss: 6.75691297e-07
Iter: 1860 loss: 6.75257638e-07
Iter: 1861 loss: 6.76916557e-07
Iter: 1862 loss: 6.75162767e-07
Iter: 1863 loss: 6.74753039e-07
Iter: 1864 loss: 6.74690341e-07
Iter: 1865 loss: 6.74407033e-07
Iter: 1866 loss: 6.73965872e-07
Iter: 1867 loss: 6.75983358e-07
Iter: 1868 loss: 6.73904424e-07
Iter: 1869 loss: 6.73453826e-07
Iter: 1870 loss: 6.75994215e-07
Iter: 1871 loss: 6.73382317e-07
Iter: 1872 loss: 6.73015336e-07
Iter: 1873 loss: 6.73528632e-07
Iter: 1874 loss: 6.72848557e-07
Iter: 1875 loss: 6.72546207e-07
Iter: 1876 loss: 6.72857368e-07
Iter: 1877 loss: 6.72356691e-07
Iter: 1878 loss: 6.71954695e-07
Iter: 1879 loss: 6.7474889e-07
Iter: 1880 loss: 6.71900352e-07
Iter: 1881 loss: 6.71477778e-07
Iter: 1882 loss: 6.71422526e-07
Iter: 1883 loss: 6.71123e-07
Iter: 1884 loss: 6.71156158e-07
Iter: 1885 loss: 6.70957377e-07
Iter: 1886 loss: 6.70818054e-07
Iter: 1887 loss: 6.70549241e-07
Iter: 1888 loss: 6.75013098e-07
Iter: 1889 loss: 6.70544921e-07
Iter: 1890 loss: 6.70240752e-07
Iter: 1891 loss: 6.7003981e-07
Iter: 1892 loss: 6.69934e-07
Iter: 1893 loss: 6.69519522e-07
Iter: 1894 loss: 6.73777777e-07
Iter: 1895 loss: 6.69519295e-07
Iter: 1896 loss: 6.69209385e-07
Iter: 1897 loss: 6.69267592e-07
Iter: 1898 loss: 6.68949156e-07
Iter: 1899 loss: 6.68522318e-07
Iter: 1900 loss: 6.71283601e-07
Iter: 1901 loss: 6.68434495e-07
Iter: 1902 loss: 6.68128848e-07
Iter: 1903 loss: 6.68065866e-07
Iter: 1904 loss: 6.67852873e-07
Iter: 1905 loss: 6.67540519e-07
Iter: 1906 loss: 6.72802912e-07
Iter: 1907 loss: 6.67539098e-07
Iter: 1908 loss: 6.67290863e-07
Iter: 1909 loss: 6.67344636e-07
Iter: 1910 loss: 6.67119707e-07
Iter: 1911 loss: 6.66736639e-07
Iter: 1912 loss: 6.66666779e-07
Iter: 1913 loss: 6.66453843e-07
Iter: 1914 loss: 6.66122787e-07
Iter: 1915 loss: 6.66096184e-07
Iter: 1916 loss: 6.6585136e-07
Iter: 1917 loss: 6.65831635e-07
Iter: 1918 loss: 6.65650873e-07
Iter: 1919 loss: 6.65438847e-07
Iter: 1920 loss: 6.65441405e-07
Iter: 1921 loss: 6.65205789e-07
Iter: 1922 loss: 6.65086304e-07
Iter: 1923 loss: 6.64975516e-07
Iter: 1924 loss: 6.64711251e-07
Iter: 1925 loss: 6.64197159e-07
Iter: 1926 loss: 6.76713398e-07
Iter: 1927 loss: 6.64203526e-07
Iter: 1928 loss: 6.63800165e-07
Iter: 1929 loss: 6.67926e-07
Iter: 1930 loss: 6.63803405e-07
Iter: 1931 loss: 6.63416358e-07
Iter: 1932 loss: 6.64204833e-07
Iter: 1933 loss: 6.63251399e-07
Iter: 1934 loss: 6.62877369e-07
Iter: 1935 loss: 6.63499918e-07
Iter: 1936 loss: 6.62734124e-07
Iter: 1937 loss: 6.62280456e-07
Iter: 1938 loss: 6.63816593e-07
Iter: 1939 loss: 6.62153298e-07
Iter: 1940 loss: 6.61762556e-07
Iter: 1941 loss: 6.62376294e-07
Iter: 1942 loss: 6.61597937e-07
Iter: 1943 loss: 6.61267222e-07
Iter: 1944 loss: 6.65465905e-07
Iter: 1945 loss: 6.61256e-07
Iter: 1946 loss: 6.60991361e-07
Iter: 1947 loss: 6.60885178e-07
Iter: 1948 loss: 6.60728688e-07
Iter: 1949 loss: 6.60354829e-07
Iter: 1950 loss: 6.61852937e-07
Iter: 1951 loss: 6.60252e-07
Iter: 1952 loss: 6.59990803e-07
Iter: 1953 loss: 6.63954324e-07
Iter: 1954 loss: 6.59984607e-07
Iter: 1955 loss: 6.59828459e-07
Iter: 1956 loss: 6.59848183e-07
Iter: 1957 loss: 6.59692e-07
Iter: 1958 loss: 6.59361831e-07
Iter: 1959 loss: 6.60058106e-07
Iter: 1960 loss: 6.59220063e-07
Iter: 1961 loss: 6.59004741e-07
Iter: 1962 loss: 6.58736155e-07
Iter: 1963 loss: 6.58688e-07
Iter: 1964 loss: 6.58307727e-07
Iter: 1965 loss: 6.58488489e-07
Iter: 1966 loss: 6.58014528e-07
Iter: 1967 loss: 6.57581722e-07
Iter: 1968 loss: 6.59608872e-07
Iter: 1969 loss: 6.57508053e-07
Iter: 1970 loss: 6.57063481e-07
Iter: 1971 loss: 6.6039388e-07
Iter: 1972 loss: 6.5701704e-07
Iter: 1973 loss: 6.56788245e-07
Iter: 1974 loss: 6.56982365e-07
Iter: 1975 loss: 6.56636757e-07
Iter: 1976 loss: 6.56229929e-07
Iter: 1977 loss: 6.56291661e-07
Iter: 1978 loss: 6.55928261e-07
Iter: 1979 loss: 6.55529675e-07
Iter: 1980 loss: 6.59278328e-07
Iter: 1981 loss: 6.55484314e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4
+ date
Wed Oct 21 14:28:00 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2/300_300_300_1 --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444be4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444be4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444ce3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444c24bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444c2dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444bac400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444b66e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444baf840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444b3e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a51d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a7f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a21f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a0d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444af5e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444a44d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444ac5ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444abf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f04214b9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042148eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0444abf6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042155a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042155ad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4b7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f042155a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4b2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4b20d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc5248c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc524840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4fb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4fbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc4799d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc3c38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc3c3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc3c32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc3a39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f03fc428620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.11113784
test_loss: 0.103638336
train_loss: 0.07230967
test_loss: 0.087605536
train_loss: 0.055875108
test_loss: 0.05916124
train_loss: 0.033990867
test_loss: 0.045454264
train_loss: 0.034245312
test_loss: 0.037927687
train_loss: 0.028211817
test_loss: 0.033453804
train_loss: 0.020020653
test_loss: 0.029648867
train_loss: 0.021903735
test_loss: 0.024972934
train_loss: 0.015644966
test_loss: 0.023035327
train_loss: 0.015281787
test_loss: 0.023028275
train_loss: 0.014986439
test_loss: 0.020076837
train_loss: 0.017617837
test_loss: 0.01905676
train_loss: 0.013957813
test_loss: 0.019608047
train_loss: 0.012739767
test_loss: 0.019935224
train_loss: 0.012277609
test_loss: 0.018281301
train_loss: 0.012426719
test_loss: 0.017655222
train_loss: 0.012218218
test_loss: 0.018900506
train_loss: 0.010872927
test_loss: 0.017211232
train_loss: 0.010666766
test_loss: 0.016713234
train_loss: 0.010587971
test_loss: 0.015734408
train_loss: 0.01073887
test_loss: 0.01660362
train_loss: 0.011589283
test_loss: 0.016152646
train_loss: 0.010863395
test_loss: 0.016274631
train_loss: 0.009089401
test_loss: 0.014487277
train_loss: 0.009151677
test_loss: 0.0149034485
train_loss: 0.013936858
test_loss: 0.0153338965
train_loss: 0.009662943
test_loss: 0.014818741
train_loss: 0.009562081
test_loss: 0.0147844795
train_loss: 0.009524833
test_loss: 0.014340233
train_loss: 0.01004879
test_loss: 0.014767204
train_loss: 0.009057126
test_loss: 0.013984456
train_loss: 0.009295777
test_loss: 0.01467546
train_loss: 0.009304381
test_loss: 0.014929706
train_loss: 0.009730693
test_loss: 0.015640752
train_loss: 0.010277199
test_loss: 0.01424824
train_loss: 0.009352272
test_loss: 0.014177121
train_loss: 0.010442993
test_loss: 0.015318344
train_loss: 0.010008495
test_loss: 0.014251357
train_loss: 0.010299167
test_loss: 0.013897427
train_loss: 0.009029772
test_loss: 0.013301242
train_loss: 0.009336826
test_loss: 0.0140931355
train_loss: 0.009710772
test_loss: 0.01466286
train_loss: 0.0092507955
test_loss: 0.014096435
train_loss: 0.007893528
test_loss: 0.012439001
train_loss: 0.008061612
test_loss: 0.013491639
train_loss: 0.007933897
test_loss: 0.013669418
train_loss: 0.009876013
test_loss: 0.013231433
train_loss: 0.0073834513
test_loss: 0.013647941
train_loss: 0.010490731
test_loss: 0.014002483
train_loss: 0.009496098
test_loss: 0.0128258085
train_loss: 0.008245522
test_loss: 0.013547526
train_loss: 0.010042472
test_loss: 0.013929698
train_loss: 0.009106504
test_loss: 0.0126455715
train_loss: 0.008342129
test_loss: 0.01315236
train_loss: 0.008964603
test_loss: 0.01335498
train_loss: 0.0090192
test_loss: 0.014540104
train_loss: 0.009047487
test_loss: 0.01261593
train_loss: 0.0092380345
test_loss: 0.012477213
train_loss: 0.008537151
test_loss: 0.014212523
train_loss: 0.009581199
test_loss: 0.013242235
train_loss: 0.00801196
test_loss: 0.013008784
train_loss: 0.0077539035
test_loss: 0.012945708
train_loss: 0.008448419
test_loss: 0.013843241
train_loss: 0.008826428
test_loss: 0.01312128
train_loss: 0.0080624735
test_loss: 0.013308975
train_loss: 0.009088159
test_loss: 0.013018409
train_loss: 0.0081472285
test_loss: 0.01235486
train_loss: 0.008678279
test_loss: 0.013541954
train_loss: 0.010880858
test_loss: 0.014619007
train_loss: 0.011449229
test_loss: 0.01612206
train_loss: 0.008346899
test_loss: 0.01456535
train_loss: 0.008398408
test_loss: 0.013056037
train_loss: 0.008888191
test_loss: 0.012920258
train_loss: 0.008508631
test_loss: 0.01236491
train_loss: 0.008895217
test_loss: 0.012993458
train_loss: 0.00797858
test_loss: 0.012204603
train_loss: 0.007493509
test_loss: 0.012038458
train_loss: 0.0074542146
test_loss: 0.013387355
train_loss: 0.008191848
test_loss: 0.014124221
train_loss: 0.008518616
test_loss: 0.0128146885
train_loss: 0.008041704
test_loss: 0.012378849
train_loss: 0.00762422
test_loss: 0.012253884
train_loss: 0.0087187905
test_loss: 0.011819782
train_loss: 0.007866611
test_loss: 0.012763295
train_loss: 0.0071104295
test_loss: 0.01186149
train_loss: 0.007460857
test_loss: 0.0129096005
train_loss: 0.0074849697
test_loss: 0.011795393
train_loss: 0.009183018
test_loss: 0.014240481
train_loss: 0.008318318
test_loss: 0.012407776
train_loss: 0.008697419
test_loss: 0.013812985
train_loss: 0.0077437866
test_loss: 0.012437413
train_loss: 0.00781022
test_loss: 0.012654387
train_loss: 0.0080870055
test_loss: 0.012843042
train_loss: 0.008933743
test_loss: 0.012331719
train_loss: 0.008856814
test_loss: 0.012044261
train_loss: 0.009133594
test_loss: 0.013156319
train_loss: 0.0080766305
test_loss: 0.011900438
train_loss: 0.0075666145
test_loss: 0.011268117
train_loss: 0.008146757
test_loss: 0.01240407
train_loss: 0.007066901
test_loss: 0.012148964
train_loss: 0.0090720495
test_loss: 0.012014954
train_loss: 0.007706686
test_loss: 0.011703523
train_loss: 0.009041301
test_loss: 0.01212128
train_loss: 0.007850224
test_loss: 0.012227464
train_loss: 0.007044385
test_loss: 0.0116727585
train_loss: 0.008479083
test_loss: 0.012852064
train_loss: 0.009269818
test_loss: 0.012765535
train_loss: 0.0076407576
test_loss: 0.013560963
train_loss: 0.006742816
test_loss: 0.011819012
train_loss: 0.007524715
test_loss: 0.012193843
train_loss: 0.0070588086
test_loss: 0.011780601
train_loss: 0.0073194536
test_loss: 0.01211751
train_loss: 0.008232205
test_loss: 0.013506749
train_loss: 0.008030159
test_loss: 0.012693264
train_loss: 0.0069533503
test_loss: 0.011953475
train_loss: 0.007604783
test_loss: 0.011696368
train_loss: 0.007187416
test_loss: 0.0124677615
train_loss: 0.0067012785
test_loss: 0.011778693
train_loss: 0.008835766
test_loss: 0.013045997
train_loss: 0.008402674
test_loss: 0.0116656255
train_loss: 0.0084988475
test_loss: 0.011934688
train_loss: 0.0074336864
test_loss: 0.011854712
train_loss: 0.0071640555
test_loss: 0.011861397
train_loss: 0.0075532724
test_loss: 0.01235826
train_loss: 0.0074509387
test_loss: 0.011981814
train_loss: 0.007373654
test_loss: 0.0124358935
train_loss: 0.0072272494
test_loss: 0.011362778
train_loss: 0.006940626
test_loss: 0.012087195
train_loss: 0.0071149124
test_loss: 0.011599782
train_loss: 0.006613852
test_loss: 0.011341634
train_loss: 0.007773948
test_loss: 0.012556628
train_loss: 0.009011768
test_loss: 0.011846264
train_loss: 0.008197102
test_loss: 0.012516154
train_loss: 0.007901074
test_loss: 0.013263455
train_loss: 0.007046383
test_loss: 0.011935712
train_loss: 0.0076784156
test_loss: 0.0121815745
train_loss: 0.0065208673
test_loss: 0.011805964
train_loss: 0.0071834554
test_loss: 0.013186012
train_loss: 0.006890911
test_loss: 0.011918389
train_loss: 0.007677941
test_loss: 0.012234249
train_loss: 0.008280936
test_loss: 0.012160951
train_loss: 0.008526599
test_loss: 0.013016127
train_loss: 0.0072933394
test_loss: 0.0114831505
train_loss: 0.0079726195
test_loss: 0.0125616705
train_loss: 0.0074658203
test_loss: 0.011904309
train_loss: 0.0074294843
test_loss: 0.0121396715
train_loss: 0.008140609
test_loss: 0.012938964
train_loss: 0.0077526122
test_loss: 0.012738179
train_loss: 0.007026072
test_loss: 0.012331123
train_loss: 0.009655562
test_loss: 0.012765007
train_loss: 0.008899383
test_loss: 0.0121462075
train_loss: 0.00806083
test_loss: 0.011592795
train_loss: 0.007233194
test_loss: 0.011746033
train_loss: 0.0064441795
test_loss: 0.0116754435
train_loss: 0.006375052
test_loss: 0.011469252
train_loss: 0.0070438203
test_loss: 0.0122223245
train_loss: 0.008041285
test_loss: 0.012782239
train_loss: 0.007294457
test_loss: 0.012997358
train_loss: 0.0068082893
test_loss: 0.011109294
train_loss: 0.007060161
test_loss: 0.011824531
train_loss: 0.006106961
test_loss: 0.0113886045
train_loss: 0.007118917
test_loss: 0.011641891
train_loss: 0.0077253487
test_loss: 0.012314122
train_loss: 0.00838735
test_loss: 0.012854867
train_loss: 0.007239521
test_loss: 0.012864782
train_loss: 0.0068179015
test_loss: 0.011481206
train_loss: 0.0071054976
test_loss: 0.012303413
train_loss: 0.0065974845
test_loss: 0.011687044
train_loss: 0.0067062546
test_loss: 0.011927767
train_loss: 0.00696594
test_loss: 0.012097321
train_loss: 0.007019705
test_loss: 0.011975246
train_loss: 0.007517174
test_loss: 0.012091786
train_loss: 0.00836535
test_loss: 0.013007318
train_loss: 0.008148783
test_loss: 0.011619035
train_loss: 0.0067519927
test_loss: 0.011676259
train_loss: 0.006219092
test_loss: 0.011481558
train_loss: 0.0070779724
test_loss: 0.011646644
train_loss: 0.0065612243
test_loss: 0.011209171
train_loss: 0.007672151
test_loss: 0.011952798
train_loss: 0.007078927
test_loss: 0.01155157
train_loss: 0.0067306985
test_loss: 0.011542093
train_loss: 0.006769175
test_loss: 0.011929148
train_loss: 0.0069565442
test_loss: 0.011985602
train_loss: 0.00705646
test_loss: 0.011467213
train_loss: 0.008232845
test_loss: 0.012087136
train_loss: 0.007146701
test_loss: 0.0121255
train_loss: 0.0069865426
test_loss: 0.011684236
train_loss: 0.0068764193
test_loss: 0.013741434
train_loss: 0.0068520172
test_loss: 0.011761556
train_loss: 0.008714597
test_loss: 0.012276991
train_loss: 0.006609738
test_loss: 0.0118170595
train_loss: 0.008550934
test_loss: 0.012206402
train_loss: 0.007559036
test_loss: 0.012435419
train_loss: 0.0076397387
test_loss: 0.012864642
train_loss: 0.007860791
test_loss: 0.012003658
train_loss: 0.00734424
test_loss: 0.01196982
train_loss: 0.0069618393
test_loss: 0.012854531
train_loss: 0.007301023
test_loss: 0.012293067
train_loss: 0.0087563945
test_loss: 0.011867478
train_loss: 0.00730275
test_loss: 0.01167395
train_loss: 0.007221381
test_loss: 0.012015861
train_loss: 0.0071989372
test_loss: 0.011814751
train_loss: 0.0077687628
test_loss: 0.012129623
train_loss: 0.0066811317
test_loss: 0.012031204
train_loss: 0.007431814
test_loss: 0.011408521
train_loss: 0.0068314997
test_loss: 0.012304478
train_loss: 0.007002011
test_loss: 0.011752457
train_loss: 0.006850267
test_loss: 0.011361064
train_loss: 0.0068231244
test_loss: 0.0125976
train_loss: 0.006596503
test_loss: 0.012390772
train_loss: 0.007642767
test_loss: 0.012587707
train_loss: 0.007134497
test_loss: 0.012755715
train_loss: 0.007516031
test_loss: 0.012137574
train_loss: 0.00797217
test_loss: 0.012304358
train_loss: 0.006942114
test_loss: 0.012299455
train_loss: 0.007469321
test_loss: 0.011472173
train_loss: 0.0063615516
test_loss: 0.011924215
train_loss: 0.0073097367
test_loss: 0.012601351
train_loss: 0.0072373506
test_loss: 0.012192585
train_loss: 0.0076510217
test_loss: 0.012665928
train_loss: 0.006827084
test_loss: 0.011610232
train_loss: 0.0076911743
test_loss: 0.012606866
train_loss: 0.007513673
test_loss: 0.012105102
train_loss: 0.0068700146
test_loss: 0.012780606
train_loss: 0.0076996344
test_loss: 0.011855867
train_loss: 0.0074824356
test_loss: 0.0118277
train_loss: 0.0071917027
test_loss: 0.011880176
train_loss: 0.0061696135
test_loss: 0.011831065
train_loss: 0.0070052035
test_loss: 0.011769097
train_loss: 0.0068733282
test_loss: 0.012296228
train_loss: 0.007008271
test_loss: 0.012144437
train_loss: 0.007906732
test_loss: 0.012725474
train_loss: 0.007314492
test_loss: 0.013285478
train_loss: 0.0067521683
test_loss: 0.012790947
train_loss: 0.0071068164
test_loss: 0.011840837
train_loss: 0.0074063176
test_loss: 0.012228715
train_loss: 0.007338493
test_loss: 0.012587089
train_loss: 0.006325949
test_loss: 0.011655393
train_loss: 0.0062143
test_loss: 0.011601353
train_loss: 0.007291948
test_loss: 0.012440736
train_loss: 0.007045515
test_loss: 0.012400051
train_loss: 0.007230856
test_loss: 0.0119189015
train_loss: 0.007688265
test_loss: 0.012086204
train_loss: 0.007664659
test_loss: 0.012397972
train_loss: 0.007195555
test_loss: 0.012396136
train_loss: 0.0071185115
test_loss: 0.011870886
train_loss: 0.0069290507
test_loss: 0.011494341
train_loss: 0.006984034
test_loss: 0.012138963
train_loss: 0.007571757
test_loss: 0.011978834
train_loss: 0.008223339
test_loss: 0.01276746
train_loss: 0.0067467615
test_loss: 0.011639198
train_loss: 0.0059401137
test_loss: 0.011463799
train_loss: 0.006958916
test_loss: 0.011743519
train_loss: 0.0074111284
test_loss: 0.011370205
train_loss: 0.006646172
test_loss: 0.0124907065
train_loss: 0.006070793
test_loss: 0.011395711
train_loss: 0.007172153
test_loss: 0.012332265
train_loss: 0.0069429614
test_loss: 0.012282536
train_loss: 0.007115784
test_loss: 0.011777147
train_loss: 0.006795982
test_loss: 0.011992016
train_loss: 0.006949027
test_loss: 0.0119715175
train_loss: 0.0066857142
test_loss: 0.01209059
train_loss: 0.008252418
test_loss: 0.012079333
train_loss: 0.006371157
test_loss: 0.011716984
train_loss: 0.007199418
test_loss: 0.012455573
train_loss: 0.0067637144
test_loss: 0.011918201
train_loss: 0.0070340396
test_loss: 0.012383398
train_loss: 0.0069195013
test_loss: 0.012243793
train_loss: 0.006372948
test_loss: 0.01179681
train_loss: 0.006867039
test_loss: 0.011853752
train_loss: 0.00711067
test_loss: 0.011725993
train_loss: 0.0075156055
test_loss: 0.012460393
train_loss: 0.007588121
test_loss: 0.012173159
train_loss: 0.007049574
test_loss: 0.0118193645
train_loss: 0.0073911976
test_loss: 0.011832864
train_loss: 0.0060540987
test_loss: 0.011197827
train_loss: 0.0073978896
test_loss: 0.0124430265
train_loss: 0.006536959
test_loss: 0.011709809
train_loss: 0.0073302025
test_loss: 0.012802052
train_loss: 0.006328063
test_loss: 0.0118053835
train_loss: 0.007492874
test_loss: 0.011667315
train_loss: 0.0067237145
test_loss: 0.011730334
train_loss: 0.008143945
test_loss: 0.011963925
train_loss: 0.007487258
test_loss: 0.012125534
train_loss: 0.007222213
test_loss: 0.012519864
train_loss: 0.0066368133
test_loss: 0.011777181
train_loss: 0.006521892
test_loss: 0.012115439
train_loss: 0.0055979574
test_loss: 0.011690299
train_loss: 0.0058798296
test_loss: 0.011458555
train_loss: 0.006503231
test_loss: 0.012103755
train_loss: 0.006456512
test_loss: 0.012304951
train_loss: 0.0077591324
test_loss: 0.01253742
train_loss: 0.0067826915
test_loss: 0.012485942
train_loss: 0.007079927
test_loss: 0.012065922
train_loss: 0.0063555455
test_loss: 0.011749436
train_loss: 0.0066778436
test_loss: 0.011534521
train_loss: 0.005742103
test_loss: 0.0115233585
train_loss: 0.0072059194
test_loss: 0.011863872
train_loss: 0.006213668
test_loss: 0.011434635
train_loss: 0.006002443
test_loss: 0.012008475
train_loss: 0.0074804644
test_loss: 0.012476137
train_loss: 0.007050577
test_loss: 0.012064957
train_loss: 0.007406099
test_loss: 0.012713896
train_loss: 0.007223595
test_loss: 0.012339581
train_loss: 0.006591539
test_loss: 0.011688868
train_loss: 0.0066388855
test_loss: 0.012016128
train_loss: 0.006105776
test_loss: 0.011456051
train_loss: 0.0067384667
test_loss: 0.011454032
train_loss: 0.0061117094
test_loss: 0.012066204
train_loss: 0.006230657
test_loss: 0.012063027
train_loss: 0.0069474326
test_loss: 0.011923617
train_loss: 0.0062675467
test_loss: 0.011394215
train_loss: 0.007877992
test_loss: 0.01263831
train_loss: 0.0058398843
test_loss: 0.011556663
train_loss: 0.0065064644
test_loss: 0.011409453
train_loss: 0.0065907105
test_loss: 0.012211656
train_loss: 0.0061308616
test_loss: 0.01157009
train_loss: 0.0062906872
test_loss: 0.012100474
train_loss: 0.007133349
test_loss: 0.012065432
train_loss: 0.00646172
test_loss: 0.012154466
train_loss: 0.0075322716
test_loss: 0.011974037
train_loss: 0.0062289466
test_loss: 0.01165154
train_loss: 0.007999493
test_loss: 0.0123769455
train_loss: 0.006786533
test_loss: 0.011639104
train_loss: 0.006264723
test_loss: 0.011898067
train_loss: 0.0060292413
test_loss: 0.011685984
train_loss: 0.00789725
test_loss: 0.011749778
train_loss: 0.006064072
test_loss: 0.012237842
train_loss: 0.0070782905
test_loss: 0.012009722
train_loss: 0.0067697624
test_loss: 0.012083216
train_loss: 0.007086354
test_loss: 0.012834379
train_loss: 0.006460142
test_loss: 0.012208375
train_loss: 0.0061704027
test_loss: 0.011254534
train_loss: 0.0075112325
test_loss: 0.013358099
train_loss: 0.0065769185
test_loss: 0.011833102
train_loss: 0.006424084
test_loss: 0.012152093
train_loss: 0.00613736
test_loss: 0.011512001
train_loss: 0.008544241
test_loss: 0.011488005
train_loss: 0.0066527147
test_loss: 0.012931211
train_loss: 0.005971137
test_loss: 0.011758545
train_loss: 0.006749497
test_loss: 0.013029736
train_loss: 0.0062631876
test_loss: 0.011540729
train_loss: 0.0070996745
test_loss: 0.011909948
train_loss: 0.005606084
test_loss: 0.011669644
train_loss: 0.0060518077
test_loss: 0.01170999
train_loss: 0.0071170647
test_loss: 0.011824885
train_loss: 0.006969415
test_loss: /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
0.012686802
train_loss: 0.006722316
test_loss: 0.011766705
train_loss: 0.008104581
test_loss: 0.012341494
train_loss: 0.007256885
test_loss: 0.011698318
train_loss: 0.0066968044
test_loss: 0.012053144
train_loss: 0.0068671945
test_loss: 0.012180887
train_loss: 0.0063430974
test_loss: 0.012465587
train_loss: 0.007104963
test_loss: 0.01231241
train_loss: 0.007155138
test_loss: 0.012715694
train_loss: 0.0066502234
test_loss: 0.012167729
train_loss: 0.005733418
test_loss: 0.012066694
train_loss: 0.0064328033
test_loss: 0.012180459
train_loss: 0.006474511
test_loss: 0.012827962
train_loss: 0.0073289084
test_loss: 0.012168319
train_loss: 0.006137084
test_loss: 0.012145183
train_loss: 0.0073936777
test_loss: 0.012054421
train_loss: 0.0063869487
test_loss: 0.011819007
train_loss: 0.0059592063
test_loss: 0.011636849
train_loss: 0.0077902447
test_loss: 0.012176597
train_loss: 0.006313584
test_loss: 0.011611727
train_loss: 0.006589255
test_loss: 0.011995992
train_loss: 0.00558519
test_loss: 0.011261238
train_loss: 0.0070633627
test_loss: 0.012207257
train_loss: 0.006429538
test_loss: 0.011790194
train_loss: 0.0075265593
test_loss: 0.012242163
train_loss: 0.005647189
test_loss: 0.011713784
train_loss: 0.006918406
test_loss: 0.012332584
train_loss: 0.0062617
test_loss: 0.012413187
train_loss: 0.006871048
test_loss: 0.012367814
train_loss: 0.006228323
test_loss: 0.0119432
train_loss: 0.0055848076
test_loss: 0.012128112
train_loss: 0.0064797327
test_loss: 0.012653506
train_loss: 0.006722371
test_loss: 0.012590708
train_loss: 0.0065328213
test_loss: 0.012040938
train_loss: 0.0061625517
test_loss: 0.011850463
train_loss: 0.0068777245
test_loss: 0.012236824
train_loss: 0.006223116
test_loss: 0.012082402
train_loss: 0.0060113077
test_loss: 0.012730959
train_loss: 0.0075473906
test_loss: 0.012501798
train_loss: 0.0082870545
test_loss: 0.014334853
train_loss: 0.007129807
test_loss: 0.012789
train_loss: 0.006152208
test_loss: 0.012347659
train_loss: 0.007758045
test_loss: 0.012670262
train_loss: 0.007717414
test_loss: 0.012825706
train_loss: 0.006256313
test_loss: 0.01283226
train_loss: 0.007969083
test_loss: 0.01316736
train_loss: 0.0059919134
test_loss: 0.012004356
train_loss: 0.0058204997
test_loss: 0.011993728
train_loss: 0.0067571662
test_loss: 0.011538887
train_loss: 0.006419266
test_loss: 0.012577695
train_loss: 0.005481103
test_loss: 0.011763854
train_loss: 0.0067166286
test_loss: 0.012128471
train_loss: 0.006948938
test_loss: 0.012362763
train_loss: 0.00600762
test_loss: 0.0116375545
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45d0a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45de7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45de7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45d26e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45d4c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45d4c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45c918c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45c46840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45c392f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45c39488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45baa510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45bb0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45b5ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1b45b93bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7ea1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7e8d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7e8dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7e79598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7e346a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7e34c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7dbf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1af7dbf1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad04e4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad04ff950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad04ff1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad04b8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad04738c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad04a2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad042f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad044f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad03f46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad03bd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad03b7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad03d7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad03d50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ad0324620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 8.68095813e-05
Iter: 2 loss: 0.000832954596
Iter: 3 loss: 6.76405689e-05
Iter: 4 loss: 6.42755331e-05
Iter: 5 loss: 6.0441911e-05
Iter: 6 loss: 5.99476953e-05
Iter: 7 loss: 5.55966835e-05
Iter: 8 loss: 6.18189661e-05
Iter: 9 loss: 5.34945793e-05
Iter: 10 loss: 5.02388357e-05
Iter: 11 loss: 4.34169342e-05
Iter: 12 loss: 0.000160143754
Iter: 13 loss: 4.32443412e-05
Iter: 14 loss: 3.83055813e-05
Iter: 15 loss: 8.30485806e-05
Iter: 16 loss: 3.80867423e-05
Iter: 17 loss: 3.57912104e-05
Iter: 18 loss: 5.36156331e-05
Iter: 19 loss: 3.56277851e-05
Iter: 20 loss: 3.34763754e-05
Iter: 21 loss: 3.40738407e-05
Iter: 22 loss: 3.19216451e-05
Iter: 23 loss: 2.95159662e-05
Iter: 24 loss: 3.06021393e-05
Iter: 25 loss: 2.78831485e-05
Iter: 26 loss: 2.58139462e-05
Iter: 27 loss: 3.50383416e-05
Iter: 28 loss: 2.5405323e-05
Iter: 29 loss: 2.38159846e-05
Iter: 30 loss: 2.80384884e-05
Iter: 31 loss: 2.32817947e-05
Iter: 32 loss: 2.19405192e-05
Iter: 33 loss: 2.44402672e-05
Iter: 34 loss: 2.13698222e-05
Iter: 35 loss: 1.98098114e-05
Iter: 36 loss: 2.80555087e-05
Iter: 37 loss: 1.95696593e-05
Iter: 38 loss: 1.85897188e-05
Iter: 39 loss: 1.97414101e-05
Iter: 40 loss: 1.80719035e-05
Iter: 41 loss: 1.87965888e-05
Iter: 42 loss: 1.75821533e-05
Iter: 43 loss: 1.73116568e-05
Iter: 44 loss: 1.66276313e-05
Iter: 45 loss: 2.2865439e-05
Iter: 46 loss: 1.65253114e-05
Iter: 47 loss: 1.58530147e-05
Iter: 48 loss: 2.05255637e-05
Iter: 49 loss: 1.57909981e-05
Iter: 50 loss: 1.53897672e-05
Iter: 51 loss: 1.94979129e-05
Iter: 52 loss: 1.53783567e-05
Iter: 53 loss: 1.50014585e-05
Iter: 54 loss: 1.55027446e-05
Iter: 55 loss: 1.48111421e-05
Iter: 56 loss: 1.43059224e-05
Iter: 57 loss: 1.42140489e-05
Iter: 58 loss: 1.38733849e-05
Iter: 59 loss: 1.33736758e-05
Iter: 60 loss: 1.51043387e-05
Iter: 61 loss: 1.32420864e-05
Iter: 62 loss: 1.28143947e-05
Iter: 63 loss: 1.31515944e-05
Iter: 64 loss: 1.25563092e-05
Iter: 65 loss: 1.20224531e-05
Iter: 66 loss: 1.28763013e-05
Iter: 67 loss: 1.17756308e-05
Iter: 68 loss: 1.13618607e-05
Iter: 69 loss: 1.60096024e-05
Iter: 70 loss: 1.13535662e-05
Iter: 71 loss: 1.10173851e-05
Iter: 72 loss: 1.11001427e-05
Iter: 73 loss: 1.07722217e-05
Iter: 74 loss: 1.0654e-05
Iter: 75 loss: 1.0594993e-05
Iter: 76 loss: 1.03844268e-05
Iter: 77 loss: 1.09732691e-05
Iter: 78 loss: 1.03170923e-05
Iter: 79 loss: 1.02182321e-05
Iter: 80 loss: 9.92875e-06
Iter: 81 loss: 1.10332767e-05
Iter: 82 loss: 9.80667937e-06
Iter: 83 loss: 9.47010813e-06
Iter: 84 loss: 1.44492442e-05
Iter: 85 loss: 9.46962791e-06
Iter: 86 loss: 9.25309087e-06
Iter: 87 loss: 1.03049742e-05
Iter: 88 loss: 9.21467836e-06
Iter: 89 loss: 9.05929301e-06
Iter: 90 loss: 1.08635486e-05
Iter: 91 loss: 9.05728848e-06
Iter: 92 loss: 8.92304342e-06
Iter: 93 loss: 8.92046774e-06
Iter: 94 loss: 8.8154311e-06
Iter: 95 loss: 8.63246078e-06
Iter: 96 loss: 8.61532499e-06
Iter: 97 loss: 8.48050786e-06
Iter: 98 loss: 8.24738436e-06
Iter: 99 loss: 9.91236902e-06
Iter: 100 loss: 8.22632865e-06
Iter: 101 loss: 8.06595926e-06
Iter: 102 loss: 8.35815354e-06
Iter: 103 loss: 7.9968413e-06
Iter: 104 loss: 7.81665e-06
Iter: 105 loss: 7.99121426e-06
Iter: 106 loss: 7.71484e-06
Iter: 107 loss: 7.57534144e-06
Iter: 108 loss: 8.97419523e-06
Iter: 109 loss: 7.57113457e-06
Iter: 110 loss: 7.46090109e-06
Iter: 111 loss: 7.67330857e-06
Iter: 112 loss: 7.41479153e-06
Iter: 113 loss: 7.23304629e-06
Iter: 114 loss: 8.48551463e-06
Iter: 115 loss: 7.21544438e-06
Iter: 116 loss: 7.1681643e-06
Iter: 117 loss: 7.06773289e-06
Iter: 118 loss: 8.67104427e-06
Iter: 119 loss: 7.06428227e-06
Iter: 120 loss: 6.96294137e-06
Iter: 121 loss: 7.08166135e-06
Iter: 122 loss: 6.90911838e-06
Iter: 123 loss: 6.85880696e-06
Iter: 124 loss: 6.84848237e-06
Iter: 125 loss: 6.80281619e-06
Iter: 126 loss: 6.81971869e-06
Iter: 127 loss: 6.7708329e-06
Iter: 128 loss: 6.69311794e-06
Iter: 129 loss: 6.78348852e-06
Iter: 130 loss: 6.65162e-06
Iter: 131 loss: 6.58347562e-06
Iter: 132 loss: 6.67652421e-06
Iter: 133 loss: 6.54974747e-06
Iter: 134 loss: 6.46988656e-06
Iter: 135 loss: 6.38576421e-06
Iter: 136 loss: 6.37132325e-06
Iter: 137 loss: 6.27858572e-06
Iter: 138 loss: 7.37333767e-06
Iter: 139 loss: 6.27721738e-06
Iter: 140 loss: 6.19469483e-06
Iter: 141 loss: 6.15514909e-06
Iter: 142 loss: 6.11467658e-06
Iter: 143 loss: 6.02820455e-06
Iter: 144 loss: 6.02818909e-06
Iter: 145 loss: 5.97451344e-06
Iter: 146 loss: 6.39563495e-06
Iter: 147 loss: 5.97068629e-06
Iter: 148 loss: 5.91225125e-06
Iter: 149 loss: 6.38763368e-06
Iter: 150 loss: 5.90807485e-06
Iter: 151 loss: 5.88981038e-06
Iter: 152 loss: 5.84642248e-06
Iter: 153 loss: 6.34499e-06
Iter: 154 loss: 5.84234976e-06
Iter: 155 loss: 5.79271682e-06
Iter: 156 loss: 5.76483762e-06
Iter: 157 loss: 5.74282512e-06
Iter: 158 loss: 5.72822046e-06
Iter: 159 loss: 5.70223392e-06
Iter: 160 loss: 5.67459e-06
Iter: 161 loss: 5.71611963e-06
Iter: 162 loss: 5.66169956e-06
Iter: 163 loss: 5.62864898e-06
Iter: 164 loss: 5.64763286e-06
Iter: 165 loss: 5.60767603e-06
Iter: 166 loss: 5.56067153e-06
Iter: 167 loss: 5.57536487e-06
Iter: 168 loss: 5.52683e-06
Iter: 169 loss: 5.47656373e-06
Iter: 170 loss: 5.69184431e-06
Iter: 171 loss: 5.46614319e-06
Iter: 172 loss: 5.42202224e-06
Iter: 173 loss: 5.38237418e-06
Iter: 174 loss: 5.37117785e-06
Iter: 175 loss: 5.31126898e-06
Iter: 176 loss: 5.94110497e-06
Iter: 177 loss: 5.30959278e-06
Iter: 178 loss: 5.25616542e-06
Iter: 179 loss: 5.22591472e-06
Iter: 180 loss: 5.20288131e-06
Iter: 181 loss: 5.50027153e-06
Iter: 182 loss: 5.19280457e-06
Iter: 183 loss: 5.18650904e-06
Iter: 184 loss: 5.16611863e-06
Iter: 185 loss: 5.18675824e-06
Iter: 186 loss: 5.14978274e-06
Iter: 187 loss: 5.10766495e-06
Iter: 188 loss: 5.08691664e-06
Iter: 189 loss: 5.06641663e-06
Iter: 190 loss: 5.01908698e-06
Iter: 191 loss: 5.39387656e-06
Iter: 192 loss: 5.01601426e-06
Iter: 193 loss: 4.98713052e-06
Iter: 194 loss: 5.24409415e-06
Iter: 195 loss: 4.98581039e-06
Iter: 196 loss: 4.9551445e-06
Iter: 197 loss: 5.01051818e-06
Iter: 198 loss: 4.94161941e-06
Iter: 199 loss: 4.91827223e-06
Iter: 200 loss: 4.99049565e-06
Iter: 201 loss: 4.91127776e-06
Iter: 202 loss: 4.88490605e-06
Iter: 203 loss: 4.85145347e-06
Iter: 204 loss: 4.84888642e-06
Iter: 205 loss: 4.80035305e-06
Iter: 206 loss: 4.99468842e-06
Iter: 207 loss: 4.78940183e-06
Iter: 208 loss: 4.75327397e-06
Iter: 209 loss: 4.81510597e-06
Iter: 210 loss: 4.73733326e-06
Iter: 211 loss: 4.70378291e-06
Iter: 212 loss: 4.97071778e-06
Iter: 213 loss: 4.7013109e-06
Iter: 214 loss: 4.67754307e-06
Iter: 215 loss: 4.68404e-06
Iter: 216 loss: 4.66024585e-06
Iter: 217 loss: 4.68804228e-06
Iter: 218 loss: 4.64820596e-06
Iter: 219 loss: 4.64251116e-06
Iter: 220 loss: 4.62502112e-06
Iter: 221 loss: 4.67626705e-06
Iter: 222 loss: 4.61575564e-06
Iter: 223 loss: 4.59497232e-06
Iter: 224 loss: 4.57138685e-06
Iter: 225 loss: 4.56814178e-06
Iter: 226 loss: 4.51855158e-06
Iter: 227 loss: 4.75332945e-06
Iter: 228 loss: 4.50940342e-06
Iter: 229 loss: 4.48860101e-06
Iter: 230 loss: 4.48831042e-06
Iter: 231 loss: 4.466674e-06
Iter: 232 loss: 4.53913799e-06
Iter: 233 loss: 4.46071408e-06
Iter: 234 loss: 4.44063e-06
Iter: 235 loss: 4.45325e-06
Iter: 236 loss: 4.42754754e-06
Iter: 237 loss: 4.40935491e-06
Iter: 238 loss: 4.50898597e-06
Iter: 239 loss: 4.40657686e-06
Iter: 240 loss: 4.3889404e-06
Iter: 241 loss: 4.37959e-06
Iter: 242 loss: 4.37154904e-06
Iter: 243 loss: 4.34391723e-06
Iter: 244 loss: 4.48365063e-06
Iter: 245 loss: 4.33926834e-06
Iter: 246 loss: 4.32175966e-06
Iter: 247 loss: 4.28615294e-06
Iter: 248 loss: 4.94268716e-06
Iter: 249 loss: 4.28551357e-06
Iter: 250 loss: 4.27549867e-06
Iter: 251 loss: 4.26631232e-06
Iter: 252 loss: 4.25426242e-06
Iter: 253 loss: 4.25393864e-06
Iter: 254 loss: 4.24984228e-06
Iter: 255 loss: 4.23773099e-06
Iter: 256 loss: 4.26469069e-06
Iter: 257 loss: 4.23036363e-06
Iter: 258 loss: 4.21088907e-06
Iter: 259 loss: 4.21989535e-06
Iter: 260 loss: 4.19748631e-06
Iter: 261 loss: 4.17225147e-06
Iter: 262 loss: 4.25441795e-06
Iter: 263 loss: 4.16516377e-06
Iter: 264 loss: 4.14101851e-06
Iter: 265 loss: 4.25206645e-06
Iter: 266 loss: 4.13609087e-06
Iter: 267 loss: 4.12343479e-06
Iter: 268 loss: 4.12303325e-06
Iter: 269 loss: 4.10918074e-06
Iter: 270 loss: 4.09811582e-06
Iter: 271 loss: 4.09419135e-06
Iter: 272 loss: 4.07708558e-06
Iter: 273 loss: 4.08369124e-06
Iter: 274 loss: 4.06530307e-06
Iter: 275 loss: 4.04717684e-06
Iter: 276 loss: 4.11949895e-06
Iter: 277 loss: 4.04316961e-06
Iter: 278 loss: 4.02271507e-06
Iter: 279 loss: 4.12983627e-06
Iter: 280 loss: 4.01968373e-06
Iter: 281 loss: 4.00821818e-06
Iter: 282 loss: 4.02666501e-06
Iter: 283 loss: 4.00296358e-06
Iter: 284 loss: 3.98871543e-06
Iter: 285 loss: 4.02578735e-06
Iter: 286 loss: 3.98407383e-06
Iter: 287 loss: 3.97136637e-06
Iter: 288 loss: 3.97093e-06
Iter: 289 loss: 3.96694759e-06
Iter: 290 loss: 3.95587722e-06
Iter: 291 loss: 4.02402748e-06
Iter: 292 loss: 3.952935e-06
Iter: 293 loss: 3.93784239e-06
Iter: 294 loss: 3.91446838e-06
Iter: 295 loss: 3.91400545e-06
Iter: 296 loss: 3.89377601e-06
Iter: 297 loss: 4.0160221e-06
Iter: 298 loss: 3.89126399e-06
Iter: 299 loss: 3.87094497e-06
Iter: 300 loss: 3.9571969e-06
Iter: 301 loss: 3.866734e-06
Iter: 302 loss: 3.85471e-06
Iter: 303 loss: 3.97915846e-06
Iter: 304 loss: 3.85444264e-06
Iter: 305 loss: 3.84093028e-06
Iter: 306 loss: 3.88128319e-06
Iter: 307 loss: 3.83684e-06
Iter: 308 loss: 3.82687722e-06
Iter: 309 loss: 3.82591679e-06
Iter: 310 loss: 3.81845257e-06
Iter: 311 loss: 3.80679148e-06
Iter: 312 loss: 3.8043795e-06
Iter: 313 loss: 3.79663743e-06
Iter: 314 loss: 3.77630067e-06
Iter: 315 loss: 3.8631506e-06
Iter: 316 loss: 3.77220454e-06
Iter: 317 loss: 3.7619252e-06
Iter: 318 loss: 3.89146226e-06
Iter: 319 loss: 3.76204525e-06
Iter: 320 loss: 3.75166474e-06
Iter: 321 loss: 3.73917192e-06
Iter: 322 loss: 3.73811713e-06
Iter: 323 loss: 3.76646972e-06
Iter: 324 loss: 3.73368425e-06
Iter: 325 loss: 3.73131707e-06
Iter: 326 loss: 3.72409204e-06
Iter: 327 loss: 3.74704359e-06
Iter: 328 loss: 3.72063641e-06
Iter: 329 loss: 3.71082615e-06
Iter: 330 loss: 3.69460622e-06
Iter: 331 loss: 3.69463623e-06
Iter: 332 loss: 3.67970301e-06
Iter: 333 loss: 3.88414e-06
Iter: 334 loss: 3.67951975e-06
Iter: 335 loss: 3.66999916e-06
Iter: 336 loss: 3.70070393e-06
Iter: 337 loss: 3.66739414e-06
Iter: 338 loss: 3.65659662e-06
Iter: 339 loss: 3.69453346e-06
Iter: 340 loss: 3.65356755e-06
Iter: 341 loss: 3.6418578e-06
Iter: 342 loss: 3.72448744e-06
Iter: 343 loss: 3.64078301e-06
Iter: 344 loss: 3.63582535e-06
Iter: 345 loss: 3.62777791e-06
Iter: 346 loss: 3.62773017e-06
Iter: 347 loss: 3.61544471e-06
Iter: 348 loss: 3.62940727e-06
Iter: 349 loss: 3.60888225e-06
Iter: 350 loss: 3.59753449e-06
Iter: 351 loss: 3.62382e-06
Iter: 352 loss: 3.59354908e-06
Iter: 353 loss: 3.58303237e-06
Iter: 354 loss: 3.73954549e-06
Iter: 355 loss: 3.58304715e-06
Iter: 356 loss: 3.57696626e-06
Iter: 357 loss: 3.61934735e-06
Iter: 358 loss: 3.57643967e-06
Iter: 359 loss: 3.56958503e-06
Iter: 360 loss: 3.60525382e-06
Iter: 361 loss: 3.56862301e-06
Iter: 362 loss: 3.56487817e-06
Iter: 363 loss: 3.55392876e-06
Iter: 364 loss: 3.60458534e-06
Iter: 365 loss: 3.5497817e-06
Iter: 366 loss: 3.54026497e-06
Iter: 367 loss: 3.61471257e-06
Iter: 368 loss: 3.53962241e-06
Iter: 369 loss: 3.5318883e-06
Iter: 370 loss: 3.522508e-06
Iter: 371 loss: 3.52188431e-06
Iter: 372 loss: 3.51245694e-06
Iter: 373 loss: 3.64922607e-06
Iter: 374 loss: 3.51241533e-06
Iter: 375 loss: 3.50545906e-06
Iter: 376 loss: 3.55265388e-06
Iter: 377 loss: 3.5047542e-06
Iter: 378 loss: 3.49782636e-06
Iter: 379 loss: 3.50668961e-06
Iter: 380 loss: 3.49424158e-06
Iter: 381 loss: 3.48781577e-06
Iter: 382 loss: 3.48591129e-06
Iter: 383 loss: 3.48228014e-06
Iter: 384 loss: 3.4715e-06
Iter: 385 loss: 3.48498224e-06
Iter: 386 loss: 3.46593833e-06
Iter: 387 loss: 3.45696458e-06
Iter: 388 loss: 3.46667321e-06
Iter: 389 loss: 3.45202102e-06
Iter: 390 loss: 3.44076057e-06
Iter: 391 loss: 3.52767165e-06
Iter: 392 loss: 3.43994884e-06
Iter: 393 loss: 3.44250748e-06
Iter: 394 loss: 3.43647616e-06
Iter: 395 loss: 3.43388638e-06
Iter: 396 loss: 3.42990097e-06
Iter: 397 loss: 3.42989165e-06
Iter: 398 loss: 3.42578e-06
Iter: 399 loss: 3.41657824e-06
Iter: 400 loss: 3.54171675e-06
Iter: 401 loss: 3.416043e-06
Iter: 402 loss: 3.40274096e-06
Iter: 403 loss: 3.42536669e-06
Iter: 404 loss: 3.39697658e-06
Iter: 405 loss: 3.38830205e-06
Iter: 406 loss: 3.41276336e-06
Iter: 407 loss: 3.3855722e-06
Iter: 408 loss: 3.37450751e-06
Iter: 409 loss: 3.41216401e-06
Iter: 410 loss: 3.37151869e-06
Iter: 411 loss: 3.36871062e-06
Iter: 412 loss: 3.36761877e-06
Iter: 413 loss: 3.36329e-06
Iter: 414 loss: 3.35536379e-06
Iter: 415 loss: 3.53851328e-06
Iter: 416 loss: 3.35541449e-06
Iter: 417 loss: 3.34731567e-06
Iter: 418 loss: 3.38082691e-06
Iter: 419 loss: 3.34567721e-06
Iter: 420 loss: 3.33865728e-06
Iter: 421 loss: 3.33771936e-06
Iter: 422 loss: 3.33257071e-06
Iter: 423 loss: 3.32341779e-06
Iter: 424 loss: 3.39608391e-06
Iter: 425 loss: 3.32273567e-06
Iter: 426 loss: 3.32421496e-06
Iter: 427 loss: 3.32051832e-06
Iter: 428 loss: 3.31798401e-06
Iter: 429 loss: 3.31610772e-06
Iter: 430 loss: 3.31544379e-06
Iter: 431 loss: 3.31172328e-06
Iter: 432 loss: 3.30443117e-06
Iter: 433 loss: 3.45409376e-06
Iter: 434 loss: 3.30441344e-06
Iter: 435 loss: 3.29641e-06
Iter: 436 loss: 3.33130311e-06
Iter: 437 loss: 3.29485306e-06
Iter: 438 loss: 3.28850274e-06
Iter: 439 loss: 3.27973521e-06
Iter: 440 loss: 3.27943667e-06
Iter: 441 loss: 3.26943473e-06
Iter: 442 loss: 3.38848213e-06
Iter: 443 loss: 3.26920872e-06
Iter: 444 loss: 3.26258714e-06
Iter: 445 loss: 3.26113718e-06
Iter: 446 loss: 3.25685073e-06
Iter: 447 loss: 3.24969096e-06
Iter: 448 loss: 3.33450271e-06
Iter: 449 loss: 3.24958592e-06
Iter: 450 loss: 3.24435e-06
Iter: 451 loss: 3.31429283e-06
Iter: 452 loss: 3.2443254e-06
Iter: 453 loss: 3.24044299e-06
Iter: 454 loss: 3.23248969e-06
Iter: 455 loss: 3.38867471e-06
Iter: 456 loss: 3.2324e-06
Iter: 457 loss: 3.22578649e-06
Iter: 458 loss: 3.26024292e-06
Iter: 459 loss: 3.22483038e-06
Iter: 460 loss: 3.21872471e-06
Iter: 461 loss: 3.22390542e-06
Iter: 462 loss: 3.21513176e-06
Iter: 463 loss: 3.21844755e-06
Iter: 464 loss: 3.21284756e-06
Iter: 465 loss: 3.21024208e-06
Iter: 466 loss: 3.20568597e-06
Iter: 467 loss: 3.20568279e-06
Iter: 468 loss: 3.20245e-06
Iter: 469 loss: 3.19541664e-06
Iter: 470 loss: 3.2988803e-06
Iter: 471 loss: 3.19512901e-06
Iter: 472 loss: 3.18526054e-06
Iter: 473 loss: 3.24815642e-06
Iter: 474 loss: 3.1841173e-06
Iter: 475 loss: 3.1789582e-06
Iter: 476 loss: 3.23730069e-06
Iter: 477 loss: 3.17891522e-06
Iter: 478 loss: 3.175252e-06
Iter: 479 loss: 3.17084641e-06
Iter: 480 loss: 3.17038575e-06
Iter: 481 loss: 3.16336559e-06
Iter: 482 loss: 3.19374112e-06
Iter: 483 loss: 3.16178102e-06
Iter: 484 loss: 3.15680654e-06
Iter: 485 loss: 3.17127433e-06
Iter: 486 loss: 3.15530792e-06
Iter: 487 loss: 3.15032207e-06
Iter: 488 loss: 3.20518802e-06
Iter: 489 loss: 3.15003695e-06
Iter: 490 loss: 3.14614545e-06
Iter: 491 loss: 3.14088493e-06
Iter: 492 loss: 3.14048702e-06
Iter: 493 loss: 3.13478677e-06
Iter: 494 loss: 3.1314371e-06
Iter: 495 loss: 3.12886891e-06
Iter: 496 loss: 3.11945132e-06
Iter: 497 loss: 3.19968285e-06
Iter: 498 loss: 3.11891722e-06
Iter: 499 loss: 3.11433519e-06
Iter: 500 loss: 3.11974691e-06
Iter: 501 loss: 3.11210033e-06
Iter: 502 loss: 3.11522854e-06
Iter: 503 loss: 3.11002941e-06
Iter: 504 loss: 3.10835094e-06
Iter: 505 loss: 3.10389464e-06
Iter: 506 loss: 3.13634064e-06
Iter: 507 loss: 3.10313499e-06
Iter: 508 loss: 3.09961365e-06
Iter: 509 loss: 3.09734469e-06
Iter: 510 loss: 3.09614961e-06
Iter: 511 loss: 3.09023108e-06
Iter: 512 loss: 3.11630674e-06
Iter: 513 loss: 3.0890169e-06
Iter: 514 loss: 3.08382459e-06
Iter: 515 loss: 3.10742917e-06
Iter: 516 loss: 3.08279573e-06
Iter: 517 loss: 3.0790543e-06
Iter: 518 loss: 3.07656569e-06
Iter: 519 loss: 3.07515597e-06
Iter: 520 loss: 3.06803349e-06
Iter: 521 loss: 3.09289362e-06
Iter: 522 loss: 3.06629727e-06
Iter: 523 loss: 3.0638339e-06
Iter: 524 loss: 3.06330662e-06
Iter: 525 loss: 3.0605911e-06
Iter: 526 loss: 3.05406638e-06
Iter: 527 loss: 3.12167322e-06
Iter: 528 loss: 3.05327762e-06
Iter: 529 loss: 3.04638206e-06
Iter: 530 loss: 3.09536063e-06
Iter: 531 loss: 3.04575246e-06
Iter: 532 loss: 3.04129367e-06
Iter: 533 loss: 3.03368824e-06
Iter: 534 loss: 3.03360116e-06
Iter: 535 loss: 3.02997387e-06
Iter: 536 loss: 3.02891317e-06
Iter: 537 loss: 3.02440708e-06
Iter: 538 loss: 3.0733579e-06
Iter: 539 loss: 3.02435501e-06
Iter: 540 loss: 3.02284957e-06
Iter: 541 loss: 3.018602e-06
Iter: 542 loss: 3.042858e-06
Iter: 543 loss: 3.01742239e-06
Iter: 544 loss: 3.01290856e-06
Iter: 545 loss: 3.03075331e-06
Iter: 546 loss: 3.01185582e-06
Iter: 547 loss: 3.0069009e-06
Iter: 548 loss: 3.00943657e-06
Iter: 549 loss: 3.00379952e-06
Iter: 550 loss: 2.99894964e-06
Iter: 551 loss: 3.06202674e-06
Iter: 552 loss: 2.99891508e-06
Iter: 553 loss: 2.99562475e-06
Iter: 554 loss: 2.99476869e-06
Iter: 555 loss: 2.99256499e-06
Iter: 556 loss: 2.98764e-06
Iter: 557 loss: 3.0017211e-06
Iter: 558 loss: 2.98593977e-06
Iter: 559 loss: 2.98198484e-06
Iter: 560 loss: 2.98203258e-06
Iter: 561 loss: 2.97984843e-06
Iter: 562 loss: 2.9772425e-06
Iter: 563 loss: 2.97692304e-06
Iter: 564 loss: 2.97255247e-06
Iter: 565 loss: 2.96865437e-06
Iter: 566 loss: 2.9676055e-06
Iter: 567 loss: 2.9617845e-06
Iter: 568 loss: 3.01190562e-06
Iter: 569 loss: 2.96154531e-06
Iter: 570 loss: 2.95856944e-06
Iter: 571 loss: 2.99070666e-06
Iter: 572 loss: 2.95855671e-06
Iter: 573 loss: 2.95472182e-06
Iter: 574 loss: 2.96211647e-06
Iter: 575 loss: 2.95303016e-06
Iter: 576 loss: 2.95126e-06
Iter: 577 loss: 2.94771735e-06
Iter: 578 loss: 3.01044201e-06
Iter: 579 loss: 2.94768961e-06
Iter: 580 loss: 2.94405481e-06
Iter: 581 loss: 2.9397529e-06
Iter: 582 loss: 2.93923244e-06
Iter: 583 loss: 2.93472908e-06
Iter: 584 loss: 2.9345606e-06
Iter: 585 loss: 2.93126118e-06
Iter: 586 loss: 2.93555149e-06
Iter: 587 loss: 2.92941058e-06
Iter: 588 loss: 2.924814e-06
Iter: 589 loss: 2.93411176e-06
Iter: 590 loss: 2.92283494e-06
Iter: 591 loss: 2.92017694e-06
Iter: 592 loss: 2.92024356e-06
Iter: 593 loss: 2.91798915e-06
Iter: 594 loss: 2.92138679e-06
Iter: 595 loss: 2.91688548e-06
Iter: 596 loss: 2.91405263e-06
Iter: 597 loss: 2.90997195e-06
Iter: 598 loss: 2.90971047e-06
Iter: 599 loss: 2.90622029e-06
Iter: 600 loss: 2.91428933e-06
Iter: 601 loss: 2.90485787e-06
Iter: 602 loss: 2.89997934e-06
Iter: 603 loss: 2.90790149e-06
Iter: 604 loss: 2.89756349e-06
Iter: 605 loss: 2.90043727e-06
Iter: 606 loss: 2.89646414e-06
Iter: 607 loss: 2.89492527e-06
Iter: 608 loss: 2.89295713e-06
Iter: 609 loss: 2.89275567e-06
Iter: 610 loss: 2.89127388e-06
Iter: 611 loss: 2.88712363e-06
Iter: 612 loss: 2.92458935e-06
Iter: 613 loss: 2.88642286e-06
Iter: 614 loss: 2.88168667e-06
Iter: 615 loss: 2.91121864e-06
Iter: 616 loss: 2.88126125e-06
Iter: 617 loss: 2.87723196e-06
Iter: 618 loss: 2.89569743e-06
Iter: 619 loss: 2.87649982e-06
Iter: 620 loss: 2.87230705e-06
Iter: 621 loss: 2.89089576e-06
Iter: 622 loss: 2.87151533e-06
Iter: 623 loss: 2.86835711e-06
Iter: 624 loss: 2.87520152e-06
Iter: 625 loss: 2.86702107e-06
Iter: 626 loss: 2.86376417e-06
Iter: 627 loss: 2.88922774e-06
Iter: 628 loss: 2.86353134e-06
Iter: 629 loss: 2.86032355e-06
Iter: 630 loss: 2.86500244e-06
Iter: 631 loss: 2.8586287e-06
Iter: 632 loss: 2.85590704e-06
Iter: 633 loss: 2.85526744e-06
Iter: 634 loss: 2.85352235e-06
Iter: 635 loss: 2.84958378e-06
Iter: 636 loss: 2.85557735e-06
Iter: 637 loss: 2.84762154e-06
Iter: 638 loss: 2.84412454e-06
Iter: 639 loss: 2.8653385e-06
Iter: 640 loss: 2.84382168e-06
Iter: 641 loss: 2.84377074e-06
Iter: 642 loss: 2.84241059e-06
Iter: 643 loss: 2.84138287e-06
Iter: 644 loss: 2.83816144e-06
Iter: 645 loss: 2.83793543e-06
Iter: 646 loss: 2.83471695e-06
Iter: 647 loss: 2.83023542e-06
Iter: 648 loss: 2.84297562e-06
Iter: 649 loss: 2.82878591e-06
Iter: 650 loss: 2.82373617e-06
Iter: 651 loss: 2.8370448e-06
Iter: 652 loss: 2.82211386e-06
Iter: 653 loss: 2.81851226e-06
Iter: 654 loss: 2.84855423e-06
Iter: 655 loss: 2.81834787e-06
Iter: 656 loss: 2.81511711e-06
Iter: 657 loss: 2.81900975e-06
Iter: 658 loss: 2.81347093e-06
Iter: 659 loss: 2.80947e-06
Iter: 660 loss: 2.8285865e-06
Iter: 661 loss: 2.80883387e-06
Iter: 662 loss: 2.80691711e-06
Iter: 663 loss: 2.80688073e-06
Iter: 664 loss: 2.80528457e-06
Iter: 665 loss: 2.80183326e-06
Iter: 666 loss: 2.86157092e-06
Iter: 667 loss: 2.80177142e-06
Iter: 668 loss: 2.79770347e-06
Iter: 669 loss: 2.81578627e-06
Iter: 670 loss: 2.79690857e-06
Iter: 671 loss: 2.79400456e-06
Iter: 672 loss: 2.79578444e-06
Iter: 673 loss: 2.79203823e-06
Iter: 674 loss: 2.78823427e-06
Iter: 675 loss: 2.80125255e-06
Iter: 676 loss: 2.78720881e-06
Iter: 677 loss: 2.7916642e-06
Iter: 678 loss: 2.78619382e-06
Iter: 679 loss: 2.78570405e-06
Iter: 680 loss: 2.78391644e-06
Iter: 681 loss: 2.78286075e-06
Iter: 682 loss: 2.78188463e-06
Iter: 683 loss: 2.7781316e-06
Iter: 684 loss: 2.78254606e-06
Iter: 685 loss: 2.7761746e-06
Iter: 686 loss: 2.77181857e-06
Iter: 687 loss: 2.79230244e-06
Iter: 688 loss: 2.7710621e-06
Iter: 689 loss: 2.7672595e-06
Iter: 690 loss: 2.76721903e-06
Iter: 691 loss: 2.76423407e-06
Iter: 692 loss: 2.76058313e-06
Iter: 693 loss: 2.80555309e-06
Iter: 694 loss: 2.76061974e-06
Iter: 695 loss: 2.75747379e-06
Iter: 696 loss: 2.76517039e-06
Iter: 697 loss: 2.75635239e-06
Iter: 698 loss: 2.75276534e-06
Iter: 699 loss: 2.76472929e-06
Iter: 700 loss: 2.75170328e-06
Iter: 701 loss: 2.74965851e-06
Iter: 702 loss: 2.7770152e-06
Iter: 703 loss: 2.74960598e-06
Iter: 704 loss: 2.74755303e-06
Iter: 705 loss: 2.74517379e-06
Iter: 706 loss: 2.74500371e-06
Iter: 707 loss: 2.74181366e-06
Iter: 708 loss: 2.75071648e-06
Iter: 709 loss: 2.74073659e-06
Iter: 710 loss: 2.73791761e-06
Iter: 711 loss: 2.73982823e-06
Iter: 712 loss: 2.73598653e-06
Iter: 713 loss: 2.73602814e-06
Iter: 714 loss: 2.7340318e-06
Iter: 715 loss: 2.73299611e-06
Iter: 716 loss: 2.73021169e-06
Iter: 717 loss: 2.73917794e-06
Iter: 718 loss: 2.72878083e-06
Iter: 719 loss: 2.72548596e-06
Iter: 720 loss: 2.72681177e-06
Iter: 721 loss: 2.723227e-06
Iter: 722 loss: 2.71908152e-06
Iter: 723 loss: 2.74742388e-06
Iter: 724 loss: 2.71857243e-06
Iter: 725 loss: 2.71617455e-06
Iter: 726 loss: 2.71758427e-06
Iter: 727 loss: 2.71469298e-06
Iter: 728 loss: 2.71090812e-06
Iter: 729 loss: 2.72312673e-06
Iter: 730 loss: 2.70983219e-06
Iter: 731 loss: 2.70702094e-06
Iter: 732 loss: 2.72228181e-06
Iter: 733 loss: 2.70657529e-06
Iter: 734 loss: 2.70324517e-06
Iter: 735 loss: 2.70561281e-06
Iter: 736 loss: 2.70101873e-06
Iter: 737 loss: 2.69858583e-06
Iter: 738 loss: 2.69857264e-06
Iter: 739 loss: 2.69664815e-06
Iter: 740 loss: 2.69913949e-06
Iter: 741 loss: 2.695695e-06
Iter: 742 loss: 2.69383872e-06
Iter: 743 loss: 2.69094244e-06
Iter: 744 loss: 2.69082102e-06
Iter: 745 loss: 2.69091947e-06
Iter: 746 loss: 2.68911708e-06
Iter: 747 loss: 2.68717304e-06
Iter: 748 loss: 2.68750955e-06
Iter: 749 loss: 2.68575377e-06
Iter: 750 loss: 2.68441136e-06
Iter: 751 loss: 2.68081567e-06
Iter: 752 loss: 2.71586578e-06
Iter: 753 loss: 2.68035274e-06
Iter: 754 loss: 2.67598034e-06
Iter: 755 loss: 2.68640792e-06
Iter: 756 loss: 2.67441101e-06
Iter: 757 loss: 2.67076484e-06
Iter: 758 loss: 2.69413e-06
Iter: 759 loss: 2.67038854e-06
Iter: 760 loss: 2.66698726e-06
Iter: 761 loss: 2.67209703e-06
Iter: 762 loss: 2.66556481e-06
Iter: 763 loss: 2.66233656e-06
Iter: 764 loss: 2.6811008e-06
Iter: 765 loss: 2.66186157e-06
Iter: 766 loss: 2.65897916e-06
Iter: 767 loss: 2.67067276e-06
Iter: 768 loss: 2.65856397e-06
Iter: 769 loss: 2.65596464e-06
Iter: 770 loss: 2.66786878e-06
Iter: 771 loss: 2.65550034e-06
Iter: 772 loss: 2.65336348e-06
Iter: 773 loss: 2.66425604e-06
Iter: 774 loss: 2.65296853e-06
Iter: 775 loss: 2.65078734e-06
Iter: 776 loss: 2.64632604e-06
Iter: 777 loss: 2.73950491e-06
Iter: 778 loss: 2.64635037e-06
Iter: 779 loss: 2.64411437e-06
Iter: 780 loss: 2.64404935e-06
Iter: 781 loss: 2.64352343e-06
Iter: 782 loss: 2.64313803e-06
Iter: 783 loss: 2.64243363e-06
Iter: 784 loss: 2.64042137e-06
Iter: 785 loss: 2.64211053e-06
Iter: 786 loss: 2.63875836e-06
Iter: 787 loss: 2.63601396e-06
Iter: 788 loss: 2.64607024e-06
Iter: 789 loss: 2.63540051e-06
Iter: 790 loss: 2.63284801e-06
Iter: 791 loss: 2.63215e-06
Iter: 792 loss: 2.63047195e-06
Iter: 793 loss: 2.62764706e-06
Iter: 794 loss: 2.64835171e-06
Iter: 795 loss: 2.6274588e-06
Iter: 796 loss: 2.62476397e-06
Iter: 797 loss: 2.62598951e-06
Iter: 798 loss: 2.62296589e-06
Iter: 799 loss: 2.62032154e-06
Iter: 800 loss: 2.65082667e-06
Iter: 801 loss: 2.62027e-06
Iter: 802 loss: 2.61817672e-06
Iter: 803 loss: 2.62216122e-06
Iter: 804 loss: 2.61732475e-06
Iter: 805 loss: 2.61477635e-06
Iter: 806 loss: 2.62381036e-06
Iter: 807 loss: 2.614e-06
Iter: 808 loss: 2.61125524e-06
Iter: 809 loss: 2.62779713e-06
Iter: 810 loss: 2.61096511e-06
Iter: 811 loss: 2.60920956e-06
Iter: 812 loss: 2.6086791e-06
Iter: 813 loss: 2.60777779e-06
Iter: 814 loss: 2.60618481e-06
Iter: 815 loss: 2.62631193e-06
Iter: 816 loss: 2.6061366e-06
Iter: 817 loss: 2.60449815e-06
Iter: 818 loss: 2.61103582e-06
Iter: 819 loss: 2.60401384e-06
Iter: 820 loss: 2.60342813e-06
Iter: 821 loss: 2.60172237e-06
Iter: 822 loss: 2.61263835e-06
Iter: 823 loss: 2.60132902e-06
Iter: 824 loss: 2.5987506e-06
Iter: 825 loss: 2.60308479e-06
Iter: 826 loss: 2.59757121e-06
Iter: 827 loss: 2.59486887e-06
Iter: 828 loss: 2.60406023e-06
Iter: 829 loss: 2.59415674e-06
Iter: 830 loss: 2.59157764e-06
Iter: 831 loss: 2.59329681e-06
Iter: 832 loss: 2.59010767e-06
Iter: 833 loss: 2.58756245e-06
Iter: 834 loss: 2.60324941e-06
Iter: 835 loss: 2.58737145e-06
Iter: 836 loss: 2.58487535e-06
Iter: 837 loss: 2.5858385e-06
Iter: 838 loss: 2.58325599e-06
Iter: 839 loss: 2.58175442e-06
Iter: 840 loss: 2.58154159e-06
Iter: 841 loss: 2.58019645e-06
Iter: 842 loss: 2.58259433e-06
Iter: 843 loss: 2.57965803e-06
Iter: 844 loss: 2.57775946e-06
Iter: 845 loss: 2.57902525e-06
Iter: 846 loss: 2.57643478e-06
Iter: 847 loss: 2.57441025e-06
Iter: 848 loss: 2.57727061e-06
Iter: 849 loss: 2.57336137e-06
Iter: 850 loss: 2.57294732e-06
Iter: 851 loss: 2.57262354e-06
Iter: 852 loss: 2.57151373e-06
Iter: 853 loss: 2.56947578e-06
Iter: 854 loss: 2.60414618e-06
Iter: 855 loss: 2.56951762e-06
Iter: 856 loss: 2.56795602e-06
Iter: 857 loss: 2.56729572e-06
Iter: 858 loss: 2.56656494e-06
Iter: 859 loss: 2.56472231e-06
Iter: 860 loss: 2.56592102e-06
Iter: 861 loss: 2.56364615e-06
Iter: 862 loss: 2.56065e-06
Iter: 863 loss: 2.56705653e-06
Iter: 864 loss: 2.55963369e-06
Iter: 865 loss: 2.55721034e-06
Iter: 866 loss: 2.57434181e-06
Iter: 867 loss: 2.55704936e-06
Iter: 868 loss: 2.55521854e-06
Iter: 869 loss: 2.55781652e-06
Iter: 870 loss: 2.55414966e-06
Iter: 871 loss: 2.55193663e-06
Iter: 872 loss: 2.55339455e-06
Iter: 873 loss: 2.55053374e-06
Iter: 874 loss: 2.54784118e-06
Iter: 875 loss: 2.56008025e-06
Iter: 876 loss: 2.54739916e-06
Iter: 877 loss: 2.54557904e-06
Iter: 878 loss: 2.54559473e-06
Iter: 879 loss: 2.54410315e-06
Iter: 880 loss: 2.54665611e-06
Iter: 881 loss: 2.54331053e-06
Iter: 882 loss: 2.54174938e-06
Iter: 883 loss: 2.54372321e-06
Iter: 884 loss: 2.54083398e-06
Iter: 885 loss: 2.53978101e-06
Iter: 886 loss: 2.53977032e-06
Iter: 887 loss: 2.53844655e-06
Iter: 888 loss: 2.53764438e-06
Iter: 889 loss: 2.53701819e-06
Iter: 890 loss: 2.53622511e-06
Iter: 891 loss: 2.5341069e-06
Iter: 892 loss: 2.55987561e-06
Iter: 893 loss: 2.53379631e-06
Iter: 894 loss: 2.53130634e-06
Iter: 895 loss: 2.54662882e-06
Iter: 896 loss: 2.53115468e-06
Iter: 897 loss: 2.52932182e-06
Iter: 898 loss: 2.52974496e-06
Iter: 899 loss: 2.52798054e-06
Iter: 900 loss: 2.52548534e-06
Iter: 901 loss: 2.53933649e-06
Iter: 902 loss: 2.52521249e-06
Iter: 903 loss: 2.52350765e-06
Iter: 904 loss: 2.52261634e-06
Iter: 905 loss: 2.52175414e-06
Iter: 906 loss: 2.51864572e-06
Iter: 907 loss: 2.53436565e-06
Iter: 908 loss: 2.51809388e-06
Iter: 909 loss: 2.51591723e-06
Iter: 910 loss: 2.53264739e-06
Iter: 911 loss: 2.51576125e-06
Iter: 912 loss: 2.51435108e-06
Iter: 913 loss: 2.5279669e-06
Iter: 914 loss: 2.51435495e-06
Iter: 915 loss: 2.51285019e-06
Iter: 916 loss: 2.51228539e-06
Iter: 917 loss: 2.51144297e-06
Iter: 918 loss: 2.50966355e-06
Iter: 919 loss: 2.51972688e-06
Iter: 920 loss: 2.50937728e-06
Iter: 921 loss: 2.50820062e-06
Iter: 922 loss: 2.50816e-06
Iter: 923 loss: 2.50758944e-06
Iter: 924 loss: 2.5057634e-06
Iter: 925 loss: 2.51109896e-06
Iter: 926 loss: 2.50503126e-06
Iter: 927 loss: 2.5026452e-06
Iter: 928 loss: 2.50713811e-06
Iter: 929 loss: 2.50174685e-06
Iter: 930 loss: 2.49955906e-06
Iter: 931 loss: 2.50564153e-06
Iter: 932 loss: 2.4988185e-06
Iter: 933 loss: 2.49672348e-06
Iter: 934 loss: 2.49982509e-06
Iter: 935 loss: 2.49558684e-06
Iter: 936 loss: 2.49309915e-06
Iter: 937 loss: 2.5016825e-06
Iter: 938 loss: 2.49241498e-06
Iter: 939 loss: 2.49003165e-06
Iter: 940 loss: 2.4977835e-06
Iter: 941 loss: 2.48928632e-06
Iter: 942 loss: 2.48715241e-06
Iter: 943 loss: 2.48878405e-06
Iter: 944 loss: 2.48575611e-06
Iter: 945 loss: 2.48283732e-06
Iter: 946 loss: 2.50108701e-06
Iter: 947 loss: 2.48243691e-06
Iter: 948 loss: 2.48094102e-06
Iter: 949 loss: 2.50185917e-06
Iter: 950 loss: 2.48095603e-06
Iter: 951 loss: 2.47963681e-06
Iter: 952 loss: 2.48114293e-06
Iter: 953 loss: 2.47882713e-06
Iter: 954 loss: 2.47739513e-06
Iter: 955 loss: 2.48104516e-06
Iter: 956 loss: 2.4767528e-06
Iter: 957 loss: 2.47493699e-06
Iter: 958 loss: 2.49679715e-06
Iter: 959 loss: 2.47490107e-06
Iter: 960 loss: 2.47432058e-06
Iter: 961 loss: 2.47335856e-06
Iter: 962 loss: 2.47337698e-06
Iter: 963 loss: 2.47210528e-06
Iter: 964 loss: 2.46947502e-06
Iter: 965 loss: 2.5173058e-06
Iter: 966 loss: 2.46946092e-06
Iter: 967 loss: 2.46678951e-06
Iter: 968 loss: 2.48425158e-06
Iter: 969 loss: 2.46649734e-06
Iter: 970 loss: 2.46399941e-06
Iter: 971 loss: 2.46709556e-06
Iter: 972 loss: 2.46272702e-06
Iter: 973 loss: 2.46045238e-06
Iter: 974 loss: 2.4734361e-06
Iter: 975 loss: 2.46015929e-06
Iter: 976 loss: 2.45823844e-06
Iter: 977 loss: 2.45903266e-06
Iter: 978 loss: 2.45695628e-06
Iter: 979 loss: 2.4540368e-06
Iter: 980 loss: 2.46619652e-06
Iter: 981 loss: 2.45361775e-06
Iter: 982 loss: 2.45142201e-06
Iter: 983 loss: 2.46599188e-06
Iter: 984 loss: 2.45132287e-06
Iter: 985 loss: 2.44961598e-06
Iter: 986 loss: 2.46427635e-06
Iter: 987 loss: 2.44962098e-06
Iter: 988 loss: 2.44832086e-06
Iter: 989 loss: 2.44942612e-06
Iter: 990 loss: 2.44761304e-06
Iter: 991 loss: 2.44696207e-06
Iter: 992 loss: 2.4467472e-06
Iter: 993 loss: 2.44614603e-06
Iter: 994 loss: 2.4447354e-06
Iter: 995 loss: 2.45634465e-06
Iter: 996 loss: 2.4444862e-06
Iter: 997 loss: 2.44307148e-06
Iter: 998 loss: 2.44642365e-06
Iter: 999 loss: 2.4426231e-06
Iter: 1000 loss: 2.44115381e-06
Iter: 1001 loss: 2.44217881e-06
Iter: 1002 loss: 2.44016451e-06
Iter: 1003 loss: 2.43832164e-06
Iter: 1004 loss: 2.4383703e-06
Iter: 1005 loss: 2.43691602e-06
Iter: 1006 loss: 2.4350602e-06
Iter: 1007 loss: 2.45621959e-06
Iter: 1008 loss: 2.43495833e-06
Iter: 1009 loss: 2.43335171e-06
Iter: 1010 loss: 2.43136265e-06
Iter: 1011 loss: 2.43122031e-06
Iter: 1012 loss: 2.42904343e-06
Iter: 1013 loss: 2.42897659e-06
Iter: 1014 loss: 2.42779561e-06
Iter: 1015 loss: 2.42799615e-06
Iter: 1016 loss: 2.42689475e-06
Iter: 1017 loss: 2.42526721e-06
Iter: 1018 loss: 2.44551961e-06
Iter: 1019 loss: 2.42523902e-06
Iter: 1020 loss: 2.42397391e-06
Iter: 1021 loss: 2.42973283e-06
Iter: 1022 loss: 2.42367037e-06
Iter: 1023 loss: 2.42299507e-06
Iter: 1024 loss: 2.43275372e-06
Iter: 1025 loss: 2.42302713e-06
Iter: 1026 loss: 2.42217902e-06
Iter: 1027 loss: 2.42159285e-06
Iter: 1028 loss: 2.42132592e-06
Iter: 1029 loss: 2.42054284e-06
Iter: 1030 loss: 2.41961152e-06
Iter: 1031 loss: 2.41953308e-06
Iter: 1032 loss: 2.41804923e-06
Iter: 1033 loss: 2.41982752e-06
Iter: 1034 loss: 2.41731823e-06
Iter: 1035 loss: 2.41572252e-06
Iter: 1036 loss: 2.42031228e-06
Iter: 1037 loss: 2.4152373e-06
Iter: 1038 loss: 2.41384396e-06
Iter: 1039 loss: 2.41532166e-06
Iter: 1040 loss: 2.41314e-06
Iter: 1041 loss: 2.41134012e-06
Iter: 1042 loss: 2.4149233e-06
Iter: 1043 loss: 2.41058024e-06
Iter: 1044 loss: 2.40875374e-06
Iter: 1045 loss: 2.41407633e-06
Iter: 1046 loss: 2.40813802e-06
Iter: 1047 loss: 2.40656482e-06
Iter: 1048 loss: 2.41426756e-06
Iter: 1049 loss: 2.4062897e-06
Iter: 1050 loss: 2.40471695e-06
Iter: 1051 loss: 2.40674808e-06
Iter: 1052 loss: 2.40384111e-06
Iter: 1053 loss: 2.40241661e-06
Iter: 1054 loss: 2.40241206e-06
Iter: 1055 loss: 2.40093277e-06
Iter: 1056 loss: 2.40151394e-06
Iter: 1057 loss: 2.39996075e-06
Iter: 1058 loss: 2.39946348e-06
Iter: 1059 loss: 2.39908377e-06
Iter: 1060 loss: 2.39860947e-06
Iter: 1061 loss: 2.39745759e-06
Iter: 1062 loss: 2.40756344e-06
Iter: 1063 loss: 2.39716564e-06
Iter: 1064 loss: 2.39585142e-06
Iter: 1065 loss: 2.39609744e-06
Iter: 1066 loss: 2.39487645e-06
Iter: 1067 loss: 2.39280143e-06
Iter: 1068 loss: 2.39773522e-06
Iter: 1069 loss: 2.39218616e-06
Iter: 1070 loss: 2.39068186e-06
Iter: 1071 loss: 2.39472683e-06
Iter: 1072 loss: 2.39011706e-06
Iter: 1073 loss: 2.38852317e-06
Iter: 1074 loss: 2.38758707e-06
Iter: 1075 loss: 2.38680332e-06
Iter: 1076 loss: 2.38478538e-06
Iter: 1077 loss: 2.4029539e-06
Iter: 1078 loss: 2.38459347e-06
Iter: 1079 loss: 2.38290477e-06
Iter: 1080 loss: 2.38339317e-06
Iter: 1081 loss: 2.38174357e-06
Iter: 1082 loss: 2.38008e-06
Iter: 1083 loss: 2.39090537e-06
Iter: 1084 loss: 2.37984455e-06
Iter: 1085 loss: 2.37810082e-06
Iter: 1086 loss: 2.38076291e-06
Iter: 1087 loss: 2.37734935e-06
Iter: 1088 loss: 2.37617542e-06
Iter: 1089 loss: 2.37621953e-06
Iter: 1090 loss: 2.3750913e-06
Iter: 1091 loss: 2.37691802e-06
Iter: 1092 loss: 2.37469567e-06
Iter: 1093 loss: 2.37413701e-06
Iter: 1094 loss: 2.37403037e-06
Iter: 1095 loss: 2.37339327e-06
Iter: 1096 loss: 2.37189965e-06
Iter: 1097 loss: 2.38883786e-06
Iter: 1098 loss: 2.37177937e-06
Iter: 1099 loss: 2.37061977e-06
Iter: 1100 loss: 2.37149084e-06
Iter: 1101 loss: 2.36991946e-06
Iter: 1102 loss: 2.3681564e-06
Iter: 1103 loss: 2.36941833e-06
Iter: 1104 loss: 2.3670118e-06
Iter: 1105 loss: 2.36539404e-06
Iter: 1106 loss: 2.37273616e-06
Iter: 1107 loss: 2.36502319e-06
Iter: 1108 loss: 2.36336155e-06
Iter: 1109 loss: 2.36382857e-06
Iter: 1110 loss: 2.36213782e-06
Iter: 1111 loss: 2.36006417e-06
Iter: 1112 loss: 2.37005247e-06
Iter: 1113 loss: 2.35977313e-06
Iter: 1114 loss: 2.35797052e-06
Iter: 1115 loss: 2.35769949e-06
Iter: 1116 loss: 2.35646849e-06
Iter: 1117 loss: 2.35366e-06
Iter: 1118 loss: 2.36501637e-06
Iter: 1119 loss: 2.35314269e-06
Iter: 1120 loss: 2.35103766e-06
Iter: 1121 loss: 2.35853531e-06
Iter: 1122 loss: 2.35046627e-06
Iter: 1123 loss: 2.34902109e-06
Iter: 1124 loss: 2.34899653e-06
Iter: 1125 loss: 2.34765912e-06
Iter: 1126 loss: 2.35073958e-06
Iter: 1127 loss: 2.34718073e-06
Iter: 1128 loss: 2.34613367e-06
Iter: 1129 loss: 2.34614117e-06
Iter: 1130 loss: 2.34561139e-06
Iter: 1131 loss: 2.34421e-06
Iter: 1132 loss: 2.35738548e-06
Iter: 1133 loss: 2.34401341e-06
Iter: 1134 loss: 2.34248137e-06
Iter: 1135 loss: 2.3423122e-06
Iter: 1136 loss: 2.3411144e-06
Iter: 1137 loss: 2.33909327e-06
Iter: 1138 loss: 2.35762468e-06
Iter: 1139 loss: 2.33901983e-06
Iter: 1140 loss: 2.33752303e-06
Iter: 1141 loss: 2.33696892e-06
Iter: 1142 loss: 2.33605169e-06
Iter: 1143 loss: 2.33389346e-06
Iter: 1144 loss: 2.3397688e-06
Iter: 1145 loss: 2.33315382e-06
Iter: 1146 loss: 2.33095125e-06
Iter: 1147 loss: 2.33296214e-06
Iter: 1148 loss: 2.32970956e-06
Iter: 1149 loss: 2.3274124e-06
Iter: 1150 loss: 2.3460309e-06
Iter: 1151 loss: 2.32723505e-06
Iter: 1152 loss: 2.32551565e-06
Iter: 1153 loss: 2.32313369e-06
Iter: 1154 loss: 2.3229818e-06
Iter: 1155 loss: 2.32113985e-06
Iter: 1156 loss: 2.32113416e-06
Iter: 1157 loss: 2.32015918e-06
Iter: 1158 loss: 2.32000366e-06
Iter: 1159 loss: 2.31937724e-06
Iter: 1160 loss: 2.32079583e-06
Iter: 1161 loss: 2.31905e-06
Iter: 1162 loss: 2.31793183e-06
Iter: 1163 loss: 2.31655804e-06
Iter: 1164 loss: 2.316478e-06
Iter: 1165 loss: 2.31514741e-06
Iter: 1166 loss: 2.31493232e-06
Iter: 1167 loss: 2.31407648e-06
Iter: 1168 loss: 2.31212061e-06
Iter: 1169 loss: 2.31543072e-06
Iter: 1170 loss: 2.31136619e-06
Iter: 1171 loss: 2.3095331e-06
Iter: 1172 loss: 2.31943e-06
Iter: 1173 loss: 2.30918795e-06
Iter: 1174 loss: 2.30782052e-06
Iter: 1175 loss: 2.31046988e-06
Iter: 1176 loss: 2.30718229e-06
Iter: 1177 loss: 2.30582054e-06
Iter: 1178 loss: 2.30510796e-06
Iter: 1179 loss: 2.30440105e-06
Iter: 1180 loss: 2.30217e-06
Iter: 1181 loss: 2.31371519e-06
Iter: 1182 loss: 2.30187516e-06
Iter: 1183 loss: 2.29995612e-06
Iter: 1184 loss: 2.30501178e-06
Iter: 1185 loss: 2.2993263e-06
Iter: 1186 loss: 2.29751822e-06
Iter: 1187 loss: 2.30089245e-06
Iter: 1188 loss: 2.29663237e-06
Iter: 1189 loss: 2.29534476e-06
Iter: 1190 loss: 2.29535362e-06
Iter: 1191 loss: 2.29407078e-06
Iter: 1192 loss: 2.30500154e-06
Iter: 1193 loss: 2.29399188e-06
Iter: 1194 loss: 2.29304601e-06
Iter: 1195 loss: 2.29653961e-06
Iter: 1196 loss: 2.29285365e-06
Iter: 1197 loss: 2.29223883e-06
Iter: 1198 loss: 2.29076431e-06
Iter: 1199 loss: 2.31115109e-06
Iter: 1200 loss: 2.29072202e-06
Iter: 1201 loss: 2.28910517e-06
Iter: 1202 loss: 2.29203079e-06
Iter: 1203 loss: 2.2885074e-06
Iter: 1204 loss: 2.28661247e-06
Iter: 1205 loss: 2.29539182e-06
Iter: 1206 loss: 2.28638305e-06
Iter: 1207 loss: 2.28516319e-06
Iter: 1208 loss: 2.28736189e-06
Iter: 1209 loss: 2.28460976e-06
Iter: 1210 loss: 2.28285671e-06
Iter: 1211 loss: 2.28168528e-06
Iter: 1212 loss: 2.28097747e-06
Iter: 1213 loss: 2.27899727e-06
Iter: 1214 loss: 2.28647059e-06
Iter: 1215 loss: 2.2784684e-06
Iter: 1216 loss: 2.27656983e-06
Iter: 1217 loss: 2.28587032e-06
Iter: 1218 loss: 2.27627197e-06
Iter: 1219 loss: 2.27499208e-06
Iter: 1220 loss: 2.27928717e-06
Iter: 1221 loss: 2.27466444e-06
Iter: 1222 loss: 2.27339478e-06
Iter: 1223 loss: 2.27802843e-06
Iter: 1224 loss: 2.27307123e-06
Iter: 1225 loss: 2.2727088e-06
Iter: 1226 loss: 2.27241458e-06
Iter: 1227 loss: 2.27200462e-06
Iter: 1228 loss: 2.27294959e-06
Iter: 1229 loss: 2.27179544e-06
Iter: 1230 loss: 2.2711838e-06
Iter: 1231 loss: 2.26988891e-06
Iter: 1232 loss: 2.28746489e-06
Iter: 1233 loss: 2.26973089e-06
Iter: 1234 loss: 2.26867e-06
Iter: 1235 loss: 2.26987549e-06
Iter: 1236 loss: 2.26799807e-06
Iter: 1237 loss: 2.26644693e-06
Iter: 1238 loss: 2.27247824e-06
Iter: 1239 loss: 2.26607517e-06
Iter: 1240 loss: 2.26463908e-06
Iter: 1241 loss: 2.26720704e-06
Iter: 1242 loss: 2.26414159e-06
Iter: 1243 loss: 2.26282691e-06
Iter: 1244 loss: 2.26791167e-06
Iter: 1245 loss: 2.26259522e-06
Iter: 1246 loss: 2.26125962e-06
Iter: 1247 loss: 2.26064e-06
Iter: 1248 loss: 2.25999338e-06
Iter: 1249 loss: 2.25828944e-06
Iter: 1250 loss: 2.26686461e-06
Iter: 1251 loss: 2.25785652e-06
Iter: 1252 loss: 2.2564775e-06
Iter: 1253 loss: 2.25770918e-06
Iter: 1254 loss: 2.25558119e-06
Iter: 1255 loss: 2.25362965e-06
Iter: 1256 loss: 2.26068892e-06
Iter: 1257 loss: 2.25305894e-06
Iter: 1258 loss: 2.25275676e-06
Iter: 1259 loss: 2.25233043e-06
Iter: 1260 loss: 2.25174e-06
Iter: 1261 loss: 2.254837e-06
Iter: 1262 loss: 2.25163058e-06
Iter: 1263 loss: 2.25118538e-06
Iter: 1264 loss: 2.25113e-06
Iter: 1265 loss: 2.25076701e-06
Iter: 1266 loss: 2.25017129e-06
Iter: 1267 loss: 2.24902078e-06
Iter: 1268 loss: 2.2489935e-06
Iter: 1269 loss: 2.24810128e-06
Iter: 1270 loss: 2.25245094e-06
Iter: 1271 loss: 2.24802579e-06
Iter: 1272 loss: 2.24685164e-06
Iter: 1273 loss: 2.24841483e-06
Iter: 1274 loss: 2.24629343e-06
Iter: 1275 loss: 2.2451618e-06
Iter: 1276 loss: 2.24837663e-06
Iter: 1277 loss: 2.24491441e-06
Iter: 1278 loss: 2.24354562e-06
Iter: 1279 loss: 2.24437167e-06
Iter: 1280 loss: 2.24280166e-06
Iter: 1281 loss: 2.2414622e-06
Iter: 1282 loss: 2.24940982e-06
Iter: 1283 loss: 2.24120549e-06
Iter: 1284 loss: 2.24018504e-06
Iter: 1285 loss: 2.23982443e-06
Iter: 1286 loss: 2.23916572e-06
Iter: 1287 loss: 2.23787765e-06
Iter: 1288 loss: 2.24181667e-06
Iter: 1289 loss: 2.23754819e-06
Iter: 1290 loss: 2.23637358e-06
Iter: 1291 loss: 2.24738096e-06
Iter: 1292 loss: 2.23626898e-06
Iter: 1293 loss: 2.23578877e-06
Iter: 1294 loss: 2.23570942e-06
Iter: 1295 loss: 2.23532334e-06
Iter: 1296 loss: 2.23511415e-06
Iter: 1297 loss: 2.23494612e-06
Iter: 1298 loss: 2.23438155e-06
Iter: 1299 loss: 2.23357824e-06
Iter: 1300 loss: 2.23359439e-06
Iter: 1301 loss: 2.23233792e-06
Iter: 1302 loss: 2.23157485e-06
Iter: 1303 loss: 2.23109646e-06
Iter: 1304 loss: 2.22973699e-06
Iter: 1305 loss: 2.24868654e-06
Iter: 1306 loss: 2.22976792e-06
Iter: 1307 loss: 2.22884455e-06
Iter: 1308 loss: 2.22969061e-06
Iter: 1309 loss: 2.22820699e-06
Iter: 1310 loss: 2.22690505e-06
Iter: 1311 loss: 2.22819722e-06
Iter: 1312 loss: 2.22625727e-06
Iter: 1313 loss: 2.22489598e-06
Iter: 1314 loss: 2.23030793e-06
Iter: 1315 loss: 2.22457879e-06
Iter: 1316 loss: 2.22331414e-06
Iter: 1317 loss: 2.22516837e-06
Iter: 1318 loss: 2.22278322e-06
Iter: 1319 loss: 2.22150038e-06
Iter: 1320 loss: 2.22489734e-06
Iter: 1321 loss: 2.22112544e-06
Iter: 1322 loss: 2.21994151e-06
Iter: 1323 loss: 2.22049766e-06
Iter: 1324 loss: 2.21913024e-06
Iter: 1325 loss: 2.22123845e-06
Iter: 1326 loss: 2.2187437e-06
Iter: 1327 loss: 2.21832852e-06
Iter: 1328 loss: 2.2178458e-06
Iter: 1329 loss: 2.21779419e-06
Iter: 1330 loss: 2.21703044e-06
Iter: 1331 loss: 2.21730215e-06
Iter: 1332 loss: 2.2164736e-06
Iter: 1333 loss: 2.21555092e-06
Iter: 1334 loss: 2.21526352e-06
Iter: 1335 loss: 2.21473852e-06
Iter: 1336 loss: 2.21378332e-06
Iter: 1337 loss: 2.21626397e-06
Iter: 1338 loss: 2.21352e-06
Iter: 1339 loss: 2.21241453e-06
Iter: 1340 loss: 2.21590926e-06
Iter: 1341 loss: 2.21210303e-06
Iter: 1342 loss: 2.21104051e-06
Iter: 1343 loss: 2.21426967e-06
Iter: 1344 loss: 2.21080063e-06
Iter: 1345 loss: 2.20982929e-06
Iter: 1346 loss: 2.20944639e-06
Iter: 1347 loss: 2.20883339e-06
Iter: 1348 loss: 2.2072029e-06
Iter: 1349 loss: 2.2141935e-06
Iter: 1350 loss: 2.20684888e-06
Iter: 1351 loss: 2.20560173e-06
Iter: 1352 loss: 2.2081681e-06
Iter: 1353 loss: 2.20511265e-06
Iter: 1354 loss: 2.20381298e-06
Iter: 1355 loss: 2.20588936e-06
Iter: 1356 loss: 2.20320726e-06
Iter: 1357 loss: 2.20268907e-06
Iter: 1358 loss: 2.20257448e-06
Iter: 1359 loss: 2.20174184e-06
Iter: 1360 loss: 2.20233892e-06
Iter: 1361 loss: 2.20127185e-06
Iter: 1362 loss: 2.20058701e-06
Iter: 1363 loss: 2.20239122e-06
Iter: 1364 loss: 2.20036054e-06
Iter: 1365 loss: 2.19967251e-06
Iter: 1366 loss: 2.19868934e-06
Iter: 1367 loss: 2.19854292e-06
Iter: 1368 loss: 2.19730509e-06
Iter: 1369 loss: 2.19783578e-06
Iter: 1370 loss: 2.19653134e-06
Iter: 1371 loss: 2.1948606e-06
Iter: 1372 loss: 2.20235461e-06
Iter: 1373 loss: 2.19459457e-06
Iter: 1374 loss: 2.19303911e-06
Iter: 1375 loss: 2.19788626e-06
Iter: 1376 loss: 2.19257936e-06
Iter: 1377 loss: 2.19117783e-06
Iter: 1378 loss: 2.19360618e-06
Iter: 1379 loss: 2.19052345e-06
Iter: 1380 loss: 2.18925243e-06
Iter: 1381 loss: 2.19217509e-06
Iter: 1382 loss: 2.18876244e-06
Iter: 1383 loss: 2.1872429e-06
Iter: 1384 loss: 2.18960258e-06
Iter: 1385 loss: 2.18641799e-06
Iter: 1386 loss: 2.18475361e-06
Iter: 1387 loss: 2.19144067e-06
Iter: 1388 loss: 2.18433206e-06
Iter: 1389 loss: 2.18293962e-06
Iter: 1390 loss: 2.18151945e-06
Iter: 1391 loss: 2.18131186e-06
Iter: 1392 loss: 2.18533864e-06
Iter: 1393 loss: 2.18060904e-06
Iter: 1394 loss: 2.18003515e-06
Iter: 1395 loss: 2.17893785e-06
Iter: 1396 loss: 2.19439562e-06
Iter: 1397 loss: 2.17887646e-06
Iter: 1398 loss: 2.17794e-06
Iter: 1399 loss: 2.18668401e-06
Iter: 1400 loss: 2.17785646e-06
Iter: 1401 loss: 2.17716683e-06
Iter: 1402 loss: 2.17532624e-06
Iter: 1403 loss: 2.1944611e-06
Iter: 1404 loss: 2.17509341e-06
Iter: 1405 loss: 2.17290199e-06
Iter: 1406 loss: 2.18033665e-06
Iter: 1407 loss: 2.17229672e-06
Iter: 1408 loss: 2.17081879e-06
Iter: 1409 loss: 2.18941705e-06
Iter: 1410 loss: 2.17082766e-06
Iter: 1411 loss: 2.16947092e-06
Iter: 1412 loss: 2.16849958e-06
Iter: 1413 loss: 2.16800368e-06
Iter: 1414 loss: 2.16618514e-06
Iter: 1415 loss: 2.16888975e-06
Iter: 1416 loss: 2.16530088e-06
Iter: 1417 loss: 2.16312492e-06
Iter: 1418 loss: 2.16791159e-06
Iter: 1419 loss: 2.16223953e-06
Iter: 1420 loss: 2.16049557e-06
Iter: 1421 loss: 2.16871422e-06
Iter: 1422 loss: 2.16008561e-06
Iter: 1423 loss: 2.15799855e-06
Iter: 1424 loss: 2.16077819e-06
Iter: 1425 loss: 2.15697719e-06
Iter: 1426 loss: 2.15535192e-06
Iter: 1427 loss: 2.16390936e-06
Iter: 1428 loss: 2.1552205e-06
Iter: 1429 loss: 2.15507589e-06
Iter: 1430 loss: 2.15447199e-06
Iter: 1431 loss: 2.15407226e-06
Iter: 1432 loss: 2.15295586e-06
Iter: 1433 loss: 2.15722321e-06
Iter: 1434 loss: 2.15237355e-06
Iter: 1435 loss: 2.15141108e-06
Iter: 1436 loss: 2.15144132e-06
Iter: 1437 loss: 2.15064983e-06
Iter: 1438 loss: 2.14882198e-06
Iter: 1439 loss: 2.17868865e-06
Iter: 1440 loss: 2.14867896e-06
Iter: 1441 loss: 2.14720558e-06
Iter: 1442 loss: 2.15136879e-06
Iter: 1443 loss: 2.14670399e-06
Iter: 1444 loss: 2.14532224e-06
Iter: 1445 loss: 2.15062119e-06
Iter: 1446 loss: 2.14496345e-06
Iter: 1447 loss: 2.14328338e-06
Iter: 1448 loss: 2.14425427e-06
Iter: 1449 loss: 2.14221473e-06
Iter: 1450 loss: 2.14044576e-06
Iter: 1451 loss: 2.15060982e-06
Iter: 1452 loss: 2.14028591e-06
Iter: 1453 loss: 2.13873773e-06
Iter: 1454 loss: 2.13839667e-06
Iter: 1455 loss: 2.13754e-06
Iter: 1456 loss: 2.13568501e-06
Iter: 1457 loss: 2.14020838e-06
Iter: 1458 loss: 2.13499061e-06
Iter: 1459 loss: 2.13339399e-06
Iter: 1460 loss: 2.13590943e-06
Iter: 1461 loss: 2.1326905e-06
Iter: 1462 loss: 2.13134149e-06
Iter: 1463 loss: 2.15411274e-06
Iter: 1464 loss: 2.13127987e-06
Iter: 1465 loss: 2.12969553e-06
Iter: 1466 loss: 2.1364242e-06
Iter: 1467 loss: 2.12938289e-06
Iter: 1468 loss: 2.12886357e-06
Iter: 1469 loss: 2.12751138e-06
Iter: 1470 loss: 2.1407543e-06
Iter: 1471 loss: 2.12741315e-06
Iter: 1472 loss: 2.12594068e-06
Iter: 1473 loss: 2.14593388e-06
Iter: 1474 loss: 2.12593159e-06
Iter: 1475 loss: 2.12484429e-06
Iter: 1476 loss: 2.12391797e-06
Iter: 1477 loss: 2.1235021e-06
Iter: 1478 loss: 2.12217083e-06
Iter: 1479 loss: 2.12098257e-06
Iter: 1480 loss: 2.12047462e-06
Iter: 1481 loss: 2.11879365e-06
Iter: 1482 loss: 2.14099191e-06
Iter: 1483 loss: 2.11882366e-06
Iter: 1484 loss: 2.11720135e-06
Iter: 1485 loss: 2.12302075e-06
Iter: 1486 loss: 2.11681254e-06
Iter: 1487 loss: 2.11519227e-06
Iter: 1488 loss: 2.11582983e-06
Iter: 1489 loss: 2.11406632e-06
Iter: 1490 loss: 2.11262227e-06
Iter: 1491 loss: 2.12076975e-06
Iter: 1492 loss: 2.11244537e-06
Iter: 1493 loss: 2.11119777e-06
Iter: 1494 loss: 2.10979374e-06
Iter: 1495 loss: 2.10961252e-06
Iter: 1496 loss: 2.10782969e-06
Iter: 1497 loss: 2.13354269e-06
Iter: 1498 loss: 2.10784e-06
Iter: 1499 loss: 2.1082426e-06
Iter: 1500 loss: 2.1074768e-06
Iter: 1501 loss: 2.10716394e-06
Iter: 1502 loss: 2.1060398e-06
Iter: 1503 loss: 2.10789267e-06
Iter: 1504 loss: 2.10534904e-06
Iter: 1505 loss: 2.10387884e-06
Iter: 1506 loss: 2.11474412e-06
Iter: 1507 loss: 2.1038054e-06
Iter: 1508 loss: 2.10266671e-06
Iter: 1509 loss: 2.11303e-06
Iter: 1510 loss: 2.10264125e-06
Iter: 1511 loss: 2.10202415e-06
Iter: 1512 loss: 2.10083e-06
Iter: 1513 loss: 2.1228243e-06
Iter: 1514 loss: 2.10075768e-06
Iter: 1515 loss: 2.09905238e-06
Iter: 1516 loss: 2.10294024e-06
Iter: 1517 loss: 2.09859695e-06
Iter: 1518 loss: 2.0969328e-06
Iter: 1519 loss: 2.10248209e-06
Iter: 1520 loss: 2.09661675e-06
Iter: 1521 loss: 2.09481141e-06
Iter: 1522 loss: 2.10128042e-06
Iter: 1523 loss: 2.0943146e-06
Iter: 1524 loss: 2.09275458e-06
Iter: 1525 loss: 2.09375162e-06
Iter: 1526 loss: 2.09160225e-06
Iter: 1527 loss: 2.08996812e-06
Iter: 1528 loss: 2.09032964e-06
Iter: 1529 loss: 2.08877e-06
Iter: 1530 loss: 2.0869461e-06
Iter: 1531 loss: 2.10155986e-06
Iter: 1532 loss: 2.08683127e-06
Iter: 1533 loss: 2.08546453e-06
Iter: 1534 loss: 2.08788e-06
Iter: 1535 loss: 2.08480651e-06
Iter: 1536 loss: 2.08491474e-06
Iter: 1537 loss: 2.08400525e-06
Iter: 1538 loss: 2.08356823e-06
Iter: 1539 loss: 2.08259371e-06
Iter: 1540 loss: 2.08497795e-06
Iter: 1541 loss: 2.08201686e-06
Iter: 1542 loss: 2.08042479e-06
Iter: 1543 loss: 2.08239703e-06
Iter: 1544 loss: 2.07961398e-06
Iter: 1545 loss: 2.07914786e-06
Iter: 1546 loss: 2.07877883e-06
Iter: 1547 loss: 2.0780858e-06
Iter: 1548 loss: 2.07744279e-06
Iter: 1549 loss: 2.07734752e-06
Iter: 1550 loss: 2.07616722e-06
Iter: 1551 loss: 2.07555513e-06
Iter: 1552 loss: 2.07505718e-06
Iter: 1553 loss: 2.07364678e-06
Iter: 1554 loss: 2.08091524e-06
Iter: 1555 loss: 2.07335643e-06
Iter: 1556 loss: 2.07183393e-06
Iter: 1557 loss: 2.07111771e-06
Iter: 1558 loss: 2.07042558e-06
Iter: 1559 loss: 2.06859795e-06
Iter: 1560 loss: 2.08161555e-06
Iter: 1561 loss: 2.06841469e-06
Iter: 1562 loss: 2.06672803e-06
Iter: 1563 loss: 2.06777304e-06
Iter: 1564 loss: 2.06553364e-06
Iter: 1565 loss: 2.06406639e-06
Iter: 1566 loss: 2.08750203e-06
Iter: 1567 loss: 2.06408322e-06
Iter: 1568 loss: 2.0630805e-06
Iter: 1569 loss: 2.06725781e-06
Iter: 1570 loss: 2.06289747e-06
Iter: 1571 loss: 2.06205777e-06
Iter: 1572 loss: 2.0620505e-06
Iter: 1573 loss: 2.06156528e-06
Iter: 1574 loss: 2.06003665e-06
Iter: 1575 loss: 2.06005961e-06
Iter: 1576 loss: 2.05827337e-06
Iter: 1577 loss: 2.05596643e-06
Iter: 1578 loss: 2.06128675e-06
Iter: 1579 loss: 2.05517654e-06
Iter: 1580 loss: 2.05343667e-06
Iter: 1581 loss: 2.07989956e-06
Iter: 1582 loss: 2.05342803e-06
Iter: 1583 loss: 2.05150855e-06
Iter: 1584 loss: 2.0579937e-06
Iter: 1585 loss: 2.05101037e-06
Iter: 1586 loss: 2.0499433e-06
Iter: 1587 loss: 2.04922935e-06
Iter: 1588 loss: 2.04881076e-06
Iter: 1589 loss: 2.04686239e-06
Iter: 1590 loss: 2.04854359e-06
Iter: 1591 loss: 2.04571256e-06
Iter: 1592 loss: 2.04363459e-06
Iter: 1593 loss: 2.05354036e-06
Iter: 1594 loss: 2.04324124e-06
Iter: 1595 loss: 2.04131493e-06
Iter: 1596 loss: 2.04269031e-06
Iter: 1597 loss: 2.04018033e-06
Iter: 1598 loss: 2.03816262e-06
Iter: 1599 loss: 2.04491971e-06
Iter: 1600 loss: 2.03763761e-06
Iter: 1601 loss: 2.03566333e-06
Iter: 1602 loss: 2.04547496e-06
Iter: 1603 loss: 2.03530794e-06
Iter: 1604 loss: 2.03458e-06
Iter: 1605 loss: 2.03441095e-06
Iter: 1606 loss: 2.03354853e-06
Iter: 1607 loss: 2.0376242e-06
Iter: 1608 loss: 2.03344621e-06
Iter: 1609 loss: 2.03295781e-06
Iter: 1610 loss: 2.0313073e-06
Iter: 1611 loss: 2.03716718e-06
Iter: 1612 loss: 2.03047694e-06
Iter: 1613 loss: 2.02830188e-06
Iter: 1614 loss: 2.04134426e-06
Iter: 1615 loss: 2.02805109e-06
Iter: 1616 loss: 2.02668616e-06
Iter: 1617 loss: 2.02509818e-06
Iter: 1618 loss: 2.02502474e-06
Iter: 1619 loss: 2.02433489e-06
Iter: 1620 loss: 2.02375622e-06
Iter: 1621 loss: 2.02269325e-06
Iter: 1622 loss: 2.02354522e-06
Iter: 1623 loss: 2.02206934e-06
Iter: 1624 loss: 2.02084493e-06
Iter: 1625 loss: 2.01895909e-06
Iter: 1626 loss: 2.01898638e-06
Iter: 1627 loss: 2.01748458e-06
Iter: 1628 loss: 2.02790397e-06
Iter: 1629 loss: 2.01740249e-06
Iter: 1630 loss: 2.01576927e-06
Iter: 1631 loss: 2.0152811e-06
Iter: 1632 loss: 2.01423154e-06
Iter: 1633 loss: 2.01191529e-06
Iter: 1634 loss: 2.03015452e-06
Iter: 1635 loss: 2.01178341e-06
Iter: 1636 loss: 2.01018156e-06
Iter: 1637 loss: 2.00926934e-06
Iter: 1638 loss: 2.00858972e-06
Iter: 1639 loss: 2.00721433e-06
Iter: 1640 loss: 2.00719751e-06
Iter: 1641 loss: 2.00582804e-06
Iter: 1642 loss: 2.02569299e-06
Iter: 1643 loss: 2.00588806e-06
Iter: 1644 loss: 2.00566478e-06
Iter: 1645 loss: 2.00471186e-06
Iter: 1646 loss: 2.0068228e-06
Iter: 1647 loss: 2.00407567e-06
Iter: 1648 loss: 2.00241402e-06
Iter: 1649 loss: 2.0111886e-06
Iter: 1650 loss: 2.00214117e-06
Iter: 1651 loss: 2.00112117e-06
Iter: 1652 loss: 2.00150339e-06
Iter: 1653 loss: 2.00026329e-06
Iter: 1654 loss: 1.99950182e-06
Iter: 1655 loss: 1.99948659e-06
Iter: 1656 loss: 1.99866076e-06
Iter: 1657 loss: 1.99783449e-06
Iter: 1658 loss: 1.99786473e-06
Iter: 1659 loss: 1.99647548e-06
Iter: 1660 loss: 1.99725764e-06
Iter: 1661 loss: 1.99580063e-06
Iter: 1662 loss: 1.99422584e-06
Iter: 1663 loss: 1.99666215e-06
Iter: 1664 loss: 1.99342776e-06
Iter: 1665 loss: 1.99189935e-06
Iter: 1666 loss: 1.99590886e-06
Iter: 1667 loss: 1.99128317e-06
Iter: 1668 loss: 1.98946645e-06
Iter: 1669 loss: 1.99647275e-06
Iter: 1670 loss: 1.98900398e-06
Iter: 1671 loss: 1.98805856e-06
Iter: 1672 loss: 1.98800149e-06
Iter: 1673 loss: 1.98752468e-06
Iter: 1674 loss: 1.98752809e-06
Iter: 1675 loss: 1.98716066e-06
Iter: 1676 loss: 1.98613202e-06
Iter: 1677 loss: 1.98537782e-06
Iter: 1678 loss: 1.98476846e-06
Iter: 1679 loss: 1.98349949e-06
Iter: 1680 loss: 1.98346856e-06
Iter: 1681 loss: 1.98268731e-06
Iter: 1682 loss: 1.9822412e-06
Iter: 1683 loss: 1.98189628e-06
Iter: 1684 loss: 1.98054204e-06
Iter: 1685 loss: 1.98254361e-06
Iter: 1686 loss: 1.97982035e-06
Iter: 1687 loss: 1.97881582e-06
Iter: 1688 loss: 1.97865734e-06
Iter: 1689 loss: 1.97817621e-06
Iter: 1690 loss: 1.97688973e-06
Iter: 1691 loss: 1.99173905e-06
Iter: 1692 loss: 1.97676877e-06
Iter: 1693 loss: 1.97530198e-06
Iter: 1694 loss: 1.97915642e-06
Iter: 1695 loss: 1.97485792e-06
Iter: 1696 loss: 1.9734257e-06
Iter: 1697 loss: 1.97682107e-06
Iter: 1698 loss: 1.97293934e-06
Iter: 1699 loss: 1.97148847e-06
Iter: 1700 loss: 1.975141e-06
Iter: 1701 loss: 1.97112558e-06
Iter: 1702 loss: 1.96974383e-06
Iter: 1703 loss: 1.97974805e-06
Iter: 1704 loss: 1.9697186e-06
Iter: 1705 loss: 1.96958399e-06
Iter: 1706 loss: 1.9691829e-06
Iter: 1707 loss: 1.96886458e-06
Iter: 1708 loss: 1.96767064e-06
Iter: 1709 loss: 1.97509962e-06
Iter: 1710 loss: 1.96742894e-06
Iter: 1711 loss: 1.9664144e-06
Iter: 1712 loss: 1.96930728e-06
Iter: 1713 loss: 1.96619862e-06
Iter: 1714 loss: 1.96510496e-06
Iter: 1715 loss: 1.96727524e-06
Iter: 1716 loss: 1.96456085e-06
Iter: 1717 loss: 1.96331439e-06
Iter: 1718 loss: 1.96595443e-06
Iter: 1719 loss: 1.96271458e-06
Iter: 1720 loss: 1.96193832e-06
Iter: 1721 loss: 1.96194287e-06
Iter: 1722 loss: 1.96104338e-06
Iter: 1723 loss: 1.96005203e-06
Iter: 1724 loss: 1.95998359e-06
Iter: 1725 loss: 1.95866733e-06
Iter: 1726 loss: 1.96085784e-06
Iter: 1727 loss: 1.95806933e-06
Iter: 1728 loss: 1.95664484e-06
Iter: 1729 loss: 1.95710618e-06
Iter: 1730 loss: 1.9556046e-06
Iter: 1731 loss: 1.95384655e-06
Iter: 1732 loss: 1.96399742e-06
Iter: 1733 loss: 1.95353414e-06
Iter: 1734 loss: 1.95225721e-06
Iter: 1735 loss: 1.95278972e-06
Iter: 1736 loss: 1.95139182e-06
Iter: 1737 loss: 1.95244183e-06
Iter: 1738 loss: 1.95071334e-06
Iter: 1739 loss: 1.95009852e-06
Iter: 1740 loss: 1.94914742e-06
Iter: 1741 loss: 1.94910126e-06
Iter: 1742 loss: 1.94843324e-06
Iter: 1743 loss: 1.94804397e-06
Iter: 1744 loss: 1.94773565e-06
Iter: 1745 loss: 1.94644053e-06
Iter: 1746 loss: 1.94766926e-06
Iter: 1747 loss: 1.94560835e-06
Iter: 1748 loss: 1.94397126e-06
Iter: 1749 loss: 1.95970733e-06
Iter: 1750 loss: 1.94396e-06
Iter: 1751 loss: 1.94286554e-06
Iter: 1752 loss: 1.94511927e-06
Iter: 1753 loss: 1.94239828e-06
Iter: 1754 loss: 1.94126096e-06
Iter: 1755 loss: 1.95061875e-06
Iter: 1756 loss: 1.94120958e-06
Iter: 1757 loss: 1.94017935e-06
Iter: 1758 loss: 1.93907772e-06
Iter: 1759 loss: 1.93891174e-06
Iter: 1760 loss: 1.93759206e-06
Iter: 1761 loss: 1.93908181e-06
Iter: 1762 loss: 1.93694314e-06
Iter: 1763 loss: 1.93566302e-06
Iter: 1764 loss: 1.94183076e-06
Iter: 1765 loss: 1.93547044e-06
Iter: 1766 loss: 1.93409528e-06
Iter: 1767 loss: 1.93480923e-06
Iter: 1768 loss: 1.93317283e-06
Iter: 1769 loss: 1.93241e-06
Iter: 1770 loss: 1.9323561e-06
Iter: 1771 loss: 1.93149936e-06
Iter: 1772 loss: 1.93756614e-06
Iter: 1773 loss: 1.93142387e-06
Iter: 1774 loss: 1.93098936e-06
Iter: 1775 loss: 1.92994844e-06
Iter: 1776 loss: 1.93217147e-06
Iter: 1777 loss: 1.92919606e-06
Iter: 1778 loss: 1.92781499e-06
Iter: 1779 loss: 1.93927622e-06
Iter: 1780 loss: 1.92776542e-06
Iter: 1781 loss: 1.92674361e-06
Iter: 1782 loss: 1.93122742e-06
Iter: 1783 loss: 1.92645075e-06
Iter: 1784 loss: 1.92536618e-06
Iter: 1785 loss: 1.92732477e-06
Iter: 1786 loss: 1.92483867e-06
Iter: 1787 loss: 1.92370317e-06
Iter: 1788 loss: 1.93460323e-06
Iter: 1789 loss: 1.92369157e-06
Iter: 1790 loss: 1.92294533e-06
Iter: 1791 loss: 1.92504081e-06
Iter: 1792 loss: 1.92267225e-06
Iter: 1793 loss: 1.9219558e-06
Iter: 1794 loss: 1.92072525e-06
Iter: 1795 loss: 1.94949735e-06
Iter: 1796 loss: 1.92076573e-06
Iter: 1797 loss: 1.91934805e-06
Iter: 1798 loss: 1.92200741e-06
Iter: 1799 loss: 1.91879349e-06
Iter: 1800 loss: 1.91711274e-06
Iter: 1801 loss: 1.91900108e-06
Iter: 1802 loss: 1.91624304e-06
Iter: 1803 loss: 1.91527624e-06
Iter: 1804 loss: 1.91523395e-06
Iter: 1805 loss: 1.91470622e-06
Iter: 1806 loss: 1.91472782e-06
Iter: 1807 loss: 1.91430422e-06
Iter: 1808 loss: 1.91309482e-06
Iter: 1809 loss: 1.91525714e-06
Iter: 1810 loss: 1.91227127e-06
Iter: 1811 loss: 1.91067329e-06
Iter: 1812 loss: 1.91617755e-06
Iter: 1813 loss: 1.91024446e-06
Iter: 1814 loss: 1.90887158e-06
Iter: 1815 loss: 1.91293634e-06
Iter: 1816 loss: 1.90843866e-06
Iter: 1817 loss: 1.90700302e-06
Iter: 1818 loss: 1.91667141e-06
Iter: 1819 loss: 1.90683431e-06
Iter: 1820 loss: 1.9056821e-06
Iter: 1821 loss: 1.90824699e-06
Iter: 1822 loss: 1.90530102e-06
Iter: 1823 loss: 1.90401011e-06
Iter: 1824 loss: 1.91198069e-06
Iter: 1825 loss: 1.90380831e-06
Iter: 1826 loss: 1.90283777e-06
Iter: 1827 loss: 1.90267065e-06
Iter: 1828 loss: 1.90200103e-06
Iter: 1829 loss: 1.90079675e-06
Iter: 1830 loss: 1.90109313e-06
Iter: 1831 loss: 1.8997406e-06
Iter: 1832 loss: 1.89796106e-06
Iter: 1833 loss: 1.90148421e-06
Iter: 1834 loss: 1.89718776e-06
Iter: 1835 loss: 1.89568777e-06
Iter: 1836 loss: 1.90166247e-06
Iter: 1837 loss: 1.89529862e-06
Iter: 1838 loss: 1.89626905e-06
Iter: 1839 loss: 1.89479715e-06
Iter: 1840 loss: 1.89439913e-06
Iter: 1841 loss: 1.8931172e-06
Iter: 1842 loss: 1.89936316e-06
Iter: 1843 loss: 1.8927085e-06
Iter: 1844 loss: 1.89175262e-06
Iter: 1845 loss: 1.89331217e-06
Iter: 1846 loss: 1.89131299e-06
Iter: 1847 loss: 1.89022069e-06
Iter: 1848 loss: 1.8905148e-06
Iter: 1849 loss: 1.88939271e-06
Iter: 1850 loss: 1.88827016e-06
Iter: 1851 loss: 1.90408605e-06
Iter: 1852 loss: 1.88829563e-06
Iter: 1853 loss: 1.88734e-06
Iter: 1854 loss: 1.89048319e-06
Iter: 1855 loss: 1.88712193e-06
Iter: 1856 loss: 1.88635568e-06
Iter: 1857 loss: 1.88993818e-06
Iter: 1858 loss: 1.88628235e-06
Iter: 1859 loss: 1.885458e-06
Iter: 1860 loss: 1.88521324e-06
Iter: 1861 loss: 1.88464833e-06
Iter: 1862 loss: 1.88355739e-06
Iter: 1863 loss: 1.88441959e-06
Iter: 1864 loss: 1.88290494e-06
Iter: 1865 loss: 1.88157787e-06
Iter: 1866 loss: 1.88477327e-06
Iter: 1867 loss: 1.88106515e-06
Iter: 1868 loss: 1.87975206e-06
Iter: 1869 loss: 1.88093156e-06
Iter: 1870 loss: 1.87897547e-06
Iter: 1871 loss: 1.87966737e-06
Iter: 1872 loss: 1.87851e-06
Iter: 1873 loss: 1.87787623e-06
Iter: 1874 loss: 1.87703961e-06
Iter: 1875 loss: 1.87696196e-06
Iter: 1876 loss: 1.87634828e-06
Iter: 1877 loss: 1.87491901e-06
Iter: 1878 loss: 1.89083607e-06
Iter: 1879 loss: 1.87474302e-06
Iter: 1880 loss: 1.87303795e-06
Iter: 1881 loss: 1.88824708e-06
Iter: 1882 loss: 1.87301907e-06
Iter: 1883 loss: 1.87181638e-06
Iter: 1884 loss: 1.87450735e-06
Iter: 1885 loss: 1.87145929e-06
Iter: 1886 loss: 1.87018725e-06
Iter: 1887 loss: 1.87898581e-06
Iter: 1888 loss: 1.87005878e-06
Iter: 1889 loss: 1.86902059e-06
Iter: 1890 loss: 1.87064961e-06
Iter: 1891 loss: 1.86863861e-06
Iter: 1892 loss: 1.86742056e-06
Iter: 1893 loss: 1.87282012e-06
Iter: 1894 loss: 1.86719967e-06
Iter: 1895 loss: 1.86642e-06
Iter: 1896 loss: 1.86511807e-06
Iter: 1897 loss: 1.86510363e-06
Iter: 1898 loss: 1.86353441e-06
Iter: 1899 loss: 1.87190381e-06
Iter: 1900 loss: 1.86326088e-06
Iter: 1901 loss: 1.86222371e-06
Iter: 1902 loss: 1.86678187e-06
Iter: 1903 loss: 1.86197792e-06
Iter: 1904 loss: 1.8611164e-06
Iter: 1905 loss: 1.8647645e-06
Iter: 1906 loss: 1.86085094e-06
Iter: 1907 loss: 1.86063608e-06
Iter: 1908 loss: 1.86041393e-06
Iter: 1909 loss: 1.86027444e-06
Iter: 1910 loss: 1.85945555e-06
Iter: 1911 loss: 1.85959095e-06
Iter: 1912 loss: 1.8587325e-06
Iter: 1913 loss: 1.8576352e-06
Iter: 1914 loss: 1.86185957e-06
Iter: 1915 loss: 1.85733529e-06
Iter: 1916 loss: 1.85630802e-06
Iter: 1917 loss: 1.85765111e-06
Iter: 1918 loss: 1.85567399e-06
Iter: 1919 loss: 1.85478552e-06
Iter: 1920 loss: 1.86783143e-06
Iter: 1921 loss: 1.85475938e-06
Iter: 1922 loss: 1.85400131e-06
Iter: 1923 loss: 1.85761508e-06
Iter: 1924 loss: 1.85386546e-06
Iter: 1925 loss: 1.85311615e-06
Iter: 1926 loss: 1.85452882e-06
Iter: 1927 loss: 1.85283943e-06
Iter: 1928 loss: 1.85198e-06
Iter: 1929 loss: 1.85244983e-06
Iter: 1930 loss: 1.85137208e-06
Iter: 1931 loss: 1.85035481e-06
Iter: 1932 loss: 1.84979444e-06
Iter: 1933 loss: 1.84935914e-06
Iter: 1934 loss: 1.84775092e-06
Iter: 1935 loss: 1.85380259e-06
Iter: 1936 loss: 1.8473238e-06
Iter: 1937 loss: 1.84599526e-06
Iter: 1938 loss: 1.85045485e-06
Iter: 1939 loss: 1.8456401e-06
Iter: 1940 loss: 1.84585224e-06
Iter: 1941 loss: 1.84501755e-06
Iter: 1942 loss: 1.84452858e-06
Iter: 1943 loss: 1.84339808e-06
Iter: 1944 loss: 1.86097213e-06
Iter: 1945 loss: 1.84337478e-06
Iter: 1946 loss: 1.8424364e-06
Iter: 1947 loss: 1.84186911e-06
Iter: 1948 loss: 1.8415484e-06
Iter: 1949 loss: 1.84047667e-06
Iter: 1950 loss: 1.84229555e-06
Iter: 1951 loss: 1.83996735e-06
Iter: 1952 loss: 1.83863563e-06
Iter: 1953 loss: 1.84250371e-06
Iter: 1954 loss: 1.83829434e-06
Iter: 1955 loss: 1.83697796e-06
Iter: 1956 loss: 1.84346345e-06
Iter: 1957 loss: 1.83681391e-06
Iter: 1958 loss: 1.83592988e-06
Iter: 1959 loss: 1.84713213e-06
Iter: 1960 loss: 1.83590646e-06
Iter: 1961 loss: 1.83521684e-06
Iter: 1962 loss: 1.83538134e-06
Iter: 1963 loss: 1.8347572e-06
Iter: 1964 loss: 1.8337214e-06
Iter: 1965 loss: 1.83438203e-06
Iter: 1966 loss: 1.83312557e-06
Iter: 1967 loss: 1.83223278e-06
Iter: 1968 loss: 1.83272152e-06
Iter: 1969 loss: 1.83161455e-06
Iter: 1970 loss: 1.83054226e-06
Iter: 1971 loss: 1.83327552e-06
Iter: 1972 loss: 1.83002498e-06
Iter: 1973 loss: 1.82900465e-06
Iter: 1974 loss: 1.83480824e-06
Iter: 1975 loss: 1.82887027e-06
Iter: 1976 loss: 1.82873612e-06
Iter: 1977 loss: 1.82838744e-06
Iter: 1978 loss: 1.82806639e-06
Iter: 1979 loss: 1.82720044e-06
Iter: 1980 loss: 1.83052975e-06
Iter: 1981 loss: 1.82685335e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.8
+ date
Wed Oct 21 16:06:35 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.4/300_300_300_1 --function f1 --psi 2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2c86158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2d78378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2d66730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2da4e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2ce8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2c5a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2bff8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2bd6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2bd6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2bff158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2b96598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2b40c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2b40a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2ae47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2a978c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2a7ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2a7b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2af8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59a2a7ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5981526e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59815d6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59815d6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5981598158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59814b12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59814b1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59814f6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c542840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c53f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c566378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c573ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c48f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c4ae0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c4ae1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c4bd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c4ae598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f595c4e7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.09092356
test_loss: 0.093454376
train_loss: 0.063582286
test_loss: 0.06573653
train_loss: 0.049567237
test_loss: 0.045977637
train_loss: 0.032646436
test_loss: 0.035093687
train_loss: 0.027755015
test_loss: 0.028708005
train_loss: 0.029307928
test_loss: 0.026419034
train_loss: 0.021827437
test_loss: 0.02399983
train_loss: 0.01828951
test_loss: 0.022249559
train_loss: 0.019717518
test_loss: 0.021101546
train_loss: 0.016770722
test_loss: 0.0206972
train_loss: 0.017292341
test_loss: 0.021182323
train_loss: 0.018520253
test_loss: 0.019608982
train_loss: 0.015831517
test_loss: 0.01936955
train_loss: 0.01634121
test_loss: 0.018879412
train_loss: 0.014525766
test_loss: 0.017846595
train_loss: 0.013541677
test_loss: 0.017446717
train_loss: 0.013299365
test_loss: 0.017016765
train_loss: 0.012909753
test_loss: 0.017211232
train_loss: 0.013219612
test_loss: 0.016837694
train_loss: 0.0129676685
test_loss: 0.01706086
train_loss: 0.015150642
test_loss: 0.01767478
train_loss: 0.011580296
test_loss: 0.015754232
train_loss: 0.013323015
test_loss: 0.016564295
train_loss: 0.0130374525
test_loss: 0.016904302
train_loss: 0.011601906
test_loss: 0.015180744
train_loss: 0.010394508
test_loss: 0.016132576
train_loss: 0.011660874
test_loss: 0.015380561
train_loss: 0.012430532
test_loss: 0.015002301
train_loss: 0.011789921
test_loss: 0.016147036
train_loss: 0.011256478
test_loss: 0.015035651
train_loss: 0.012605592
test_loss: 0.015668433
train_loss: 0.011701867
test_loss: 0.015001551
train_loss: 0.01152214
test_loss: 0.014812396
train_loss: 0.011127097
test_loss: 0.014796646
train_loss: 0.013735676
test_loss: 0.015070031
train_loss: 0.011129
test_loss: 0.014284627
train_loss: 0.010059381
test_loss: 0.01434255
train_loss: 0.010408834
test_loss: 0.014234488
train_loss: 0.010621607
test_loss: 0.013924876
train_loss: 0.010960684
test_loss: 0.014198928
train_loss: 0.0104145985
test_loss: 0.0141846435
train_loss: 0.010473836
test_loss: 0.0141779715
train_loss: 0.010346815
test_loss: 0.013934827
train_loss: 0.010887913
test_loss: 0.014163618
train_loss: 0.011122669
test_loss: 0.014100659
train_loss: 0.010336155
test_loss: 0.013463661
train_loss: 0.011422181
test_loss: 0.01408596
train_loss: 0.010435779
test_loss: 0.01352826
train_loss: 0.010082757
test_loss: 0.014325692
train_loss: 0.010183363
test_loss: 0.013709809
train_loss: 0.010069266
test_loss: 0.013966057
train_loss: 0.010680454
test_loss: 0.014241786
train_loss: 0.010969671
test_loss: 0.013834231
train_loss: 0.0094631715
test_loss: 0.013829251
train_loss: 0.009733919
test_loss: 0.013361334
train_loss: 0.009516133
test_loss: 0.013707744
train_loss: 0.010127551
test_loss: 0.013906035
train_loss: 0.008808827
test_loss: 0.013569708
train_loss: 0.010343334
test_loss: 0.013939133
train_loss: 0.0088216495
test_loss: 0.013416298
train_loss: 0.009185628
test_loss: 0.013399385
train_loss: 0.009956274
test_loss: 0.013123985
train_loss: 0.0098444205
test_loss: 0.013470769
train_loss: 0.009400796
test_loss: 0.0133425165
train_loss: 0.009887355
test_loss: 0.01333425
train_loss: 0.010662034
test_loss: 0.0129581075
train_loss: 0.009084679
test_loss: 0.013039146
train_loss: 0.008811977
test_loss: 0.012944155
train_loss: 0.009067698
test_loss: 0.013296364
train_loss: 0.009025534
test_loss: 0.013237674
train_loss: 0.009733262
test_loss: 0.012919666
train_loss: 0.009988732
test_loss: 0.0139548555
train_loss: 0.008150784
test_loss: 0.012823888
train_loss: 0.0086752
test_loss: 0.01337588
train_loss: 0.009173257
test_loss: 0.013409728
train_loss: 0.0083666295
test_loss: 0.013255952
train_loss: 0.008321119
test_loss: 0.012907137
train_loss: 0.008889096
test_loss: 0.013265247
train_loss: 0.0093857795
test_loss: 0.013003782
train_loss: 0.009076161
test_loss: 0.012953694
train_loss: 0.00894598
test_loss: 0.012972529
train_loss: 0.009105023
test_loss: 0.012898393
train_loss: 0.0086817555
test_loss: 0.01270861
train_loss: 0.009561336
test_loss: 0.01306009
train_loss: 0.008862052
test_loss: 0.012409333
train_loss: 0.010003216
test_loss: 0.01344894
train_loss: 0.010178027
test_loss: 0.013018241
train_loss: 0.008482389
test_loss: 0.012651794
train_loss: 0.008314181
test_loss: 0.012547083
train_loss: 0.008997155
test_loss: 0.013041084
train_loss: 0.008878665
test_loss: 0.012410593
train_loss: 0.009005962
test_loss: 0.012437348
train_loss: 0.008944687
test_loss: 0.013060775
train_loss: 0.008836942
test_loss: 0.012614601
train_loss: 0.008285083
test_loss: 0.012903976
train_loss: 0.008143988
test_loss: 0.011982421
train_loss: 0.009263277
test_loss: 0.012309852
train_loss: 0.008030586
test_loss: 0.0118539855
train_loss: 0.008737094
test_loss: 0.012988765
train_loss: 0.008419467
test_loss: 0.012414382
train_loss: 0.009193732
test_loss: 0.012337706
train_loss: 0.008457613
test_loss: 0.012056292
train_loss: 0.0098695215
test_loss: 0.012399586
train_loss: 0.007610406
test_loss: 0.011829827
train_loss: 0.008393269
test_loss: 0.012062403
train_loss: 0.007986017
test_loss: 0.01239631
train_loss: 0.008112793
test_loss: 0.0122600645
train_loss: 0.008216977
test_loss: 0.011925038
train_loss: 0.009233394
test_loss: 0.01232722
train_loss: 0.009024682
test_loss: 0.013134179
train_loss: 0.01080247
test_loss: 0.012631973
train_loss: 0.0077900616
test_loss: 0.012385494
train_loss: 0.008719879
test_loss: 0.01230253
train_loss: 0.008729195
test_loss: 0.012264114
train_loss: 0.010005612
test_loss: 0.01233464
train_loss: 0.007981571
test_loss: 0.012251405
train_loss: 0.008809094
test_loss: 0.011744581
train_loss: 0.008340593
test_loss: 0.012163451
train_loss: 0.0077817906
test_loss: 0.011768749
train_loss: 0.007767717
test_loss: 0.011866685
train_loss: 0.008665965
test_loss: 0.0120541835
train_loss: 0.008130801
test_loss: 0.011931129
train_loss: 0.008367823
test_loss: 0.012192765
train_loss: 0.0077992287
test_loss: 0.012149716
train_loss: 0.0068999166
test_loss: 0.01190872
train_loss: 0.008321501
test_loss: 0.01175522
train_loss: 0.007965517
test_loss: 0.012348374
train_loss: 0.0070124054
test_loss: 0.011686973
train_loss: 0.008665639
test_loss: 0.01235987
train_loss: 0.009007136
test_loss: 0.012001896
train_loss: 0.007544595
test_loss: 0.011784122
train_loss: 0.007424775
test_loss: 0.011265713
train_loss: 0.007095781
test_loss: 0.011627948
train_loss: 0.00649348
test_loss: 0.011835934
train_loss: 0.007871323
test_loss: 0.01184322
train_loss: 0.00793656
test_loss: 0.012034307
train_loss: 0.0072282543
test_loss: 0.011913502
train_loss: 0.00817767
test_loss: 0.011450565
train_loss: 0.006893929
test_loss: 0.011419978
train_loss: 0.009021314
test_loss: 0.012329729
train_loss: 0.007443689
test_loss: 0.011744961
train_loss: 0.007726733
test_loss: 0.011951441
train_loss: 0.0071946946
test_loss: 0.011249165
train_loss: 0.007179492
test_loss: 0.011883246
train_loss: 0.0065671923
test_loss: 0.011179425
train_loss: 0.008370989
test_loss: 0.011773726
train_loss: 0.008251419
test_loss: 0.01206473
train_loss: 0.008304054
test_loss: 0.012319978
train_loss: 0.007999456
test_loss: 0.011835239
train_loss: 0.008437026
test_loss: 0.01151287
train_loss: 0.0073685125
test_loss: 0.011318159
train_loss: 0.0085616335
test_loss: 0.012110171
train_loss: 0.009659786
test_loss: 0.012733443
train_loss: 0.007559644
test_loss: 0.011885969
train_loss: 0.0077833794
test_loss: 0.011647118
train_loss: 0.007471552
test_loss: 0.011595999
train_loss: 0.0076341573
test_loss: 0.011946326
train_loss: 0.0073206956
test_loss: 0.011753995
train_loss: 0.007306543
test_loss: 0.011718452
train_loss: 0.007885166
test_loss: 0.011614352
train_loss: 0.008002389
test_loss: 0.011745146
train_loss: 0.00722096
test_loss: 0.011634107
train_loss: 0.007536947
test_loss: 0.011834632
train_loss: 0.0076384144
test_loss: 0.01167603
train_loss: 0.0069464315
test_loss: 0.011199682
train_loss: 0.0075090337
test_loss: 0.011460356
train_loss: 0.0071875523
test_loss: 0.011438577
train_loss: 0.008406922
test_loss: 0.011802932
train_loss: 0.0066355504
test_loss: 0.0113767525
train_loss: 0.0075696525
test_loss: 0.011428648
train_loss: 0.007023352
test_loss: 0.011460504
train_loss: 0.0073824897
test_loss: 0.011273061
train_loss: 0.0073625795
test_loss: 0.011602665
train_loss: 0.0068531176
test_loss: 0.011677752
train_loss: 0.008346093
test_loss: 0.011216542
train_loss: 0.0063836942
test_loss: 0.010965246
train_loss: 0.0067257513
test_loss: 0.011061311
train_loss: 0.006645728
test_loss: 0.011675402
train_loss: 0.007865876
test_loss: 0.012148995
train_loss: 0.0072003887
test_loss: 0.011671491
train_loss: 0.0077742413
test_loss: 0.011166373
train_loss: 0.00822118
test_loss: 0.011716839
train_loss: 0.0074110436
test_loss: 0.011629145
train_loss: 0.0085148895
test_loss: 0.011654203
train_loss: 0.007831533
test_loss: 0.011569737
train_loss: 0.007604019
test_loss: 0.011393892
train_loss: 0.0065489956
test_loss: 0.011601794
train_loss: 0.0068263584
test_loss: 0.0113093
train_loss: 0.007567262
test_loss: 0.011729621
train_loss: 0.007887483
test_loss: 0.01133576
train_loss: 0.0072610015
test_loss: 0.011393277
train_loss: 0.008598965
test_loss: 0.011217457
train_loss: 0.008048711
test_loss: 0.011287369
train_loss: 0.008071899
test_loss: 0.011263902
train_loss: 0.0074265106
test_loss: 0.011462653
train_loss: 0.007084706
test_loss: 0.011480952
train_loss: 0.0075770654
test_loss: 0.011355075
train_loss: 0.007449846
test_loss: 0.011345672
train_loss: 0.006998594
test_loss: 0.011262804
train_loss: 0.0068090367
test_loss: 0.011289879
train_loss: 0.006836644
test_loss: 0.01118241
train_loss: 0.0071726614
test_loss: 0.011178742
train_loss: 0.007242892
test_loss: 0.011135779
train_loss: 0.007227771
test_loss: 0.011468687
train_loss: 0.006675857
test_loss: 0.011349629
train_loss: 0.0069897114
test_loss: 0.011324209
train_loss: 0.007868795
test_loss: 0.011605011
train_loss: 0.0066918996
test_loss: 0.010737421
train_loss: 0.00722605
test_loss: 0.011326125
train_loss: 0.006654208
test_loss: 0.011076044
train_loss: 0.0063342857
test_loss: 0.010657781
train_loss: 0.0065298574
test_loss: 0.0112727145
train_loss: 0.006338176
test_loss: 0.011383278
train_loss: 0.007809355
test_loss: 0.01134164
train_loss: 0.0067404853
test_loss: 0.011396349
train_loss: 0.005991209
test_loss: 0.01086583
train_loss: 0.0071901944
test_loss: 0.010949481
train_loss: 0.0065817633
test_loss: 0.011000753
train_loss: 0.0069036153
test_loss: 0.011073476
train_loss: 0.007148966
test_loss: 0.011198227
train_loss: 0.006571419
test_loss: 0.01094868
train_loss: 0.007009751
test_loss: 0.011473866
train_loss: 0.0068734176
test_loss: 0.01143864
train_loss: 0.007286758
test_loss: 0.010889631
train_loss: 0.0068447357
test_loss: 0.011073458
train_loss: 0.0075722504
test_loss: 0.011038786
train_loss: 0.0070660533
test_loss: 0.010760162
train_loss: 0.0063919527
test_loss: 0.01109986
train_loss: 0.0063955886
test_loss: 0.011222467
train_loss: 0.0062821712
test_loss: 0.010725094
train_loss: 0.0069327536
test_loss: 0.010935944
train_loss: 0.005925286
test_loss: 0.010625907
train_loss: 0.0067725363
test_loss: 0.010431871
train_loss: 0.0063504972
test_loss: 0.010599273
train_loss: 0.0068373447
test_loss: 0.0106523
train_loss: 0.0067005013
test_loss: 0.011133787
train_loss: 0.0065829647
test_loss: 0.010743076
train_loss: 0.006883383
test_loss: 0.011115104
train_loss: 0.006140154
test_loss: 0.010487059
train_loss: 0.008021463
test_loss: 0.010952483
train_loss: 0.00669361
test_loss: 0.010569129
train_loss: 0.006960901
test_loss: 0.010974043
train_loss: 0.0072850175
test_loss: 0.011082293
train_loss: 0.006745803
test_loss: 0.010937441
train_loss: 0.005940982
test_loss: 0.01052437
train_loss: 0.0060610836
test_loss: 0.010289578
train_loss: 0.0061546476
test_loss: 0.0107629625
train_loss: 0.006280426
test_loss: 0.010877664
train_loss: 0.0065978062
test_loss: 0.011301071
train_loss: 0.006683457
test_loss: 0.010780499
train_loss: 0.006028262
test_loss: 0.010832798
train_loss: 0.0070960233
test_loss: 0.01101247
train_loss: 0.006203418
test_loss: 0.010570513
train_loss: 0.006518551
test_loss: 0.010589034
train_loss: 0.0075596976
test_loss: 0.0105519015
train_loss: 0.006261404
test_loss: 0.010539939
train_loss: 0.0072911945
test_loss: 0.01104164
train_loss: 0.006566045
test_loss: 0.010802383
train_loss: 0.0060276603
test_loss: 0.01108029
train_loss: 0.006326678
test_loss: 0.0107113905
train_loss: 0.0071152174
test_loss: 0.011674377
train_loss: 0.0062887063
test_loss: 0.010738501
train_loss: 0.0064786198
test_loss: 0.010518308
train_loss: 0.00697889
test_loss: 0.011002879
train_loss: 0.006425295
test_loss: 0.010878599
train_loss: 0.006786711
test_loss: 0.011017246
train_loss: 0.007124025
test_loss: 0.011198936
train_loss: 0.0065764505
test_loss: 0.010958921
train_loss: 0.0059404527
test_loss: 0.011172883
train_loss: 0.006395628
test_loss: 0.011013303
train_loss: 0.008218344
test_loss: 0.011511992
train_loss: 0.007115945
test_loss: 0.010629048
train_loss: 0.0067157373
test_loss: 0.010527385
train_loss: 0.0070291026
test_loss: 0.011194505
train_loss: 0.0068922504
test_loss: 0.010710853
train_loss: 0.0065281764
test_loss: 0.010634798
train_loss: 0.007064191
test_loss: 0.010644262
train_loss: 0.006582196
test_loss: 0.010847642
train_loss: 0.006213859
test_loss: 0.01092863
train_loss: 0.006011315
test_loss: 0.010611455
train_loss: 0.0061543193
test_loss: 0.010511891
train_loss: 0.0068562524
test_loss: 0.01072309
train_loss: 0.006763919
test_loss: 0.011004465
train_loss: 0.00597153
test_loss: 0.011021348
train_loss: 0.006302661
test_loss: 0.010703854
train_loss: 0.0070099114
test_loss: 0.010702732
train_loss: 0.0073460448
test_loss: 0.011027928
train_loss: 0.0068121403
test_loss: 0.011053091
train_loss: 0.006322912
test_loss: 0.010946967
train_loss: 0.006810845
test_loss: 0.011158605
train_loss: 0.007066181
test_loss: 0.011019562
train_loss: 0.007910524
test_loss: 0.011109933
train_loss: 0.0062059276
test_loss: 0.010726262
train_loss: 0.006277057
test_loss: 0.010577975
train_loss: 0.0065497044
test_loss: 0.010875226
train_loss: 0.006641336
test_loss: 0.010463711
train_loss: 0.0065695294
test_loss: 0.010884876
train_loss: 0.006226492
test_loss: 0.010614706
train_loss: 0.006370808
test_loss: 0.010864226
train_loss: 0.006335345
test_loss: 0.010610559
train_loss: 0.006536944
test_loss: 0.010463309
train_loss: 0.0070213107
test_loss: 0.010665048
train_loss: 0.006123744
test_loss: 0.010630251
train_loss: 0.006857565
test_loss: 0.011099223
train_loss: 0.00691262
test_loss: 0.010683168
train_loss: 0.0064481352
test_loss: 0.0104413275
train_loss: 0.00741922
test_loss: 0.011402684
train_loss: 0.0067461287
test_loss: 0.010897547
train_loss: 0.005689393
test_loss: 0.010450847
train_loss: 0.0061150454
test_loss: 0.01069793
train_loss: 0.0065072095
test_loss: 0.011072586
train_loss: 0.006292442
test_loss: 0.010482133
train_loss: 0.0060922774
test_loss: 0.010478146
train_loss: 0.0070412364
test_loss: 0.010525952
train_loss: 0.006568567
test_loss: 0.011051298
train_loss: 0.006381816
test_loss: 0.011002667
train_loss: 0.006444135
test_loss: 0.010383432
train_loss: 0.0063489014
test_loss: 0.010474703
train_loss: 0.006365806
test_loss: 0.010863928
train_loss: 0.00834308
test_loss: 0.01055235
train_loss: 0.006195953
test_loss: 0.010610223
train_loss: 0.0070354426
test_loss: 0.010794059
train_loss: 0.0065486757
test_loss: 0.010395575
train_loss: 0.005682331
test_loss: 0.010283403
train_loss: 0.0061903736
test_loss: 0.010523902
train_loss: 0.0065746177
test_loss: 0.010824573
train_loss: 0.005900937
test_loss: 0.010357276
train_loss: 0.0056519425
test_loss: 0.010254869
train_loss: 0.0053804647
test_loss: 0.010129017
train_loss: 0.006330888
test_loss: 0.010214895
train_loss: 0.0073921904
test_loss: 0.010731114
train_loss: 0.0063223317
test_loss: 0.01030883
train_loss: 0.006241261
test_loss: 0.010238705
train_loss: 0.005952987
test_loss: 0.010281826
train_loss: 0.006366486
test_loss: 0.010343859
train_loss: 0.006207431
test_loss: 0.010707925
train_loss: 0.006738295
test_loss: 0.010915948
train_loss: 0.007198682
test_loss: 0.010687892
train_loss: 0.0065731583
test_loss: 0.01054337
train_loss: 0.006608625
test_loss: 0.010427387
train_loss: 0.0067513697
test_loss: 0.010801446
train_loss: 0.005866646
test_loss: 0.010320118
train_loss: 0.006054567
test_loss: 0.010480536
train_loss: 0.0058812737
test_loss: 0.010308076
train_loss: 0.0058261803
test_loss: 0.010253789
train_loss: 0.005754114
test_loss: 0.010444746
train_loss: 0.0055925185
test_loss: 0.010190431
train_loss: 0.006120245
test_loss:/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
 0.010148579
train_loss: 0.007223737
test_loss: 0.010141746
train_loss: 0.00575356
test_loss: 0.009876452
train_loss: 0.005953461
test_loss: 0.011167877
train_loss: 0.0067347623
test_loss: 0.01061089
train_loss: 0.0062945
test_loss: 0.010520637
train_loss: 0.0063122977
test_loss: 0.010665147
train_loss: 0.0061161746
test_loss: 0.010436243
train_loss: 0.0054979557
test_loss: 0.01024602
train_loss: 0.0063263923
test_loss: 0.01023224
train_loss: 0.0066572977
test_loss: 0.011126994
train_loss: 0.0058746072
test_loss: 0.010170097
train_loss: 0.006191185
test_loss: 0.010934445
train_loss: 0.006098976
test_loss: 0.010173987
train_loss: 0.0060433205
test_loss: 0.010064332
train_loss: 0.0063370317
test_loss: 0.010270166
train_loss: 0.0061515365
test_loss: 0.010177994
train_loss: 0.0059174635
test_loss: 0.010526502
train_loss: 0.0065654074
test_loss: 0.010458704
train_loss: 0.0082671065
test_loss: 0.010519263
train_loss: 0.0055116564
test_loss: 0.010236942
train_loss: 0.0069421916
test_loss: 0.010859049
train_loss: 0.006991161
test_loss: 0.010464154
train_loss: 0.0063085645
test_loss: 0.010153057
train_loss: 0.006215699
test_loss: 0.010266688
train_loss: 0.006120411
test_loss: 0.010193638
train_loss: 0.006714035
test_loss: 0.010533477
train_loss: 0.008237298
test_loss: 0.010566659
train_loss: 0.0061553307
test_loss: 0.010219166
train_loss: 0.0063338554
test_loss: 0.010125051
train_loss: 0.0061531747
test_loss: 0.010381152
train_loss: 0.007625049
test_loss: 0.010438532
train_loss: 0.006038606
test_loss: 0.010294823
train_loss: 0.0063397125
test_loss: 0.010643704
train_loss: 0.0063884654
test_loss: 0.010727141
train_loss: 0.0062610595
test_loss: 0.010632383
train_loss: 0.0057867244
test_loss: 0.010409277
train_loss: 0.006577989
test_loss: 0.010672992
train_loss: 0.0061982973
test_loss: 0.010275437
train_loss: 0.0063595623
test_loss: 0.010324455
train_loss: 0.005659163
test_loss: 0.010144051
train_loss: 0.006561636
test_loss: 0.010532215
train_loss: 0.0062885853
test_loss: 0.010227119
train_loss: 0.006153988
test_loss: 0.010807553
train_loss: 0.005706517
test_loss: 0.010292222
train_loss: 0.005923121
test_loss: 0.01022154
train_loss: 0.005570853
test_loss: 0.010503344
train_loss: 0.005489488
test_loss: 0.01017107
train_loss: 0.0062556816
test_loss: 0.010052299
train_loss: 0.0053590136
test_loss: 0.00999194
train_loss: 0.005862766
test_loss: 0.0101810945
train_loss: 0.0060591623
test_loss: 0.010203209
train_loss: 0.0057805157
test_loss: 0.010367822
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98913aeea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891371048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891371a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891343ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f989122cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f989122cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98911ddd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98911e9048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98911b8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98911b8158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891119d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891169510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98911b89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98910e2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98910ae950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98910acc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891052620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98910686a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891018c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891045488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9891045510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9890fe0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98542ad950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98542b1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98542b1bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9854285510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98542418c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9854264840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98541f0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f985421a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f98541c6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f982c7a62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f982c7a61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f982c7b1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f982c78a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f982c791d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.30077154e-05
Iter: 2 loss: 6.58879726e-05
Iter: 3 loss: 6.244627e-05
Iter: 4 loss: 5.61704e-05
Iter: 5 loss: 7.97096727e-05
Iter: 6 loss: 5.46433257e-05
Iter: 7 loss: 5.11497565e-05
Iter: 8 loss: 4.48394348e-05
Iter: 9 loss: 0.000195867833
Iter: 10 loss: 4.48376268e-05
Iter: 11 loss: 3.90025562e-05
Iter: 12 loss: 8.25577372e-05
Iter: 13 loss: 3.85459716e-05
Iter: 14 loss: 3.49927e-05
Iter: 15 loss: 3.50890623e-05
Iter: 16 loss: 3.21791122e-05
Iter: 17 loss: 2.85234091e-05
Iter: 18 loss: 7.58972383e-05
Iter: 19 loss: 2.84928337e-05
Iter: 20 loss: 2.6806254e-05
Iter: 21 loss: 2.89441014e-05
Iter: 22 loss: 2.59401e-05
Iter: 23 loss: 2.37129043e-05
Iter: 24 loss: 3.30207113e-05
Iter: 25 loss: 2.32369257e-05
Iter: 26 loss: 2.19714493e-05
Iter: 27 loss: 2.69519151e-05
Iter: 28 loss: 2.16822918e-05
Iter: 29 loss: 2.05472788e-05
Iter: 30 loss: 2.09579157e-05
Iter: 31 loss: 1.97488334e-05
Iter: 32 loss: 1.86149628e-05
Iter: 33 loss: 2.19665199e-05
Iter: 34 loss: 1.82669392e-05
Iter: 35 loss: 1.72087803e-05
Iter: 36 loss: 2.49608238e-05
Iter: 37 loss: 1.71207212e-05
Iter: 38 loss: 1.64441699e-05
Iter: 39 loss: 1.68118586e-05
Iter: 40 loss: 1.59997435e-05
Iter: 41 loss: 1.56634778e-05
Iter: 42 loss: 1.55164125e-05
Iter: 43 loss: 1.50341293e-05
Iter: 44 loss: 1.57103677e-05
Iter: 45 loss: 1.4796442e-05
Iter: 46 loss: 1.45072609e-05
Iter: 47 loss: 1.39617787e-05
Iter: 48 loss: 2.58508589e-05
Iter: 49 loss: 1.39595832e-05
Iter: 50 loss: 1.33928661e-05
Iter: 51 loss: 1.92472016e-05
Iter: 52 loss: 1.33763569e-05
Iter: 53 loss: 1.30832786e-05
Iter: 54 loss: 1.38364785e-05
Iter: 55 loss: 1.29826058e-05
Iter: 56 loss: 1.25865363e-05
Iter: 57 loss: 1.24120052e-05
Iter: 58 loss: 1.22110123e-05
Iter: 59 loss: 1.1815092e-05
Iter: 60 loss: 1.60243599e-05
Iter: 61 loss: 1.1805022e-05
Iter: 62 loss: 1.15209914e-05
Iter: 63 loss: 1.17826112e-05
Iter: 64 loss: 1.13569595e-05
Iter: 65 loss: 1.0981259e-05
Iter: 66 loss: 1.3125612e-05
Iter: 67 loss: 1.09296871e-05
Iter: 68 loss: 1.07315464e-05
Iter: 69 loss: 1.0639741e-05
Iter: 70 loss: 1.05417603e-05
Iter: 71 loss: 1.02801541e-05
Iter: 72 loss: 1.2608918e-05
Iter: 73 loss: 1.02669765e-05
Iter: 74 loss: 1.00846037e-05
Iter: 75 loss: 9.9441877e-06
Iter: 76 loss: 9.88570355e-06
Iter: 77 loss: 1.00441957e-05
Iter: 78 loss: 9.78638e-06
Iter: 79 loss: 9.70120072e-06
Iter: 80 loss: 9.68029781e-06
Iter: 81 loss: 9.62594913e-06
Iter: 82 loss: 9.52412e-06
Iter: 83 loss: 9.30124952e-06
Iter: 84 loss: 1.25843471e-05
Iter: 85 loss: 9.29152884e-06
Iter: 86 loss: 9.11140887e-06
Iter: 87 loss: 1.16877891e-05
Iter: 88 loss: 9.11114694e-06
Iter: 89 loss: 8.98533744e-06
Iter: 90 loss: 9.05547131e-06
Iter: 91 loss: 8.90308911e-06
Iter: 92 loss: 8.73703721e-06
Iter: 93 loss: 9.99559688e-06
Iter: 94 loss: 8.72443252e-06
Iter: 95 loss: 8.63725109e-06
Iter: 96 loss: 8.66598e-06
Iter: 97 loss: 8.57592386e-06
Iter: 98 loss: 8.44613169e-06
Iter: 99 loss: 8.95153789e-06
Iter: 100 loss: 8.41610745e-06
Iter: 101 loss: 8.315169e-06
Iter: 102 loss: 9.10151903e-06
Iter: 103 loss: 8.30798308e-06
Iter: 104 loss: 8.232244e-06
Iter: 105 loss: 8.11284553e-06
Iter: 106 loss: 8.11149766e-06
Iter: 107 loss: 7.99824647e-06
Iter: 108 loss: 9.40882546e-06
Iter: 109 loss: 7.99728605e-06
Iter: 110 loss: 7.91568527e-06
Iter: 111 loss: 8.03300463e-06
Iter: 112 loss: 7.87585304e-06
Iter: 113 loss: 7.87825e-06
Iter: 114 loss: 7.83717e-06
Iter: 115 loss: 7.80664323e-06
Iter: 116 loss: 7.75250646e-06
Iter: 117 loss: 7.75262197e-06
Iter: 118 loss: 7.69316375e-06
Iter: 119 loss: 7.59761588e-06
Iter: 120 loss: 7.59674367e-06
Iter: 121 loss: 7.49556511e-06
Iter: 122 loss: 7.95738652e-06
Iter: 123 loss: 7.47608919e-06
Iter: 124 loss: 7.37288519e-06
Iter: 125 loss: 7.97729263e-06
Iter: 126 loss: 7.35944514e-06
Iter: 127 loss: 7.3047795e-06
Iter: 128 loss: 7.88439411e-06
Iter: 129 loss: 7.30340889e-06
Iter: 130 loss: 7.26489498e-06
Iter: 131 loss: 7.18067713e-06
Iter: 132 loss: 8.44745591e-06
Iter: 133 loss: 7.17715875e-06
Iter: 134 loss: 7.12691053e-06
Iter: 135 loss: 7.12143037e-06
Iter: 136 loss: 7.07672916e-06
Iter: 137 loss: 7.0500314e-06
Iter: 138 loss: 7.03171645e-06
Iter: 139 loss: 6.95345261e-06
Iter: 140 loss: 7.27776842e-06
Iter: 141 loss: 6.93648326e-06
Iter: 142 loss: 6.89553872e-06
Iter: 143 loss: 7.01732824e-06
Iter: 144 loss: 6.88327145e-06
Iter: 145 loss: 6.82837208e-06
Iter: 146 loss: 6.83382e-06
Iter: 147 loss: 6.78582501e-06
Iter: 148 loss: 6.8465597e-06
Iter: 149 loss: 6.76693162e-06
Iter: 150 loss: 6.75369074e-06
Iter: 151 loss: 6.71576117e-06
Iter: 152 loss: 6.88334421e-06
Iter: 153 loss: 6.70141435e-06
Iter: 154 loss: 6.65177049e-06
Iter: 155 loss: 6.83197e-06
Iter: 156 loss: 6.63919218e-06
Iter: 157 loss: 6.602535e-06
Iter: 158 loss: 6.5873669e-06
Iter: 159 loss: 6.56837165e-06
Iter: 160 loss: 6.5186714e-06
Iter: 161 loss: 7.15092756e-06
Iter: 162 loss: 6.51830578e-06
Iter: 163 loss: 6.48757577e-06
Iter: 164 loss: 6.47259412e-06
Iter: 165 loss: 6.45772707e-06
Iter: 166 loss: 6.41191946e-06
Iter: 167 loss: 6.85088889e-06
Iter: 168 loss: 6.40999133e-06
Iter: 169 loss: 6.38055053e-06
Iter: 170 loss: 6.48521655e-06
Iter: 171 loss: 6.37299399e-06
Iter: 172 loss: 6.33959644e-06
Iter: 173 loss: 6.32271667e-06
Iter: 174 loss: 6.30693103e-06
Iter: 175 loss: 6.27531426e-06
Iter: 176 loss: 6.5655e-06
Iter: 177 loss: 6.27412e-06
Iter: 178 loss: 6.24455424e-06
Iter: 179 loss: 6.2538179e-06
Iter: 180 loss: 6.22351672e-06
Iter: 181 loss: 6.1845285e-06
Iter: 182 loss: 6.47617344e-06
Iter: 183 loss: 6.18158856e-06
Iter: 184 loss: 6.1765727e-06
Iter: 185 loss: 6.17091337e-06
Iter: 186 loss: 6.16119e-06
Iter: 187 loss: 6.13222801e-06
Iter: 188 loss: 6.23639517e-06
Iter: 189 loss: 6.11952782e-06
Iter: 190 loss: 6.08182518e-06
Iter: 191 loss: 6.19094862e-06
Iter: 192 loss: 6.06998856e-06
Iter: 193 loss: 6.03352237e-06
Iter: 194 loss: 6.0072407e-06
Iter: 195 loss: 5.99492205e-06
Iter: 196 loss: 5.95122583e-06
Iter: 197 loss: 6.1877472e-06
Iter: 198 loss: 5.94450512e-06
Iter: 199 loss: 5.9105264e-06
Iter: 200 loss: 6.31529565e-06
Iter: 201 loss: 5.91002754e-06
Iter: 202 loss: 5.88443754e-06
Iter: 203 loss: 5.89906085e-06
Iter: 204 loss: 5.86731721e-06
Iter: 205 loss: 5.83356905e-06
Iter: 206 loss: 6.07044512e-06
Iter: 207 loss: 5.83051042e-06
Iter: 208 loss: 5.8107189e-06
Iter: 209 loss: 5.86693113e-06
Iter: 210 loss: 5.80433971e-06
Iter: 211 loss: 5.78041181e-06
Iter: 212 loss: 5.76083767e-06
Iter: 213 loss: 5.75354125e-06
Iter: 214 loss: 5.72923182e-06
Iter: 215 loss: 6.04195611e-06
Iter: 216 loss: 5.72894214e-06
Iter: 217 loss: 5.70762222e-06
Iter: 218 loss: 5.75032391e-06
Iter: 219 loss: 5.69870554e-06
Iter: 220 loss: 5.67781626e-06
Iter: 221 loss: 5.67790266e-06
Iter: 222 loss: 5.66689505e-06
Iter: 223 loss: 5.65744813e-06
Iter: 224 loss: 5.65436676e-06
Iter: 225 loss: 5.64068068e-06
Iter: 226 loss: 5.60876879e-06
Iter: 227 loss: 5.99764189e-06
Iter: 228 loss: 5.60623448e-06
Iter: 229 loss: 5.58678857e-06
Iter: 230 loss: 5.58509419e-06
Iter: 231 loss: 5.57054955e-06
Iter: 232 loss: 5.54143571e-06
Iter: 233 loss: 6.09188919e-06
Iter: 234 loss: 5.54107055e-06
Iter: 235 loss: 5.5193068e-06
Iter: 236 loss: 5.51845733e-06
Iter: 237 loss: 5.49945435e-06
Iter: 238 loss: 5.48806474e-06
Iter: 239 loss: 5.48012122e-06
Iter: 240 loss: 5.46295814e-06
Iter: 241 loss: 5.46243427e-06
Iter: 242 loss: 5.447685e-06
Iter: 243 loss: 5.44294835e-06
Iter: 244 loss: 5.43475653e-06
Iter: 245 loss: 5.41071e-06
Iter: 246 loss: 5.49329525e-06
Iter: 247 loss: 5.40472047e-06
Iter: 248 loss: 5.38796849e-06
Iter: 249 loss: 5.46188539e-06
Iter: 250 loss: 5.38468748e-06
Iter: 251 loss: 5.36998141e-06
Iter: 252 loss: 5.46522142e-06
Iter: 253 loss: 5.36807602e-06
Iter: 254 loss: 5.35175695e-06
Iter: 255 loss: 5.43750184e-06
Iter: 256 loss: 5.34905394e-06
Iter: 257 loss: 5.34262836e-06
Iter: 258 loss: 5.3352187e-06
Iter: 259 loss: 5.33419961e-06
Iter: 260 loss: 5.31972e-06
Iter: 261 loss: 5.3102458e-06
Iter: 262 loss: 5.30473608e-06
Iter: 263 loss: 5.28758119e-06
Iter: 264 loss: 5.35364916e-06
Iter: 265 loss: 5.2835e-06
Iter: 266 loss: 5.2636733e-06
Iter: 267 loss: 5.2726773e-06
Iter: 268 loss: 5.25050382e-06
Iter: 269 loss: 5.23223298e-06
Iter: 270 loss: 5.28742294e-06
Iter: 271 loss: 5.22654591e-06
Iter: 272 loss: 5.20572303e-06
Iter: 273 loss: 5.30817442e-06
Iter: 274 loss: 5.20211779e-06
Iter: 275 loss: 5.1868119e-06
Iter: 276 loss: 5.19840387e-06
Iter: 277 loss: 5.17751641e-06
Iter: 278 loss: 5.15856209e-06
Iter: 279 loss: 5.37387086e-06
Iter: 280 loss: 5.15809597e-06
Iter: 281 loss: 5.14812655e-06
Iter: 282 loss: 5.16554883e-06
Iter: 283 loss: 5.14349222e-06
Iter: 284 loss: 5.13066061e-06
Iter: 285 loss: 5.13807572e-06
Iter: 286 loss: 5.12223596e-06
Iter: 287 loss: 5.11838334e-06
Iter: 288 loss: 5.11475537e-06
Iter: 289 loss: 5.1073016e-06
Iter: 290 loss: 5.09714755e-06
Iter: 291 loss: 5.09653728e-06
Iter: 292 loss: 5.08415451e-06
Iter: 293 loss: 5.07762115e-06
Iter: 294 loss: 5.07177811e-06
Iter: 295 loss: 5.05933167e-06
Iter: 296 loss: 5.19470632e-06
Iter: 297 loss: 5.0589997e-06
Iter: 298 loss: 5.04811305e-06
Iter: 299 loss: 5.03394631e-06
Iter: 300 loss: 5.03305e-06
Iter: 301 loss: 5.01896557e-06
Iter: 302 loss: 5.19498553e-06
Iter: 303 loss: 5.01871455e-06
Iter: 304 loss: 5.00773513e-06
Iter: 305 loss: 4.99144153e-06
Iter: 306 loss: 4.99102771e-06
Iter: 307 loss: 4.97516885e-06
Iter: 308 loss: 5.17051103e-06
Iter: 309 loss: 4.9750779e-06
Iter: 310 loss: 4.95970335e-06
Iter: 311 loss: 4.95624863e-06
Iter: 312 loss: 4.94636424e-06
Iter: 313 loss: 4.93449534e-06
Iter: 314 loss: 4.93435073e-06
Iter: 315 loss: 4.92429626e-06
Iter: 316 loss: 4.91441824e-06
Iter: 317 loss: 4.91227729e-06
Iter: 318 loss: 4.89590639e-06
Iter: 319 loss: 5.06759443e-06
Iter: 320 loss: 4.89537615e-06
Iter: 321 loss: 4.89312788e-06
Iter: 322 loss: 4.89118975e-06
Iter: 323 loss: 4.88766591e-06
Iter: 324 loss: 4.87722355e-06
Iter: 325 loss: 4.91758146e-06
Iter: 326 loss: 4.87286e-06
Iter: 327 loss: 4.8591055e-06
Iter: 328 loss: 4.90892398e-06
Iter: 329 loss: 4.8552929e-06
Iter: 330 loss: 4.84549037e-06
Iter: 331 loss: 4.86778936e-06
Iter: 332 loss: 4.84163957e-06
Iter: 333 loss: 4.82670885e-06
Iter: 334 loss: 4.84419661e-06
Iter: 335 loss: 4.81887673e-06
Iter: 336 loss: 4.80810922e-06
Iter: 337 loss: 4.85648616e-06
Iter: 338 loss: 4.80588915e-06
Iter: 339 loss: 4.79455866e-06
Iter: 340 loss: 4.78328684e-06
Iter: 341 loss: 4.78084621e-06
Iter: 342 loss: 4.76862306e-06
Iter: 343 loss: 4.92103482e-06
Iter: 344 loss: 4.76866444e-06
Iter: 345 loss: 4.75684919e-06
Iter: 346 loss: 4.7674157e-06
Iter: 347 loss: 4.7501776e-06
Iter: 348 loss: 4.73997261e-06
Iter: 349 loss: 4.82819405e-06
Iter: 350 loss: 4.73932278e-06
Iter: 351 loss: 4.72997135e-06
Iter: 352 loss: 4.74313765e-06
Iter: 353 loss: 4.72528609e-06
Iter: 354 loss: 4.71752446e-06
Iter: 355 loss: 4.82855239e-06
Iter: 356 loss: 4.71752e-06
Iter: 357 loss: 4.71131762e-06
Iter: 358 loss: 4.75912e-06
Iter: 359 loss: 4.71087833e-06
Iter: 360 loss: 4.70670557e-06
Iter: 361 loss: 4.69550605e-06
Iter: 362 loss: 4.76074274e-06
Iter: 363 loss: 4.69227052e-06
Iter: 364 loss: 4.67837253e-06
Iter: 365 loss: 4.75974184e-06
Iter: 366 loss: 4.6765922e-06
Iter: 367 loss: 4.66657093e-06
Iter: 368 loss: 4.66538495e-06
Iter: 369 loss: 4.65809717e-06
Iter: 370 loss: 4.64724735e-06
Iter: 371 loss: 4.64722143e-06
Iter: 372 loss: 4.6399814e-06
Iter: 373 loss: 4.62839807e-06
Iter: 374 loss: 4.62836397e-06
Iter: 375 loss: 4.61635182e-06
Iter: 376 loss: 4.74859735e-06
Iter: 377 loss: 4.61612e-06
Iter: 378 loss: 4.60773344e-06
Iter: 379 loss: 4.60079355e-06
Iter: 380 loss: 4.59834064e-06
Iter: 381 loss: 4.58441355e-06
Iter: 382 loss: 4.73159434e-06
Iter: 383 loss: 4.58415479e-06
Iter: 384 loss: 4.57594206e-06
Iter: 385 loss: 4.57018632e-06
Iter: 386 loss: 4.56733233e-06
Iter: 387 loss: 4.5621473e-06
Iter: 388 loss: 4.56094e-06
Iter: 389 loss: 4.55607733e-06
Iter: 390 loss: 4.58164777e-06
Iter: 391 loss: 4.55520785e-06
Iter: 392 loss: 4.54904784e-06
Iter: 393 loss: 4.54505698e-06
Iter: 394 loss: 4.54260226e-06
Iter: 395 loss: 4.53490975e-06
Iter: 396 loss: 4.53680377e-06
Iter: 397 loss: 4.52920631e-06
Iter: 398 loss: 4.52098038e-06
Iter: 399 loss: 4.52006771e-06
Iter: 400 loss: 4.51421829e-06
Iter: 401 loss: 4.50232938e-06
Iter: 402 loss: 4.58578143e-06
Iter: 403 loss: 4.50121024e-06
Iter: 404 loss: 4.49362733e-06
Iter: 405 loss: 4.51080632e-06
Iter: 406 loss: 4.49104027e-06
Iter: 407 loss: 4.47960429e-06
Iter: 408 loss: 4.49066647e-06
Iter: 409 loss: 4.47336788e-06
Iter: 410 loss: 4.4656872e-06
Iter: 411 loss: 4.48981609e-06
Iter: 412 loss: 4.46351805e-06
Iter: 413 loss: 4.4547869e-06
Iter: 414 loss: 4.47421962e-06
Iter: 415 loss: 4.45163096e-06
Iter: 416 loss: 4.44397847e-06
Iter: 417 loss: 4.46070499e-06
Iter: 418 loss: 4.44122907e-06
Iter: 419 loss: 4.43060162e-06
Iter: 420 loss: 4.44200577e-06
Iter: 421 loss: 4.42466308e-06
Iter: 422 loss: 4.41788643e-06
Iter: 423 loss: 4.41788e-06
Iter: 424 loss: 4.41121847e-06
Iter: 425 loss: 4.44925217e-06
Iter: 426 loss: 4.41033762e-06
Iter: 427 loss: 4.40558733e-06
Iter: 428 loss: 4.40461554e-06
Iter: 429 loss: 4.40161784e-06
Iter: 430 loss: 4.39675659e-06
Iter: 431 loss: 4.39660198e-06
Iter: 432 loss: 4.39267069e-06
Iter: 433 loss: 4.3841269e-06
Iter: 434 loss: 4.38111601e-06
Iter: 435 loss: 4.37628023e-06
Iter: 436 loss: 4.36655318e-06
Iter: 437 loss: 4.39978794e-06
Iter: 438 loss: 4.36383561e-06
Iter: 439 loss: 4.35376705e-06
Iter: 440 loss: 4.42213104e-06
Iter: 441 loss: 4.35281436e-06
Iter: 442 loss: 4.34591448e-06
Iter: 443 loss: 4.37647168e-06
Iter: 444 loss: 4.34470803e-06
Iter: 445 loss: 4.33787864e-06
Iter: 446 loss: 4.33561354e-06
Iter: 447 loss: 4.33180048e-06
Iter: 448 loss: 4.32379875e-06
Iter: 449 loss: 4.39400264e-06
Iter: 450 loss: 4.32329398e-06
Iter: 451 loss: 4.31660374e-06
Iter: 452 loss: 4.30994442e-06
Iter: 453 loss: 4.30874934e-06
Iter: 454 loss: 4.30009368e-06
Iter: 455 loss: 4.41497286e-06
Iter: 456 loss: 4.30015689e-06
Iter: 457 loss: 4.29484498e-06
Iter: 458 loss: 4.32038269e-06
Iter: 459 loss: 4.29402462e-06
Iter: 460 loss: 4.28644398e-06
Iter: 461 loss: 4.30234559e-06
Iter: 462 loss: 4.28346084e-06
Iter: 463 loss: 4.27936584e-06
Iter: 464 loss: 4.28165686e-06
Iter: 465 loss: 4.27682653e-06
Iter: 466 loss: 4.27129407e-06
Iter: 467 loss: 4.26729639e-06
Iter: 468 loss: 4.2654442e-06
Iter: 469 loss: 4.2555962e-06
Iter: 470 loss: 4.27850318e-06
Iter: 471 loss: 4.25173675e-06
Iter: 472 loss: 4.24457039e-06
Iter: 473 loss: 4.2550987e-06
Iter: 474 loss: 4.2409165e-06
Iter: 475 loss: 4.23307711e-06
Iter: 476 loss: 4.29270904e-06
Iter: 477 loss: 4.23240726e-06
Iter: 478 loss: 4.22695393e-06
Iter: 479 loss: 4.23971278e-06
Iter: 480 loss: 4.22484845e-06
Iter: 481 loss: 4.21699087e-06
Iter: 482 loss: 4.22349649e-06
Iter: 483 loss: 4.21236246e-06
Iter: 484 loss: 4.20613105e-06
Iter: 485 loss: 4.23415668e-06
Iter: 486 loss: 4.20478818e-06
Iter: 487 loss: 4.19745584e-06
Iter: 488 loss: 4.2011261e-06
Iter: 489 loss: 4.1926296e-06
Iter: 490 loss: 4.18487616e-06
Iter: 491 loss: 4.20600145e-06
Iter: 492 loss: 4.18240097e-06
Iter: 493 loss: 4.17781848e-06
Iter: 494 loss: 4.17730098e-06
Iter: 495 loss: 4.17178762e-06
Iter: 496 loss: 4.17585352e-06
Iter: 497 loss: 4.16836156e-06
Iter: 498 loss: 4.16513922e-06
Iter: 499 loss: 4.15902196e-06
Iter: 500 loss: 4.29225474e-06
Iter: 501 loss: 4.1589683e-06
Iter: 502 loss: 4.15148224e-06
Iter: 503 loss: 4.22174526e-06
Iter: 504 loss: 4.15095928e-06
Iter: 505 loss: 4.14662372e-06
Iter: 506 loss: 4.14012538e-06
Iter: 507 loss: 4.13982116e-06
Iter: 508 loss: 4.12998907e-06
Iter: 509 loss: 4.18326636e-06
Iter: 510 loss: 4.12860027e-06
Iter: 511 loss: 4.1224157e-06
Iter: 512 loss: 4.13151793e-06
Iter: 513 loss: 4.11943211e-06
Iter: 514 loss: 4.11169322e-06
Iter: 515 loss: 4.1543658e-06
Iter: 516 loss: 4.11065184e-06
Iter: 517 loss: 4.10537086e-06
Iter: 518 loss: 4.13318867e-06
Iter: 519 loss: 4.10444409e-06
Iter: 520 loss: 4.09897393e-06
Iter: 521 loss: 4.09291852e-06
Iter: 522 loss: 4.09217773e-06
Iter: 523 loss: 4.0852924e-06
Iter: 524 loss: 4.18789159e-06
Iter: 525 loss: 4.08523374e-06
Iter: 526 loss: 4.08007782e-06
Iter: 527 loss: 4.07316293e-06
Iter: 528 loss: 4.07287371e-06
Iter: 529 loss: 4.0773848e-06
Iter: 530 loss: 4.06918116e-06
Iter: 531 loss: 4.06668596e-06
Iter: 532 loss: 4.0623022e-06
Iter: 533 loss: 4.16358e-06
Iter: 534 loss: 4.06220124e-06
Iter: 535 loss: 4.05796573e-06
Iter: 536 loss: 4.05218179e-06
Iter: 537 loss: 4.05197898e-06
Iter: 538 loss: 4.04623188e-06
Iter: 539 loss: 4.04596494e-06
Iter: 540 loss: 4.04209368e-06
Iter: 541 loss: 4.03388e-06
Iter: 542 loss: 4.17885803e-06
Iter: 543 loss: 4.03361719e-06
Iter: 544 loss: 4.02585829e-06
Iter: 545 loss: 4.11148721e-06
Iter: 546 loss: 4.02566684e-06
Iter: 547 loss: 4.01928082e-06
Iter: 548 loss: 4.01855596e-06
Iter: 549 loss: 4.01399075e-06
Iter: 550 loss: 4.00690215e-06
Iter: 551 loss: 4.06512027e-06
Iter: 552 loss: 4.00637964e-06
Iter: 553 loss: 4.00015097e-06
Iter: 554 loss: 4.00932049e-06
Iter: 555 loss: 3.9971269e-06
Iter: 556 loss: 3.99025748e-06
Iter: 557 loss: 4.03877357e-06
Iter: 558 loss: 3.98970042e-06
Iter: 559 loss: 3.98395e-06
Iter: 560 loss: 3.98374914e-06
Iter: 561 loss: 3.97937401e-06
Iter: 562 loss: 3.97430267e-06
Iter: 563 loss: 4.04707316e-06
Iter: 564 loss: 3.97433541e-06
Iter: 565 loss: 3.97226222e-06
Iter: 566 loss: 3.97227632e-06
Iter: 567 loss: 3.97000622e-06
Iter: 568 loss: 3.96397036e-06
Iter: 569 loss: 3.98871452e-06
Iter: 570 loss: 3.96144378e-06
Iter: 571 loss: 3.95537791e-06
Iter: 572 loss: 3.98323937e-06
Iter: 573 loss: 3.954151e-06
Iter: 574 loss: 3.9479537e-06
Iter: 575 loss: 3.94847029e-06
Iter: 576 loss: 3.9432889e-06
Iter: 577 loss: 3.93815526e-06
Iter: 578 loss: 4.00113277e-06
Iter: 579 loss: 3.93806113e-06
Iter: 580 loss: 3.93343362e-06
Iter: 581 loss: 3.93652e-06
Iter: 582 loss: 3.93061146e-06
Iter: 583 loss: 3.92359107e-06
Iter: 584 loss: 3.95670213e-06
Iter: 585 loss: 3.9223105e-06
Iter: 586 loss: 3.91850244e-06
Iter: 587 loss: 3.91637514e-06
Iter: 588 loss: 3.91450476e-06
Iter: 589 loss: 3.90685182e-06
Iter: 590 loss: 3.9365782e-06
Iter: 591 loss: 3.9049728e-06
Iter: 592 loss: 3.89963225e-06
Iter: 593 loss: 3.90992091e-06
Iter: 594 loss: 3.89723e-06
Iter: 595 loss: 3.89086108e-06
Iter: 596 loss: 3.92001311e-06
Iter: 597 loss: 3.8895887e-06
Iter: 598 loss: 3.88461331e-06
Iter: 599 loss: 3.90314472e-06
Iter: 600 loss: 3.88334229e-06
Iter: 601 loss: 3.87974023e-06
Iter: 602 loss: 3.87970704e-06
Iter: 603 loss: 3.87606451e-06
Iter: 604 loss: 3.87532418e-06
Iter: 605 loss: 3.87315367e-06
Iter: 606 loss: 3.86990496e-06
Iter: 607 loss: 3.86384227e-06
Iter: 608 loss: 3.9973379e-06
Iter: 609 loss: 3.86382635e-06
Iter: 610 loss: 3.8571784e-06
Iter: 611 loss: 3.89915294e-06
Iter: 612 loss: 3.85624116e-06
Iter: 613 loss: 3.85151816e-06
Iter: 614 loss: 3.85213e-06
Iter: 615 loss: 3.8478147e-06
Iter: 616 loss: 3.84128316e-06
Iter: 617 loss: 3.8880471e-06
Iter: 618 loss: 3.84078248e-06
Iter: 619 loss: 3.83592078e-06
Iter: 620 loss: 3.84080158e-06
Iter: 621 loss: 3.83301176e-06
Iter: 622 loss: 3.82632288e-06
Iter: 623 loss: 3.86118245e-06
Iter: 624 loss: 3.82526832e-06
Iter: 625 loss: 3.82111148e-06
Iter: 626 loss: 3.84711e-06
Iter: 627 loss: 3.82069811e-06
Iter: 628 loss: 3.81671589e-06
Iter: 629 loss: 3.81117206e-06
Iter: 630 loss: 3.81070868e-06
Iter: 631 loss: 3.8046644e-06
Iter: 632 loss: 3.87180171e-06
Iter: 633 loss: 3.804671e-06
Iter: 634 loss: 3.80054848e-06
Iter: 635 loss: 3.80090364e-06
Iter: 636 loss: 3.79738526e-06
Iter: 637 loss: 3.79659787e-06
Iter: 638 loss: 3.79425842e-06
Iter: 639 loss: 3.79290736e-06
Iter: 640 loss: 3.78896129e-06
Iter: 641 loss: 3.79031894e-06
Iter: 642 loss: 3.78514096e-06
Iter: 643 loss: 3.7797422e-06
Iter: 644 loss: 3.86298143e-06
Iter: 645 loss: 3.77967717e-06
Iter: 646 loss: 3.77565425e-06
Iter: 647 loss: 3.78123241e-06
Iter: 648 loss: 3.77364586e-06
Iter: 649 loss: 3.768532e-06
Iter: 650 loss: 3.77523293e-06
Iter: 651 loss: 3.76592288e-06
Iter: 652 loss: 3.76113394e-06
Iter: 653 loss: 3.78328605e-06
Iter: 654 loss: 3.76027219e-06
Iter: 655 loss: 3.75554509e-06
Iter: 656 loss: 3.76373237e-06
Iter: 657 loss: 3.75349055e-06
Iter: 658 loss: 3.74854926e-06
Iter: 659 loss: 3.74725687e-06
Iter: 660 loss: 3.74428987e-06
Iter: 661 loss: 3.73771104e-06
Iter: 662 loss: 3.80379743e-06
Iter: 663 loss: 3.73751595e-06
Iter: 664 loss: 3.73386524e-06
Iter: 665 loss: 3.7658715e-06
Iter: 666 loss: 3.73368175e-06
Iter: 667 loss: 3.73037687e-06
Iter: 668 loss: 3.7262248e-06
Iter: 669 loss: 3.72597106e-06
Iter: 670 loss: 3.72283853e-06
Iter: 671 loss: 3.72266823e-06
Iter: 672 loss: 3.71920555e-06
Iter: 673 loss: 3.73159605e-06
Iter: 674 loss: 3.71839496e-06
Iter: 675 loss: 3.71606052e-06
Iter: 676 loss: 3.70952057e-06
Iter: 677 loss: 3.74173533e-06
Iter: 678 loss: 3.70737507e-06
Iter: 679 loss: 3.7011871e-06
Iter: 680 loss: 3.75325158e-06
Iter: 681 loss: 3.70096495e-06
Iter: 682 loss: 3.69555846e-06
Iter: 683 loss: 3.7051027e-06
Iter: 684 loss: 3.6932795e-06
Iter: 685 loss: 3.68837959e-06
Iter: 686 loss: 3.72520822e-06
Iter: 687 loss: 3.68796668e-06
Iter: 688 loss: 3.6831334e-06
Iter: 689 loss: 3.68531892e-06
Iter: 690 loss: 3.67978282e-06
Iter: 691 loss: 3.67542498e-06
Iter: 692 loss: 3.70577618e-06
Iter: 693 loss: 3.67489497e-06
Iter: 694 loss: 3.67029384e-06
Iter: 695 loss: 3.67025e-06
Iter: 696 loss: 3.66651238e-06
Iter: 697 loss: 3.66197037e-06
Iter: 698 loss: 3.67199868e-06
Iter: 699 loss: 3.66020618e-06
Iter: 700 loss: 3.65462097e-06
Iter: 701 loss: 3.6802669e-06
Iter: 702 loss: 3.65346136e-06
Iter: 703 loss: 3.6489439e-06
Iter: 704 loss: 3.67994517e-06
Iter: 705 loss: 3.64846778e-06
Iter: 706 loss: 3.64525363e-06
Iter: 707 loss: 3.6925212e-06
Iter: 708 loss: 3.64532684e-06
Iter: 709 loss: 3.64242123e-06
Iter: 710 loss: 3.63930371e-06
Iter: 711 loss: 3.63860045e-06
Iter: 712 loss: 3.63578033e-06
Iter: 713 loss: 3.63756794e-06
Iter: 714 loss: 3.63398726e-06
Iter: 715 loss: 3.63014306e-06
Iter: 716 loss: 3.62410083e-06
Iter: 717 loss: 3.62405672e-06
Iter: 718 loss: 3.61837101e-06
Iter: 719 loss: 3.70915086e-06
Iter: 720 loss: 3.61840057e-06
Iter: 721 loss: 3.61422303e-06
Iter: 722 loss: 3.61966704e-06
Iter: 723 loss: 3.61208095e-06
Iter: 724 loss: 3.60804688e-06
Iter: 725 loss: 3.63496e-06
Iter: 726 loss: 3.6076292e-06
Iter: 727 loss: 3.60371587e-06
Iter: 728 loss: 3.60053173e-06
Iter: 729 loss: 3.59932778e-06
Iter: 730 loss: 3.59518845e-06
Iter: 731 loss: 3.59522096e-06
Iter: 732 loss: 3.59170781e-06
Iter: 733 loss: 3.58828083e-06
Iter: 734 loss: 3.58757461e-06
Iter: 735 loss: 3.58245825e-06
Iter: 736 loss: 3.61409866e-06
Iter: 737 loss: 3.58185343e-06
Iter: 738 loss: 3.57781255e-06
Iter: 739 loss: 3.58511261e-06
Iter: 740 loss: 3.5760761e-06
Iter: 741 loss: 3.57511749e-06
Iter: 742 loss: 3.57359977e-06
Iter: 743 loss: 3.57259864e-06
Iter: 744 loss: 3.5693929e-06
Iter: 745 loss: 3.57933368e-06
Iter: 746 loss: 3.56787359e-06
Iter: 747 loss: 3.56298688e-06
Iter: 748 loss: 3.58270154e-06
Iter: 749 loss: 3.56184364e-06
Iter: 750 loss: 3.55782163e-06
Iter: 751 loss: 3.58560851e-06
Iter: 752 loss: 3.55728594e-06
Iter: 753 loss: 3.55413977e-06
Iter: 754 loss: 3.55183897e-06
Iter: 755 loss: 3.5508092e-06
Iter: 756 loss: 3.54571648e-06
Iter: 757 loss: 3.57969247e-06
Iter: 758 loss: 3.54515464e-06
Iter: 759 loss: 3.54136e-06
Iter: 760 loss: 3.53949736e-06
Iter: 761 loss: 3.53773862e-06
Iter: 762 loss: 3.53225187e-06
Iter: 763 loss: 3.581378e-06
Iter: 764 loss: 3.5318219e-06
Iter: 765 loss: 3.52847405e-06
Iter: 766 loss: 3.52628876e-06
Iter: 767 loss: 3.52505435e-06
Iter: 768 loss: 3.52024722e-06
Iter: 769 loss: 3.57739896e-06
Iter: 770 loss: 3.5203027e-06
Iter: 771 loss: 3.5166936e-06
Iter: 772 loss: 3.5246776e-06
Iter: 773 loss: 3.5155922e-06
Iter: 774 loss: 3.51238327e-06
Iter: 775 loss: 3.54511985e-06
Iter: 776 loss: 3.51227891e-06
Iter: 777 loss: 3.50963364e-06
Iter: 778 loss: 3.52890765e-06
Iter: 779 loss: 3.50938444e-06
Iter: 780 loss: 3.5078092e-06
Iter: 781 loss: 3.5034077e-06
Iter: 782 loss: 3.52243205e-06
Iter: 783 loss: 3.50184337e-06
Iter: 784 loss: 3.49674383e-06
Iter: 785 loss: 3.54514668e-06
Iter: 786 loss: 3.49656716e-06
Iter: 787 loss: 3.49283891e-06
Iter: 788 loss: 3.49290576e-06
Iter: 789 loss: 3.48985509e-06
Iter: 790 loss: 3.48469621e-06
Iter: 791 loss: 3.52154893e-06
Iter: 792 loss: 3.48425988e-06
Iter: 793 loss: 3.48081153e-06
Iter: 794 loss: 3.49119159e-06
Iter: 795 loss: 3.47984019e-06
Iter: 796 loss: 3.4758832e-06
Iter: 797 loss: 3.48214985e-06
Iter: 798 loss: 3.47386663e-06
Iter: 799 loss: 3.47049672e-06
Iter: 800 loss: 3.47850619e-06
Iter: 801 loss: 3.46931211e-06
Iter: 802 loss: 3.46548541e-06
Iter: 803 loss: 3.46682782e-06
Iter: 804 loss: 3.46287402e-06
Iter: 805 loss: 3.45853095e-06
Iter: 806 loss: 3.46736465e-06
Iter: 807 loss: 3.45674903e-06
Iter: 808 loss: 3.45093667e-06
Iter: 809 loss: 3.46773254e-06
Iter: 810 loss: 3.44907221e-06
Iter: 811 loss: 3.44865657e-06
Iter: 812 loss: 3.44698651e-06
Iter: 813 loss: 3.44506816e-06
Iter: 814 loss: 3.44666523e-06
Iter: 815 loss: 3.4438724e-06
Iter: 816 loss: 3.44169553e-06
Iter: 817 loss: 3.43914598e-06
Iter: 818 loss: 3.4388263e-06
Iter: 819 loss: 3.43559623e-06
Iter: 820 loss: 3.44212913e-06
Iter: 821 loss: 3.43450097e-06
Iter: 822 loss: 3.43013699e-06
Iter: 823 loss: 3.43005468e-06
Iter: 824 loss: 3.42662793e-06
Iter: 825 loss: 3.42236103e-06
Iter: 826 loss: 3.45089938e-06
Iter: 827 loss: 3.42181147e-06
Iter: 828 loss: 3.417578e-06
Iter: 829 loss: 3.42446515e-06
Iter: 830 loss: 3.41563327e-06
Iter: 831 loss: 3.41199802e-06
Iter: 832 loss: 3.4211007e-06
Iter: 833 loss: 3.41045848e-06
Iter: 834 loss: 3.40611291e-06
Iter: 835 loss: 3.43484226e-06
Iter: 836 loss: 3.40570659e-06
Iter: 837 loss: 3.40266729e-06
Iter: 838 loss: 3.40825932e-06
Iter: 839 loss: 3.40139354e-06
Iter: 840 loss: 3.39803501e-06
Iter: 841 loss: 3.39514918e-06
Iter: 842 loss: 3.39429516e-06
Iter: 843 loss: 3.39032908e-06
Iter: 844 loss: 3.43022293e-06
Iter: 845 loss: 3.39025291e-06
Iter: 846 loss: 3.38675409e-06
Iter: 847 loss: 3.38815062e-06
Iter: 848 loss: 3.38433756e-06
Iter: 849 loss: 3.38281802e-06
Iter: 850 loss: 3.38222435e-06
Iter: 851 loss: 3.38008635e-06
Iter: 852 loss: 3.38597965e-06
Iter: 853 loss: 3.37947745e-06
Iter: 854 loss: 3.37744814e-06
Iter: 855 loss: 3.37344341e-06
Iter: 856 loss: 3.4418772e-06
Iter: 857 loss: 3.37333904e-06
Iter: 858 loss: 3.36905805e-06
Iter: 859 loss: 3.39372423e-06
Iter: 860 loss: 3.36847052e-06
Iter: 861 loss: 3.36548783e-06
Iter: 862 loss: 3.36613311e-06
Iter: 863 loss: 3.36341577e-06
Iter: 864 loss: 3.35939785e-06
Iter: 865 loss: 3.37970278e-06
Iter: 866 loss: 3.3587271e-06
Iter: 867 loss: 3.35528648e-06
Iter: 868 loss: 3.3536719e-06
Iter: 869 loss: 3.35184814e-06
Iter: 870 loss: 3.34841343e-06
Iter: 871 loss: 3.34841729e-06
Iter: 872 loss: 3.34601373e-06
Iter: 873 loss: 3.35033201e-06
Iter: 874 loss: 3.34492347e-06
Iter: 875 loss: 3.34146466e-06
Iter: 876 loss: 3.34191941e-06
Iter: 877 loss: 3.33894241e-06
Iter: 878 loss: 3.33531034e-06
Iter: 879 loss: 3.3414849e-06
Iter: 880 loss: 3.33375237e-06
Iter: 881 loss: 3.32972672e-06
Iter: 882 loss: 3.35047275e-06
Iter: 883 loss: 3.32913055e-06
Iter: 884 loss: 3.32593254e-06
Iter: 885 loss: 3.33700655e-06
Iter: 886 loss: 3.32520654e-06
Iter: 887 loss: 3.32171e-06
Iter: 888 loss: 3.36671246e-06
Iter: 889 loss: 3.32165905e-06
Iter: 890 loss: 3.31985734e-06
Iter: 891 loss: 3.3172214e-06
Iter: 892 loss: 3.31717683e-06
Iter: 893 loss: 3.31431397e-06
Iter: 894 loss: 3.31265778e-06
Iter: 895 loss: 3.3114809e-06
Iter: 896 loss: 3.30678722e-06
Iter: 897 loss: 3.32847412e-06
Iter: 898 loss: 3.30584453e-06
Iter: 899 loss: 3.30264879e-06
Iter: 900 loss: 3.31457659e-06
Iter: 901 loss: 3.30184889e-06
Iter: 902 loss: 3.29814225e-06
Iter: 903 loss: 3.31075853e-06
Iter: 904 loss: 3.29723639e-06
Iter: 905 loss: 3.29433055e-06
Iter: 906 loss: 3.29693239e-06
Iter: 907 loss: 3.2926016e-06
Iter: 908 loss: 3.28842384e-06
Iter: 909 loss: 3.29354884e-06
Iter: 910 loss: 3.28632905e-06
Iter: 911 loss: 3.28250508e-06
Iter: 912 loss: 3.29636441e-06
Iter: 913 loss: 3.2815467e-06
Iter: 914 loss: 3.27803173e-06
Iter: 915 loss: 3.31143906e-06
Iter: 916 loss: 3.2779808e-06
Iter: 917 loss: 3.2756675e-06
Iter: 918 loss: 3.27847852e-06
Iter: 919 loss: 3.27440557e-06
Iter: 920 loss: 3.27161524e-06
Iter: 921 loss: 3.277215e-06
Iter: 922 loss: 3.27043335e-06
Iter: 923 loss: 3.2687135e-06
Iter: 924 loss: 3.26825534e-06
Iter: 925 loss: 3.26717895e-06
Iter: 926 loss: 3.2647456e-06
Iter: 927 loss: 3.29501336e-06
Iter: 928 loss: 3.26459576e-06
Iter: 929 loss: 3.26109034e-06
Iter: 930 loss: 3.25776887e-06
Iter: 931 loss: 3.25715973e-06
Iter: 932 loss: 3.25292262e-06
Iter: 933 loss: 3.27857242e-06
Iter: 934 loss: 3.25232054e-06
Iter: 935 loss: 3.24857115e-06
Iter: 936 loss: 3.25967721e-06
Iter: 937 loss: 3.24728944e-06
Iter: 938 loss: 3.24391749e-06
Iter: 939 loss: 3.25628048e-06
Iter: 940 loss: 3.24311486e-06
Iter: 941 loss: 3.23937115e-06
Iter: 942 loss: 3.24518919e-06
Iter: 943 loss: 3.23751692e-06
Iter: 944 loss: 3.23439485e-06
Iter: 945 loss: 3.27183216e-06
Iter: 946 loss: 3.23434915e-06
Iter: 947 loss: 3.23227e-06
Iter: 948 loss: 3.23023141e-06
Iter: 949 loss: 3.22973301e-06
Iter: 950 loss: 3.22600431e-06
Iter: 951 loss: 3.2491721e-06
Iter: 952 loss: 3.22562232e-06
Iter: 953 loss: 3.22254596e-06
Iter: 954 loss: 3.22066217e-06
Iter: 955 loss: 3.21938523e-06
Iter: 956 loss: 3.21699326e-06
Iter: 957 loss: 3.21679454e-06
Iter: 958 loss: 3.21386324e-06
Iter: 959 loss: 3.22259916e-06
Iter: 960 loss: 3.21300513e-06
Iter: 961 loss: 3.21116931e-06
Iter: 962 loss: 3.20738468e-06
Iter: 963 loss: 3.26345685e-06
Iter: 964 loss: 3.20728691e-06
Iter: 965 loss: 3.20445542e-06
Iter: 966 loss: 3.24797384e-06
Iter: 967 loss: 3.20441518e-06
Iter: 968 loss: 3.20191248e-06
Iter: 969 loss: 3.19903529e-06
Iter: 970 loss: 3.19853211e-06
Iter: 971 loss: 3.19516698e-06
Iter: 972 loss: 3.21161338e-06
Iter: 973 loss: 3.19472883e-06
Iter: 974 loss: 3.19101719e-06
Iter: 975 loss: 3.19256446e-06
Iter: 976 loss: 3.18862976e-06
Iter: 977 loss: 3.18484558e-06
Iter: 978 loss: 3.2180958e-06
Iter: 979 loss: 3.18465618e-06
Iter: 980 loss: 3.18213483e-06
Iter: 981 loss: 3.18476032e-06
Iter: 982 loss: 3.18083812e-06
Iter: 983 loss: 3.17750801e-06
Iter: 984 loss: 3.19702167e-06
Iter: 985 loss: 3.17709032e-06
Iter: 986 loss: 3.17480385e-06
Iter: 987 loss: 3.17688182e-06
Iter: 988 loss: 3.17355079e-06
Iter: 989 loss: 3.17006766e-06
Iter: 990 loss: 3.17677632e-06
Iter: 991 loss: 3.1686136e-06
Iter: 992 loss: 3.16827982e-06
Iter: 993 loss: 3.16754358e-06
Iter: 994 loss: 3.16626938e-06
Iter: 995 loss: 3.16293335e-06
Iter: 996 loss: 3.19154469e-06
Iter: 997 loss: 3.16251908e-06
Iter: 998 loss: 3.15952502e-06
Iter: 999 loss: 3.16997148e-06
Iter: 1000 loss: 3.15880925e-06
Iter: 1001 loss: 3.15604507e-06
Iter: 1002 loss: 3.15644365e-06
Iter: 1003 loss: 3.15387479e-06
Iter: 1004 loss: 3.15085981e-06
Iter: 1005 loss: 3.18728689e-06
Iter: 1006 loss: 3.15076477e-06
Iter: 1007 loss: 3.14846147e-06
Iter: 1008 loss: 3.14863769e-06
Iter: 1009 loss: 3.14669433e-06
Iter: 1010 loss: 3.14316753e-06
Iter: 1011 loss: 3.15892794e-06
Iter: 1012 loss: 3.14241925e-06
Iter: 1013 loss: 3.13995724e-06
Iter: 1014 loss: 3.13869259e-06
Iter: 1015 loss: 3.13744704e-06
Iter: 1016 loss: 3.13364444e-06
Iter: 1017 loss: 3.15461239e-06
Iter: 1018 loss: 3.1331017e-06
Iter: 1019 loss: 3.12999e-06
Iter: 1020 loss: 3.1344789e-06
Iter: 1021 loss: 3.12844918e-06
Iter: 1022 loss: 3.1246127e-06
Iter: 1023 loss: 3.14189401e-06
Iter: 1024 loss: 3.12372458e-06
Iter: 1025 loss: 3.12116026e-06
Iter: 1026 loss: 3.12645898e-06
Iter: 1027 loss: 3.12011525e-06
Iter: 1028 loss: 3.11738722e-06
Iter: 1029 loss: 3.14569752e-06
Iter: 1030 loss: 3.11728263e-06
Iter: 1031 loss: 3.11457393e-06
Iter: 1032 loss: 3.12703128e-06
Iter: 1033 loss: 3.11415033e-06
Iter: 1034 loss: 3.11261556e-06
Iter: 1035 loss: 3.11104941e-06
Iter: 1036 loss: 3.11077156e-06
Iter: 1037 loss: 3.10822043e-06
Iter: 1038 loss: 3.10538417e-06
Iter: 1039 loss: 3.10493965e-06
Iter: 1040 loss: 3.10181395e-06
Iter: 1041 loss: 3.13565101e-06
Iter: 1042 loss: 3.10169094e-06
Iter: 1043 loss: 3.09888242e-06
Iter: 1044 loss: 3.09857364e-06
Iter: 1045 loss: 3.09646907e-06
Iter: 1046 loss: 3.09310917e-06
Iter: 1047 loss: 3.10948144e-06
Iter: 1048 loss: 3.09241341e-06
Iter: 1049 loss: 3.08898234e-06
Iter: 1050 loss: 3.10683299e-06
Iter: 1051 loss: 3.08852941e-06
Iter: 1052 loss: 3.0865333e-06
Iter: 1053 loss: 3.09176153e-06
Iter: 1054 loss: 3.0858705e-06
Iter: 1055 loss: 3.08285757e-06
Iter: 1056 loss: 3.08439985e-06
Iter: 1057 loss: 3.08098242e-06
Iter: 1058 loss: 3.07793562e-06
Iter: 1059 loss: 3.09039706e-06
Iter: 1060 loss: 3.07743517e-06
Iter: 1061 loss: 3.07441019e-06
Iter: 1062 loss: 3.07406322e-06
Iter: 1063 loss: 3.07197479e-06
Iter: 1064 loss: 3.07009486e-06
Iter: 1065 loss: 3.06987363e-06
Iter: 1066 loss: 3.06795482e-06
Iter: 1067 loss: 3.08001654e-06
Iter: 1068 loss: 3.06784568e-06
Iter: 1069 loss: 3.06639163e-06
Iter: 1070 loss: 3.0633646e-06
Iter: 1071 loss: 3.1147456e-06
Iter: 1072 loss: 3.0633405e-06
Iter: 1073 loss: 3.06076663e-06
Iter: 1074 loss: 3.07645723e-06
Iter: 1075 loss: 3.06039601e-06
Iter: 1076 loss: 3.05798449e-06
Iter: 1077 loss: 3.05624053e-06
Iter: 1078 loss: 3.05554136e-06
Iter: 1079 loss: 3.05187746e-06
Iter: 1080 loss: 3.07972232e-06
Iter: 1081 loss: 3.05156618e-06
Iter: 1082 loss: 3.0491592e-06
Iter: 1083 loss: 3.04998e-06
Iter: 1084 loss: 3.04762034e-06
Iter: 1085 loss: 3.04400578e-06
Iter: 1086 loss: 3.05360049e-06
Iter: 1087 loss: 3.04271452e-06
Iter: 1088 loss: 3.03968682e-06
Iter: 1089 loss: 3.04340438e-06
Iter: 1090 loss: 3.03821776e-06
Iter: 1091 loss: 3.03582169e-06
Iter: 1092 loss: 3.03573802e-06
Iter: 1093 loss: 3.03371053e-06
Iter: 1094 loss: 3.03304705e-06
Iter: 1095 loss: 3.03206889e-06
Iter: 1096 loss: 3.02922217e-06
Iter: 1097 loss: 3.04517789e-06
Iter: 1098 loss: 3.02884337e-06
Iter: 1099 loss: 3.02702324e-06
Iter: 1100 loss: 3.03125967e-06
Iter: 1101 loss: 3.02626313e-06
Iter: 1102 loss: 3.02383e-06
Iter: 1103 loss: 3.05023059e-06
Iter: 1104 loss: 3.02378498e-06
Iter: 1105 loss: 3.02246053e-06
Iter: 1106 loss: 3.01983528e-06
Iter: 1107 loss: 3.07313576e-06
Iter: 1108 loss: 3.01979026e-06
Iter: 1109 loss: 3.01763748e-06
Iter: 1110 loss: 3.01674527e-06
Iter: 1111 loss: 3.01562841e-06
Iter: 1112 loss: 3.01270325e-06
Iter: 1113 loss: 3.04746618e-06
Iter: 1114 loss: 3.01257637e-06
Iter: 1115 loss: 3.01053137e-06
Iter: 1116 loss: 3.01288901e-06
Iter: 1117 loss: 3.00935153e-06
Iter: 1118 loss: 3.00617739e-06
Iter: 1119 loss: 3.01058662e-06
Iter: 1120 loss: 3.00452621e-06
Iter: 1121 loss: 3.00195643e-06
Iter: 1122 loss: 3.00262786e-06
Iter: 1123 loss: 3.00015199e-06
Iter: 1124 loss: 2.99664543e-06
Iter: 1125 loss: 3.02956096e-06
Iter: 1126 loss: 2.99655585e-06
Iter: 1127 loss: 2.99425369e-06
Iter: 1128 loss: 2.99315025e-06
Iter: 1129 loss: 2.99190469e-06
Iter: 1130 loss: 2.98854184e-06
Iter: 1131 loss: 3.02685839e-06
Iter: 1132 loss: 2.9885673e-06
Iter: 1133 loss: 2.98648138e-06
Iter: 1134 loss: 2.988711e-06
Iter: 1135 loss: 2.98548139e-06
Iter: 1136 loss: 2.98288796e-06
Iter: 1137 loss: 3.00254123e-06
Iter: 1138 loss: 2.98273267e-06
Iter: 1139 loss: 2.981433e-06
Iter: 1140 loss: 2.98140139e-06
Iter: 1141 loss: 2.98071427e-06
Iter: 1142 loss: 2.97855468e-06
Iter: 1143 loss: 2.98174928e-06
Iter: 1144 loss: 2.97717133e-06
Iter: 1145 loss: 2.97413658e-06
Iter: 1146 loss: 2.99598241e-06
Iter: 1147 loss: 2.97386487e-06
Iter: 1148 loss: 2.97135557e-06
Iter: 1149 loss: 2.97278848e-06
Iter: 1150 loss: 2.96973849e-06
Iter: 1151 loss: 2.96659209e-06
Iter: 1152 loss: 2.98783948e-06
Iter: 1153 loss: 2.96621488e-06
Iter: 1154 loss: 2.96384314e-06
Iter: 1155 loss: 2.96595249e-06
Iter: 1156 loss: 2.96252347e-06
Iter: 1157 loss: 2.95905375e-06
Iter: 1158 loss: 2.9741484e-06
Iter: 1159 loss: 2.95845666e-06
Iter: 1160 loss: 2.95649579e-06
Iter: 1161 loss: 2.95819291e-06
Iter: 1162 loss: 2.95522614e-06
Iter: 1163 loss: 2.95219115e-06
Iter: 1164 loss: 2.95459e-06
Iter: 1165 loss: 2.95015798e-06
Iter: 1166 loss: 2.94742063e-06
Iter: 1167 loss: 2.96727421e-06
Iter: 1168 loss: 2.94726647e-06
Iter: 1169 loss: 2.94488268e-06
Iter: 1170 loss: 2.94702932e-06
Iter: 1171 loss: 2.94346273e-06
Iter: 1172 loss: 2.9440514e-06
Iter: 1173 loss: 2.94248593e-06
Iter: 1174 loss: 2.94160532e-06
Iter: 1175 loss: 2.93990865e-06
Iter: 1176 loss: 2.93997027e-06
Iter: 1177 loss: 2.93763696e-06
Iter: 1178 loss: 2.93716448e-06
Iter: 1179 loss: 2.93566245e-06
Iter: 1180 loss: 2.93340327e-06
Iter: 1181 loss: 2.93926905e-06
Iter: 1182 loss: 2.9327116e-06
Iter: 1183 loss: 2.93004337e-06
Iter: 1184 loss: 2.93393987e-06
Iter: 1185 loss: 2.92882282e-06
Iter: 1186 loss: 2.92628465e-06
Iter: 1187 loss: 2.9332316e-06
Iter: 1188 loss: 2.92537698e-06
Iter: 1189 loss: 2.92247205e-06
Iter: 1190 loss: 2.92410596e-06
Iter: 1191 loss: 2.9204914e-06
Iter: 1192 loss: 2.91782635e-06
Iter: 1193 loss: 2.95094e-06
Iter: 1194 loss: 2.91775405e-06
Iter: 1195 loss: 2.91505603e-06
Iter: 1196 loss: 2.91446759e-06
Iter: 1197 loss: 2.91268702e-06
Iter: 1198 loss: 2.91016204e-06
Iter: 1199 loss: 2.93002654e-06
Iter: 1200 loss: 2.91002448e-06
Iter: 1201 loss: 2.90756066e-06
Iter: 1202 loss: 2.90615435e-06
Iter: 1203 loss: 2.905271e-06
Iter: 1204 loss: 2.90275966e-06
Iter: 1205 loss: 2.92690856e-06
Iter: 1206 loss: 2.90258504e-06
Iter: 1207 loss: 2.90032972e-06
Iter: 1208 loss: 2.90994876e-06
Iter: 1209 loss: 2.89987292e-06
Iter: 1210 loss: 2.89737523e-06
Iter: 1211 loss: 2.91920151e-06
Iter: 1212 loss: 2.89717445e-06
Iter: 1213 loss: 2.89655054e-06
Iter: 1214 loss: 2.89459194e-06
Iter: 1215 loss: 2.90361368e-06
Iter: 1216 loss: 2.89376453e-06
Iter: 1217 loss: 2.89063905e-06
Iter: 1218 loss: 2.90586468e-06
Iter: 1219 loss: 2.89006766e-06
Iter: 1220 loss: 2.8875811e-06
Iter: 1221 loss: 2.89194077e-06
Iter: 1222 loss: 2.88649926e-06
Iter: 1223 loss: 2.88330921e-06
Iter: 1224 loss: 2.90128901e-06
Iter: 1225 loss: 2.88297383e-06
Iter: 1226 loss: 2.88095e-06
Iter: 1227 loss: 2.88666547e-06
Iter: 1228 loss: 2.88028878e-06
Iter: 1229 loss: 2.87798184e-06
Iter: 1230 loss: 2.87648663e-06
Iter: 1231 loss: 2.87570128e-06
Iter: 1232 loss: 2.87315788e-06
Iter: 1233 loss: 2.90689559e-06
Iter: 1234 loss: 2.87314197e-06
Iter: 1235 loss: 2.87101284e-06
Iter: 1236 loss: 2.8695963e-06
Iter: 1237 loss: 2.86882414e-06
Iter: 1238 loss: 2.86566365e-06
Iter: 1239 loss: 2.89235527e-06
Iter: 1240 loss: 2.86541e-06
Iter: 1241 loss: 2.86335489e-06
Iter: 1242 loss: 2.8632503e-06
Iter: 1243 loss: 2.86170643e-06
Iter: 1244 loss: 2.85957685e-06
Iter: 1245 loss: 2.85963642e-06
Iter: 1246 loss: 2.85778515e-06
Iter: 1247 loss: 2.87138528e-06
Iter: 1248 loss: 2.8576419e-06
Iter: 1249 loss: 2.8567797e-06
Iter: 1250 loss: 2.85445731e-06
Iter: 1251 loss: 2.8659083e-06
Iter: 1252 loss: 2.85351e-06
Iter: 1253 loss: 2.8507452e-06
Iter: 1254 loss: 2.87280886e-06
Iter: 1255 loss: 2.85057013e-06
Iter: 1256 loss: 2.84832277e-06
Iter: 1257 loss: 2.84831276e-06
Iter: 1258 loss: 2.84646103e-06
Iter: 1259 loss: 2.84395333e-06
Iter: 1260 loss: 2.8719578e-06
Iter: 1261 loss: 2.84391035e-06
Iter: 1262 loss: 2.84201269e-06
Iter: 1263 loss: 2.83968279e-06
Iter: 1264 loss: 2.83950976e-06
Iter: 1265 loss: 2.83660893e-06
Iter: 1266 loss: 2.83672784e-06
Iter: 1267 loss: 2.83467398e-06
Iter: 1268 loss: 2.83468125e-06
Iter: 1269 loss: 2.8332322e-06
Iter: 1270 loss: 2.83008649e-06
Iter: 1271 loss: 2.83353802e-06
Iter: 1272 loss: 2.82848464e-06
Iter: 1273 loss: 2.82616588e-06
Iter: 1274 loss: 2.83578061e-06
Iter: 1275 loss: 2.82565838e-06
Iter: 1276 loss: 2.8229133e-06
Iter: 1277 loss: 2.82438941e-06
Iter: 1278 loss: 2.82106544e-06
Iter: 1279 loss: 2.82070346e-06
Iter: 1280 loss: 2.82003111e-06
Iter: 1281 loss: 2.81866642e-06
Iter: 1282 loss: 2.81674488e-06
Iter: 1283 loss: 2.81661414e-06
Iter: 1284 loss: 2.8143113e-06
Iter: 1285 loss: 2.81636949e-06
Iter: 1286 loss: 2.81278494e-06
Iter: 1287 loss: 2.81097414e-06
Iter: 1288 loss: 2.81877146e-06
Iter: 1289 loss: 2.81058396e-06
Iter: 1290 loss: 2.80845597e-06
Iter: 1291 loss: 2.80713311e-06
Iter: 1292 loss: 2.80626318e-06
Iter: 1293 loss: 2.80415543e-06
Iter: 1294 loss: 2.82390465e-06
Iter: 1295 loss: 2.8040281e-06
Iter: 1296 loss: 2.80183e-06
Iter: 1297 loss: 2.79869232e-06
Iter: 1298 loss: 2.79854385e-06
Iter: 1299 loss: 2.7958904e-06
Iter: 1300 loss: 2.79592632e-06
Iter: 1301 loss: 2.79376354e-06
Iter: 1302 loss: 2.79421374e-06
Iter: 1303 loss: 2.79219103e-06
Iter: 1304 loss: 2.79011238e-06
Iter: 1305 loss: 2.80927907e-06
Iter: 1306 loss: 2.79004303e-06
Iter: 1307 loss: 2.78806101e-06
Iter: 1308 loss: 2.78798393e-06
Iter: 1309 loss: 2.78637413e-06
Iter: 1310 loss: 2.78412131e-06
Iter: 1311 loss: 2.79736469e-06
Iter: 1312 loss: 2.78370749e-06
Iter: 1313 loss: 2.78269908e-06
Iter: 1314 loss: 2.78259313e-06
Iter: 1315 loss: 2.78128073e-06
Iter: 1316 loss: 2.77862773e-06
Iter: 1317 loss: 2.82261544e-06
Iter: 1318 loss: 2.77846289e-06
Iter: 1319 loss: 2.7763715e-06
Iter: 1320 loss: 2.78184098e-06
Iter: 1321 loss: 2.77560798e-06
Iter: 1322 loss: 2.77337699e-06
Iter: 1323 loss: 2.77614777e-06
Iter: 1324 loss: 2.77220192e-06
Iter: 1325 loss: 2.76951869e-06
Iter: 1326 loss: 2.7824035e-06
Iter: 1327 loss: 2.76898754e-06
Iter: 1328 loss: 2.76728019e-06
Iter: 1329 loss: 2.76932678e-06
Iter: 1330 loss: 2.76638775e-06
Iter: 1331 loss: 2.76402784e-06
Iter: 1332 loss: 2.7681e-06
Iter: 1333 loss: 2.76295395e-06
Iter: 1334 loss: 2.76067112e-06
Iter: 1335 loss: 2.76276683e-06
Iter: 1336 loss: 2.75936668e-06
Iter: 1337 loss: 2.75628713e-06
Iter: 1338 loss: 2.77676622e-06
Iter: 1339 loss: 2.75598177e-06
Iter: 1340 loss: 2.75401067e-06
Iter: 1341 loss: 2.75616367e-06
Iter: 1342 loss: 2.75307821e-06
Iter: 1343 loss: 2.75022444e-06
Iter: 1344 loss: 2.76322521e-06
Iter: 1345 loss: 2.74970193e-06
Iter: 1346 loss: 2.74803233e-06
Iter: 1347 loss: 2.75551565e-06
Iter: 1348 loss: 2.7475719e-06
Iter: 1349 loss: 2.74624472e-06
Iter: 1350 loss: 2.74623744e-06
Iter: 1351 loss: 2.74505487e-06
Iter: 1352 loss: 2.74201375e-06
Iter: 1353 loss: 2.7648598e-06
Iter: 1354 loss: 2.74130343e-06
Iter: 1355 loss: 2.73906335e-06
Iter: 1356 loss: 2.75265324e-06
Iter: 1357 loss: 2.73881824e-06
Iter: 1358 loss: 2.73643e-06
Iter: 1359 loss: 2.73617843e-06
Iter: 1360 loss: 2.73439673e-06
Iter: 1361 loss: 2.73186583e-06
Iter: 1362 loss: 2.77029608e-06
Iter: 1363 loss: 2.73189698e-06
Iter: 1364 loss: 2.73030173e-06
Iter: 1365 loss: 2.72841726e-06
Iter: 1366 loss: 2.72826287e-06
Iter: 1367 loss: 2.72527882e-06
Iter: 1368 loss: 2.74565082e-06
Iter: 1369 loss: 2.72497186e-06
Iter: 1370 loss: 2.72268562e-06
Iter: 1371 loss: 2.72073294e-06
Iter: 1372 loss: 2.72007765e-06
Iter: 1373 loss: 2.71716408e-06
Iter: 1374 loss: 2.75390676e-06
Iter: 1375 loss: 2.71716e-06
Iter: 1376 loss: 2.71488125e-06
Iter: 1377 loss: 2.71604426e-06
Iter: 1378 loss: 2.71342151e-06
Iter: 1379 loss: 2.71099361e-06
Iter: 1380 loss: 2.73567457e-06
Iter: 1381 loss: 2.71089857e-06
Iter: 1382 loss: 2.70923329e-06
Iter: 1383 loss: 2.71855674e-06
Iter: 1384 loss: 2.70899181e-06
Iter: 1385 loss: 2.70710143e-06
Iter: 1386 loss: 2.715964e-06
Iter: 1387 loss: 2.70668033e-06
Iter: 1388 loss: 2.70524743e-06
Iter: 1389 loss: 2.704395e-06
Iter: 1390 loss: 2.70390228e-06
Iter: 1391 loss: 2.70215378e-06
Iter: 1392 loss: 2.69963402e-06
Iter: 1393 loss: 2.69955308e-06
Iter: 1394 loss: 2.6962191e-06
Iter: 1395 loss: 2.72371312e-06
Iter: 1396 loss: 2.6959865e-06
Iter: 1397 loss: 2.69385509e-06
Iter: 1398 loss: 2.69696284e-06
Iter: 1399 loss: 2.69286238e-06
Iter: 1400 loss: 2.69008342e-06
Iter: 1401 loss: 2.70026612e-06
Iter: 1402 loss: 2.68937288e-06
Iter: 1403 loss: 2.68731446e-06
Iter: 1404 loss: 2.69684051e-06
Iter: 1405 loss: 2.68680401e-06
Iter: 1406 loss: 2.68460872e-06
Iter: 1407 loss: 2.6838386e-06
Iter: 1408 loss: 2.6825353e-06
Iter: 1409 loss: 2.68039889e-06
Iter: 1410 loss: 2.69126895e-06
Iter: 1411 loss: 2.67995438e-06
Iter: 1412 loss: 2.6771886e-06
Iter: 1413 loss: 2.67830842e-06
Iter: 1414 loss: 2.67535984e-06
Iter: 1415 loss: 2.67292262e-06
Iter: 1416 loss: 2.6905559e-06
Iter: 1417 loss: 2.67279574e-06
Iter: 1418 loss: 2.67157884e-06
Iter: 1419 loss: 2.67138603e-06
Iter: 1420 loss: 2.67005271e-06
Iter: 1421 loss: 2.66788675e-06
Iter: 1422 loss: 2.66794723e-06
Iter: 1423 loss: 2.66623351e-06
Iter: 1424 loss: 2.67125506e-06
Iter: 1425 loss: 2.66574716e-06
Iter: 1426 loss: 2.66370148e-06
Iter: 1427 loss: 2.6620728e-06
Iter: 1428 loss: 2.66152983e-06
Iter: 1429 loss: 2.65890458e-06
Iter: 1430 loss: 2.67171185e-06
Iter: 1431 loss: 2.65849599e-06
Iter: 1432 loss: 2.65594122e-06
Iter: 1433 loss: 2.65957533e-06
Iter: 1434 loss: 2.65472909e-06
Iter: 1435 loss: 2.65250083e-06
Iter: 1436 loss: 2.6642e-06
Iter: 1437 loss: 2.65210065e-06
Iter: 1438 loss: 2.6497753e-06
Iter: 1439 loss: 2.65045446e-06
Iter: 1440 loss: 2.64806567e-06
Iter: 1441 loss: 2.64592791e-06
Iter: 1442 loss: 2.6636626e-06
Iter: 1443 loss: 2.64580694e-06
Iter: 1444 loss: 2.64350092e-06
Iter: 1445 loss: 2.64396613e-06
Iter: 1446 loss: 2.64182836e-06
Iter: 1447 loss: 2.63944753e-06
Iter: 1448 loss: 2.65919516e-06
Iter: 1449 loss: 2.63929928e-06
Iter: 1450 loss: 2.63764264e-06
Iter: 1451 loss: 2.6364819e-06
Iter: 1452 loss: 2.63586253e-06
Iter: 1453 loss: 2.63624042e-06
Iter: 1454 loss: 2.6347102e-06
Iter: 1455 loss: 2.63388051e-06
Iter: 1456 loss: 2.63208813e-06
Iter: 1457 loss: 2.66480561e-06
Iter: 1458 loss: 2.63209881e-06
Iter: 1459 loss: 2.63022093e-06
Iter: 1460 loss: 2.62745834e-06
Iter: 1461 loss: 2.62739559e-06
Iter: 1462 loss: 2.62562025e-06
Iter: 1463 loss: 2.62544518e-06
Iter: 1464 loss: 2.62416779e-06
Iter: 1465 loss: 2.62364392e-06
Iter: 1466 loss: 2.62309186e-06
Iter: 1467 loss: 2.62067351e-06
Iter: 1468 loss: 2.62474987e-06
Iter: 1469 loss: 2.61967193e-06
Iter: 1470 loss: 2.61755667e-06
Iter: 1471 loss: 2.61964874e-06
Iter: 1472 loss: 2.61619653e-06
Iter: 1473 loss: 2.61317e-06
Iter: 1474 loss: 2.62178742e-06
Iter: 1475 loss: 2.61223749e-06
Iter: 1476 loss: 2.6096659e-06
Iter: 1477 loss: 2.62170033e-06
Iter: 1478 loss: 2.60912111e-06
Iter: 1479 loss: 2.60674187e-06
Iter: 1480 loss: 2.60962452e-06
Iter: 1481 loss: 2.60544289e-06
Iter: 1482 loss: 2.60321167e-06
Iter: 1483 loss: 2.61264358e-06
Iter: 1484 loss: 2.60261436e-06
Iter: 1485 loss: 2.60048478e-06
Iter: 1486 loss: 2.61860782e-06
Iter: 1487 loss: 2.60041497e-06
Iter: 1488 loss: 2.59864373e-06
Iter: 1489 loss: 2.61980949e-06
Iter: 1490 loss: 2.59871422e-06
Iter: 1491 loss: 2.59787112e-06
Iter: 1492 loss: 2.59644412e-06
Iter: 1493 loss: 2.62713911e-06
Iter: 1494 loss: 2.59643639e-06
Iter: 1495 loss: 2.59437593e-06
Iter: 1496 loss: 2.59509852e-06
Iter: 1497 loss: 2.59294984e-06
Iter: 1498 loss: 2.59111357e-06
Iter: 1499 loss: 2.59608623e-06
Iter: 1500 loss: 2.59044486e-06
Iter: 1501 loss: 2.58798673e-06
Iter: 1502 loss: 2.59188619e-06
Iter: 1503 loss: 2.5867871e-06
Iter: 1504 loss: 2.58501e-06
Iter: 1505 loss: 2.60111756e-06
Iter: 1506 loss: 2.58491514e-06
Iter: 1507 loss: 2.58313548e-06
Iter: 1508 loss: 2.58289924e-06
Iter: 1509 loss: 2.58158934e-06
Iter: 1510 loss: 2.57944407e-06
Iter: 1511 loss: 2.59452145e-06
Iter: 1512 loss: 2.57919532e-06
Iter: 1513 loss: 2.57742181e-06
Iter: 1514 loss: 2.57536249e-06
Iter: 1515 loss: 2.57511033e-06
Iter: 1516 loss: 2.57276542e-06
Iter: 1517 loss: 2.60378965e-06
Iter: 1518 loss: 2.57284591e-06
Iter: 1519 loss: 2.57087618e-06
Iter: 1520 loss: 2.56788326e-06
Iter: 1521 loss: 2.56784188e-06
Iter: 1522 loss: 2.57057627e-06
Iter: 1523 loss: 2.56677e-06
Iter: 1524 loss: 2.56572821e-06
Iter: 1525 loss: 2.56451563e-06
Iter: 1526 loss: 2.56437306e-06
Iter: 1527 loss: 2.56249541e-06
Iter: 1528 loss: 2.56061071e-06
Iter: 1529 loss: 2.56030216e-06
Iter: 1530 loss: 2.5581985e-06
Iter: 1531 loss: 2.57898773e-06
Iter: 1532 loss: 2.55816235e-06
Iter: 1533 loss: 2.55625628e-06
Iter: 1534 loss: 2.55589407e-06
Iter: 1535 loss: 2.55471014e-06
Iter: 1536 loss: 2.5522902e-06
Iter: 1537 loss: 2.57249258e-06
Iter: 1538 loss: 2.55222358e-06
Iter: 1539 loss: 2.55082841e-06
Iter: 1540 loss: 2.54953352e-06
Iter: 1541 loss: 2.54916381e-06
Iter: 1542 loss: 2.54674774e-06
Iter: 1543 loss: 2.56126259e-06
Iter: 1544 loss: 2.54627867e-06
Iter: 1545 loss: 2.54427459e-06
Iter: 1546 loss: 2.55122222e-06
Iter: 1547 loss: 2.54370957e-06
Iter: 1548 loss: 2.54157044e-06
Iter: 1549 loss: 2.54757515e-06
Iter: 1550 loss: 2.5408192e-06
Iter: 1551 loss: 2.53917233e-06
Iter: 1552 loss: 2.53927465e-06
Iter: 1553 loss: 2.53785811e-06
Iter: 1554 loss: 2.53538974e-06
Iter: 1555 loss: 2.55050622e-06
Iter: 1556 loss: 2.53511189e-06
Iter: 1557 loss: 2.53483404e-06
Iter: 1558 loss: 2.53422604e-06
Iter: 1559 loss: 2.53355779e-06
Iter: 1560 loss: 2.53195458e-06
Iter: 1561 loss: 2.54890165e-06
Iter: 1562 loss: 2.53180065e-06
Iter: 1563 loss: 2.52963855e-06
Iter: 1564 loss: 2.52899395e-06
Iter: 1565 loss: 2.52774475e-06
Iter: 1566 loss: 2.52560949e-06
Iter: 1567 loss: 2.54638735e-06
Iter: 1568 loss: 2.52557311e-06
Iter: 1569 loss: 2.52360087e-06
Iter: 1570 loss: 2.5229624e-06
Iter: 1571 loss: 2.52187556e-06
Iter: 1572 loss: 2.51974734e-06
Iter: 1573 loss: 2.54950305e-06
Iter: 1574 loss: 2.51973779e-06
Iter: 1575 loss: 2.51836195e-06
Iter: 1576 loss: 2.51639494e-06
Iter: 1577 loss: 2.51633264e-06
Iter: 1578 loss: 2.51378833e-06
Iter: 1579 loss: 2.5373472e-06
Iter: 1580 loss: 2.51371262e-06
Iter: 1581 loss: 2.51193615e-06
Iter: 1582 loss: 2.51409597e-06
Iter: 1583 loss: 2.5109548e-06
Iter: 1584 loss: 2.50883159e-06
Iter: 1585 loss: 2.51980941e-06
Iter: 1586 loss: 2.50842e-06
Iter: 1587 loss: 2.50696485e-06
Iter: 1588 loss: 2.50830431e-06
Iter: 1589 loss: 2.50605945e-06
Iter: 1590 loss: 2.50430594e-06
Iter: 1591 loss: 2.5169204e-06
Iter: 1592 loss: 2.50414155e-06
Iter: 1593 loss: 2.50253447e-06
Iter: 1594 loss: 2.51641177e-06
Iter: 1595 loss: 2.50242329e-06
Iter: 1596 loss: 2.5017298e-06
Iter: 1597 loss: 2.50002654e-06
Iter: 1598 loss: 2.51970209e-06
Iter: 1599 loss: 2.49992513e-06
Iter: 1600 loss: 2.49754248e-06
Iter: 1601 loss: 2.50192033e-06
Iter: 1602 loss: 2.49649497e-06
Iter: 1603 loss: 2.4947135e-06
Iter: 1604 loss: 2.50136077e-06
Iter: 1605 loss: 2.49425375e-06
Iter: 1606 loss: 2.4924118e-06
Iter: 1607 loss: 2.49940194e-06
Iter: 1608 loss: 2.49199502e-06
Iter: 1609 loss: 2.49015352e-06
Iter: 1610 loss: 2.4934393e-06
Iter: 1611 loss: 2.48943888e-06
Iter: 1612 loss: 2.48727542e-06
Iter: 1613 loss: 2.48851029e-06
Iter: 1614 loss: 2.4859778e-06
Iter: 1615 loss: 2.48412289e-06
Iter: 1616 loss: 2.504642e-06
Iter: 1617 loss: 2.48402807e-06
Iter: 1618 loss: 2.48260494e-06
Iter: 1619 loss: 2.48234414e-06
Iter: 1620 loss: 2.48132574e-06
Iter: 1621 loss: 2.47895105e-06
Iter: 1622 loss: 2.49293703e-06
Iter: 1623 loss: 2.47863272e-06
Iter: 1624 loss: 2.47703724e-06
Iter: 1625 loss: 2.47972184e-06
Iter: 1626 loss: 2.47642788e-06
Iter: 1627 loss: 2.47521689e-06
Iter: 1628 loss: 2.47521848e-06
Iter: 1629 loss: 2.47403273e-06
Iter: 1630 loss: 2.47259914e-06
Iter: 1631 loss: 2.47250705e-06
Iter: 1632 loss: 2.47136268e-06
Iter: 1633 loss: 2.47047228e-06
Iter: 1634 loss: 2.47011349e-06
Iter: 1635 loss: 2.46786271e-06
Iter: 1636 loss: 2.47303819e-06
Iter: 1637 loss: 2.46687864e-06
Iter: 1638 loss: 2.4651813e-06
Iter: 1639 loss: 2.47609887e-06
Iter: 1640 loss: 2.46496893e-06
Iter: 1641 loss: 2.46327795e-06
Iter: 1642 loss: 2.46368973e-06
Iter: 1643 loss: 2.46205173e-06
Iter: 1644 loss: 2.46015679e-06
Iter: 1645 loss: 2.47586013e-06
Iter: 1646 loss: 2.46009699e-06
Iter: 1647 loss: 2.45858087e-06
Iter: 1648 loss: 2.45788897e-06
Iter: 1649 loss: 2.45722822e-06
Iter: 1650 loss: 2.45522915e-06
Iter: 1651 loss: 2.46828586e-06
Iter: 1652 loss: 2.45508363e-06
Iter: 1653 loss: 2.45341016e-06
Iter: 1654 loss: 2.4556175e-06
Iter: 1655 loss: 2.45253568e-06
Iter: 1656 loss: 2.45037836e-06
Iter: 1657 loss: 2.45849924e-06
Iter: 1658 loss: 2.44979969e-06
Iter: 1659 loss: 2.44830039e-06
Iter: 1660 loss: 2.45601268e-06
Iter: 1661 loss: 2.44805346e-06
Iter: 1662 loss: 2.44639796e-06
Iter: 1663 loss: 2.46013769e-06
Iter: 1664 loss: 2.44626949e-06
Iter: 1665 loss: 2.4453152e-06
Iter: 1666 loss: 2.44391913e-06
Iter: 1667 loss: 2.4439164e-06
Iter: 1668 loss: 2.4423166e-06
Iter: 1669 loss: 2.4416895e-06
Iter: 1670 loss: 2.44083867e-06
Iter: 1671 loss: 2.43871909e-06
Iter: 1672 loss: 2.45543856e-06
Iter: 1673 loss: 2.43858358e-06
Iter: 1674 loss: 2.43726663e-06
Iter: 1675 loss: 2.44000557e-06
Iter: 1676 loss: 2.4366409e-06
Iter: 1677 loss: 2.43468389e-06
Iter: 1678 loss: 2.43624845e-06
Iter: 1679 loss: 2.43347131e-06
Iter: 1680 loss: 2.43158047e-06
Iter: 1681 loss: 2.44731086e-06
Iter: 1682 loss: 2.43144723e-06
Iter: 1683 loss: 2.4299834e-06
Iter: 1684 loss: 2.42840861e-06
Iter: 1685 loss: 2.42813303e-06
Iter: 1686 loss: 2.42596616e-06
Iter: 1687 loss: 2.44950115e-06
Iter: 1688 loss: 2.42586248e-06
Iter: 1689 loss: 2.42445094e-06
Iter: 1690 loss: 2.42660099e-06
Iter: 1691 loss: 2.42385568e-06
Iter: 1692 loss: 2.42208375e-06
Iter: 1693 loss: 2.42882493e-06
Iter: 1694 loss: 2.42149235e-06
Iter: 1695 loss: 2.42050146e-06
Iter: 1696 loss: 2.43385193e-06
Iter: 1697 loss: 2.42046235e-06
Iter: 1698 loss: 2.419152e-06
Iter: 1699 loss: 2.41944599e-06
Iter: 1700 loss: 2.41819816e-06
Iter: 1701 loss: 2.416808e-06
Iter: 1702 loss: 2.41586895e-06
Iter: 1703 loss: 2.41541147e-06
Iter: 1704 loss: 2.41363659e-06
Iter: 1705 loss: 2.415469e-06
Iter: 1706 loss: 2.41272642e-06
Iter: 1707 loss: 2.41038583e-06
Iter: 1708 loss: 2.42035458e-06
Iter: 1709 loss: 2.40998179e-06
Iter: 1710 loss: 2.40837676e-06
Iter: 1711 loss: 2.40930149e-06
Iter: 1712 loss: 2.40738382e-06
Iter: 1713 loss: 2.40516852e-06
Iter: 1714 loss: 2.42277929e-06
Iter: 1715 loss: 2.40497457e-06
Iter: 1716 loss: 2.40358236e-06
Iter: 1717 loss: 2.40430813e-06
Iter: 1718 loss: 2.40262671e-06
Iter: 1719 loss: 2.40051054e-06
Iter: 1720 loss: 2.40441204e-06
Iter: 1721 loss: 2.39956648e-06
Iter: 1722 loss: 2.39774727e-06
Iter: 1723 loss: 2.4065007e-06
Iter: 1724 loss: 2.39745441e-06
Iter: 1725 loss: 2.39575365e-06
Iter: 1726 loss: 2.39938709e-06
Iter: 1727 loss: 2.39521228e-06
Iter: 1728 loss: 2.39389192e-06
Iter: 1729 loss: 2.40592726e-06
Iter: 1730 loss: 2.39382416e-06
Iter: 1731 loss: 2.39286419e-06
Iter: 1732 loss: 2.39878136e-06
Iter: 1733 loss: 2.3928028e-06
Iter: 1734 loss: 2.39155861e-06
Iter: 1735 loss: 2.39151677e-06
Iter: 1736 loss: 2.39056158e-06
Iter: 1737 loss: 2.38941811e-06
Iter: 1738 loss: 2.38899702e-06
Iter: 1739 loss: 2.38837811e-06
Iter: 1740 loss: 2.38689427e-06
Iter: 1741 loss: 2.38592793e-06
Iter: 1742 loss: 2.38535222e-06
Iter: 1743 loss: 2.38318853e-06
Iter: 1744 loss: 2.40763438e-06
Iter: 1745 loss: 2.38309212e-06
Iter: 1746 loss: 2.38170742e-06
Iter: 1747 loss: 2.38072585e-06
Iter: 1748 loss: 2.38015218e-06
Iter: 1749 loss: 2.37816448e-06
Iter: 1750 loss: 2.40250552e-06
Iter: 1751 loss: 2.37804579e-06
Iter: 1752 loss: 2.37665517e-06
Iter: 1753 loss: 2.37792256e-06
Iter: 1754 loss: 2.37591212e-06
Iter: 1755 loss: 2.37396193e-06
Iter: 1756 loss: 2.37653558e-06
Iter: 1757 loss: 2.37297718e-06
Iter: 1758 loss: 2.37122072e-06
Iter: 1759 loss: 2.37772247e-06
Iter: 1760 loss: 2.370831e-06
Iter: 1761 loss: 2.36892674e-06
Iter: 1762 loss: 2.37384529e-06
Iter: 1763 loss: 2.3683142e-06
Iter: 1764 loss: 2.3666471e-06
Iter: 1765 loss: 2.37318886e-06
Iter: 1766 loss: 2.36632604e-06
Iter: 1767 loss: 2.36488654e-06
Iter: 1768 loss: 2.37820859e-06
Iter: 1769 loss: 2.36486494e-06
Iter: 1770 loss: 2.36363326e-06
Iter: 1771 loss: 2.3711284e-06
Iter: 1772 loss: 2.36347341e-06
Iter: 1773 loss: 2.36289202e-06
Iter: 1774 loss: 2.36191863e-06
Iter: 1775 loss: 2.36189271e-06
Iter: 1776 loss: 2.3605337e-06
Iter: 1777 loss: 2.35898415e-06
Iter: 1778 loss: 2.35874859e-06
Iter: 1779 loss: 2.35700099e-06
Iter: 1780 loss: 2.37606469e-06
Iter: 1781 loss: 2.357e-06
Iter: 1782 loss: 2.35531979e-06
Iter: 1783 loss: 2.3555267e-06
Iter: 1784 loss: 2.35413154e-06
Iter: 1785 loss: 2.35282823e-06
Iter: 1786 loss: 2.37129029e-06
Iter: 1787 loss: 2.35291554e-06
Iter: 1788 loss: 2.3517207e-06
Iter: 1789 loss: 2.35055722e-06
Iter: 1790 loss: 2.35038556e-06
Iter: 1791 loss: 2.34837034e-06
Iter: 1792 loss: 2.36479855e-06
Iter: 1793 loss: 2.34822619e-06
Iter: 1794 loss: 2.34675554e-06
Iter: 1795 loss: 2.34643e-06
Iter: 1796 loss: 2.34554545e-06
Iter: 1797 loss: 2.34369077e-06
Iter: 1798 loss: 2.3524774e-06
Iter: 1799 loss: 2.34328718e-06
Iter: 1800 loss: 2.34176809e-06
Iter: 1801 loss: 2.3482844e-06
Iter: 1802 loss: 2.34152503e-06
Iter: 1803 loss: 2.34008621e-06
Iter: 1804 loss: 2.34259505e-06
Iter: 1805 loss: 2.3394216e-06
Iter: 1806 loss: 2.33806e-06
Iter: 1807 loss: 2.34996901e-06
Iter: 1808 loss: 2.3380303e-06
Iter: 1809 loss: 2.33670085e-06
Iter: 1810 loss: 2.34431104e-06
Iter: 1811 loss: 2.33649985e-06
Iter: 1812 loss: 2.33558058e-06
Iter: 1813 loss: 2.33441142e-06
Iter: 1814 loss: 2.33432502e-06
Iter: 1815 loss: 2.33307719e-06
Iter: 1816 loss: 2.3330781e-06
Iter: 1817 loss: 2.33212359e-06
Iter: 1818 loss: 2.33025139e-06
Iter: 1819 loss: 2.3370103e-06
Iter: 1820 loss: 2.3297855e-06
Iter: 1821 loss: 2.32841421e-06
Iter: 1822 loss: 2.3305106e-06
Iter: 1823 loss: 2.32756724e-06
Iter: 1824 loss: 2.32585126e-06
Iter: 1825 loss: 2.33665446e-06
Iter: 1826 loss: 2.32565685e-06
Iter: 1827 loss: 2.32440607e-06
Iter: 1828 loss: 2.32638217e-06
Iter: 1829 loss: 2.32389539e-06
Iter: 1830 loss: 2.3221462e-06
Iter: 1831 loss: 2.32584443e-06
Iter: 1832 loss: 2.3214925e-06
Iter: 1833 loss: 2.32020534e-06
Iter: 1834 loss: 2.32490356e-06
Iter: 1835 loss: 2.31987451e-06
Iter: 1836 loss: 2.31844137e-06
Iter: 1837 loss: 2.31839249e-06
Iter: 1838 loss: 2.3172272e-06
Iter: 1839 loss: 2.3156997e-06
Iter: 1840 loss: 2.33281662e-06
Iter: 1841 loss: 2.31559079e-06
Iter: 1842 loss: 2.31432568e-06
Iter: 1843 loss: 2.31579043e-06
Iter: 1844 loss: 2.31358263e-06
Iter: 1845 loss: 2.3125649e-06
Iter: 1846 loss: 2.31242348e-06
Iter: 1847 loss: 2.31174226e-06
Iter: 1848 loss: 2.31076865e-06
Iter: 1849 loss: 2.31069339e-06
Iter: 1850 loss: 2.30968408e-06
Iter: 1851 loss: 2.30919113e-06
Iter: 1852 loss: 2.30859314e-06
Iter: 1853 loss: 2.30700789e-06
Iter: 1854 loss: 2.31385957e-06
Iter: 1855 loss: 2.30672094e-06
Iter: 1856 loss: 2.30520732e-06
Iter: 1857 loss: 2.30448904e-06
Iter: 1858 loss: 2.3038283e-06
Iter: 1859 loss: 2.30218347e-06
Iter: 1860 loss: 2.32327602e-06
Iter: 1861 loss: 2.30209275e-06
Iter: 1862 loss: 2.3005623e-06
Iter: 1863 loss: 2.30203591e-06
Iter: 1864 loss: 2.29964644e-06
Iter: 1865 loss: 2.29801708e-06
Iter: 1866 loss: 2.30995693e-06
Iter: 1867 loss: 2.2979234e-06
Iter: 1868 loss: 2.29675652e-06
Iter: 1869 loss: 2.29666944e-06
Iter: 1870 loss: 2.29581428e-06
Iter: 1871 loss: 2.29403804e-06
Iter: 1872 loss: 2.29904208e-06
Iter: 1873 loss: 2.2933109e-06
Iter: 1874 loss: 2.29188072e-06
Iter: 1875 loss: 2.29739408e-06
Iter: 1876 loss: 2.29151919e-06
Iter: 1877 loss: 2.28981071e-06
Iter: 1878 loss: 2.2935003e-06
Iter: 1879 loss: 2.28925819e-06
Iter: 1880 loss: 2.2886602e-06
Iter: 1881 loss: 2.28836257e-06
Iter: 1882 loss: 2.28773388e-06
Iter: 1883 loss: 2.28724707e-06
Iter: 1884 loss: 2.28692807e-06
Iter: 1885 loss: 2.2859922e-06
Iter: 1886 loss: 2.28444378e-06
Iter: 1887 loss: 2.28446788e-06
Iter: 1888 loss: 2.2829372e-06
Iter: 1889 loss: 2.29670604e-06
Iter: 1890 loss: 2.28284739e-06
Iter: 1891 loss: 2.28162435e-06
Iter: 1892 loss: 2.28109297e-06
Iter: 1893 loss: 2.28047656e-06
Iter: 1894 loss: 2.2788663e-06
Iter: 1895 loss: 2.28792669e-06
Iter: 1896 loss: 2.27862301e-06
Iter: 1897 loss: 2.27694272e-06
Iter: 1898 loss: 2.28036515e-06
Iter: 1899 loss: 2.27632609e-06
Iter: 1900 loss: 2.27481928e-06
Iter: 1901 loss: 2.28492e-06
Iter: 1902 loss: 2.27474516e-06
Iter: 1903 loss: 2.27348301e-06
Iter: 1904 loss: 2.27317832e-06
Iter: 1905 loss: 2.27240048e-06
Iter: 1906 loss: 2.27075634e-06
Iter: 1907 loss: 2.2834015e-06
Iter: 1908 loss: 2.27064947e-06
Iter: 1909 loss: 2.26950942e-06
Iter: 1910 loss: 2.268554e-06
Iter: 1911 loss: 2.26813563e-06
Iter: 1912 loss: 2.26714837e-06
Iter: 1913 loss: 2.26708971e-06
Iter: 1914 loss: 2.26619704e-06
Iter: 1915 loss: 2.27758846e-06
Iter: 1916 loss: 2.26619795e-06
Iter: 1917 loss: 2.2658e-06
Iter: 1918 loss: 2.26436487e-06
Iter: 1919 loss: 2.26980274e-06
Iter: 1920 loss: 2.2638269e-06
Iter: 1921 loss: 2.26239808e-06
Iter: 1922 loss: 2.28073236e-06
Iter: 1923 loss: 2.26242241e-06
Iter: 1924 loss: 2.26135921e-06
Iter: 1925 loss: 2.26145835e-06
Iter: 1926 loss: 2.26061366e-06
Iter: 1927 loss: 2.25906388e-06
Iter: 1928 loss: 2.26578732e-06
Iter: 1929 loss: 2.25879876e-06
Iter: 1930 loss: 2.25753365e-06
Iter: 1931 loss: 2.25745453e-06
Iter: 1932 loss: 2.25649501e-06
Iter: 1933 loss: 2.25473104e-06
Iter: 1934 loss: 2.26503926e-06
Iter: 1935 loss: 2.25447047e-06
Iter: 1936 loss: 2.25319241e-06
Iter: 1937 loss: 2.25433223e-06
Iter: 1938 loss: 2.25247231e-06
Iter: 1939 loss: 2.25090139e-06
Iter: 1940 loss: 2.25943086e-06
Iter: 1941 loss: 2.25080453e-06
Iter: 1942 loss: 2.2495019e-06
Iter: 1943 loss: 2.25197596e-06
Iter: 1944 loss: 2.24899759e-06
Iter: 1945 loss: 2.24735231e-06
Iter: 1946 loss: 2.25052736e-06
Iter: 1947 loss: 2.24671203e-06
Iter: 1948 loss: 2.2452607e-06
Iter: 1949 loss: 2.2524041e-06
Iter: 1950 loss: 2.24504561e-06
Iter: 1951 loss: 2.2442207e-06
Iter: 1952 loss: 2.24421365e-06
Iter: 1953 loss: 2.24334372e-06
Iter: 1954 loss: 2.24205246e-06
Iter: 1955 loss: 2.24195583e-06
Iter: 1956 loss: 2.24100859e-06
Iter: 1957 loss: 2.24124233e-06
Iter: 1958 loss: 2.24037285e-06
Iter: 1959 loss: 2.23877828e-06
Iter: 1960 loss: 2.23986945e-06
Iter: 1961 loss: 2.23786333e-06
Iter: 1962 loss: 2.23641973e-06
Iter: 1963 loss: 2.25375902e-06
Iter: 1964 loss: 2.23644383e-06
Iter: 1965 loss: 2.23550387e-06
Iter: 1966 loss: 2.23411075e-06
Iter: 1967 loss: 2.23410461e-06
Iter: 1968 loss: 2.23261077e-06
Iter: 1969 loss: 2.2486729e-06
Iter: 1970 loss: 2.23260076e-06
Iter: 1971 loss: 2.23142683e-06
Iter: 1972 loss: 2.23064626e-06
Iter: 1973 loss: 2.23016673e-06
Iter: 1974 loss: 2.22854601e-06
Iter: 1975 loss: 2.241012e-06
Iter: 1976 loss: 2.22839708e-06
Iter: 1977 loss: 2.22707922e-06
Iter: 1978 loss: 2.22801827e-06
Iter: 1979 loss: 2.22621293e-06
Iter: 1980 loss: 2.22460721e-06
Iter: 1981 loss: 2.23585448e-06
Iter: 1982 loss: 2.22448352e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi3
+ date
Wed Oct 21 17:42:49 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi2.8/300_300_300_1 --function f1 --psi 2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb244202d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb266782158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb26679e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb26679ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2666bc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb244182510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2440f1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2441287b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb244128510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2441e2b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2441a56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c1a6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c1ae8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c177bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c0cb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c0b6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c0ad400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c0adb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c083598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c083f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c01f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c01f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2001d9048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb200205598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb200205ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb200194598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb200149620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb20012b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c0f6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb21c0f6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb20010a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2000ca1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2000ca378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb2000ca048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb20009c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb200038620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.024335112
test_loss: 0.02417551
train_loss: 0.012965041
test_loss: 0.013952075
train_loss: 0.0086828135
test_loss: 0.011658531
train_loss: 0.008199213
test_loss: 0.010561342
train_loss: 0.0070527117
test_loss: 0.010344209
train_loss: 0.006650396
test_loss: 0.010270149
train_loss: 0.0070436355
test_loss: 0.010217974
train_loss: 0.006779106
test_loss: 0.00987938
train_loss: 0.006444389
test_loss: 0.0098425485
train_loss: 0.007864776
test_loss: 0.01000008
train_loss: 0.006012438
test_loss: 0.009986135
train_loss: 0.0066129067
test_loss: 0.009723568
train_loss: 0.0066496725
test_loss: 0.009993739
train_loss: 0.0060680034
test_loss: 0.009699962
train_loss: 0.006478503
test_loss: 0.009576189
train_loss: 0.006558867
test_loss: 0.009360086
train_loss: 0.0066930177
test_loss: 0.009966987
train_loss: 0.0061330996
test_loss: 0.009924444
train_loss: 0.0062137553
test_loss: 0.009464211
train_loss: 0.0058308244
test_loss: 0.0097813085
train_loss: 0.006355233
test_loss: 0.010109107
train_loss: 0.0058014384
test_loss: 0.00930709
train_loss: 0.0065679764
test_loss: 0.009600481
train_loss: 0.0061056437
test_loss: 0.009727075
train_loss: 0.0061909435
test_loss: 0.00979185
train_loss: 0.0057271724
test_loss: 0.009590949
train_loss: 0.005738501
test_loss: 0.009224368
train_loss: 0.006089223
test_loss: 0.009446278
train_loss: 0.0054401467
test_loss: 0.00930685
train_loss: 0.005899585
test_loss: 0.009315098
train_loss: 0.008208081
test_loss: 0.009628181
train_loss: 0.0060275975
test_loss: 0.0095725255
train_loss: 0.00530391
test_loss: 0.009507724
train_loss: 0.0057795746
test_loss: 0.009433667
train_loss: 0.0053271954
test_loss: 0.0095887445
train_loss: 0.0057326225
test_loss: 0.0092644775
train_loss: 0.0054691695
test_loss: 0.0093256645
train_loss: 0.0056314324
test_loss: 0.0094224075
train_loss: 0.005352836
test_loss: 0.0096024815
train_loss: 0.006537355
test_loss: 0.009754221
train_loss: 0.00562503
test_loss: 0.009434925
train_loss: 0.0058607203
test_loss: 0.009477029
train_loss: 0.0057738344
test_loss: 0.009631404
train_loss: 0.0056503965
test_loss: 0.00952587
train_loss: 0.0055857776
test_loss: 0.009304414
train_loss: 0.0053638034
test_loss: 0.009398368
train_loss: 0.0054015657
test_loss: 0.009646293
train_loss: 0.005825221
test_loss: 0.009653612
train_loss: 0.00560167
test_loss: 0.009097394
train_loss: 0.005552816
test_loss: 0.009353808
train_loss: 0.005585683
test_loss: 0.009285508
train_loss: 0.0051008845
test_loss: 0.00920605
train_loss: 0.005302959
test_loss: 0.009240256
train_loss: 0.006186002
test_loss: 0.009434053
train_loss: 0.0057585407
test_loss: 0.009323648
train_loss: 0.0062176166
test_loss: 0.0091086235
train_loss: 0.005875958
test_loss: 0.009344882
train_loss: 0.005100077
test_loss: 0.009365092
train_loss: 0.0058632763
test_loss: 0.009602342
train_loss: 0.005841419
test_loss: 0.009280146
train_loss: 0.005588944
test_loss: 0.0091770645
train_loss: 0.005649653
test_loss: 0.009202934
train_loss: 0.005675661
test_loss: 0.009309937
train_loss: 0.0056206086
test_loss: 0.00914776
train_loss: 0.0057133865
test_loss: 0.009300328
train_loss: 0.00512248
test_loss: 0.009159798
train_loss: 0.0053175846
test_loss: 0.009263955
train_loss: 0.0049339617
test_loss: 0.00932397
train_loss: 0.0054459875
test_loss: 0.009291032
train_loss: 0.0061214734
test_loss: 0.009254041
train_loss: 0.0054553063
test_loss: 0.008846104
train_loss: 0.0051498376
test_loss: 0.009257844
train_loss: 0.0053845383
test_loss: 0.009216251
train_loss: 0.0058913995
test_loss: 0.009295368
train_loss: 0.0058363955
test_loss: 0.009290064
train_loss: 0.0056159073
test_loss: 0.009059463
train_loss: 0.0054275747
test_loss: 0.009243063
train_loss: 0.005536981
test_loss: 0.0090877935
train_loss: 0.00607494
test_loss: 0.009536688
train_loss: 0.005314191
test_loss: 0.009253327
train_loss: 0.0055128713
test_loss: 0.009026404
train_loss: 0.005047673
test_loss: 0.008914675
train_loss: 0.0049016722
test_loss: 0.008986811
train_loss: 0.0056990944
test_loss: 0.009060535
train_loss: 0.0049011745
test_loss: 0.009166478
train_loss: 0.0050426745
test_loss: 0.008810946
train_loss: 0.005792767
test_loss: 0.009446043
train_loss: 0.0055217207
test_loss: 0.009038125
train_loss: 0.0053508473
test_loss: 0.009067039
train_loss: 0.005328361
test_loss: 0.009039725
train_loss: 0.005102032
test_loss: 0.009058149
train_loss: 0.005552184
test_loss: 0.009482208
train_loss: 0.0055225086
test_loss: 0.009388375
train_loss: 0.005472242
test_loss: 0.009078789
train_loss: 0.005196567
test_loss: 0.008876919
train_loss: 0.0051113567
test_loss: 0.009198219
train_loss: 0.006186881
test_loss: 0.009491576
train_loss: 0.005467424
test_loss: 0.009401194
train_loss: 0.005129791
test_loss: 0.009115912
train_loss: 0.005217506
test_loss: 0.00893539
train_loss: 0.0053664506
test_loss: 0.009127694
train_loss: 0.0050072493
test_loss: 0.009067264
train_loss: 0.0049768705
test_loss: 0.009214297
train_loss: 0.005339157
test_loss: 0.009200656
train_loss: 0.0051688743
test_loss: 0.009061948
train_loss: 0.0058928146
test_loss: 0.00943499
train_loss: 0.0052372036
test_loss: 0.009271741
train_loss: 0.0048646596
test_loss: 0.009210553
train_loss: 0.005233844
test_loss: 0.009066177
train_loss: 0.005565612
test_loss: 0.009179697
train_loss: 0.005139565
test_loss: 0.008901011
train_loss: 0.0047043995
test_loss: 0.008915963
train_loss: 0.005161483
test_loss: 0.008671917
train_loss: 0.005679846
test_loss: 0.009029563
train_loss: 0.00527384
test_loss: 0.009038208
train_loss: 0.0062967334
test_loss: 0.008962097
train_loss: 0.004924799
test_loss: 0.008925306
train_loss: 0.0054123956
test_loss: 0.00876549
train_loss: 0.005037665
test_loss: 0.008953572
train_loss: 0.004941027
test_loss: 0.009255951
train_loss: 0.005344037
test_loss: 0.0093694
train_loss: 0.0051600793
test_loss: 0.009003835
train_loss: 0.005364921
test_loss: 0.008959032
train_loss: 0.004983204
test_loss: 0.009013603
train_loss: 0.004870207
test_loss: 0.008671134
train_loss: 0.0048052035
test_loss: 0.008725939
train_loss: 0.0052872105
test_loss: 0.009236162
train_loss: 0.0054578986
test_loss: 0.00916567
train_loss: 0.004924884
test_loss: 0.0091108475
train_loss: 0.0051261466
test_loss: 0.008811498
train_loss: 0.005046176
test_loss: 0.008789525
train_loss: 0.005340217
test_loss: 0.009032154
train_loss: 0.005134946
test_loss: 0.009070542
train_loss: 0.0052444683
test_loss: 0.009099982
train_loss: 0.004857349
test_loss: 0.008858741
train_loss: 0.0052069887
test_loss: 0.009031855
train_loss: 0.0051660077
test_loss: 0.008923105
train_loss: 0.005092872
test_loss: 0.008964302
train_loss: 0.0057833744
test_loss: 0.008921821
train_loss: 0.0052964445
test_loss: 0.0091761425
train_loss: 0.0052528055
test_loss: 0.0090645645
train_loss: 0.005005919
test_loss: 0.008933916
train_loss: 0.0052562156
test_loss: 0.008932877
train_loss: 0.0053874794
test_loss: 0.008918093
train_loss: 0.005092026
test_loss: 0.008696788
train_loss: 0.00533054
test_loss: 0.008778208
train_loss: 0.005044678
test_loss: 0.009044456
train_loss: 0.0049811504
test_loss: 0.008856054
train_loss: 0.004981488
test_loss: 0.008615641
train_loss: 0.004712232
test_loss: 0.009020306
train_loss: 0.0048601325
test_loss: 0.008844542
train_loss: 0.004611123
test_loss: 0.009122985
train_loss: 0.005015743
test_loss: 0.008819464
train_loss: 0.0050530555
test_loss: 0.008774345
train_loss: 0.00500419
test_loss: 0.00902846
train_loss: 0.0049176943
test_loss: 0.009172339
train_loss: 0.005024343
test_loss: 0.008940451
train_loss: 0.0050802864
test_loss: 0.009176574
train_loss: 0.0053546336
test_loss: 0.008674
train_loss: 0.005241112
test_loss: 0.008655195
train_loss: 0.0054095015
test_loss: 0.008882292
train_loss: 0.00516283
test_loss: 0.0087918835
train_loss: 0.004688834
test_loss: 0.008737361
train_loss: 0.0054104156
test_loss: 0.009193454
train_loss: 0.0052240184
test_loss: 0.008906629
train_loss: 0.0049168807
test_loss: 0.009092576
train_loss: 0.005068517
test_loss: 0.008956843
train_loss: 0.0051394533
test_loss: 0.008872475
train_loss: 0.0051823184
test_loss: 0.008641337
train_loss: 0.0051258
test_loss: 0.008933845
train_loss: 0.005829287
test_loss: 0.008976279
train_loss: 0.0045923973
test_loss: 0.009057218
train_loss: 0.0054420982
test_loss: