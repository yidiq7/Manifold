+ RUN=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='2 3'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2
+ date
Wed Oct 21 09:35:39 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1 --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc3bc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc3bc1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc2f6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc25bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc255950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc166510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc1bcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc189ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc189730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc118b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0d3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0dd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc0a1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efbffea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc003b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc02f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5e3c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d8f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d8ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d53268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5d0ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5cc5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5cd36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5cd37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5dfc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5dfc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5c83ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5c09510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5c29f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3efc03f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5b8c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5b8c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5b8c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5bf5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3ef5bd7d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.019547626
test_loss: 0.019671382
train_loss: 0.0090725655
test_loss: 0.00898119
train_loss: 0.006269957
test_loss: 0.0066933804
train_loss: 0.005477133
test_loss: 0.0056914627
train_loss: 0.0053307153
test_loss: 0.005301539
train_loss: 0.005177132
test_loss: 0.0051679346
train_loss: 0.004584152
test_loss: 0.0050352486
train_loss: 0.004709363
test_loss: 0.0047426084
train_loss: 0.0045332955
test_loss: 0.004968934
train_loss: 0.004450911
test_loss: 0.004846005
train_loss: 0.0042535053
test_loss: 0.004670961
train_loss: 0.004462671
test_loss: 0.004772661
train_loss: 0.0041610645
test_loss: 0.0046513537
train_loss: 0.004110224
test_loss: 0.004519015
train_loss: 0.0041226554
test_loss: 0.0048108515
train_loss: 0.0041026324
test_loss: 0.0044956687
train_loss: 0.0039682505
test_loss: 0.004372526
train_loss: 0.004067778
test_loss: 0.004321856
train_loss: 0.0039699683
test_loss: 0.0043749553
train_loss: 0.003813988
test_loss: 0.0041737743
train_loss: 0.0040311324
test_loss: 0.0042153453
train_loss: 0.0039775805
test_loss: 0.0042263716
train_loss: 0.0038991314
test_loss: 0.004353083
train_loss: 0.004083889
test_loss: 0.0041781124
train_loss: 0.0038717086
test_loss: 0.004348058
train_loss: 0.004047476
test_loss: 0.0041527497
train_loss: 0.004040085
test_loss: 0.0044506085
train_loss: 0.0037939711
test_loss: 0.0041256505
train_loss: 0.0038523804
test_loss: 0.0042982325
train_loss: 0.003646867
test_loss: 0.004183979
train_loss: 0.003758031
test_loss: 0.004175936
train_loss: 0.0034912326
test_loss: 0.0041251816
train_loss: 0.0035836175
test_loss: 0.0042388123
train_loss: 0.003477535
test_loss: 0.003987771
train_loss: 0.0038018895
test_loss: 0.003973167
train_loss: 0.0035137008
test_loss: 0.003928891
train_loss: 0.00344377
test_loss: 0.0039027405
train_loss: 0.0034626015
test_loss: 0.0038776102
train_loss: 0.0036062435
test_loss: 0.0039891796
train_loss: 0.0034179867
test_loss: 0.0040519987
train_loss: 0.0034938576
test_loss: 0.003945753
train_loss: 0.0034357607
test_loss: 0.003971588
train_loss: 0.0036368994
test_loss: 0.0040100412
train_loss: 0.0035285421
test_loss: 0.0038912948
train_loss: 0.0034754837
test_loss: 0.004097174
train_loss: 0.0035042984
test_loss: 0.0039125034
train_loss: 0.0034002087
test_loss: 0.0037513967
train_loss: 0.0033820462
test_loss: 0.0039441707
train_loss: 0.0032757826
test_loss: 0.003755735
train_loss: 0.0035601624
test_loss: 0.0037517531
train_loss: 0.003427818
test_loss: 0.0037525482
train_loss: 0.0033806544
test_loss: 0.0038233057
train_loss: 0.003265006
test_loss: 0.0036814178
train_loss: 0.003451409
test_loss: 0.0037147857
train_loss: 0.0033702694
test_loss: 0.0038042273
train_loss: 0.0034005356
test_loss: 0.0040830285
train_loss: 0.0035511795
test_loss: 0.004015981
train_loss: 0.0032240718
test_loss: 0.0035995601
train_loss: 0.0033367996
test_loss: 0.0036661767
train_loss: 0.0032932581
test_loss: 0.0037487417
train_loss: 0.003391197
test_loss: 0.0038559055
train_loss: 0.0032253219
test_loss: 0.0037323972
train_loss: 0.0031579952
test_loss: 0.0035365745
train_loss: 0.0032422296
test_loss: 0.0035806636
train_loss: 0.0032111001
test_loss: 0.0038482659
train_loss: 0.003291798
test_loss: 0.0036947862
train_loss: 0.003109728
test_loss: 0.0036800758
train_loss: 0.0032820161
test_loss: 0.0036755698
train_loss: 0.003481783
test_loss: 0.0036724657
train_loss: 0.0029738753
test_loss: 0.0034515145
train_loss: 0.003355905
test_loss: 0.0037474388
train_loss: 0.003100234
test_loss: 0.0036929282
train_loss: 0.0031402444
test_loss: 0.0036745954
train_loss: 0.003534715
test_loss: 0.0035946153
train_loss: 0.003177234
test_loss: 0.0037799387
train_loss: 0.00330197
test_loss: 0.0037961465
train_loss: 0.002825023
test_loss: 0.0034027852
train_loss: 0.003039434
test_loss: 0.0035345107
train_loss: 0.0030412555
test_loss: 0.0036148957
train_loss: 0.0031319219
test_loss: 0.003370568
train_loss: 0.0031488682
test_loss: 0.003540186
train_loss: 0.0028720784
test_loss: 0.0035075166
train_loss: 0.0030095554
test_loss: 0.0036241552
train_loss: 0.0030054187
test_loss: 0.0036150832
train_loss: 0.0031567193
test_loss: 0.0035198943
train_loss: 0.0031195716
test_loss: 0.0035026195
train_loss: 0.003205
test_loss: 0.0035274802
train_loss: 0.0029213028
test_loss: 0.0035053417
train_loss: 0.0029996864
test_loss: 0.0035015382
train_loss: 0.0031255074
test_loss: 0.0037172693
train_loss: 0.0030857595
test_loss: 0.0036228434
train_loss: 0.0030892007
test_loss: 0.0035819702
train_loss: 0.0030290368
test_loss: 0.0035269032
train_loss: 0.0030759373
test_loss: 0.0035402868
train_loss: 0.0033104937
test_loss: 0.0037094196
train_loss: 0.0030366126
test_loss: 0.0037057057
train_loss: 0.0030122912
test_loss: 0.0036834236
train_loss: 0.0029721374
test_loss: 0.0035096689
train_loss: 0.0032497342
test_loss: 0.0034526417
train_loss: 0.0029798911
test_loss: 0.00347377
train_loss: 0.0027542568
test_loss: 0.003471824
train_loss: 0.0029923492
test_loss: 0.0035209518
train_loss: 0.0031998272
test_loss: 0.00355104
train_loss: 0.0032948111
test_loss: 0.003453693
train_loss: 0.0031483178
test_loss: 0.003475432
train_loss: 0.0033348862
test_loss: 0.0036273901
train_loss: 0.0030804984
test_loss: 0.0036315022
train_loss: 0.0032577193
test_loss: 0.0036114228
train_loss: 0.0031659636
test_loss: 0.003802141
train_loss: 0.0030901646
test_loss: 0.0035083375
train_loss: 0.0029157498
test_loss: 0.0034951034
train_loss: 0.0028182624
test_loss: 0.0033984596
train_loss: 0.0028133206
test_loss: 0.003326088
train_loss: 0.0029177275
test_loss: 0.0032958766
train_loss: 0.0029230635
test_loss: 0.0034473774
train_loss: 0.0030684215
test_loss: 0.0035568478
train_loss: 0.0033718667
test_loss: 0.0035185178
train_loss: 0.0029587739
test_loss: 0.0035073983
train_loss: 0.002879326
test_loss: 0.0034426556
train_loss: 0.002886285
test_loss: 0.003563428
train_loss: 0.0028952968
test_loss: 0.0034910107
train_loss: 0.0027492498
test_loss: 0.0035135318
train_loss: 0.0028979182
test_loss: 0.003454611
train_loss: 0.0030358643
test_loss: 0.0035228077
train_loss: 0.002753323
test_loss: 0.0033736704
train_loss: 0.0030144933
test_loss: 0.0034357475
train_loss: 0.0027485294
test_loss: 0.0035915799
train_loss: 0.0029275483
test_loss: 0.003461708
train_loss: 0.0026932012
test_loss: 0.0035387974
train_loss: 0.0029065844
test_loss: 0.003393748
train_loss: 0.002970633
test_loss: 0.0034050585
train_loss: 0.0031778896
test_loss: 0.0035654195
train_loss: 0.003054358
test_loss: 0.0034266133
train_loss: 0.0028854788
test_loss: 0.003346553
train_loss: 0.0028133374
test_loss: 0.0033250407
train_loss: 0.002825835
test_loss: 0.0033496371
train_loss: 0.002874393
test_loss: 0.0033193908
train_loss: 0.002926884
test_loss: 0.003607652
train_loss: 0.003126409
test_loss: 0.003431352
train_loss: 0.0030203417
test_loss: 0.0033926833
train_loss: 0.0028834217
test_loss: 0.003385762
train_loss: 0.0028411853
test_loss: 0.0033201354
train_loss: 0.0031297058
test_loss: 0.003518323
train_loss: 0.0028077648
test_loss: 0.0034006243
train_loss: 0.003007865
test_loss: 0.0033842642
train_loss: 0.0028573652
test_loss: 0.0034196775
train_loss: 0.002902547
test_loss: 0.0034724858
train_loss: 0.002824613
test_loss: 0.0032689595
train_loss: 0.002866327
test_loss: 0.003336344
train_loss: 0.0026882575
test_loss: 0.0032234036
train_loss: 0.0029545038
test_loss: 0.0033085416
train_loss: 0.0029573021
test_loss: 0.00350564
train_loss: 0.002799062
test_loss: 0.0034960934
train_loss: 0.0027994132
test_loss: 0.003284992
train_loss: 0.002702122
test_loss: 0.0033569399
train_loss: 0.0028348344
test_loss: 0.0033491221
train_loss: 0.0029710871
test_loss: 0.003295566
train_loss: 0.003029543
test_loss: 0.0035612963
train_loss: 0.0029697614
test_loss: 0.0034142302
train_loss: 0.002884876
test_loss: 0.003381833
train_loss: 0.0028450328
test_loss: 0.0033320785
train_loss: 0.0027937444
test_loss: 0.003335728
train_loss: 0.002887777
test_loss: 0.0032338463
train_loss: 0.002654045
test_loss: 0.0032966551
train_loss: 0.0029149055
test_loss: 0.0033503133
train_loss: 0.0026438679
test_loss: 0.003198512
train_loss: 0.0026022391
test_loss: 0.0032339573
train_loss: 0.0029360927
test_loss: 0.0031914106
train_loss: 0.0027526254
test_loss: 0.0033213145
train_loss: 0.0026618934
test_loss: 0.0032308914
train_loss: 0.0027489283
test_loss: 0.0033388557
train_loss: 0.003222722
test_loss: 0.0034210582
train_loss: 0.0034299411
test_loss: 0.0035947682
train_loss: 0.002843319
test_loss: 0.0035041643
train_loss: 0.0027061112
test_loss: 0.0033784953
train_loss: 0.003024421
test_loss: 0.0031818687
train_loss: 0.0030919171
test_loss: 0.0035386032
train_loss: 0.0028586753
test_loss: 0.0032383043
train_loss: 0.0027233376
test_loss: 0.0032418056
train_loss: 0.003011302
test_loss: 0.0033342505
train_loss: 0.0027760505
test_loss: 0.003261263
train_loss: 0.0030223336
test_loss: 0.0032019895
train_loss: 0.0026941323
test_loss: 0.0033408583
train_loss: 0.0028003599
test_loss: 0.0031526724
train_loss: 0.0029007941
test_loss: 0.0034161375
train_loss: 0.002626876
test_loss: 0.0033419884
train_loss: 0.002694023
test_loss: 0.003144593
train_loss: 0.00271307
test_loss: 0.003214994
train_loss: 0.0027537274
test_loss: 0.0031517926
train_loss: 0.0028093643
test_loss: 0.0031971983
train_loss: 0.0028385757
test_loss: 0.0032446086
train_loss: 0.0027733813
test_loss: 0.0034759967
train_loss: 0.0028015967
test_loss: 0.0034257513
train_loss: 0.002635348
test_loss: 0.0031476042
train_loss: 0.0023957689
test_loss: 0.0030443403
train_loss: 0.0026956652
test_loss: 0.0031948178
train_loss: 0.0027950779
test_loss: 0.0031661435
train_loss: 0.002696092
test_loss: 0.0032547778
train_loss: 0.0027798621
test_loss: 0.0032376836
train_loss: 0.002688308
test_loss: 0.003332539
train_loss: 0.0028173276
test_loss: 0.0031972623
train_loss: 0.0028132463
test_loss: 0.0033499089
train_loss: 0.0031623738
test_loss: 0.0033269152
train_loss: 0.002786164
test_loss: 0.0033834744
train_loss: 0.0031135888
test_loss: 0.0033840556
train_loss: 0.0026651553
test_loss: 0.0032382691
train_loss: 0.0026890999
test_loss: 0.003319003
train_loss: 0.002792087
test_loss: 0.0032163612
train_loss: 0.0026116234
test_loss: 0.0033806683
train_loss: 0.0026158623
test_loss: 0.0031239614
train_loss: 0.0026314107
test_loss: 0.0032441325
train_loss: 0.0026469356
test_loss: 0.0030629989
train_loss: 0.0025724557
test_loss: 0.00327408
train_loss: 0.0027236014
test_loss: 0.0032804494
train_loss: 0.0026818425
test_loss: 0.0032046246
train_loss: 0.0027574685
test_loss: 0.0032881144
train_loss: 0.0027022338
test_loss: 0.003108547
train_loss: 0.002743667
test_loss: 0.0033077
train_loss: 0.002777342
test_loss: 0.0033761356
train_loss: 0.002554835
test_loss: 0.0032199097
train_loss: 0.0026825909
test_loss: 0.0032697592
train_loss: 0.0026999621
test_loss: 0.0031754095
train_loss: 0.0024217698
test_loss: 0.0033690229
train_loss: 0.0025917653
test_loss: 0.0031233663
train_loss: 0.0028114829
test_loss: 0.0032382866
train_loss: 0.0027476624
test_loss: 0.003409831
train_loss: 0.0028319359
test_loss: 0.003225152
train_loss: 0.0025178567
test_loss: 0.003197765
train_loss: 0.0027305246
test_loss: 0.0032894956
train_loss: 0.002803234
test_loss: 0.0034056862
train_loss: 0.0028627482
test_loss: 0.0032638193
train_loss: 0.0029985865
test_loss: 0.0033122015
train_loss: 0.0028155248
test_loss: 0.0033387826
train_loss: 0.0027036667
test_loss: 0.0032714938
train_loss: 0.0028630737
test_loss: 0.0032813158
train_loss: 0.0026126604
test_loss: 0.0031988195
train_loss: 0.0026209902
test_loss: 0.0032417884
train_loss: 0.0027234002
test_loss: 0.0033010526
train_loss: 0.0026880282
test_loss: 0.0032584174
train_loss: 0.002762279
test_loss: 0.003310852
train_loss: 0.002726587
test_loss: 0.0032861647
train_loss: 0.0026257313
test_loss: 0.003142028
train_loss: 0.0025778664
test_loss: 0.0030626336
train_loss: 0.0025943327
test_loss: 0.0031601323
train_loss: 0.0026659602
test_loss: 0.0032112047
train_loss: 0.0025574851
test_loss: 0.0032067134
train_loss: 0.00271732
test_loss: 0.0032462399
train_loss: 0.0025556935
test_loss: 0.003166267
train_loss: 0.0025719004
test_loss: 0.0032335941
train_loss: 0.0027727224
test_loss: 0.0033520472
train_loss: 0.0026562877
test_loss: 0.0031408158
train_loss: 0.0026923446
test_loss: 0.003292029
train_loss: 0.0025160247
test_loss: 0.0031453564
train_loss: 0.0024189523
test_loss: 0.003115839
train_loss: 0.0026687554
test_loss: 0.0031823325
train_loss: 0.002682491
test_loss: 0.0031937123
train_loss: 0.0025997362
test_loss: 0.003151958
train_loss: 0.0027012008
test_loss: 0.0031147015
train_loss: 0.0025992217
test_loss: 0.003533864
train_loss: 0.0025204343
test_loss: 0.0032185288
train_loss: 0.0027215588
test_loss: 0.0032751097
train_loss: 0.0025348132
test_loss: 0.0031375815
train_loss: 0.0028095096
test_loss: 0.0032115935
train_loss: 0.0025713623
test_loss: 0.0032993476
train_loss: 0.002621413
test_loss: 0.0031954846
train_loss: 0.0024814971
test_loss: 0.0030397547
train_loss: 0.0026260803
test_loss: 0.0031995226
train_loss: 0.0025979038
test_loss: 0.0031914676
train_loss: 0.0025971734
test_loss: 0.003248692
train_loss: 0.0030938676
test_loss: 0.0034732234
train_loss: 0.0026483994
test_loss: 0.0032684952
train_loss: 0.0027586455
test_loss: 0.0033304596
train_loss: 0.0028772866
test_loss: 0.003260725
train_loss: 0.0027161753
test_loss: 0.0031898678
train_loss: 0.002536872
test_loss: 0.0031462116
train_loss: 0.002630613
test_loss: 0.0031388162
train_loss: 0.0025586563
test_loss: 0.003173081
train_loss: 0.0028531898
test_loss: 0.0031996535
train_loss: 0.0027549951
test_loss: 0.0033838912
train_loss: 0.0025846928
test_loss: 0.003104193
train_loss: 0.0024986614
test_loss: 0.0033008936
train_loss: 0.002775758
test_loss: 0.0032895943
train_loss: 0.002772307
test_loss: 0.0033783403
train_loss: 0.002718167
test_loss: 0.0032204606
train_loss: 0.0027224112
test_loss: 0.0030071212
train_loss: 0.0024644127
test_loss: 0.0031549106
train_loss: 0.002532889
test_loss: 0.0030257497
train_loss: 0.0024496533
test_loss: 0.0031925093
train_loss: 0.0023963158
test_loss: 0.0029779105
train_loss: 0.0024233332
test_loss: 0.0030250254
train_loss: 0.002536714
test_loss: 0.003176634
train_loss: 0.0027666967
test_loss: 0.0032432538
train_loss: 0.0024197388
test_loss: 0.003253465
train_loss: 0.0029169878
test_loss: 0.0034578792
train_loss: 0.0027120921
test_loss: 0.0031953193
train_loss: 0.0025132964
test_loss: 0.0030944238
train_loss: 0.002676434
test_loss: 0.0030667614
train_loss: 0.0023834049
test_loss: 0.00330171
train_loss: 0.0024319836
test_loss: 0.003156773
train_loss: 0.0024584215
test_loss: 0.0030539038
train_loss: 0.0027637458
test_loss: 0.0031473078
train_loss: 0.002597585
test_loss: 0.0032146797
train_loss: 0.0026314098
test_loss: 0.0032535386
train_loss: 0.0028311112
test_loss: 0.0031532736
train_loss: 0.0027307014
test_loss: 0.003292303
train_loss: 0.00267726
test_loss: 0.003286467
train_loss: 0.0026124779
test_loss: 0.0033061751
train_loss: 0.0025667772
test_loss: 0.0031463292
train_loss: 0.0025587752
test_loss: 0.0031405352
train_loss: 0.0024749776
test_loss: 0.0030493222
train_loss: 0.0025178455
test_loss: 0.0031446076
train_loss: 0.0024154168
test_loss: 0.0031210517
train_loss: 0.002609696
test_loss: 0.003080377
train_loss: 0.0026693458
test_loss: 0.0031875062
train_loss: 0.0025300256
test_loss: 0.0031683906
train_loss: 0.0024807556
test_loss: 0.003110037
train_loss: 0.0024542338
test_loss: 0.0030096434
train_loss: 0.002450978
test_loss: 0.0029865035
train_loss: 0.0026240128
test_loss: 0.0030784276
train_loss: 0.0025440054
test_loss: 0.0032461074
train_loss: 0.002582653
test_loss: 0.0030958874
train_loss: 0.002752099
test_loss: 0.0031691738
train_loss: 0.00269428
test_loss: 0.0030995032
train_loss: 0.0025932903
test_loss: 0.003092225
train_loss: 0.002448188
test_loss: 0.00313418
train_loss: 0.0025063078
test_loss: 0.0029509466
train_loss: 0.0024693797
test_loss: 0.0029468816
train_loss: 0.00262089
test_loss: 0.003243929
train_loss: 0.002646273
test_loss: 0.003057043
train_loss: 0.0025041103
test_loss: 0.0030433333
train_loss: 0.0029375395
test_loss: 0.0035068288
train_loss: 0.0024414447
test_loss: 0.0029233298
train_loss: 0.002566438
test_loss: 0.0032200178
train_loss: 0.0025737358
test_loss: 0.0033183282
train_loss: 0.0024729944
test_loss: 0.0032712352
train_loss: 0.0027658953
test_loss: 0.003351998
train_loss: 0.0025987923
test_loss: 0.0031846971
train_loss: 0.0026269965
test_loss: 0.0032000847
train_loss: 0.0024411494/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.

test_loss: 0.0032126354
train_loss: 0.002560078
test_loss: 0.003026019
train_loss: 0.002644809
test_loss: 0.003232324
train_loss: 0.0023800246
test_loss: 0.0029141563
train_loss: 0.0025498027
test_loss: 0.0029841291
train_loss: 0.0026895276
test_loss: 0.0031211667
train_loss: 0.0027047307
test_loss: 0.0032030554
train_loss: 0.0024833623
test_loss: 0.0031813611
train_loss: 0.0025495898
test_loss: 0.0030673563
train_loss: 0.0027053724
test_loss: 0.0033673234
train_loss: 0.002633778
test_loss: 0.0030812847
train_loss: 0.0025631562
test_loss: 0.0031251484
train_loss: 0.0024574527
test_loss: 0.003230523
train_loss: 0.002821933
test_loss: 0.003120549
train_loss: 0.0025756587
test_loss: 0.0031189083
train_loss: 0.00249699
test_loss: 0.0031774729
train_loss: 0.0026065325
test_loss: 0.0030327952
train_loss: 0.0025449756
test_loss: 0.0031637473
train_loss: 0.0026330429
test_loss: 0.003210316
train_loss: 0.0025113681
test_loss: 0.0030163305
train_loss: 0.002590535
test_loss: 0.003071573
train_loss: 0.0024417685
test_loss: 0.0031393494
train_loss: 0.0023713158
test_loss: 0.0029934049
train_loss: 0.0025528274
test_loss: 0.0029964452
train_loss: 0.0026006075
test_loss: 0.0032278167
train_loss: 0.0024897803
test_loss: 0.0030757496
train_loss: 0.002413476
test_loss: 0.0031063438
train_loss: 0.0025376377
test_loss: 0.0030348764
train_loss: 0.0024660714
test_loss: 0.0030175198
train_loss: 0.002412226
test_loss: 0.0030282724
train_loss: 0.00232203
test_loss: 0.002991953
train_loss: 0.0024197064
test_loss: 0.0031470598
train_loss: 0.0025783249
test_loss: 0.0031458843
train_loss: 0.0024773595
test_loss: 0.0030919118
train_loss: 0.0024919591
test_loss: 0.003150029
train_loss: 0.0025356773
test_loss: 0.0031160566
train_loss: 0.0025923483
test_loss: 0.0031655896
train_loss: 0.0024821889
test_loss: 0.0031022725
train_loss: 0.002557021
test_loss: 0.0029873028
train_loss: 0.002798224
test_loss: 0.0032589764
train_loss: 0.0027342234
test_loss: 0.003073332
train_loss: 0.0024486268
test_loss: 0.0030973046
train_loss: 0.002436045
test_loss: 0.0030808318
train_loss: 0.0029918118
test_loss: 0.0031587463
train_loss: 0.0025038186
test_loss: 0.00307083
train_loss: 0.0024309482
test_loss: 0.002891278
train_loss: 0.0023055123
test_loss: 0.0028691937
train_loss: 0.0022931718
test_loss: 0.0029486648
train_loss: 0.0024666006
test_loss: 0.0031697948
train_loss: 0.002434828
test_loss: 0.0031721594
train_loss: 0.002684794
test_loss: 0.0031193423
train_loss: 0.0025489626
test_loss: 0.0031713264
train_loss: 0.0026219536
test_loss: 0.0031988425
train_loss: 0.0027065915
test_loss: 0.003382226
train_loss: 0.0026149082
test_loss: 0.0031611882
train_loss: 0.002571876
test_loss: 0.003026249
train_loss: 0.0025392056
test_loss: 0.0030589036
train_loss: 0.0024663433
test_loss: 0.0030268452
train_loss: 0.0025131316
test_loss: 0.0030573246
train_loss: 0.0024927997
test_loss: 0.0030460132
train_loss: 0.0024919226
test_loss: 0.0030590286
train_loss: 0.002466178
test_loss: 0.0032317145
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f301d82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f301d8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f302ad048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f301e0e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30215378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f3016ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30157840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300fd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300fdd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300fd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300c4048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30098620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300c46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f30056840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f300560d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10094488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10094158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10050400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10012378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1f10039f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e647cf950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e647cfc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e647a8158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64751268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64751ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646fe6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646cc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646ccd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646f6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646cce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e6464d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64672950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e646727b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64635378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e645d32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1e64579268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.20475843e-05
Iter: 2 loss: 1.07282967e-05
Iter: 3 loss: 9.60075886e-06
Iter: 4 loss: 8.76355e-06
Iter: 5 loss: 8.44306851e-06
Iter: 6 loss: 7.98728706e-06
Iter: 7 loss: 7.4197319e-06
Iter: 8 loss: 9.58292458e-06
Iter: 9 loss: 7.28441273e-06
Iter: 10 loss: 6.83092367e-06
Iter: 11 loss: 1.06186853e-05
Iter: 12 loss: 6.80403264e-06
Iter: 13 loss: 6.48051855e-06
Iter: 14 loss: 6.70939426e-06
Iter: 15 loss: 6.27997815e-06
Iter: 16 loss: 5.95777192e-06
Iter: 17 loss: 6.47154593e-06
Iter: 18 loss: 5.80829874e-06
Iter: 19 loss: 5.47589116e-06
Iter: 20 loss: 8.82174572e-06
Iter: 21 loss: 5.46545198e-06
Iter: 22 loss: 5.24184543e-06
Iter: 23 loss: 4.84430711e-06
Iter: 24 loss: 1.47764376e-05
Iter: 25 loss: 4.84428438e-06
Iter: 26 loss: 4.74110402e-06
Iter: 27 loss: 4.66936945e-06
Iter: 28 loss: 4.50187054e-06
Iter: 29 loss: 4.385377e-06
Iter: 30 loss: 4.32525258e-06
Iter: 31 loss: 4.14454962e-06
Iter: 32 loss: 4.28057683e-06
Iter: 33 loss: 4.03414197e-06
Iter: 34 loss: 3.81890459e-06
Iter: 35 loss: 3.86415877e-06
Iter: 36 loss: 3.65972119e-06
Iter: 37 loss: 3.63894878e-06
Iter: 38 loss: 3.55120392e-06
Iter: 39 loss: 3.43443344e-06
Iter: 40 loss: 3.33964408e-06
Iter: 41 loss: 3.30505873e-06
Iter: 42 loss: 3.15753778e-06
Iter: 43 loss: 3.27017437e-06
Iter: 44 loss: 3.06776656e-06
Iter: 45 loss: 3.03495631e-06
Iter: 46 loss: 2.99806379e-06
Iter: 47 loss: 2.95365726e-06
Iter: 48 loss: 2.87242142e-06
Iter: 49 loss: 4.77785488e-06
Iter: 50 loss: 2.87232206e-06
Iter: 51 loss: 2.78791458e-06
Iter: 52 loss: 3.51804e-06
Iter: 53 loss: 2.78344305e-06
Iter: 54 loss: 2.72244324e-06
Iter: 55 loss: 2.91235574e-06
Iter: 56 loss: 2.70460851e-06
Iter: 57 loss: 2.63651282e-06
Iter: 58 loss: 2.62994104e-06
Iter: 59 loss: 2.57990655e-06
Iter: 60 loss: 2.51113829e-06
Iter: 61 loss: 2.82366591e-06
Iter: 62 loss: 2.49793084e-06
Iter: 63 loss: 2.4140179e-06
Iter: 64 loss: 2.75533148e-06
Iter: 65 loss: 2.39547717e-06
Iter: 66 loss: 2.35556877e-06
Iter: 67 loss: 2.31339232e-06
Iter: 68 loss: 2.30621163e-06
Iter: 69 loss: 2.24544442e-06
Iter: 70 loss: 2.40341865e-06
Iter: 71 loss: 2.22477183e-06
Iter: 72 loss: 2.2070526e-06
Iter: 73 loss: 2.19802564e-06
Iter: 74 loss: 2.16815056e-06
Iter: 75 loss: 2.11784618e-06
Iter: 76 loss: 2.1176902e-06
Iter: 77 loss: 2.07874973e-06
Iter: 78 loss: 2.15191017e-06
Iter: 79 loss: 2.06220489e-06
Iter: 80 loss: 2.02366618e-06
Iter: 81 loss: 2.6221287e-06
Iter: 82 loss: 2.02366255e-06
Iter: 83 loss: 1.99591295e-06
Iter: 84 loss: 1.97023201e-06
Iter: 85 loss: 1.96364158e-06
Iter: 86 loss: 1.93588767e-06
Iter: 87 loss: 2.24007727e-06
Iter: 88 loss: 1.93526239e-06
Iter: 89 loss: 1.91058234e-06
Iter: 90 loss: 1.93829646e-06
Iter: 91 loss: 1.89719231e-06
Iter: 92 loss: 1.87156309e-06
Iter: 93 loss: 1.92643438e-06
Iter: 94 loss: 1.86156149e-06
Iter: 95 loss: 1.83688212e-06
Iter: 96 loss: 1.9255026e-06
Iter: 97 loss: 1.83061388e-06
Iter: 98 loss: 1.80538109e-06
Iter: 99 loss: 1.96951805e-06
Iter: 100 loss: 1.80270717e-06
Iter: 101 loss: 1.78597838e-06
Iter: 102 loss: 1.75176069e-06
Iter: 103 loss: 2.35989933e-06
Iter: 104 loss: 1.75104299e-06
Iter: 105 loss: 1.71490774e-06
Iter: 106 loss: 1.82133545e-06
Iter: 107 loss: 1.70372925e-06
Iter: 108 loss: 1.69595592e-06
Iter: 109 loss: 1.68801103e-06
Iter: 110 loss: 1.67285816e-06
Iter: 111 loss: 1.66232689e-06
Iter: 112 loss: 1.65688994e-06
Iter: 113 loss: 1.64021253e-06
Iter: 114 loss: 1.6363706e-06
Iter: 115 loss: 1.62562787e-06
Iter: 116 loss: 1.61658863e-06
Iter: 117 loss: 1.61360481e-06
Iter: 118 loss: 1.60305103e-06
Iter: 119 loss: 1.58066302e-06
Iter: 120 loss: 1.94111522e-06
Iter: 121 loss: 1.57991894e-06
Iter: 122 loss: 1.56510225e-06
Iter: 123 loss: 1.753883e-06
Iter: 124 loss: 1.56503688e-06
Iter: 125 loss: 1.54793599e-06
Iter: 126 loss: 1.53968915e-06
Iter: 127 loss: 1.53146607e-06
Iter: 128 loss: 1.51436882e-06
Iter: 129 loss: 1.63492257e-06
Iter: 130 loss: 1.5128785e-06
Iter: 131 loss: 1.50066421e-06
Iter: 132 loss: 1.55946077e-06
Iter: 133 loss: 1.49848347e-06
Iter: 134 loss: 1.48522565e-06
Iter: 135 loss: 1.50683718e-06
Iter: 136 loss: 1.47913545e-06
Iter: 137 loss: 1.46716707e-06
Iter: 138 loss: 1.4712864e-06
Iter: 139 loss: 1.45866852e-06
Iter: 140 loss: 1.44668047e-06
Iter: 141 loss: 1.43987006e-06
Iter: 142 loss: 1.43469867e-06
Iter: 143 loss: 1.43766715e-06
Iter: 144 loss: 1.42668432e-06
Iter: 145 loss: 1.42073657e-06
Iter: 146 loss: 1.40547809e-06
Iter: 147 loss: 1.53088683e-06
Iter: 148 loss: 1.40277632e-06
Iter: 149 loss: 1.38673431e-06
Iter: 150 loss: 1.4671441e-06
Iter: 151 loss: 1.38406631e-06
Iter: 152 loss: 1.37535835e-06
Iter: 153 loss: 1.37499774e-06
Iter: 154 loss: 1.36811445e-06
Iter: 155 loss: 1.35532036e-06
Iter: 156 loss: 1.63853383e-06
Iter: 157 loss: 1.3552816e-06
Iter: 158 loss: 1.34745403e-06
Iter: 159 loss: 1.44916498e-06
Iter: 160 loss: 1.34741845e-06
Iter: 161 loss: 1.33922572e-06
Iter: 162 loss: 1.3501882e-06
Iter: 163 loss: 1.33510173e-06
Iter: 164 loss: 1.32906678e-06
Iter: 165 loss: 1.32855871e-06
Iter: 166 loss: 1.32411787e-06
Iter: 167 loss: 1.31238835e-06
Iter: 168 loss: 1.36544054e-06
Iter: 169 loss: 1.31012712e-06
Iter: 170 loss: 1.2997973e-06
Iter: 171 loss: 1.32306889e-06
Iter: 172 loss: 1.2958867e-06
Iter: 173 loss: 1.28872193e-06
Iter: 174 loss: 1.29362309e-06
Iter: 175 loss: 1.28430418e-06
Iter: 176 loss: 1.27556814e-06
Iter: 177 loss: 1.28879969e-06
Iter: 178 loss: 1.27144017e-06
Iter: 179 loss: 1.27102862e-06
Iter: 180 loss: 1.26737245e-06
Iter: 181 loss: 1.26496093e-06
Iter: 182 loss: 1.25816371e-06
Iter: 183 loss: 1.2896171e-06
Iter: 184 loss: 1.25569e-06
Iter: 185 loss: 1.2480632e-06
Iter: 186 loss: 1.28360261e-06
Iter: 187 loss: 1.24657504e-06
Iter: 188 loss: 1.2391564e-06
Iter: 189 loss: 1.33258254e-06
Iter: 190 loss: 1.23910354e-06
Iter: 191 loss: 1.23403618e-06
Iter: 192 loss: 1.22424376e-06
Iter: 193 loss: 1.43131433e-06
Iter: 194 loss: 1.22418305e-06
Iter: 195 loss: 1.21736889e-06
Iter: 196 loss: 1.21738663e-06
Iter: 197 loss: 1.2118885e-06
Iter: 198 loss: 1.2372343e-06
Iter: 199 loss: 1.21080677e-06
Iter: 200 loss: 1.20751577e-06
Iter: 201 loss: 1.20309801e-06
Iter: 202 loss: 1.20285802e-06
Iter: 203 loss: 1.19759238e-06
Iter: 204 loss: 1.19753395e-06
Iter: 205 loss: 1.19457241e-06
Iter: 206 loss: 1.19063975e-06
Iter: 207 loss: 1.19039487e-06
Iter: 208 loss: 1.18433707e-06
Iter: 209 loss: 1.19288939e-06
Iter: 210 loss: 1.18139167e-06
Iter: 211 loss: 1.17410252e-06
Iter: 212 loss: 1.19245806e-06
Iter: 213 loss: 1.17158902e-06
Iter: 214 loss: 1.16682452e-06
Iter: 215 loss: 1.16640706e-06
Iter: 216 loss: 1.16383706e-06
Iter: 217 loss: 1.15817068e-06
Iter: 218 loss: 1.24125063e-06
Iter: 219 loss: 1.15792045e-06
Iter: 220 loss: 1.153021e-06
Iter: 221 loss: 1.18459025e-06
Iter: 222 loss: 1.15247281e-06
Iter: 223 loss: 1.14899831e-06
Iter: 224 loss: 1.14901673e-06
Iter: 225 loss: 1.14680677e-06
Iter: 226 loss: 1.14119121e-06
Iter: 227 loss: 1.18980392e-06
Iter: 228 loss: 1.14029308e-06
Iter: 229 loss: 1.13488841e-06
Iter: 230 loss: 1.20012896e-06
Iter: 231 loss: 1.13481724e-06
Iter: 232 loss: 1.12963914e-06
Iter: 233 loss: 1.14993168e-06
Iter: 234 loss: 1.12850535e-06
Iter: 235 loss: 1.12510497e-06
Iter: 236 loss: 1.12259909e-06
Iter: 237 loss: 1.12143118e-06
Iter: 238 loss: 1.11742122e-06
Iter: 239 loss: 1.11732288e-06
Iter: 240 loss: 1.11504505e-06
Iter: 241 loss: 1.11217935e-06
Iter: 242 loss: 1.11196937e-06
Iter: 243 loss: 1.10903034e-06
Iter: 244 loss: 1.11743475e-06
Iter: 245 loss: 1.10817564e-06
Iter: 246 loss: 1.10476765e-06
Iter: 247 loss: 1.11935583e-06
Iter: 248 loss: 1.10402993e-06
Iter: 249 loss: 1.10120106e-06
Iter: 250 loss: 1.13610395e-06
Iter: 251 loss: 1.10118481e-06
Iter: 252 loss: 1.09937264e-06
Iter: 253 loss: 1.09437758e-06
Iter: 254 loss: 1.12036093e-06
Iter: 255 loss: 1.09275402e-06
Iter: 256 loss: 1.08749214e-06
Iter: 257 loss: 1.15216324e-06
Iter: 258 loss: 1.08741472e-06
Iter: 259 loss: 1.08416725e-06
Iter: 260 loss: 1.13504871e-06
Iter: 261 loss: 1.0841718e-06
Iter: 262 loss: 1.08199515e-06
Iter: 263 loss: 1.07757057e-06
Iter: 264 loss: 1.15942134e-06
Iter: 265 loss: 1.07748133e-06
Iter: 266 loss: 1.07489404e-06
Iter: 267 loss: 1.10307724e-06
Iter: 268 loss: 1.07484777e-06
Iter: 269 loss: 1.07211781e-06
Iter: 270 loss: 1.08216e-06
Iter: 271 loss: 1.07144217e-06
Iter: 272 loss: 1.06949744e-06
Iter: 273 loss: 1.0674446e-06
Iter: 274 loss: 1.06706091e-06
Iter: 275 loss: 1.0640648e-06
Iter: 276 loss: 1.06407674e-06
Iter: 277 loss: 1.06195876e-06
Iter: 278 loss: 1.05840888e-06
Iter: 279 loss: 1.058409e-06
Iter: 280 loss: 1.05409549e-06
Iter: 281 loss: 1.05436379e-06
Iter: 282 loss: 1.05064498e-06
Iter: 283 loss: 1.05076776e-06
Iter: 284 loss: 1.04845958e-06
Iter: 285 loss: 1.04695312e-06
Iter: 286 loss: 1.05032268e-06
Iter: 287 loss: 1.04634546e-06
Iter: 288 loss: 1.04475464e-06
Iter: 289 loss: 1.04200194e-06
Iter: 290 loss: 1.04201672e-06
Iter: 291 loss: 1.03918035e-06
Iter: 292 loss: 1.05110621e-06
Iter: 293 loss: 1.03858952e-06
Iter: 294 loss: 1.036409e-06
Iter: 295 loss: 1.03641401e-06
Iter: 296 loss: 1.03492653e-06
Iter: 297 loss: 1.03164211e-06
Iter: 298 loss: 1.08116103e-06
Iter: 299 loss: 1.03153377e-06
Iter: 300 loss: 1.02828812e-06
Iter: 301 loss: 1.0423056e-06
Iter: 302 loss: 1.02760168e-06
Iter: 303 loss: 1.02464878e-06
Iter: 304 loss: 1.06251821e-06
Iter: 305 loss: 1.024624e-06
Iter: 306 loss: 1.02299077e-06
Iter: 307 loss: 1.02118599e-06
Iter: 308 loss: 1.0209335e-06
Iter: 309 loss: 1.01933176e-06
Iter: 310 loss: 1.01923308e-06
Iter: 311 loss: 1.01785849e-06
Iter: 312 loss: 1.01501041e-06
Iter: 313 loss: 1.06120854e-06
Iter: 314 loss: 1.01494095e-06
Iter: 315 loss: 1.01266767e-06
Iter: 316 loss: 1.02043202e-06
Iter: 317 loss: 1.01208389e-06
Iter: 318 loss: 1.01018236e-06
Iter: 319 loss: 1.03604202e-06
Iter: 320 loss: 1.01018134e-06
Iter: 321 loss: 1.00811917e-06
Iter: 322 loss: 1.00815328e-06
Iter: 323 loss: 1.00646434e-06
Iter: 324 loss: 1.00431521e-06
Iter: 325 loss: 1.00688862e-06
Iter: 326 loss: 1.00315833e-06
Iter: 327 loss: 1.00069826e-06
Iter: 328 loss: 1.0058211e-06
Iter: 329 loss: 9.99724193e-07
Iter: 330 loss: 9.98282644e-07
Iter: 331 loss: 9.98156338e-07
Iter: 332 loss: 9.97059374e-07
Iter: 333 loss: 9.94639322e-07
Iter: 334 loss: 1.02983267e-06
Iter: 335 loss: 9.94525e-07
Iter: 336 loss: 9.9239935e-07
Iter: 337 loss: 1.0041515e-06
Iter: 338 loss: 9.92098e-07
Iter: 339 loss: 9.90394255e-07
Iter: 340 loss: 1.01170406e-06
Iter: 341 loss: 9.9039562e-07
Iter: 342 loss: 9.88985903e-07
Iter: 343 loss: 9.86529585e-07
Iter: 344 loss: 9.86527311e-07
Iter: 345 loss: 9.84743224e-07
Iter: 346 loss: 9.84674671e-07
Iter: 347 loss: 9.82872621e-07
Iter: 348 loss: 9.79462698e-07
Iter: 349 loss: 1.05569461e-06
Iter: 350 loss: 9.79477818e-07
Iter: 351 loss: 9.77143e-07
Iter: 352 loss: 9.84179451e-07
Iter: 353 loss: 9.76425554e-07
Iter: 354 loss: 9.75107e-07
Iter: 355 loss: 9.74920795e-07
Iter: 356 loss: 9.73625674e-07
Iter: 357 loss: 9.72241423e-07
Iter: 358 loss: 9.72034741e-07
Iter: 359 loss: 9.70503379e-07
Iter: 360 loss: 9.72481757e-07
Iter: 361 loss: 9.69742587e-07
Iter: 362 loss: 9.67872438e-07
Iter: 363 loss: 9.76287e-07
Iter: 364 loss: 9.67468509e-07
Iter: 365 loss: 9.66048333e-07
Iter: 366 loss: 9.83036216e-07
Iter: 367 loss: 9.66036851e-07
Iter: 368 loss: 9.65064146e-07
Iter: 369 loss: 9.62922172e-07
Iter: 370 loss: 9.96670678e-07
Iter: 371 loss: 9.62870331e-07
Iter: 372 loss: 9.60935381e-07
Iter: 373 loss: 9.73162e-07
Iter: 374 loss: 9.60731541e-07
Iter: 375 loss: 9.59026806e-07
Iter: 376 loss: 9.76691581e-07
Iter: 377 loss: 9.58983492e-07
Iter: 378 loss: 9.58104238e-07
Iter: 379 loss: 9.57691555e-07
Iter: 380 loss: 9.57237262e-07
Iter: 381 loss: 9.56138138e-07
Iter: 382 loss: 9.69541702e-07
Iter: 383 loss: 9.56112444e-07
Iter: 384 loss: 9.5504015e-07
Iter: 385 loss: 9.53076494e-07
Iter: 386 loss: 9.98296e-07
Iter: 387 loss: 9.5307314e-07
Iter: 388 loss: 9.51441962e-07
Iter: 389 loss: 9.54883717e-07
Iter: 390 loss: 9.50838626e-07
Iter: 391 loss: 9.4944761e-07
Iter: 392 loss: 9.49391733e-07
Iter: 393 loss: 9.48482352e-07
Iter: 394 loss: 9.46378236e-07
Iter: 395 loss: 9.72584075e-07
Iter: 396 loss: 9.46225e-07
Iter: 397 loss: 9.44400767e-07
Iter: 398 loss: 9.58626742e-07
Iter: 399 loss: 9.44276053e-07
Iter: 400 loss: 9.42875886e-07
Iter: 401 loss: 9.5145225e-07
Iter: 402 loss: 9.42692964e-07
Iter: 403 loss: 9.41530857e-07
Iter: 404 loss: 9.45919851e-07
Iter: 405 loss: 9.41229644e-07
Iter: 406 loss: 9.40205382e-07
Iter: 407 loss: 9.3888076e-07
Iter: 408 loss: 9.3883034e-07
Iter: 409 loss: 9.36788922e-07
Iter: 410 loss: 9.40335e-07
Iter: 411 loss: 9.35840831e-07
Iter: 412 loss: 9.34671903e-07
Iter: 413 loss: 9.34489947e-07
Iter: 414 loss: 9.33620413e-07
Iter: 415 loss: 9.31887143e-07
Iter: 416 loss: 9.64431592e-07
Iter: 417 loss: 9.31864065e-07
Iter: 418 loss: 9.30779834e-07
Iter: 419 loss: 9.30615101e-07
Iter: 420 loss: 9.29722262e-07
Iter: 421 loss: 9.28192321e-07
Iter: 422 loss: 9.28185614e-07
Iter: 423 loss: 9.27033113e-07
Iter: 424 loss: 9.3496908e-07
Iter: 425 loss: 9.26901691e-07
Iter: 426 loss: 9.25723612e-07
Iter: 427 loss: 9.34999889e-07
Iter: 428 loss: 9.25642837e-07
Iter: 429 loss: 9.24928713e-07
Iter: 430 loss: 9.23368589e-07
Iter: 431 loss: 9.47222532e-07
Iter: 432 loss: 9.23295602e-07
Iter: 433 loss: 9.21843309e-07
Iter: 434 loss: 9.29808095e-07
Iter: 435 loss: 9.21674427e-07
Iter: 436 loss: 9.20089576e-07
Iter: 437 loss: 9.28043903e-07
Iter: 438 loss: 9.19841398e-07
Iter: 439 loss: 9.18676733e-07
Iter: 440 loss: 9.21139531e-07
Iter: 441 loss: 9.18234264e-07
Iter: 442 loss: 9.17117632e-07
Iter: 443 loss: 9.16168e-07
Iter: 444 loss: 9.15793635e-07
Iter: 445 loss: 9.14305588e-07
Iter: 446 loss: 9.27926578e-07
Iter: 447 loss: 9.1427404e-07
Iter: 448 loss: 9.13285589e-07
Iter: 449 loss: 9.24435085e-07
Iter: 450 loss: 9.13268e-07
Iter: 451 loss: 9.1255805e-07
Iter: 452 loss: 9.10904362e-07
Iter: 453 loss: 9.32707394e-07
Iter: 454 loss: 9.10812503e-07
Iter: 455 loss: 9.09750838e-07
Iter: 456 loss: 9.0956928e-07
Iter: 457 loss: 9.08849131e-07
Iter: 458 loss: 9.07273659e-07
Iter: 459 loss: 9.33617e-07
Iter: 460 loss: 9.07263029e-07
Iter: 461 loss: 9.06135767e-07
Iter: 462 loss: 9.06131334e-07
Iter: 463 loss: 9.04861622e-07
Iter: 464 loss: 9.06246e-07
Iter: 465 loss: 9.04200363e-07
Iter: 466 loss: 9.03314799e-07
Iter: 467 loss: 9.02596867e-07
Iter: 468 loss: 9.02321744e-07
Iter: 469 loss: 9.01238081e-07
Iter: 470 loss: 9.06204605e-07
Iter: 471 loss: 9.01024634e-07
Iter: 472 loss: 8.99725364e-07
Iter: 473 loss: 9.08971799e-07
Iter: 474 loss: 8.99615941e-07
Iter: 475 loss: 8.98804387e-07
Iter: 476 loss: 8.98108397e-07
Iter: 477 loss: 8.97889095e-07
Iter: 478 loss: 8.96440781e-07
Iter: 479 loss: 8.98965141e-07
Iter: 480 loss: 8.95753033e-07
Iter: 481 loss: 8.94562163e-07
Iter: 482 loss: 9.02938041e-07
Iter: 483 loss: 8.94429036e-07
Iter: 484 loss: 8.93242941e-07
Iter: 485 loss: 8.98788244e-07
Iter: 486 loss: 8.93023298e-07
Iter: 487 loss: 8.92223113e-07
Iter: 488 loss: 8.92562923e-07
Iter: 489 loss: 8.91691286e-07
Iter: 490 loss: 8.90903e-07
Iter: 491 loss: 9.02205215e-07
Iter: 492 loss: 8.90884621e-07
Iter: 493 loss: 8.90337105e-07
Iter: 494 loss: 8.89031696e-07
Iter: 495 loss: 9.07025878e-07
Iter: 496 loss: 8.88950694e-07
Iter: 497 loss: 8.88086504e-07
Iter: 498 loss: 8.88044042e-07
Iter: 499 loss: 8.87113515e-07
Iter: 500 loss: 8.88767204e-07
Iter: 501 loss: 8.86705834e-07
Iter: 502 loss: 8.85926966e-07
Iter: 503 loss: 8.83931421e-07
Iter: 504 loss: 8.96882625e-07
Iter: 505 loss: 8.83434723e-07
Iter: 506 loss: 8.82917e-07
Iter: 507 loss: 8.82254824e-07
Iter: 508 loss: 8.81350331e-07
Iter: 509 loss: 8.85073518e-07
Iter: 510 loss: 8.81151664e-07
Iter: 511 loss: 8.80575612e-07
Iter: 512 loss: 8.79671347e-07
Iter: 513 loss: 8.79650543e-07
Iter: 514 loss: 8.78607295e-07
Iter: 515 loss: 8.85073518e-07
Iter: 516 loss: 8.78478772e-07
Iter: 517 loss: 8.77481739e-07
Iter: 518 loss: 8.80662242e-07
Iter: 519 loss: 8.77203547e-07
Iter: 520 loss: 8.76102035e-07
Iter: 521 loss: 8.8191166e-07
Iter: 522 loss: 8.7592025e-07
Iter: 523 loss: 8.75141836e-07
Iter: 524 loss: 8.74489217e-07
Iter: 525 loss: 8.74277305e-07
Iter: 526 loss: 8.72937562e-07
Iter: 527 loss: 8.86430371e-07
Iter: 528 loss: 8.72884812e-07
Iter: 529 loss: 8.72220085e-07
Iter: 530 loss: 8.70939118e-07
Iter: 531 loss: 8.98156259e-07
Iter: 532 loss: 8.70945e-07
Iter: 533 loss: 8.70626309e-07
Iter: 534 loss: 8.70313215e-07
Iter: 535 loss: 8.69672249e-07
Iter: 536 loss: 8.69330904e-07
Iter: 537 loss: 8.69121266e-07
Iter: 538 loss: 8.68396569e-07
Iter: 539 loss: 8.67656581e-07
Iter: 540 loss: 8.67529138e-07
Iter: 541 loss: 8.66751066e-07
Iter: 542 loss: 8.66729863e-07
Iter: 543 loss: 8.66200594e-07
Iter: 544 loss: 8.65175593e-07
Iter: 545 loss: 8.86459418e-07
Iter: 546 loss: 8.65172296e-07
Iter: 547 loss: 8.63836362e-07
Iter: 548 loss: 8.65780407e-07
Iter: 549 loss: 8.63198e-07
Iter: 550 loss: 8.62027548e-07
Iter: 551 loss: 8.65568438e-07
Iter: 552 loss: 8.61632486e-07
Iter: 553 loss: 8.60752607e-07
Iter: 554 loss: 8.73235251e-07
Iter: 555 loss: 8.6075022e-07
Iter: 556 loss: 8.59844704e-07
Iter: 557 loss: 8.61714454e-07
Iter: 558 loss: 8.59445038e-07
Iter: 559 loss: 8.58873591e-07
Iter: 560 loss: 8.60174623e-07
Iter: 561 loss: 8.58654346e-07
Iter: 562 loss: 8.57818634e-07
Iter: 563 loss: 8.59308784e-07
Iter: 564 loss: 8.57440114e-07
Iter: 565 loss: 8.56547103e-07
Iter: 566 loss: 8.56162956e-07
Iter: 567 loss: 8.55708322e-07
Iter: 568 loss: 8.54969471e-07
Iter: 569 loss: 8.54952248e-07
Iter: 570 loss: 8.54389043e-07
Iter: 571 loss: 8.56728093e-07
Iter: 572 loss: 8.54253415e-07
Iter: 573 loss: 8.53773713e-07
Iter: 574 loss: 8.53023323e-07
Iter: 575 loss: 8.53001e-07
Iter: 576 loss: 8.52147082e-07
Iter: 577 loss: 8.53725339e-07
Iter: 578 loss: 8.51801133e-07
Iter: 579 loss: 8.51137088e-07
Iter: 580 loss: 8.51098946e-07
Iter: 581 loss: 8.5058889e-07
Iter: 582 loss: 8.49456512e-07
Iter: 583 loss: 8.62537149e-07
Iter: 584 loss: 8.49349931e-07
Iter: 585 loss: 8.48234e-07
Iter: 586 loss: 8.52005542e-07
Iter: 587 loss: 8.47896672e-07
Iter: 588 loss: 8.46767193e-07
Iter: 589 loss: 8.5059412e-07
Iter: 590 loss: 8.4649696e-07
Iter: 591 loss: 8.45991053e-07
Iter: 592 loss: 8.45890952e-07
Iter: 593 loss: 8.45397608e-07
Iter: 594 loss: 8.44536032e-07
Iter: 595 loss: 8.44548538e-07
Iter: 596 loss: 8.43997611e-07
Iter: 597 loss: 8.43989255e-07
Iter: 598 loss: 8.43429063e-07
Iter: 599 loss: 8.42460736e-07
Iter: 600 loss: 8.42476425e-07
Iter: 601 loss: 8.41549877e-07
Iter: 602 loss: 8.45297564e-07
Iter: 603 loss: 8.41367864e-07
Iter: 604 loss: 8.40289886e-07
Iter: 605 loss: 8.47998422e-07
Iter: 606 loss: 8.40223493e-07
Iter: 607 loss: 8.39517497e-07
Iter: 608 loss: 8.38600158e-07
Iter: 609 loss: 8.38527626e-07
Iter: 610 loss: 8.37586754e-07
Iter: 611 loss: 8.44689851e-07
Iter: 612 loss: 8.37515756e-07
Iter: 613 loss: 8.36798563e-07
Iter: 614 loss: 8.40188534e-07
Iter: 615 loss: 8.36690901e-07
Iter: 616 loss: 8.35836886e-07
Iter: 617 loss: 8.36806862e-07
Iter: 618 loss: 8.35384071e-07
Iter: 619 loss: 8.34814841e-07
Iter: 620 loss: 8.33676381e-07
Iter: 621 loss: 8.55742087e-07
Iter: 622 loss: 8.33667286e-07
Iter: 623 loss: 8.32488809e-07
Iter: 624 loss: 8.39207246e-07
Iter: 625 loss: 8.32328055e-07
Iter: 626 loss: 8.31689647e-07
Iter: 627 loss: 8.31618195e-07
Iter: 628 loss: 8.31007185e-07
Iter: 629 loss: 8.30376166e-07
Iter: 630 loss: 8.30304657e-07
Iter: 631 loss: 8.29509645e-07
Iter: 632 loss: 8.34257833e-07
Iter: 633 loss: 8.2938061e-07
Iter: 634 loss: 8.28540919e-07
Iter: 635 loss: 8.30037436e-07
Iter: 636 loss: 8.28162456e-07
Iter: 637 loss: 8.27589929e-07
Iter: 638 loss: 8.27927352e-07
Iter: 639 loss: 8.27223346e-07
Iter: 640 loss: 8.26513e-07
Iter: 641 loss: 8.26507289e-07
Iter: 642 loss: 8.2614838e-07
Iter: 643 loss: 8.25328414e-07
Iter: 644 loss: 8.35152036e-07
Iter: 645 loss: 8.2526833e-07
Iter: 646 loss: 8.24248787e-07
Iter: 647 loss: 8.28283248e-07
Iter: 648 loss: 8.24051938e-07
Iter: 649 loss: 8.23384767e-07
Iter: 650 loss: 8.23383232e-07
Iter: 651 loss: 8.22875279e-07
Iter: 652 loss: 8.228958e-07
Iter: 653 loss: 8.22484935e-07
Iter: 654 loss: 8.21790593e-07
Iter: 655 loss: 8.21685376e-07
Iter: 656 loss: 8.21185267e-07
Iter: 657 loss: 8.20417483e-07
Iter: 658 loss: 8.22158199e-07
Iter: 659 loss: 8.20100126e-07
Iter: 660 loss: 8.19356273e-07
Iter: 661 loss: 8.21448396e-07
Iter: 662 loss: 8.19138165e-07
Iter: 663 loss: 8.18442743e-07
Iter: 664 loss: 8.18427225e-07
Iter: 665 loss: 8.18000558e-07
Iter: 666 loss: 8.17034106e-07
Iter: 667 loss: 8.29179385e-07
Iter: 668 loss: 8.16960664e-07
Iter: 669 loss: 8.16267743e-07
Iter: 670 loss: 8.16224428e-07
Iter: 671 loss: 8.15675094e-07
Iter: 672 loss: 8.15259398e-07
Iter: 673 loss: 8.15058115e-07
Iter: 674 loss: 8.14615419e-07
Iter: 675 loss: 8.14596945e-07
Iter: 676 loss: 8.14174427e-07
Iter: 677 loss: 8.13272038e-07
Iter: 678 loss: 8.25364054e-07
Iter: 679 loss: 8.13192457e-07
Iter: 680 loss: 8.12569965e-07
Iter: 681 loss: 8.19832394e-07
Iter: 682 loss: 8.12542623e-07
Iter: 683 loss: 8.11963787e-07
Iter: 684 loss: 8.17965656e-07
Iter: 685 loss: 8.11958557e-07
Iter: 686 loss: 8.11520067e-07
Iter: 687 loss: 8.10924121e-07
Iter: 688 loss: 8.10893312e-07
Iter: 689 loss: 8.10035772e-07
Iter: 690 loss: 8.10240408e-07
Iter: 691 loss: 8.09423625e-07
Iter: 692 loss: 8.0837026e-07
Iter: 693 loss: 8.12490043e-07
Iter: 694 loss: 8.08134246e-07
Iter: 695 loss: 8.0732724e-07
Iter: 696 loss: 8.14849727e-07
Iter: 697 loss: 8.07252e-07
Iter: 698 loss: 8.06422861e-07
Iter: 699 loss: 8.10100914e-07
Iter: 700 loss: 8.06249432e-07
Iter: 701 loss: 8.05854029e-07
Iter: 702 loss: 8.0537643e-07
Iter: 703 loss: 8.05324817e-07
Iter: 704 loss: 8.04598358e-07
Iter: 705 loss: 8.13325869e-07
Iter: 706 loss: 8.04587501e-07
Iter: 707 loss: 8.04177e-07
Iter: 708 loss: 8.04485808e-07
Iter: 709 loss: 8.03935166e-07
Iter: 710 loss: 8.03355192e-07
Iter: 711 loss: 8.06749028e-07
Iter: 712 loss: 8.03292835e-07
Iter: 713 loss: 8.02827e-07
Iter: 714 loss: 8.01715828e-07
Iter: 715 loss: 8.14698467e-07
Iter: 716 loss: 8.01623344e-07
Iter: 717 loss: 8.01170813e-07
Iter: 718 loss: 8.01011652e-07
Iter: 719 loss: 8.00429916e-07
Iter: 720 loss: 7.9998108e-07
Iter: 721 loss: 7.99803274e-07
Iter: 722 loss: 7.99060558e-07
Iter: 723 loss: 8.00663713e-07
Iter: 724 loss: 7.98796e-07
Iter: 725 loss: 7.98046187e-07
Iter: 726 loss: 7.99547934e-07
Iter: 727 loss: 7.97765324e-07
Iter: 728 loss: 7.97001064e-07
Iter: 729 loss: 7.9725794e-07
Iter: 730 loss: 7.96481e-07
Iter: 731 loss: 7.96095378e-07
Iter: 732 loss: 7.95972085e-07
Iter: 733 loss: 7.95521657e-07
Iter: 734 loss: 7.94826406e-07
Iter: 735 loss: 7.94827372e-07
Iter: 736 loss: 7.94162361e-07
Iter: 737 loss: 7.98191422e-07
Iter: 738 loss: 7.94069706e-07
Iter: 739 loss: 7.93349727e-07
Iter: 740 loss: 7.95502103e-07
Iter: 741 loss: 7.93126787e-07
Iter: 742 loss: 7.92737069e-07
Iter: 743 loss: 7.96075e-07
Iter: 744 loss: 7.92708533e-07
Iter: 745 loss: 7.92315745e-07
Iter: 746 loss: 7.92277547e-07
Iter: 747 loss: 7.9199981e-07
Iter: 748 loss: 7.91533807e-07
Iter: 749 loss: 7.90948775e-07
Iter: 750 loss: 7.90883291e-07
Iter: 751 loss: 7.90144441e-07
Iter: 752 loss: 7.90136482e-07
Iter: 753 loss: 7.89758076e-07
Iter: 754 loss: 7.89070214e-07
Iter: 755 loss: 7.89073681e-07
Iter: 756 loss: 7.88218586e-07
Iter: 757 loss: 7.89541218e-07
Iter: 758 loss: 7.87832448e-07
Iter: 759 loss: 7.86881515e-07
Iter: 760 loss: 7.9203312e-07
Iter: 761 loss: 7.86749752e-07
Iter: 762 loss: 7.86121745e-07
Iter: 763 loss: 7.87361955e-07
Iter: 764 loss: 7.85843895e-07
Iter: 765 loss: 7.85435816e-07
Iter: 766 loss: 7.85394207e-07
Iter: 767 loss: 7.85099246e-07
Iter: 768 loss: 7.84389044e-07
Iter: 769 loss: 7.93985805e-07
Iter: 770 loss: 7.84310373e-07
Iter: 771 loss: 7.83687e-07
Iter: 772 loss: 7.92321259e-07
Iter: 773 loss: 7.83680207e-07
Iter: 774 loss: 7.83063797e-07
Iter: 775 loss: 7.83704195e-07
Iter: 776 loss: 7.82719894e-07
Iter: 777 loss: 7.82212226e-07
Iter: 778 loss: 7.8576e-07
Iter: 779 loss: 7.82158963e-07
Iter: 780 loss: 7.81534084e-07
Iter: 781 loss: 7.81158064e-07
Iter: 782 loss: 7.80920857e-07
Iter: 783 loss: 7.8036112e-07
Iter: 784 loss: 7.80531366e-07
Iter: 785 loss: 7.7998925e-07
Iter: 786 loss: 7.79565653e-07
Iter: 787 loss: 7.79473794e-07
Iter: 788 loss: 7.79264496e-07
Iter: 789 loss: 7.7867918e-07
Iter: 790 loss: 7.84992e-07
Iter: 791 loss: 7.78621313e-07
Iter: 792 loss: 7.77943569e-07
Iter: 793 loss: 7.78918775e-07
Iter: 794 loss: 7.77616151e-07
Iter: 795 loss: 7.76870309e-07
Iter: 796 loss: 7.81926e-07
Iter: 797 loss: 7.76811703e-07
Iter: 798 loss: 7.76072852e-07
Iter: 799 loss: 7.78379444e-07
Iter: 800 loss: 7.75872195e-07
Iter: 801 loss: 7.75144e-07
Iter: 802 loss: 7.80307232e-07
Iter: 803 loss: 7.75088665e-07
Iter: 804 loss: 7.7464847e-07
Iter: 805 loss: 7.74045e-07
Iter: 806 loss: 7.74019099e-07
Iter: 807 loss: 7.73476074e-07
Iter: 808 loss: 7.73475961e-07
Iter: 809 loss: 7.72928388e-07
Iter: 810 loss: 7.72864041e-07
Iter: 811 loss: 7.72482736e-07
Iter: 812 loss: 7.72105636e-07
Iter: 813 loss: 7.72103249e-07
Iter: 814 loss: 7.71850409e-07
Iter: 815 loss: 7.71184318e-07
Iter: 816 loss: 7.76198306e-07
Iter: 817 loss: 7.71066e-07
Iter: 818 loss: 7.70295458e-07
Iter: 819 loss: 7.72281226e-07
Iter: 820 loss: 7.70042504e-07
Iter: 821 loss: 7.69490498e-07
Iter: 822 loss: 7.69467647e-07
Iter: 823 loss: 7.6904746e-07
Iter: 824 loss: 7.68531493e-07
Iter: 825 loss: 7.68484938e-07
Iter: 826 loss: 7.67812e-07
Iter: 827 loss: 7.67576807e-07
Iter: 828 loss: 7.6721426e-07
Iter: 829 loss: 7.66559083e-07
Iter: 830 loss: 7.75023523e-07
Iter: 831 loss: 7.66534299e-07
Iter: 832 loss: 7.6602953e-07
Iter: 833 loss: 7.70227246e-07
Iter: 834 loss: 7.65986215e-07
Iter: 835 loss: 7.65551704e-07
Iter: 836 loss: 7.66756045e-07
Iter: 837 loss: 7.654005e-07
Iter: 838 loss: 7.65022435e-07
Iter: 839 loss: 7.64584797e-07
Iter: 840 loss: 7.64525339e-07
Iter: 841 loss: 7.63949856e-07
Iter: 842 loss: 7.71859845e-07
Iter: 843 loss: 7.6395105e-07
Iter: 844 loss: 7.63461344e-07
Iter: 845 loss: 7.64461447e-07
Iter: 846 loss: 7.63277399e-07
Iter: 847 loss: 7.62867671e-07
Iter: 848 loss: 7.65323307e-07
Iter: 849 loss: 7.62843399e-07
Iter: 850 loss: 7.62451123e-07
Iter: 851 loss: 7.61768774e-07
Iter: 852 loss: 7.78507456e-07
Iter: 853 loss: 7.61766955e-07
Iter: 854 loss: 7.61249794e-07
Iter: 855 loss: 7.6202582e-07
Iter: 856 loss: 7.61002525e-07
Iter: 857 loss: 7.60291641e-07
Iter: 858 loss: 7.6143408e-07
Iter: 859 loss: 7.60008447e-07
Iter: 860 loss: 7.5966318e-07
Iter: 861 loss: 7.5958792e-07
Iter: 862 loss: 7.59147497e-07
Iter: 863 loss: 7.58480212e-07
Iter: 864 loss: 7.58486863e-07
Iter: 865 loss: 7.57837938e-07
Iter: 866 loss: 7.57466751e-07
Iter: 867 loss: 7.57183784e-07
Iter: 868 loss: 7.5667765e-07
Iter: 869 loss: 7.56629163e-07
Iter: 870 loss: 7.56164582e-07
Iter: 871 loss: 7.58860665e-07
Iter: 872 loss: 7.5609978e-07
Iter: 873 loss: 7.55761562e-07
Iter: 874 loss: 7.55472456e-07
Iter: 875 loss: 7.55368092e-07
Iter: 876 loss: 7.54963935e-07
Iter: 877 loss: 7.58856913e-07
Iter: 878 loss: 7.54916243e-07
Iter: 879 loss: 7.54536757e-07
Iter: 880 loss: 7.55785209e-07
Iter: 881 loss: 7.54407097e-07
Iter: 882 loss: 7.54006635e-07
Iter: 883 loss: 7.54449047e-07
Iter: 884 loss: 7.53811491e-07
Iter: 885 loss: 7.53377549e-07
Iter: 886 loss: 7.5612337e-07
Iter: 887 loss: 7.53354925e-07
Iter: 888 loss: 7.53020572e-07
Iter: 889 loss: 7.52241e-07
Iter: 890 loss: 7.62030709e-07
Iter: 891 loss: 7.52197479e-07
Iter: 892 loss: 7.51506718e-07
Iter: 893 loss: 7.56591248e-07
Iter: 894 loss: 7.51427706e-07
Iter: 895 loss: 7.50965796e-07
Iter: 896 loss: 7.51984317e-07
Iter: 897 loss: 7.50743e-07
Iter: 898 loss: 7.50461879e-07
Iter: 899 loss: 7.50445054e-07
Iter: 900 loss: 7.50158e-07
Iter: 901 loss: 7.49654248e-07
Iter: 902 loss: 7.49649871e-07
Iter: 903 loss: 7.49185915e-07
Iter: 904 loss: 7.48891921e-07
Iter: 905 loss: 7.48747652e-07
Iter: 906 loss: 7.48253115e-07
Iter: 907 loss: 7.48211619e-07
Iter: 908 loss: 7.47757213e-07
Iter: 909 loss: 7.47432068e-07
Iter: 910 loss: 7.47305648e-07
Iter: 911 loss: 7.46811907e-07
Iter: 912 loss: 7.46586693e-07
Iter: 913 loss: 7.46337321e-07
Iter: 914 loss: 7.46074306e-07
Iter: 915 loss: 7.459887e-07
Iter: 916 loss: 7.45643661e-07
Iter: 917 loss: 7.45969487e-07
Iter: 918 loss: 7.4544414e-07
Iter: 919 loss: 7.45081365e-07
Iter: 920 loss: 7.45795091e-07
Iter: 921 loss: 7.44918907e-07
Iter: 922 loss: 7.44442332e-07
Iter: 923 loss: 7.4541822e-07
Iter: 924 loss: 7.44266629e-07
Iter: 925 loss: 7.43760324e-07
Iter: 926 loss: 7.44115823e-07
Iter: 927 loss: 7.43457e-07
Iter: 928 loss: 7.42970315e-07
Iter: 929 loss: 7.42389034e-07
Iter: 930 loss: 7.42316786e-07
Iter: 931 loss: 7.41956569e-07
Iter: 932 loss: 7.41865847e-07
Iter: 933 loss: 7.41445376e-07
Iter: 934 loss: 7.418476e-07
Iter: 935 loss: 7.41178155e-07
Iter: 936 loss: 7.40777182e-07
Iter: 937 loss: 7.40576354e-07
Iter: 938 loss: 7.40391e-07
Iter: 939 loss: 7.39869392e-07
Iter: 940 loss: 7.4142423e-07
Iter: 941 loss: 7.39676182e-07
Iter: 942 loss: 7.39147e-07
Iter: 943 loss: 7.39153904e-07
Iter: 944 loss: 7.38829954e-07
Iter: 945 loss: 7.38036874e-07
Iter: 946 loss: 7.44008446e-07
Iter: 947 loss: 7.37879361e-07
Iter: 948 loss: 7.37254709e-07
Iter: 949 loss: 7.45745069e-07
Iter: 950 loss: 7.37247888e-07
Iter: 951 loss: 7.36894322e-07
Iter: 952 loss: 7.36891707e-07
Iter: 953 loss: 7.36585434e-07
Iter: 954 loss: 7.36085326e-07
Iter: 955 loss: 7.47949457e-07
Iter: 956 loss: 7.36080096e-07
Iter: 957 loss: 7.35736648e-07
Iter: 958 loss: 7.3574256e-07
Iter: 959 loss: 7.35449589e-07
Iter: 960 loss: 7.34837045e-07
Iter: 961 loss: 7.4385531e-07
Iter: 962 loss: 7.34825562e-07
Iter: 963 loss: 7.34183175e-07
Iter: 964 loss: 7.39350696e-07
Iter: 965 loss: 7.3412474e-07
Iter: 966 loss: 7.3362321e-07
Iter: 967 loss: 7.33514923e-07
Iter: 968 loss: 7.33199442e-07
Iter: 969 loss: 7.32732644e-07
Iter: 970 loss: 7.32712351e-07
Iter: 971 loss: 7.32307058e-07
Iter: 972 loss: 7.32154888e-07
Iter: 973 loss: 7.31939053e-07
Iter: 974 loss: 7.31517616e-07
Iter: 975 loss: 7.31394152e-07
Iter: 976 loss: 7.31143189e-07
Iter: 977 loss: 7.30972943e-07
Iter: 978 loss: 7.30862439e-07
Iter: 979 loss: 7.30588852e-07
Iter: 980 loss: 7.29974488e-07
Iter: 981 loss: 7.40363248e-07
Iter: 982 loss: 7.2996761e-07
Iter: 983 loss: 7.29337444e-07
Iter: 984 loss: 7.30294e-07
Iter: 985 loss: 7.29076e-07
Iter: 986 loss: 7.28722057e-07
Iter: 987 loss: 7.28663338e-07
Iter: 988 loss: 7.28269299e-07
Iter: 989 loss: 7.27666759e-07
Iter: 990 loss: 7.2762839e-07
Iter: 991 loss: 7.27215706e-07
Iter: 992 loss: 7.33098545e-07
Iter: 993 loss: 7.27203087e-07
Iter: 994 loss: 7.26755957e-07
Iter: 995 loss: 7.26584801e-07
Iter: 996 loss: 7.26368398e-07
Iter: 997 loss: 7.25988855e-07
Iter: 998 loss: 7.26936776e-07
Iter: 999 loss: 7.25832365e-07
Iter: 1000 loss: 7.25410189e-07
Iter: 1001 loss: 7.26664609e-07
Iter: 1002 loss: 7.25284735e-07
Iter: 1003 loss: 7.24882057e-07
Iter: 1004 loss: 7.27706492e-07
Iter: 1005 loss: 7.24843801e-07
Iter: 1006 loss: 7.2442225e-07
Iter: 1007 loss: 7.24467895e-07
Iter: 1008 loss: 7.24075619e-07
Iter: 1009 loss: 7.23553285e-07
Iter: 1010 loss: 7.23222968e-07
Iter: 1011 loss: 7.23025e-07
Iter: 1012 loss: 7.22614686e-07
Iter: 1013 loss: 7.22596212e-07
Iter: 1014 loss: 7.22134587e-07
Iter: 1015 loss: 7.22082518e-07
Iter: 1016 loss: 7.21727474e-07
Iter: 1017 loss: 7.21257948e-07
Iter: 1018 loss: 7.21025458e-07
Iter: 1019 loss: 7.20768469e-07
Iter: 1020 loss: 7.20203843e-07
Iter: 1021 loss: 7.22166419e-07
Iter: 1022 loss: 7.2007424e-07
Iter: 1023 loss: 7.19663944e-07
Iter: 1024 loss: 7.19637e-07
Iter: 1025 loss: 7.19300488e-07
Iter: 1026 loss: 7.18609272e-07
Iter: 1027 loss: 7.30278941e-07
Iter: 1028 loss: 7.18604156e-07
Iter: 1029 loss: 7.18059198e-07
Iter: 1030 loss: 7.22227071e-07
Iter: 1031 loss: 7.18004e-07
Iter: 1032 loss: 7.17421926e-07
Iter: 1033 loss: 7.18598358e-07
Iter: 1034 loss: 7.17169428e-07
Iter: 1035 loss: 7.16743671e-07
Iter: 1036 loss: 7.16289719e-07
Iter: 1037 loss: 7.16208774e-07
Iter: 1038 loss: 7.1577017e-07
Iter: 1039 loss: 7.20018704e-07
Iter: 1040 loss: 7.1575414e-07
Iter: 1041 loss: 7.15274837e-07
Iter: 1042 loss: 7.182731e-07
Iter: 1043 loss: 7.15235103e-07
Iter: 1044 loss: 7.14948101e-07
Iter: 1045 loss: 7.14699468e-07
Iter: 1046 loss: 7.14647854e-07
Iter: 1047 loss: 7.14061628e-07
Iter: 1048 loss: 7.14459475e-07
Iter: 1049 loss: 7.13710961e-07
Iter: 1050 loss: 7.13046916e-07
Iter: 1051 loss: 7.16053648e-07
Iter: 1052 loss: 7.12932888e-07
Iter: 1053 loss: 7.12480755e-07
Iter: 1054 loss: 7.12491e-07
Iter: 1055 loss: 7.12045392e-07
Iter: 1056 loss: 7.11158577e-07
Iter: 1057 loss: 7.26368967e-07
Iter: 1058 loss: 7.11141809e-07
Iter: 1059 loss: 7.10749305e-07
Iter: 1060 loss: 7.10733e-07
Iter: 1061 loss: 7.10321331e-07
Iter: 1062 loss: 7.11565e-07
Iter: 1063 loss: 7.10205313e-07
Iter: 1064 loss: 7.09934284e-07
Iter: 1065 loss: 7.093671e-07
Iter: 1066 loss: 7.19991931e-07
Iter: 1067 loss: 7.09365054e-07
Iter: 1068 loss: 7.08958169e-07
Iter: 1069 loss: 7.08946118e-07
Iter: 1070 loss: 7.08568223e-07
Iter: 1071 loss: 7.08473749e-07
Iter: 1072 loss: 7.08232051e-07
Iter: 1073 loss: 7.07791969e-07
Iter: 1074 loss: 7.0704084e-07
Iter: 1075 loss: 7.07039931e-07
Iter: 1076 loss: 7.06994342e-07
Iter: 1077 loss: 7.06724904e-07
Iter: 1078 loss: 7.0636861e-07
Iter: 1079 loss: 7.06356445e-07
Iter: 1080 loss: 7.06112701e-07
Iter: 1081 loss: 7.05733044e-07
Iter: 1082 loss: 7.05623506e-07
Iter: 1083 loss: 7.05390505e-07
Iter: 1084 loss: 7.0487414e-07
Iter: 1085 loss: 7.04725494e-07
Iter: 1086 loss: 7.04399156e-07
Iter: 1087 loss: 7.03906835e-07
Iter: 1088 loss: 7.0386136e-07
Iter: 1089 loss: 7.03427304e-07
Iter: 1090 loss: 7.04635909e-07
Iter: 1091 loss: 7.03279284e-07
Iter: 1092 loss: 7.02840168e-07
Iter: 1093 loss: 7.02823968e-07
Iter: 1094 loss: 7.02430043e-07
Iter: 1095 loss: 7.02030093e-07
Iter: 1096 loss: 7.04046386e-07
Iter: 1097 loss: 7.01922943e-07
Iter: 1098 loss: 7.01509748e-07
Iter: 1099 loss: 7.05451725e-07
Iter: 1100 loss: 7.01482463e-07
Iter: 1101 loss: 7.01195063e-07
Iter: 1102 loss: 7.00630778e-07
Iter: 1103 loss: 7.11176654e-07
Iter: 1104 loss: 7.0062e-07
Iter: 1105 loss: 7.00060923e-07
Iter: 1106 loss: 7.01721717e-07
Iter: 1107 loss: 6.99897839e-07
Iter: 1108 loss: 6.99489419e-07
Iter: 1109 loss: 6.99456507e-07
Iter: 1110 loss: 6.99189e-07
Iter: 1111 loss: 6.98514555e-07
Iter: 1112 loss: 7.06496166e-07
Iter: 1113 loss: 6.98473514e-07
Iter: 1114 loss: 6.97750579e-07
Iter: 1115 loss: 7.02357283e-07
Iter: 1116 loss: 6.97680889e-07
Iter: 1117 loss: 6.97377345e-07
Iter: 1118 loss: 6.97330279e-07
Iter: 1119 loss: 6.97086762e-07
Iter: 1120 loss: 6.96548909e-07
Iter: 1121 loss: 7.02655427e-07
Iter: 1122 loss: 6.96481038e-07
Iter: 1123 loss: 6.95881226e-07
Iter: 1124 loss: 6.97032647e-07
Iter: 1125 loss: 6.95619747e-07
Iter: 1126 loss: 6.95016297e-07
Iter: 1127 loss: 6.96834832e-07
Iter: 1128 loss: 6.94825815e-07
Iter: 1129 loss: 6.94472533e-07
Iter: 1130 loss: 6.9440506e-07
Iter: 1131 loss: 6.94162054e-07
Iter: 1132 loss: 6.93588845e-07
Iter: 1133 loss: 6.99761301e-07
Iter: 1134 loss: 6.93507673e-07
Iter: 1135 loss: 6.9317997e-07
Iter: 1136 loss: 6.93141146e-07
Iter: 1137 loss: 6.92848744e-07
Iter: 1138 loss: 6.93476181e-07
Iter: 1139 loss: 6.92697313e-07
Iter: 1140 loss: 6.92387232e-07
Iter: 1141 loss: 6.92138656e-07
Iter: 1142 loss: 6.9204566e-07
Iter: 1143 loss: 6.9159762e-07
Iter: 1144 loss: 6.93548259e-07
Iter: 1145 loss: 6.91497576e-07
Iter: 1146 loss: 6.91143839e-07
Iter: 1147 loss: 6.94212929e-07
Iter: 1148 loss: 6.9111951e-07
Iter: 1149 loss: 6.90767706e-07
Iter: 1150 loss: 6.90106731e-07
Iter: 1151 loss: 7.03649334e-07
Iter: 1152 loss: 6.90103207e-07
Iter: 1153 loss: 6.89617423e-07
Iter: 1154 loss: 6.96551638e-07
Iter: 1155 loss: 6.89611909e-07
Iter: 1156 loss: 6.89101569e-07
Iter: 1157 loss: 6.90541469e-07
Iter: 1158 loss: 6.88933198e-07
Iter: 1159 loss: 6.88631303e-07
Iter: 1160 loss: 6.88047066e-07
Iter: 1161 loss: 7.0153726e-07
Iter: 1162 loss: 6.88050875e-07
Iter: 1163 loss: 6.87382681e-07
Iter: 1164 loss: 6.90725415e-07
Iter: 1165 loss: 6.87247962e-07
Iter: 1166 loss: 6.87014563e-07
Iter: 1167 loss: 6.87006946e-07
Iter: 1168 loss: 6.86677083e-07
Iter: 1169 loss: 6.86144517e-07
Iter: 1170 loss: 6.992218e-07
Iter: 1171 loss: 6.86145427e-07
Iter: 1172 loss: 6.8559757e-07
Iter: 1173 loss: 6.87229715e-07
Iter: 1174 loss: 6.85434429e-07
Iter: 1175 loss: 6.85001851e-07
Iter: 1176 loss: 6.84996166e-07
Iter: 1177 loss: 6.84709221e-07
Iter: 1178 loss: 6.84121e-07
Iter: 1179 loss: 6.95935455e-07
Iter: 1180 loss: 6.84123677e-07
Iter: 1181 loss: 6.8358e-07
Iter: 1182 loss: 6.85133955e-07
Iter: 1183 loss: 6.83405915e-07
Iter: 1184 loss: 6.82838049e-07
Iter: 1185 loss: 6.87179408e-07
Iter: 1186 loss: 6.82794905e-07
Iter: 1187 loss: 6.82434461e-07
Iter: 1188 loss: 6.84801876e-07
Iter: 1189 loss: 6.82382961e-07
Iter: 1190 loss: 6.82109089e-07
Iter: 1191 loss: 6.81638426e-07
Iter: 1192 loss: 6.81623135e-07
Iter: 1193 loss: 6.81233871e-07
Iter: 1194 loss: 6.81238134e-07
Iter: 1195 loss: 6.80802486e-07
Iter: 1196 loss: 6.8035115e-07
Iter: 1197 loss: 6.80262644e-07
Iter: 1198 loss: 6.7972104e-07
Iter: 1199 loss: 6.80632184e-07
Iter: 1200 loss: 6.79461436e-07
Iter: 1201 loss: 6.7904773e-07
Iter: 1202 loss: 6.84436486e-07
Iter: 1203 loss: 6.79060236e-07
Iter: 1204 loss: 6.78616175e-07
Iter: 1205 loss: 6.79163406e-07
Iter: 1206 loss: 6.78394713e-07
Iter: 1207 loss: 6.78118283e-07
Iter: 1208 loss: 6.77931666e-07
Iter: 1209 loss: 6.77837647e-07
Iter: 1210 loss: 6.77442927e-07
Iter: 1211 loss: 6.77438038e-07
Iter: 1212 loss: 6.77163143e-07
Iter: 1213 loss: 6.76579873e-07
Iter: 1214 loss: 6.85532882e-07
Iter: 1215 loss: 6.76534626e-07
Iter: 1216 loss: 6.75922308e-07
Iter: 1217 loss: 6.77098228e-07
Iter: 1218 loss: 6.75671572e-07
Iter: 1219 loss: 6.75049591e-07
Iter: 1220 loss: 6.8063423e-07
Iter: 1221 loss: 6.75037427e-07
Iter: 1222 loss: 6.74537887e-07
Iter: 1223 loss: 6.77359594e-07
Iter: 1224 loss: 6.74499574e-07
Iter: 1225 loss: 6.74151238e-07
Iter: 1226 loss: 6.7493886e-07
Iter: 1227 loss: 6.74052671e-07
Iter: 1228 loss: 6.73740658e-07
Iter: 1229 loss: 6.74417549e-07
Iter: 1230 loss: 6.73637487e-07
Iter: 1231 loss: 6.73240152e-07
Iter: 1232 loss: 6.74383e-07
Iter: 1233 loss: 6.73127204e-07
Iter: 1234 loss: 6.72768465e-07
Iter: 1235 loss: 6.72313945e-07
Iter: 1236 loss: 6.72285523e-07
Iter: 1237 loss: 6.71651094e-07
Iter: 1238 loss: 6.72526141e-07
Iter: 1239 loss: 6.7135187e-07
Iter: 1240 loss: 6.71056341e-07
Iter: 1241 loss: 6.70928785e-07
Iter: 1242 loss: 6.70605573e-07
Iter: 1243 loss: 6.70206191e-07
Iter: 1244 loss: 6.7016208e-07
Iter: 1245 loss: 6.69850579e-07
Iter: 1246 loss: 6.73217869e-07
Iter: 1247 loss: 6.6984262e-07
Iter: 1248 loss: 6.69482574e-07
Iter: 1249 loss: 6.69968472e-07
Iter: 1250 loss: 6.69287e-07
Iter: 1251 loss: 6.6904056e-07
Iter: 1252 loss: 6.68504526e-07
Iter: 1253 loss: 6.75555839e-07
Iter: 1254 loss: 6.68451889e-07
Iter: 1255 loss: 6.67738846e-07
Iter: 1256 loss: 6.71401267e-07
Iter: 1257 loss: 6.67622487e-07
Iter: 1258 loss: 6.67220775e-07
Iter: 1259 loss: 6.67218046e-07
Iter: 1260 loss: 6.66827191e-07
Iter: 1261 loss: 6.66332426e-07
Iter: 1262 loss: 6.66278595e-07
Iter: 1263 loss: 6.6595851e-07
Iter: 1264 loss: 6.65949415e-07
Iter: 1265 loss: 6.65608e-07
Iter: 1266 loss: 6.6537757e-07
Iter: 1267 loss: 6.65256835e-07
Iter: 1268 loss: 6.64914921e-07
Iter: 1269 loss: 6.66377559e-07
Iter: 1270 loss: 6.64836421e-07
Iter: 1271 loss: 6.64467052e-07
Iter: 1272 loss: 6.64105301e-07
Iter: 1273 loss: 6.64048116e-07
Iter: 1274 loss: 6.63717969e-07
Iter: 1275 loss: 6.63693413e-07
Iter: 1276 loss: 6.6333223e-07
Iter: 1277 loss: 6.63025389e-07
Iter: 1278 loss: 6.62963771e-07
Iter: 1279 loss: 6.62483842e-07
Iter: 1280 loss: 6.62727871e-07
Iter: 1281 loss: 6.62180469e-07
Iter: 1282 loss: 6.61928311e-07
Iter: 1283 loss: 6.61870502e-07
Iter: 1284 loss: 6.61594243e-07
Iter: 1285 loss: 6.60987666e-07
Iter: 1286 loss: 6.72918645e-07
Iter: 1287 loss: 6.60999149e-07
Iter: 1288 loss: 6.60544288e-07
Iter: 1289 loss: 6.60914679e-07
Iter: 1290 loss: 6.603006e-07
Iter: 1291 loss: 6.59819193e-07
Iter: 1292 loss: 6.59833177e-07
Iter: 1293 loss: 6.5940327e-07
Iter: 1294 loss: 6.59720058e-07
Iter: 1295 loss: 6.59158786e-07
Iter: 1296 loss: 6.58751333e-07
Iter: 1297 loss: 6.58779754e-07
Iter: 1298 loss: 6.58452336e-07
Iter: 1299 loss: 6.58022827e-07
Iter: 1300 loss: 6.58026e-07
Iter: 1301 loss: 6.57724968e-07
Iter: 1302 loss: 6.5709321e-07
Iter: 1303 loss: 6.64956588e-07
Iter: 1304 loss: 6.57031705e-07
Iter: 1305 loss: 6.56543534e-07
Iter: 1306 loss: 6.62495097e-07
Iter: 1307 loss: 6.56547911e-07
Iter: 1308 loss: 6.56127327e-07
Iter: 1309 loss: 6.5674044e-07
Iter: 1310 loss: 6.55942e-07
Iter: 1311 loss: 6.55619885e-07
Iter: 1312 loss: 6.55616191e-07
Iter: 1313 loss: 6.55340557e-07
Iter: 1314 loss: 6.54904454e-07
Iter: 1315 loss: 6.54904738e-07
Iter: 1316 loss: 6.54428277e-07
Iter: 1317 loss: 6.542287e-07
Iter: 1318 loss: 6.53975235e-07
Iter: 1319 loss: 6.53416805e-07
Iter: 1320 loss: 6.5824355e-07
Iter: 1321 loss: 6.53412087e-07
Iter: 1322 loss: 6.52946255e-07
Iter: 1323 loss: 6.58701822e-07
Iter: 1324 loss: 6.52936365e-07
Iter: 1325 loss: 6.52637823e-07
Iter: 1326 loss: 6.52077858e-07
Iter: 1327 loss: 6.62670175e-07
Iter: 1328 loss: 6.52061317e-07
Iter: 1329 loss: 6.51654318e-07
Iter: 1330 loss: 6.54817086e-07
Iter: 1331 loss: 6.51618279e-07
Iter: 1332 loss: 6.51247092e-07
Iter: 1333 loss: 6.54261441e-07
Iter: 1334 loss: 6.51197e-07
Iter: 1335 loss: 6.50939569e-07
Iter: 1336 loss: 6.50623178e-07
Iter: 1337 loss: 6.50598508e-07
Iter: 1338 loss: 6.50284164e-07
Iter: 1339 loss: 6.50268362e-07
Iter: 1340 loss: 6.50012453e-07
Iter: 1341 loss: 6.49495746e-07
Iter: 1342 loss: 6.59935e-07
Iter: 1343 loss: 6.49459707e-07
Iter: 1344 loss: 6.48910145e-07
Iter: 1345 loss: 6.49251433e-07
Iter: 1346 loss: 6.48550099e-07
Iter: 1347 loss: 6.48363937e-07
Iter: 1348 loss: 6.48231378e-07
Iter: 1349 loss: 6.47972058e-07
Iter: 1350 loss: 6.48233e-07
Iter: 1351 loss: 6.47848765e-07
Iter: 1352 loss: 6.47494289e-07
Iter: 1353 loss: 6.47297384e-07
Iter: 1354 loss: 6.47159766e-07
Iter: 1355 loss: 6.46708941e-07
Iter: 1356 loss: 6.48605749e-07
Iter: 1357 loss: 6.46610715e-07
Iter: 1358 loss: 6.46331614e-07
Iter: 1359 loss: 6.46328203e-07
Iter: 1360 loss: 6.46000501e-07
Iter: 1361 loss: 6.45392561e-07
Iter: 1362 loss: 6.56550128e-07
Iter: 1363 loss: 6.45392504e-07
Iter: 1364 loss: 6.4488097e-07
Iter: 1365 loss: 6.4644e-07
Iter: 1366 loss: 6.44739032e-07
Iter: 1367 loss: 6.44209251e-07
Iter: 1368 loss: 6.49675599e-07
Iter: 1369 loss: 6.44213344e-07
Iter: 1370 loss: 6.43959083e-07
Iter: 1371 loss: 6.43854946e-07
Iter: 1372 loss: 6.4368578e-07
Iter: 1373 loss: 6.43350745e-07
Iter: 1374 loss: 6.47753211e-07
Iter: 1375 loss: 6.43333408e-07
Iter: 1376 loss: 6.43124736e-07
Iter: 1377 loss: 6.42653276e-07
Iter: 1378 loss: 6.51051096e-07
Iter: 1379 loss: 6.42669875e-07
Iter: 1380 loss: 6.42184602e-07
Iter: 1381 loss: 6.44725105e-07
Iter: 1382 loss: 6.42144471e-07
Iter: 1383 loss: 6.41757424e-07
Iter: 1384 loss: 6.4544065e-07
Iter: 1385 loss: 6.41741963e-07
Iter: 1386 loss: 6.41462577e-07
Iter: 1387 loss: 6.41140559e-07
Iter: 1388 loss: 6.41089855e-07
Iter: 1389 loss: 6.40516305e-07
Iter: 1390 loss: 6.41217355e-07
Iter: 1391 loss: 6.40225323e-07
Iter: 1392 loss: 6.39748805e-07
Iter: 1393 loss: 6.42771624e-07
Iter: 1394 loss: 6.39679342e-07
Iter: 1395 loss: 6.39359598e-07
Iter: 1396 loss: 6.39356472e-07
Iter: 1397 loss: 6.39193956e-07
Iter: 1398 loss: 6.38732047e-07
Iter: 1399 loss: 6.44236422e-07
Iter: 1400 loss: 6.38713232e-07
Iter: 1401 loss: 6.38374104e-07
Iter: 1402 loss: 6.42452505e-07
Iter: 1403 loss: 6.38348922e-07
Iter: 1404 loss: 6.3797296e-07
Iter: 1405 loss: 6.38652182e-07
Iter: 1406 loss: 6.37839321e-07
Iter: 1407 loss: 6.37434596e-07
Iter: 1408 loss: 6.37567155e-07
Iter: 1409 loss: 6.37180392e-07
Iter: 1410 loss: 6.36663572e-07
Iter: 1411 loss: 6.41458939e-07
Iter: 1412 loss: 6.36635832e-07
Iter: 1413 loss: 6.36394589e-07
Iter: 1414 loss: 6.35850085e-07
Iter: 1415 loss: 6.44526949e-07
Iter: 1416 loss: 6.35828314e-07
Iter: 1417 loss: 6.35484753e-07
Iter: 1418 loss: 6.35478727e-07
Iter: 1419 loss: 6.35110837e-07
Iter: 1420 loss: 6.35888284e-07
Iter: 1421 loss: 6.34991807e-07
Iter: 1422 loss: 6.3472271e-07
Iter: 1423 loss: 6.34715661e-07
Iter: 1424 loss: 6.34460662e-07
Iter: 1425 loss: 6.34085268e-07
Iter: 1426 loss: 6.35393121e-07
Iter: 1427 loss: 6.33970558e-07
Iter: 1428 loss: 6.33641434e-07
Iter: 1429 loss: 6.36148798e-07
Iter: 1430 loss: 6.33628304e-07
Iter: 1431 loss: 6.33207719e-07
Iter: 1432 loss: 6.33160425e-07
Iter: 1433 loss: 6.32871263e-07
Iter: 1434 loss: 6.32494789e-07
Iter: 1435 loss: 6.32467618e-07
Iter: 1436 loss: 6.32186811e-07
Iter: 1437 loss: 6.31912485e-07
Iter: 1438 loss: 6.31906175e-07
Iter: 1439 loss: 6.31650607e-07
Iter: 1440 loss: 6.31566252e-07
Iter: 1441 loss: 6.31443356e-07
Iter: 1442 loss: 6.31166472e-07
Iter: 1443 loss: 6.33643936e-07
Iter: 1444 loss: 6.31153114e-07
Iter: 1445 loss: 6.30867646e-07
Iter: 1446 loss: 6.30713259e-07
Iter: 1447 loss: 6.30577802e-07
Iter: 1448 loss: 6.3029529e-07
Iter: 1449 loss: 6.30188651e-07
Iter: 1450 loss: 6.30010902e-07
Iter: 1451 loss: 6.29707188e-07
Iter: 1452 loss: 6.29680471e-07
Iter: 1453 loss: 6.29429564e-07
Iter: 1454 loss: 6.2895856e-07
Iter: 1455 loss: 6.28965836e-07
Iter: 1456 loss: 6.28492e-07
Iter: 1457 loss: 6.30171144e-07
Iter: 1458 loss: 6.28380519e-07
Iter: 1459 loss: 6.27981535e-07
Iter: 1460 loss: 6.32183401e-07
Iter: 1461 loss: 6.27980057e-07
Iter: 1462 loss: 6.27664861e-07
Iter: 1463 loss: 6.29244482e-07
Iter: 1464 loss: 6.27595909e-07
Iter: 1465 loss: 6.27339773e-07
Iter: 1466 loss: 6.27008262e-07
Iter: 1467 loss: 6.26995416e-07
Iter: 1468 loss: 6.26693748e-07
Iter: 1469 loss: 6.29912506e-07
Iter: 1470 loss: 6.26675e-07
Iter: 1471 loss: 6.26359e-07
Iter: 1472 loss: 6.26504516e-07
Iter: 1473 loss: 6.26124574e-07
Iter: 1474 loss: 6.25749408e-07
Iter: 1475 loss: 6.28030818e-07
Iter: 1476 loss: 6.25703137e-07
Iter: 1477 loss: 6.25354176e-07
Iter: 1478 loss: 6.25743155e-07
Iter: 1479 loss: 6.25157668e-07
Iter: 1480 loss: 6.24812628e-07
Iter: 1481 loss: 6.24479412e-07
Iter: 1482 loss: 6.2440165e-07
Iter: 1483 loss: 6.24110726e-07
Iter: 1484 loss: 6.24109e-07
Iter: 1485 loss: 6.23798599e-07
Iter: 1486 loss: 6.24142899e-07
Iter: 1487 loss: 6.23616813e-07
Iter: 1488 loss: 6.23375797e-07
Iter: 1489 loss: 6.23012795e-07
Iter: 1490 loss: 6.22982725e-07
Iter: 1491 loss: 6.22505922e-07
Iter: 1492 loss: 6.2605136e-07
Iter: 1493 loss: 6.22462323e-07
Iter: 1494 loss: 6.22126208e-07
Iter: 1495 loss: 6.26258384e-07
Iter: 1496 loss: 6.22143375e-07
Iter: 1497 loss: 6.21770937e-07
Iter: 1498 loss: 6.21385539e-07
Iter: 1499 loss: 6.21355071e-07
Iter: 1500 loss: 6.20887192e-07
Iter: 1501 loss: 6.21139293e-07
Iter: 1502 loss: 6.20602236e-07
Iter: 1503 loss: 6.20175683e-07
Iter: 1504 loss: 6.20160904e-07
Iter: 1505 loss: 6.19893e-07
Iter: 1506 loss: 6.1994092e-07
Iter: 1507 loss: 6.19684556e-07
Iter: 1508 loss: 6.1941364e-07
Iter: 1509 loss: 6.22340053e-07
Iter: 1510 loss: 6.19405512e-07
Iter: 1511 loss: 6.19138177e-07
Iter: 1512 loss: 6.18814909e-07
Iter: 1513 loss: 6.18789272e-07
Iter: 1514 loss: 6.1841871e-07
Iter: 1515 loss: 6.18457e-07
Iter: 1516 loss: 6.18118747e-07
Iter: 1517 loss: 6.17831859e-07
Iter: 1518 loss: 6.17798264e-07
Iter: 1519 loss: 6.17561e-07
Iter: 1520 loss: 6.17150477e-07
Iter: 1521 loss: 6.27151621e-07
Iter: 1522 loss: 6.17153034e-07
Iter: 1523 loss: 6.16654518e-07
Iter: 1524 loss: 6.16588125e-07
Iter: 1525 loss: 6.16222792e-07
Iter: 1526 loss: 6.15869567e-07
Iter: 1527 loss: 6.15844215e-07
Iter: 1528 loss: 6.15504746e-07
Iter: 1529 loss: 6.17095e-07
Iter: 1530 loss: 6.15448812e-07
Iter: 1531 loss: 6.15169483e-07
Iter: 1532 loss: 6.15144074e-07
Iter: 1533 loss: 6.14941428e-07
Iter: 1534 loss: 6.14614123e-07
Iter: 1535 loss: 6.14246801e-07
Iter: 1536 loss: 6.1421639e-07
Iter: 1537 loss: 6.13968382e-07
Iter: 1538 loss: 6.13872146e-07
Iter: 1539 loss: 6.13638804e-07
Iter: 1540 loss: 6.13344241e-07
Iter: 1541 loss: 6.13334805e-07
Iter: 1542 loss: 6.12956967e-07
Iter: 1543 loss: 6.17764726e-07
Iter: 1544 loss: 6.12951567e-07
Iter: 1545 loss: 6.12692531e-07
Iter: 1546 loss: 6.12464191e-07
Iter: 1547 loss: 6.1239291e-07
Iter: 1548 loss: 6.12064071e-07
Iter: 1549 loss: 6.11947883e-07
Iter: 1550 loss: 6.11731934e-07
Iter: 1551 loss: 6.11677e-07
Iter: 1552 loss: 6.11541225e-07
Iter: 1553 loss: 6.11326755e-07
Iter: 1554 loss: 6.1104231e-07
Iter: 1555 loss: 6.1102088e-07
Iter: 1556 loss: 6.10645e-07
Iter: 1557 loss: 6.10853135e-07
Iter: 1558 loss: 6.10382244e-07
Iter: 1559 loss: 6.1017e-07
Iter: 1560 loss: 6.10139068e-07
Iter: 1561 loss: 6.09834558e-07
Iter: 1562 loss: 6.09578763e-07
Iter: 1563 loss: 6.0951777e-07
Iter: 1564 loss: 6.09192625e-07
Iter: 1565 loss: 6.09786582e-07
Iter: 1566 loss: 6.09083827e-07
Iter: 1567 loss: 6.08767664e-07
Iter: 1568 loss: 6.10590291e-07
Iter: 1569 loss: 6.08714117e-07
Iter: 1570 loss: 6.08483731e-07
Iter: 1571 loss: 6.10028735e-07
Iter: 1572 loss: 6.08461278e-07
Iter: 1573 loss: 6.0822191e-07
Iter: 1574 loss: 6.08062e-07
Iter: 1575 loss: 6.07987e-07
Iter: 1576 loss: 6.07655352e-07
Iter: 1577 loss: 6.11013661e-07
Iter: 1578 loss: 6.07672121e-07
Iter: 1579 loss: 6.07411494e-07
Iter: 1580 loss: 6.07219704e-07
Iter: 1581 loss: 6.07152288e-07
Iter: 1582 loss: 6.06800654e-07
Iter: 1583 loss: 6.06514845e-07
Iter: 1584 loss: 6.06401898e-07
Iter: 1585 loss: 6.0586774e-07
Iter: 1586 loss: 6.08041205e-07
Iter: 1587 loss: 6.05746152e-07
Iter: 1588 loss: 6.05505193e-07
Iter: 1589 loss: 6.05496552e-07
Iter: 1590 loss: 6.05227683e-07
Iter: 1591 loss: 6.05557887e-07
Iter: 1592 loss: 6.0508421e-07
Iter: 1593 loss: 6.0487946e-07
Iter: 1594 loss: 6.04786237e-07
Iter: 1595 loss: 6.04684942e-07
Iter: 1596 loss: 6.04278796e-07
Iter: 1597 loss: 6.07159336e-07
Iter: 1598 loss: 6.04228774e-07
Iter: 1599 loss: 6.03999752e-07
Iter: 1600 loss: 6.03660624e-07
Iter: 1601 loss: 6.03669093e-07
Iter: 1602 loss: 6.03218837e-07
Iter: 1603 loss: 6.05277251e-07
Iter: 1604 loss: 6.03136527e-07
Iter: 1605 loss: 6.02787111e-07
Iter: 1606 loss: 6.06294691e-07
Iter: 1607 loss: 6.02791602e-07
Iter: 1608 loss: 6.02564e-07
Iter: 1609 loss: 6.02928367e-07
Iter: 1610 loss: 6.02457703e-07
Iter: 1611 loss: 6.02205432e-07
Iter: 1612 loss: 6.02782848e-07
Iter: 1613 loss: 6.0211471e-07
Iter: 1614 loss: 6.01825263e-07
Iter: 1615 loss: 6.02712873e-07
Iter: 1616 loss: 6.01759e-07
Iter: 1617 loss: 6.01523766e-07
Iter: 1618 loss: 6.01093461e-07
Iter: 1619 loss: 6.10187499e-07
Iter: 1620 loss: 6.01101647e-07
Iter: 1621 loss: 6.00623878e-07
Iter: 1622 loss: 6.0178138e-07
Iter: 1623 loss: 6.00450676e-07
Iter: 1624 loss: 5.99935163e-07
Iter: 1625 loss: 6.02779892e-07
Iter: 1626 loss: 5.99874056e-07
Iter: 1627 loss: 5.9971137e-07
Iter: 1628 loss: 5.99644068e-07
Iter: 1629 loss: 5.99463192e-07
Iter: 1630 loss: 5.99110308e-07
Iter: 1631 loss: 6.06976528e-07
Iter: 1632 loss: 5.99094506e-07
Iter: 1633 loss: 5.98965585e-07
Iter: 1634 loss: 5.98946144e-07
Iter: 1635 loss: 5.98771294e-07
Iter: 1636 loss: 5.98481051e-07
Iter: 1637 loss: 6.04561365e-07
Iter: 1638 loss: 5.98481e-07
Iter: 1639 loss: 5.98177166e-07
Iter: 1640 loss: 5.98568306e-07
Iter: 1641 loss: 5.98031875e-07
Iter: 1642 loss: 5.97730377e-07
Iter: 1643 loss: 6.00379906e-07
Iter: 1644 loss: 5.97693884e-07
Iter: 1645 loss: 5.97404835e-07
Iter: 1646 loss: 5.97903806e-07
Iter: 1647 loss: 5.97257497e-07
Iter: 1648 loss: 5.96949803e-07
Iter: 1649 loss: 5.97534608e-07
Iter: 1650 loss: 5.96830489e-07
Iter: 1651 loss: 5.96542691e-07
Iter: 1652 loss: 5.98772772e-07
Iter: 1653 loss: 5.9652416e-07
Iter: 1654 loss: 5.96302755e-07
Iter: 1655 loss: 5.96348514e-07
Iter: 1656 loss: 5.9616923e-07
Iter: 1657 loss: 5.95929691e-07
Iter: 1658 loss: 5.95722724e-07
Iter: 1659 loss: 5.9563348e-07
Iter: 1660 loss: 5.95260872e-07
Iter: 1661 loss: 5.96413486e-07
Iter: 1662 loss: 5.95141501e-07
Iter: 1663 loss: 5.94763719e-07
Iter: 1664 loss: 5.96446114e-07
Iter: 1665 loss: 5.94714038e-07
Iter: 1666 loss: 5.94232461e-07
Iter: 1667 loss: 5.97326675e-07
Iter: 1668 loss: 5.94189373e-07
Iter: 1669 loss: 5.93988148e-07
Iter: 1670 loss: 5.93995082e-07
Iter: 1671 loss: 5.93829e-07
Iter: 1672 loss: 5.93484629e-07
Iter: 1673 loss: 5.96245854e-07
Iter: 1674 loss: 5.9349793e-07
Iter: 1675 loss: 5.93286813e-07
Iter: 1676 loss: 5.92798131e-07
Iter: 1677 loss: 5.98822567e-07
Iter: 1678 loss: 5.92726053e-07
Iter: 1679 loss: 5.92353e-07
Iter: 1680 loss: 5.94279754e-07
Iter: 1681 loss: 5.92286767e-07
Iter: 1682 loss: 5.91939397e-07
Iter: 1683 loss: 5.93170739e-07
Iter: 1684 loss: 5.91847652e-07
Iter: 1685 loss: 5.91588673e-07
Iter: 1686 loss: 5.95145195e-07
Iter: 1687 loss: 5.91578896e-07
Iter: 1688 loss: 5.91314461e-07
Iter: 1689 loss: 5.91323897e-07
Iter: 1690 loss: 5.91142225e-07
Iter: 1691 loss: 5.90815262e-07
Iter: 1692 loss: 5.92457582e-07
Iter: 1693 loss: 5.90781099e-07
Iter: 1694 loss: 5.90526156e-07
Iter: 1695 loss: 5.91794731e-07
Iter: 1696 loss: 5.90479772e-07
Iter: 1697 loss: 5.90246486e-07
Iter: 1698 loss: 5.90206923e-07
Iter: 1699 loss: 5.90019e-07
Iter: 1700 loss: 5.89792194e-07
Iter: 1701 loss: 5.89829369e-07
Iter: 1702 loss: 5.89572778e-07
Iter: 1703 loss: 5.89289584e-07
Iter: 1704 loss: 5.92712809e-07
Iter: 1705 loss: 5.89286628e-07
Iter: 1706 loss: 5.88966259e-07
Iter: 1707 loss: 5.89928845e-07
Iter: 1708 loss: 5.88852686e-07
Iter: 1709 loss: 5.88628779e-07
Iter: 1710 loss: 5.88448415e-07
Iter: 1711 loss: 5.88397e-07
Iter: 1712 loss: 5.87960699e-07
Iter: 1713 loss: 5.91444518e-07
Iter: 1714 loss: 5.87921193e-07
Iter: 1715 loss: 5.87730767e-07
Iter: 1716 loss: 5.87273519e-07
Iter: 1717 loss: 5.91382388e-07
Iter: 1718 loss: 5.87197178e-07
Iter: 1719 loss: 5.86751923e-07
Iter: 1720 loss: 5.92018523e-07
Iter: 1721 loss: 5.86741123e-07
Iter: 1722 loss: 5.86517046e-07
Iter: 1723 loss: 5.88509806e-07
Iter: 1724 loss: 5.8650852e-07
Iter: 1725 loss: 5.86243857e-07
Iter: 1726 loss: 5.86154158e-07
Iter: 1727 loss: 5.85995508e-07
Iter: 1728 loss: 5.85716464e-07
Iter: 1729 loss: 5.8826646e-07
Iter: 1730 loss: 5.85697592e-07
Iter: 1731 loss: 5.85448106e-07
Iter: 1732 loss: 5.85525754e-07
Iter: 1733 loss: 5.85286102e-07
Iter: 1734 loss: 5.84934128e-07
Iter: 1735 loss: 5.85382168e-07
Iter: 1736 loss: 5.84743589e-07
Iter: 1737 loss: 5.84374447e-07
Iter: 1738 loss: 5.84291968e-07
Iter: 1739 loss: 5.84070676e-07
Iter: 1740 loss: 5.8399803e-07
Iter: 1741 loss: 5.83872747e-07
Iter: 1742 loss: 5.83686187e-07
Iter: 1743 loss: 5.83579379e-07
Iter: 1744 loss: 5.83505312e-07
Iter: 1745 loss: 5.83292831e-07
Iter: 1746 loss: 5.84341e-07
Iter: 1747 loss: 5.83258156e-07
Iter: 1748 loss: 5.83011399e-07
Iter: 1749 loss: 5.83056078e-07
Iter: 1750 loss: 5.82800851e-07
Iter: 1751 loss: 5.82522944e-07
Iter: 1752 loss: 5.82140046e-07
Iter: 1753 loss: 5.82139933e-07
Iter: 1754 loss: 5.81646532e-07
Iter: 1755 loss: 5.83796407e-07
Iter: 1756 loss: 5.81553252e-07
Iter: 1757 loss: 5.81377662e-07
Iter: 1758 loss: 5.81317522e-07
Iter: 1759 loss: 5.81128688e-07
Iter: 1760 loss: 5.80860331e-07
Iter: 1761 loss: 5.80861297e-07
Iter: 1762 loss: 5.8061903e-07
Iter: 1763 loss: 5.84439476e-07
Iter: 1764 loss: 5.80617041e-07
Iter: 1765 loss: 5.80436051e-07
Iter: 1766 loss: 5.80518417e-07
Iter: 1767 loss: 5.80279732e-07
Iter: 1768 loss: 5.80066853e-07
Iter: 1769 loss: 5.79830839e-07
Iter: 1770 loss: 5.79795312e-07
Iter: 1771 loss: 5.79336074e-07
Iter: 1772 loss: 5.81818881e-07
Iter: 1773 loss: 5.79281959e-07
Iter: 1774 loss: 5.78976255e-07
Iter: 1775 loss: 5.80177471e-07
Iter: 1776 loss: 5.78902814e-07
Iter: 1777 loss: 5.78540039e-07
Iter: 1778 loss: 5.80073333e-07
Iter: 1779 loss: 5.78447271e-07
Iter: 1780 loss: 5.78258e-07
Iter: 1781 loss: 5.78004403e-07
Iter: 1782 loss: 5.77976436e-07
Iter: 1783 loss: 5.77724791e-07
Iter: 1784 loss: 5.7770535e-07
Iter: 1785 loss: 5.77592232e-07
Iter: 1786 loss: 5.77320179e-07
Iter: 1787 loss: 5.80191113e-07
Iter: 1788 loss: 5.77273681e-07
Iter: 1789 loss: 5.76929153e-07
Iter: 1790 loss: 5.79390701e-07
Iter: 1791 loss: 5.76896e-07
Iter: 1792 loss: 5.76637035e-07
Iter: 1793 loss: 5.7930913e-07
Iter: 1794 loss: 5.7662362e-07
Iter: 1795 loss: 5.76474577e-07
Iter: 1796 loss: 5.76071272e-07
Iter: 1797 loss: 5.82487e-07
Iter: 1798 loss: 5.76053537e-07
Iter: 1799 loss: 5.7565984e-07
Iter: 1800 loss: 5.75661034e-07
Iter: 1801 loss: 5.75460717e-07
Iter: 1802 loss: 5.75344245e-07
Iter: 1803 loss: 5.7524926e-07
Iter: 1804 loss: 5.74943556e-07
Iter: 1805 loss: 5.75441e-07
Iter: 1806 loss: 5.7482157e-07
Iter: 1807 loss: 5.74544401e-07
Iter: 1808 loss: 5.76221112e-07
Iter: 1809 loss: 5.74504384e-07
Iter: 1810 loss: 5.74342835e-07
Iter: 1811 loss: 5.76678076e-07
Iter: 1812 loss: 5.74335957e-07
Iter: 1813 loss: 5.74127512e-07
Iter: 1814 loss: 5.73797934e-07
Iter: 1815 loss: 5.73784575e-07
Iter: 1816 loss: 5.73500643e-07
Iter: 1817 loss: 5.74474598e-07
Iter: 1818 loss: 5.73429361e-07
Iter: 1819 loss: 5.73073805e-07
Iter: 1820 loss: 5.75139211e-07
Iter: 1821 loss: 5.73028274e-07
Iter: 1822 loss: 5.72803401e-07
Iter: 1823 loss: 5.72388899e-07
Iter: 1824 loss: 5.80952474e-07
Iter: 1825 loss: 5.72393901e-07
Iter: 1826 loss: 5.72062902e-07
Iter: 1827 loss: 5.72067279e-07
Iter: 1828 loss: 5.71806368e-07
Iter: 1829 loss: 5.72723764e-07
Iter: 1830 loss: 5.71730538e-07
Iter: 1831 loss: 5.71548355e-07
Iter: 1832 loss: 5.71591386e-07
Iter: 1833 loss: 5.71438761e-07
Iter: 1834 loss: 5.71170062e-07
Iter: 1835 loss: 5.72653562e-07
Iter: 1836 loss: 5.71127202e-07
Iter: 1837 loss: 5.70963948e-07
Iter: 1838 loss: 5.70637326e-07
Iter: 1839 loss: 5.77107e-07
Iter: 1840 loss: 5.7064824e-07
Iter: 1841 loss: 5.70264888e-07
Iter: 1842 loss: 5.7250088e-07
Iter: 1843 loss: 5.70201166e-07
Iter: 1844 loss: 5.69829922e-07
Iter: 1845 loss: 5.70311386e-07
Iter: 1846 loss: 5.69628355e-07
Iter: 1847 loss: 5.69393478e-07
Iter: 1848 loss: 5.69372673e-07
Iter: 1849 loss: 5.69159965e-07
Iter: 1850 loss: 5.69022e-07
Iter: 1851 loss: 5.68937708e-07
Iter: 1852 loss: 5.68729433e-07
Iter: 1853 loss: 5.69253189e-07
Iter: 1854 loss: 5.68636267e-07
Iter: 1855 loss: 5.68332723e-07
Iter: 1856 loss: 5.69887106e-07
Iter: 1857 loss: 5.68294809e-07
Iter: 1858 loss: 5.68131384e-07
Iter: 1859 loss: 5.67763777e-07
Iter: 1860 loss: 5.73778493e-07
Iter: 1861 loss: 5.67756445e-07
Iter: 1862 loss: 5.67529185e-07
Iter: 1863 loss: 5.6750406e-07
Iter: 1864 loss: 5.67259349e-07
Iter: 1865 loss: 5.67227175e-07
Iter: 1866 loss: 5.67044424e-07
Iter: 1867 loss: 5.66752874e-07
Iter: 1868 loss: 5.66548692e-07
Iter: 1869 loss: 5.66458198e-07
Iter: 1870 loss: 5.66268341e-07
Iter: 1871 loss: 5.66227811e-07
Iter: 1872 loss: 5.66015274e-07
Iter: 1873 loss: 5.65701271e-07
Iter: 1874 loss: 5.65687e-07
Iter: 1875 loss: 5.65382038e-07
Iter: 1876 loss: 5.6587362e-07
Iter: 1877 loss: 5.65215885e-07
Iter: 1878 loss: 5.649066e-07
Iter: 1879 loss: 5.67416578e-07
Iter: 1880 loss: 5.64912227e-07
Iter: 1881 loss: 5.64654101e-07
Iter: 1882 loss: 5.6606109e-07
Iter: 1883 loss: 5.64613913e-07
Iter: 1884 loss: 5.64384322e-07
Iter: 1885 loss: 5.64269271e-07
Iter: 1886 loss: 5.6412506e-07
Iter: 1887 loss: 5.63852268e-07
Iter: 1888 loss: 5.64221409e-07
Iter: 1889 loss: 5.63705271e-07
Iter: 1890 loss: 5.63470735e-07
Iter: 1891 loss: 5.63468461e-07
Iter: 1892 loss: 5.63292645e-07
Iter: 1893 loss: 5.63061803e-07
Iter: 1894 loss: 5.63043955e-07
Iter: 1895 loss: 5.62783612e-07
Iter: 1896 loss: 5.62652815e-07
Iter: 1897 loss: 5.62525e-07
Iter: 1898 loss: 5.6240583e-07
Iter: 1899 loss: 5.62303455e-07
Iter: 1900 loss: 5.62156856e-07
Iter: 1901 loss: 5.61775e-07
Iter: 1902 loss: 5.65298e-07
Iter: 1903 loss: 5.61732634e-07
Iter: 1904 loss: 5.61372417e-07
Iter: 1905 loss: 5.65507662e-07
Iter: 1906 loss: 5.61352806e-07
Iter: 1907 loss: 5.61023569e-07
Iter: 1908 loss: 5.62045898e-07
Iter: 1909 loss: 5.60899252e-07
Iter: 1910 loss: 5.60693763e-07
Iter: 1911 loss: 5.60366e-07
Iter: 1912 loss: 5.60351054e-07
Iter: 1913 loss: 5.6000863e-07
Iter: 1914 loss: 5.61775778e-07
Iter: 1915 loss: 5.59932801e-07
Iter: 1916 loss: 5.59733905e-07
Iter: 1917 loss: 5.59700482e-07
Iter: 1918 loss: 5.5960129e-07
Iter: 1919 loss: 5.593227e-07
Iter: 1920 loss: 5.61924821e-07
Iter: 1921 loss: 5.59285127e-07
Iter: 1922 loss: 5.58887848e-07
Iter: 1923 loss: 5.60824049e-07
Iter: 1924 loss: 5.58857664e-07
Iter: 1925 loss: 5.58558668e-07
Iter: 1926 loss: 5.61896854e-07
Iter: 1927 loss: 5.58550425e-07
Iter: 1928 loss: 5.58348461e-07
Iter: 1929 loss: 5.58134388e-07
Iter: 1930 loss: 5.58081865e-07
Iter: 1931 loss: 5.57696126e-07
Iter: 1932 loss: 5.57789861e-07
Iter: 1933 loss: 5.5741657e-07
Iter: 1934 loss: 5.57062e-07
Iter: 1935 loss: 5.58173156e-07
Iter: 1936 loss: 5.56948919e-07
Iter: 1937 loss: 5.56787882e-07
Iter: 1938 loss: 5.56747295e-07
Iter: 1939 loss: 5.56577561e-07
Iter: 1940 loss: 5.56271743e-07
Iter: 1941 loss: 5.56268787e-07
Iter: 1942 loss: 5.5598656e-07
Iter: 1943 loss: 5.57393207e-07
Iter: 1944 loss: 5.55951715e-07
Iter: 1945 loss: 5.55608153e-07
Iter: 1946 loss: 5.56406235e-07
Iter: 1947 loss: 5.5547514e-07
Iter: 1948 loss: 5.55249528e-07
Iter: 1949 loss: 5.55191491e-07
Iter: 1950 loss: 5.55047905e-07
Iter: 1951 loss: 5.54900794e-07
Iter: 1952 loss: 5.54851511e-07
Iter: 1953 loss: 5.54707242e-07
Iter: 1954 loss: 5.54353733e-07
Iter: 1955 loss: 5.57930434e-07
Iter: 1956 loss: 5.54293479e-07
Iter: 1957 loss: 5.53968334e-07
Iter: 1958 loss: 5.55653855e-07
Iter: 1959 loss: 5.53914617e-07
Iter: 1960 loss: 5.53730331e-07
Iter: 1961 loss: 5.53721236e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.6
+ date
Wed Oct 21 11:12:28 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee813df158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81429f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81429268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812f4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812ef598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812ef840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee812796a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81215598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81215ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee81297598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee8129d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d7c1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d7c1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee811b9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee811b9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d77c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d77c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d6fd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d6c5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d6aed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee811d99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d626ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d656620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d761268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d761a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d5a7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d58a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d59b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d52a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d552ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d60a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4ed6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4ed730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4f0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d4da8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fee7d673268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
