+ RUN=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ LAYERS='300_300_300_1 500_500_500_500_1'
+ case $RUN in
+ PSI='2 3'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 100000 				 --batch_size 5000 				 --max_epochs 1000 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output61
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output62
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.yidi/biholo/f0_psi0.5/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0
+ date
Tue Oct 20 16:32:33 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc1a8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc1dc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc298510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc1f1d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc2de6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc209950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc2deea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc2de510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc127b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc157ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc1277b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc06c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc06cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc06c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc08a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90bc3add90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4e88488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4e88b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4df7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4e26ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4db7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4de3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4d7e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4d8cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4d307b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4d309d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4cf1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4cb2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4cb2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4c79f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90b4c79e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90367b82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f90367b8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9036787378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f903672ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9036787048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.80999392e-06
Iter: 2 loss: 6.78424249e-06
Iter: 3 loss: 4.85026248e-06
Iter: 4 loss: 4.3503469e-06
Iter: 5 loss: 3.78269169e-06
Iter: 6 loss: 3.70975658e-06
Iter: 7 loss: 3.51643575e-06
Iter: 8 loss: 3.46506249e-06
Iter: 9 loss: 3.32076638e-06
Iter: 10 loss: 2.91717583e-06
Iter: 11 loss: 5.05496928e-06
Iter: 12 loss: 2.79062124e-06
Iter: 13 loss: 2.7341132e-06
Iter: 14 loss: 2.61259083e-06
Iter: 15 loss: 2.48314313e-06
Iter: 16 loss: 2.75951402e-06
Iter: 17 loss: 2.43246859e-06
Iter: 18 loss: 2.27976852e-06
Iter: 19 loss: 2.12648206e-06
Iter: 20 loss: 2.09559062e-06
Iter: 21 loss: 1.88570561e-06
Iter: 22 loss: 3.0565393e-06
Iter: 23 loss: 1.85593058e-06
Iter: 24 loss: 1.73054752e-06
Iter: 25 loss: 1.72790124e-06
Iter: 26 loss: 1.69438704e-06
Iter: 27 loss: 1.61333367e-06
Iter: 28 loss: 2.47617459e-06
Iter: 29 loss: 1.60444517e-06
Iter: 30 loss: 1.51886729e-06
Iter: 31 loss: 1.54494137e-06
Iter: 32 loss: 1.45766751e-06
Iter: 33 loss: 1.355118e-06
Iter: 34 loss: 1.70003784e-06
Iter: 35 loss: 1.32729474e-06
Iter: 36 loss: 1.27451403e-06
Iter: 37 loss: 1.25808845e-06
Iter: 38 loss: 1.21654011e-06
Iter: 39 loss: 1.11503743e-06
Iter: 40 loss: 2.16498665e-06
Iter: 41 loss: 1.10316012e-06
Iter: 42 loss: 1.1046651e-06
Iter: 43 loss: 1.05769152e-06
Iter: 44 loss: 1.03694993e-06
Iter: 45 loss: 9.89371642e-07
Iter: 46 loss: 1.60174409e-06
Iter: 47 loss: 9.86097e-07
Iter: 48 loss: 9.69809435e-07
Iter: 49 loss: 9.64025276e-07
Iter: 50 loss: 9.4342181e-07
Iter: 51 loss: 8.95035441e-07
Iter: 52 loss: 1.46676871e-06
Iter: 53 loss: 8.90871e-07
Iter: 54 loss: 8.37176344e-07
Iter: 55 loss: 1.0641827e-06
Iter: 56 loss: 8.25880079e-07
Iter: 57 loss: 7.8174611e-07
Iter: 58 loss: 9.44656335e-07
Iter: 59 loss: 7.70893053e-07
Iter: 60 loss: 7.43483099e-07
Iter: 61 loss: 7.43285455e-07
Iter: 62 loss: 7.21249194e-07
Iter: 63 loss: 6.8629987e-07
Iter: 64 loss: 6.85971372e-07
Iter: 65 loss: 6.6228074e-07
Iter: 66 loss: 7.37457299e-07
Iter: 67 loss: 6.55489202e-07
Iter: 68 loss: 6.42949317e-07
Iter: 69 loss: 6.4238634e-07
Iter: 70 loss: 6.28671785e-07
Iter: 71 loss: 6.1125877e-07
Iter: 72 loss: 6.09970584e-07
Iter: 73 loss: 5.90978061e-07
Iter: 74 loss: 6.48534296e-07
Iter: 75 loss: 5.85315e-07
Iter: 76 loss: 5.55121346e-07
Iter: 77 loss: 5.71338433e-07
Iter: 78 loss: 5.35261506e-07
Iter: 79 loss: 5.22702294e-07
Iter: 80 loss: 5.81955646e-07
Iter: 81 loss: 5.20406e-07
Iter: 82 loss: 5.07610252e-07
Iter: 83 loss: 5.90063223e-07
Iter: 84 loss: 5.06237484e-07
Iter: 85 loss: 4.99448447e-07
Iter: 86 loss: 4.9029245e-07
Iter: 87 loss: 4.89824288e-07
Iter: 88 loss: 4.80080587e-07
Iter: 89 loss: 5.69461e-07
Iter: 90 loss: 4.79649543e-07
Iter: 91 loss: 4.72520412e-07
Iter: 92 loss: 4.87865918e-07
Iter: 93 loss: 4.69756799e-07
Iter: 94 loss: 4.5736752e-07
Iter: 95 loss: 4.61758731e-07
Iter: 96 loss: 4.48651377e-07
Iter: 97 loss: 4.36497629e-07
Iter: 98 loss: 4.30997716e-07
Iter: 99 loss: 4.24917658e-07
Iter: 100 loss: 4.18734288e-07
Iter: 101 loss: 4.17462957e-07
Iter: 102 loss: 4.10182793e-07
Iter: 103 loss: 4.18062541e-07
Iter: 104 loss: 4.06191305e-07
Iter: 105 loss: 4.01077443e-07
Iter: 106 loss: 4.0149024e-07
Iter: 107 loss: 3.97122648e-07
Iter: 108 loss: 3.90645511e-07
Iter: 109 loss: 4.86126339e-07
Iter: 110 loss: 3.9065128e-07
Iter: 111 loss: 3.86871847e-07
Iter: 112 loss: 3.77396418e-07
Iter: 113 loss: 4.62496956e-07
Iter: 114 loss: 3.75945405e-07
Iter: 115 loss: 3.73471039e-07
Iter: 116 loss: 3.70568301e-07
Iter: 117 loss: 3.65803118e-07
Iter: 118 loss: 3.5325877e-07
Iter: 119 loss: 4.48104259e-07
Iter: 120 loss: 3.50735434e-07
Iter: 121 loss: 3.42380872e-07
Iter: 122 loss: 3.423647e-07
Iter: 123 loss: 3.36701618e-07
Iter: 124 loss: 3.44054172e-07
Iter: 125 loss: 3.33819031e-07
Iter: 126 loss: 3.30828385e-07
Iter: 127 loss: 3.30232126e-07
Iter: 128 loss: 3.27892053e-07
Iter: 129 loss: 3.2184937e-07
Iter: 130 loss: 3.7195457e-07
Iter: 131 loss: 3.20801519e-07
Iter: 132 loss: 3.1255712e-07
Iter: 133 loss: 3.10844086e-07
Iter: 134 loss: 3.05422589e-07
Iter: 135 loss: 2.93807403e-07
Iter: 136 loss: 3.32575212e-07
Iter: 137 loss: 2.90630396e-07
Iter: 138 loss: 3.00954412e-07
Iter: 139 loss: 2.87864225e-07
Iter: 140 loss: 2.85622775e-07
Iter: 141 loss: 2.80826e-07
Iter: 142 loss: 3.57134127e-07
Iter: 143 loss: 2.80661453e-07
Iter: 144 loss: 2.78219773e-07
Iter: 145 loss: 3.04087848e-07
Iter: 146 loss: 2.7815679e-07
Iter: 147 loss: 2.75134113e-07
Iter: 148 loss: 2.74428544e-07
Iter: 149 loss: 2.72461563e-07
Iter: 150 loss: 2.68981921e-07
Iter: 151 loss: 2.64462642e-07
Iter: 152 loss: 2.6416032e-07
Iter: 153 loss: 2.64692858e-07
Iter: 154 loss: 2.61654492e-07
Iter: 155 loss: 2.59857558e-07
Iter: 156 loss: 2.55607773e-07
Iter: 157 loss: 3.0456647e-07
Iter: 158 loss: 2.55210892e-07
Iter: 159 loss: 2.52138136e-07
Iter: 160 loss: 2.85454348e-07
Iter: 161 loss: 2.52065718e-07
Iter: 162 loss: 2.49509327e-07
Iter: 163 loss: 2.67308877e-07
Iter: 164 loss: 2.49276383e-07
Iter: 165 loss: 2.47230076e-07
Iter: 166 loss: 2.49762877e-07
Iter: 167 loss: 2.46167929e-07
Iter: 168 loss: 2.44110879e-07
Iter: 169 loss: 2.40689019e-07
Iter: 170 loss: 2.40682255e-07
Iter: 171 loss: 2.35867091e-07
Iter: 172 loss: 2.37536867e-07
Iter: 173 loss: 2.32485462e-07
Iter: 174 loss: 2.39102803e-07
Iter: 175 loss: 2.30653129e-07
Iter: 176 loss: 2.2968706e-07
Iter: 177 loss: 2.27313805e-07
Iter: 178 loss: 2.51491031e-07
Iter: 179 loss: 2.27049185e-07
Iter: 180 loss: 2.2533338e-07
Iter: 181 loss: 2.44108321e-07
Iter: 182 loss: 2.25294855e-07
Iter: 183 loss: 2.23176471e-07
Iter: 184 loss: 2.22600804e-07
Iter: 185 loss: 2.21292794e-07
Iter: 186 loss: 2.19229776e-07
Iter: 187 loss: 2.16530836e-07
Iter: 188 loss: 2.16361116e-07
Iter: 189 loss: 2.14105739e-07
Iter: 190 loss: 2.13675278e-07
Iter: 191 loss: 2.11966466e-07
Iter: 192 loss: 2.07760337e-07
Iter: 193 loss: 2.51131155e-07
Iter: 194 loss: 2.07274724e-07
Iter: 195 loss: 2.04140321e-07
Iter: 196 loss: 2.28630867e-07
Iter: 197 loss: 2.03909991e-07
Iter: 198 loss: 2.02955704e-07
Iter: 199 loss: 2.02666897e-07
Iter: 200 loss: 2.01972114e-07
Iter: 201 loss: 2.01331673e-07
Iter: 202 loss: 2.01171957e-07
Iter: 203 loss: 1.99660533e-07
Iter: 204 loss: 1.9769314e-07
Iter: 205 loss: 1.97566152e-07
Iter: 206 loss: 1.94773477e-07
Iter: 207 loss: 1.97300722e-07
Iter: 208 loss: 1.93145937e-07
Iter: 209 loss: 1.91635067e-07
Iter: 210 loss: 1.91504398e-07
Iter: 211 loss: 1.89554981e-07
Iter: 212 loss: 1.92210223e-07
Iter: 213 loss: 1.8857088e-07
Iter: 214 loss: 1.87646336e-07
Iter: 215 loss: 1.88050876e-07
Iter: 216 loss: 1.87004062e-07
Iter: 217 loss: 1.86150231e-07
Iter: 218 loss: 1.86130876e-07
Iter: 219 loss: 1.85626533e-07
Iter: 220 loss: 1.84059886e-07
Iter: 221 loss: 1.86215772e-07
Iter: 222 loss: 1.82894297e-07
Iter: 223 loss: 1.82588764e-07
Iter: 224 loss: 1.81732418e-07
Iter: 225 loss: 1.80469385e-07
Iter: 226 loss: 1.78382422e-07
Iter: 227 loss: 1.78370101e-07
Iter: 228 loss: 1.76728321e-07
Iter: 229 loss: 1.78977785e-07
Iter: 230 loss: 1.7593689e-07
Iter: 231 loss: 1.74989964e-07
Iter: 232 loss: 1.74962281e-07
Iter: 233 loss: 1.73795257e-07
Iter: 234 loss: 1.73909953e-07
Iter: 235 loss: 1.72892499e-07
Iter: 236 loss: 1.72035442e-07
Iter: 237 loss: 1.71058986e-07
Iter: 238 loss: 1.70923883e-07
Iter: 239 loss: 1.6888913e-07
Iter: 240 loss: 1.7364988e-07
Iter: 241 loss: 1.68126974e-07
Iter: 242 loss: 1.65910748e-07
Iter: 243 loss: 1.70599861e-07
Iter: 244 loss: 1.65037633e-07
Iter: 245 loss: 1.66117488e-07
Iter: 246 loss: 1.64527535e-07
Iter: 247 loss: 1.6419817e-07
Iter: 248 loss: 1.63177162e-07
Iter: 249 loss: 1.65047211e-07
Iter: 250 loss: 1.62510901e-07
Iter: 251 loss: 1.62055159e-07
Iter: 252 loss: 1.6186911e-07
Iter: 253 loss: 1.61211062e-07
Iter: 254 loss: 1.61439829e-07
Iter: 255 loss: 1.60737329e-07
Iter: 256 loss: 1.60055805e-07
Iter: 257 loss: 1.59073878e-07
Iter: 258 loss: 1.5904709e-07
Iter: 259 loss: 1.5754371e-07
Iter: 260 loss: 1.75049919e-07
Iter: 261 loss: 1.57526088e-07
Iter: 262 loss: 1.56802685e-07
Iter: 263 loss: 1.5670318e-07
Iter: 264 loss: 1.56181983e-07
Iter: 265 loss: 1.55262512e-07
Iter: 266 loss: 1.67546915e-07
Iter: 267 loss: 1.55258476e-07
Iter: 268 loss: 1.54790371e-07
Iter: 269 loss: 1.53961892e-07
Iter: 270 loss: 1.53964606e-07
Iter: 271 loss: 1.5312574e-07
Iter: 272 loss: 1.5567278e-07
Iter: 273 loss: 1.52888049e-07
Iter: 274 loss: 1.51939773e-07
Iter: 275 loss: 1.54169697e-07
Iter: 276 loss: 1.51592189e-07
Iter: 277 loss: 1.50922e-07
Iter: 278 loss: 1.50901087e-07
Iter: 279 loss: 1.5035171e-07
Iter: 280 loss: 1.49206556e-07
Iter: 281 loss: 1.67786311e-07
Iter: 282 loss: 1.49169225e-07
Iter: 283 loss: 1.48181272e-07
Iter: 284 loss: 1.49969878e-07
Iter: 285 loss: 1.47745013e-07
Iter: 286 loss: 1.46845935e-07
Iter: 287 loss: 1.46832022e-07
Iter: 288 loss: 1.46453047e-07
Iter: 289 loss: 1.45653587e-07
Iter: 290 loss: 1.60635722e-07
Iter: 291 loss: 1.45657935e-07
Iter: 292 loss: 1.45409629e-07
Iter: 293 loss: 1.45268544e-07
Iter: 294 loss: 1.44859797e-07
Iter: 295 loss: 1.43863048e-07
Iter: 296 loss: 1.53189632e-07
Iter: 297 loss: 1.4371696e-07
Iter: 298 loss: 1.42820781e-07
Iter: 299 loss: 1.52140146e-07
Iter: 300 loss: 1.4279729e-07
Iter: 301 loss: 1.41831748e-07
Iter: 302 loss: 1.44336298e-07
Iter: 303 loss: 1.41503293e-07
Iter: 304 loss: 1.41039735e-07
Iter: 305 loss: 1.40185705e-07
Iter: 306 loss: 1.59944761e-07
Iter: 307 loss: 1.40187836e-07
Iter: 308 loss: 1.3951464e-07
Iter: 309 loss: 1.39499008e-07
Iter: 310 loss: 1.39022475e-07
Iter: 311 loss: 1.40065822e-07
Iter: 312 loss: 1.38836413e-07
Iter: 313 loss: 1.38267467e-07
Iter: 314 loss: 1.37296325e-07
Iter: 315 loss: 1.37304781e-07
Iter: 316 loss: 1.3778174e-07
Iter: 317 loss: 1.36857054e-07
Iter: 318 loss: 1.36464e-07
Iter: 319 loss: 1.36154142e-07
Iter: 320 loss: 1.36021484e-07
Iter: 321 loss: 1.35699295e-07
Iter: 322 loss: 1.3805284e-07
Iter: 323 loss: 1.3567842e-07
Iter: 324 loss: 1.35298478e-07
Iter: 325 loss: 1.35230707e-07
Iter: 326 loss: 1.34967962e-07
Iter: 327 loss: 1.3461748e-07
Iter: 328 loss: 1.34215199e-07
Iter: 329 loss: 1.34164907e-07
Iter: 330 loss: 1.33838356e-07
Iter: 331 loss: 1.33811028e-07
Iter: 332 loss: 1.33384788e-07
Iter: 333 loss: 1.32931433e-07
Iter: 334 loss: 1.32857195e-07
Iter: 335 loss: 1.32278942e-07
Iter: 336 loss: 1.31964669e-07
Iter: 337 loss: 1.31697192e-07
Iter: 338 loss: 1.31485592e-07
Iter: 339 loss: 1.31223743e-07
Iter: 340 loss: 1.31042896e-07
Iter: 341 loss: 1.30698766e-07
Iter: 342 loss: 1.38224379e-07
Iter: 343 loss: 1.30702773e-07
Iter: 344 loss: 1.30327749e-07
Iter: 345 loss: 1.30669946e-07
Iter: 346 loss: 1.30122316e-07
Iter: 347 loss: 1.29526981e-07
Iter: 348 loss: 1.30233829e-07
Iter: 349 loss: 1.29216289e-07
Iter: 350 loss: 1.29052296e-07
Iter: 351 loss: 1.28960295e-07
Iter: 352 loss: 1.28688797e-07
Iter: 353 loss: 1.28129528e-07
Iter: 354 loss: 1.3818223e-07
Iter: 355 loss: 1.28121769e-07
Iter: 356 loss: 1.27843066e-07
Iter: 357 loss: 1.27829026e-07
Iter: 358 loss: 1.27579767e-07
Iter: 359 loss: 1.27166103e-07
Iter: 360 loss: 1.27163261e-07
Iter: 361 loss: 1.26821405e-07
Iter: 362 loss: 1.28243116e-07
Iter: 363 loss: 1.26743544e-07
Iter: 364 loss: 1.26349136e-07
Iter: 365 loss: 1.31553492e-07
Iter: 366 loss: 1.26342186e-07
Iter: 367 loss: 1.26153438e-07
Iter: 368 loss: 1.2559282e-07
Iter: 369 loss: 1.28636088e-07
Iter: 370 loss: 1.2543299e-07
Iter: 371 loss: 1.25390116e-07
Iter: 372 loss: 1.25104322e-07
Iter: 373 loss: 1.24813354e-07
Iter: 374 loss: 1.2486349e-07
Iter: 375 loss: 1.24594351e-07
Iter: 376 loss: 1.24336665e-07
Iter: 377 loss: 1.24171734e-07
Iter: 378 loss: 1.24069743e-07
Iter: 379 loss: 1.23763201e-07
Iter: 380 loss: 1.23272713e-07
Iter: 381 loss: 1.23260321e-07
Iter: 382 loss: 1.229063e-07
Iter: 383 loss: 1.22958483e-07
Iter: 384 loss: 1.22641652e-07
Iter: 385 loss: 1.2199979e-07
Iter: 386 loss: 1.23091439e-07
Iter: 387 loss: 1.21717918e-07
Iter: 388 loss: 1.21187639e-07
Iter: 389 loss: 1.24320749e-07
Iter: 390 loss: 1.21113587e-07
Iter: 391 loss: 1.20776804e-07
Iter: 392 loss: 1.2077507e-07
Iter: 393 loss: 1.20609954e-07
Iter: 394 loss: 1.20146268e-07
Iter: 395 loss: 1.2275035e-07
Iter: 396 loss: 1.20002952e-07
Iter: 397 loss: 1.19429572e-07
Iter: 398 loss: 1.19826112e-07
Iter: 399 loss: 1.19059045e-07
Iter: 400 loss: 1.19499688e-07
Iter: 401 loss: 1.18808714e-07
Iter: 402 loss: 1.18592958e-07
Iter: 403 loss: 1.18053208e-07
Iter: 404 loss: 1.2251e-07
Iter: 405 loss: 1.17960909e-07
Iter: 406 loss: 1.17279313e-07
Iter: 407 loss: 1.17806636e-07
Iter: 408 loss: 1.16867298e-07
Iter: 409 loss: 1.17003125e-07
Iter: 410 loss: 1.16551533e-07
Iter: 411 loss: 1.1634927e-07
Iter: 412 loss: 1.16003285e-07
Iter: 413 loss: 1.15998155e-07
Iter: 414 loss: 1.15749096e-07
Iter: 415 loss: 1.17928039e-07
Iter: 416 loss: 1.15740178e-07
Iter: 417 loss: 1.15462655e-07
Iter: 418 loss: 1.16532625e-07
Iter: 419 loss: 1.15403779e-07
Iter: 420 loss: 1.15251794e-07
Iter: 421 loss: 1.14961367e-07
Iter: 422 loss: 1.20607879e-07
Iter: 423 loss: 1.14958077e-07
Iter: 424 loss: 1.1474522e-07
Iter: 425 loss: 1.14733176e-07
Iter: 426 loss: 1.14516169e-07
Iter: 427 loss: 1.14417858e-07
Iter: 428 loss: 1.14308328e-07
Iter: 429 loss: 1.14001764e-07
Iter: 430 loss: 1.14240372e-07
Iter: 431 loss: 1.13823319e-07
Iter: 432 loss: 1.13427582e-07
Iter: 433 loss: 1.14010291e-07
Iter: 434 loss: 1.13230293e-07
Iter: 435 loss: 1.12967108e-07
Iter: 436 loss: 1.12929314e-07
Iter: 437 loss: 1.12746015e-07
Iter: 438 loss: 1.12564244e-07
Iter: 439 loss: 1.12478496e-07
Iter: 440 loss: 1.1231397e-07
Iter: 441 loss: 1.11900931e-07
Iter: 442 loss: 1.15768991e-07
Iter: 443 loss: 1.11831028e-07
Iter: 444 loss: 1.11364443e-07
Iter: 445 loss: 1.11505052e-07
Iter: 446 loss: 1.11020363e-07
Iter: 447 loss: 1.11108953e-07
Iter: 448 loss: 1.10798425e-07
Iter: 449 loss: 1.10594101e-07
Iter: 450 loss: 1.10530742e-07
Iter: 451 loss: 1.10415272e-07
Iter: 452 loss: 1.10274549e-07
Iter: 453 loss: 1.10273035e-07
Iter: 454 loss: 1.10141436e-07
Iter: 455 loss: 1.09882023e-07
Iter: 456 loss: 1.15541496e-07
Iter: 457 loss: 1.09879601e-07
Iter: 458 loss: 1.09638073e-07
Iter: 459 loss: 1.10633366e-07
Iter: 460 loss: 1.09586296e-07
Iter: 461 loss: 1.09278574e-07
Iter: 462 loss: 1.10075689e-07
Iter: 463 loss: 1.09181187e-07
Iter: 464 loss: 1.08982327e-07
Iter: 465 loss: 1.09002272e-07
Iter: 466 loss: 1.08846848e-07
Iter: 467 loss: 1.08564528e-07
Iter: 468 loss: 1.09007381e-07
Iter: 469 loss: 1.08439892e-07
Iter: 470 loss: 1.08115792e-07
Iter: 471 loss: 1.08472079e-07
Iter: 472 loss: 1.07941624e-07
Iter: 473 loss: 1.07940643e-07
Iter: 474 loss: 1.07815502e-07
Iter: 475 loss: 1.07707343e-07
Iter: 476 loss: 1.07403174e-07
Iter: 477 loss: 1.08558581e-07
Iter: 478 loss: 1.07272136e-07
Iter: 479 loss: 1.06982881e-07
Iter: 480 loss: 1.10776028e-07
Iter: 481 loss: 1.06981268e-07
Iter: 482 loss: 1.06711312e-07
Iter: 483 loss: 1.08733303e-07
Iter: 484 loss: 1.06687914e-07
Iter: 485 loss: 1.06556882e-07
Iter: 486 loss: 1.0671674e-07
Iter: 487 loss: 1.06490326e-07
Iter: 488 loss: 1.06325956e-07
Iter: 489 loss: 1.07460309e-07
Iter: 490 loss: 1.06300909e-07
Iter: 491 loss: 1.06207594e-07
Iter: 492 loss: 1.06018035e-07
Iter: 493 loss: 1.09960268e-07
Iter: 494 loss: 1.06020252e-07
Iter: 495 loss: 1.05941105e-07
Iter: 496 loss: 1.05909223e-07
Iter: 497 loss: 1.05815481e-07
Iter: 498 loss: 1.05624579e-07
Iter: 499 loss: 1.09841025e-07
Iter: 500 loss: 1.05625986e-07
Iter: 501 loss: 1.05409953e-07
Iter: 502 loss: 1.05294376e-07
Iter: 503 loss: 1.05200343e-07
Iter: 504 loss: 1.04812692e-07
Iter: 505 loss: 1.06780838e-07
Iter: 506 loss: 1.04751052e-07
Iter: 507 loss: 1.04510974e-07
Iter: 508 loss: 1.06785151e-07
Iter: 509 loss: 1.0450438e-07
Iter: 510 loss: 1.04284752e-07
Iter: 511 loss: 1.04899541e-07
Iter: 512 loss: 1.04225236e-07
Iter: 513 loss: 1.04119614e-07
Iter: 514 loss: 1.03935683e-07
Iter: 515 loss: 1.03938063e-07
Iter: 516 loss: 1.03860046e-07
Iter: 517 loss: 1.03818273e-07
Iter: 518 loss: 1.03728091e-07
Iter: 519 loss: 1.03488659e-07
Iter: 520 loss: 1.05301496e-07
Iter: 521 loss: 1.0343286e-07
Iter: 522 loss: 1.03507084e-07
Iter: 523 loss: 1.03341051e-07
Iter: 524 loss: 1.03249462e-07
Iter: 525 loss: 1.03099623e-07
Iter: 526 loss: 1.03101534e-07
Iter: 527 loss: 1.02919017e-07
Iter: 528 loss: 1.0438643e-07
Iter: 529 loss: 1.02904508e-07
Iter: 530 loss: 1.02779993e-07
Iter: 531 loss: 1.02749041e-07
Iter: 532 loss: 1.02679962e-07
Iter: 533 loss: 1.02543645e-07
Iter: 534 loss: 1.02442229e-07
Iter: 535 loss: 1.02401131e-07
Iter: 536 loss: 1.02223687e-07
Iter: 537 loss: 1.021156e-07
Iter: 538 loss: 1.02048269e-07
Iter: 539 loss: 1.01762225e-07
Iter: 540 loss: 1.02127331e-07
Iter: 541 loss: 1.01609928e-07
Iter: 542 loss: 1.01369707e-07
Iter: 543 loss: 1.02358044e-07
Iter: 544 loss: 1.0131928e-07
Iter: 545 loss: 1.01197976e-07
Iter: 546 loss: 1.01873731e-07
Iter: 547 loss: 1.011812e-07
Iter: 548 loss: 1.01015246e-07
Iter: 549 loss: 1.01266338e-07
Iter: 550 loss: 1.00929768e-07
Iter: 551 loss: 1.0076797e-07
Iter: 552 loss: 1.01454432e-07
Iter: 553 loss: 1.00733189e-07
Iter: 554 loss: 1.00624533e-07
Iter: 555 loss: 1.00381698e-07
Iter: 556 loss: 1.04252777e-07
Iter: 557 loss: 1.00365256e-07
Iter: 558 loss: 1.00101602e-07
Iter: 559 loss: 1.01382128e-07
Iter: 560 loss: 1.00049917e-07
Iter: 561 loss: 9.99135352e-08
Iter: 562 loss: 9.98777381e-08
Iter: 563 loss: 9.98231116e-08
Iter: 564 loss: 9.96857636e-08
Iter: 565 loss: 1.01200868e-07
Iter: 566 loss: 9.96687746e-08
Iter: 567 loss: 9.95867e-08
Iter: 568 loss: 9.95672877e-08
Iter: 569 loss: 9.94957929e-08
Iter: 570 loss: 9.9279255e-08
Iter: 571 loss: 1.00047259e-07
Iter: 572 loss: 9.91847457e-08
Iter: 573 loss: 9.89046356e-08
Iter: 574 loss: 9.96258365e-08
Iter: 575 loss: 9.88189157e-08
Iter: 576 loss: 9.86491671e-08
Iter: 577 loss: 9.86442643e-08
Iter: 578 loss: 9.85032926e-08
Iter: 579 loss: 9.87632163e-08
Iter: 580 loss: 9.84555797e-08
Iter: 581 loss: 9.83118156e-08
Iter: 582 loss: 9.90376705e-08
Iter: 583 loss: 9.82954731e-08
Iter: 584 loss: 9.82000188e-08
Iter: 585 loss: 9.84707356e-08
Iter: 586 loss: 9.81704744e-08
Iter: 587 loss: 9.80453763e-08
Iter: 588 loss: 9.81718884e-08
Iter: 589 loss: 9.79847812e-08
Iter: 590 loss: 9.78536079e-08
Iter: 591 loss: 9.7529167e-08
Iter: 592 loss: 1.00141676e-07
Iter: 593 loss: 9.74698509e-08
Iter: 594 loss: 9.76830918e-08
Iter: 595 loss: 9.73494778e-08
Iter: 596 loss: 9.72542e-08
Iter: 597 loss: 9.71027276e-08
Iter: 598 loss: 9.70948548e-08
Iter: 599 loss: 9.69791074e-08
Iter: 600 loss: 9.68933733e-08
Iter: 601 loss: 9.68537393e-08
Iter: 602 loss: 9.68349383e-08
Iter: 603 loss: 9.67678844e-08
Iter: 604 loss: 9.66795568e-08
Iter: 605 loss: 9.65064402e-08
Iter: 606 loss: 9.96183616e-08
Iter: 607 loss: 9.65043654e-08
Iter: 608 loss: 9.63594289e-08
Iter: 609 loss: 9.78733539e-08
Iter: 610 loss: 9.63486713e-08
Iter: 611 loss: 9.62369242e-08
Iter: 612 loss: 9.65172546e-08
Iter: 613 loss: 9.61864899e-08
Iter: 614 loss: 9.60533413e-08
Iter: 615 loss: 9.65731672e-08
Iter: 616 loss: 9.60180557e-08
Iter: 617 loss: 9.59342685e-08
Iter: 618 loss: 9.63012283e-08
Iter: 619 loss: 9.59244488e-08
Iter: 620 loss: 9.58665e-08
Iter: 621 loss: 9.58696091e-08
Iter: 622 loss: 9.58162e-08
Iter: 623 loss: 9.57279838e-08
Iter: 624 loss: 9.69945688e-08
Iter: 625 loss: 9.57137232e-08
Iter: 626 loss: 9.56568e-08
Iter: 627 loss: 9.56541939e-08
Iter: 628 loss: 9.55863584e-08
Iter: 629 loss: 9.55726e-08
Iter: 630 loss: 9.55259623e-08
Iter: 631 loss: 9.54676906e-08
Iter: 632 loss: 9.53198e-08
Iter: 633 loss: 9.72196545e-08
Iter: 634 loss: 9.53142631e-08
Iter: 635 loss: 9.52484527e-08
Iter: 636 loss: 9.52172243e-08
Iter: 637 loss: 9.51455235e-08
Iter: 638 loss: 9.50292787e-08
Iter: 639 loss: 9.50250794e-08
Iter: 640 loss: 9.49252e-08
Iter: 641 loss: 9.55996e-08
Iter: 642 loss: 9.49121e-08
Iter: 643 loss: 9.48262624e-08
Iter: 644 loss: 9.5251167e-08
Iter: 645 loss: 9.48148937e-08
Iter: 646 loss: 9.47320729e-08
Iter: 647 loss: 9.49551264e-08
Iter: 648 loss: 9.4704518e-08
Iter: 649 loss: 9.46243262e-08
Iter: 650 loss: 9.45623384e-08
Iter: 651 loss: 9.45362402e-08
Iter: 652 loss: 9.44257863e-08
Iter: 653 loss: 9.44284082e-08
Iter: 654 loss: 9.43532541e-08
Iter: 655 loss: 9.43053777e-08
Iter: 656 loss: 9.42711935e-08
Iter: 657 loss: 9.41777074e-08
Iter: 658 loss: 9.42799829e-08
Iter: 659 loss: 9.41221e-08
Iter: 660 loss: 9.40187377e-08
Iter: 661 loss: 9.55492396e-08
Iter: 662 loss: 9.40206561e-08
Iter: 663 loss: 9.39856619e-08
Iter: 664 loss: 9.3884907e-08
Iter: 665 loss: 9.45733731e-08
Iter: 666 loss: 9.38639886e-08
Iter: 667 loss: 9.37318916e-08
Iter: 668 loss: 9.3898862e-08
Iter: 669 loss: 9.36625497e-08
Iter: 670 loss: 9.35555846e-08
Iter: 671 loss: 9.35478681e-08
Iter: 672 loss: 9.3504255e-08
Iter: 673 loss: 9.34179099e-08
Iter: 674 loss: 9.50854e-08
Iter: 675 loss: 9.34204323e-08
Iter: 676 loss: 9.33259798e-08
Iter: 677 loss: 9.40937781e-08
Iter: 678 loss: 9.33298097e-08
Iter: 679 loss: 9.32579596e-08
Iter: 680 loss: 9.35441165e-08
Iter: 681 loss: 9.32397626e-08
Iter: 682 loss: 9.31599686e-08
Iter: 683 loss: 9.32728597e-08
Iter: 684 loss: 9.31340907e-08
Iter: 685 loss: 9.30555615e-08
Iter: 686 loss: 9.32012512e-08
Iter: 687 loss: 9.3015295e-08
Iter: 688 loss: 9.28956041e-08
Iter: 689 loss: 9.30388779e-08
Iter: 690 loss: 9.28275909e-08
Iter: 691 loss: 9.27374373e-08
Iter: 692 loss: 9.27959e-08
Iter: 693 loss: 9.26773396e-08
Iter: 694 loss: 9.25941492e-08
Iter: 695 loss: 9.25918897e-08
Iter: 696 loss: 9.25449939e-08
Iter: 697 loss: 9.24467e-08
Iter: 698 loss: 9.45209493e-08
Iter: 699 loss: 9.2448019e-08
Iter: 700 loss: 9.2379544e-08
Iter: 701 loss: 9.26287811e-08
Iter: 702 loss: 9.23503833e-08
Iter: 703 loss: 9.22990324e-08
Iter: 704 loss: 9.22996577e-08
Iter: 705 loss: 9.22290297e-08
Iter: 706 loss: 9.21028516e-08
Iter: 707 loss: 9.41097795e-08
Iter: 708 loss: 9.21027237e-08
Iter: 709 loss: 9.19915664e-08
Iter: 710 loss: 9.25561068e-08
Iter: 711 loss: 9.1979274e-08
Iter: 712 loss: 9.19074736e-08
Iter: 713 loss: 9.31005957e-08
Iter: 714 loss: 9.19096692e-08
Iter: 715 loss: 9.18486265e-08
Iter: 716 loss: 9.18791727e-08
Iter: 717 loss: 9.18168155e-08
Iter: 718 loss: 9.17282108e-08
Iter: 719 loss: 9.17480065e-08
Iter: 720 loss: 9.16709411e-08
Iter: 721 loss: 9.15630096e-08
Iter: 722 loss: 9.25675749e-08
Iter: 723 loss: 9.15576e-08
Iter: 724 loss: 9.15017324e-08
Iter: 725 loss: 9.14416205e-08
Iter: 726 loss: 9.14386646e-08
Iter: 727 loss: 9.13670846e-08
Iter: 728 loss: 9.13631908e-08
Iter: 729 loss: 9.13100422e-08
Iter: 730 loss: 9.11680402e-08
Iter: 731 loss: 9.28142185e-08
Iter: 732 loss: 9.11536659e-08
Iter: 733 loss: 9.10395244e-08
Iter: 734 loss: 9.13360054e-08
Iter: 735 loss: 9.09904116e-08
Iter: 736 loss: 9.09309605e-08
Iter: 737 loss: 9.09244093e-08
Iter: 738 loss: 9.08446367e-08
Iter: 739 loss: 9.07993041e-08
Iter: 740 loss: 9.0771195e-08
Iter: 741 loss: 9.06745186e-08
Iter: 742 loss: 9.06032938e-08
Iter: 743 loss: 9.05713762e-08
Iter: 744 loss: 9.04634589e-08
Iter: 745 loss: 9.04565667e-08
Iter: 746 loss: 9.03629171e-08
Iter: 747 loss: 9.03987214e-08
Iter: 748 loss: 9.02998494e-08
Iter: 749 loss: 9.01760728e-08
Iter: 750 loss: 9.02900439e-08
Iter: 751 loss: 9.00928256e-08
Iter: 752 loss: 8.99477186e-08
Iter: 753 loss: 9.11634075e-08
Iter: 754 loss: 8.99304098e-08
Iter: 755 loss: 8.98181e-08
Iter: 756 loss: 8.9697366e-08
Iter: 757 loss: 8.96755381e-08
Iter: 758 loss: 8.9639812e-08
Iter: 759 loss: 8.96168117e-08
Iter: 760 loss: 8.95537653e-08
Iter: 761 loss: 8.94430343e-08
Iter: 762 loss: 9.1484921e-08
Iter: 763 loss: 8.94385224e-08
Iter: 764 loss: 8.93076e-08
Iter: 765 loss: 8.92233771e-08
Iter: 766 loss: 8.9179693e-08
Iter: 767 loss: 8.90777727e-08
Iter: 768 loss: 8.90702694e-08
Iter: 769 loss: 8.89594247e-08
Iter: 770 loss: 8.90962042e-08
Iter: 771 loss: 8.88973375e-08
Iter: 772 loss: 8.87932927e-08
Iter: 773 loss: 8.86128504e-08
Iter: 774 loss: 8.86070524e-08
Iter: 775 loss: 8.85198e-08
Iter: 776 loss: 8.85024036e-08
Iter: 777 loss: 8.84133e-08
Iter: 778 loss: 8.86175542e-08
Iter: 779 loss: 8.83762183e-08
Iter: 780 loss: 8.83013058e-08
Iter: 781 loss: 8.84192488e-08
Iter: 782 loss: 8.82642581e-08
Iter: 783 loss: 8.81726123e-08
Iter: 784 loss: 8.87024925e-08
Iter: 785 loss: 8.8168008e-08
Iter: 786 loss: 8.8071495e-08
Iter: 787 loss: 8.79660647e-08
Iter: 788 loss: 8.79468161e-08
Iter: 789 loss: 8.78535786e-08
Iter: 790 loss: 8.87451606e-08
Iter: 791 loss: 8.78444268e-08
Iter: 792 loss: 8.7741526e-08
Iter: 793 loss: 8.78470559e-08
Iter: 794 loss: 8.76732145e-08
Iter: 795 loss: 8.75932926e-08
Iter: 796 loss: 8.74099726e-08
Iter: 797 loss: 9.06838906e-08
Iter: 798 loss: 8.74072867e-08
Iter: 799 loss: 8.72716797e-08
Iter: 800 loss: 8.89842724e-08
Iter: 801 loss: 8.72719141e-08
Iter: 802 loss: 8.71923476e-08
Iter: 803 loss: 8.71850574e-08
Iter: 804 loss: 8.71394832e-08
Iter: 805 loss: 8.70254908e-08
Iter: 806 loss: 8.79760336e-08
Iter: 807 loss: 8.69928272e-08
Iter: 808 loss: 8.68732e-08
Iter: 809 loss: 8.83559323e-08
Iter: 810 loss: 8.68775e-08
Iter: 811 loss: 8.67747474e-08
Iter: 812 loss: 8.75299691e-08
Iter: 813 loss: 8.67671e-08
Iter: 814 loss: 8.6709619e-08
Iter: 815 loss: 8.66487255e-08
Iter: 816 loss: 8.66269545e-08
Iter: 817 loss: 8.65101413e-08
Iter: 818 loss: 8.72317187e-08
Iter: 819 loss: 8.64961791e-08
Iter: 820 loss: 8.63841265e-08
Iter: 821 loss: 8.66349836e-08
Iter: 822 loss: 8.63382184e-08
Iter: 823 loss: 8.62694094e-08
Iter: 824 loss: 8.63463043e-08
Iter: 825 loss: 8.62360281e-08
Iter: 826 loss: 8.61579394e-08
Iter: 827 loss: 8.7089262e-08
Iter: 828 loss: 8.61596519e-08
Iter: 829 loss: 8.61172182e-08
Iter: 830 loss: 8.60104308e-08
Iter: 831 loss: 8.71019523e-08
Iter: 832 loss: 8.59993889e-08
Iter: 833 loss: 8.5897085e-08
Iter: 834 loss: 8.61879741e-08
Iter: 835 loss: 8.58579483e-08
Iter: 836 loss: 8.58220943e-08
Iter: 837 loss: 8.57977369e-08
Iter: 838 loss: 8.57445e-08
Iter: 839 loss: 8.56210534e-08
Iter: 840 loss: 8.69829364e-08
Iter: 841 loss: 8.56103384e-08
Iter: 842 loss: 8.54846078e-08
Iter: 843 loss: 8.59579217e-08
Iter: 844 loss: 8.54632916e-08
Iter: 845 loss: 8.54024762e-08
Iter: 846 loss: 8.53910223e-08
Iter: 847 loss: 8.53545e-08
Iter: 848 loss: 8.52951416e-08
Iter: 849 loss: 8.5294019e-08
Iter: 850 loss: 8.52236042e-08
Iter: 851 loss: 8.56314699e-08
Iter: 852 loss: 8.52116813e-08
Iter: 853 loss: 8.51295567e-08
Iter: 854 loss: 8.53773e-08
Iter: 855 loss: 8.51101873e-08
Iter: 856 loss: 8.50529744e-08
Iter: 857 loss: 8.50171205e-08
Iter: 858 loss: 8.49914272e-08
Iter: 859 loss: 8.4960476e-08
Iter: 860 loss: 8.4949157e-08
Iter: 861 loss: 8.49038173e-08
Iter: 862 loss: 8.47811776e-08
Iter: 863 loss: 8.54687841e-08
Iter: 864 loss: 8.47409467e-08
Iter: 865 loss: 8.46261585e-08
Iter: 866 loss: 8.53213038e-08
Iter: 867 loss: 8.46135038e-08
Iter: 868 loss: 8.45695709e-08
Iter: 869 loss: 8.45704e-08
Iter: 870 loss: 8.45196553e-08
Iter: 871 loss: 8.44813144e-08
Iter: 872 loss: 8.44659098e-08
Iter: 873 loss: 8.44089953e-08
Iter: 874 loss: 8.43654959e-08
Iter: 875 loss: 8.43528909e-08
Iter: 876 loss: 8.43515267e-08
Iter: 877 loss: 8.43120418e-08
Iter: 878 loss: 8.42879047e-08
Iter: 879 loss: 8.42176036e-08
Iter: 880 loss: 8.52753246e-08
Iter: 881 loss: 8.42123669e-08
Iter: 882 loss: 8.41252756e-08
Iter: 883 loss: 8.45912282e-08
Iter: 884 loss: 8.41100274e-08
Iter: 885 loss: 8.403876e-08
Iter: 886 loss: 8.44631e-08
Iter: 887 loss: 8.40230712e-08
Iter: 888 loss: 8.39729353e-08
Iter: 889 loss: 8.40607655e-08
Iter: 890 loss: 8.39458494e-08
Iter: 891 loss: 8.39094412e-08
Iter: 892 loss: 8.40369339e-08
Iter: 893 loss: 8.39014334e-08
Iter: 894 loss: 8.38562784e-08
Iter: 895 loss: 8.39201348e-08
Iter: 896 loss: 8.38279348e-08
Iter: 897 loss: 8.38012184e-08
Iter: 898 loss: 8.37652152e-08
Iter: 899 loss: 8.37660963e-08
Iter: 900 loss: 8.37068654e-08
Iter: 901 loss: 8.38347631e-08
Iter: 902 loss: 8.36894e-08
Iter: 903 loss: 8.36094642e-08
Iter: 904 loss: 8.42860572e-08
Iter: 905 loss: 8.36036591e-08
Iter: 906 loss: 8.35621563e-08
Iter: 907 loss: 8.34545091e-08
Iter: 908 loss: 8.46848422e-08
Iter: 909 loss: 8.34424156e-08
Iter: 910 loss: 8.34285245e-08
Iter: 911 loss: 8.33966709e-08
Iter: 912 loss: 8.33583442e-08
Iter: 913 loss: 8.33386764e-08
Iter: 914 loss: 8.33192786e-08
Iter: 915 loss: 8.32664e-08
Iter: 916 loss: 8.33188381e-08
Iter: 917 loss: 8.32386178e-08
Iter: 918 loss: 8.31900877e-08
Iter: 919 loss: 8.39401e-08
Iter: 920 loss: 8.31935765e-08
Iter: 921 loss: 8.315061e-08
Iter: 922 loss: 8.31780369e-08
Iter: 923 loss: 8.31297626e-08
Iter: 924 loss: 8.30918339e-08
Iter: 925 loss: 8.30495139e-08
Iter: 926 loss: 8.30401632e-08
Iter: 927 loss: 8.2967631e-08
Iter: 928 loss: 8.29675599e-08
Iter: 929 loss: 8.29288e-08
Iter: 930 loss: 8.28583495e-08
Iter: 931 loss: 8.45494839e-08
Iter: 932 loss: 8.285744e-08
Iter: 933 loss: 8.27987421e-08
Iter: 934 loss: 8.29571576e-08
Iter: 935 loss: 8.2772118e-08
Iter: 936 loss: 8.27395894e-08
Iter: 937 loss: 8.27338198e-08
Iter: 938 loss: 8.2706336e-08
Iter: 939 loss: 8.26487891e-08
Iter: 940 loss: 8.37301144e-08
Iter: 941 loss: 8.26489952e-08
Iter: 942 loss: 8.26022202e-08
Iter: 943 loss: 8.29314786e-08
Iter: 944 loss: 8.25994277e-08
Iter: 945 loss: 8.25443323e-08
Iter: 946 loss: 8.26452e-08
Iter: 947 loss: 8.2529418e-08
Iter: 948 loss: 8.24882278e-08
Iter: 949 loss: 8.23688708e-08
Iter: 950 loss: 8.34749585e-08
Iter: 951 loss: 8.23574879e-08
Iter: 952 loss: 8.22557382e-08
Iter: 953 loss: 8.22498833e-08
Iter: 954 loss: 8.21769e-08
Iter: 955 loss: 8.27419839e-08
Iter: 956 loss: 8.21608737e-08
Iter: 957 loss: 8.21176656e-08
Iter: 958 loss: 8.2118838e-08
Iter: 959 loss: 8.20700876e-08
Iter: 960 loss: 8.20323223e-08
Iter: 961 loss: 8.26091551e-08
Iter: 962 loss: 8.20337e-08
Iter: 963 loss: 8.19962906e-08
Iter: 964 loss: 8.20471939e-08
Iter: 965 loss: 8.19741359e-08
Iter: 966 loss: 8.19438668e-08
Iter: 967 loss: 8.18888068e-08
Iter: 968 loss: 8.18952302e-08
Iter: 969 loss: 8.18531447e-08
Iter: 970 loss: 8.18480856e-08
Iter: 971 loss: 8.18114287e-08
Iter: 972 loss: 8.17510255e-08
Iter: 973 loss: 8.1748496e-08
Iter: 974 loss: 8.1679886e-08
Iter: 975 loss: 8.17670269e-08
Iter: 976 loss: 8.16465047e-08
Iter: 977 loss: 8.16067711e-08
Iter: 978 loss: 8.16001773e-08
Iter: 979 loss: 8.15719403e-08
Iter: 980 loss: 8.1517527e-08
Iter: 981 loss: 8.23634849e-08
Iter: 982 loss: 8.15175838e-08
Iter: 983 loss: 8.14477801e-08
Iter: 984 loss: 8.14919616e-08
Iter: 985 loss: 8.14032504e-08
Iter: 986 loss: 8.13719438e-08
Iter: 987 loss: 8.13760437e-08
Iter: 988 loss: 8.1322824e-08
Iter: 989 loss: 8.13386691e-08
Iter: 990 loss: 8.1298225e-08
Iter: 991 loss: 8.12592e-08
Iter: 992 loss: 8.14135177e-08
Iter: 993 loss: 8.12448206e-08
Iter: 994 loss: 8.11986638e-08
Iter: 995 loss: 8.1386716e-08
Iter: 996 loss: 8.11837e-08
Iter: 997 loss: 8.11422964e-08
Iter: 998 loss: 8.11667036e-08
Iter: 999 loss: 8.11125673e-08
Iter: 1000 loss: 8.10758394e-08
Iter: 1001 loss: 8.10554184e-08
Iter: 1002 loss: 8.1038948e-08
Iter: 1003 loss: 8.10044156e-08
Iter: 1004 loss: 8.10072365e-08
Iter: 1005 loss: 8.09661458e-08
Iter: 1006 loss: 8.08758358e-08
Iter: 1007 loss: 8.18679808e-08
Iter: 1008 loss: 8.08654477e-08
Iter: 1009 loss: 8.07893485e-08
Iter: 1010 loss: 8.17213177e-08
Iter: 1011 loss: 8.07861369e-08
Iter: 1012 loss: 8.07159068e-08
Iter: 1013 loss: 8.09773e-08
Iter: 1014 loss: 8.06986264e-08
Iter: 1015 loss: 8.06365961e-08
Iter: 1016 loss: 8.05573777e-08
Iter: 1017 loss: 8.05567169e-08
Iter: 1018 loss: 8.04781308e-08
Iter: 1019 loss: 8.0776e-08
Iter: 1020 loss: 8.04649929e-08
Iter: 1021 loss: 8.04125762e-08
Iter: 1022 loss: 8.07043108e-08
Iter: 1023 loss: 8.04069842e-08
Iter: 1024 loss: 8.03409677e-08
Iter: 1025 loss: 8.06192e-08
Iter: 1026 loss: 8.03235451e-08
Iter: 1027 loss: 8.02786815e-08
Iter: 1028 loss: 8.01876752e-08
Iter: 1029 loss: 8.19617796e-08
Iter: 1030 loss: 8.01816284e-08
Iter: 1031 loss: 8.0073761e-08
Iter: 1032 loss: 8.01325371e-08
Iter: 1033 loss: 8.00050941e-08
Iter: 1034 loss: 7.99123612e-08
Iter: 1035 loss: 7.99051847e-08
Iter: 1036 loss: 7.98490447e-08
Iter: 1037 loss: 8.04193121e-08
Iter: 1038 loss: 7.98514677e-08
Iter: 1039 loss: 7.98149458e-08
Iter: 1040 loss: 7.9740218e-08
Iter: 1041 loss: 8.10420957e-08
Iter: 1042 loss: 7.9733077e-08
Iter: 1043 loss: 7.96612269e-08
Iter: 1044 loss: 7.96851083e-08
Iter: 1045 loss: 7.96070267e-08
Iter: 1046 loss: 7.95866555e-08
Iter: 1047 loss: 7.95594843e-08
Iter: 1048 loss: 7.95282205e-08
Iter: 1049 loss: 7.94476449e-08
Iter: 1050 loss: 8.02402482e-08
Iter: 1051 loss: 7.94366883e-08
Iter: 1052 loss: 7.93943116e-08
Iter: 1053 loss: 7.93917678e-08
Iter: 1054 loss: 7.93412696e-08
Iter: 1055 loss: 7.93439412e-08
Iter: 1056 loss: 7.93064743e-08
Iter: 1057 loss: 7.92607864e-08
Iter: 1058 loss: 7.92221329e-08
Iter: 1059 loss: 7.92150274e-08
Iter: 1060 loss: 7.9142751e-08
Iter: 1061 loss: 7.92172088e-08
Iter: 1062 loss: 7.9107e-08
Iter: 1063 loss: 7.90459325e-08
Iter: 1064 loss: 7.90372781e-08
Iter: 1065 loss: 7.89985606e-08
Iter: 1066 loss: 7.89151144e-08
Iter: 1067 loss: 8.05442113e-08
Iter: 1068 loss: 7.89179921e-08
Iter: 1069 loss: 7.88398324e-08
Iter: 1070 loss: 7.91195447e-08
Iter: 1071 loss: 7.88170098e-08
Iter: 1072 loss: 7.87605288e-08
Iter: 1073 loss: 7.87600172e-08
Iter: 1074 loss: 7.87355248e-08
Iter: 1075 loss: 7.86763223e-08
Iter: 1076 loss: 7.91523149e-08
Iter: 1077 loss: 7.8662751e-08
Iter: 1078 loss: 7.86289434e-08
Iter: 1079 loss: 7.86275436e-08
Iter: 1080 loss: 7.85901335e-08
Iter: 1081 loss: 7.85859555e-08
Iter: 1082 loss: 7.85632892e-08
Iter: 1083 loss: 7.85156544e-08
Iter: 1084 loss: 7.86394e-08
Iter: 1085 loss: 7.85016852e-08
Iter: 1086 loss: 7.84441e-08
Iter: 1087 loss: 7.83519e-08
Iter: 1088 loss: 8.00864797e-08
Iter: 1089 loss: 7.83564e-08
Iter: 1090 loss: 7.82664955e-08
Iter: 1091 loss: 7.8267675e-08
Iter: 1092 loss: 7.8208565e-08
Iter: 1093 loss: 7.82029161e-08
Iter: 1094 loss: 7.81627278e-08
Iter: 1095 loss: 7.81177505e-08
Iter: 1096 loss: 7.8124117e-08
Iter: 1097 loss: 7.80767806e-08
Iter: 1098 loss: 7.80848595e-08
Iter: 1099 loss: 7.80359812e-08
Iter: 1100 loss: 7.7986364e-08
Iter: 1101 loss: 7.80354696e-08
Iter: 1102 loss: 7.79580773e-08
Iter: 1103 loss: 7.79123042e-08
Iter: 1104 loss: 7.79937395e-08
Iter: 1105 loss: 7.78928779e-08
Iter: 1106 loss: 7.78335121e-08
Iter: 1107 loss: 7.77678508e-08
Iter: 1108 loss: 7.77479698e-08
Iter: 1109 loss: 7.76527e-08
Iter: 1110 loss: 7.77121656e-08
Iter: 1111 loss: 7.75942794e-08
Iter: 1112 loss: 7.75656872e-08
Iter: 1113 loss: 7.75316948e-08
Iter: 1114 loss: 7.74930413e-08
Iter: 1115 loss: 7.74608e-08
Iter: 1116 loss: 7.74517872e-08
Iter: 1117 loss: 7.74012534e-08
Iter: 1118 loss: 7.73971465e-08
Iter: 1119 loss: 7.73617614e-08
Iter: 1120 loss: 7.73272717e-08
Iter: 1121 loss: 7.73155477e-08
Iter: 1122 loss: 7.72766455e-08
Iter: 1123 loss: 7.72076874e-08
Iter: 1124 loss: 7.84842484e-08
Iter: 1125 loss: 7.72029125e-08
Iter: 1126 loss: 7.71188e-08
Iter: 1127 loss: 7.78160114e-08
Iter: 1128 loss: 7.71099522e-08
Iter: 1129 loss: 7.70667299e-08
Iter: 1130 loss: 7.77310163e-08
Iter: 1131 loss: 7.70671846e-08
Iter: 1132 loss: 7.70184485e-08
Iter: 1133 loss: 7.69589406e-08
Iter: 1134 loss: 7.69613e-08
Iter: 1135 loss: 7.69381145e-08
Iter: 1136 loss: 7.69323e-08
Iter: 1137 loss: 7.69059199e-08
Iter: 1138 loss: 7.68742936e-08
Iter: 1139 loss: 7.68717712e-08
Iter: 1140 loss: 7.68262254e-08
Iter: 1141 loss: 7.67834649e-08
Iter: 1142 loss: 7.67729915e-08
Iter: 1143 loss: 7.6728611e-08
Iter: 1144 loss: 7.67237651e-08
Iter: 1145 loss: 7.66847208e-08
Iter: 1146 loss: 7.67328459e-08
Iter: 1147 loss: 7.66580825e-08
Iter: 1148 loss: 7.66055308e-08
Iter: 1149 loss: 7.65614203e-08
Iter: 1150 loss: 7.65510038e-08
Iter: 1151 loss: 7.6507007e-08
Iter: 1152 loss: 7.65010952e-08
Iter: 1153 loss: 7.64649286e-08
Iter: 1154 loss: 7.6464076e-08
Iter: 1155 loss: 7.64311707e-08
Iter: 1156 loss: 7.63820083e-08
Iter: 1157 loss: 7.64166401e-08
Iter: 1158 loss: 7.63549224e-08
Iter: 1159 loss: 7.62942e-08
Iter: 1160 loss: 7.66069661e-08
Iter: 1161 loss: 7.62824754e-08
Iter: 1162 loss: 7.62107746e-08
Iter: 1163 loss: 7.62464225e-08
Iter: 1164 loss: 7.61648238e-08
Iter: 1165 loss: 7.60911121e-08
Iter: 1166 loss: 7.61624648e-08
Iter: 1167 loss: 7.60461702e-08
Iter: 1168 loss: 7.59588943e-08
Iter: 1169 loss: 7.69757236e-08
Iter: 1170 loss: 7.59565211e-08
Iter: 1171 loss: 7.59235093e-08
Iter: 1172 loss: 7.58854881e-08
Iter: 1173 loss: 7.58900711e-08
Iter: 1174 loss: 7.58364749e-08
Iter: 1175 loss: 7.62234649e-08
Iter: 1176 loss: 7.58359846e-08
Iter: 1177 loss: 7.57991501e-08
Iter: 1178 loss: 7.58211769e-08
Iter: 1179 loss: 7.57721708e-08
Iter: 1180 loss: 7.57180487e-08
Iter: 1181 loss: 7.5708293e-08
Iter: 1182 loss: 7.56832037e-08
Iter: 1183 loss: 7.56131371e-08
Iter: 1184 loss: 7.61742e-08
Iter: 1185 loss: 7.56070406e-08
Iter: 1186 loss: 7.55263798e-08
Iter: 1187 loss: 7.57338512e-08
Iter: 1188 loss: 7.55034364e-08
Iter: 1189 loss: 7.5453e-08
Iter: 1190 loss: 7.55090355e-08
Iter: 1191 loss: 7.5415258e-08
Iter: 1192 loss: 7.53675522e-08
Iter: 1193 loss: 7.57389671e-08
Iter: 1194 loss: 7.5355473e-08
Iter: 1195 loss: 7.53068363e-08
Iter: 1196 loss: 7.55207e-08
Iter: 1197 loss: 7.52929168e-08
Iter: 1198 loss: 7.52573541e-08
Iter: 1199 loss: 7.52277742e-08
Iter: 1200 loss: 7.52185443e-08
Iter: 1201 loss: 7.51740714e-08
Iter: 1202 loss: 7.51698863e-08
Iter: 1203 loss: 7.51349916e-08
Iter: 1204 loss: 7.50601146e-08
Iter: 1205 loss: 7.63155e-08
Iter: 1206 loss: 7.50608393e-08
Iter: 1207 loss: 7.50155493e-08
Iter: 1208 loss: 7.50173257e-08
Iter: 1209 loss: 7.49757305e-08
Iter: 1210 loss: 7.50328297e-08
Iter: 1211 loss: 7.49577254e-08
Iter: 1212 loss: 7.49202655e-08
Iter: 1213 loss: 7.48824931e-08
Iter: 1214 loss: 7.48733697e-08
Iter: 1215 loss: 7.48235536e-08
Iter: 1216 loss: 7.50736859e-08
Iter: 1217 loss: 7.48142099e-08
Iter: 1218 loss: 7.47562581e-08
Iter: 1219 loss: 7.52594786e-08
Iter: 1220 loss: 7.47554054e-08
Iter: 1221 loss: 7.4723566e-08
Iter: 1222 loss: 7.46936593e-08
Iter: 1223 loss: 7.46823758e-08
Iter: 1224 loss: 7.46290141e-08
Iter: 1225 loss: 7.47409246e-08
Iter: 1226 loss: 7.46047704e-08
Iter: 1227 loss: 7.45392086e-08
Iter: 1228 loss: 7.51872591e-08
Iter: 1229 loss: 7.4535194e-08
Iter: 1230 loss: 7.44916662e-08
Iter: 1231 loss: 7.44591802e-08
Iter: 1232 loss: 7.44433422e-08
Iter: 1233 loss: 7.44236317e-08
Iter: 1234 loss: 7.44108277e-08
Iter: 1235 loss: 7.43944213e-08
Iter: 1236 loss: 7.43368531e-08
Iter: 1237 loss: 7.50575353e-08
Iter: 1238 loss: 7.4331723e-08
Iter: 1239 loss: 7.4267831e-08
Iter: 1240 loss: 7.4609332e-08
Iter: 1241 loss: 7.42699555e-08
Iter: 1242 loss: 7.42144621e-08
Iter: 1243 loss: 7.43743556e-08
Iter: 1244 loss: 7.41915329e-08
Iter: 1245 loss: 7.41492e-08
Iter: 1246 loss: 7.41808037e-08
Iter: 1247 loss: 7.41147304e-08
Iter: 1248 loss: 7.40549808e-08
Iter: 1249 loss: 7.41015782e-08
Iter: 1250 loss: 7.40203205e-08
Iter: 1251 loss: 7.39974126e-08
Iter: 1252 loss: 7.39821786e-08
Iter: 1253 loss: 7.39551922e-08
Iter: 1254 loss: 7.39120409e-08
Iter: 1255 loss: 7.49139275e-08
Iter: 1256 loss: 7.39064347e-08
Iter: 1257 loss: 7.38526538e-08
Iter: 1258 loss: 7.40372386e-08
Iter: 1259 loss: 7.38324957e-08
Iter: 1260 loss: 7.37837809e-08
Iter: 1261 loss: 7.42510764e-08
Iter: 1262 loss: 7.37820329e-08
Iter: 1263 loss: 7.37258432e-08
Iter: 1264 loss: 7.36780947e-08
Iter: 1265 loss: 7.3664e-08
Iter: 1266 loss: 7.36104937e-08
Iter: 1267 loss: 7.42082449e-08
Iter: 1268 loss: 7.3612469e-08
Iter: 1269 loss: 7.35652605e-08
Iter: 1270 loss: 7.36520178e-08
Iter: 1271 loss: 7.35413e-08
Iter: 1272 loss: 7.35067616e-08
Iter: 1273 loss: 7.35097387e-08
Iter: 1274 loss: 7.34820418e-08
Iter: 1275 loss: 7.34397787e-08
Iter: 1276 loss: 7.39290869e-08
Iter: 1277 loss: 7.34400203e-08
Iter: 1278 loss: 7.34154355e-08
Iter: 1279 loss: 7.33765e-08
Iter: 1280 loss: 7.3375638e-08
Iter: 1281 loss: 7.33164e-08
Iter: 1282 loss: 7.33151921e-08
Iter: 1283 loss: 7.32667544e-08
Iter: 1284 loss: 7.32609919e-08
Iter: 1285 loss: 7.32368619e-08
Iter: 1286 loss: 7.32104724e-08
Iter: 1287 loss: 7.31779792e-08
Iter: 1288 loss: 7.31670866e-08
Iter: 1289 loss: 7.31199137e-08
Iter: 1290 loss: 7.3296647e-08
Iter: 1291 loss: 7.31024343e-08
Iter: 1292 loss: 7.30509129e-08
Iter: 1293 loss: 7.32179473e-08
Iter: 1294 loss: 7.30386631e-08
Iter: 1295 loss: 7.29937568e-08
Iter: 1296 loss: 7.31602583e-08
Iter: 1297 loss: 7.29828e-08
Iter: 1298 loss: 7.29423562e-08
Iter: 1299 loss: 7.29323233e-08
Iter: 1300 loss: 7.29085201e-08
Iter: 1301 loss: 7.28280725e-08
Iter: 1302 loss: 7.31528118e-08
Iter: 1303 loss: 7.28135e-08
Iter: 1304 loss: 7.27617646e-08
Iter: 1305 loss: 7.27198852e-08
Iter: 1306 loss: 7.27012619e-08
Iter: 1307 loss: 7.26450793e-08
Iter: 1308 loss: 7.26451361e-08
Iter: 1309 loss: 7.26027451e-08
Iter: 1310 loss: 7.2573684e-08
Iter: 1311 loss: 7.25541582e-08
Iter: 1312 loss: 7.25033829e-08
Iter: 1313 loss: 7.24845037e-08
Iter: 1314 loss: 7.24517761e-08
Iter: 1315 loss: 7.23730054e-08
Iter: 1316 loss: 7.24837719e-08
Iter: 1317 loss: 7.23305575e-08
Iter: 1318 loss: 7.22606188e-08
Iter: 1319 loss: 7.22498186e-08
Iter: 1320 loss: 7.22129414e-08
Iter: 1321 loss: 7.21331048e-08
Iter: 1322 loss: 7.36244132e-08
Iter: 1323 loss: 7.21335e-08
Iter: 1324 loss: 7.20513071e-08
Iter: 1325 loss: 7.31164249e-08
Iter: 1326 loss: 7.2048266e-08
Iter: 1327 loss: 7.20035e-08
Iter: 1328 loss: 7.24828269e-08
Iter: 1329 loss: 7.19989188e-08
Iter: 1330 loss: 7.19717264e-08
Iter: 1331 loss: 7.19087e-08
Iter: 1332 loss: 7.19084312e-08
Iter: 1333 loss: 7.18655357e-08
Iter: 1334 loss: 7.18619191e-08
Iter: 1335 loss: 7.1831991e-08
Iter: 1336 loss: 7.17776061e-08
Iter: 1337 loss: 7.26335e-08
Iter: 1338 loss: 7.17744228e-08
Iter: 1339 loss: 7.17118311e-08
Iter: 1340 loss: 7.25678433e-08
Iter: 1341 loss: 7.17049318e-08
Iter: 1342 loss: 7.16487563e-08
Iter: 1343 loss: 7.17888753e-08
Iter: 1344 loss: 7.16266442e-08
Iter: 1345 loss: 7.15847364e-08
Iter: 1346 loss: 7.16269e-08
Iter: 1347 loss: 7.1565367e-08
Iter: 1348 loss: 7.15073867e-08
Iter: 1349 loss: 7.14850756e-08
Iter: 1350 loss: 7.14528809e-08
Iter: 1351 loss: 7.14254753e-08
Iter: 1352 loss: 7.1416558e-08
Iter: 1353 loss: 7.13748491e-08
Iter: 1354 loss: 7.13878876e-08
Iter: 1355 loss: 7.13444592e-08
Iter: 1356 loss: 7.13050099e-08
Iter: 1357 loss: 7.12575812e-08
Iter: 1358 loss: 7.12569346e-08
Iter: 1359 loss: 7.12228143e-08
Iter: 1360 loss: 7.1209584e-08
Iter: 1361 loss: 7.11904633e-08
Iter: 1362 loss: 7.11820576e-08
Iter: 1363 loss: 7.11671788e-08
Iter: 1364 loss: 7.11223e-08
Iter: 1365 loss: 7.13699961e-08
Iter: 1366 loss: 7.11218036e-08
Iter: 1367 loss: 7.10958616e-08
Iter: 1368 loss: 7.12407342e-08
Iter: 1369 loss: 7.10923587e-08
Iter: 1370 loss: 7.10638162e-08
Iter: 1371 loss: 7.10000521e-08
Iter: 1372 loss: 7.1254604e-08
Iter: 1373 loss: 7.0971609e-08
Iter: 1374 loss: 7.09602617e-08
Iter: 1375 loss: 7.09300068e-08
Iter: 1376 loss: 7.0892952e-08
Iter: 1377 loss: 7.08206613e-08
Iter: 1378 loss: 7.25974516e-08
Iter: 1379 loss: 7.08223311e-08
Iter: 1380 loss: 7.07547514e-08
Iter: 1381 loss: 7.08308789e-08
Iter: 1382 loss: 7.0719075e-08
Iter: 1383 loss: 7.0653e-08
Iter: 1384 loss: 7.10218586e-08
Iter: 1385 loss: 7.06482695e-08
Iter: 1386 loss: 7.0616764e-08
Iter: 1387 loss: 7.06121313e-08
Iter: 1388 loss: 7.05871841e-08
Iter: 1389 loss: 7.05260561e-08
Iter: 1390 loss: 7.13558e-08
Iter: 1391 loss: 7.05235124e-08
Iter: 1392 loss: 7.04592793e-08
Iter: 1393 loss: 7.06711774e-08
Iter: 1394 loss: 7.04418852e-08
Iter: 1395 loss: 7.03615e-08
Iter: 1396 loss: 7.09275341e-08
Iter: 1397 loss: 7.03485554e-08
Iter: 1398 loss: 7.03227556e-08
Iter: 1399 loss: 7.05022956e-08
Iter: 1400 loss: 7.03200129e-08
Iter: 1401 loss: 7.02866387e-08
Iter: 1402 loss: 7.03272889e-08
Iter: 1403 loss: 7.02681504e-08
Iter: 1404 loss: 7.02531651e-08
Iter: 1405 loss: 7.03165313e-08
Iter: 1406 loss: 7.02440204e-08
Iter: 1407 loss: 7.02185616e-08
Iter: 1408 loss: 7.01859548e-08
Iter: 1409 loss: 7.01870277e-08
Iter: 1410 loss: 7.01627414e-08
Iter: 1411 loss: 7.01590608e-08
Iter: 1412 loss: 7.01416525e-08
Iter: 1413 loss: 7.00896123e-08
Iter: 1414 loss: 7.04573182e-08
Iter: 1415 loss: 7.00789258e-08
Iter: 1416 loss: 7.00054557e-08
Iter: 1417 loss: 7.00094347e-08
Iter: 1418 loss: 6.99492801e-08
Iter: 1419 loss: 6.99790306e-08
Iter: 1420 loss: 6.99126161e-08
Iter: 1421 loss: 6.98856439e-08
Iter: 1422 loss: 6.98660045e-08
Iter: 1423 loss: 6.98537121e-08
Iter: 1424 loss: 6.98215e-08
Iter: 1425 loss: 6.98318914e-08
Iter: 1426 loss: 6.97994e-08
Iter: 1427 loss: 6.97720495e-08
Iter: 1428 loss: 6.97702518e-08
Iter: 1429 loss: 6.97512377e-08
Iter: 1430 loss: 6.97146e-08
Iter: 1431 loss: 6.9717494e-08
Iter: 1432 loss: 6.968272e-08
Iter: 1433 loss: 7.02446954e-08
Iter: 1434 loss: 6.96798281e-08
Iter: 1435 loss: 6.96576592e-08
Iter: 1436 loss: 6.96224589e-08
Iter: 1437 loss: 6.96209881e-08
Iter: 1438 loss: 6.95715556e-08
Iter: 1439 loss: 6.9820743e-08
Iter: 1440 loss: 6.95653597e-08
Iter: 1441 loss: 6.95240843e-08
Iter: 1442 loss: 6.96819313e-08
Iter: 1443 loss: 6.95231961e-08
Iter: 1444 loss: 6.94941065e-08
Iter: 1445 loss: 6.9588495e-08
Iter: 1446 loss: 6.94825886e-08
Iter: 1447 loss: 6.94653437e-08
Iter: 1448 loss: 6.94225548e-08
Iter: 1449 loss: 6.99782063e-08
Iter: 1450 loss: 6.94209632e-08
Iter: 1451 loss: 6.93930815e-08
Iter: 1452 loss: 6.93903459e-08
Iter: 1453 loss: 6.93622795e-08
Iter: 1454 loss: 6.93569575e-08
Iter: 1455 loss: 6.93448357e-08
Iter: 1456 loss: 6.93075819e-08
Iter: 1457 loss: 6.92825068e-08
Iter: 1458 loss: 6.92762399e-08
Iter: 1459 loss: 6.92350639e-08
Iter: 1460 loss: 6.96350639e-08
Iter: 1461 loss: 6.92369184e-08
Iter: 1462 loss: 6.91976538e-08
Iter: 1463 loss: 6.92290953e-08
Iter: 1464 loss: 6.91643294e-08
Iter: 1465 loss: 6.91478519e-08
Iter: 1466 loss: 6.91512909e-08
Iter: 1467 loss: 6.91268696e-08
Iter: 1468 loss: 6.90986326e-08
Iter: 1469 loss: 6.9097716e-08
Iter: 1470 loss: 6.90662745e-08
Iter: 1471 loss: 6.9151227e-08
Iter: 1472 loss: 6.90568527e-08
Iter: 1473 loss: 6.90217874e-08
Iter: 1474 loss: 6.91276085e-08
Iter: 1475 loss: 6.90093884e-08
Iter: 1476 loss: 6.89715876e-08
Iter: 1477 loss: 6.90995421e-08
Iter: 1478 loss: 6.89639705e-08
Iter: 1479 loss: 6.89261697e-08
Iter: 1480 loss: 6.88888235e-08
Iter: 1481 loss: 6.88884754e-08
Iter: 1482 loss: 6.88311e-08
Iter: 1483 loss: 6.88570552e-08
Iter: 1484 loss: 6.88038924e-08
Iter: 1485 loss: 6.87694381e-08
Iter: 1486 loss: 6.87556962e-08
Iter: 1487 loss: 6.8736334e-08
Iter: 1488 loss: 6.87166448e-08
Iter: 1489 loss: 6.87199915e-08
Iter: 1490 loss: 6.86875e-08
Iter: 1491 loss: 6.88510227e-08
Iter: 1492 loss: 6.8681004e-08
Iter: 1493 loss: 6.86561776e-08
Iter: 1494 loss: 6.87030237e-08
Iter: 1495 loss: 6.86458463e-08
Iter: 1496 loss: 6.86077257e-08
Iter: 1497 loss: 6.86662318e-08
Iter: 1498 loss: 6.86027803e-08
Iter: 1499 loss: 6.85595083e-08
Iter: 1500 loss: 6.87761315e-08
Iter: 1501 loss: 6.85484594e-08
Iter: 1502 loss: 6.8528081e-08
Iter: 1503 loss: 6.85110848e-08
Iter: 1504 loss: 6.85072e-08
Iter: 1505 loss: 6.84589239e-08
Iter: 1506 loss: 6.8813037e-08
Iter: 1507 loss: 6.84539359e-08
Iter: 1508 loss: 6.84277381e-08
Iter: 1509 loss: 6.85079087e-08
Iter: 1510 loss: 6.8422537e-08
Iter: 1511 loss: 6.8388367e-08
Iter: 1512 loss: 6.84629242e-08
Iter: 1513 loss: 6.83801389e-08
Iter: 1514 loss: 6.83513e-08
Iter: 1515 loss: 6.83111594e-08
Iter: 1516 loss: 6.83139874e-08
Iter: 1517 loss: 6.82926498e-08
Iter: 1518 loss: 6.82840948e-08
Iter: 1519 loss: 6.82531578e-08
Iter: 1520 loss: 6.82287e-08
Iter: 1521 loss: 6.82220787e-08
Iter: 1522 loss: 6.81869281e-08
Iter: 1523 loss: 6.84214641e-08
Iter: 1524 loss: 6.81785792e-08
Iter: 1525 loss: 6.81515075e-08
Iter: 1526 loss: 6.82441765e-08
Iter: 1527 loss: 6.81417447e-08
Iter: 1528 loss: 6.81210679e-08
Iter: 1529 loss: 6.81580161e-08
Iter: 1530 loss: 6.81141188e-08
Iter: 1531 loss: 6.80935202e-08
Iter: 1532 loss: 6.81835957e-08
Iter: 1533 loss: 6.80889656e-08
Iter: 1534 loss: 6.80705625e-08
Iter: 1535 loss: 6.80569272e-08
Iter: 1536 loss: 6.80481165e-08
Iter: 1537 loss: 6.80254644e-08
Iter: 1538 loss: 6.83102925e-08
Iter: 1539 loss: 6.80215848e-08
Iter: 1540 loss: 6.79935823e-08
Iter: 1541 loss: 6.80702428e-08
Iter: 1542 loss: 6.79902854e-08
Iter: 1543 loss: 6.79719676e-08
Iter: 1544 loss: 6.80037857e-08
Iter: 1545 loss: 6.79599879e-08
Iter: 1546 loss: 6.79320351e-08
Iter: 1547 loss: 6.78978793e-08
Iter: 1548 loss: 6.78870222e-08
Iter: 1549 loss: 6.78690313e-08
Iter: 1550 loss: 6.78655283e-08
Iter: 1551 loss: 6.78454768e-08
Iter: 1552 loss: 6.79484202e-08
Iter: 1553 loss: 6.78367584e-08
Iter: 1554 loss: 6.78187178e-08
Iter: 1555 loss: 6.78375329e-08
Iter: 1556 loss: 6.78186325e-08
Iter: 1557 loss: 6.77955612e-08
Iter: 1558 loss: 6.7819343e-08
Iter: 1559 loss: 6.77861962e-08
Iter: 1560 loss: 6.77635654e-08
Iter: 1561 loss: 6.78347334e-08
Iter: 1562 loss: 6.7760368e-08
Iter: 1563 loss: 6.77462211e-08
Iter: 1564 loss: 6.78121381e-08
Iter: 1565 loss: 6.77415528e-08
Iter: 1566 loss: 6.77194905e-08
Iter: 1567 loss: 6.7687516e-08
Iter: 1568 loss: 6.768704e-08
Iter: 1569 loss: 6.76483651e-08
Iter: 1570 loss: 6.78091041e-08
Iter: 1571 loss: 6.76401726e-08
Iter: 1572 loss: 6.75860576e-08
Iter: 1573 loss: 6.77991707e-08
Iter: 1574 loss: 6.75731187e-08
Iter: 1575 loss: 6.7531353e-08
Iter: 1576 loss: 6.76323708e-08
Iter: 1577 loss: 6.7515245e-08
Iter: 1578 loss: 6.74727829e-08
Iter: 1579 loss: 6.74909444e-08
Iter: 1580 loss: 6.74489442e-08
Iter: 1581 loss: 6.7409438e-08
Iter: 1582 loss: 6.74613787e-08
Iter: 1583 loss: 6.73925911e-08
Iter: 1584 loss: 6.73629614e-08
Iter: 1585 loss: 6.73642e-08
Iter: 1586 loss: 6.73385898e-08
Iter: 1587 loss: 6.73264182e-08
Iter: 1588 loss: 6.73250184e-08
Iter: 1589 loss: 6.72940814e-08
Iter: 1590 loss: 6.7456071e-08
Iter: 1591 loss: 6.72899461e-08
Iter: 1592 loss: 6.72712162e-08
Iter: 1593 loss: 6.72789469e-08
Iter: 1594 loss: 6.72599469e-08
Iter: 1595 loss: 6.72293865e-08
Iter: 1596 loss: 6.73850451e-08
Iter: 1597 loss: 6.72230485e-08
Iter: 1598 loss: 6.72014835e-08
Iter: 1599 loss: 6.71774387e-08
Iter: 1600 loss: 6.71779432e-08
Iter: 1601 loss: 6.71434606e-08
Iter: 1602 loss: 6.72893918e-08
Iter: 1603 loss: 6.71346712e-08
Iter: 1604 loss: 6.71155505e-08
Iter: 1605 loss: 6.72252156e-08
Iter: 1606 loss: 6.71050202e-08
Iter: 1607 loss: 6.70769538e-08
Iter: 1608 loss: 6.71229756e-08
Iter: 1609 loss: 6.70633113e-08
Iter: 1610 loss: 6.7032687e-08
Iter: 1611 loss: 6.70261429e-08
Iter: 1612 loss: 6.7004855e-08
Iter: 1613 loss: 6.69652138e-08
Iter: 1614 loss: 6.69809e-08
Iter: 1615 loss: 6.69385827e-08
Iter: 1616 loss: 6.69131737e-08
Iter: 1617 loss: 6.69059261e-08
Iter: 1618 loss: 6.68783e-08
Iter: 1619 loss: 6.68288749e-08
Iter: 1620 loss: 6.6832385e-08
Iter: 1621 loss: 6.67900935e-08
Iter: 1622 loss: 6.6790605e-08
Iter: 1623 loss: 6.67678606e-08
Iter: 1624 loss: 6.67781563e-08
Iter: 1625 loss: 6.67550637e-08
Iter: 1626 loss: 6.67269759e-08
Iter: 1627 loss: 6.69024089e-08
Iter: 1628 loss: 6.67241125e-08
Iter: 1629 loss: 6.67062068e-08
Iter: 1630 loss: 6.66780835e-08
Iter: 1631 loss: 6.66777922e-08
Iter: 1632 loss: 6.66466491e-08
Iter: 1633 loss: 6.67128504e-08
Iter: 1634 loss: 6.66336106e-08
Iter: 1635 loss: 6.65999238e-08
Iter: 1636 loss: 6.67243683e-08
Iter: 1637 loss: 6.6597174e-08
Iter: 1638 loss: 6.65706068e-08
Iter: 1639 loss: 6.65910136e-08
Iter: 1640 loss: 6.65537954e-08
Iter: 1641 loss: 6.6518183e-08
Iter: 1642 loss: 6.66974245e-08
Iter: 1643 loss: 6.65075532e-08
Iter: 1644 loss: 6.64765309e-08
Iter: 1645 loss: 6.64708821e-08
Iter: 1646 loss: 6.64468e-08
Iter: 1647 loss: 6.64226292e-08
Iter: 1648 loss: 6.64206254e-08
Iter: 1649 loss: 6.63912729e-08
Iter: 1650 loss: 6.63945627e-08
Iter: 1651 loss: 6.63725785e-08
Iter: 1652 loss: 6.63440431e-08
Iter: 1653 loss: 6.64464608e-08
Iter: 1654 loss: 6.63397444e-08
Iter: 1655 loss: 6.6294767e-08
Iter: 1656 loss: 6.63118556e-08
Iter: 1657 loss: 6.62727e-08
Iter: 1658 loss: 6.62390818e-08
Iter: 1659 loss: 6.66268889e-08
Iter: 1660 loss: 6.62394655e-08
Iter: 1661 loss: 6.6213417e-08
Iter: 1662 loss: 6.61748558e-08
Iter: 1663 loss: 6.61778472e-08
Iter: 1664 loss: 6.61404869e-08
Iter: 1665 loss: 6.63781776e-08
Iter: 1666 loss: 6.61331683e-08
Iter: 1667 loss: 6.60983943e-08
Iter: 1668 loss: 6.62938504e-08
Iter: 1669 loss: 6.60924115e-08
Iter: 1670 loss: 6.60622419e-08
Iter: 1671 loss: 6.6046681e-08
Iter: 1672 loss: 6.60307506e-08
Iter: 1673 loss: 6.59949677e-08
Iter: 1674 loss: 6.63640307e-08
Iter: 1675 loss: 6.59957777e-08
Iter: 1676 loss: 6.59684787e-08
Iter: 1677 loss: 6.59244179e-08
Iter: 1678 loss: 6.59229187e-08
Iter: 1679 loss: 6.58741612e-08
Iter: 1680 loss: 6.61623574e-08
Iter: 1681 loss: 6.58652084e-08
Iter: 1682 loss: 6.5836538e-08
Iter: 1683 loss: 6.58328716e-08
Iter: 1684 loss: 6.58165504e-08
Iter: 1685 loss: 6.58050325e-08
Iter: 1686 loss: 6.58034907e-08
Iter: 1687 loss: 6.57698607e-08
Iter: 1688 loss: 6.60080275e-08
Iter: 1689 loss: 6.57689654e-08
Iter: 1690 loss: 6.57531416e-08
Iter: 1691 loss: 6.58499602e-08
Iter: 1692 loss: 6.57472228e-08
Iter: 1693 loss: 6.57387886e-08
Iter: 1694 loss: 6.57182e-08
Iter: 1695 loss: 6.57126833e-08
Iter: 1696 loss: 6.56902515e-08
Iter: 1697 loss: 6.56936905e-08
Iter: 1698 loss: 6.56727437e-08
Iter: 1699 loss: 6.56454446e-08
Iter: 1700 loss: 6.5955291e-08
Iter: 1701 loss: 6.56477113e-08
Iter: 1702 loss: 6.56229844e-08
Iter: 1703 loss: 6.56516193e-08
Iter: 1704 loss: 6.56150903e-08
Iter: 1705 loss: 6.55930847e-08
Iter: 1706 loss: 6.56064429e-08
Iter: 1707 loss: 6.5579755e-08
Iter: 1708 loss: 6.55524488e-08
Iter: 1709 loss: 6.56480807e-08
Iter: 1710 loss: 6.55410943e-08
Iter: 1711 loss: 6.55169572e-08
Iter: 1712 loss: 6.54774226e-08
Iter: 1713 loss: 6.5478531e-08
Iter: 1714 loss: 6.54822401e-08
Iter: 1715 loss: 6.54610801e-08
Iter: 1716 loss: 6.54504078e-08
Iter: 1717 loss: 6.54735075e-08
Iter: 1718 loss: 6.5436538e-08
Iter: 1719 loss: 6.54265193e-08
Iter: 1720 loss: 6.53995755e-08
Iter: 1721 loss: 6.53998e-08
Iter: 1722 loss: 6.53739036e-08
Iter: 1723 loss: 6.5373662e-08
Iter: 1724 loss: 6.53619807e-08
Iter: 1725 loss: 6.53570567e-08
Iter: 1726 loss: 6.5350676e-08
Iter: 1727 loss: 6.53377299e-08
Iter: 1728 loss: 6.55343655e-08
Iter: 1729 loss: 6.53360814e-08
Iter: 1730 loss: 6.53243646e-08
Iter: 1731 loss: 6.53050805e-08
Iter: 1732 loss: 6.53041425e-08
Iter: 1733 loss: 6.52803394e-08
Iter: 1734 loss: 6.52912462e-08
Iter: 1735 loss: 6.52659722e-08
Iter: 1736 loss: 6.52509158e-08
Iter: 1737 loss: 6.52841763e-08
Iter: 1738 loss: 6.5240215e-08
Iter: 1739 loss: 6.52105641e-08
Iter: 1740 loss: 6.52466596e-08
Iter: 1741 loss: 6.51964e-08
Iter: 1742 loss: 6.51637606e-08
Iter: 1743 loss: 6.51286598e-08
Iter: 1744 loss: 6.51304646e-08
Iter: 1745 loss: 6.50923582e-08
Iter: 1746 loss: 6.53579662e-08
Iter: 1747 loss: 6.50913776e-08
Iter: 1748 loss: 6.50566108e-08
Iter: 1749 loss: 6.52698233e-08
Iter: 1750 loss: 6.50538112e-08
Iter: 1751 loss: 6.50276277e-08
Iter: 1752 loss: 6.53774777e-08
Iter: 1753 loss: 6.50276704e-08
Iter: 1754 loss: 6.50132108e-08
Iter: 1755 loss: 6.49977352e-08
Iter: 1756 loss: 6.49976073e-08
Iter: 1757 loss: 6.49847607e-08
Iter: 1758 loss: 6.49828422e-08
Iter: 1759 loss: 6.49711396e-08
Iter: 1760 loss: 6.49760779e-08
Iter: 1761 loss: 6.49651568e-08
Iter: 1762 loss: 6.49473932e-08
Iter: 1763 loss: 6.49332819e-08
Iter: 1764 loss: 6.49289e-08
Iter: 1765 loss: 6.4899254e-08
Iter: 1766 loss: 6.49074394e-08
Iter: 1767 loss: 6.48887095e-08
Iter: 1768 loss: 6.48712941e-08
Iter: 1769 loss: 6.48674927e-08
Iter: 1770 loss: 6.48550369e-08
Iter: 1771 loss: 6.48658727e-08
Iter: 1772 loss: 6.48435119e-08
Iter: 1773 loss: 6.48180958e-08
Iter: 1774 loss: 6.4767292e-08
Iter: 1775 loss: 6.59348416e-08
Iter: 1776 loss: 6.47713705e-08
Iter: 1777 loss: 6.47397158e-08
Iter: 1778 loss: 6.49605596e-08
Iter: 1779 loss: 6.47379608e-08
Iter: 1780 loss: 6.47140439e-08
Iter: 1781 loss: 6.46903615e-08
Iter: 1782 loss: 6.46883862e-08
Iter: 1783 loss: 6.4685e-08
Iter: 1784 loss: 6.4670644e-08
Iter: 1785 loss: 6.46558789e-08
Iter: 1786 loss: 6.46605116e-08
Iter: 1787 loss: 6.46430038e-08
Iter: 1788 loss: 6.46259934e-08
Iter: 1789 loss: 6.46515659e-08
Iter: 1790 loss: 6.46232223e-08
Iter: 1791 loss: 6.45913261e-08
Iter: 1792 loss: 6.46377316e-08
Iter: 1793 loss: 6.45826645e-08
Iter: 1794 loss: 6.45688303e-08
Iter: 1795 loss: 6.46034195e-08
Iter: 1796 loss: 6.45596785e-08
Iter: 1797 loss: 6.45332676e-08
Iter: 1798 loss: 6.46076472e-08
Iter: 1799 loss: 6.4522915e-08
Iter: 1800 loss: 6.45050449e-08
Iter: 1801 loss: 6.44999503e-08
Iter: 1802 loss: 6.44949338e-08
Iter: 1803 loss: 6.44752944e-08
Iter: 1804 loss: 6.46822684e-08
Iter: 1805 loss: 6.44754365e-08
Iter: 1806 loss: 6.44516902e-08
Iter: 1807 loss: 6.44221103e-08
Iter: 1808 loss: 6.44217479e-08
Iter: 1809 loss: 6.43917133e-08
Iter: 1810 loss: 6.45849241e-08
Iter: 1811 loss: 6.43863558e-08
Iter: 1812 loss: 6.43614229e-08
Iter: 1813 loss: 6.43469136e-08
Iter: 1814 loss: 6.43354241e-08
Iter: 1815 loss: 6.43131486e-08
Iter: 1816 loss: 6.44881482e-08
Iter: 1817 loss: 6.43070877e-08
Iter: 1818 loss: 6.42882441e-08
Iter: 1819 loss: 6.4287e-08
Iter: 1820 loss: 6.42812381e-08
Iter: 1821 loss: 6.42537259e-08
Iter: 1822 loss: 6.4433614e-08
Iter: 1823 loss: 6.42462084e-08
Iter: 1824 loss: 6.42254605e-08
Iter: 1825 loss: 6.42245297e-08
Iter: 1826 loss: 6.42039e-08
Iter: 1827 loss: 6.4181e-08
Iter: 1828 loss: 6.41735767e-08
Iter: 1829 loss: 6.41507896e-08
Iter: 1830 loss: 6.44167812e-08
Iter: 1831 loss: 6.41465618e-08
Iter: 1832 loss: 6.41265316e-08
Iter: 1833 loss: 6.41145448e-08
Iter: 1834 loss: 6.41061e-08
Iter: 1835 loss: 6.40759e-08
Iter: 1836 loss: 6.43000533e-08
Iter: 1837 loss: 6.40723528e-08
Iter: 1838 loss: 6.40509796e-08
Iter: 1839 loss: 6.40850644e-08
Iter: 1840 loss: 6.40399378e-08
Iter: 1841 loss: 6.40111537e-08
Iter: 1842 loss: 6.40176125e-08
Iter: 1843 loss: 6.39908677e-08
Iter: 1844 loss: 6.39608686e-08
Iter: 1845 loss: 6.40065707e-08
Iter: 1846 loss: 6.39483915e-08
Iter: 1847 loss: 6.39051905e-08
Iter: 1848 loss: 6.39169428e-08
Iter: 1849 loss: 6.38835331e-08
Iter: 1850 loss: 6.38510897e-08
Iter: 1851 loss: 6.38543867e-08
Iter: 1852 loss: 6.38351381e-08
Iter: 1853 loss: 6.38150581e-08
Iter: 1854 loss: 6.38130686e-08
Iter: 1855 loss: 6.37900754e-08
Iter: 1856 loss: 6.37745146e-08
Iter: 1857 loss: 6.37617248e-08
Iter: 1858 loss: 6.37307e-08
Iter: 1859 loss: 6.38318696e-08
Iter: 1860 loss: 6.3722e-08
Iter: 1861 loss: 6.37094573e-08
Iter: 1862 loss: 6.36976836e-08
Iter: 1863 loss: 6.36842117e-08
Iter: 1864 loss: 6.36493951e-08
Iter: 1865 loss: 6.43149e-08
Iter: 1866 loss: 6.36449613e-08
Iter: 1867 loss: 6.36221174e-08
Iter: 1868 loss: 6.36172928e-08
Iter: 1869 loss: 6.36040909e-08
Iter: 1870 loss: 6.36062509e-08
Iter: 1871 loss: 6.35867252e-08
Iter: 1872 loss: 6.3567974e-08
Iter: 1873 loss: 6.37542428e-08
Iter: 1874 loss: 6.3565345e-08
Iter: 1875 loss: 6.35542037e-08
Iter: 1876 loss: 6.3561771e-08
Iter: 1877 loss: 6.35462598e-08
Iter: 1878 loss: 6.35239843e-08
Iter: 1879 loss: 6.35323616e-08
Iter: 1880 loss: 6.35123456e-08
Iter: 1881 loss: 6.34833697e-08
Iter: 1882 loss: 6.34847837e-08
Iter: 1883 loss: 6.34561559e-08
Iter: 1884 loss: 6.34670343e-08
Iter: 1885 loss: 6.34467625e-08
Iter: 1886 loss: 6.34383071e-08
Iter: 1887 loss: 6.34066879e-08
Iter: 1888 loss: 6.40086455e-08
Iter: 1889 loss: 6.3406091e-08
Iter: 1890 loss: 6.33945092e-08
Iter: 1891 loss: 6.3413637e-08
Iter: 1892 loss: 6.3383105e-08
Iter: 1893 loss: 6.33702797e-08
Iter: 1894 loss: 6.33445225e-08
Iter: 1895 loss: 6.33439612e-08
Iter: 1896 loss: 6.3308633e-08
Iter: 1897 loss: 6.34464357e-08
Iter: 1898 loss: 6.33014139e-08
Iter: 1899 loss: 6.32776249e-08
Iter: 1900 loss: 6.33744293e-08
Iter: 1901 loss: 6.3274e-08
Iter: 1902 loss: 6.3246695e-08
Iter: 1903 loss: 6.33298072e-08
Iter: 1904 loss: 6.32428865e-08
Iter: 1905 loss: 6.32209805e-08
Iter: 1906 loss: 6.31796837e-08
Iter: 1907 loss: 6.38697202e-08
Iter: 1908 loss: 6.3180444e-08
Iter: 1909 loss: 6.31570884e-08
Iter: 1910 loss: 6.31566905e-08
Iter: 1911 loss: 6.31346566e-08
Iter: 1912 loss: 6.31391686e-08
Iter: 1913 loss: 6.31101642e-08
Iter: 1914 loss: 6.30977937e-08
Iter: 1915 loss: 6.31042241e-08
Iter: 1916 loss: 6.30800656e-08
Iter: 1917 loss: 6.30586356e-08
Iter: 1918 loss: 6.3105162e-08
Iter: 1919 loss: 6.30507913e-08
Iter: 1920 loss: 6.3032374e-08
Iter: 1921 loss: 6.32408046e-08
Iter: 1922 loss: 6.30311163e-08
Iter: 1923 loss: 6.30069366e-08
Iter: 1924 loss: 6.29900185e-08
Iter: 1925 loss: 6.29832826e-08
Iter: 1926 loss: 6.29619308e-08
Iter: 1927 loss: 6.2911937e-08
Iter: 1928 loss: 6.29136068e-08
Iter: 1929 loss: 6.29114822e-08
Iter: 1930 loss: 6.28807e-08
Iter: 1931 loss: 6.28626e-08
Iter: 1932 loss: 6.2822437e-08
Iter: 1933 loss: 6.36647073e-08
Iter: 1934 loss: 6.28212646e-08
Iter: 1935 loss: 6.27833288e-08
Iter: 1936 loss: 6.3268935e-08
Iter: 1937 loss: 6.27858725e-08
Iter: 1938 loss: 6.27686916e-08
Iter: 1939 loss: 6.2740412e-08
Iter: 1940 loss: 6.27363619e-08
Iter: 1941 loss: 6.27089634e-08
Iter: 1942 loss: 6.28178896e-08
Iter: 1943 loss: 6.26999821e-08
Iter: 1944 loss: 6.2678609e-08
Iter: 1945 loss: 6.27359924e-08
Iter: 1946 loss: 6.26652081e-08
Iter: 1947 loss: 6.26395e-08
Iter: 1948 loss: 6.26421652e-08
Iter: 1949 loss: 6.26223482e-08
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4
+ date
Tue Oct 20 16:38:46 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8
+ date
Tue Oct 20 16:38:46 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1 ']'
+ echo 'Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1'
Already computed model for /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd74b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd785bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd785f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd785d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd6e6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd6488c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd6199d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd5bf6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd5d9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd59ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd589158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd5611e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd572840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd52c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd4d28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd4ee9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8dd4e56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d33379d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d3312268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d3312048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d3312730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d3312b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d3263ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d3245598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d3245b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d31ed488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d31b2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d31dc840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d316b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d318eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d312e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d316b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d3150b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d30eeae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d30ee378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff8d30d0b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 7.16933391e-06
Iter: 2 loss: 5.77406718e-06
Iter: 3 loss: 4.57205533e-06
Iter: 4 loss: 3.8670405e-06
Iter: 5 loss: 4.78305037e-06
Iter: 6 loss: 3.50852815e-06
Iter: 7 loss: 3.16327987e-06
Iter: 8 loss: 4.24186601e-06
Iter: 9 loss: 3.06270567e-06
Iter: 10 loss: 2.82471319e-06
Iter: 11 loss: 5.67329516e-06
Iter: 12 loss: 2.82167184e-06
Iter: 13 loss: 2.67483165e-06
Iter: 14 loss: 2.47666048e-06
Iter: 15 loss: 2.46651757e-06
Iter: 16 loss: 2.30807336e-06
Iter: 17 loss: 3.42917497e-06
Iter: 18 loss: 2.29380908e-06
Iter: 19 loss: 2.1548376e-06
Iter: 20 loss: 3.22472897e-06
Iter: 21 loss: 2.14456395e-06
Iter: 22 loss: 2.05652532e-06
Iter: 23 loss: 1.83583973e-06
Iter: 24 loss: 3.88837452e-06
Iter: 25 loss: 1.80418601e-06
Iter: 26 loss: 1.69072518e-06
Iter: 27 loss: 1.68234249e-06
Iter: 28 loss: 1.60999e-06
Iter: 29 loss: 1.60980244e-06
Iter: 30 loss: 1.57808017e-06
Iter: 31 loss: 1.4913046e-06
Iter: 32 loss: 2.02606907e-06
Iter: 33 loss: 1.46821799e-06
Iter: 34 loss: 1.35775144e-06
Iter: 35 loss: 1.98356656e-06
Iter: 36 loss: 1.34244294e-06
Iter: 37 loss: 1.31763e-06
Iter: 38 loss: 1.305252e-06
Iter: 39 loss: 1.26427858e-06
Iter: 40 loss: 1.17572836e-06
Iter: 41 loss: 2.54599e-06
Iter: 42 loss: 1.17247021e-06
Iter: 43 loss: 1.11258055e-06
Iter: 44 loss: 1.11257214e-06
Iter: 45 loss: 1.06694017e-06
Iter: 46 loss: 1.26220596e-06
Iter: 47 loss: 1.05749621e-06
Iter: 48 loss: 1.02054571e-06
Iter: 49 loss: 1.02606168e-06
Iter: 50 loss: 9.92613195e-07
Iter: 51 loss: 9.56816393e-07
Iter: 52 loss: 1.02739693e-06
Iter: 53 loss: 9.42085421e-07
Iter: 54 loss: 9.1121592e-07
Iter: 55 loss: 9.11212965e-07
Iter: 56 loss: 8.89129296e-07
Iter: 57 loss: 8.4836131e-07
Iter: 58 loss: 1.78582536e-06
Iter: 59 loss: 8.48320326e-07
Iter: 60 loss: 8.09478593e-07
Iter: 61 loss: 8.86913256e-07
Iter: 62 loss: 7.93602567e-07
Iter: 63 loss: 7.74572186e-07
Iter: 64 loss: 7.68018822e-07
Iter: 65 loss: 7.50946924e-07
Iter: 66 loss: 7.07125594e-07
Iter: 67 loss: 1.07513108e-06
Iter: 68 loss: 6.99624877e-07
Iter: 69 loss: 6.65441064e-07
Iter: 70 loss: 8.30329668e-07
Iter: 71 loss: 6.59374962e-07
Iter: 72 loss: 6.34193384e-07
Iter: 73 loss: 7.6609e-07
Iter: 74 loss: 6.30233274e-07
Iter: 75 loss: 6.23773246e-07
Iter: 76 loss: 6.19258117e-07
Iter: 77 loss: 6.10188749e-07
Iter: 78 loss: 5.90472268e-07
Iter: 79 loss: 8.86620796e-07
Iter: 80 loss: 5.89640365e-07
Iter: 81 loss: 5.77277092e-07
Iter: 82 loss: 5.77262085e-07
Iter: 83 loss: 5.64625395e-07
Iter: 84 loss: 5.74591184e-07
Iter: 85 loss: 5.5696529e-07
Iter: 86 loss: 5.46203864e-07
Iter: 87 loss: 5.58123475e-07
Iter: 88 loss: 5.40365e-07
Iter: 89 loss: 5.2857348e-07
Iter: 90 loss: 5.81032168e-07
Iter: 91 loss: 5.26240797e-07
Iter: 92 loss: 5.16834291e-07
Iter: 93 loss: 5.97621352e-07
Iter: 94 loss: 5.163281e-07
Iter: 95 loss: 5.09157076e-07
Iter: 96 loss: 4.98666566e-07
Iter: 97 loss: 4.98385589e-07
Iter: 98 loss: 4.87756438e-07
Iter: 99 loss: 5.00501699e-07
Iter: 100 loss: 4.82190103e-07
Iter: 101 loss: 4.72167613e-07
Iter: 102 loss: 4.7180913e-07
Iter: 103 loss: 4.64469906e-07
Iter: 104 loss: 4.48079504e-07
Iter: 105 loss: 6.78195931e-07
Iter: 106 loss: 4.47218554e-07
Iter: 107 loss: 4.33842644e-07
Iter: 108 loss: 4.91353262e-07
Iter: 109 loss: 4.31091394e-07
Iter: 110 loss: 4.28651873e-07
Iter: 111 loss: 4.26477754e-07
Iter: 112 loss: 4.21663714e-07
Iter: 113 loss: 4.148811e-07
Iter: 114 loss: 4.14619763e-07
Iter: 115 loss: 4.08626363e-07
Iter: 116 loss: 4.22167403e-07
Iter: 117 loss: 4.06372777e-07
Iter: 118 loss: 3.97578475e-07
Iter: 119 loss: 4.39602076e-07
Iter: 120 loss: 3.96012865e-07
Iter: 121 loss: 3.91809237e-07
Iter: 122 loss: 3.8225636e-07
Iter: 123 loss: 5.09058282e-07
Iter: 124 loss: 3.81685282e-07
Iter: 125 loss: 3.75162415e-07
Iter: 126 loss: 3.74293478e-07
Iter: 127 loss: 3.69637689e-07
Iter: 128 loss: 3.81887389e-07
Iter: 129 loss: 3.68062672e-07
Iter: 130 loss: 3.64123593e-07
Iter: 131 loss: 3.66962922e-07
Iter: 132 loss: 3.61679412e-07
Iter: 133 loss: 3.57709069e-07
Iter: 134 loss: 3.61904341e-07
Iter: 135 loss: 3.5553029e-07
Iter: 136 loss: 3.52048858e-07
Iter: 137 loss: 3.51958647e-07
Iter: 138 loss: 3.48996082e-07
Iter: 139 loss: 3.41148166e-07
Iter: 140 loss: 3.95396967e-07
Iter: 141 loss: 3.39347963e-07
Iter: 142 loss: 3.31316784e-07
Iter: 143 loss: 3.47208498e-07
Iter: 144 loss: 3.2798556e-07
Iter: 145 loss: 3.28776366e-07
Iter: 146 loss: 3.25013048e-07
Iter: 147 loss: 3.21941968e-07
Iter: 148 loss: 3.20299563e-07
Iter: 149 loss: 3.18941431e-07
Iter: 150 loss: 3.16438246e-07
Iter: 151 loss: 3.1997e-07
Iter: 152 loss: 3.15192153e-07
Iter: 153 loss: 3.11505573e-07
Iter: 154 loss: 3.33824147e-07
Iter: 155 loss: 3.1105975e-07
Iter: 156 loss: 3.08555855e-07
Iter: 157 loss: 3.03309889e-07
Iter: 158 loss: 3.91326239e-07
Iter: 159 loss: 3.03163574e-07
Iter: 160 loss: 2.98282629e-07
Iter: 161 loss: 3.44528132e-07
Iter: 162 loss: 2.98080465e-07
Iter: 163 loss: 2.93125879e-07
Iter: 164 loss: 3.15649e-07
Iter: 165 loss: 2.92189e-07
Iter: 166 loss: 2.89491652e-07
Iter: 167 loss: 2.86861678e-07
Iter: 168 loss: 2.86278436e-07
Iter: 169 loss: 2.82563207e-07
Iter: 170 loss: 3.22952246e-07
Iter: 171 loss: 2.82489395e-07
Iter: 172 loss: 2.80572351e-07
Iter: 173 loss: 3.03647283e-07
Iter: 174 loss: 2.80553081e-07
Iter: 175 loss: 2.78884e-07
Iter: 176 loss: 2.77161774e-07
Iter: 177 loss: 2.76842059e-07
Iter: 178 loss: 2.73707258e-07
Iter: 179 loss: 2.69441074e-07
Iter: 180 loss: 2.69227854e-07
Iter: 181 loss: 2.67009227e-07
Iter: 182 loss: 2.66834064e-07
Iter: 183 loss: 2.63899381e-07
Iter: 184 loss: 2.67901697e-07
Iter: 185 loss: 2.62440579e-07
Iter: 186 loss: 2.60551218e-07
Iter: 187 loss: 2.59871854e-07
Iter: 188 loss: 2.58821757e-07
Iter: 189 loss: 2.58048829e-07
Iter: 190 loss: 2.57521265e-07
Iter: 191 loss: 2.56688793e-07
Iter: 192 loss: 2.5459326e-07
Iter: 193 loss: 2.7266e-07
Iter: 194 loss: 2.54272976e-07
Iter: 195 loss: 2.51487677e-07
Iter: 196 loss: 2.53556578e-07
Iter: 197 loss: 2.49767425e-07
Iter: 198 loss: 2.47960173e-07
Iter: 199 loss: 2.47543852e-07
Iter: 200 loss: 2.4626479e-07
Iter: 201 loss: 2.43539887e-07
Iter: 202 loss: 2.86517178e-07
Iter: 203 loss: 2.43446095e-07
Iter: 204 loss: 2.41213399e-07
Iter: 205 loss: 2.50283563e-07
Iter: 206 loss: 2.40713035e-07
Iter: 207 loss: 2.38701148e-07
Iter: 208 loss: 2.38697339e-07
Iter: 209 loss: 2.37678151e-07
Iter: 210 loss: 2.38120677e-07
Iter: 211 loss: 2.36960915e-07
Iter: 212 loss: 2.35621059e-07
Iter: 213 loss: 2.34086599e-07
Iter: 214 loss: 2.33887675e-07
Iter: 215 loss: 2.31954573e-07
Iter: 216 loss: 2.50438291e-07
Iter: 217 loss: 2.31878914e-07
Iter: 218 loss: 2.29791581e-07
Iter: 219 loss: 2.39064661e-07
Iter: 220 loss: 2.29397244e-07
Iter: 221 loss: 2.28246108e-07
Iter: 222 loss: 2.2635021e-07
Iter: 223 loss: 2.26348e-07
Iter: 224 loss: 2.2553219e-07
Iter: 225 loss: 2.25207856e-07
Iter: 226 loss: 2.24228188e-07
Iter: 227 loss: 2.22771192e-07
Iter: 228 loss: 2.22730762e-07
Iter: 229 loss: 2.21617967e-07
Iter: 230 loss: 2.22841123e-07
Iter: 231 loss: 2.21002693e-07
Iter: 232 loss: 2.19889515e-07
Iter: 233 loss: 2.19884768e-07
Iter: 234 loss: 2.18995055e-07
Iter: 235 loss: 2.1678278e-07
Iter: 236 loss: 2.37447367e-07
Iter: 237 loss: 2.16457352e-07
Iter: 238 loss: 2.14009219e-07
Iter: 239 loss: 2.22615597e-07
Iter: 240 loss: 2.13370242e-07
Iter: 241 loss: 2.12146148e-07
Iter: 242 loss: 2.12063867e-07
Iter: 243 loss: 2.1063245e-07
Iter: 244 loss: 2.10506585e-07
Iter: 245 loss: 2.09429118e-07
Iter: 246 loss: 2.08622382e-07
Iter: 247 loss: 2.07438262e-07
Iter: 248 loss: 2.07401513e-07
Iter: 249 loss: 2.06449187e-07
Iter: 250 loss: 2.06372661e-07
Iter: 251 loss: 2.05231601e-07
Iter: 252 loss: 2.06120461e-07
Iter: 253 loss: 2.04530522e-07
Iter: 254 loss: 2.03458114e-07
Iter: 255 loss: 2.01482081e-07
Iter: 256 loss: 2.46248192e-07
Iter: 257 loss: 2.01457823e-07
Iter: 258 loss: 2.01040905e-07
Iter: 259 loss: 2.00411378e-07
Iter: 260 loss: 1.99557235e-07
Iter: 261 loss: 1.99464381e-07
Iter: 262 loss: 1.98850813e-07
Iter: 263 loss: 1.97896e-07
Iter: 264 loss: 1.99776082e-07
Iter: 265 loss: 1.97473895e-07
Iter: 266 loss: 1.96564329e-07
Iter: 267 loss: 1.96415641e-07
Iter: 268 loss: 1.95774518e-07
Iter: 269 loss: 1.94878538e-07
Iter: 270 loss: 1.9488094e-07
Iter: 271 loss: 1.93824519e-07
Iter: 272 loss: 1.93107525e-07
Iter: 273 loss: 1.9272e-07
Iter: 274 loss: 1.91563913e-07
Iter: 275 loss: 1.93033983e-07
Iter: 276 loss: 1.90975214e-07
Iter: 277 loss: 1.90182675e-07
Iter: 278 loss: 1.90073024e-07
Iter: 279 loss: 1.89639778e-07
Iter: 280 loss: 1.88579833e-07
Iter: 281 loss: 2.01332838e-07
Iter: 282 loss: 1.88508182e-07
Iter: 283 loss: 1.87916029e-07
Iter: 284 loss: 1.87892084e-07
Iter: 285 loss: 1.8720462e-07
Iter: 286 loss: 1.87702256e-07
Iter: 287 loss: 1.86762776e-07
Iter: 288 loss: 1.86199117e-07
Iter: 289 loss: 1.85508839e-07
Iter: 290 loss: 1.8544489e-07
Iter: 291 loss: 1.84622479e-07
Iter: 292 loss: 1.8462049e-07
Iter: 293 loss: 1.83873027e-07
Iter: 294 loss: 1.82399674e-07
Iter: 295 loss: 2.11925936e-07
Iter: 296 loss: 1.82399134e-07
Iter: 297 loss: 1.81405724e-07
Iter: 298 loss: 1.89458262e-07
Iter: 299 loss: 1.81336034e-07
Iter: 300 loss: 1.80396228e-07
Iter: 301 loss: 1.81118935e-07
Iter: 302 loss: 1.79841237e-07
Iter: 303 loss: 1.79405731e-07
Iter: 304 loss: 1.79310035e-07
Iter: 305 loss: 1.78909517e-07
Iter: 306 loss: 1.78375075e-07
Iter: 307 loss: 1.78350192e-07
Iter: 308 loss: 1.77685834e-07
Iter: 309 loss: 1.78010538e-07
Iter: 310 loss: 1.77249262e-07
Iter: 311 loss: 1.76429936e-07
Iter: 312 loss: 1.7642563e-07
Iter: 313 loss: 1.75901107e-07
Iter: 314 loss: 1.74647511e-07
Iter: 315 loss: 1.88160286e-07
Iter: 316 loss: 1.74503754e-07
Iter: 317 loss: 1.73334627e-07
Iter: 318 loss: 1.77671268e-07
Iter: 319 loss: 1.73041514e-07
Iter: 320 loss: 1.72061135e-07
Iter: 321 loss: 1.74146109e-07
Iter: 322 loss: 1.71658e-07
Iter: 323 loss: 1.70734353e-07
Iter: 324 loss: 1.76675172e-07
Iter: 325 loss: 1.70642664e-07
Iter: 326 loss: 1.70149832e-07
Iter: 327 loss: 1.70082856e-07
Iter: 328 loss: 1.69735046e-07
Iter: 329 loss: 1.68815291e-07
Iter: 330 loss: 1.76867445e-07
Iter: 331 loss: 1.68669828e-07
Iter: 332 loss: 1.68224062e-07
Iter: 333 loss: 1.68083872e-07
Iter: 334 loss: 1.67573e-07
Iter: 335 loss: 1.6684325e-07
Iter: 336 loss: 1.66815525e-07
Iter: 337 loss: 1.66189267e-07
Iter: 338 loss: 1.69978961e-07
Iter: 339 loss: 1.66119975e-07
Iter: 340 loss: 1.65658079e-07
Iter: 341 loss: 1.67253205e-07
Iter: 342 loss: 1.65546311e-07
Iter: 343 loss: 1.6500627e-07
Iter: 344 loss: 1.66746773e-07
Iter: 345 loss: 1.64851656e-07
Iter: 346 loss: 1.64391551e-07
Iter: 347 loss: 1.64161449e-07
Iter: 348 loss: 1.63949693e-07
Iter: 349 loss: 1.63477381e-07
Iter: 350 loss: 1.69028752e-07
Iter: 351 loss: 1.63460527e-07
Iter: 352 loss: 1.62875068e-07
Iter: 353 loss: 1.62268122e-07
Iter: 354 loss: 1.62162564e-07
Iter: 355 loss: 1.61504516e-07
Iter: 356 loss: 1.61694672e-07
Iter: 357 loss: 1.61040418e-07
Iter: 358 loss: 1.60277e-07
Iter: 359 loss: 1.61547177e-07
Iter: 360 loss: 1.5991975e-07
Iter: 361 loss: 1.59948229e-07
Iter: 362 loss: 1.59554077e-07
Iter: 363 loss: 1.59287737e-07
Iter: 364 loss: 1.58850483e-07
Iter: 365 loss: 1.58859834e-07
Iter: 366 loss: 1.58539706e-07
Iter: 367 loss: 1.61896168e-07
Iter: 368 loss: 1.58526149e-07
Iter: 369 loss: 1.58132934e-07
Iter: 370 loss: 1.57407939e-07
Iter: 371 loss: 1.57403406e-07
Iter: 372 loss: 1.56768408e-07
Iter: 373 loss: 1.57062786e-07
Iter: 374 loss: 1.56357117e-07
Iter: 375 loss: 1.55771829e-07
Iter: 376 loss: 1.55762308e-07
Iter: 377 loss: 1.55310033e-07
Iter: 378 loss: 1.5638156e-07
Iter: 379 loss: 1.55122976e-07
Iter: 380 loss: 1.5470961e-07
Iter: 381 loss: 1.55182761e-07
Iter: 382 loss: 1.54489371e-07
Iter: 383 loss: 1.54067052e-07
Iter: 384 loss: 1.54944772e-07
Iter: 385 loss: 1.5390404e-07
Iter: 386 loss: 1.53468477e-07
Iter: 387 loss: 1.58532217e-07
Iter: 388 loss: 1.53461556e-07
Iter: 389 loss: 1.53163967e-07
Iter: 390 loss: 1.52318322e-07
Iter: 391 loss: 1.56681779e-07
Iter: 392 loss: 1.52057098e-07
Iter: 393 loss: 1.51122109e-07
Iter: 394 loss: 1.5632402e-07
Iter: 395 loss: 1.50977485e-07
Iter: 396 loss: 1.5097109e-07
Iter: 397 loss: 1.50637192e-07
Iter: 398 loss: 1.50367981e-07
Iter: 399 loss: 1.49809779e-07
Iter: 400 loss: 1.58580406e-07
Iter: 401 loss: 1.49793365e-07
Iter: 402 loss: 1.49535026e-07
Iter: 403 loss: 1.49522364e-07
Iter: 404 loss: 1.49244073e-07
Iter: 405 loss: 1.49205874e-07
Iter: 406 loss: 1.4899976e-07
Iter: 407 loss: 1.48699883e-07
Iter: 408 loss: 1.48120449e-07
Iter: 409 loss: 1.59483164e-07
Iter: 410 loss: 1.48117351e-07
Iter: 411 loss: 1.47624178e-07
Iter: 412 loss: 1.4761963e-07
Iter: 413 loss: 1.47089878e-07
Iter: 414 loss: 1.48028519e-07
Iter: 415 loss: 1.46867208e-07
Iter: 416 loss: 1.464349e-07
Iter: 417 loss: 1.47535076e-07
Iter: 418 loss: 1.46290432e-07
Iter: 419 loss: 1.4593229e-07
Iter: 420 loss: 1.46370795e-07
Iter: 421 loss: 1.45737076e-07
Iter: 422 loss: 1.45499271e-07
Iter: 423 loss: 1.45486524e-07
Iter: 424 loss: 1.45272338e-07
Iter: 425 loss: 1.45091249e-07
Iter: 426 loss: 1.45037518e-07
Iter: 427 loss: 1.44720119e-07
Iter: 428 loss: 1.44218177e-07
Iter: 429 loss: 1.44221957e-07
Iter: 430 loss: 1.44272832e-07
Iter: 431 loss: 1.43958985e-07
Iter: 432 loss: 1.43720214e-07
Iter: 433 loss: 1.43109588e-07
Iter: 434 loss: 1.48005356e-07
Iter: 435 loss: 1.42996058e-07
Iter: 436 loss: 1.42608556e-07
Iter: 437 loss: 1.4260138e-07
Iter: 438 loss: 1.42230192e-07
Iter: 439 loss: 1.43175811e-07
Iter: 440 loss: 1.42075763e-07
Iter: 441 loss: 1.41833354e-07
Iter: 442 loss: 1.41504188e-07
Iter: 443 loss: 1.41475653e-07
Iter: 444 loss: 1.41104039e-07
Iter: 445 loss: 1.41681966e-07
Iter: 446 loss: 1.40936891e-07
Iter: 447 loss: 1.40671744e-07
Iter: 448 loss: 1.40638562e-07
Iter: 449 loss: 1.40376102e-07
Iter: 450 loss: 1.39861953e-07
Iter: 451 loss: 1.50058455e-07
Iter: 452 loss: 1.39851295e-07
Iter: 453 loss: 1.39359116e-07
Iter: 454 loss: 1.43283799e-07
Iter: 455 loss: 1.39344621e-07
Iter: 456 loss: 1.38983253e-07
Iter: 457 loss: 1.40777871e-07
Iter: 458 loss: 1.38928655e-07
Iter: 459 loss: 1.38608e-07
Iter: 460 loss: 1.40192839e-07
Iter: 461 loss: 1.38543413e-07
Iter: 462 loss: 1.38338393e-07
Iter: 463 loss: 1.37999351e-07
Iter: 464 loss: 1.37993894e-07
Iter: 465 loss: 1.37736635e-07
Iter: 466 loss: 1.41279187e-07
Iter: 467 loss: 1.37728378e-07
Iter: 468 loss: 1.37405223e-07
Iter: 469 loss: 1.37821644e-07
Iter: 470 loss: 1.37241329e-07
Iter: 471 loss: 1.36973071e-07
Iter: 472 loss: 1.36808623e-07
Iter: 473 loss: 1.3670757e-07
Iter: 474 loss: 1.36184795e-07
Iter: 475 loss: 1.4015086e-07
Iter: 476 loss: 1.361378e-07
Iter: 477 loss: 1.35896229e-07
Iter: 478 loss: 1.35478516e-07
Iter: 479 loss: 1.3547529e-07
Iter: 480 loss: 1.35081137e-07
Iter: 481 loss: 1.38182926e-07
Iter: 482 loss: 1.35057135e-07
Iter: 483 loss: 1.34704692e-07
Iter: 484 loss: 1.36923063e-07
Iter: 485 loss: 1.34663168e-07
Iter: 486 loss: 1.34484452e-07
Iter: 487 loss: 1.34346067e-07
Iter: 488 loss: 1.34298119e-07
Iter: 489 loss: 1.33984898e-07
Iter: 490 loss: 1.34975522e-07
Iter: 491 loss: 1.3389463e-07
Iter: 492 loss: 1.33576776e-07
Iter: 493 loss: 1.35795261e-07
Iter: 494 loss: 1.33547502e-07
Iter: 495 loss: 1.33248875e-07
Iter: 496 loss: 1.33327404e-07
Iter: 497 loss: 1.33027925e-07
Iter: 498 loss: 1.32707569e-07
Iter: 499 loss: 1.32642043e-07
Iter: 500 loss: 1.32429633e-07
Iter: 501 loss: 1.32343033e-07
Iter: 502 loss: 1.32232344e-07
Iter: 503 loss: 1.32072415e-07
Iter: 504 loss: 1.31799126e-07
Iter: 505 loss: 1.31803702e-07
Iter: 506 loss: 1.31651348e-07
Iter: 507 loss: 1.31648903e-07
Iter: 508 loss: 1.31486729e-07
Iter: 509 loss: 1.31227608e-07
Iter: 510 loss: 1.31216368e-07
Iter: 511 loss: 1.30948067e-07
Iter: 512 loss: 1.30881944e-07
Iter: 513 loss: 1.30707519e-07
Iter: 514 loss: 1.30430323e-07
Iter: 515 loss: 1.30413724e-07
Iter: 516 loss: 1.30160885e-07
Iter: 517 loss: 1.29958067e-07
Iter: 518 loss: 1.29873769e-07
Iter: 519 loss: 1.29571902e-07
Iter: 520 loss: 1.29348649e-07
Iter: 521 loss: 1.29234579e-07
Iter: 522 loss: 1.28984524e-07
Iter: 523 loss: 1.2896416e-07
Iter: 524 loss: 1.28756824e-07
Iter: 525 loss: 1.30133799e-07
Iter: 526 loss: 1.28728146e-07
Iter: 527 loss: 1.28569951e-07
Iter: 528 loss: 1.28239037e-07
Iter: 529 loss: 1.34271218e-07
Iter: 530 loss: 1.2822855e-07
Iter: 531 loss: 1.27855344e-07
Iter: 532 loss: 1.28890449e-07
Iter: 533 loss: 1.27729635e-07
Iter: 534 loss: 1.27517268e-07
Iter: 535 loss: 1.27478557e-07
Iter: 536 loss: 1.27325691e-07
Iter: 537 loss: 1.27068063e-07
Iter: 538 loss: 1.33366228e-07
Iter: 539 loss: 1.27062279e-07
Iter: 540 loss: 1.2694305e-07
Iter: 541 loss: 1.2691784e-07
Iter: 542 loss: 1.26782311e-07
Iter: 543 loss: 1.264454e-07
Iter: 544 loss: 1.29312781e-07
Iter: 545 loss: 1.2636481e-07
Iter: 546 loss: 1.26092573e-07
Iter: 547 loss: 1.27706898e-07
Iter: 548 loss: 1.26058012e-07
Iter: 549 loss: 1.25821728e-07
Iter: 550 loss: 1.28849663e-07
Iter: 551 loss: 1.25831363e-07
Iter: 552 loss: 1.2563055e-07
Iter: 553 loss: 1.25187825e-07
Iter: 554 loss: 1.30946361e-07
Iter: 555 loss: 1.25148887e-07
Iter: 556 loss: 1.24747544e-07
Iter: 557 loss: 1.26492367e-07
Iter: 558 loss: 1.24670905e-07
Iter: 559 loss: 1.24394603e-07
Iter: 560 loss: 1.24969546e-07
Iter: 561 loss: 1.2429274e-07
Iter: 562 loss: 1.24140882e-07
Iter: 563 loss: 1.24124909e-07
Iter: 564 loss: 1.2398587e-07
Iter: 565 loss: 1.2387811e-07
Iter: 566 loss: 1.23830802e-07
Iter: 567 loss: 1.23646302e-07
Iter: 568 loss: 1.23737038e-07
Iter: 569 loss: 1.23497486e-07
Iter: 570 loss: 1.23255631e-07
Iter: 571 loss: 1.25640469e-07
Iter: 572 loss: 1.2324854e-07
Iter: 573 loss: 1.23005719e-07
Iter: 574 loss: 1.2318992e-07
Iter: 575 loss: 1.22855454e-07
Iter: 576 loss: 1.22599246e-07
Iter: 577 loss: 1.22395107e-07
Iter: 578 loss: 1.22324877e-07
Iter: 579 loss: 1.22156095e-07
Iter: 580 loss: 1.22092729e-07
Iter: 581 loss: 1.21989217e-07
Iter: 582 loss: 1.2170338e-07
Iter: 583 loss: 1.23317903e-07
Iter: 584 loss: 1.21617347e-07
Iter: 585 loss: 1.21308958e-07
Iter: 586 loss: 1.22863781e-07
Iter: 587 loss: 1.2126074e-07
Iter: 588 loss: 1.21096761e-07
Iter: 589 loss: 1.21088334e-07
Iter: 590 loss: 1.20929684e-07
Iter: 591 loss: 1.2078101e-07
Iter: 592 loss: 1.20741248e-07
Iter: 593 loss: 1.20485112e-07
Iter: 594 loss: 1.20802071e-07
Iter: 595 loss: 1.20336836e-07
Iter: 596 loss: 1.20049719e-07
Iter: 597 loss: 1.23666936e-07
Iter: 598 loss: 1.20042785e-07
Iter: 599 loss: 1.19877569e-07
Iter: 600 loss: 1.19767407e-07
Iter: 601 loss: 1.19704097e-07
Iter: 602 loss: 1.19508428e-07
Iter: 603 loss: 1.20843481e-07
Iter: 604 loss: 1.19497983e-07
Iter: 605 loss: 1.19346026e-07
Iter: 606 loss: 1.20719591e-07
Iter: 607 loss: 1.19340783e-07
Iter: 608 loss: 1.19214164e-07
Iter: 609 loss: 1.19042213e-07
Iter: 610 loss: 1.19035079e-07
Iter: 611 loss: 1.1885993e-07
Iter: 612 loss: 1.20183287e-07
Iter: 613 loss: 1.1885102e-07
Iter: 614 loss: 1.18652935e-07
Iter: 615 loss: 1.18946865e-07
Iter: 616 loss: 1.18563634e-07
Iter: 617 loss: 1.18367112e-07
Iter: 618 loss: 1.17892995e-07
Iter: 619 loss: 1.22908986e-07
Iter: 620 loss: 1.17833061e-07
Iter: 621 loss: 1.17542449e-07
Iter: 622 loss: 1.17542619e-07
Iter: 623 loss: 1.1736914e-07
Iter: 624 loss: 1.17366667e-07
Iter: 625 loss: 1.17229519e-07
Iter: 626 loss: 1.17104037e-07
Iter: 627 loss: 1.17073299e-07
Iter: 628 loss: 1.16944378e-07
Iter: 629 loss: 1.16948719e-07
Iter: 630 loss: 1.16817397e-07
Iter: 631 loss: 1.16623561e-07
Iter: 632 loss: 1.1661399e-07
Iter: 633 loss: 1.16362067e-07
Iter: 634 loss: 1.16904644e-07
Iter: 635 loss: 1.16271693e-07
Iter: 636 loss: 1.16059866e-07
Iter: 637 loss: 1.18859916e-07
Iter: 638 loss: 1.16055631e-07
Iter: 639 loss: 1.15860892e-07
Iter: 640 loss: 1.16004763e-07
Iter: 641 loss: 1.15751291e-07
Iter: 642 loss: 1.15560482e-07
Iter: 643 loss: 1.15662687e-07
Iter: 644 loss: 1.15427376e-07
Iter: 645 loss: 1.15316546e-07
Iter: 646 loss: 1.15304786e-07
Iter: 647 loss: 1.15187305e-07
Iter: 648 loss: 1.14973489e-07
Iter: 649 loss: 1.19466179e-07
Iter: 650 loss: 1.14971172e-07
Iter: 651 loss: 1.14756368e-07
Iter: 652 loss: 1.14603317e-07
Iter: 653 loss: 1.1452876e-07
Iter: 654 loss: 1.14197974e-07
Iter: 655 loss: 1.15994908e-07
Iter: 656 loss: 1.14147014e-07
Iter: 657 loss: 1.13847761e-07
Iter: 658 loss: 1.18227305e-07
Iter: 659 loss: 1.13844024e-07
Iter: 660 loss: 1.13680962e-07
Iter: 661 loss: 1.13514062e-07
Iter: 662 loss: 1.13486692e-07
Iter: 663 loss: 1.13353352e-07
Iter: 664 loss: 1.13331964e-07
Iter: 665 loss: 1.1322706e-07
Iter: 666 loss: 1.13003303e-07
Iter: 667 loss: 1.17398642e-07
Iter: 668 loss: 1.1300147e-07
Iter: 669 loss: 1.12818242e-07
Iter: 670 loss: 1.13984967e-07
Iter: 671 loss: 1.12788115e-07
Iter: 672 loss: 1.12647442e-07
Iter: 673 loss: 1.14342242e-07
Iter: 674 loss: 1.12644734e-07
Iter: 675 loss: 1.12523239e-07
Iter: 676 loss: 1.12446756e-07
Iter: 677 loss: 1.12401011e-07
Iter: 678 loss: 1.12219624e-07
Iter: 679 loss: 1.12379205e-07
Iter: 680 loss: 1.12115622e-07
Iter: 681 loss: 1.11910289e-07
Iter: 682 loss: 1.14572941e-07
Iter: 683 loss: 1.11908861e-07
Iter: 684 loss: 1.11773119e-07
Iter: 685 loss: 1.11615861e-07
Iter: 686 loss: 1.11596705e-07
Iter: 687 loss: 1.11414217e-07
Iter: 688 loss: 1.112045e-07
Iter: 689 loss: 1.11168319e-07
Iter: 690 loss: 1.10898021e-07
Iter: 691 loss: 1.13719096e-07
Iter: 692 loss: 1.10893033e-07
Iter: 693 loss: 1.10758791e-07
Iter: 694 loss: 1.10752566e-07
Iter: 695 loss: 1.10621684e-07
Iter: 696 loss: 1.10258064e-07
Iter: 697 loss: 1.13445914e-07
Iter: 698 loss: 1.10211275e-07
Iter: 699 loss: 1.10163946e-07
Iter: 700 loss: 1.1007544e-07
Iter: 701 loss: 1.09930454e-07
Iter: 702 loss: 1.09771364e-07
Iter: 703 loss: 1.09756428e-07
Iter: 704 loss: 1.09584654e-07
Iter: 705 loss: 1.09798364e-07
Iter: 706 loss: 1.09509628e-07
Iter: 707 loss: 1.09353181e-07
Iter: 708 loss: 1.09351419e-07
Iter: 709 loss: 1.09274687e-07
Iter: 710 loss: 1.09193486e-07
Iter: 711 loss: 1.09170543e-07
Iter: 712 loss: 1.09033778e-07
Iter: 713 loss: 1.09620018e-07
Iter: 714 loss: 1.09015538e-07
Iter: 715 loss: 1.08895676e-07
Iter: 716 loss: 1.08984864e-07
Iter: 717 loss: 1.08815051e-07
Iter: 718 loss: 1.08645864e-07
Iter: 719 loss: 1.08661538e-07
Iter: 720 loss: 1.08521135e-07
Iter: 721 loss: 1.08324201e-07
Iter: 722 loss: 1.08565501e-07
Iter: 723 loss: 1.08215715e-07
Iter: 724 loss: 1.08009907e-07
Iter: 725 loss: 1.08244336e-07
Iter: 726 loss: 1.07904654e-07
Iter: 727 loss: 1.07681956e-07
Iter: 728 loss: 1.09695975e-07
Iter: 729 loss: 1.0767441e-07
Iter: 730 loss: 1.0753412e-07
Iter: 731 loss: 1.07537851e-07
Iter: 732 loss: 1.07417236e-07
Iter: 733 loss: 1.07138831e-07
Iter: 734 loss: 1.10211225e-07
Iter: 735 loss: 1.07116492e-07
Iter: 736 loss: 1.06966034e-07
Iter: 737 loss: 1.06953472e-07
Iter: 738 loss: 1.06773136e-07
Iter: 739 loss: 1.06765661e-07
Iter: 740 loss: 1.06624853e-07
Iter: 741 loss: 1.06495733e-07
Iter: 742 loss: 1.06771353e-07
Iter: 743 loss: 1.0644392e-07
Iter: 744 loss: 1.06335733e-07
Iter: 745 loss: 1.06337168e-07
Iter: 746 loss: 1.06245878e-07
Iter: 747 loss: 1.06251846e-07
Iter: 748 loss: 1.06186469e-07
Iter: 749 loss: 1.06053157e-07
Iter: 750 loss: 1.06296611e-07
Iter: 751 loss: 1.06000726e-07
Iter: 752 loss: 1.05880147e-07
Iter: 753 loss: 1.07134767e-07
Iter: 754 loss: 1.05880929e-07
Iter: 755 loss: 1.05780892e-07
Iter: 756 loss: 1.05607178e-07
Iter: 757 loss: 1.09029706e-07
Iter: 758 loss: 1.05602354e-07
Iter: 759 loss: 1.05368109e-07
Iter: 760 loss: 1.05355639e-07
Iter: 761 loss: 1.051718e-07
Iter: 762 loss: 1.04960463e-07
Iter: 763 loss: 1.06834364e-07
Iter: 764 loss: 1.04955241e-07
Iter: 765 loss: 1.04825055e-07
Iter: 766 loss: 1.04825389e-07
Iter: 767 loss: 1.04705265e-07
Iter: 768 loss: 1.04521192e-07
Iter: 769 loss: 1.04520637e-07
Iter: 770 loss: 1.04372106e-07
Iter: 771 loss: 1.05448144e-07
Iter: 772 loss: 1.04354761e-07
Iter: 773 loss: 1.04176713e-07
Iter: 774 loss: 1.04512218e-07
Iter: 775 loss: 1.04093623e-07
Iter: 776 loss: 1.03963785e-07
Iter: 777 loss: 1.03803643e-07
Iter: 778 loss: 1.03794221e-07
Iter: 779 loss: 1.03684982e-07
Iter: 780 loss: 1.03656944e-07
Iter: 781 loss: 1.0355167e-07
Iter: 782 loss: 1.03640417e-07
Iter: 783 loss: 1.03493164e-07
Iter: 784 loss: 1.03388572e-07
Iter: 785 loss: 1.03638271e-07
Iter: 786 loss: 1.03349166e-07
Iter: 787 loss: 1.03235664e-07
Iter: 788 loss: 1.03772905e-07
Iter: 789 loss: 1.03212422e-07
Iter: 790 loss: 1.03096497e-07
Iter: 791 loss: 1.03018827e-07
Iter: 792 loss: 1.02978795e-07
Iter: 793 loss: 1.02824309e-07
Iter: 794 loss: 1.026511e-07
Iter: 795 loss: 1.02623886e-07
Iter: 796 loss: 1.02375367e-07
Iter: 797 loss: 1.03317532e-07
Iter: 798 loss: 1.02318694e-07
Iter: 799 loss: 1.02170077e-07
Iter: 800 loss: 1.03651736e-07
Iter: 801 loss: 1.02161948e-07
Iter: 802 loss: 1.0205423e-07
Iter: 803 loss: 1.02053043e-07
Iter: 804 loss: 1.01973519e-07
Iter: 805 loss: 1.01799593e-07
Iter: 806 loss: 1.04396705e-07
Iter: 807 loss: 1.01795052e-07
Iter: 808 loss: 1.0166611e-07
Iter: 809 loss: 1.03520094e-07
Iter: 810 loss: 1.01663659e-07
Iter: 811 loss: 1.01496042e-07
Iter: 812 loss: 1.01494884e-07
Iter: 813 loss: 1.01351773e-07
Iter: 814 loss: 1.01186906e-07
Iter: 815 loss: 1.0134859e-07
Iter: 816 loss: 1.01107325e-07
Iter: 817 loss: 1.00941023e-07
Iter: 818 loss: 1.00950643e-07
Iter: 819 loss: 1.00841845e-07
Iter: 820 loss: 1.00703815e-07
Iter: 821 loss: 1.00701854e-07
Iter: 822 loss: 1.00571988e-07
Iter: 823 loss: 1.02320996e-07
Iter: 824 loss: 1.00567433e-07
Iter: 825 loss: 1.0046918e-07
Iter: 826 loss: 1.00808975e-07
Iter: 827 loss: 1.00441575e-07
Iter: 828 loss: 1.00346234e-07
Iter: 829 loss: 1.00150302e-07
Iter: 830 loss: 1.03724531e-07
Iter: 831 loss: 1.00149308e-07
Iter: 832 loss: 9.99488137e-08
Iter: 833 loss: 1.00438463e-07
Iter: 834 loss: 9.98843745e-08
Iter: 835 loss: 9.96850247e-08
Iter: 836 loss: 1.00193581e-07
Iter: 837 loss: 9.96127625e-08
Iter: 838 loss: 9.95165408e-08
Iter: 839 loss: 9.95008094e-08
Iter: 840 loss: 9.94003457e-08
Iter: 841 loss: 9.93778855e-08
Iter: 842 loss: 9.93088491e-08
Iter: 843 loss: 9.91998519e-08
Iter: 844 loss: 9.92640139e-08
Iter: 845 loss: 9.91373597e-08
Iter: 846 loss: 9.89654367e-08
Iter: 847 loss: 1.00294564e-07
Iter: 848 loss: 9.89416691e-08
Iter: 849 loss: 9.88580595e-08
Iter: 850 loss: 9.87116522e-08
Iter: 851 loss: 9.87061384e-08
Iter: 852 loss: 9.85836834e-08
Iter: 853 loss: 9.85751285e-08
Iter: 854 loss: 9.84861401e-08
Iter: 855 loss: 9.83836657e-08
Iter: 856 loss: 9.83739e-08
Iter: 857 loss: 9.825159e-08
Iter: 858 loss: 9.95697178e-08
Iter: 859 loss: 9.82505384e-08
Iter: 860 loss: 9.81339383e-08
Iter: 861 loss: 9.82356454e-08
Iter: 862 loss: 9.80758799e-08
Iter: 863 loss: 9.79637491e-08
Iter: 864 loss: 9.79313e-08
Iter: 865 loss: 9.78549366e-08
Iter: 866 loss: 9.77148744e-08
Iter: 867 loss: 9.76328636e-08
Iter: 868 loss: 9.75719701e-08
Iter: 869 loss: 9.73693446e-08
Iter: 870 loss: 9.85253e-08
Iter: 871 loss: 9.73306911e-08
Iter: 872 loss: 9.72216725e-08
Iter: 873 loss: 9.72027578e-08
Iter: 874 loss: 9.71117586e-08
Iter: 875 loss: 9.69668e-08
Iter: 876 loss: 9.69658913e-08
Iter: 877 loss: 9.68627489e-08
Iter: 878 loss: 9.85629072e-08
Iter: 879 loss: 9.68594094e-08
Iter: 880 loss: 9.67386669e-08
Iter: 881 loss: 9.6677951e-08
Iter: 882 loss: 9.66177254e-08
Iter: 883 loss: 9.65126929e-08
Iter: 884 loss: 9.7206339e-08
Iter: 885 loss: 9.65003e-08
Iter: 886 loss: 9.63721547e-08
Iter: 887 loss: 9.67679128e-08
Iter: 888 loss: 9.63330962e-08
Iter: 889 loss: 9.62573523e-08
Iter: 890 loss: 9.63141389e-08
Iter: 891 loss: 9.62082964e-08
Iter: 892 loss: 9.6071787e-08
Iter: 893 loss: 9.64705222e-08
Iter: 894 loss: 9.60324371e-08
Iter: 895 loss: 9.59291668e-08
Iter: 896 loss: 9.60848752e-08
Iter: 897 loss: 9.58728918e-08
Iter: 898 loss: 9.57830153e-08
Iter: 899 loss: 9.56588266e-08
Iter: 900 loss: 9.56512594e-08
Iter: 901 loss: 9.54785122e-08
Iter: 902 loss: 9.60169757e-08
Iter: 903 loss: 9.54316306e-08
Iter: 904 loss: 9.52641699e-08
Iter: 905 loss: 9.63160076e-08
Iter: 906 loss: 9.52534e-08
Iter: 907 loss: 9.50843173e-08
Iter: 908 loss: 9.58562225e-08
Iter: 909 loss: 9.50527e-08
Iter: 910 loss: 9.49353733e-08
Iter: 911 loss: 9.47566434e-08
Iter: 912 loss: 9.47525152e-08
Iter: 913 loss: 9.46539132e-08
Iter: 914 loss: 9.46274952e-08
Iter: 915 loss: 9.45319627e-08
Iter: 916 loss: 9.4400292e-08
Iter: 917 loss: 9.43984872e-08
Iter: 918 loss: 9.42972e-08
Iter: 919 loss: 9.58202406e-08
Iter: 920 loss: 9.42970075e-08
Iter: 921 loss: 9.41884863e-08
Iter: 922 loss: 9.41752134e-08
Iter: 923 loss: 9.40979419e-08
Iter: 924 loss: 9.40090317e-08
Iter: 925 loss: 9.53470547e-08
Iter: 926 loss: 9.40110567e-08
Iter: 927 loss: 9.39448057e-08
Iter: 928 loss: 9.37695148e-08
Iter: 929 loss: 9.55270565e-08
Iter: 930 loss: 9.37458111e-08
Iter: 931 loss: 9.35403364e-08
Iter: 932 loss: 9.39755438e-08
Iter: 933 loss: 9.34624751e-08
Iter: 934 loss: 9.32673814e-08
Iter: 935 loss: 9.58520658e-08
Iter: 936 loss: 9.32623294e-08
Iter: 937 loss: 9.31477899e-08
Iter: 938 loss: 9.33037327e-08
Iter: 939 loss: 9.30830097e-08
Iter: 940 loss: 9.2977146e-08
Iter: 941 loss: 9.40577394e-08
Iter: 942 loss: 9.29791213e-08
Iter: 943 loss: 9.28666708e-08
Iter: 944 loss: 9.29267827e-08
Iter: 945 loss: 9.27873742e-08
Iter: 946 loss: 9.26747248e-08
Iter: 947 loss: 9.27448269e-08
Iter: 948 loss: 9.26096391e-08
Iter: 949 loss: 9.24952559e-08
Iter: 950 loss: 9.24977e-08
Iter: 951 loss: 9.24326713e-08
Iter: 952 loss: 9.22748598e-08
Iter: 953 loss: 9.39871754e-08
Iter: 954 loss: 9.22666743e-08
Iter: 955 loss: 9.21528098e-08
Iter: 956 loss: 9.21433809e-08
Iter: 957 loss: 9.20568368e-08
Iter: 958 loss: 9.20350089e-08
Iter: 959 loss: 9.19821588e-08
Iter: 960 loss: 9.1887415e-08
Iter: 961 loss: 9.18923e-08
Iter: 962 loss: 9.18374639e-08
Iter: 963 loss: 9.17133605e-08
Iter: 964 loss: 9.3307662e-08
Iter: 965 loss: 9.16990857e-08
Iter: 966 loss: 9.15560108e-08
Iter: 967 loss: 9.26475963e-08
Iter: 968 loss: 9.1542276e-08
Iter: 969 loss: 9.1431275e-08
Iter: 970 loss: 9.13334475e-08
Iter: 971 loss: 9.12941545e-08
Iter: 972 loss: 9.11727369e-08
Iter: 973 loss: 9.1163578e-08
Iter: 974 loss: 9.10178812e-08
Iter: 975 loss: 9.14581477e-08
Iter: 976 loss: 9.0982482e-08
Iter: 977 loss: 9.08723905e-08
Iter: 978 loss: 9.08051661e-08
Iter: 979 loss: 9.07514917e-08
Iter: 980 loss: 9.0661807e-08
Iter: 981 loss: 9.06558384e-08
Iter: 982 loss: 9.05609454e-08
Iter: 983 loss: 9.0637e-08
Iter: 984 loss: 9.04968402e-08
Iter: 985 loss: 9.04026294e-08
Iter: 986 loss: 9.03506532e-08
Iter: 987 loss: 9.03025423e-08
Iter: 988 loss: 9.01655426e-08
Iter: 989 loss: 9.01677879e-08
Iter: 990 loss: 9.00993484e-08
Iter: 991 loss: 9.00696051e-08
Iter: 992 loss: 9.00368562e-08
Iter: 993 loss: 8.99309072e-08
Iter: 994 loss: 9.04456741e-08
Iter: 995 loss: 8.98946197e-08
Iter: 996 loss: 8.98368242e-08
Iter: 997 loss: 8.97352e-08
Iter: 998 loss: 8.97325094e-08
Iter: 999 loss: 8.95951047e-08
Iter: 1000 loss: 9.01996913e-08
Iter: 1001 loss: 8.9563116e-08
Iter: 1002 loss: 8.94245602e-08
Iter: 1003 loss: 8.96714383e-08
Iter: 1004 loss: 8.93605687e-08
Iter: 1005 loss: 8.92707845e-08
Iter: 1006 loss: 8.92688234e-08
Iter: 1007 loss: 8.91679548e-08
Iter: 1008 loss: 8.91050504e-08
Iter: 1009 loss: 8.9076039e-08
Iter: 1010 loss: 8.89435938e-08
Iter: 1011 loss: 8.90502108e-08
Iter: 1012 loss: 8.88679921e-08
Iter: 1013 loss: 8.8784617e-08
Iter: 1014 loss: 8.87778e-08
Iter: 1015 loss: 8.87147493e-08
Iter: 1016 loss: 8.85955842e-08
Iter: 1017 loss: 9.06828888e-08
Iter: 1018 loss: 8.85979148e-08
Iter: 1019 loss: 8.85120528e-08
Iter: 1020 loss: 8.85120102e-08
Iter: 1021 loss: 8.84345255e-08
Iter: 1022 loss: 8.83436684e-08
Iter: 1023 loss: 8.8330971e-08
Iter: 1024 loss: 8.82658355e-08
Iter: 1025 loss: 8.82673064e-08
Iter: 1026 loss: 8.8189509e-08
Iter: 1027 loss: 8.80588e-08
Iter: 1028 loss: 9.07507385e-08
Iter: 1029 loss: 8.80610571e-08
Iter: 1030 loss: 8.79171154e-08
Iter: 1031 loss: 8.79458142e-08
Iter: 1032 loss: 8.78172131e-08
Iter: 1033 loss: 8.76597142e-08
Iter: 1034 loss: 8.82082105e-08
Iter: 1035 loss: 8.76209754e-08
Iter: 1036 loss: 8.74774742e-08
Iter: 1037 loss: 8.92457521e-08
Iter: 1038 loss: 8.74759891e-08
Iter: 1039 loss: 8.7381288e-08
Iter: 1040 loss: 8.77476154e-08
Iter: 1041 loss: 8.73594317e-08
Iter: 1042 loss: 8.72564527e-08
Iter: 1043 loss: 8.71343815e-08
Iter: 1044 loss: 8.71199859e-08
Iter: 1045 loss: 8.70128858e-08
Iter: 1046 loss: 8.80229436e-08
Iter: 1047 loss: 8.70108963e-08
Iter: 1048 loss: 8.68937633e-08
Iter: 1049 loss: 8.73497399e-08
Iter: 1050 loss: 8.68746923e-08
Iter: 1051 loss: 8.67879635e-08
Iter: 1052 loss: 8.66657217e-08
Iter: 1053 loss: 8.66624674e-08
Iter: 1054 loss: 8.65747651e-08
Iter: 1055 loss: 8.65626362e-08
Iter: 1056 loss: 8.65064322e-08
Iter: 1057 loss: 8.63597549e-08
Iter: 1058 loss: 8.76092514e-08
Iter: 1059 loss: 8.63414371e-08
Iter: 1060 loss: 8.62824336e-08
Iter: 1061 loss: 8.6254083e-08
Iter: 1062 loss: 8.61737419e-08
Iter: 1063 loss: 8.60527578e-08
Iter: 1064 loss: 8.60495746e-08
Iter: 1065 loss: 8.59297273e-08
Iter: 1066 loss: 8.58583888e-08
Iter: 1067 loss: 8.58101785e-08
Iter: 1068 loss: 8.56410196e-08
Iter: 1069 loss: 8.6404e-08
Iter: 1070 loss: 8.56172306e-08
Iter: 1071 loss: 8.54903774e-08
Iter: 1072 loss: 8.70202399e-08
Iter: 1073 loss: 8.54854e-08
Iter: 1074 loss: 8.53909725e-08
Iter: 1075 loss: 8.55751e-08
Iter: 1076 loss: 8.5353669e-08
Iter: 1077 loss: 8.52294448e-08
Iter: 1078 loss: 8.52465263e-08
Iter: 1079 loss: 8.5140492e-08
Iter: 1080 loss: 8.5025718e-08
Iter: 1081 loss: 8.55196873e-08
Iter: 1082 loss: 8.50052e-08
Iter: 1083 loss: 8.48845332e-08
Iter: 1084 loss: 8.57293614e-08
Iter: 1085 loss: 8.48822665e-08
Iter: 1086 loss: 8.48167119e-08
Iter: 1087 loss: 8.47348716e-08
Iter: 1088 loss: 8.47286898e-08
Iter: 1089 loss: 8.46952304e-08
Iter: 1090 loss: 8.46718251e-08
Iter: 1091 loss: 8.46329158e-08
Iter: 1092 loss: 8.45280042e-08
Iter: 1093 loss: 8.55085887e-08
Iter: 1094 loss: 8.45112709e-08
Iter: 1095 loss: 8.44572696e-08
Iter: 1096 loss: 8.44501926e-08
Iter: 1097 loss: 8.43825916e-08
Iter: 1098 loss: 8.42999412e-08
Iter: 1099 loss: 8.42857e-08
Iter: 1100 loss: 8.41814227e-08
Iter: 1101 loss: 8.4062961e-08
Iter: 1102 loss: 8.40416448e-08
Iter: 1103 loss: 8.38408e-08
Iter: 1104 loss: 8.50164596e-08
Iter: 1105 loss: 8.38192591e-08
Iter: 1106 loss: 8.38257819e-08
Iter: 1107 loss: 8.37298e-08
Iter: 1108 loss: 8.36794811e-08
Iter: 1109 loss: 8.36487715e-08
Iter: 1110 loss: 8.36265457e-08
Iter: 1111 loss: 8.35371736e-08
Iter: 1112 loss: 8.35232612e-08
Iter: 1113 loss: 8.34720808e-08
Iter: 1114 loss: 8.33647817e-08
Iter: 1115 loss: 8.41886e-08
Iter: 1116 loss: 8.3355431e-08
Iter: 1117 loss: 8.32674445e-08
Iter: 1118 loss: 8.40071692e-08
Iter: 1119 loss: 8.32557134e-08
Iter: 1120 loss: 8.31976266e-08
Iter: 1121 loss: 8.30726066e-08
Iter: 1122 loss: 8.50649684e-08
Iter: 1123 loss: 8.3063469e-08
Iter: 1124 loss: 8.30390405e-08
Iter: 1125 loss: 8.30122673e-08
Iter: 1126 loss: 8.29470537e-08
Iter: 1127 loss: 8.28687803e-08
Iter: 1128 loss: 8.28667908e-08
Iter: 1129 loss: 8.27899456e-08
Iter: 1130 loss: 8.30158e-08
Iter: 1131 loss: 8.27709741e-08
Iter: 1132 loss: 8.26773743e-08
Iter: 1133 loss: 8.32030622e-08
Iter: 1134 loss: 8.26688407e-08
Iter: 1135 loss: 8.26093896e-08
Iter: 1136 loss: 8.24976425e-08
Iter: 1137 loss: 8.47341255e-08
Iter: 1138 loss: 8.24991e-08
Iter: 1139 loss: 8.23867339e-08
Iter: 1140 loss: 8.26969355e-08
Iter: 1141 loss: 8.23535942e-08
Iter: 1142 loss: 8.22406463e-08
Iter: 1143 loss: 8.22367525e-08
Iter: 1144 loss: 8.21806125e-08
Iter: 1145 loss: 8.20800707e-08
Iter: 1146 loss: 8.20807e-08
Iter: 1147 loss: 8.19810069e-08
Iter: 1148 loss: 8.27559461e-08
Iter: 1149 loss: 8.196659e-08
Iter: 1150 loss: 8.18821e-08
Iter: 1151 loss: 8.19580279e-08
Iter: 1152 loss: 8.1833619e-08
Iter: 1153 loss: 8.17452275e-08
Iter: 1154 loss: 8.31757347e-08
Iter: 1155 loss: 8.1751466e-08
Iter: 1156 loss: 8.17000085e-08
Iter: 1157 loss: 8.15964896e-08
Iter: 1158 loss: 8.30877269e-08
Iter: 1159 loss: 8.15956298e-08
Iter: 1160 loss: 8.1540577e-08
Iter: 1161 loss: 8.15326615e-08
Iter: 1162 loss: 8.14733312e-08
Iter: 1163 loss: 8.13894303e-08
Iter: 1164 loss: 8.13907377e-08
Iter: 1165 loss: 8.13037246e-08
Iter: 1166 loss: 8.1789814e-08
Iter: 1167 loss: 8.12981042e-08
Iter: 1168 loss: 8.11944503e-08
Iter: 1169 loss: 8.12439964e-08
Iter: 1170 loss: 8.11305938e-08
Iter: 1171 loss: 8.10522778e-08
Iter: 1172 loss: 8.10604419e-08
Iter: 1173 loss: 8.09861689e-08
Iter: 1174 loss: 8.0901934e-08
Iter: 1175 loss: 8.13293468e-08
Iter: 1176 loss: 8.08922351e-08
Iter: 1177 loss: 8.07748e-08
Iter: 1178 loss: 8.09671263e-08
Iter: 1179 loss: 8.07216125e-08
Iter: 1180 loss: 8.06326241e-08
Iter: 1181 loss: 8.05811453e-08
Iter: 1182 loss: 8.05463e-08
Iter: 1183 loss: 8.0424968e-08
Iter: 1184 loss: 8.13295e-08
Iter: 1185 loss: 8.04140114e-08
Iter: 1186 loss: 8.03135123e-08
Iter: 1187 loss: 8.05484603e-08
Iter: 1188 loss: 8.02829163e-08
Iter: 1189 loss: 8.01865241e-08
Iter: 1190 loss: 8.12698815e-08
Iter: 1191 loss: 8.01834759e-08
Iter: 1192 loss: 8.01354645e-08
Iter: 1193 loss: 8.00875242e-08
Iter: 1194 loss: 8.00719704e-08
Iter: 1195 loss: 7.99913877e-08
Iter: 1196 loss: 8.09722422e-08
Iter: 1197 loss: 7.99901727e-08
Iter: 1198 loss: 7.9936342e-08
Iter: 1199 loss: 7.98266697e-08
Iter: 1200 loss: 8.17155552e-08
Iter: 1201 loss: 7.98219162e-08
Iter: 1202 loss: 7.97436357e-08
Iter: 1203 loss: 7.97414828e-08
Iter: 1204 loss: 7.96873891e-08
Iter: 1205 loss: 7.95679682e-08
Iter: 1206 loss: 8.12536456e-08
Iter: 1207 loss: 7.95666892e-08
Iter: 1208 loss: 7.94510271e-08
Iter: 1209 loss: 7.95361217e-08
Iter: 1210 loss: 7.93767896e-08
Iter: 1211 loss: 7.93306327e-08
Iter: 1212 loss: 7.93135371e-08
Iter: 1213 loss: 7.92582853e-08
Iter: 1214 loss: 7.92147077e-08
Iter: 1215 loss: 7.9194578e-08
Iter: 1216 loss: 7.91231258e-08
Iter: 1217 loss: 7.9076159e-08
Iter: 1218 loss: 7.90492791e-08
Iter: 1219 loss: 7.89534909e-08
Iter: 1220 loss: 7.94821915e-08
Iter: 1221 loss: 7.89464139e-08
Iter: 1222 loss: 7.8866627e-08
Iter: 1223 loss: 7.91164609e-08
Iter: 1224 loss: 7.88360239e-08
Iter: 1225 loss: 7.87378269e-08
Iter: 1226 loss: 7.89819552e-08
Iter: 1227 loss: 7.86958125e-08
Iter: 1228 loss: 7.86327377e-08
Iter: 1229 loss: 7.9189924e-08
Iter: 1230 loss: 7.86270178e-08
Iter: 1231 loss: 7.85561838e-08
Iter: 1232 loss: 7.84568215e-08
Iter: 1233 loss: 7.845059e-08
Iter: 1234 loss: 7.83558818e-08
Iter: 1235 loss: 7.84903733e-08
Iter: 1236 loss: 7.83130503e-08
Iter: 1237 loss: 7.82519436e-08
Iter: 1238 loss: 7.82461456e-08
Iter: 1239 loss: 7.8207691e-08
Iter: 1240 loss: 7.81017491e-08
Iter: 1241 loss: 7.89430388e-08
Iter: 1242 loss: 7.80782443e-08
Iter: 1243 loss: 7.79972709e-08
Iter: 1244 loss: 7.88871262e-08
Iter: 1245 loss: 7.79880551e-08
Iter: 1246 loss: 7.78885862e-08
Iter: 1247 loss: 7.82977096e-08
Iter: 1248 loss: 7.78606264e-08
Iter: 1249 loss: 7.77748e-08
Iter: 1250 loss: 7.78277638e-08
Iter: 1251 loss: 7.77225466e-08
Iter: 1252 loss: 7.76221754e-08
Iter: 1253 loss: 7.79685649e-08
Iter: 1254 loss: 7.76042839e-08
Iter: 1255 loss: 7.7517484e-08
Iter: 1256 loss: 7.77937785e-08
Iter: 1257 loss: 7.74836622e-08
Iter: 1258 loss: 7.73899416e-08
Iter: 1259 loss: 7.76750397e-08
Iter: 1260 loss: 7.73672184e-08
Iter: 1261 loss: 7.72888669e-08
Iter: 1262 loss: 7.74049198e-08
Iter: 1263 loss: 7.72374449e-08
Iter: 1264 loss: 7.71648274e-08
Iter: 1265 loss: 7.83224294e-08
Iter: 1266 loss: 7.71658435e-08
Iter: 1267 loss: 7.71057103e-08
Iter: 1268 loss: 7.70112507e-08
Iter: 1269 loss: 7.90417829e-08
Iter: 1270 loss: 7.6999676e-08
Iter: 1271 loss: 7.69129898e-08
Iter: 1272 loss: 7.70895809e-08
Iter: 1273 loss: 7.68767237e-08
Iter: 1274 loss: 7.67817241e-08
Iter: 1275 loss: 7.67776811e-08
Iter: 1276 loss: 7.67263373e-08
Iter: 1277 loss: 7.66117267e-08
Iter: 1278 loss: 7.77361109e-08
Iter: 1279 loss: 7.66000596e-08
Iter: 1280 loss: 7.6508357e-08
Iter: 1281 loss: 7.65062325e-08
Iter: 1282 loss: 7.64223529e-08
Iter: 1283 loss: 7.65482895e-08
Iter: 1284 loss: 7.63783632e-08
Iter: 1285 loss: 7.63150112e-08
Iter: 1286 loss: 7.62148815e-08
Iter: 1287 loss: 7.62141212e-08
Iter: 1288 loss: 7.61037597e-08
Iter: 1289 loss: 7.61571215e-08
Iter: 1290 loss: 7.60331318e-08
Iter: 1291 loss: 7.59566063e-08
Iter: 1292 loss: 7.59588943e-08
Iter: 1293 loss: 7.59005516e-08
Iter: 1294 loss: 7.57947234e-08
Iter: 1295 loss: 7.81565603e-08
Iter: 1296 loss: 7.57892735e-08
Iter: 1297 loss: 7.57199672e-08
Iter: 1298 loss: 7.5718404e-08
Iter: 1299 loss: 7.56765957e-08
Iter: 1300 loss: 7.56394769e-08
Iter: 1301 loss: 7.56321e-08
Iter: 1302 loss: 7.55560663e-08
Iter: 1303 loss: 7.5653162e-08
Iter: 1304 loss: 7.55145777e-08
Iter: 1305 loss: 7.54237e-08
Iter: 1306 loss: 7.56211449e-08
Iter: 1307 loss: 7.53953131e-08
Iter: 1308 loss: 7.53300569e-08
Iter: 1309 loss: 7.53328209e-08
Iter: 1310 loss: 7.52791181e-08
Iter: 1311 loss: 7.51745e-08
Iter: 1312 loss: 7.65400188e-08
Iter: 1313 loss: 7.51741212e-08
Iter: 1314 loss: 7.50537481e-08
Iter: 1315 loss: 7.5244138e-08
Iter: 1316 loss: 7.50013527e-08
Iter: 1317 loss: 7.49827933e-08
Iter: 1318 loss: 7.49473656e-08
Iter: 1319 loss: 7.49091598e-08
Iter: 1320 loss: 7.48535243e-08
Iter: 1321 loss: 7.48544551e-08
Iter: 1322 loss: 7.47849782e-08
Iter: 1323 loss: 7.47421254e-08
Iter: 1324 loss: 7.47161124e-08
Iter: 1325 loss: 7.46033848e-08
Iter: 1326 loss: 7.55799903e-08
Iter: 1327 loss: 7.46015729e-08
Iter: 1328 loss: 7.45204645e-08
Iter: 1329 loss: 7.55107e-08
Iter: 1330 loss: 7.45134088e-08
Iter: 1331 loss: 7.44550803e-08
Iter: 1332 loss: 7.43809565e-08
Iter: 1333 loss: 7.43743698e-08
Iter: 1334 loss: 7.43341104e-08
Iter: 1335 loss: 7.43267776e-08
Iter: 1336 loss: 7.42828306e-08
Iter: 1337 loss: 7.42294759e-08
Iter: 1338 loss: 7.4223145e-08
Iter: 1339 loss: 7.41598569e-08
Iter: 1340 loss: 7.44101385e-08
Iter: 1341 loss: 7.41373896e-08
Iter: 1342 loss: 7.40932933e-08
Iter: 1343 loss: 7.46704671e-08
Iter: 1344 loss: 7.40855768e-08
Iter: 1345 loss: 7.40435e-08
Iter: 1346 loss: 7.39573522e-08
Iter: 1347 loss: 7.58164802e-08
Iter: 1348 loss: 7.39612958e-08
Iter: 1349 loss: 7.38807842e-08
Iter: 1350 loss: 7.38502592e-08
Iter: 1351 loss: 7.37971746e-08
Iter: 1352 loss: 7.37401606e-08
Iter: 1353 loss: 7.3725964e-08
Iter: 1354 loss: 7.36609351e-08
Iter: 1355 loss: 7.36935917e-08
Iter: 1356 loss: 7.36072394e-08
Iter: 1357 loss: 7.35332364e-08
Iter: 1358 loss: 7.34219157e-08
Iter: 1359 loss: 7.34217451e-08
Iter: 1360 loss: 7.33160732e-08
Iter: 1361 loss: 7.42620259e-08
Iter: 1362 loss: 7.33103036e-08
Iter: 1363 loss: 7.32521812e-08
Iter: 1364 loss: 7.32542418e-08
Iter: 1365 loss: 7.3202628e-08
Iter: 1366 loss: 7.30758529e-08
Iter: 1367 loss: 7.48425535e-08
Iter: 1368 loss: 7.3073366e-08
Iter: 1369 loss: 7.30053955e-08
Iter: 1370 loss: 7.30023686e-08
Iter: 1371 loss: 7.29365155e-08
Iter: 1372 loss: 7.29471168e-08
Iter: 1373 loss: 7.28877652e-08
Iter: 1374 loss: 7.28069693e-08
Iter: 1375 loss: 7.28384464e-08
Iter: 1376 loss: 7.27589367e-08
Iter: 1377 loss: 7.26755047e-08
Iter: 1378 loss: 7.37691437e-08
Iter: 1379 loss: 7.26763432e-08
Iter: 1380 loss: 7.26081453e-08
Iter: 1381 loss: 7.27144567e-08
Iter: 1382 loss: 7.25798657e-08
Iter: 1383 loss: 7.25313285e-08
Iter: 1384 loss: 7.2460665e-08
Iter: 1385 loss: 7.24563591e-08
Iter: 1386 loss: 7.23758689e-08
Iter: 1387 loss: 7.30233722e-08
Iter: 1388 loss: 7.2374938e-08
Iter: 1389 loss: 7.22912077e-08
Iter: 1390 loss: 7.25564391e-08
Iter: 1391 loss: 7.22587288e-08
Iter: 1392 loss: 7.2202532e-08
Iter: 1393 loss: 7.2089577e-08
Iter: 1394 loss: 7.20875093e-08
Iter: 1395 loss: 7.19827113e-08
Iter: 1396 loss: 7.25851734e-08
Iter: 1397 loss: 7.19631856e-08
Iter: 1398 loss: 7.19137461e-08
Iter: 1399 loss: 7.1907138e-08
Iter: 1400 loss: 7.18674329e-08
Iter: 1401 loss: 7.17891808e-08
Iter: 1402 loss: 7.30394873e-08
Iter: 1403 loss: 7.17869e-08
Iter: 1404 loss: 7.17324724e-08
Iter: 1405 loss: 7.17260775e-08
Iter: 1406 loss: 7.16835657e-08
Iter: 1407 loss: 7.16179329e-08
Iter: 1408 loss: 7.16138e-08
Iter: 1409 loss: 7.15446404e-08
Iter: 1410 loss: 7.16841058e-08
Iter: 1411 loss: 7.15115931e-08
Iter: 1412 loss: 7.14374409e-08
Iter: 1413 loss: 7.14381869e-08
Iter: 1414 loss: 7.13949788e-08
Iter: 1415 loss: 7.13739396e-08
Iter: 1416 loss: 7.13588264e-08
Iter: 1417 loss: 7.13055925e-08
Iter: 1418 loss: 7.14161956e-08
Iter: 1419 loss: 7.12851644e-08
Iter: 1420 loss: 7.12301897e-08
Iter: 1421 loss: 7.14835e-08
Iter: 1422 loss: 7.12176202e-08
Iter: 1423 loss: 7.11387145e-08
Iter: 1424 loss: 7.11493158e-08
Iter: 1425 loss: 7.10838535e-08
Iter: 1426 loss: 7.10283388e-08
Iter: 1427 loss: 7.10402119e-08
Iter: 1428 loss: 7.09803487e-08
Iter: 1429 loss: 7.09105947e-08
Iter: 1430 loss: 7.12793877e-08
Iter: 1431 loss: 7.09036883e-08
Iter: 1432 loss: 7.08154815e-08
Iter: 1433 loss: 7.14479143e-08
Iter: 1434 loss: 7.08068626e-08
Iter: 1435 loss: 7.07746892e-08
Iter: 1436 loss: 7.07026615e-08
Iter: 1437 loss: 7.2208671e-08
Iter: 1438 loss: 7.07045302e-08
Iter: 1439 loss: 7.0657876e-08
Iter: 1440 loss: 7.06448731e-08
Iter: 1441 loss: 7.06164371e-08
Iter: 1442 loss: 7.05378227e-08
Iter: 1443 loss: 7.11784693e-08
Iter: 1444 loss: 7.05184959e-08
Iter: 1445 loss: 7.0486422e-08
Iter: 1446 loss: 7.04735257e-08
Iter: 1447 loss: 7.04379488e-08
Iter: 1448 loss: 7.03895608e-08
Iter: 1449 loss: 7.03852123e-08
Iter: 1450 loss: 7.03178884e-08
Iter: 1451 loss: 7.02279479e-08
Iter: 1452 loss: 7.02279124e-08
Iter: 1453 loss: 7.01258784e-08
Iter: 1454 loss: 7.05023098e-08
Iter: 1455 loss: 7.01010734e-08
Iter: 1456 loss: 7.00978e-08
Iter: 1457 loss: 7.00634359e-08
Iter: 1458 loss: 7.00343321e-08
Iter: 1459 loss: 6.99552913e-08
Iter: 1460 loss: 7.04028764e-08
Iter: 1461 loss: 6.99350693e-08
Iter: 1462 loss: 6.98434235e-08
Iter: 1463 loss: 6.98459388e-08
Iter: 1464 loss: 6.97840221e-08
Iter: 1465 loss: 7.00375722e-08
Iter: 1466 loss: 6.97765827e-08
Iter: 1467 loss: 6.97054929e-08
Iter: 1468 loss: 6.99316445e-08
Iter: 1469 loss: 6.96852211e-08
Iter: 1470 loss: 6.96377924e-08
Iter: 1471 loss: 6.96772418e-08
Iter: 1472 loss: 6.96145079e-08
Iter: 1473 loss: 6.95508078e-08
Iter: 1474 loss: 7.0063507e-08
Iter: 1475 loss: 6.95509641e-08
Iter: 1476 loss: 6.95145701e-08
Iter: 1477 loss: 6.95387783e-08
Iter: 1478 loss: 6.9497581e-08
Iter: 1479 loss: 6.94473e-08
Iter: 1480 loss: 6.95193e-08
Iter: 1481 loss: 6.94218e-08
Iter: 1482 loss: 6.93837663e-08
Iter: 1483 loss: 6.93146944e-08
Iter: 1484 loss: 7.08230417e-08
Iter: 1485 loss: 6.93107651e-08
Iter: 1486 loss: 6.92207891e-08
Iter: 1487 loss: 6.96028835e-08
Iter: 1488 loss: 6.91979523e-08
Iter: 1489 loss: 6.9126969e-08
Iter: 1490 loss: 7.00102945e-08
Iter: 1491 loss: 6.9128518e-08
Iter: 1492 loss: 6.90579469e-08
Iter: 1493 loss: 6.92057e-08
Iter: 1494 loss: 6.90310742e-08
Iter: 1495 loss: 6.89834394e-08
Iter: 1496 loss: 6.89541935e-08
Iter: 1497 loss: 6.89266528e-08
Iter: 1498 loss: 6.88744421e-08
Iter: 1499 loss: 6.95379825e-08
Iter: 1500 loss: 6.88722679e-08
Iter: 1501 loss: 6.88360515e-08
Iter: 1502 loss: 6.89486086e-08
Iter: 1503 loss: 6.88233328e-08
Iter: 1504 loss: 6.87826187e-08
Iter: 1505 loss: 6.8850639e-08
Iter: 1506 loss: 6.87478661e-08
Iter: 1507 loss: 6.87102e-08
Iter: 1508 loss: 6.89966271e-08
Iter: 1509 loss: 6.8717128e-08
Iter: 1510 loss: 6.8677366e-08
Iter: 1511 loss: 6.86152646e-08
Iter: 1512 loss: 6.86151438e-08
Iter: 1513 loss: 6.85604959e-08
Iter: 1514 loss: 6.85581867e-08
Iter: 1515 loss: 6.85091521e-08
Iter: 1516 loss: 6.8430019e-08
Iter: 1517 loss: 6.84278874e-08
Iter: 1518 loss: 6.83513548e-08
Iter: 1519 loss: 6.83185064e-08
Iter: 1520 loss: 6.82792916e-08
Iter: 1521 loss: 6.81897916e-08
Iter: 1522 loss: 6.84184442e-08
Iter: 1523 loss: 6.81631889e-08
Iter: 1524 loss: 6.80916088e-08
Iter: 1525 loss: 6.80913885e-08
Iter: 1526 loss: 6.80409329e-08
Iter: 1527 loss: 6.83866119e-08
Iter: 1528 loss: 6.80286192e-08
Iter: 1529 loss: 6.79811123e-08
Iter: 1530 loss: 6.7924e-08
Iter: 1531 loss: 6.7917405e-08
Iter: 1532 loss: 6.78831071e-08
Iter: 1533 loss: 6.78737067e-08
Iter: 1534 loss: 6.78254253e-08
Iter: 1535 loss: 6.78652867e-08
Iter: 1536 loss: 6.7804713e-08
Iter: 1537 loss: 6.77576679e-08
Iter: 1538 loss: 6.78005563e-08
Iter: 1539 loss: 6.77264396e-08
Iter: 1540 loss: 6.76798209e-08
Iter: 1541 loss: 6.8285928e-08
Iter: 1542 loss: 6.76814054e-08
Iter: 1543 loss: 6.76441374e-08
Iter: 1544 loss: 6.75979095e-08
Iter: 1545 loss: 6.75949252e-08
Iter: 1546 loss: 6.75396663e-08
Iter: 1547 loss: 6.82664592e-08
Iter: 1548 loss: 6.75442493e-08
Iter: 1549 loss: 6.75033931e-08
Iter: 1550 loss: 6.74414835e-08
Iter: 1551 loss: 6.7437071e-08
Iter: 1552 loss: 6.73741809e-08
Iter: 1553 loss: 6.73459937e-08
Iter: 1554 loss: 6.73139766e-08
Iter: 1555 loss: 6.72215634e-08
Iter: 1556 loss: 6.76351846e-08
Iter: 1557 loss: 6.71979947e-08
Iter: 1558 loss: 6.71251286e-08
Iter: 1559 loss: 6.72435192e-08
Iter: 1560 loss: 6.70832776e-08
Iter: 1561 loss: 6.70323601e-08
Iter: 1562 loss: 6.79619276e-08
Iter: 1563 loss: 6.70298732e-08
Iter: 1564 loss: 6.69699e-08
Iter: 1565 loss: 6.71742129e-08
Iter: 1566 loss: 6.69575257e-08
Iter: 1567 loss: 6.69054501e-08
Iter: 1568 loss: 6.68006592e-08
Iter: 1569 loss: 6.82180286e-08
Iter: 1570 loss: 6.67942714e-08
Iter: 1571 loss: 6.67062707e-08
Iter: 1572 loss: 6.77422847e-08
Iter: 1573 loss: 6.67047857e-08
Iter: 1574 loss: 6.66348328e-08
Iter: 1575 loss: 6.77087684e-08
Iter: 1576 loss: 6.66337456e-08
Iter: 1577 loss: 6.66015936e-08
Iter: 1578 loss: 6.65553372e-08
Iter: 1579 loss: 6.7752957e-08
Iter: 1580 loss: 6.65508111e-08
Iter: 1581 loss: 6.65137208e-08
Iter: 1582 loss: 6.65055779e-08
Iter: 1583 loss: 6.64750957e-08
Iter: 1584 loss: 6.64308146e-08
Iter: 1585 loss: 6.6430843e-08
Iter: 1586 loss: 6.63732607e-08
Iter: 1587 loss: 6.6477817e-08
Iter: 1588 loss: 6.63438e-08
Iter: 1589 loss: 6.62824675e-08
Iter: 1590 loss: 6.68781e-08
Iter: 1591 loss: 6.6279938e-08
Iter: 1592 loss: 6.62261925e-08
Iter: 1593 loss: 6.62610802e-08
Iter: 1594 loss: 6.61939197e-08
Iter: 1595 loss: 6.61477628e-08
Iter: 1596 loss: 6.60865638e-08
Iter: 1597 loss: 6.6081391e-08
Iter: 1598 loss: 6.59949464e-08
Iter: 1599 loss: 6.62726194e-08
Iter: 1600 loss: 6.59686634e-08
Iter: 1601 loss: 6.58968347e-08
Iter: 1602 loss: 6.62162165e-08
Iter: 1603 loss: 6.58809185e-08
Iter: 1604 loss: 6.5837412e-08
Iter: 1605 loss: 6.58331061e-08
Iter: 1606 loss: 6.57889814e-08
Iter: 1607 loss: 6.57052084e-08
Iter: 1608 loss: 6.57028068e-08
Iter: 1609 loss: 6.56627748e-08
Iter: 1610 loss: 6.56632935e-08
Iter: 1611 loss: 6.56037287e-08
Iter: 1612 loss: 6.56138468e-08
Iter: 1613 loss: 6.55683223e-08
Iter: 1614 loss: 6.55302514e-08
Iter: 1615 loss: 6.56745129e-08
Iter: 1616 loss: 6.55173551e-08
Iter: 1617 loss: 6.5461407e-08
Iter: 1618 loss: 6.56476757e-08
Iter: 1619 loss: 6.54529089e-08
Iter: 1620 loss: 6.54198544e-08
Iter: 1621 loss: 6.53599699e-08
Iter: 1622 loss: 6.53606449e-08
Iter: 1623 loss: 6.53021246e-08
Iter: 1624 loss: 6.5301812e-08
Iter: 1625 loss: 6.52580638e-08
Iter: 1626 loss: 6.52459917e-08
Iter: 1627 loss: 6.52205685e-08
Iter: 1628 loss: 6.51562218e-08
Iter: 1629 loss: 6.51476455e-08
Iter: 1630 loss: 6.51013e-08
Iter: 1631 loss: 6.50411209e-08
Iter: 1632 loss: 6.51007639e-08
Iter: 1633 loss: 6.50081446e-08
Iter: 1634 loss: 6.49523e-08
Iter: 1635 loss: 6.56930865e-08
Iter: 1636 loss: 6.49517702e-08
Iter: 1637 loss: 6.49001137e-08
Iter: 1638 loss: 6.50522e-08
Iter: 1639 loss: 6.48846381e-08
Iter: 1640 loss: 6.48438601e-08
Iter: 1641 loss: 6.49142891e-08
Iter: 1642 loss: 6.48312479e-08
Iter: 1643 loss: 6.47837055e-08
Iter: 1644 loss: 6.49658674e-08
Iter: 1645 loss: 6.47713776e-08
Iter: 1646 loss: 6.47350404e-08
Iter: 1647 loss: 6.47055316e-08
Iter: 1648 loss: 6.46924576e-08
Iter: 1649 loss: 6.46410641e-08
Iter: 1650 loss: 6.46466489e-08
Iter: 1651 loss: 6.46051319e-08
Iter: 1652 loss: 6.45461142e-08
Iter: 1653 loss: 6.45470664e-08
Iter: 1654 loss: 6.45038298e-08
Iter: 1655 loss: 6.49153478e-08
Iter: 1656 loss: 6.44998437e-08
Iter: 1657 loss: 6.44466738e-08
Iter: 1658 loss: 6.45681908e-08
Iter: 1659 loss: 6.44228919e-08
Iter: 1660 loss: 6.4383805e-08
Iter: 1661 loss: 6.44280291e-08
Iter: 1662 loss: 6.43608473e-08
Iter: 1663 loss: 6.43095319e-08
Iter: 1664 loss: 6.42822329e-08
Iter: 1665 loss: 6.42589342e-08
Iter: 1666 loss: 6.4184988e-08
Iter: 1667 loss: 6.4354893e-08
Iter: 1668 loss: 6.41735483e-08
Iter: 1669 loss: 6.40927595e-08
Iter: 1670 loss: 6.49073399e-08
Iter: 1671 loss: 6.40884252e-08
Iter: 1672 loss: 6.4037792e-08
Iter: 1673 loss: 6.40972786e-08
Iter: 1674 loss: 6.40098534e-08
Iter: 1675 loss: 6.39680806e-08
Iter: 1676 loss: 6.45784297e-08
Iter: 1677 loss: 6.39681446e-08
Iter: 1678 loss: 6.39449e-08
Iter: 1679 loss: 6.3904082e-08
Iter: 1680 loss: 6.3908594e-08
Iter: 1681 loss: 6.38702176e-08
Iter: 1682 loss: 6.3869166e-08
Iter: 1683 loss: 6.38345483e-08
Iter: 1684 loss: 6.38160316e-08
Iter: 1685 loss: 6.37934434e-08
Iter: 1686 loss: 6.37449205e-08
Iter: 1687 loss: 6.36904147e-08
Iter: 1688 loss: 6.36845456e-08
Iter: 1689 loss: 6.36385096e-08
Iter: 1690 loss: 6.36298267e-08
Iter: 1691 loss: 6.35923669e-08
Iter: 1692 loss: 6.35308197e-08
Iter: 1693 loss: 6.3527807e-08
Iter: 1694 loss: 6.34577e-08
Iter: 1695 loss: 6.35739639e-08
Iter: 1696 loss: 6.3426441e-08
Iter: 1697 loss: 6.3346242e-08
Iter: 1698 loss: 6.34928909e-08
Iter: 1699 loss: 6.33078798e-08
Iter: 1700 loss: 6.32551291e-08
Iter: 1701 loss: 6.32570334e-08
Iter: 1702 loss: 6.32083541e-08
Iter: 1703 loss: 6.35074429e-08
Iter: 1704 loss: 6.32042614e-08
Iter: 1705 loss: 6.31696935e-08
Iter: 1706 loss: 6.32202557e-08
Iter: 1707 loss: 6.31502886e-08
Iter: 1708 loss: 6.31079e-08
Iter: 1709 loss: 6.31622683e-08
Iter: 1710 loss: 6.30853592e-08
Iter: 1711 loss: 6.30478638e-08
Iter: 1712 loss: 6.30482333e-08
Iter: 1713 loss: 6.30265191e-08
Iter: 1714 loss: 6.29713739e-08
Iter: 1715 loss: 6.3989539e-08
Iter: 1716 loss: 6.29710826e-08
Iter: 1717 loss: 6.29506118e-08
Iter: 1718 loss: 6.29461425e-08
Iter: 1719 loss: 6.29152e-08
Iter: 1720 loss: 6.28638688e-08
Iter: 1721 loss: 6.41289901e-08
Iter: 1722 loss: 6.28633927e-08
Iter: 1723 loss: 6.28214423e-08
Iter: 1724 loss: 6.28209e-08
Iter: 1725 loss: 6.27785113e-08
Iter: 1726 loss: 6.27459258e-08
Iter: 1727 loss: 6.27266914e-08
Iter: 1728 loss: 6.26750634e-08
Iter: 1729 loss: 6.2722826e-08
Iter: 1730 loss: 6.26447445e-08
Iter: 1731 loss: 6.25959871e-08
Iter: 1732 loss: 6.25881711e-08
Iter: 1733 loss: 6.2563366e-08
Iter: 1734 loss: 6.25140615e-08
Iter: 1735 loss: 6.2510658e-08
Iter: 1736 loss: 6.24700149e-08
Iter: 1737 loss: 6.26248919e-08
Iter: 1738 loss: 6.24696455e-08
Iter: 1739 loss: 6.24328251e-08
Iter: 1740 loss: 6.2413136e-08
Iter: 1741 loss: 6.23974046e-08
Iter: 1742 loss: 6.23482208e-08
Iter: 1743 loss: 6.30179073e-08
Iter: 1744 loss: 6.23503382e-08
Iter: 1745 loss: 6.2310896e-08
Iter: 1746 loss: 6.22626857e-08
Iter: 1747 loss: 6.22634246e-08
Iter: 1748 loss: 6.2213914e-08
Iter: 1749 loss: 6.24961345e-08
Iter: 1750 loss: 6.22091605e-08
Iter: 1751 loss: 6.21664213e-08
Iter: 1752 loss: 6.25455456e-08
Iter: 1753 loss: 6.21647587e-08
Iter: 1754 loss: 6.21359248e-08
Iter: 1755 loss: 6.2089029e-08
Iter: 1756 loss: 6.319938e-08
Iter: 1757 loss: 6.20880627e-08
Iter: 1758 loss: 6.20482723e-08
Iter: 1759 loss: 6.20495e-08
Iter: 1760 loss: 6.20258831e-08
Iter: 1761 loss: 6.19899652e-08
Iter: 1762 loss: 6.28099386e-08
Iter: 1763 loss: 6.19840677e-08
Iter: 1764 loss: 6.19284393e-08
Iter: 1765 loss: 6.1978028e-08
Iter: 1766 loss: 6.18960243e-08
Iter: 1767 loss: 6.18192644e-08
Iter: 1768 loss: 6.20162766e-08
Iter: 1769 loss: 6.17955251e-08
Iter: 1770 loss: 6.17404652e-08
Iter: 1771 loss: 6.1739307e-08
Iter: 1772 loss: 6.16989055e-08
Iter: 1773 loss: 6.1716392e-08
Iter: 1774 loss: 6.16750313e-08
Iter: 1775 loss: 6.16442719e-08
Iter: 1776 loss: 6.20652472e-08
Iter: 1777 loss: 6.16444495e-08
Iter: 1778 loss: 6.16112e-08
Iter: 1779 loss: 6.16510079e-08
Iter: 1780 loss: 6.1590363e-08
Iter: 1781 loss: 6.15609324e-08
Iter: 1782 loss: 6.15151166e-08
Iter: 1783 loss: 6.15166e-08
Iter: 1784 loss: 6.14767686e-08
Iter: 1785 loss: 6.14762712e-08
Iter: 1786 loss: 6.14279e-08
Iter: 1787 loss: 6.13834e-08
Iter: 1788 loss: 6.13741449e-08
Iter: 1789 loss: 6.13273201e-08
Iter: 1790 loss: 6.19449736e-08
Iter: 1791 loss: 6.13276612e-08
Iter: 1792 loss: 6.12818596e-08
Iter: 1793 loss: 6.12356885e-08
Iter: 1794 loss: 6.12351556e-08
Iter: 1795 loss: 6.11741768e-08
Iter: 1796 loss: 6.15448243e-08
Iter: 1797 loss: 6.11691036e-08
Iter: 1798 loss: 6.112937e-08
Iter: 1799 loss: 6.10853661e-08
Iter: 1800 loss: 6.10813871e-08
Iter: 1801 loss: 6.10118533e-08
Iter: 1802 loss: 6.13385609e-08
Iter: 1803 loss: 6.10005344e-08
Iter: 1804 loss: 6.09610069e-08
Iter: 1805 loss: 6.09627264e-08
Iter: 1806 loss: 6.09297e-08
Iter: 1807 loss: 6.08617654e-08
Iter: 1808 loss: 6.19744469e-08
Iter: 1809 loss: 6.08590653e-08
Iter: 1810 loss: 6.08241706e-08
Iter: 1811 loss: 6.08201915e-08
Iter: 1812 loss: 6.0782881e-08
Iter: 1813 loss: 6.07376052e-08
Iter: 1814 loss: 6.0736e-08
Iter: 1815 loss: 6.06947737e-08
Iter: 1816 loss: 6.08333295e-08
Iter: 1817 loss: 6.06757311e-08
Iter: 1818 loss: 6.06420656e-08
Iter: 1819 loss: 6.06377384e-08
Iter: 1820 loss: 6.06087127e-08
Iter: 1821 loss: 6.05802271e-08
Iter: 1822 loss: 6.05766814e-08
Iter: 1823 loss: 6.05430799e-08
Iter: 1824 loss: 6.05731572e-08
Iter: 1825 loss: 6.05199943e-08
Iter: 1826 loss: 6.04856751e-08
Iter: 1827 loss: 6.04805734e-08
Iter: 1828 loss: 6.04571e-08
Iter: 1829 loss: 6.04173351e-08
Iter: 1830 loss: 6.10369781e-08
Iter: 1831 loss: 6.04165535e-08
Iter: 1832 loss: 6.03838e-08
Iter: 1833 loss: 6.03507146e-08
Iter: 1834 loss: 6.03475172e-08
Iter: 1835 loss: 6.03066894e-08
Iter: 1836 loss: 6.04350561e-08
Iter: 1837 loss: 6.0295946e-08
Iter: 1838 loss: 6.02588059e-08
Iter: 1839 loss: 6.061736e-08
Iter: 1840 loss: 6.02627637e-08
Iter: 1841 loss: 6.02280892e-08
Iter: 1842 loss: 6.02187455e-08
Iter: 1843 loss: 6.01990138e-08
Iter: 1844 loss: 6.01652559e-08
Iter: 1845 loss: 6.01502634e-08
Iter: 1846 loss: 6.01304464e-08
Iter: 1847 loss: 6.00942442e-08
Iter: 1848 loss: 6.00970651e-08
Iter: 1849 loss: 6.00632646e-08
Iter: 1850 loss: 6.00296701e-08
Iter: 1851 loss: 6.00233534e-08
Iter: 1852 loss: 5.99783e-08
Iter: 1853 loss: 6.00583121e-08
Iter: 1854 loss: 5.99567898e-08
Iter: 1855 loss: 5.99129635e-08
Iter: 1856 loss: 6.00635843e-08
Iter: 1857 loss: 5.99114927e-08
Iter: 1858 loss: 5.98752052e-08
Iter: 1859 loss: 5.99638952e-08
Iter: 1860 loss: 5.98654708e-08
Iter: 1861 loss: 5.98200458e-08
Iter: 1862 loss: 6.01164345e-08
Iter: 1863 loss: 5.98087695e-08
Iter: 1864 loss: 5.9786565e-08
Iter: 1865 loss: 5.97527e-08
Iter: 1866 loss: 5.97546688e-08
Iter: 1867 loss: 5.97221259e-08
Iter: 1868 loss: 5.971863e-08
Iter: 1869 loss: 5.969261e-08
Iter: 1870 loss: 5.96679826e-08
Iter: 1871 loss: 5.96637193e-08
Iter: 1872 loss: 5.96170224e-08
Iter: 1873 loss: 5.96724874e-08
Iter: 1874 loss: 5.96000191e-08
Iter: 1875 loss: 5.95685812e-08
Iter: 1876 loss: 5.95703504e-08
Iter: 1877 loss: 5.9550711e-08
Iter: 1878 loss: 5.95279168e-08
Iter: 1879 loss: 5.95268581e-08
Iter: 1880 loss: 5.94986638e-08
Iter: 1881 loss: 5.95834919e-08
Iter: 1882 loss: 5.94892526e-08
Iter: 1883 loss: 5.94548126e-08
Iter: 1884 loss: 5.97272845e-08
Iter: 1885 loss: 5.94540808e-08
Iter: 1886 loss: 5.94345622e-08
Iter: 1887 loss: 5.94003353e-08
Iter: 1888 loss: 6.02848473e-08
Iter: 1889 loss: 5.94013088e-08
Iter: 1890 loss: 5.93572302e-08
Iter: 1891 loss: 5.94395111e-08
Iter: 1892 loss: 5.93316649e-08
Iter: 1893 loss: 5.92908052e-08
Iter: 1894 loss: 5.96150826e-08
Iter: 1895 loss: 5.92883609e-08
Iter: 1896 loss: 5.92502367e-08
Iter: 1897 loss: 5.95900929e-08
Iter: 1898 loss: 5.9249178e-08
Iter: 1899 loss: 5.92327396e-08
Iter: 1900 loss: 5.92104641e-08
Iter: 1901 loss: 5.92077143e-08
Iter: 1902 loss: 5.91907323e-08
Iter: 1903 loss: 5.91895031e-08
Iter: 1904 loss: 5.91710183e-08
Iter: 1905 loss: 5.91569922e-08
Iter: 1906 loss: 5.91500822e-08
Iter: 1907 loss: 5.91181504e-08
Iter: 1908 loss: 5.91746456e-08
Iter: 1909 loss: 5.91079576e-08
Iter: 1910 loss: 5.90743e-08
Iter: 1911 loss: 5.94154592e-08
Iter: 1912 loss: 5.90736455e-08
Iter: 1913 loss: 5.90454476e-08
Iter: 1914 loss: 5.90139848e-08
Iter: 1915 loss: 5.90115654e-08
Iter: 1916 loss: 5.89718141e-08
Iter: 1917 loss: 5.92950542e-08
Iter: 1918 loss: 5.89649183e-08
Iter: 1919 loss: 5.89267266e-08
Iter: 1920 loss: 5.89863127e-08
Iter: 1921 loss: 5.89027493e-08
Iter: 1922 loss: 5.88677906e-08
Iter: 1923 loss: 5.88604578e-08
Iter: 1924 loss: 5.88395679e-08
Iter: 1925 loss: 5.87997135e-08
Iter: 1926 loss: 5.89656537e-08
Iter: 1927 loss: 5.87902313e-08
Iter: 1928 loss: 5.87438436e-08
Iter: 1929 loss: 5.90158642e-08
Iter: 1930 loss: 5.87425362e-08
Iter: 1931 loss: 5.86967204e-08
Iter: 1932 loss: 5.87133329e-08
Iter: 1933 loss: 5.86630264e-08
Iter: 1934 loss: 5.8625659e-08
Iter: 1935 loss: 5.87179052e-08
Iter: 1936 loss: 5.86122404e-08
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script63
+ '[' -r STOP.script63 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi1.2
+ date
Tue Oct 20 16:44:59 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.8/300_300_300_1 --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 100000 --batch_size 5000 --max_epochs 1000 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b82e50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b0608950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b0608d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b82afd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b05d22f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b05ccd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b059a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b04327b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b0432e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b059a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b01aa950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b010f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b010f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b02cb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b02ed400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20600ccc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20600da620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20600dab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f206007c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f206007cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20600922f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2060092ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20600926a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f206003e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f206003ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b0167510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b00c5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b04e8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b04e8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b04e8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b0259488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b02b68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b02b6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b029c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b038f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f20b0383d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
train_loss: 0.0055964915
test_loss: 0.005490208
train_loss: 0.004583944
test_loss: 0.0046435357
train_loss: 0.0045531075
test_loss: 0.0046103443
train_loss: 0.004211535
test_loss: 0.0042396258
train_loss: 0.0043571587
test_loss: 0.004126166
train_loss: 0.004036898
test_loss: 0.0041435263
train_loss: 0.0036901522
test_loss: 0.0037826872
train_loss: 0.003989183
test_loss: 0.004115932
train_loss: 0.003799289
test_loss: 0.0038426784
train_loss: 0.0037028468
test_loss: 0.0037031865
train_loss: 0.0035404353
test_loss: 0.003832817
train_loss: 0.0035292946
test_loss: 0.0037534547
train_loss: 0.0035782657
test_loss: 0.0037155796
train_loss: 0.0034713435
test_loss: 0.0035761874
train_loss: 0.003370726
test_loss: 0.0035859668
train_loss: 0.0033858938
test_loss: 0.003549532
train_loss: 0.003419438
test_loss: 0.0035582245
train_loss: 0.0033226167
test_loss: 0.0034317558
train_loss: 0.0031846883
test_loss: 0.003335634
train_loss: 0.0033997574
test_loss: 0.0034263874
train_loss: 0.0031699678
test_loss: 0.0033331027
train_loss: 0.003373811
test_loss: 0.003406168
train_loss: 0.003328469
test_loss: 0.0034741457
train_loss: 0.0031801448
test_loss: 0.003255847
train_loss: 0.003134214
test_loss: 0.0032036158
train_loss: 0.0032140084
test_loss: 0.0032092012
train_loss: 0.0034906147
test_loss: 0.003444904
train_loss: 0.0033331919
test_loss: 0.0033924468
train_loss: 0.0030433517
test_loss: 0.0033940866
train_loss: 0.003211202
test_loss: 0.00324775
train_loss: 0.003041098
test_loss: 0.003185805
train_loss: 0.0029804297
test_loss: 0.003034608
train_loss: 0.0031665973
test_loss: 0.003243209
train_loss: 0.0032337878
test_loss: 0.0031483679
train_loss: 0.0031614772
test_loss: 0.0031812135
train_loss: 0.0033172753
test_loss: 0.003142014
train_loss: 0.002853536
test_loss: 0.0031639102
train_loss: 0.00286244
test_loss: 0.0029659134
train_loss: 0.0029199119
test_loss: 0.0030531697
train_loss: 0.003138152
test_loss: 0.0030952957
train_loss: 0.00290145
test_loss: 0.0030762402
train_loss: 0.0032047667
test_loss: 0.0032420475
train_loss: 0.0029798755
test_loss: 0.003149427
train_loss: 0.0030463822
test_loss: 0.0030005674
train_loss: 0.0029667679
test_loss: 0.0029022524
train_loss: 0.0031905025
test_loss: 0.0031934637
train_loss: 0.0029008435
test_loss: 0.0030134427
train_loss: 0.0030948552
test_loss: 0.0032179013
train_loss: 0.0028607207
test_loss: 0.003029392
train_loss: 0.0031118607
test_loss: 0.003191373
train_loss: 0.0029409176
test_loss: 0.003129753
train_loss: 0.0028315452
test_loss: 0.0028805672
train_loss: 0.0029792413
test_loss: 0.0030161815
train_loss: 0.0027836873
test_loss: 0.0028901035
train_loss: 0.0028628246
test_loss: 0.0028878956
train_loss: 0.0027453876
test_loss: 0.0029585245
train_loss: 0.0028037957
test_loss: 0.0030437382
train_loss: 0.0028496506
test_loss: 0.002959474
train_loss: 0.0027216407
test_loss: 0.0030014827
train_loss: 0.0029543012
test_loss: 0.0031453208
train_loss: 0.0029176315
test_loss: 0.0029834993
train_loss: 0.002926524
test_loss: 0.0029684068
train_loss: 0.0029576914
test_loss: 0.0029007734
train_loss: 0.0027603046
test_loss: 0.0029837785
train_loss: 0.0028380307
test_loss: 0.0028875868
train_loss: 0.0027333836
test_loss: 0.0029276921
train_loss: 0.002794886
test_loss: 0.0028982456
train_loss: 0.0029872688
test_loss: 0.0032294798
train_loss: 0.0025500786
test_loss: 0.0026641942
train_loss: 0.0027363263
test_loss: 0.002813839
train_loss: 0.002693723
test_loss: 0.0028216918
train_loss: 0.0029721083
test_loss: 0.0029191764
train_loss: 0.0029422306
test_loss: 0.0030800446
train_loss: 0.0027210629
test_loss: 0.002793419
train_loss: 0.0027128072
test_loss: 0.0028248762
train_loss: 0.002885023
test_loss: 0.0027641226
train_loss: 0.0026900596
test_loss: 0.0028420708
train_loss: 0.0026367912
test_loss: 0.0027227625
train_loss: 0.0028266641
test_loss: 0.0027422025
train_loss: 0.0026527494
test_loss: 0.0027304017
train_loss: 0.002794668
test_loss: 0.002790941
train_loss: 0.002732121
test_loss: 0.0029014447
train_loss: 0.0027455718
test_loss: 0.0028206732
train_loss: 0.0029762636
test_loss: 0.002783249
train_loss: 0.0028354842
test_loss: 0.002854366
train_loss: 0.0027796743
test_loss: 0.0029833838
train_loss: 0.002725517
test_loss: 0.002802288
train_loss: 0.0026032198
test_loss: 0.0027960073
train_loss: 0.0027299155
test_loss: 0.0028416216
train_loss: 0.0027974022
test_loss: 0.0029565888
train_loss: 0.0025995853
test_loss: 0.0027430751
train_loss: 0.002583444
test_loss: 0.0027356225
train_loss: 0.0026909083
test_loss: 0.002741725
train_loss: 0.002765445
test_loss: 0.0028407201
train_loss: 0.0027019905
test_loss: 0.0029374217
train_loss: 0.002594728
test_loss: 0.0027146188
train_loss: 0.0028362595
test_loss: 0.0026693128
train_loss: 0.0028724624
test_loss: 0.0028570783
train_loss: 0.0025737793
test_loss: 0.0026018876
train_loss: 0.0024454235
test_loss: 0.0027146032
train_loss: 0.0026500782
test_loss: 0.002694632
train_loss: 0.0025835647
test_loss: 0.0025757763
train_loss: 0.00293225
test_loss: 0.0029393085
train_loss: 0.0026534663
test_loss: 0.0028304972
train_loss: 0.002764917
test_loss: 0.0027352693
train_loss: 0.0027654537
test_loss: 0.0027176335
train_loss: 0.002947066
test_loss: 0.0030325719
train_loss: 0.002548152
test_loss: 0.0028479502
train_loss: 0.0024909917
test_loss: 0.0026775054
train_loss: 0.0025464755
test_loss: 0.0027664427
train_loss: 0.002579344
test_loss: 0.0026453326
train_loss: 0.002919174
test_loss: 0.002738136
train_loss: 0.002485935
test_loss: 0.0026058166
train_loss: 0.002546073
test_loss: 0.0031023596
train_loss: 0.0025511389
test_loss: 0.0025545915
train_loss: 0.0026006636
test_loss: 0.0026559306
train_loss: 0.002653877
test_loss: 0.0026522328
train_loss: 0.00265573
test_loss: 0.0028359964
train_loss: 0.0024095976
test_loss: 0.0026580899
train_loss: 0.0025153575
test_loss: 0.0025213105
train_loss: 0.0027711187
test_loss: 0.0027293847
train_loss: 0.002709738
test_loss: 0.0026392895
train_loss: 0.0025747519
test_loss: 0.0026498481
train_loss: 0.0026254167
test_loss: 0.0027085964
train_loss: 0.0025694524
test_loss: 0.0025478706
train_loss: 0.0027168428
test_loss: 0.0027603277
train_loss: 0.0026495636
test_loss: 0.0027615232
train_loss: 0.0024635934
test_loss: 0.002528206
train_loss: 0.0026567283
test_loss: 0.0027297402
train_loss: 0.002664668
test_loss: 0.0026915122
train_loss: 0.002531066
test_loss: 0.002742292
train_loss: 0.002764369
test_loss: 0.0026347092
train_loss: 0.002510647
test_loss: 0.0025224797
train_loss: 0.0025280747
test_loss: 0.0026069032
train_loss: 0.0025164397
test_loss: 0.0027185786
train_loss: 0.0024955426
test_loss: 0.0025671895
train_loss: 0.0025279163
test_loss: 0.0027202212
train_loss: 0.0023983363
test_loss: 0.0024655804
train_loss: 0.0025529207
test_loss: 0.0024831991
train_loss: 0.002586334
test_loss: 0.0027604557
train_loss: 0.002516351
test_loss: 0.0026701156
train_loss: 0.00262507
test_loss: 0.0026646336
train_loss: 0.002586159
test_loss: 0.0025850465
train_loss: 0.0026768325
test_loss: 0.0026226626
train_loss: 0.0027674204
test_loss: 0.0025900125
train_loss: 0.0025162208
test_loss: 0.0025946053
train_loss: 0.0026650294
test_loss: 0.0026735456
train_loss: 0.002579064
test_loss: 0.0026895069
train_loss: 0.0025913492
test_loss: 0.0025872434
train_loss: 0.0025825135
test_loss: 0.00264081
train_loss: 0.002685003
test_loss: 0.002526692
train_loss: 0.0023358993
test_loss: 0.0024728791
train_loss: 0.0025224523
test_loss: 0.0025178082
train_loss: 0.0026464495
test_loss: 0.002528467
train_loss: 0.0026317872
test_loss: 0.0024343263
train_loss: 0.0025625823
test_loss: 0.0025177186
train_loss: 0.0027009165
test_loss: 0.0025422531
train_loss: 0.0024747844
test_loss: 0.00270998
train_loss: 0.0023624357
test_loss: 0.0024799802
train_loss: 0.0026177396
test_loss: 0.0024682314
train_loss: 0.0027950942
test_loss: 0.0027439443
train_loss: 0.002398308
test_loss: 0.0026674373
train_loss: 0.0025632381
test_loss: 0.0024927438
train_loss: 0.0025485465
test_loss: 0.0028596986
train_loss: 0.0026344527
test_loss: 0.0026168055
train_loss: 0.0023370541
test_loss: 0.002503861
train_loss: 0.0023926762
test_loss: 0.002463142
train_loss: 0.0024758377
test_loss: 0.0026609811
train_loss: 0.0024735297
test_loss: 0.0025981136
train_loss: