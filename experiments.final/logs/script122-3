+ RUN=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ LAYERS=300_100_100_100_1
+ case $RUN in
+ PSI='2 3'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 800 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output120
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output122
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.final/output120/f1_psi0_phi0/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi0
+ date
Sun Nov  8 14:31:08 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0/300_100_100_100_1 ']'
+ LOAD='--load_model experiments.final/output120/f1_psi0_phi0/300_100_100_100_1'
+ python biholoNN_train.py --seed 1234 --load_model experiments.final/output120/f1_psi0_phi0/300_100_100_100_1 --function f1 --psi 2 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a194ef6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a195078c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a194ef730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a1951ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a19499048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a19499840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a193c6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a194992f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a193aa378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a192dd950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a192dd268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a19367ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a19367950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59f561f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a19393598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a193936a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a19393ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59f55b8bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a193937b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a193599d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a19359bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a19359730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5a19359f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59d05a9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59d05a9ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59d0552378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59d05a91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59d061abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59d0552840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59f5657158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59f56570d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59d043f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59d043f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59d04e5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59d04ff9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f59d0512e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.030694786
test_loss: 0.030770868
train_loss: 0.030450346
test_loss: 0.030104872
train_loss: 0.028789464
test_loss: 0.029787436
train_loss: 0.027752582
test_loss: 0.029384444
train_loss: 0.027432041
test_loss: 0.029361794
train_loss: 0.026577672
test_loss: 0.027520368
train_loss: 0.02352716
test_loss: 0.025527531
train_loss: 0.020212164
test_loss: 0.022042677
train_loss: 0.015482239
test_loss: 0.016829906
train_loss: 0.012003943
test_loss: 0.012608623
train_loss: 0.009382555
test_loss: 0.0098314015
train_loss: 0.008091881
test_loss: 0.008143171
train_loss: 0.007163005
test_loss: 0.006931133
train_loss: 0.006570556
test_loss: 0.0065050046
train_loss: 0.006142449
test_loss: 0.0060796337
train_loss: 0.0055937273
test_loss: 0.0057004183
train_loss: 0.005355349
test_loss: 0.0051998645
train_loss: 0.005549427
test_loss: 0.0060853786
train_loss: 0.005123049
test_loss: 0.005153845
train_loss: 0.004879865
test_loss: 0.0053962655
train_loss: 0.004952943
test_loss: 0.0048607574
train_loss: 0.0049921833
test_loss: 0.0049427142
train_loss: 0.004704866
test_loss: 0.0049835644
train_loss: 0.0044910694
test_loss: 0.0046419213
train_loss: 0.004225779
test_loss: 0.0044260044
train_loss: 0.0044535315
test_loss: 0.0043387245
train_loss: 0.0047983658
test_loss: 0.0048427093
train_loss: 0.004201
test_loss: 0.0045699105
train_loss: 0.004331606
test_loss: 0.0043925233
train_loss: 0.0043902737
test_loss: 0.0046134116
train_loss: 0.0040288544
test_loss: 0.004437836
train_loss: 0.004243242
test_loss: 0.004413452
train_loss: 0.0040447707
test_loss: 0.0043020365
train_loss: 0.0040699895
test_loss: 0.004397353
train_loss: 0.0042241635
test_loss: 0.00466865
train_loss: 0.0043106144
test_loss: 0.0048068115
train_loss: 0.0042552245
test_loss: 0.0045000343
train_loss: 0.0041147037
test_loss: 0.004440429
train_loss: 0.0037009143
test_loss: 0.0039183707
train_loss: 0.0046281945
test_loss: 0.0040435432
train_loss: 0.0037913946
test_loss: 0.0039111045
train_loss: 0.0042327195
test_loss: 0.004245528
train_loss: 0.0036118818
test_loss: 0.0041296207
train_loss: 0.0037841897
test_loss: 0.0044043814
train_loss: 0.004004238
test_loss: 0.004202523
train_loss: 0.0039254283
test_loss: 0.0039879167
train_loss: 0.0036576334
test_loss: 0.0036529056
train_loss: 0.003929858
test_loss: 0.003964357
train_loss: 0.004171553
test_loss: 0.004039455
train_loss: 0.0036940146
test_loss: 0.0040715467
train_loss: 0.0037888708
test_loss: 0.0038773227
train_loss: 0.0039334246
test_loss: 0.0038887642
train_loss: 0.0040217545
test_loss: 0.0039605093
train_loss: 0.0037435335
test_loss: 0.0037713302
train_loss: 0.0037952203
test_loss: 0.0037735617
train_loss: 0.0034406944
test_loss: 0.003516566
train_loss: 0.0037135796
test_loss: 0.0041522216
train_loss: 0.0036083218
test_loss: 0.003588321
train_loss: 0.00378619
test_loss: 0.003822537
train_loss: 0.0037515904
test_loss: 0.003873891
train_loss: 0.004020853
test_loss: 0.0037831126
train_loss: 0.0037152283
test_loss: 0.003973667
train_loss: 0.0037867525
test_loss: 0.0039798236
train_loss: 0.0037143065
test_loss: 0.0036702303
train_loss: 0.0036527403
test_loss: 0.003637205
train_loss: 0.003509018
test_loss: 0.003692594
train_loss: 0.0035365557
test_loss: 0.0035667191
train_loss: 0.0035474945
test_loss: 0.0035912122
train_loss: 0.0037630028
test_loss: 0.0037008924
train_loss: 0.004103588
test_loss: 0.0038211027
train_loss: 0.004196028
test_loss: 0.0038396385
train_loss: 0.0037162788
test_loss: 0.003635418
train_loss: 0.003587109
test_loss: 0.0039692055
train_loss: 0.0034424262
test_loss: 0.003622381
train_loss: 0.0038402332
test_loss: 0.0035996295
train_loss: 0.0035764223
test_loss: 0.0037143894
train_loss: 0.0036033657
test_loss: 0.0036136562
train_loss: 0.003499101
test_loss: 0.0036934752
train_loss: 0.0038379268
test_loss: 0.0038970946
train_loss: 0.0034620631
test_loss: 0.0037452609
train_loss: 0.0039323876
test_loss: 0.0040058387
train_loss: 0.0035047946
test_loss: 0.0035887402
train_loss: 0.0033478434
test_loss: 0.0039809644
train_loss: 0.0032307706
test_loss: 0.0033666745
train_loss: 0.0035025673
test_loss: 0.003475779
train_loss: 0.0035190773
test_loss: 0.0034564293
train_loss: 0.0032380507
test_loss: 0.0034297844
train_loss: 0.0035939803
test_loss: 0.0036405348
train_loss: 0.0032419586
test_loss: 0.0034116963
train_loss: 0.003487151
test_loss: 0.0037477664
train_loss: 0.0035263747
test_loss: 0.0037511922
train_loss: 0.003474
test_loss: 0.00358923
train_loss: 0.003363482
test_loss: 0.0034942483
train_loss: 0.0034504903
test_loss: 0.003591093
train_loss: 0.003354511
test_loss: 0.0034702492
train_loss: 0.0035566487
test_loss: 0.0035790503
train_loss: 0.00343669
test_loss: 0.0036077746
train_loss: 0.0036360961
test_loss: 0.003769363
train_loss: 0.003290759
test_loss: 0.0033719197
train_loss: 0.0032192492
test_loss: 0.0035398463
train_loss: 0.0034171403
test_loss: 0.0035543903
train_loss: 0.003750972
test_loss: 0.003869722
train_loss: 0.003435673
test_loss: 0.0032408505
train_loss: 0.0032800527
test_loss: 0.0034619225
train_loss: 0.003505629
test_loss: 0.00373267
train_loss: 0.0032861792
test_loss: 0.0035198804
train_loss: 0.0033629523
test_loss: 0.0037324044
train_loss: 0.0031777876
test_loss: 0.0033894945
train_loss: 0.0035761632
test_loss: 0.003687918
train_loss: 0.0037110746
test_loss: 0.0038366602
train_loss: 0.0030504104
test_loss: 0.0031808035
train_loss: 0.0034149794
test_loss: 0.0034159606
train_loss: 0.0034414525
test_loss: 0.003527252
train_loss: 0.0034437957
test_loss: 0.0035146861
train_loss: 0.0032682016
test_loss: 0.0037001523
train_loss: 0.003393045
test_loss: 0.0035401604
train_loss: 0.0032871184
test_loss: 0.0035104407
train_loss: 0.0033101048
test_loss: 0.0035110598
train_loss: 0.0035947298
test_loss: 0.0032588614
train_loss: 0.003439832
test_loss: 0.003554695
train_loss: 0.0032516713
test_loss: 0.0033040596
train_loss: 0.0032994435
test_loss: 0.0032940765
train_loss: 0.0031184014
test_loss: 0.0033826139
train_loss: 0.0035294278
test_loss: 0.0034872189
train_loss: 0.0033442571
test_loss: 0.0035464864
train_loss: 0.0032496562
test_loss: 0.003787768
train_loss: 0.0030409112
test_loss: 0.0032769446
train_loss: 0.0031465886
test_loss: 0.0037463629
train_loss: 0.0032832925
test_loss: 0.0032254932
train_loss: 0.003122172
test_loss: 0.0033481594
train_loss: 0.003218813
test_loss: 0.0034050734
train_loss: 0.003177279
test_loss: 0.0032912125
train_loss: 0.0031966786
test_loss: 0.0033852754
train_loss: 0.0032461362
test_loss: 0.0033311218
train_loss: 0.0031478112
test_loss: 0.0033434501
train_loss: 0.0030533846
test_loss: 0.0034895414
train_loss: 0.0031401634
test_loss: 0.0032328786
train_loss: 0.0032334365
test_loss: 0.0031395806
train_loss: 0.0031553553
test_loss: 0.0032676836
train_loss: 0.003072509
test_loss: 0.0032111914
train_loss: 0.00306933
test_loss: 0.0032417753
train_loss: 0.0033170988
test_loss: 0.00314627
train_loss: 0.003190849
test_loss: 0.003533136
train_loss: 0.002999543
test_loss: 0.0031784896
train_loss: 0.003210616
test_loss: 0.0031882217
train_loss: 0.0032869985
test_loss: 0.0035887994
train_loss: 0.0029602356
test_loss: 0.0032541614
train_loss: 0.0035679664
test_loss: 0.0035097268
train_loss: 0.0032458769
test_loss: 0.0034113058
train_loss: 0.0030962843
test_loss: 0.0032093837
train_loss: 0.0028101082
test_loss: 0.0030908184
train_loss: 0.0031236103
test_loss: 0.0031684942
train_loss: 0.0030476025
test_loss: 0.0031770212
train_loss: 0.002909139
test_loss: 0.0031308956
train_loss: 0.0031297659
test_loss: 0.0034724958
train_loss: 0.0032589207
test_loss: 0.0031927805
train_loss: 0.0030648988
test_loss: 0.0032796536
train_loss: 0.002998469
test_loss: 0.0032104761
train_loss: 0.00295958
test_loss: 0.0030619397
train_loss: 0.003129902
test_loss: 0.003382984
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi0/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 2 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi0/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdaca13a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdaca0688c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdaca0686a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdaca0a3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdaca0d9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdaca0d9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdaca01db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdaca01d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdaca0d98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9fb7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdaca0d9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9f99048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9f90e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9f542f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9f49d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9ea9d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9ebb510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9ebb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9e8d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9e8d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9e4f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9e077b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9db56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdac9dcbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdab53c1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdab53c1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdab5374b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdab53c1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdab533d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdab533d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdab5359510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdab52c3840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdab533dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdab5289c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdab52cd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdab523e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.70655567e-05
Iter: 2 loss: 1.52935645e-05
Iter: 3 loss: 1.32769437e-05
Iter: 4 loss: 1.16676438e-05
Iter: 5 loss: 1.41978935e-05
Iter: 6 loss: 1.09174e-05
Iter: 7 loss: 9.81827543e-06
Iter: 8 loss: 1.78547962e-05
Iter: 9 loss: 9.724351e-06
Iter: 10 loss: 9.14399607e-06
Iter: 11 loss: 1.07678179e-05
Iter: 12 loss: 8.95774065e-06
Iter: 13 loss: 8.26131873e-06
Iter: 14 loss: 1.05069403e-05
Iter: 15 loss: 8.06486878e-06
Iter: 16 loss: 7.87593672e-06
Iter: 17 loss: 8.76407466e-06
Iter: 18 loss: 7.8412213e-06
Iter: 19 loss: 7.57000635e-06
Iter: 20 loss: 7.0245128e-06
Iter: 21 loss: 1.72880355e-05
Iter: 22 loss: 7.01696808e-06
Iter: 23 loss: 6.60567366e-06
Iter: 24 loss: 7.39156121e-06
Iter: 25 loss: 6.43257636e-06
Iter: 26 loss: 6.35749302e-06
Iter: 27 loss: 6.27955e-06
Iter: 28 loss: 6.1069768e-06
Iter: 29 loss: 5.73295893e-06
Iter: 30 loss: 1.1434583e-05
Iter: 31 loss: 5.7183197e-06
Iter: 32 loss: 5.39559733e-06
Iter: 33 loss: 5.6061358e-06
Iter: 34 loss: 5.19157129e-06
Iter: 35 loss: 4.85828696e-06
Iter: 36 loss: 5.89269257e-06
Iter: 37 loss: 4.76094419e-06
Iter: 38 loss: 4.49017898e-06
Iter: 39 loss: 5.74711157e-06
Iter: 40 loss: 4.43963381e-06
Iter: 41 loss: 4.454193e-06
Iter: 42 loss: 4.33372361e-06
Iter: 43 loss: 4.28805743e-06
Iter: 44 loss: 4.17507181e-06
Iter: 45 loss: 5.28260807e-06
Iter: 46 loss: 4.16037437e-06
Iter: 47 loss: 4.02633486e-06
Iter: 48 loss: 5.99937539e-06
Iter: 49 loss: 4.02613296e-06
Iter: 50 loss: 3.94689414e-06
Iter: 51 loss: 4.19868684e-06
Iter: 52 loss: 3.92420861e-06
Iter: 53 loss: 3.84985788e-06
Iter: 54 loss: 3.90833975e-06
Iter: 55 loss: 3.80507549e-06
Iter: 56 loss: 3.76966136e-06
Iter: 57 loss: 3.76888397e-06
Iter: 58 loss: 3.73843432e-06
Iter: 59 loss: 3.64737866e-06
Iter: 60 loss: 3.92685342e-06
Iter: 61 loss: 3.60203967e-06
Iter: 62 loss: 3.49556763e-06
Iter: 63 loss: 4.08689175e-06
Iter: 64 loss: 3.48031608e-06
Iter: 65 loss: 3.46549132e-06
Iter: 66 loss: 3.43547731e-06
Iter: 67 loss: 3.40571773e-06
Iter: 68 loss: 3.32196646e-06
Iter: 69 loss: 3.75912123e-06
Iter: 70 loss: 3.29523891e-06
Iter: 71 loss: 3.21606808e-06
Iter: 72 loss: 3.64919242e-06
Iter: 73 loss: 3.20446907e-06
Iter: 74 loss: 3.1491054e-06
Iter: 75 loss: 3.24060693e-06
Iter: 76 loss: 3.12393877e-06
Iter: 77 loss: 3.08471e-06
Iter: 78 loss: 3.0771912e-06
Iter: 79 loss: 3.0569463e-06
Iter: 80 loss: 3.02289118e-06
Iter: 81 loss: 3.02285684e-06
Iter: 82 loss: 3.00256488e-06
Iter: 83 loss: 2.99817953e-06
Iter: 84 loss: 2.98490886e-06
Iter: 85 loss: 2.96480539e-06
Iter: 86 loss: 2.9643993e-06
Iter: 87 loss: 2.92807772e-06
Iter: 88 loss: 2.98704617e-06
Iter: 89 loss: 2.91143988e-06
Iter: 90 loss: 2.88929232e-06
Iter: 91 loss: 2.88931915e-06
Iter: 92 loss: 2.87465764e-06
Iter: 93 loss: 2.83752479e-06
Iter: 94 loss: 3.16729847e-06
Iter: 95 loss: 2.83175359e-06
Iter: 96 loss: 2.78957759e-06
Iter: 97 loss: 2.8316465e-06
Iter: 98 loss: 2.76582409e-06
Iter: 99 loss: 2.75260345e-06
Iter: 100 loss: 2.74678268e-06
Iter: 101 loss: 2.72580223e-06
Iter: 102 loss: 2.75940829e-06
Iter: 103 loss: 2.71606723e-06
Iter: 104 loss: 2.70011606e-06
Iter: 105 loss: 2.66001553e-06
Iter: 106 loss: 3.02995522e-06
Iter: 107 loss: 2.65418521e-06
Iter: 108 loss: 2.62168078e-06
Iter: 109 loss: 3.01006116e-06
Iter: 110 loss: 2.62124854e-06
Iter: 111 loss: 2.59811486e-06
Iter: 112 loss: 2.61221339e-06
Iter: 113 loss: 2.58316481e-06
Iter: 114 loss: 2.60491743e-06
Iter: 115 loss: 2.57341298e-06
Iter: 116 loss: 2.56768953e-06
Iter: 117 loss: 2.55349e-06
Iter: 118 loss: 2.69672682e-06
Iter: 119 loss: 2.55178111e-06
Iter: 120 loss: 2.53942426e-06
Iter: 121 loss: 2.53897042e-06
Iter: 122 loss: 2.53096732e-06
Iter: 123 loss: 2.51511733e-06
Iter: 124 loss: 2.82327483e-06
Iter: 125 loss: 2.51493748e-06
Iter: 126 loss: 2.50176868e-06
Iter: 127 loss: 2.50150015e-06
Iter: 128 loss: 2.49379309e-06
Iter: 129 loss: 2.50352468e-06
Iter: 130 loss: 2.48976085e-06
Iter: 131 loss: 2.47869821e-06
Iter: 132 loss: 2.48915967e-06
Iter: 133 loss: 2.47219e-06
Iter: 134 loss: 2.46146419e-06
Iter: 135 loss: 2.44164767e-06
Iter: 136 loss: 2.90368757e-06
Iter: 137 loss: 2.44166768e-06
Iter: 138 loss: 2.439014e-06
Iter: 139 loss: 2.43307295e-06
Iter: 140 loss: 2.42362739e-06
Iter: 141 loss: 2.42259898e-06
Iter: 142 loss: 2.41570933e-06
Iter: 143 loss: 2.40691247e-06
Iter: 144 loss: 2.39107067e-06
Iter: 145 loss: 2.77103982e-06
Iter: 146 loss: 2.39112705e-06
Iter: 147 loss: 2.37512313e-06
Iter: 148 loss: 2.48656215e-06
Iter: 149 loss: 2.37364475e-06
Iter: 150 loss: 2.37687459e-06
Iter: 151 loss: 2.36867527e-06
Iter: 152 loss: 2.36529422e-06
Iter: 153 loss: 2.35542848e-06
Iter: 154 loss: 2.39152882e-06
Iter: 155 loss: 2.35103448e-06
Iter: 156 loss: 2.34943491e-06
Iter: 157 loss: 2.34450317e-06
Iter: 158 loss: 2.34102708e-06
Iter: 159 loss: 2.33327955e-06
Iter: 160 loss: 2.44132116e-06
Iter: 161 loss: 2.33269566e-06
Iter: 162 loss: 2.32648927e-06
Iter: 163 loss: 2.32631851e-06
Iter: 164 loss: 2.32090906e-06
Iter: 165 loss: 2.31360309e-06
Iter: 166 loss: 2.31326726e-06
Iter: 167 loss: 2.30390197e-06
Iter: 168 loss: 2.39162819e-06
Iter: 169 loss: 2.30355408e-06
Iter: 170 loss: 2.29871966e-06
Iter: 171 loss: 2.29031093e-06
Iter: 172 loss: 2.29035913e-06
Iter: 173 loss: 2.28523186e-06
Iter: 174 loss: 2.28464296e-06
Iter: 175 loss: 2.27820715e-06
Iter: 176 loss: 2.27183182e-06
Iter: 177 loss: 2.27048326e-06
Iter: 178 loss: 2.26343718e-06
Iter: 179 loss: 2.26141606e-06
Iter: 180 loss: 2.25715121e-06
Iter: 181 loss: 2.24664723e-06
Iter: 182 loss: 2.2851907e-06
Iter: 183 loss: 2.24409177e-06
Iter: 184 loss: 2.23794268e-06
Iter: 185 loss: 2.23661505e-06
Iter: 186 loss: 2.23413917e-06
Iter: 187 loss: 2.22777203e-06
Iter: 188 loss: 2.28179874e-06
Iter: 189 loss: 2.22663471e-06
Iter: 190 loss: 2.22308836e-06
Iter: 191 loss: 2.22184508e-06
Iter: 192 loss: 2.21943969e-06
Iter: 193 loss: 2.21222899e-06
Iter: 194 loss: 2.23541406e-06
Iter: 195 loss: 2.20877041e-06
Iter: 196 loss: 2.20913466e-06
Iter: 197 loss: 2.20475e-06
Iter: 198 loss: 2.2015754e-06
Iter: 199 loss: 2.20202173e-06
Iter: 200 loss: 2.19911112e-06
Iter: 201 loss: 2.19501521e-06
Iter: 202 loss: 2.20025777e-06
Iter: 203 loss: 2.19282947e-06
Iter: 204 loss: 2.18860555e-06
Iter: 205 loss: 2.183167e-06
Iter: 206 loss: 2.18278774e-06
Iter: 207 loss: 2.18440709e-06
Iter: 208 loss: 2.17963225e-06
Iter: 209 loss: 2.17792012e-06
Iter: 210 loss: 2.17359525e-06
Iter: 211 loss: 2.21157825e-06
Iter: 212 loss: 2.17278966e-06
Iter: 213 loss: 2.1648284e-06
Iter: 214 loss: 2.16464059e-06
Iter: 215 loss: 2.16251465e-06
Iter: 216 loss: 2.1604103e-06
Iter: 217 loss: 2.15861951e-06
Iter: 218 loss: 2.15480532e-06
Iter: 219 loss: 2.21753e-06
Iter: 220 loss: 2.15464843e-06
Iter: 221 loss: 2.15113823e-06
Iter: 222 loss: 2.15108548e-06
Iter: 223 loss: 2.14899774e-06
Iter: 224 loss: 2.14332886e-06
Iter: 225 loss: 2.18210812e-06
Iter: 226 loss: 2.14200281e-06
Iter: 227 loss: 2.13551402e-06
Iter: 228 loss: 2.13558314e-06
Iter: 229 loss: 2.13031399e-06
Iter: 230 loss: 2.12885743e-06
Iter: 231 loss: 2.12719397e-06
Iter: 232 loss: 2.12448549e-06
Iter: 233 loss: 2.12327109e-06
Iter: 234 loss: 2.12193959e-06
Iter: 235 loss: 2.11791985e-06
Iter: 236 loss: 2.11885481e-06
Iter: 237 loss: 2.11503789e-06
Iter: 238 loss: 2.11016368e-06
Iter: 239 loss: 2.1117819e-06
Iter: 240 loss: 2.10676649e-06
Iter: 241 loss: 2.10243184e-06
Iter: 242 loss: 2.15429372e-06
Iter: 243 loss: 2.10240864e-06
Iter: 244 loss: 2.0972534e-06
Iter: 245 loss: 2.10907911e-06
Iter: 246 loss: 2.09541804e-06
Iter: 247 loss: 2.09311293e-06
Iter: 248 loss: 2.09078144e-06
Iter: 249 loss: 2.0903035e-06
Iter: 250 loss: 2.08953588e-06
Iter: 251 loss: 2.0882776e-06
Iter: 252 loss: 2.08680149e-06
Iter: 253 loss: 2.08232586e-06
Iter: 254 loss: 2.09370364e-06
Iter: 255 loss: 2.07975381e-06
Iter: 256 loss: 2.08337633e-06
Iter: 257 loss: 2.07787798e-06
Iter: 258 loss: 2.07657354e-06
Iter: 259 loss: 2.07267203e-06
Iter: 260 loss: 2.08425126e-06
Iter: 261 loss: 2.07054427e-06
Iter: 262 loss: 2.06566529e-06
Iter: 263 loss: 2.06928485e-06
Iter: 264 loss: 2.06258528e-06
Iter: 265 loss: 2.05878155e-06
Iter: 266 loss: 2.07088897e-06
Iter: 267 loss: 2.05776041e-06
Iter: 268 loss: 2.0538364e-06
Iter: 269 loss: 2.05382685e-06
Iter: 270 loss: 2.05170818e-06
Iter: 271 loss: 2.04796083e-06
Iter: 272 loss: 2.14020247e-06
Iter: 273 loss: 2.04795833e-06
Iter: 274 loss: 2.04508729e-06
Iter: 275 loss: 2.04508797e-06
Iter: 276 loss: 2.04313051e-06
Iter: 277 loss: 2.04034268e-06
Iter: 278 loss: 2.04026719e-06
Iter: 279 loss: 2.03636273e-06
Iter: 280 loss: 2.08889332e-06
Iter: 281 loss: 2.03629679e-06
Iter: 282 loss: 2.03427749e-06
Iter: 283 loss: 2.03114769e-06
Iter: 284 loss: 2.03107174e-06
Iter: 285 loss: 2.02954834e-06
Iter: 286 loss: 2.02881756e-06
Iter: 287 loss: 2.02768092e-06
Iter: 288 loss: 2.0245102e-06
Iter: 289 loss: 2.03362015e-06
Iter: 290 loss: 2.02289903e-06
Iter: 291 loss: 2.02244064e-06
Iter: 292 loss: 2.02079241e-06
Iter: 293 loss: 2.01897092e-06
Iter: 294 loss: 2.01604939e-06
Iter: 295 loss: 2.01603757e-06
Iter: 296 loss: 2.01313173e-06
Iter: 297 loss: 2.03549371e-06
Iter: 298 loss: 2.0129e-06
Iter: 299 loss: 2.01124112e-06
Iter: 300 loss: 2.01114108e-06
Iter: 301 loss: 2.00931163e-06
Iter: 302 loss: 2.00608065e-06
Iter: 303 loss: 2.08454549e-06
Iter: 304 loss: 2.00609111e-06
Iter: 305 loss: 2.00338718e-06
Iter: 306 loss: 2.01503235e-06
Iter: 307 loss: 2.00275599e-06
Iter: 308 loss: 1.99948681e-06
Iter: 309 loss: 2.00847762e-06
Iter: 310 loss: 1.9983363e-06
Iter: 311 loss: 1.99610531e-06
Iter: 312 loss: 1.99915303e-06
Iter: 313 loss: 1.9949855e-06
Iter: 314 loss: 1.99212468e-06
Iter: 315 loss: 2.02191086e-06
Iter: 316 loss: 1.99211354e-06
Iter: 317 loss: 1.99062e-06
Iter: 318 loss: 1.98771909e-06
Iter: 319 loss: 2.04077355e-06
Iter: 320 loss: 1.98767884e-06
Iter: 321 loss: 1.9865638e-06
Iter: 322 loss: 1.98576572e-06
Iter: 323 loss: 1.98475709e-06
Iter: 324 loss: 1.98207454e-06
Iter: 325 loss: 1.99871e-06
Iter: 326 loss: 1.9813574e-06
Iter: 327 loss: 1.97987561e-06
Iter: 328 loss: 1.97929648e-06
Iter: 329 loss: 1.97829695e-06
Iter: 330 loss: 1.97577629e-06
Iter: 331 loss: 2.0023806e-06
Iter: 332 loss: 1.97553663e-06
Iter: 333 loss: 1.97383338e-06
Iter: 334 loss: 1.97377676e-06
Iter: 335 loss: 1.97264535e-06
Iter: 336 loss: 1.97063673e-06
Iter: 337 loss: 1.97060444e-06
Iter: 338 loss: 1.96711e-06
Iter: 339 loss: 1.98949374e-06
Iter: 340 loss: 1.96681322e-06
Iter: 341 loss: 1.9648453e-06
Iter: 342 loss: 1.96441033e-06
Iter: 343 loss: 1.96322617e-06
Iter: 344 loss: 1.96142128e-06
Iter: 345 loss: 1.96135284e-06
Iter: 346 loss: 1.96014366e-06
Iter: 347 loss: 1.95999496e-06
Iter: 348 loss: 1.95845314e-06
Iter: 349 loss: 1.95548546e-06
Iter: 350 loss: 2.01339935e-06
Iter: 351 loss: 1.95549046e-06
Iter: 352 loss: 1.95355528e-06
Iter: 353 loss: 1.9714505e-06
Iter: 354 loss: 1.95346593e-06
Iter: 355 loss: 1.95094071e-06
Iter: 356 loss: 1.95270445e-06
Iter: 357 loss: 1.94932295e-06
Iter: 358 loss: 1.94794711e-06
Iter: 359 loss: 1.95222515e-06
Iter: 360 loss: 1.94757149e-06
Iter: 361 loss: 1.94567929e-06
Iter: 362 loss: 1.94983022e-06
Iter: 363 loss: 1.94496943e-06
Iter: 364 loss: 1.9438487e-06
Iter: 365 loss: 1.94249424e-06
Iter: 366 loss: 1.94232962e-06
Iter: 367 loss: 1.94023687e-06
Iter: 368 loss: 1.96927294e-06
Iter: 369 loss: 1.94023687e-06
Iter: 370 loss: 1.93916821e-06
Iter: 371 loss: 1.93683445e-06
Iter: 372 loss: 1.96645578e-06
Iter: 373 loss: 1.93660117e-06
Iter: 374 loss: 1.93440587e-06
Iter: 375 loss: 1.95336e-06
Iter: 376 loss: 1.93432197e-06
Iter: 377 loss: 1.93197206e-06
Iter: 378 loss: 1.93779101e-06
Iter: 379 loss: 1.93113465e-06
Iter: 380 loss: 1.92948164e-06
Iter: 381 loss: 1.931827e-06
Iter: 382 loss: 1.92874768e-06
Iter: 383 loss: 1.92696643e-06
Iter: 384 loss: 1.92695779e-06
Iter: 385 loss: 1.92608354e-06
Iter: 386 loss: 1.92352422e-06
Iter: 387 loss: 1.93532469e-06
Iter: 388 loss: 1.92257858e-06
Iter: 389 loss: 1.92384778e-06
Iter: 390 loss: 1.92132734e-06
Iter: 391 loss: 1.92033917e-06
Iter: 392 loss: 1.91860818e-06
Iter: 393 loss: 1.91855543e-06
Iter: 394 loss: 1.91745221e-06
Iter: 395 loss: 1.91743334e-06
Iter: 396 loss: 1.91676327e-06
Iter: 397 loss: 1.91485583e-06
Iter: 398 loss: 1.92341531e-06
Iter: 399 loss: 1.91423442e-06
Iter: 400 loss: 1.9142476e-06
Iter: 401 loss: 1.91303252e-06
Iter: 402 loss: 1.91221488e-06
Iter: 403 loss: 1.9110139e-06
Iter: 404 loss: 1.91099434e-06
Iter: 405 loss: 1.90966739e-06
Iter: 406 loss: 1.90964738e-06
Iter: 407 loss: 1.9085528e-06
Iter: 408 loss: 1.90734022e-06
Iter: 409 loss: 1.90598053e-06
Iter: 410 loss: 1.90583773e-06
Iter: 411 loss: 1.90434173e-06
Iter: 412 loss: 1.91182971e-06
Iter: 413 loss: 1.90410867e-06
Iter: 414 loss: 1.90254241e-06
Iter: 415 loss: 1.90731771e-06
Iter: 416 loss: 1.90217634e-06
Iter: 417 loss: 1.9009442e-06
Iter: 418 loss: 1.90327796e-06
Iter: 419 loss: 1.90047626e-06
Iter: 420 loss: 1.89931063e-06
Iter: 421 loss: 1.90417745e-06
Iter: 422 loss: 1.89913294e-06
Iter: 423 loss: 1.89768821e-06
Iter: 424 loss: 1.90155367e-06
Iter: 425 loss: 1.89724938e-06
Iter: 426 loss: 1.89635637e-06
Iter: 427 loss: 1.89901891e-06
Iter: 428 loss: 1.89611478e-06
Iter: 429 loss: 1.89488105e-06
Iter: 430 loss: 1.89429829e-06
Iter: 431 loss: 1.89371633e-06
Iter: 432 loss: 1.89241814e-06
Iter: 433 loss: 1.89313334e-06
Iter: 434 loss: 1.89163927e-06
Iter: 435 loss: 1.88988554e-06
Iter: 436 loss: 1.9100537e-06
Iter: 437 loss: 1.88981983e-06
Iter: 438 loss: 1.88904892e-06
Iter: 439 loss: 1.88762351e-06
Iter: 440 loss: 1.91614208e-06
Iter: 441 loss: 1.88763602e-06
Iter: 442 loss: 1.88542174e-06
Iter: 443 loss: 1.90105288e-06
Iter: 444 loss: 1.88530953e-06
Iter: 445 loss: 1.88395279e-06
Iter: 446 loss: 1.88559989e-06
Iter: 447 loss: 1.88321701e-06
Iter: 448 loss: 1.88199226e-06
Iter: 449 loss: 1.8919709e-06
Iter: 450 loss: 1.88190256e-06
Iter: 451 loss: 1.88090087e-06
Iter: 452 loss: 1.88022966e-06
Iter: 453 loss: 1.87981618e-06
Iter: 454 loss: 1.87789578e-06
Iter: 455 loss: 1.88368892e-06
Iter: 456 loss: 1.87728108e-06
Iter: 457 loss: 1.87625756e-06
Iter: 458 loss: 1.87617013e-06
Iter: 459 loss: 1.87544356e-06
Iter: 460 loss: 1.87431601e-06
Iter: 461 loss: 1.8742619e-06
Iter: 462 loss: 1.87335127e-06
Iter: 463 loss: 1.87335615e-06
Iter: 464 loss: 1.87270348e-06
Iter: 465 loss: 1.8709942e-06
Iter: 466 loss: 1.88121942e-06
Iter: 467 loss: 1.87057913e-06
Iter: 468 loss: 1.87074579e-06
Iter: 469 loss: 1.86988143e-06
Iter: 470 loss: 1.86928833e-06
Iter: 471 loss: 1.868048e-06
Iter: 472 loss: 1.89416255e-06
Iter: 473 loss: 1.86805983e-06
Iter: 474 loss: 1.86684838e-06
Iter: 475 loss: 1.87007868e-06
Iter: 476 loss: 1.8664856e-06
Iter: 477 loss: 1.86488853e-06
Iter: 478 loss: 1.86575437e-06
Iter: 479 loss: 1.86388684e-06
Iter: 480 loss: 1.86268971e-06
Iter: 481 loss: 1.8728748e-06
Iter: 482 loss: 1.86260479e-06
Iter: 483 loss: 1.86146701e-06
Iter: 484 loss: 1.86070463e-06
Iter: 485 loss: 1.86031934e-06
Iter: 486 loss: 1.85893782e-06
Iter: 487 loss: 1.86584339e-06
Iter: 488 loss: 1.85872352e-06
Iter: 489 loss: 1.85782528e-06
Iter: 490 loss: 1.86940474e-06
Iter: 491 loss: 1.85783495e-06
Iter: 492 loss: 1.85707381e-06
Iter: 493 loss: 1.8596877e-06
Iter: 494 loss: 1.85684246e-06
Iter: 495 loss: 1.85618342e-06
Iter: 496 loss: 1.85520958e-06
Iter: 497 loss: 1.85517627e-06
Iter: 498 loss: 1.85430008e-06
Iter: 499 loss: 1.85423823e-06
Iter: 500 loss: 1.85386091e-06
Iter: 501 loss: 1.85286149e-06
Iter: 502 loss: 1.86176578e-06
Iter: 503 loss: 1.85276031e-06
Iter: 504 loss: 1.852178e-06
Iter: 505 loss: 1.85209171e-06
Iter: 506 loss: 1.85140868e-06
Iter: 507 loss: 1.85053375e-06
Iter: 508 loss: 1.85048452e-06
Iter: 509 loss: 1.84955866e-06
Iter: 510 loss: 1.85191857e-06
Iter: 511 loss: 1.84925671e-06
Iter: 512 loss: 1.84819442e-06
Iter: 513 loss: 1.85394128e-06
Iter: 514 loss: 1.84797887e-06
Iter: 515 loss: 1.84727264e-06
Iter: 516 loss: 1.84597343e-06
Iter: 517 loss: 1.87657406e-06
Iter: 518 loss: 1.84594717e-06
Iter: 519 loss: 1.84527403e-06
Iter: 520 loss: 1.84520115e-06
Iter: 521 loss: 1.84462147e-06
Iter: 522 loss: 1.84474959e-06
Iter: 523 loss: 1.84415512e-06
Iter: 524 loss: 1.84351279e-06
Iter: 525 loss: 1.85143085e-06
Iter: 526 loss: 1.84352291e-06
Iter: 527 loss: 1.84309772e-06
Iter: 528 loss: 1.84212308e-06
Iter: 529 loss: 1.85477268e-06
Iter: 530 loss: 1.84207329e-06
Iter: 531 loss: 1.84131341e-06
Iter: 532 loss: 1.84870373e-06
Iter: 533 loss: 1.84132455e-06
Iter: 534 loss: 1.84065493e-06
Iter: 535 loss: 1.84255919e-06
Iter: 536 loss: 1.84046758e-06
Iter: 537 loss: 1.83968052e-06
Iter: 538 loss: 1.84223722e-06
Iter: 539 loss: 1.83946941e-06
Iter: 540 loss: 1.83890324e-06
Iter: 541 loss: 1.83769669e-06
Iter: 542 loss: 1.85654403e-06
Iter: 543 loss: 1.83763859e-06
Iter: 544 loss: 1.83695943e-06
Iter: 545 loss: 1.83685324e-06
Iter: 546 loss: 1.83591931e-06
Iter: 547 loss: 1.83541931e-06
Iter: 548 loss: 1.83498537e-06
Iter: 549 loss: 1.83413727e-06
Iter: 550 loss: 1.833537e-06
Iter: 551 loss: 1.8332081e-06
Iter: 552 loss: 1.83229008e-06
Iter: 553 loss: 1.84535054e-06
Iter: 554 loss: 1.83234215e-06
Iter: 555 loss: 1.83132624e-06
Iter: 556 loss: 1.83139548e-06
Iter: 557 loss: 1.83054749e-06
Iter: 558 loss: 1.82981694e-06
Iter: 559 loss: 1.83353848e-06
Iter: 560 loss: 1.82972087e-06
Iter: 561 loss: 1.82895405e-06
Iter: 562 loss: 1.83227405e-06
Iter: 563 loss: 1.82881354e-06
Iter: 564 loss: 1.82815916e-06
Iter: 565 loss: 1.8326042e-06
Iter: 566 loss: 1.82809038e-06
Iter: 567 loss: 1.82762858e-06
Iter: 568 loss: 1.82654946e-06
Iter: 569 loss: 1.83938732e-06
Iter: 570 loss: 1.82641099e-06
Iter: 571 loss: 1.82538804e-06
Iter: 572 loss: 1.8350097e-06
Iter: 573 loss: 1.82529948e-06
Iter: 574 loss: 1.8245961e-06
Iter: 575 loss: 1.83153554e-06
Iter: 576 loss: 1.82453209e-06
Iter: 577 loss: 1.82382473e-06
Iter: 578 loss: 1.82440897e-06
Iter: 579 loss: 1.82346389e-06
Iter: 580 loss: 1.82272845e-06
Iter: 581 loss: 1.82150904e-06
Iter: 582 loss: 1.85133649e-06
Iter: 583 loss: 1.82151848e-06
Iter: 584 loss: 1.82133044e-06
Iter: 585 loss: 1.82092208e-06
Iter: 586 loss: 1.82038082e-06
Iter: 587 loss: 1.81977941e-06
Iter: 588 loss: 1.81961889e-06
Iter: 589 loss: 1.81878625e-06
Iter: 590 loss: 1.81838038e-06
Iter: 591 loss: 1.81796236e-06
Iter: 592 loss: 1.81680969e-06
Iter: 593 loss: 1.81682776e-06
Iter: 594 loss: 1.81635619e-06
Iter: 595 loss: 1.81577434e-06
Iter: 596 loss: 1.81574069e-06
Iter: 597 loss: 1.81521034e-06
Iter: 598 loss: 1.81518544e-06
Iter: 599 loss: 1.81482892e-06
Iter: 600 loss: 1.8145397e-06
Iter: 601 loss: 1.81444784e-06
Iter: 602 loss: 1.81366306e-06
Iter: 603 loss: 1.81308451e-06
Iter: 604 loss: 1.81272344e-06
Iter: 605 loss: 1.81207395e-06
Iter: 606 loss: 1.81423502e-06
Iter: 607 loss: 1.81181144e-06
Iter: 608 loss: 1.81111386e-06
Iter: 609 loss: 1.82017925e-06
Iter: 610 loss: 1.8111632e-06
Iter: 611 loss: 1.81062637e-06
Iter: 612 loss: 1.81008295e-06
Iter: 613 loss: 1.80999734e-06
Iter: 614 loss: 1.80921802e-06
Iter: 615 loss: 1.81085898e-06
Iter: 616 loss: 1.80884854e-06
Iter: 617 loss: 1.80828511e-06
Iter: 618 loss: 1.81334372e-06
Iter: 619 loss: 1.80827828e-06
Iter: 620 loss: 1.807535e-06
Iter: 621 loss: 1.80655138e-06
Iter: 622 loss: 1.80648635e-06
Iter: 623 loss: 1.8056071e-06
Iter: 624 loss: 1.80959091e-06
Iter: 625 loss: 1.80549364e-06
Iter: 626 loss: 1.80472762e-06
Iter: 627 loss: 1.81060716e-06
Iter: 628 loss: 1.80464372e-06
Iter: 629 loss: 1.80426298e-06
Iter: 630 loss: 1.80440554e-06
Iter: 631 loss: 1.80393363e-06
Iter: 632 loss: 1.80320058e-06
Iter: 633 loss: 1.80713675e-06
Iter: 634 loss: 1.80308e-06
Iter: 635 loss: 1.80266466e-06
Iter: 636 loss: 1.80323491e-06
Iter: 637 loss: 1.8024225e-06
Iter: 638 loss: 1.80189193e-06
Iter: 639 loss: 1.80135817e-06
Iter: 640 loss: 1.80118263e-06
Iter: 641 loss: 1.80045083e-06
Iter: 642 loss: 1.80058771e-06
Iter: 643 loss: 1.79986807e-06
Iter: 644 loss: 1.79921506e-06
Iter: 645 loss: 1.79911501e-06
Iter: 646 loss: 1.79873064e-06
Iter: 647 loss: 1.79796302e-06
Iter: 648 loss: 1.81349321e-06
Iter: 649 loss: 1.79793767e-06
Iter: 650 loss: 1.79723918e-06
Iter: 651 loss: 1.8020105e-06
Iter: 652 loss: 1.79712526e-06
Iter: 653 loss: 1.79648077e-06
Iter: 654 loss: 1.79901019e-06
Iter: 655 loss: 1.79628603e-06
Iter: 656 loss: 1.79537244e-06
Iter: 657 loss: 1.79536698e-06
Iter: 658 loss: 1.79469157e-06
Iter: 659 loss: 1.79403855e-06
Iter: 660 loss: 1.79884762e-06
Iter: 661 loss: 1.7939816e-06
Iter: 662 loss: 1.7932673e-06
Iter: 663 loss: 1.79300389e-06
Iter: 664 loss: 1.79265635e-06
Iter: 665 loss: 1.79225174e-06
Iter: 666 loss: 1.79212179e-06
Iter: 667 loss: 1.79177073e-06
Iter: 668 loss: 1.79095071e-06
Iter: 669 loss: 1.79089034e-06
Iter: 670 loss: 1.79019457e-06
Iter: 671 loss: 1.79632707e-06
Iter: 672 loss: 1.79011772e-06
Iter: 673 loss: 1.78948051e-06
Iter: 674 loss: 1.78826258e-06
Iter: 675 loss: 1.81116206e-06
Iter: 676 loss: 1.78830851e-06
Iter: 677 loss: 1.78721882e-06
Iter: 678 loss: 1.79782955e-06
Iter: 679 loss: 1.78719142e-06
Iter: 680 loss: 1.78615096e-06
Iter: 681 loss: 1.79428571e-06
Iter: 682 loss: 1.78606399e-06
Iter: 683 loss: 1.78571372e-06
Iter: 684 loss: 1.78489154e-06
Iter: 685 loss: 1.80014047e-06
Iter: 686 loss: 1.7848879e-06
Iter: 687 loss: 1.78374614e-06
Iter: 688 loss: 1.78899415e-06
Iter: 689 loss: 1.78348409e-06
Iter: 690 loss: 1.78270272e-06
Iter: 691 loss: 1.79359745e-06
Iter: 692 loss: 1.78272262e-06
Iter: 693 loss: 1.78214827e-06
Iter: 694 loss: 1.78219216e-06
Iter: 695 loss: 1.78170524e-06
Iter: 696 loss: 1.78103789e-06
Iter: 697 loss: 1.78317032e-06
Iter: 698 loss: 1.7807937e-06
Iter: 699 loss: 1.78016364e-06
Iter: 700 loss: 1.78045752e-06
Iter: 701 loss: 1.77973175e-06
Iter: 702 loss: 1.77907373e-06
Iter: 703 loss: 1.77905463e-06
Iter: 704 loss: 1.77875904e-06
Iter: 705 loss: 1.77817367e-06
Iter: 706 loss: 1.78865321e-06
Iter: 707 loss: 1.77815662e-06
Iter: 708 loss: 1.77730419e-06
Iter: 709 loss: 1.7794664e-06
Iter: 710 loss: 1.77701577e-06
Iter: 711 loss: 1.77621075e-06
Iter: 712 loss: 1.77532365e-06
Iter: 713 loss: 1.77515619e-06
Iter: 714 loss: 1.77522975e-06
Iter: 715 loss: 1.77468928e-06
Iter: 716 loss: 1.77432344e-06
Iter: 717 loss: 1.77331322e-06
Iter: 718 loss: 1.7893833e-06
Iter: 719 loss: 1.77331867e-06
Iter: 720 loss: 1.77239383e-06
Iter: 721 loss: 1.77239281e-06
Iter: 722 loss: 1.77165396e-06
Iter: 723 loss: 1.7707606e-06
Iter: 724 loss: 1.77076276e-06
Iter: 725 loss: 1.77026322e-06
Iter: 726 loss: 1.77406332e-06
Iter: 727 loss: 1.77026482e-06
Iter: 728 loss: 1.76976687e-06
Iter: 729 loss: 1.76933361e-06
Iter: 730 loss: 1.76921321e-06
Iter: 731 loss: 1.76833555e-06
Iter: 732 loss: 1.77121979e-06
Iter: 733 loss: 1.76804895e-06
Iter: 734 loss: 1.76752826e-06
Iter: 735 loss: 1.77151355e-06
Iter: 736 loss: 1.76750541e-06
Iter: 737 loss: 1.76688434e-06
Iter: 738 loss: 1.76670846e-06
Iter: 739 loss: 1.76631875e-06
Iter: 740 loss: 1.76563901e-06
Iter: 741 loss: 1.76546177e-06
Iter: 742 loss: 1.76502897e-06
Iter: 743 loss: 1.7639918e-06
Iter: 744 loss: 1.77067545e-06
Iter: 745 loss: 1.76377694e-06
Iter: 746 loss: 1.76310266e-06
Iter: 747 loss: 1.76285175e-06
Iter: 748 loss: 1.76253513e-06
Iter: 749 loss: 1.76230628e-06
Iter: 750 loss: 1.7620381e-06
Iter: 751 loss: 1.7617806e-06
Iter: 752 loss: 1.76096171e-06
Iter: 753 loss: 1.76318974e-06
Iter: 754 loss: 1.76047911e-06
Iter: 755 loss: 1.75929222e-06
Iter: 756 loss: 1.76442495e-06
Iter: 757 loss: 1.7590761e-06
Iter: 758 loss: 1.75817445e-06
Iter: 759 loss: 1.76141498e-06
Iter: 760 loss: 1.75792229e-06
Iter: 761 loss: 1.7569439e-06
Iter: 762 loss: 1.76807248e-06
Iter: 763 loss: 1.75694402e-06
Iter: 764 loss: 1.75649825e-06
Iter: 765 loss: 1.75591038e-06
Iter: 766 loss: 1.75589173e-06
Iter: 767 loss: 1.75521086e-06
Iter: 768 loss: 1.75523212e-06
Iter: 769 loss: 1.75479249e-06
Iter: 770 loss: 1.75397281e-06
Iter: 771 loss: 1.77062623e-06
Iter: 772 loss: 1.75395962e-06
Iter: 773 loss: 1.75372679e-06
Iter: 774 loss: 1.75352363e-06
Iter: 775 loss: 1.7531512e-06
Iter: 776 loss: 1.75260516e-06
Iter: 777 loss: 1.76579761e-06
Iter: 778 loss: 1.75260084e-06
Iter: 779 loss: 1.7519003e-06
Iter: 780 loss: 1.75442187e-06
Iter: 781 loss: 1.75175455e-06
Iter: 782 loss: 1.75111518e-06
Iter: 783 loss: 1.75081198e-06
Iter: 784 loss: 1.75055334e-06
Iter: 785 loss: 1.75034154e-06
Iter: 786 loss: 1.75014384e-06
Iter: 787 loss: 1.74984211e-06
Iter: 788 loss: 1.74924833e-06
Iter: 789 loss: 1.76002982e-06
Iter: 790 loss: 1.74922229e-06
Iter: 791 loss: 1.74836987e-06
Iter: 792 loss: 1.75221044e-06
Iter: 793 loss: 1.74823106e-06
Iter: 794 loss: 1.74757429e-06
Iter: 795 loss: 1.75484683e-06
Iter: 796 loss: 1.74757815e-06
Iter: 797 loss: 1.74702541e-06
Iter: 798 loss: 1.74643219e-06
Iter: 799 loss: 1.74631009e-06
Iter: 800 loss: 1.74556897e-06
Iter: 801 loss: 1.74844433e-06
Iter: 802 loss: 1.74542288e-06
Iter: 803 loss: 1.74453567e-06
Iter: 804 loss: 1.74845354e-06
Iter: 805 loss: 1.7442967e-06
Iter: 806 loss: 1.74384172e-06
Iter: 807 loss: 1.74461843e-06
Iter: 808 loss: 1.74362845e-06
Iter: 809 loss: 1.74295087e-06
Iter: 810 loss: 1.74604304e-06
Iter: 811 loss: 1.74283468e-06
Iter: 812 loss: 1.74254637e-06
Iter: 813 loss: 1.74205695e-06
Iter: 814 loss: 1.74209902e-06
Iter: 815 loss: 1.74117667e-06
Iter: 816 loss: 1.74330717e-06
Iter: 817 loss: 1.74082606e-06
Iter: 818 loss: 1.7402815e-06
Iter: 819 loss: 1.74083789e-06
Iter: 820 loss: 1.73986882e-06
Iter: 821 loss: 1.73922353e-06
Iter: 822 loss: 1.7499184e-06
Iter: 823 loss: 1.73924218e-06
Iter: 824 loss: 1.73881517e-06
Iter: 825 loss: 1.73765125e-06
Iter: 826 loss: 1.74350976e-06
Iter: 827 loss: 1.73735737e-06
Iter: 828 loss: 1.73649914e-06
Iter: 829 loss: 1.74688535e-06
Iter: 830 loss: 1.7364971e-06
Iter: 831 loss: 1.73584499e-06
Iter: 832 loss: 1.74447644e-06
Iter: 833 loss: 1.73586704e-06
Iter: 834 loss: 1.73549904e-06
Iter: 835 loss: 1.73470903e-06
Iter: 836 loss: 1.74298248e-06
Iter: 837 loss: 1.73457829e-06
Iter: 838 loss: 1.734536e-06
Iter: 839 loss: 1.73421199e-06
Iter: 840 loss: 1.73388526e-06
Iter: 841 loss: 1.73324418e-06
Iter: 842 loss: 1.74544766e-06
Iter: 843 loss: 1.73318199e-06
Iter: 844 loss: 1.73260719e-06
Iter: 845 loss: 1.73204057e-06
Iter: 846 loss: 1.73188698e-06
Iter: 847 loss: 1.73160197e-06
Iter: 848 loss: 1.73154308e-06
Iter: 849 loss: 1.73119349e-06
Iter: 850 loss: 1.73132719e-06
Iter: 851 loss: 1.73091848e-06
Iter: 852 loss: 1.73049216e-06
Iter: 853 loss: 1.72986734e-06
Iter: 854 loss: 1.72988223e-06
Iter: 855 loss: 1.72915202e-06
Iter: 856 loss: 1.7308904e-06
Iter: 857 loss: 1.72884302e-06
Iter: 858 loss: 1.72860075e-06
Iter: 859 loss: 1.72843465e-06
Iter: 860 loss: 1.72821535e-06
Iter: 861 loss: 1.72768e-06
Iter: 862 loss: 1.73246212e-06
Iter: 863 loss: 1.72757632e-06
Iter: 864 loss: 1.7273037e-06
Iter: 865 loss: 1.7272539e-06
Iter: 866 loss: 1.72694627e-06
Iter: 867 loss: 1.7263867e-06
Iter: 868 loss: 1.73162039e-06
Iter: 869 loss: 1.72620219e-06
Iter: 870 loss: 1.7259797e-06
Iter: 871 loss: 1.7259822e-06
Iter: 872 loss: 1.72566808e-06
Iter: 873 loss: 1.72521425e-06
Iter: 874 loss: 1.72520356e-06
Iter: 875 loss: 1.7247678e-06
Iter: 876 loss: 1.72472483e-06
Iter: 877 loss: 1.72442969e-06
Iter: 878 loss: 1.72399587e-06
Iter: 879 loss: 1.72407886e-06
Iter: 880 loss: 1.72369948e-06
Iter: 881 loss: 1.72315697e-06
Iter: 882 loss: 1.72779164e-06
Iter: 883 loss: 1.72318096e-06
Iter: 884 loss: 1.72275202e-06
Iter: 885 loss: 1.72174805e-06
Iter: 886 loss: 1.73509579e-06
Iter: 887 loss: 1.72177556e-06
Iter: 888 loss: 1.72067598e-06
Iter: 889 loss: 1.72988621e-06
Iter: 890 loss: 1.72064847e-06
Iter: 891 loss: 1.72087175e-06
Iter: 892 loss: 1.7201902e-06
Iter: 893 loss: 1.71991769e-06
Iter: 894 loss: 1.71921249e-06
Iter: 895 loss: 1.72542832e-06
Iter: 896 loss: 1.71907072e-06
Iter: 897 loss: 1.71884869e-06
Iter: 898 loss: 1.71866213e-06
Iter: 899 loss: 1.7184035e-06
Iter: 900 loss: 1.71768693e-06
Iter: 901 loss: 1.72678165e-06
Iter: 902 loss: 1.71771649e-06
Iter: 903 loss: 1.71714237e-06
Iter: 904 loss: 1.72461682e-06
Iter: 905 loss: 1.71707836e-06
Iter: 906 loss: 1.71674196e-06
Iter: 907 loss: 1.71684906e-06
Iter: 908 loss: 1.71644228e-06
Iter: 909 loss: 1.71576607e-06
Iter: 910 loss: 1.71997794e-06
Iter: 911 loss: 1.71566899e-06
Iter: 912 loss: 1.71538613e-06
Iter: 913 loss: 1.71482066e-06
Iter: 914 loss: 1.72485716e-06
Iter: 915 loss: 1.7147712e-06
Iter: 916 loss: 1.71428167e-06
Iter: 917 loss: 1.71427666e-06
Iter: 918 loss: 1.71379043e-06
Iter: 919 loss: 1.71631416e-06
Iter: 920 loss: 1.71373404e-06
Iter: 921 loss: 1.71334182e-06
Iter: 922 loss: 1.71264264e-06
Iter: 923 loss: 1.72678779e-06
Iter: 924 loss: 1.71265526e-06
Iter: 925 loss: 1.71196154e-06
Iter: 926 loss: 1.71245176e-06
Iter: 927 loss: 1.71144893e-06
Iter: 928 loss: 1.7115932e-06
Iter: 929 loss: 1.71116358e-06
Iter: 930 loss: 1.71084048e-06
Iter: 931 loss: 1.71014847e-06
Iter: 932 loss: 1.720829e-06
Iter: 933 loss: 1.71005695e-06
Iter: 934 loss: 1.70947089e-06
Iter: 935 loss: 1.71080728e-06
Iter: 936 loss: 1.70916508e-06
Iter: 937 loss: 1.70858664e-06
Iter: 938 loss: 1.70857265e-06
Iter: 939 loss: 1.70832277e-06
Iter: 940 loss: 1.70765861e-06
Iter: 941 loss: 1.71405702e-06
Iter: 942 loss: 1.70762951e-06
Iter: 943 loss: 1.70744522e-06
Iter: 944 loss: 1.70720546e-06
Iter: 945 loss: 1.7069043e-06
Iter: 946 loss: 1.70635428e-06
Iter: 947 loss: 1.70634439e-06
Iter: 948 loss: 1.70573958e-06
Iter: 949 loss: 1.70577596e-06
Iter: 950 loss: 1.70531405e-06
Iter: 951 loss: 1.70526073e-06
Iter: 952 loss: 1.70496139e-06
Iter: 953 loss: 1.70439034e-06
Iter: 954 loss: 1.7043667e-06
Iter: 955 loss: 1.70374153e-06
Iter: 956 loss: 1.70761803e-06
Iter: 957 loss: 1.70371129e-06
Iter: 958 loss: 1.70329065e-06
Iter: 959 loss: 1.7029015e-06
Iter: 960 loss: 1.70284159e-06
Iter: 961 loss: 1.70254089e-06
Iter: 962 loss: 1.70244596e-06
Iter: 963 loss: 1.70218118e-06
Iter: 964 loss: 1.70156306e-06
Iter: 965 loss: 1.70892963e-06
Iter: 966 loss: 1.70148553e-06
Iter: 967 loss: 1.70116846e-06
Iter: 968 loss: 1.70108399e-06
Iter: 969 loss: 1.70072258e-06
Iter: 970 loss: 1.7001538e-06
Iter: 971 loss: 1.71461261e-06
Iter: 972 loss: 1.70013834e-06
Iter: 973 loss: 1.69956888e-06
Iter: 974 loss: 1.70095814e-06
Iter: 975 loss: 1.6994652e-06
Iter: 976 loss: 1.69879547e-06
Iter: 977 loss: 1.70381611e-06
Iter: 978 loss: 1.69866905e-06
Iter: 979 loss: 1.69836528e-06
Iter: 980 loss: 1.69838586e-06
Iter: 981 loss: 1.69807834e-06
Iter: 982 loss: 1.69768384e-06
Iter: 983 loss: 1.69808447e-06
Iter: 984 loss: 1.69746704e-06
Iter: 985 loss: 1.69711e-06
Iter: 986 loss: 1.69737461e-06
Iter: 987 loss: 1.69685006e-06
Iter: 988 loss: 1.69638656e-06
Iter: 989 loss: 1.6970846e-06
Iter: 990 loss: 1.69617647e-06
Iter: 991 loss: 1.69562929e-06
Iter: 992 loss: 1.69692578e-06
Iter: 993 loss: 1.6954873e-06
Iter: 994 loss: 1.69508701e-06
Iter: 995 loss: 1.6968811e-06
Iter: 996 loss: 1.69497366e-06
Iter: 997 loss: 1.69462623e-06
Iter: 998 loss: 1.69877285e-06
Iter: 999 loss: 1.69459986e-06
Iter: 1000 loss: 1.6943975e-06
Iter: 1001 loss: 1.69402381e-06
Iter: 1002 loss: 1.70048543e-06
Iter: 1003 loss: 1.69398538e-06
Iter: 1004 loss: 1.69374403e-06
Iter: 1005 loss: 1.69370594e-06
Iter: 1006 loss: 1.6935237e-06
Iter: 1007 loss: 1.69302496e-06
Iter: 1008 loss: 1.69545535e-06
Iter: 1009 loss: 1.69291172e-06
Iter: 1010 loss: 1.69240843e-06
Iter: 1011 loss: 1.69563737e-06
Iter: 1012 loss: 1.69235761e-06
Iter: 1013 loss: 1.69206078e-06
Iter: 1014 loss: 1.69693931e-06
Iter: 1015 loss: 1.69205987e-06
Iter: 1016 loss: 1.69185614e-06
Iter: 1017 loss: 1.69124792e-06
Iter: 1018 loss: 1.6967283e-06
Iter: 1019 loss: 1.69111138e-06
Iter: 1020 loss: 1.69134921e-06
Iter: 1021 loss: 1.69090936e-06
Iter: 1022 loss: 1.69077157e-06
Iter: 1023 loss: 1.6904371e-06
Iter: 1024 loss: 1.69560656e-06
Iter: 1025 loss: 1.69040936e-06
Iter: 1026 loss: 1.68997587e-06
Iter: 1027 loss: 1.69115469e-06
Iter: 1028 loss: 1.68980841e-06
Iter: 1029 loss: 1.6893498e-06
Iter: 1030 loss: 1.68915801e-06
Iter: 1031 loss: 1.68887379e-06
Iter: 1032 loss: 1.68813585e-06
Iter: 1033 loss: 1.68873214e-06
Iter: 1034 loss: 1.68765382e-06
Iter: 1035 loss: 1.68758265e-06
Iter: 1036 loss: 1.68727229e-06
Iter: 1037 loss: 1.68706447e-06
Iter: 1038 loss: 1.68647819e-06
Iter: 1039 loss: 1.69587554e-06
Iter: 1040 loss: 1.68644885e-06
Iter: 1041 loss: 1.68583517e-06
Iter: 1042 loss: 1.69494433e-06
Iter: 1043 loss: 1.68585007e-06
Iter: 1044 loss: 1.6855646e-06
Iter: 1045 loss: 1.68496706e-06
Iter: 1046 loss: 1.6970605e-06
Iter: 1047 loss: 1.68492033e-06
Iter: 1048 loss: 1.68443421e-06
Iter: 1049 loss: 1.68662507e-06
Iter: 1050 loss: 1.68441807e-06
Iter: 1051 loss: 1.68397924e-06
Iter: 1052 loss: 1.68949987e-06
Iter: 1053 loss: 1.68394263e-06
Iter: 1054 loss: 1.68373981e-06
Iter: 1055 loss: 1.68347844e-06
Iter: 1056 loss: 1.68341558e-06
Iter: 1057 loss: 1.68283577e-06
Iter: 1058 loss: 1.68581869e-06
Iter: 1059 loss: 1.6828202e-06
Iter: 1060 loss: 1.68254439e-06
Iter: 1061 loss: 1.68240217e-06
Iter: 1062 loss: 1.6822197e-06
Iter: 1063 loss: 1.68178303e-06
Iter: 1064 loss: 1.68256827e-06
Iter: 1065 loss: 1.68160943e-06
Iter: 1066 loss: 1.68106442e-06
Iter: 1067 loss: 1.68032796e-06
Iter: 1068 loss: 1.68030863e-06
Iter: 1069 loss: 1.68000884e-06
Iter: 1070 loss: 1.67977942e-06
Iter: 1071 loss: 1.67937128e-06
Iter: 1072 loss: 1.67931876e-06
Iter: 1073 loss: 1.67896758e-06
Iter: 1074 loss: 1.6786845e-06
Iter: 1075 loss: 1.68171073e-06
Iter: 1076 loss: 1.67867199e-06
Iter: 1077 loss: 1.67823487e-06
Iter: 1078 loss: 1.67760436e-06
Iter: 1079 loss: 1.67764119e-06
Iter: 1080 loss: 1.67698977e-06
Iter: 1081 loss: 1.67731628e-06
Iter: 1082 loss: 1.67657436e-06
Iter: 1083 loss: 1.67598546e-06
Iter: 1084 loss: 1.67598989e-06
Iter: 1085 loss: 1.67557619e-06
Iter: 1086 loss: 1.67591179e-06
Iter: 1087 loss: 1.67533835e-06
Iter: 1088 loss: 1.67488088e-06
Iter: 1089 loss: 1.67755297e-06
Iter: 1090 loss: 1.67484961e-06
Iter: 1091 loss: 1.67449775e-06
Iter: 1092 loss: 1.67396149e-06
Iter: 1093 loss: 1.67398366e-06
Iter: 1094 loss: 1.67341057e-06
Iter: 1095 loss: 1.68051929e-06
Iter: 1096 loss: 1.67343126e-06
Iter: 1097 loss: 1.67300414e-06
Iter: 1098 loss: 1.67252051e-06
Iter: 1099 loss: 1.67247629e-06
Iter: 1100 loss: 1.6719132e-06
Iter: 1101 loss: 1.67437315e-06
Iter: 1102 loss: 1.67173789e-06
Iter: 1103 loss: 1.670963e-06
Iter: 1104 loss: 1.67749204e-06
Iter: 1105 loss: 1.67097824e-06
Iter: 1106 loss: 1.67059238e-06
Iter: 1107 loss: 1.67073608e-06
Iter: 1108 loss: 1.67032613e-06
Iter: 1109 loss: 1.66979908e-06
Iter: 1110 loss: 1.67188955e-06
Iter: 1111 loss: 1.66968107e-06
Iter: 1112 loss: 1.66933557e-06
Iter: 1113 loss: 1.66892016e-06
Iter: 1114 loss: 1.66879772e-06
Iter: 1115 loss: 1.6683623e-06
Iter: 1116 loss: 1.67594487e-06
Iter: 1117 loss: 1.66834764e-06
Iter: 1118 loss: 1.66800658e-06
Iter: 1119 loss: 1.66819814e-06
Iter: 1120 loss: 1.66766438e-06
Iter: 1121 loss: 1.66725022e-06
Iter: 1122 loss: 1.66910547e-06
Iter: 1123 loss: 1.66707605e-06
Iter: 1124 loss: 1.6666811e-06
Iter: 1125 loss: 1.66640689e-06
Iter: 1126 loss: 1.66627922e-06
Iter: 1127 loss: 1.66575478e-06
Iter: 1128 loss: 1.6688399e-06
Iter: 1129 loss: 1.66566281e-06
Iter: 1130 loss: 1.66506834e-06
Iter: 1131 loss: 1.66519874e-06
Iter: 1132 loss: 1.66463246e-06
Iter: 1133 loss: 1.66415066e-06
Iter: 1134 loss: 1.6641842e-06
Iter: 1135 loss: 1.66373138e-06
Iter: 1136 loss: 1.66358245e-06
Iter: 1137 loss: 1.6634126e-06
Iter: 1138 loss: 1.66308439e-06
Iter: 1139 loss: 1.66240079e-06
Iter: 1140 loss: 1.67362236e-06
Iter: 1141 loss: 1.66242887e-06
Iter: 1142 loss: 1.66206337e-06
Iter: 1143 loss: 1.66199356e-06
Iter: 1144 loss: 1.66172617e-06
Iter: 1145 loss: 1.66116013e-06
Iter: 1146 loss: 1.66907125e-06
Iter: 1147 loss: 1.66115092e-06
Iter: 1148 loss: 1.66065888e-06
Iter: 1149 loss: 1.66734662e-06
Iter: 1150 loss: 1.66068548e-06
Iter: 1151 loss: 1.66030054e-06
Iter: 1152 loss: 1.66072778e-06
Iter: 1153 loss: 1.66006805e-06
Iter: 1154 loss: 1.65955635e-06
Iter: 1155 loss: 1.66194559e-06
Iter: 1156 loss: 1.6595244e-06
Iter: 1157 loss: 1.65917368e-06
Iter: 1158 loss: 1.65864719e-06
Iter: 1159 loss: 1.65862798e-06
Iter: 1160 loss: 1.6579321e-06
Iter: 1161 loss: 1.65766562e-06
Iter: 1162 loss: 1.65727874e-06
Iter: 1163 loss: 1.65650681e-06
Iter: 1164 loss: 1.65950803e-06
Iter: 1165 loss: 1.65635345e-06
Iter: 1166 loss: 1.65589904e-06
Iter: 1167 loss: 1.65594281e-06
Iter: 1168 loss: 1.65552376e-06
Iter: 1169 loss: 1.65464769e-06
Iter: 1170 loss: 1.66352788e-06
Iter: 1171 loss: 1.65448466e-06
Iter: 1172 loss: 1.65480697e-06
Iter: 1173 loss: 1.65422091e-06
Iter: 1174 loss: 1.65386712e-06
Iter: 1175 loss: 1.65313793e-06
Iter: 1176 loss: 1.65951985e-06
Iter: 1177 loss: 1.65294261e-06
Iter: 1178 loss: 1.65229801e-06
Iter: 1179 loss: 1.65904476e-06
Iter: 1180 loss: 1.65229835e-06
Iter: 1181 loss: 1.65156905e-06
Iter: 1182 loss: 1.65524727e-06
Iter: 1183 loss: 1.65148413e-06
Iter: 1184 loss: 1.65120571e-06
Iter: 1185 loss: 1.65119059e-06
Iter: 1186 loss: 1.65093184e-06
Iter: 1187 loss: 1.65036761e-06
Iter: 1188 loss: 1.6514532e-06
Iter: 1189 loss: 1.65015695e-06
Iter: 1190 loss: 1.64967787e-06
Iter: 1191 loss: 1.64907601e-06
Iter: 1192 loss: 1.64913354e-06
Iter: 1193 loss: 1.64905885e-06
Iter: 1194 loss: 1.64882965e-06
Iter: 1195 loss: 1.64855862e-06
Iter: 1196 loss: 1.64816493e-06
Iter: 1197 loss: 1.64816299e-06
Iter: 1198 loss: 1.64769153e-06
Iter: 1199 loss: 1.65093752e-06
Iter: 1200 loss: 1.64765868e-06
Iter: 1201 loss: 1.64719131e-06
Iter: 1202 loss: 1.64646588e-06
Iter: 1203 loss: 1.64644712e-06
Iter: 1204 loss: 1.6458531e-06
Iter: 1205 loss: 1.65051642e-06
Iter: 1206 loss: 1.64582104e-06
Iter: 1207 loss: 1.64520497e-06
Iter: 1208 loss: 1.64988944e-06
Iter: 1209 loss: 1.64523101e-06
Iter: 1210 loss: 1.6449186e-06
Iter: 1211 loss: 1.64419964e-06
Iter: 1212 loss: 1.65100732e-06
Iter: 1213 loss: 1.64417816e-06
Iter: 1214 loss: 1.64431276e-06
Iter: 1215 loss: 1.64387575e-06
Iter: 1216 loss: 1.643627e-06
Iter: 1217 loss: 1.64307914e-06
Iter: 1218 loss: 1.6500552e-06
Iter: 1219 loss: 1.64301332e-06
Iter: 1220 loss: 1.64253242e-06
Iter: 1221 loss: 1.64253811e-06
Iter: 1222 loss: 1.64200378e-06
Iter: 1223 loss: 1.64293783e-06
Iter: 1224 loss: 1.6417639e-06
Iter: 1225 loss: 1.64157677e-06
Iter: 1226 loss: 1.64104529e-06
Iter: 1227 loss: 1.65231063e-06
Iter: 1228 loss: 1.64107826e-06
Iter: 1229 loss: 1.64051505e-06
Iter: 1230 loss: 1.64056473e-06
Iter: 1231 loss: 1.64027711e-06
Iter: 1232 loss: 1.63976893e-06
Iter: 1233 loss: 1.64928338e-06
Iter: 1234 loss: 1.63971424e-06
Iter: 1235 loss: 1.63901e-06
Iter: 1236 loss: 1.64601192e-06
Iter: 1237 loss: 1.63894788e-06
Iter: 1238 loss: 1.63835375e-06
Iter: 1239 loss: 1.63839104e-06
Iter: 1240 loss: 1.6380975e-06
Iter: 1241 loss: 1.63890581e-06
Iter: 1242 loss: 1.63802008e-06
Iter: 1243 loss: 1.63756681e-06
Iter: 1244 loss: 1.6372004e-06
Iter: 1245 loss: 1.63705101e-06
Iter: 1246 loss: 1.63667414e-06
Iter: 1247 loss: 1.63652157e-06
Iter: 1248 loss: 1.6362668e-06
Iter: 1249 loss: 1.63586174e-06
Iter: 1250 loss: 1.63582422e-06
Iter: 1251 loss: 1.63550283e-06
Iter: 1252 loss: 1.63476022e-06
Iter: 1253 loss: 1.64386108e-06
Iter: 1254 loss: 1.63467314e-06
Iter: 1255 loss: 1.63453637e-06
Iter: 1256 loss: 1.63437483e-06
Iter: 1257 loss: 1.6340224e-06
Iter: 1258 loss: 1.63342224e-06
Iter: 1259 loss: 1.64608741e-06
Iter: 1260 loss: 1.63338245e-06
Iter: 1261 loss: 1.6328753e-06
Iter: 1262 loss: 1.63336858e-06
Iter: 1263 loss: 1.6325788e-06
Iter: 1264 loss: 1.63252776e-06
Iter: 1265 loss: 1.63227514e-06
Iter: 1266 loss: 1.63204982e-06
Iter: 1267 loss: 1.63151117e-06
Iter: 1268 loss: 1.63629068e-06
Iter: 1269 loss: 1.63139248e-06
Iter: 1270 loss: 1.63095774e-06
Iter: 1271 loss: 1.63160371e-06
Iter: 1272 loss: 1.63073855e-06
Iter: 1273 loss: 1.63046843e-06
Iter: 1274 loss: 1.63002687e-06
Iter: 1275 loss: 1.63003631e-06
Iter: 1276 loss: 1.62961385e-06
Iter: 1277 loss: 1.63069649e-06
Iter: 1278 loss: 1.62945196e-06
Iter: 1279 loss: 1.6290104e-06
Iter: 1280 loss: 1.63180641e-06
Iter: 1281 loss: 1.62890353e-06
Iter: 1282 loss: 1.62853564e-06
Iter: 1283 loss: 1.62803076e-06
Iter: 1284 loss: 1.6279688e-06
Iter: 1285 loss: 1.62829701e-06
Iter: 1286 loss: 1.62784272e-06
Iter: 1287 loss: 1.62771312e-06
Iter: 1288 loss: 1.62728611e-06
Iter: 1289 loss: 1.6291275e-06
Iter: 1290 loss: 1.62711729e-06
Iter: 1291 loss: 1.62681215e-06
Iter: 1292 loss: 1.62677884e-06
Iter: 1293 loss: 1.62657079e-06
Iter: 1294 loss: 1.62642323e-06
Iter: 1295 loss: 1.62631216e-06
Iter: 1296 loss: 1.62596052e-06
Iter: 1297 loss: 1.62780645e-06
Iter: 1298 loss: 1.62595063e-06
Iter: 1299 loss: 1.62577305e-06
Iter: 1300 loss: 1.6254553e-06
Iter: 1301 loss: 1.63061554e-06
Iter: 1302 loss: 1.62546644e-06
Iter: 1303 loss: 1.62522815e-06
Iter: 1304 loss: 1.6252e-06
Iter: 1305 loss: 1.62501703e-06
Iter: 1306 loss: 1.62540482e-06
Iter: 1307 loss: 1.62488425e-06
Iter: 1308 loss: 1.62457582e-06
Iter: 1309 loss: 1.62452704e-06
Iter: 1310 loss: 1.62432798e-06
Iter: 1311 loss: 1.62385436e-06
Iter: 1312 loss: 1.62762979e-06
Iter: 1313 loss: 1.62384617e-06
Iter: 1314 loss: 1.62332174e-06
Iter: 1315 loss: 1.62391382e-06
Iter: 1316 loss: 1.62300785e-06
Iter: 1317 loss: 1.62252957e-06
Iter: 1318 loss: 1.62215599e-06
Iter: 1319 loss: 1.62193726e-06
Iter: 1320 loss: 1.62129982e-06
Iter: 1321 loss: 1.62263507e-06
Iter: 1322 loss: 1.62101924e-06
Iter: 1323 loss: 1.62058325e-06
Iter: 1324 loss: 1.6205089e-06
Iter: 1325 loss: 1.62012986e-06
Iter: 1326 loss: 1.62345736e-06
Iter: 1327 loss: 1.62011793e-06
Iter: 1328 loss: 1.61991181e-06
Iter: 1329 loss: 1.61958042e-06
Iter: 1330 loss: 1.6195379e-06
Iter: 1331 loss: 1.61917274e-06
Iter: 1332 loss: 1.62550805e-06
Iter: 1333 loss: 1.61913863e-06
Iter: 1334 loss: 1.6189482e-06
Iter: 1335 loss: 1.61885089e-06
Iter: 1336 loss: 1.61879018e-06
Iter: 1337 loss: 1.61846151e-06
Iter: 1338 loss: 1.61951777e-06
Iter: 1339 loss: 1.61837761e-06
Iter: 1340 loss: 1.61813205e-06
Iter: 1341 loss: 1.61758066e-06
Iter: 1342 loss: 1.62389347e-06
Iter: 1343 loss: 1.61756498e-06
Iter: 1344 loss: 1.61768958e-06
Iter: 1345 loss: 1.61739433e-06
Iter: 1346 loss: 1.61716059e-06
Iter: 1347 loss: 1.61700086e-06
Iter: 1348 loss: 1.6169281e-06
Iter: 1349 loss: 1.61672813e-06
Iter: 1350 loss: 1.61638786e-06
Iter: 1351 loss: 1.61640799e-06
Iter: 1352 loss: 1.61602793e-06
Iter: 1353 loss: 1.61635478e-06
Iter: 1354 loss: 1.6158242e-06
Iter: 1355 loss: 1.6154147e-06
Iter: 1356 loss: 1.61995683e-06
Iter: 1357 loss: 1.61537059e-06
Iter: 1358 loss: 1.61513458e-06
Iter: 1359 loss: 1.61484058e-06
Iter: 1360 loss: 1.61477351e-06
Iter: 1361 loss: 1.61439084e-06
Iter: 1362 loss: 1.61811124e-06
Iter: 1363 loss: 1.61432285e-06
Iter: 1364 loss: 1.61401681e-06
Iter: 1365 loss: 1.61685637e-06
Iter: 1366 loss: 1.61402386e-06
Iter: 1367 loss: 1.61371781e-06
Iter: 1368 loss: 1.61327625e-06
Iter: 1369 loss: 1.62230367e-06
Iter: 1370 loss: 1.61323408e-06
Iter: 1371 loss: 1.61313733e-06
Iter: 1372 loss: 1.61304069e-06
Iter: 1373 loss: 1.61285391e-06
Iter: 1374 loss: 1.61234868e-06
Iter: 1375 loss: 1.61940022e-06
Iter: 1376 loss: 1.61235675e-06
Iter: 1377 loss: 1.61205389e-06
Iter: 1378 loss: 1.61200205e-06
Iter: 1379 loss: 1.61183073e-06
Iter: 1380 loss: 1.6117724e-06
Iter: 1381 loss: 1.61165849e-06
Iter: 1382 loss: 1.61128673e-06
Iter: 1383 loss: 1.61130208e-06
Iter: 1384 loss: 1.61098626e-06
Iter: 1385 loss: 1.61066725e-06
Iter: 1386 loss: 1.61099479e-06
Iter: 1387 loss: 1.610445e-06
Iter: 1388 loss: 1.60999787e-06
Iter: 1389 loss: 1.61318417e-06
Iter: 1390 loss: 1.60995057e-06
Iter: 1391 loss: 1.60956392e-06
Iter: 1392 loss: 1.61262176e-06
Iter: 1393 loss: 1.60960531e-06
Iter: 1394 loss: 1.60936361e-06
Iter: 1395 loss: 1.60990567e-06
Iter: 1396 loss: 1.60918808e-06
Iter: 1397 loss: 1.60893887e-06
Iter: 1398 loss: 1.61036098e-06
Iter: 1399 loss: 1.6089034e-06
Iter: 1400 loss: 1.60865193e-06
Iter: 1401 loss: 1.60928766e-06
Iter: 1402 loss: 1.60861589e-06
Iter: 1403 loss: 1.60841182e-06
Iter: 1404 loss: 1.60825e-06
Iter: 1405 loss: 1.60817581e-06
Iter: 1406 loss: 1.60786897e-06
Iter: 1407 loss: 1.61161711e-06
Iter: 1408 loss: 1.60781292e-06
Iter: 1409 loss: 1.60764887e-06
Iter: 1410 loss: 1.60776449e-06
Iter: 1411 loss: 1.60760112e-06
Iter: 1412 loss: 1.60732611e-06
Iter: 1413 loss: 1.60755405e-06
Iter: 1414 loss: 1.60718014e-06
Iter: 1415 loss: 1.60685818e-06
Iter: 1416 loss: 1.60697959e-06
Iter: 1417 loss: 1.60665627e-06
Iter: 1418 loss: 1.60632294e-06
Iter: 1419 loss: 1.60668287e-06
Iter: 1420 loss: 1.60608465e-06
Iter: 1421 loss: 1.60572051e-06
Iter: 1422 loss: 1.60622471e-06
Iter: 1423 loss: 1.60554248e-06
Iter: 1424 loss: 1.60506727e-06
Iter: 1425 loss: 1.61032017e-06
Iter: 1426 loss: 1.60506693e-06
Iter: 1427 loss: 1.60469028e-06
Iter: 1428 loss: 1.60604714e-06
Iter: 1429 loss: 1.60465129e-06
Iter: 1430 loss: 1.60436457e-06
Iter: 1431 loss: 1.6053217e-06
Iter: 1432 loss: 1.60424111e-06
Iter: 1433 loss: 1.60393006e-06
Iter: 1434 loss: 1.60480477e-06
Iter: 1435 loss: 1.60382376e-06
Iter: 1436 loss: 1.60345098e-06
Iter: 1437 loss: 1.60319939e-06
Iter: 1438 loss: 1.60305115e-06
Iter: 1439 loss: 1.60284139e-06
Iter: 1440 loss: 1.60280968e-06
Iter: 1441 loss: 1.60257616e-06
Iter: 1442 loss: 1.60216291e-06
Iter: 1443 loss: 1.60219986e-06
Iter: 1444 loss: 1.60188733e-06
Iter: 1445 loss: 1.60185687e-06
Iter: 1446 loss: 1.60161835e-06
Iter: 1447 loss: 1.60119066e-06
Iter: 1448 loss: 1.61052833e-06
Iter: 1449 loss: 1.60116747e-06
Iter: 1450 loss: 1.60068953e-06
Iter: 1451 loss: 1.60321986e-06
Iter: 1452 loss: 1.60064633e-06
Iter: 1453 loss: 1.60028492e-06
Iter: 1454 loss: 1.60100899e-06
Iter: 1455 loss: 1.60013974e-06
Iter: 1456 loss: 1.59982505e-06
Iter: 1457 loss: 1.60195873e-06
Iter: 1458 loss: 1.59970489e-06
Iter: 1459 loss: 1.59942579e-06
Iter: 1460 loss: 1.60096215e-06
Iter: 1461 loss: 1.59933688e-06
Iter: 1462 loss: 1.59904891e-06
Iter: 1463 loss: 1.60005402e-06
Iter: 1464 loss: 1.59891761e-06
Iter: 1465 loss: 1.59864396e-06
Iter: 1466 loss: 1.6003105e-06
Iter: 1467 loss: 1.59865419e-06
Iter: 1468 loss: 1.59843012e-06
Iter: 1469 loss: 1.598258e-06
Iter: 1470 loss: 1.59817932e-06
Iter: 1471 loss: 1.59784599e-06
Iter: 1472 loss: 1.6000746e-06
Iter: 1473 loss: 1.59786964e-06
Iter: 1474 loss: 1.5974606e-06
Iter: 1475 loss: 1.59729575e-06
Iter: 1476 loss: 1.59710169e-06
Iter: 1477 loss: 1.59680565e-06
Iter: 1478 loss: 1.60054083e-06
Iter: 1479 loss: 1.59684782e-06
Iter: 1480 loss: 1.59648175e-06
Iter: 1481 loss: 1.59605793e-06
Iter: 1482 loss: 1.59602405e-06
Iter: 1483 loss: 1.59554315e-06
Iter: 1484 loss: 1.59663421e-06
Iter: 1485 loss: 1.59541048e-06
Iter: 1486 loss: 1.5950402e-06
Iter: 1487 loss: 1.59701676e-06
Iter: 1488 loss: 1.59503702e-06
Iter: 1489 loss: 1.59470233e-06
Iter: 1490 loss: 1.59539661e-06
Iter: 1491 loss: 1.59455294e-06
Iter: 1492 loss: 1.59424371e-06
Iter: 1493 loss: 1.59614228e-06
Iter: 1494 loss: 1.59419892e-06
Iter: 1495 loss: 1.59385354e-06
Iter: 1496 loss: 1.59523404e-06
Iter: 1497 loss: 1.59376339e-06
Iter: 1498 loss: 1.59347655e-06
Iter: 1499 loss: 1.59451167e-06
Iter: 1500 loss: 1.59348406e-06
Iter: 1501 loss: 1.59323213e-06
Iter: 1502 loss: 1.59341312e-06
Iter: 1503 loss: 1.59312754e-06
Iter: 1504 loss: 1.59290755e-06
Iter: 1505 loss: 1.59358933e-06
Iter: 1506 loss: 1.5927701e-06
Iter: 1507 loss: 1.59248896e-06
Iter: 1508 loss: 1.59296064e-06
Iter: 1509 loss: 1.59234287e-06
Iter: 1510 loss: 1.59209549e-06
Iter: 1511 loss: 1.5923963e-06
Iter: 1512 loss: 1.59198294e-06
Iter: 1513 loss: 1.59168872e-06
Iter: 1514 loss: 1.59368744e-06
Iter: 1515 loss: 1.59161607e-06
Iter: 1516 loss: 1.59154047e-06
Iter: 1517 loss: 1.59119008e-06
Iter: 1518 loss: 1.59625506e-06
Iter: 1519 loss: 1.5911578e-06
Iter: 1520 loss: 1.59074943e-06
Iter: 1521 loss: 1.59205615e-06
Iter: 1522 loss: 1.59061813e-06
Iter: 1523 loss: 1.59015497e-06
Iter: 1524 loss: 1.59230262e-06
Iter: 1525 loss: 1.59013155e-06
Iter: 1526 loss: 1.5897615e-06
Iter: 1527 loss: 1.59161812e-06
Iter: 1528 loss: 1.58968783e-06
Iter: 1529 loss: 1.58934029e-06
Iter: 1530 loss: 1.59060573e-06
Iter: 1531 loss: 1.58926139e-06
Iter: 1532 loss: 1.58895375e-06
Iter: 1533 loss: 1.59006413e-06
Iter: 1534 loss: 1.58882858e-06
Iter: 1535 loss: 1.58855187e-06
Iter: 1536 loss: 1.58959369e-06
Iter: 1537 loss: 1.58845342e-06
Iter: 1538 loss: 1.58825196e-06
Iter: 1539 loss: 1.58844273e-06
Iter: 1540 loss: 1.58812543e-06
Iter: 1541 loss: 1.58784496e-06
Iter: 1542 loss: 1.58962871e-06
Iter: 1543 loss: 1.58788328e-06
Iter: 1544 loss: 1.58769922e-06
Iter: 1545 loss: 1.58741693e-06
Iter: 1546 loss: 1.58742728e-06
Iter: 1547 loss: 1.58721195e-06
Iter: 1548 loss: 1.58716489e-06
Iter: 1549 loss: 1.5870437e-06
Iter: 1550 loss: 1.58667706e-06
Iter: 1551 loss: 1.5903845e-06
Iter: 1552 loss: 1.58665625e-06
Iter: 1553 loss: 1.58625244e-06
Iter: 1554 loss: 1.58778539e-06
Iter: 1555 loss: 1.58623266e-06
Iter: 1556 loss: 1.58588614e-06
Iter: 1557 loss: 1.58607224e-06
Iter: 1558 loss: 1.58564694e-06
Iter: 1559 loss: 1.58536272e-06
Iter: 1560 loss: 1.58968351e-06
Iter: 1561 loss: 1.58536864e-06
Iter: 1562 loss: 1.58512148e-06
Iter: 1563 loss: 1.58574903e-06
Iter: 1564 loss: 1.58502667e-06
Iter: 1565 loss: 1.58482078e-06
Iter: 1566 loss: 1.58578951e-06
Iter: 1567 loss: 1.58475143e-06
Iter: 1568 loss: 1.58449677e-06
Iter: 1569 loss: 1.58468174e-06
Iter: 1570 loss: 1.58437194e-06
Iter: 1571 loss: 1.58401701e-06
Iter: 1572 loss: 1.58397802e-06
Iter: 1573 loss: 1.58373177e-06
Iter: 1574 loss: 1.58344972e-06
Iter: 1575 loss: 1.58349269e-06
Iter: 1576 loss: 1.58323678e-06
Iter: 1577 loss: 1.58294688e-06
Iter: 1578 loss: 1.58291482e-06
Iter: 1579 loss: 1.58268745e-06
Iter: 1580 loss: 1.58267187e-06
Iter: 1581 loss: 1.58238959e-06
Iter: 1582 loss: 1.58189732e-06
Iter: 1583 loss: 1.59313549e-06
Iter: 1584 loss: 1.58192506e-06
Iter: 1585 loss: 1.58152773e-06
Iter: 1586 loss: 1.58365049e-06
Iter: 1587 loss: 1.58147373e-06
Iter: 1588 loss: 1.58108094e-06
Iter: 1589 loss: 1.58117518e-06
Iter: 1590 loss: 1.5808622e-06
Iter: 1591 loss: 1.58041144e-06
Iter: 1592 loss: 1.58135595e-06
Iter: 1593 loss: 1.58020248e-06
Iter: 1594 loss: 1.57987688e-06
Iter: 1595 loss: 1.57985562e-06
Iter: 1596 loss: 1.57964973e-06
Iter: 1597 loss: 1.58029411e-06
Iter: 1598 loss: 1.57961279e-06
Iter: 1599 loss: 1.57937279e-06
Iter: 1600 loss: 1.57947716e-06
Iter: 1601 loss: 1.57921454e-06
Iter: 1602 loss: 1.57886598e-06
Iter: 1603 loss: 1.5798e-06
Iter: 1604 loss: 1.57878958e-06
Iter: 1605 loss: 1.57845523e-06
Iter: 1606 loss: 1.5789384e-06
Iter: 1607 loss: 1.57833051e-06
Iter: 1608 loss: 1.57797808e-06
Iter: 1609 loss: 1.57870886e-06
Iter: 1610 loss: 1.57773547e-06
Iter: 1611 loss: 1.57743614e-06
Iter: 1612 loss: 1.5773528e-06
Iter: 1613 loss: 1.57712725e-06
Iter: 1614 loss: 1.5766376e-06
Iter: 1615 loss: 1.58310513e-06
Iter: 1616 loss: 1.57663158e-06
Iter: 1617 loss: 1.57632007e-06
Iter: 1618 loss: 1.57587817e-06
Iter: 1619 loss: 1.58240277e-06
Iter: 1620 loss: 1.57574982e-06
Iter: 1621 loss: 1.57525074e-06
Iter: 1622 loss: 1.58134333e-06
Iter: 1623 loss: 1.57520799e-06
Iter: 1624 loss: 1.57481372e-06
Iter: 1625 loss: 1.57442787e-06
Iter: 1626 loss: 1.57434124e-06
Iter: 1627 loss: 1.57373847e-06
Iter: 1628 loss: 1.57989291e-06
Iter: 1629 loss: 1.57368231e-06
Iter: 1630 loss: 1.57315139e-06
Iter: 1631 loss: 1.57524278e-06
Iter: 1632 loss: 1.57297814e-06
Iter: 1633 loss: 1.57268391e-06
Iter: 1634 loss: 1.57500676e-06
Iter: 1635 loss: 1.57267777e-06
Iter: 1636 loss: 1.57233569e-06
Iter: 1637 loss: 1.57235934e-06
Iter: 1638 loss: 1.57206625e-06
Iter: 1639 loss: 1.57168051e-06
Iter: 1640 loss: 1.57418253e-06
Iter: 1641 loss: 1.57172133e-06
Iter: 1642 loss: 1.57145087e-06
Iter: 1643 loss: 1.57163663e-06
Iter: 1644 loss: 1.5713166e-06
Iter: 1645 loss: 1.57097611e-06
Iter: 1646 loss: 1.57227544e-06
Iter: 1647 loss: 1.57085401e-06
Iter: 1648 loss: 1.57057332e-06
Iter: 1649 loss: 1.56998453e-06
Iter: 1650 loss: 1.58110583e-06
Iter: 1651 loss: 1.56993019e-06
Iter: 1652 loss: 1.56954127e-06
Iter: 1653 loss: 1.5694618e-06
Iter: 1654 loss: 1.56920987e-06
Iter: 1655 loss: 1.56862779e-06
Iter: 1656 loss: 1.57994282e-06
Iter: 1657 loss: 1.56863541e-06
Iter: 1658 loss: 1.56818351e-06
Iter: 1659 loss: 1.57106956e-06
Iter: 1660 loss: 1.56816554e-06
Iter: 1661 loss: 1.56759802e-06
Iter: 1662 loss: 1.56676231e-06
Iter: 1663 loss: 1.56670774e-06
Iter: 1664 loss: 1.56611577e-06
Iter: 1665 loss: 1.5661052e-06
Iter: 1666 loss: 1.56549061e-06
Iter: 1667 loss: 1.5665579e-06
Iter: 1668 loss: 1.56521037e-06
Iter: 1669 loss: 1.56483839e-06
Iter: 1670 loss: 1.56730744e-06
Iter: 1671 loss: 1.5648144e-06
Iter: 1672 loss: 1.56435749e-06
Iter: 1673 loss: 1.56433987e-06
Iter: 1674 loss: 1.56398278e-06
Iter: 1675 loss: 1.56361614e-06
Iter: 1676 loss: 1.56339399e-06
Iter: 1677 loss: 1.56328724e-06
Iter: 1678 loss: 1.56322153e-06
Iter: 1679 loss: 1.56300916e-06
Iter: 1680 loss: 1.56284602e-06
Iter: 1681 loss: 1.56232477e-06
Iter: 1682 loss: 1.56812894e-06
Iter: 1683 loss: 1.56230021e-06
Iter: 1684 loss: 1.56159274e-06
Iter: 1685 loss: 1.56936471e-06
Iter: 1686 loss: 1.56162014e-06
Iter: 1687 loss: 1.56129249e-06
Iter: 1688 loss: 1.56168744e-06
Iter: 1689 loss: 1.5611181e-06
Iter: 1690 loss: 1.56066835e-06
Iter: 1691 loss: 1.56123588e-06
Iter: 1692 loss: 1.5603058e-06
Iter: 1693 loss: 1.55991302e-06
Iter: 1694 loss: 1.55952534e-06
Iter: 1695 loss: 1.55938369e-06
Iter: 1696 loss: 1.55856821e-06
Iter: 1697 loss: 1.56395288e-06
Iter: 1698 loss: 1.55851058e-06
Iter: 1699 loss: 1.5580822e-06
Iter: 1700 loss: 1.56219517e-06
Iter: 1701 loss: 1.55807766e-06
Iter: 1702 loss: 1.55764906e-06
Iter: 1703 loss: 1.55710813e-06
Iter: 1704 loss: 1.55707289e-06
Iter: 1705 loss: 1.55682483e-06
Iter: 1706 loss: 1.55673513e-06
Iter: 1707 loss: 1.55645671e-06
Iter: 1708 loss: 1.55576708e-06
Iter: 1709 loss: 1.56253282e-06
Iter: 1710 loss: 1.55566113e-06
Iter: 1711 loss: 1.55522366e-06
Iter: 1712 loss: 1.5552464e-06
Iter: 1713 loss: 1.55465273e-06
Iter: 1714 loss: 1.5544872e-06
Iter: 1715 loss: 1.55419855e-06
Iter: 1716 loss: 1.55362216e-06
Iter: 1717 loss: 1.55603402e-06
Iter: 1718 loss: 1.55358157e-06
Iter: 1719 loss: 1.55293833e-06
Iter: 1720 loss: 1.55558132e-06
Iter: 1721 loss: 1.552801e-06
Iter: 1722 loss: 1.5524073e-06
Iter: 1723 loss: 1.55413954e-06
Iter: 1724 loss: 1.55246653e-06
Iter: 1725 loss: 1.55208954e-06
Iter: 1726 loss: 1.55148439e-06
Iter: 1727 loss: 1.5515277e-06
Iter: 1728 loss: 1.55098553e-06
Iter: 1729 loss: 1.55059809e-06
Iter: 1730 loss: 1.55038288e-06
Iter: 1731 loss: 1.55000055e-06
Iter: 1732 loss: 1.54994541e-06
Iter: 1733 loss: 1.54948668e-06
Iter: 1734 loss: 1.55025953e-06
Iter: 1735 loss: 1.54924192e-06
Iter: 1736 loss: 1.54877921e-06
Iter: 1737 loss: 1.55256294e-06
Iter: 1738 loss: 1.54880013e-06
Iter: 1739 loss: 1.54844315e-06
Iter: 1740 loss: 1.54908685e-06
Iter: 1741 loss: 1.54833856e-06
Iter: 1742 loss: 1.54800478e-06
Iter: 1743 loss: 1.54773829e-06
Iter: 1744 loss: 1.54761562e-06
Iter: 1745 loss: 1.54734244e-06
Iter: 1746 loss: 1.54731947e-06
Iter: 1747 loss: 1.54701843e-06
Iter: 1748 loss: 1.54642419e-06
Iter: 1749 loss: 1.54643146e-06
Iter: 1750 loss: 1.54620011e-06
Iter: 1751 loss: 1.5461751e-06
Iter: 1752 loss: 1.54588793e-06
Iter: 1753 loss: 1.54557119e-06
Iter: 1754 loss: 1.54553072e-06
Iter: 1755 loss: 1.54523605e-06
Iter: 1756 loss: 1.54526242e-06
Iter: 1757 loss: 1.54506142e-06
Iter: 1758 loss: 1.54462305e-06
Iter: 1759 loss: 1.55321572e-06
Iter: 1760 loss: 1.54457666e-06
Iter: 1761 loss: 1.54427732e-06
Iter: 1762 loss: 1.54505119e-06
Iter: 1763 loss: 1.54407803e-06
Iter: 1764 loss: 1.54378199e-06
Iter: 1765 loss: 1.5472956e-06
Iter: 1766 loss: 1.54377244e-06
Iter: 1767 loss: 1.54343513e-06
Iter: 1768 loss: 1.54345503e-06
Iter: 1769 loss: 1.54317229e-06
Iter: 1770 loss: 1.54284726e-06
Iter: 1771 loss: 1.5428875e-06
Iter: 1772 loss: 1.54267696e-06
Iter: 1773 loss: 1.54248846e-06
Iter: 1774 loss: 1.54246845e-06
Iter: 1775 loss: 1.54222482e-06
Iter: 1776 loss: 1.54339295e-06
Iter: 1777 loss: 1.54218537e-06
Iter: 1778 loss: 1.54196528e-06
Iter: 1779 loss: 1.54260056e-06
Iter: 1780 loss: 1.5418367e-06
Iter: 1781 loss: 1.54166958e-06
Iter: 1782 loss: 1.54188717e-06
Iter: 1783 loss: 1.54159193e-06
Iter: 1784 loss: 1.54132567e-06
Iter: 1785 loss: 1.54249744e-06
Iter: 1786 loss: 1.54126008e-06
Iter: 1787 loss: 1.5411058e-06
Iter: 1788 loss: 1.54152906e-06
Iter: 1789 loss: 1.54103043e-06
Iter: 1790 loss: 1.54081135e-06
Iter: 1791 loss: 1.54091117e-06
Iter: 1792 loss: 1.54068357e-06
Iter: 1793 loss: 1.54049087e-06
Iter: 1794 loss: 1.5406946e-06
Iter: 1795 loss: 1.54039935e-06
Iter: 1796 loss: 1.54017835e-06
Iter: 1797 loss: 1.54149757e-06
Iter: 1798 loss: 1.54012446e-06
Iter: 1799 loss: 1.53998349e-06
Iter: 1800 loss: 1.54134091e-06
Iter: 1801 loss: 1.53993767e-06
Iter: 1802 loss: 1.53988708e-06
Iter: 1803 loss: 1.54008399e-06
Iter: 1804 loss: 1.53984502e-06
Iter: 1805 loss: 1.5396497e-06
Iter: 1806 loss: 1.53950782e-06
Iter: 1807 loss: 1.53937822e-06
Iter: 1808 loss: 1.53924645e-06
Iter: 1809 loss: 1.54008558e-06
Iter: 1810 loss: 1.53913231e-06
Iter: 1811 loss: 1.53894757e-06
Iter: 1812 loss: 1.54111513e-06
Iter: 1813 loss: 1.53897304e-06
Iter: 1814 loss: 1.53887186e-06
Iter: 1815 loss: 1.53870587e-06
Iter: 1816 loss: 1.54356667e-06
Iter: 1817 loss: 1.53870656e-06
Iter: 1818 loss: 1.53844951e-06
Iter: 1819 loss: 1.5405634e-06
Iter: 1820 loss: 1.53844496e-06
Iter: 1821 loss: 1.53821907e-06
Iter: 1822 loss: 1.5382e-06
Iter: 1823 loss: 1.53806718e-06
Iter: 1824 loss: 1.53783037e-06
Iter: 1825 loss: 1.5393839e-06
Iter: 1826 loss: 1.53775272e-06
Iter: 1827 loss: 1.53759015e-06
Iter: 1828 loss: 1.53738335e-06
Iter: 1829 loss: 1.53733845e-06
Iter: 1830 loss: 1.53700887e-06
Iter: 1831 loss: 1.53766757e-06
Iter: 1832 loss: 1.53689462e-06
Iter: 1833 loss: 1.5367275e-06
Iter: 1834 loss: 1.53667338e-06
Iter: 1835 loss: 1.53657356e-06
Iter: 1836 loss: 1.53658982e-06
Iter: 1837 loss: 1.53645624e-06
Iter: 1838 loss: 1.53629435e-06
Iter: 1839 loss: 1.53709993e-06
Iter: 1840 loss: 1.53627821e-06
Iter: 1841 loss: 1.5360572e-06
Iter: 1842 loss: 1.53589849e-06
Iter: 1843 loss: 1.5359002e-06
Iter: 1844 loss: 1.53573103e-06
Iter: 1845 loss: 1.53570488e-06
Iter: 1846 loss: 1.53556891e-06
Iter: 1847 loss: 1.5354409e-06
Iter: 1848 loss: 1.53533404e-06
Iter: 1849 loss: 1.53523251e-06
Iter: 1850 loss: 1.53698534e-06
Iter: 1851 loss: 1.53520364e-06
Iter: 1852 loss: 1.53507222e-06
Iter: 1853 loss: 1.53507801e-06
Iter: 1854 loss: 1.53490146e-06
Iter: 1855 loss: 1.53479743e-06
Iter: 1856 loss: 1.53587052e-06
Iter: 1857 loss: 1.53475321e-06
Iter: 1858 loss: 1.53462202e-06
Iter: 1859 loss: 1.53443943e-06
Iter: 1860 loss: 1.53440465e-06
Iter: 1861 loss: 1.53413589e-06
Iter: 1862 loss: 1.53448559e-06
Iter: 1863 loss: 1.53408178e-06
Iter: 1864 loss: 1.53375527e-06
Iter: 1865 loss: 1.53500309e-06
Iter: 1866 loss: 1.53371e-06
Iter: 1867 loss: 1.533572e-06
Iter: 1868 loss: 1.53359497e-06
Iter: 1869 loss: 1.53347321e-06
Iter: 1870 loss: 1.53402652e-06
Iter: 1871 loss: 1.53341921e-06
Iter: 1872 loss: 1.53331689e-06
Iter: 1873 loss: 1.53310839e-06
Iter: 1874 loss: 1.53588769e-06
Iter: 1875 loss: 1.53307155e-06
Iter: 1876 loss: 1.5329058e-06
Iter: 1877 loss: 1.53544738e-06
Iter: 1878 loss: 1.532905e-06
Iter: 1879 loss: 1.5326973e-06
Iter: 1880 loss: 1.53306269e-06
Iter: 1881 loss: 1.53262727e-06
Iter: 1882 loss: 1.53249471e-06
Iter: 1883 loss: 1.53285237e-06
Iter: 1884 loss: 1.53245298e-06
Iter: 1885 loss: 1.53229678e-06
Iter: 1886 loss: 1.53313567e-06
Iter: 1887 loss: 1.53232713e-06
Iter: 1888 loss: 1.53226688e-06
Iter: 1889 loss: 1.53217093e-06
Iter: 1890 loss: 1.53213841e-06
Iter: 1891 loss: 1.53192286e-06
Iter: 1892 loss: 1.53217115e-06
Iter: 1893 loss: 1.53183305e-06
Iter: 1894 loss: 1.53168776e-06
Iter: 1895 loss: 1.53158226e-06
Iter: 1896 loss: 1.53150177e-06
Iter: 1897 loss: 1.53122016e-06
Iter: 1898 loss: 1.53252188e-06
Iter: 1899 loss: 1.53115286e-06
Iter: 1900 loss: 1.53100279e-06
Iter: 1901 loss: 1.53343512e-06
Iter: 1902 loss: 1.53099609e-06
Iter: 1903 loss: 1.5308558e-06
Iter: 1904 loss: 1.53143787e-06
Iter: 1905 loss: 1.53080248e-06
Iter: 1906 loss: 1.53068049e-06
Iter: 1907 loss: 1.5310186e-06
Iter: 1908 loss: 1.53062058e-06
Iter: 1909 loss: 1.53050655e-06
Iter: 1910 loss: 1.53041026e-06
Iter: 1911 loss: 1.53040116e-06
Iter: 1912 loss: 1.53020892e-06
Iter: 1913 loss: 1.53303131e-06
Iter: 1914 loss: 1.53022268e-06
Iter: 1915 loss: 1.53009501e-06
Iter: 1916 loss: 1.52999303e-06
Iter: 1917 loss: 1.53003134e-06
Iter: 1918 loss: 1.52986797e-06
Iter: 1919 loss: 1.52990549e-06
Iter: 1920 loss: 1.52981011e-06
Iter: 1921 loss: 1.52968335e-06
Iter: 1922 loss: 1.52967789e-06
Iter: 1923 loss: 1.52957386e-06
Iter: 1924 loss: 1.53043152e-06
Iter: 1925 loss: 1.52955e-06
Iter: 1926 loss: 1.52943528e-06
Iter: 1927 loss: 1.5292984e-06
Iter: 1928 loss: 1.52929204e-06
Iter: 1929 loss: 1.52909126e-06
Iter: 1930 loss: 1.52999291e-06
Iter: 1931 loss: 1.52908e-06
Iter: 1932 loss: 1.52898281e-06
Iter: 1933 loss: 1.52902555e-06
Iter: 1934 loss: 1.52890141e-06
Iter: 1935 loss: 1.52875805e-06
Iter: 1936 loss: 1.52991151e-06
Iter: 1937 loss: 1.52875418e-06
Iter: 1938 loss: 1.52864732e-06
Iter: 1939 loss: 1.52864015e-06
Iter: 1940 loss: 1.5285392e-06
Iter: 1941 loss: 1.52844643e-06
Iter: 1942 loss: 1.52853283e-06
Iter: 1943 loss: 1.52834946e-06
Iter: 1944 loss: 1.52825146e-06
Iter: 1945 loss: 1.52939e-06
Iter: 1946 loss: 1.52828113e-06
Iter: 1947 loss: 1.52812663e-06
Iter: 1948 loss: 1.52800317e-06
Iter: 1949 loss: 1.52800851e-06
Iter: 1950 loss: 1.52789198e-06
Iter: 1951 loss: 1.52790449e-06
Iter: 1952 loss: 1.52773475e-06
Iter: 1953 loss: 1.52773976e-06
Iter: 1954 loss: 1.52767791e-06
Iter: 1955 loss: 1.52757775e-06
Iter: 1956 loss: 1.52850589e-06
Iter: 1957 loss: 1.52753978e-06
Iter: 1958 loss: 1.52743519e-06
Iter: 1959 loss: 1.52734049e-06
Iter: 1960 loss: 1.52727023e-06
Iter: 1961 loss: 1.52717666e-06
Iter: 1962 loss: 1.527206e-06
Iter: 1963 loss: 1.52697885e-06
Iter: 1964 loss: 1.52678467e-06
Iter: 1965 loss: 1.52901612e-06
Iter: 1966 loss: 1.52673465e-06
Iter: 1967 loss: 1.52662915e-06
Iter: 1968 loss: 1.528e-06
Iter: 1969 loss: 1.5266412e-06
Iter: 1970 loss: 1.52652774e-06
Iter: 1971 loss: 1.52656412e-06
Iter: 1972 loss: 1.52645873e-06
Iter: 1973 loss: 1.52631469e-06
Iter: 1974 loss: 1.52657265e-06
Iter: 1975 loss: 1.52622931e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi0.4
+ date
Sun Nov  8 15:39:40 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.4/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.4/300_100_100_100_1 ']'
+ LOAD='--load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0/300_100_100_100_1'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0/300_100_100_100_1 --function f1 --psi 2 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9da438510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9da541598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9da541488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9da5a8d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9da457400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9da39c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9da3119d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d193f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9da311378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d1940378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9da3ed158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d18bd7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d18bd378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d188a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d190d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d1903c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d18e5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d18e5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d17c0620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d17c0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d1775488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d17c0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d1853730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d16fe950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d16fe8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d175e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d17638c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d1690840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d1690048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d16907b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9da38ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d15c1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d15c1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d15d3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d16bea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe9d1549840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.006838508
test_loss: 0.0070738127
train_loss: 0.0051383087
test_loss: 0.005056471
train_loss: 0.004735452
test_loss: 0.0045564366
train_loss: 0.0042248736
test_loss: 0.004124916
train_loss: 0.0038541064
test_loss: 0.0041447594
train_loss: 0.003716171
test_loss: 0.004057949
train_loss: 0.003813154
test_loss: 0.0039783414
train_loss: 0.0036602141
test_loss: 0.0040560868
train_loss: 0.003653522
test_loss: 0.0038912508
train_loss: 0.0037176113
test_loss: 0.0037618333
train_loss: 0.0039820923
test_loss: 0.0037752078
train_loss: 0.0035753348
test_loss: 0.0037890887
train_loss: 0.003871381
test_loss: 0.0038404898
train_loss: 0.0034586033
test_loss: 0.0036563878
train_loss: 0.0034048986
test_loss: 0.0036958237
train_loss: 0.0035367864
test_loss: 0.003746828
train_loss: 0.0033215259
test_loss: 0.0037484781
train_loss: 0.0033290456
test_loss: 0.0035286695
train_loss: 0.0033484679
test_loss: 0.0034623116
train_loss: 0.0033877855
test_loss: 0.0035232336
train_loss: 0.0034556156
test_loss: 0.0034277374
train_loss: 0.0032466804
test_loss: 0.0035188175
train_loss: 0.0034298254
test_loss: 0.0036995492
train_loss: 0.0033068082
test_loss: 0.003313967
train_loss: 0.0036588265
test_loss: 0.0037146858
train_loss: 0.0033251515
test_loss: 0.003541927
train_loss: 0.0032803281
test_loss: 0.0035001673
train_loss: 0.003223383
test_loss: 0.0034343475
train_loss: 0.0034959544
test_loss: 0.0035828878
train_loss: 0.0034097247
test_loss: 0.0034937393
train_loss: 0.0033343113
test_loss: 0.0033672745
train_loss: 0.0030701393
test_loss: 0.003366504
train_loss: 0.0032928162
test_loss: 0.0034691722
train_loss: 0.0032100198
test_loss: 0.003333531
train_loss: 0.0034252438
test_loss: 0.0033249783
train_loss: 0.0031361836
test_loss: 0.003398702
train_loss: 0.0033678594
test_loss: 0.0037738446
train_loss: 0.003120502
test_loss: 0.0033354794
train_loss: 0.003075881
test_loss: 0.0035675892
train_loss: 0.0030195236
test_loss: 0.0032733968
train_loss: 0.0030646014
test_loss: 0.0032106515
train_loss: 0.0030793475
test_loss: 0.0033473622
train_loss: 0.0030871066
test_loss: 0.0034939165
train_loss: 0.003214564
test_loss: 0.0033411465
train_loss: 0.003079902
test_loss: 0.003272555
train_loss: 0.0032818508
test_loss: 0.0037357837
train_loss: 0.0031113245
test_loss: 0.003593364
train_loss: 0.003225742
test_loss: 0.003522289
train_loss: 0.0031196666
test_loss: 0.0032792478
train_loss: 0.0029640282
test_loss: 0.0034300173
train_loss: 0.0033390434
test_loss: 0.0036568076
train_loss: 0.0031671138
test_loss: 0.003288869
train_loss: 0.003255406
test_loss: 0.0033506264
train_loss: 0.0032389734
test_loss: 0.0034183299
train_loss: 0.003129673
test_loss: 0.0033953073
train_loss: 0.0032068132
test_loss: 0.0032423194
train_loss: 0.0031547842
test_loss: 0.0032368195
train_loss: 0.0034816966
test_loss: 0.003594828
train_loss: 0.0030561578
test_loss: 0.0032992281
train_loss: 0.0030990539
test_loss: 0.0035591978
train_loss: 0.0031306117
test_loss: 0.0032348954
train_loss: 0.0031112873
test_loss: 0.0033233026
train_loss: 0.0031440738
test_loss: 0.0034219637
train_loss: 0.0034089033
test_loss: 0.0035475153
train_loss: 0.0027703135
test_loss: 0.003071296
train_loss: 0.0030129952
test_loss: 0.0031712903
train_loss: 0.0032908672
test_loss: 0.0034721813
train_loss: 0.0030270938
test_loss: 0.0032541072
train_loss: 0.0030444271
test_loss: 0.003184837
train_loss: 0.0030491534
test_loss: 0.0034112378
train_loss: 0.0031664246
test_loss: 0.0035059701
train_loss: 0.0029337793
test_loss: 0.0031324986
train_loss: 0.0030127484
test_loss: 0.0034619612
train_loss: 0.0029063406
test_loss: 0.0033601022
train_loss: 0.002765684
test_loss: 0.0031016567
train_loss: 0.0029423991
test_loss: 0.0033137475
train_loss: 0.0031411098
test_loss: 0.003426628
train_loss: 0.0027125678
test_loss: 0.003073839
train_loss: 0.003017066
test_loss: 0.0032296153
train_loss: 0.0031065985
test_loss: 0.00336507
train_loss: 0.002833982
test_loss: 0.003134753
train_loss: 0.0031470582
test_loss: 0.00319953
train_loss: 0.0030151114
test_loss: 0.003058192
train_loss: 0.0029208004
test_loss: 0.0031441248
train_loss: 0.003088967
test_loss: 0.0032505717
train_loss: 0.003101385
test_loss: 0.0034108313
train_loss: 0.003173275
test_loss: 0.0032613617
train_loss: 0.0029527582
test_loss: 0.0032903347
train_loss: 0.003070986
test_loss: 0.003314539
train_loss: 0.003079703
test_loss: 0.003643622
train_loss: 0.0027094295
test_loss: 0.0031348576
train_loss: 0.0031049522
test_loss: 0.003214943
train_loss: 0.0029829354
test_loss: 0.0033010019
train_loss: 0.0028457534
test_loss: 0.0030468474
train_loss: 0.0031542205
test_loss: 0.0037259874
train_loss: 0.0029409518
test_loss: 0.0031755553
train_loss: 0.002874392
test_loss: 0.003269709
train_loss: 0.0027802414
test_loss: 0.0030569467
train_loss: 0.0027686511
test_loss: 0.0030305567
train_loss: 0.0029069772
test_loss: 0.003074885
train_loss: 0.0031781083
test_loss: 0.0035268322
train_loss: 0.0030075735
test_loss: 0.0031861044
train_loss: 0.0028287387
test_loss: 0.0032233067
train_loss: 0.0030928792
test_loss: 0.00320243
train_loss: 0.0027827509
test_loss: 0.0030930417
train_loss: 0.0029932535
test_loss: 0.0030609288
train_loss: 0.0027431077
test_loss: 0.0030856496
train_loss: 0.003136178
test_loss: 0.0031547525
train_loss: 0.0026970734
test_loss: 0.0032990288
train_loss: 0.0027125622
test_loss: 0.0030289341
train_loss: 0.0028362183
test_loss: 0.003040339
train_loss: 0.0028185728
test_loss: 0.0030533706
train_loss: 0.0028923412
test_loss: 0.0030698231
train_loss: 0.002680564
test_loss: 0.0030809694
train_loss: 0.0028256213
test_loss: 0.003091285
train_loss: 0.002806425
test_loss: 0.002912531
train_loss: 0.0027591244
test_loss: 0.0030695978
train_loss: 0.0027130481
test_loss: 0.002932891
train_loss: 0.0026890894
test_loss: 0.002998596
train_loss: 0.0028283107
test_loss: 0.0031492407
train_loss: 0.0028144305
test_loss: 0.0030413463
train_loss: 0.0031230308
test_loss: 0.0031157401
train_loss: 0.0028022325
test_loss: 0.0031239472
train_loss: 0.0030230363
test_loss: 0.0033710045
train_loss: 0.002821472
test_loss: 0.0029542139
train_loss: 0.002670212
test_loss: 0.0030505224
train_loss: 0.0027298955
test_loss: 0.0029865317
train_loss: 0.0028144151
test_loss: 0.0029358566
train_loss: 0.002792883
test_loss: 0.0032362738
train_loss: 0.0025771102
test_loss: 0.0029409013
train_loss: 0.0026941074
test_loss: 0.003299442
train_loss: 0.0028272327
test_loss: 0.0031708647
train_loss: 0.0027333163
test_loss: 0.0030271928
train_loss: 0.0026930696
test_loss: 0.0033050384
train_loss: 0.003004651
test_loss: 0.0029382957
train_loss: 0.0027896038
test_loss: 0.0034654823
train_loss: 0.0027598375
test_loss: 0.0032280588
train_loss: 0.002775989
test_loss: 0.0030016021
train_loss: 0.0027797031
test_loss: 0.0030314266
train_loss: 0.0027235246
test_loss: 0.0029720673
train_loss: 0.0025215452
test_loss: 0.0028727965
train_loss: 0.002747758
test_loss: 0.003274201
train_loss: 0.0026490944
test_loss: 0.003144199
train_loss: 0.002653696
test_loss: 0.003063304
train_loss: 0.002933899
test_loss: 0.0032779176
train_loss: 0.00292528
test_loss: 0.0031888492
train_loss: 0.0028179088
test_loss: 0.0031337664
train_loss: 0.00272443
test_loss: 0.0030604643
train_loss: 0.0027371857
test_loss: 0.0029864863
train_loss: 0.0025771437
test_loss: 0.0029130967
train_loss: 0.0029763016
test_loss: 0.0031051547
train_loss: 0.0030511739
test_loss: 0.0031479734
train_loss: 0.0027698886
test_loss: 0.003149056
train_loss: 0.002732053
test_loss: 0.0030038473
train_loss: 0.0027846661
test_loss: 0.0029100042
train_loss: 0.0026753875
test_loss: 0.0028445486
train_loss: 0.0024810224
test_loss: 0.0031178088
train_loss: 0.0028266353
test_loss: 0.0031676725
train_loss: 0.002720935
test_loss: 0.0029116808
train_loss: 0.0026764867
test_loss: 0.0029156534
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi0.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi0.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb1bb82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb1bf3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb1bf3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb1b18048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb1b37400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb1b37ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb1a7c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb1a229d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb1a23598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb1a23620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb19ea730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb19bb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb19bb2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb1956620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb1924400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb193de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb18c6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb18c6e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb18b1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7facb18b1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9fad4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9fafdea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9fa45950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9fa73620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9fa73378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9fa0e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9f9ce730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9fa00400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9fa00158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9f9ad598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9f958b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9f9236a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9f917488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9f93fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9f8e46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fac9f88d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.13977094e-05
Iter: 2 loss: 9.70403653e-06
Iter: 3 loss: 9.59978388e-06
Iter: 4 loss: 8.57124905e-06
Iter: 5 loss: 1.21423427e-05
Iter: 6 loss: 8.30062527e-06
Iter: 7 loss: 7.78952108e-06
Iter: 8 loss: 9.35626849e-06
Iter: 9 loss: 7.63914795e-06
Iter: 10 loss: 7.1942095e-06
Iter: 11 loss: 7.36100901e-06
Iter: 12 loss: 6.88411228e-06
Iter: 13 loss: 6.64547952e-06
Iter: 14 loss: 6.60730211e-06
Iter: 15 loss: 6.49677531e-06
Iter: 16 loss: 6.21268646e-06
Iter: 17 loss: 8.56827e-06
Iter: 18 loss: 6.16278885e-06
Iter: 19 loss: 5.83022666e-06
Iter: 20 loss: 7.39930147e-06
Iter: 21 loss: 5.76950697e-06
Iter: 22 loss: 5.45174589e-06
Iter: 23 loss: 9.80285768e-06
Iter: 24 loss: 5.4505781e-06
Iter: 25 loss: 5.33895809e-06
Iter: 26 loss: 5.10239533e-06
Iter: 27 loss: 8.94886216e-06
Iter: 28 loss: 5.09535857e-06
Iter: 29 loss: 4.92894378e-06
Iter: 30 loss: 4.9257028e-06
Iter: 31 loss: 4.72330612e-06
Iter: 32 loss: 4.48952642e-06
Iter: 33 loss: 4.46150625e-06
Iter: 34 loss: 4.29528154e-06
Iter: 35 loss: 4.93544439e-06
Iter: 36 loss: 4.25631424e-06
Iter: 37 loss: 4.17449564e-06
Iter: 38 loss: 4.16053899e-06
Iter: 39 loss: 4.08994447e-06
Iter: 40 loss: 4.02492378e-06
Iter: 41 loss: 4.00793124e-06
Iter: 42 loss: 3.89414072e-06
Iter: 43 loss: 4.08510641e-06
Iter: 44 loss: 3.84294526e-06
Iter: 45 loss: 3.76137882e-06
Iter: 46 loss: 4.46162448e-06
Iter: 47 loss: 3.75684476e-06
Iter: 48 loss: 3.67597409e-06
Iter: 49 loss: 3.74341153e-06
Iter: 50 loss: 3.62779565e-06
Iter: 51 loss: 3.54910139e-06
Iter: 52 loss: 3.40678025e-06
Iter: 53 loss: 6.83569669e-06
Iter: 54 loss: 3.40677252e-06
Iter: 55 loss: 3.3756528e-06
Iter: 56 loss: 3.33860635e-06
Iter: 57 loss: 3.26808595e-06
Iter: 58 loss: 3.28319402e-06
Iter: 59 loss: 3.21611333e-06
Iter: 60 loss: 3.16474711e-06
Iter: 61 loss: 3.11732674e-06
Iter: 62 loss: 3.10497171e-06
Iter: 63 loss: 3.07958499e-06
Iter: 64 loss: 3.05974345e-06
Iter: 65 loss: 3.02401077e-06
Iter: 66 loss: 2.94995334e-06
Iter: 67 loss: 4.23300571e-06
Iter: 68 loss: 2.94828669e-06
Iter: 69 loss: 2.90999265e-06
Iter: 70 loss: 3.42049088e-06
Iter: 71 loss: 2.90982325e-06
Iter: 72 loss: 2.86964246e-06
Iter: 73 loss: 3.01809541e-06
Iter: 74 loss: 2.85983378e-06
Iter: 75 loss: 2.83188137e-06
Iter: 76 loss: 2.87960029e-06
Iter: 77 loss: 2.81937764e-06
Iter: 78 loss: 2.78770653e-06
Iter: 79 loss: 2.78400921e-06
Iter: 80 loss: 2.76125206e-06
Iter: 81 loss: 2.73682326e-06
Iter: 82 loss: 2.73633464e-06
Iter: 83 loss: 2.71531962e-06
Iter: 84 loss: 2.67583937e-06
Iter: 85 loss: 3.55309976e-06
Iter: 86 loss: 2.6757225e-06
Iter: 87 loss: 2.63139441e-06
Iter: 88 loss: 2.65915241e-06
Iter: 89 loss: 2.60296929e-06
Iter: 90 loss: 2.61133118e-06
Iter: 91 loss: 2.58508453e-06
Iter: 92 loss: 2.56969361e-06
Iter: 93 loss: 2.53674489e-06
Iter: 94 loss: 3.06265542e-06
Iter: 95 loss: 2.5356635e-06
Iter: 96 loss: 2.50417952e-06
Iter: 97 loss: 2.53083977e-06
Iter: 98 loss: 2.48556216e-06
Iter: 99 loss: 2.47236653e-06
Iter: 100 loss: 2.463481e-06
Iter: 101 loss: 2.45200613e-06
Iter: 102 loss: 2.42680699e-06
Iter: 103 loss: 2.79188885e-06
Iter: 104 loss: 2.42553779e-06
Iter: 105 loss: 2.40560212e-06
Iter: 106 loss: 2.57762667e-06
Iter: 107 loss: 2.40452346e-06
Iter: 108 loss: 2.37976883e-06
Iter: 109 loss: 2.44179046e-06
Iter: 110 loss: 2.37098493e-06
Iter: 111 loss: 2.3581681e-06
Iter: 112 loss: 2.39827841e-06
Iter: 113 loss: 2.35431526e-06
Iter: 114 loss: 2.34072104e-06
Iter: 115 loss: 2.35641096e-06
Iter: 116 loss: 2.33337778e-06
Iter: 117 loss: 2.32107391e-06
Iter: 118 loss: 2.4315882e-06
Iter: 119 loss: 2.32047614e-06
Iter: 120 loss: 2.30893306e-06
Iter: 121 loss: 2.28546833e-06
Iter: 122 loss: 2.71130966e-06
Iter: 123 loss: 2.28507793e-06
Iter: 124 loss: 2.26637826e-06
Iter: 125 loss: 2.33859919e-06
Iter: 126 loss: 2.26198631e-06
Iter: 127 loss: 2.25678014e-06
Iter: 128 loss: 2.25307872e-06
Iter: 129 loss: 2.24636619e-06
Iter: 130 loss: 2.23054212e-06
Iter: 131 loss: 2.41407292e-06
Iter: 132 loss: 2.22911581e-06
Iter: 133 loss: 2.21482355e-06
Iter: 134 loss: 2.26393149e-06
Iter: 135 loss: 2.21105188e-06
Iter: 136 loss: 2.20165111e-06
Iter: 137 loss: 2.20085508e-06
Iter: 138 loss: 2.19564799e-06
Iter: 139 loss: 2.18117293e-06
Iter: 140 loss: 2.2550953e-06
Iter: 141 loss: 2.17641332e-06
Iter: 142 loss: 2.16819353e-06
Iter: 143 loss: 2.16619105e-06
Iter: 144 loss: 2.15509363e-06
Iter: 145 loss: 2.16169474e-06
Iter: 146 loss: 2.14797501e-06
Iter: 147 loss: 2.14076499e-06
Iter: 148 loss: 2.15872205e-06
Iter: 149 loss: 2.1381768e-06
Iter: 150 loss: 2.12839655e-06
Iter: 151 loss: 2.13672865e-06
Iter: 152 loss: 2.1225369e-06
Iter: 153 loss: 2.11258566e-06
Iter: 154 loss: 2.19838489e-06
Iter: 155 loss: 2.11205543e-06
Iter: 156 loss: 2.10468056e-06
Iter: 157 loss: 2.09356904e-06
Iter: 158 loss: 2.0933353e-06
Iter: 159 loss: 2.08405459e-06
Iter: 160 loss: 2.12616555e-06
Iter: 161 loss: 2.08226675e-06
Iter: 162 loss: 2.07212611e-06
Iter: 163 loss: 2.16593821e-06
Iter: 164 loss: 2.07171638e-06
Iter: 165 loss: 2.06612e-06
Iter: 166 loss: 2.05441575e-06
Iter: 167 loss: 2.24553924e-06
Iter: 168 loss: 2.05406604e-06
Iter: 169 loss: 2.04636649e-06
Iter: 170 loss: 2.16709304e-06
Iter: 171 loss: 2.0464e-06
Iter: 172 loss: 2.03820491e-06
Iter: 173 loss: 2.06697086e-06
Iter: 174 loss: 2.03619379e-06
Iter: 175 loss: 2.03097557e-06
Iter: 176 loss: 2.0213115e-06
Iter: 177 loss: 2.23964639e-06
Iter: 178 loss: 2.02130786e-06
Iter: 179 loss: 2.02459091e-06
Iter: 180 loss: 2.01730063e-06
Iter: 181 loss: 2.01434204e-06
Iter: 182 loss: 2.00720251e-06
Iter: 183 loss: 2.07991616e-06
Iter: 184 loss: 2.00636123e-06
Iter: 185 loss: 1.99874853e-06
Iter: 186 loss: 2.05278e-06
Iter: 187 loss: 1.99812348e-06
Iter: 188 loss: 1.98984435e-06
Iter: 189 loss: 2.00531463e-06
Iter: 190 loss: 1.98620774e-06
Iter: 191 loss: 1.97919871e-06
Iter: 192 loss: 2.03195532e-06
Iter: 193 loss: 1.97869554e-06
Iter: 194 loss: 1.97498457e-06
Iter: 195 loss: 1.967034e-06
Iter: 196 loss: 2.08631309e-06
Iter: 197 loss: 1.96672636e-06
Iter: 198 loss: 1.96299652e-06
Iter: 199 loss: 1.96190808e-06
Iter: 200 loss: 1.95704069e-06
Iter: 201 loss: 1.95652e-06
Iter: 202 loss: 1.95306302e-06
Iter: 203 loss: 1.94927088e-06
Iter: 204 loss: 1.94728295e-06
Iter: 205 loss: 1.94562745e-06
Iter: 206 loss: 1.94012864e-06
Iter: 207 loss: 2.01478974e-06
Iter: 208 loss: 1.94013182e-06
Iter: 209 loss: 1.93558321e-06
Iter: 210 loss: 1.92803418e-06
Iter: 211 loss: 1.92803805e-06
Iter: 212 loss: 1.92319703e-06
Iter: 213 loss: 1.95881216e-06
Iter: 214 loss: 1.92285574e-06
Iter: 215 loss: 1.91786e-06
Iter: 216 loss: 1.96147198e-06
Iter: 217 loss: 1.91767208e-06
Iter: 218 loss: 1.91497475e-06
Iter: 219 loss: 1.90830292e-06
Iter: 220 loss: 1.96746601e-06
Iter: 221 loss: 1.90721312e-06
Iter: 222 loss: 1.90557762e-06
Iter: 223 loss: 1.90363426e-06
Iter: 224 loss: 1.90100104e-06
Iter: 225 loss: 1.8988444e-06
Iter: 226 loss: 1.89805689e-06
Iter: 227 loss: 1.89336788e-06
Iter: 228 loss: 1.91422396e-06
Iter: 229 loss: 1.89238676e-06
Iter: 230 loss: 1.88858121e-06
Iter: 231 loss: 1.88299487e-06
Iter: 232 loss: 1.88286538e-06
Iter: 233 loss: 1.88298293e-06
Iter: 234 loss: 1.88030094e-06
Iter: 235 loss: 1.87807836e-06
Iter: 236 loss: 1.87395426e-06
Iter: 237 loss: 1.964896e-06
Iter: 238 loss: 1.87397291e-06
Iter: 239 loss: 1.87030037e-06
Iter: 240 loss: 1.88282843e-06
Iter: 241 loss: 1.86937132e-06
Iter: 242 loss: 1.86454781e-06
Iter: 243 loss: 1.8857321e-06
Iter: 244 loss: 1.86360398e-06
Iter: 245 loss: 1.86097122e-06
Iter: 246 loss: 1.85657086e-06
Iter: 247 loss: 1.85657382e-06
Iter: 248 loss: 1.85536521e-06
Iter: 249 loss: 1.85445083e-06
Iter: 250 loss: 1.8524845e-06
Iter: 251 loss: 1.84743328e-06
Iter: 252 loss: 1.88725471e-06
Iter: 253 loss: 1.84642056e-06
Iter: 254 loss: 1.8424729e-06
Iter: 255 loss: 1.86059196e-06
Iter: 256 loss: 1.84175587e-06
Iter: 257 loss: 1.83911857e-06
Iter: 258 loss: 1.83887573e-06
Iter: 259 loss: 1.83730856e-06
Iter: 260 loss: 1.83470161e-06
Iter: 261 loss: 1.83471798e-06
Iter: 262 loss: 1.83036968e-06
Iter: 263 loss: 1.8438941e-06
Iter: 264 loss: 1.8290508e-06
Iter: 265 loss: 1.82644942e-06
Iter: 266 loss: 1.82551616e-06
Iter: 267 loss: 1.82411645e-06
Iter: 268 loss: 1.82315512e-06
Iter: 269 loss: 1.82223243e-06
Iter: 270 loss: 1.82082033e-06
Iter: 271 loss: 1.81700716e-06
Iter: 272 loss: 1.84147507e-06
Iter: 273 loss: 1.81605674e-06
Iter: 274 loss: 1.81291534e-06
Iter: 275 loss: 1.81286282e-06
Iter: 276 loss: 1.80955453e-06
Iter: 277 loss: 1.81822043e-06
Iter: 278 loss: 1.8083706e-06
Iter: 279 loss: 1.80620145e-06
Iter: 280 loss: 1.80400616e-06
Iter: 281 loss: 1.80358734e-06
Iter: 282 loss: 1.79944959e-06
Iter: 283 loss: 1.85007059e-06
Iter: 284 loss: 1.79942435e-06
Iter: 285 loss: 1.7977336e-06
Iter: 286 loss: 1.79363224e-06
Iter: 287 loss: 1.83780799e-06
Iter: 288 loss: 1.79314748e-06
Iter: 289 loss: 1.7889738e-06
Iter: 290 loss: 1.78870664e-06
Iter: 291 loss: 1.7855325e-06
Iter: 292 loss: 1.78319203e-06
Iter: 293 loss: 1.78267351e-06
Iter: 294 loss: 1.77982884e-06
Iter: 295 loss: 1.78408436e-06
Iter: 296 loss: 1.77848426e-06
Iter: 297 loss: 1.77651054e-06
Iter: 298 loss: 1.78212349e-06
Iter: 299 loss: 1.77588277e-06
Iter: 300 loss: 1.77374e-06
Iter: 301 loss: 1.77267464e-06
Iter: 302 loss: 1.77163167e-06
Iter: 303 loss: 1.77042011e-06
Iter: 304 loss: 1.77007792e-06
Iter: 305 loss: 1.76871606e-06
Iter: 306 loss: 1.76588742e-06
Iter: 307 loss: 1.8114772e-06
Iter: 308 loss: 1.76577259e-06
Iter: 309 loss: 1.76383696e-06
Iter: 310 loss: 1.79380788e-06
Iter: 311 loss: 1.7638763e-06
Iter: 312 loss: 1.76166077e-06
Iter: 313 loss: 1.7602606e-06
Iter: 314 loss: 1.75943592e-06
Iter: 315 loss: 1.75722084e-06
Iter: 316 loss: 1.75525292e-06
Iter: 317 loss: 1.75464493e-06
Iter: 318 loss: 1.7518264e-06
Iter: 319 loss: 1.77358982e-06
Iter: 320 loss: 1.75161495e-06
Iter: 321 loss: 1.74902311e-06
Iter: 322 loss: 1.78267851e-06
Iter: 323 loss: 1.74893489e-06
Iter: 324 loss: 1.74778029e-06
Iter: 325 loss: 1.74425861e-06
Iter: 326 loss: 1.75725097e-06
Iter: 327 loss: 1.74270167e-06
Iter: 328 loss: 1.73890976e-06
Iter: 329 loss: 1.75790717e-06
Iter: 330 loss: 1.7382215e-06
Iter: 331 loss: 1.73569617e-06
Iter: 332 loss: 1.77248319e-06
Iter: 333 loss: 1.7357205e-06
Iter: 334 loss: 1.73344642e-06
Iter: 335 loss: 1.74355216e-06
Iter: 336 loss: 1.73303624e-06
Iter: 337 loss: 1.73168132e-06
Iter: 338 loss: 1.74223305e-06
Iter: 339 loss: 1.73153387e-06
Iter: 340 loss: 1.73019339e-06
Iter: 341 loss: 1.72897603e-06
Iter: 342 loss: 1.72861075e-06
Iter: 343 loss: 1.7267173e-06
Iter: 344 loss: 1.72727437e-06
Iter: 345 loss: 1.72543355e-06
Iter: 346 loss: 1.72392106e-06
Iter: 347 loss: 1.7236755e-06
Iter: 348 loss: 1.72280545e-06
Iter: 349 loss: 1.72052808e-06
Iter: 350 loss: 1.74074057e-06
Iter: 351 loss: 1.7202027e-06
Iter: 352 loss: 1.71809972e-06
Iter: 353 loss: 1.73391368e-06
Iter: 354 loss: 1.71795739e-06
Iter: 355 loss: 1.71542899e-06
Iter: 356 loss: 1.72578029e-06
Iter: 357 loss: 1.71492366e-06
Iter: 358 loss: 1.71372767e-06
Iter: 359 loss: 1.71207296e-06
Iter: 360 loss: 1.71200986e-06
Iter: 361 loss: 1.71122429e-06
Iter: 362 loss: 1.71071792e-06
Iter: 363 loss: 1.70994599e-06
Iter: 364 loss: 1.70768817e-06
Iter: 365 loss: 1.71342151e-06
Iter: 366 loss: 1.70637531e-06
Iter: 367 loss: 1.70441172e-06
Iter: 368 loss: 1.7044041e-06
Iter: 369 loss: 1.70244448e-06
Iter: 370 loss: 1.70876899e-06
Iter: 371 loss: 1.7019139e-06
Iter: 372 loss: 1.70046292e-06
Iter: 373 loss: 1.70659678e-06
Iter: 374 loss: 1.700192e-06
Iter: 375 loss: 1.69836801e-06
Iter: 376 loss: 1.69735461e-06
Iter: 377 loss: 1.69662439e-06
Iter: 378 loss: 1.6953843e-06
Iter: 379 loss: 1.69537418e-06
Iter: 380 loss: 1.69415125e-06
Iter: 381 loss: 1.69358896e-06
Iter: 382 loss: 1.6929996e-06
Iter: 383 loss: 1.69163957e-06
Iter: 384 loss: 1.68970325e-06
Iter: 385 loss: 1.68963413e-06
Iter: 386 loss: 1.68922952e-06
Iter: 387 loss: 1.68861061e-06
Iter: 388 loss: 1.68753854e-06
Iter: 389 loss: 1.68531119e-06
Iter: 390 loss: 1.7200706e-06
Iter: 391 loss: 1.68518022e-06
Iter: 392 loss: 1.68439988e-06
Iter: 393 loss: 1.68419467e-06
Iter: 394 loss: 1.6830769e-06
Iter: 395 loss: 1.68112547e-06
Iter: 396 loss: 1.68115469e-06
Iter: 397 loss: 1.67943654e-06
Iter: 398 loss: 1.68066845e-06
Iter: 399 loss: 1.67845974e-06
Iter: 400 loss: 1.67704445e-06
Iter: 401 loss: 1.69671557e-06
Iter: 402 loss: 1.67703854e-06
Iter: 403 loss: 1.67552616e-06
Iter: 404 loss: 1.67863118e-06
Iter: 405 loss: 1.67491635e-06
Iter: 406 loss: 1.67348048e-06
Iter: 407 loss: 1.67701262e-06
Iter: 408 loss: 1.67290034e-06
Iter: 409 loss: 1.67145117e-06
Iter: 410 loss: 1.67108499e-06
Iter: 411 loss: 1.67011956e-06
Iter: 412 loss: 1.66960956e-06
Iter: 413 loss: 1.66922291e-06
Iter: 414 loss: 1.66866448e-06
Iter: 415 loss: 1.66714676e-06
Iter: 416 loss: 1.67597318e-06
Iter: 417 loss: 1.66673794e-06
Iter: 418 loss: 1.66525308e-06
Iter: 419 loss: 1.68412396e-06
Iter: 420 loss: 1.66528366e-06
Iter: 421 loss: 1.66367352e-06
Iter: 422 loss: 1.66723953e-06
Iter: 423 loss: 1.66310133e-06
Iter: 424 loss: 1.66208406e-06
Iter: 425 loss: 1.66121936e-06
Iter: 426 loss: 1.66093207e-06
Iter: 427 loss: 1.6594978e-06
Iter: 428 loss: 1.65950178e-06
Iter: 429 loss: 1.65888616e-06
Iter: 430 loss: 1.6572385e-06
Iter: 431 loss: 1.67082703e-06
Iter: 432 loss: 1.65700294e-06
Iter: 433 loss: 1.65525068e-06
Iter: 434 loss: 1.67340488e-06
Iter: 435 loss: 1.65525398e-06
Iter: 436 loss: 1.65371625e-06
Iter: 437 loss: 1.66082168e-06
Iter: 438 loss: 1.65344954e-06
Iter: 439 loss: 1.65248935e-06
Iter: 440 loss: 1.65764595e-06
Iter: 441 loss: 1.65236588e-06
Iter: 442 loss: 1.65115239e-06
Iter: 443 loss: 1.65078586e-06
Iter: 444 loss: 1.65017775e-06
Iter: 445 loss: 1.64903713e-06
Iter: 446 loss: 1.66167911e-06
Iter: 447 loss: 1.64897722e-06
Iter: 448 loss: 1.64786e-06
Iter: 449 loss: 1.6473067e-06
Iter: 450 loss: 1.64675248e-06
Iter: 451 loss: 1.64576386e-06
Iter: 452 loss: 1.64529445e-06
Iter: 453 loss: 1.64479616e-06
Iter: 454 loss: 1.64420953e-06
Iter: 455 loss: 1.64398546e-06
Iter: 456 loss: 1.64311496e-06
Iter: 457 loss: 1.64137271e-06
Iter: 458 loss: 1.67421979e-06
Iter: 459 loss: 1.64131916e-06
Iter: 460 loss: 1.64059566e-06
Iter: 461 loss: 1.64060395e-06
Iter: 462 loss: 1.63962477e-06
Iter: 463 loss: 1.63794471e-06
Iter: 464 loss: 1.67611529e-06
Iter: 465 loss: 1.6379023e-06
Iter: 466 loss: 1.63628852e-06
Iter: 467 loss: 1.63669711e-06
Iter: 468 loss: 1.63512311e-06
Iter: 469 loss: 1.63437426e-06
Iter: 470 loss: 1.63423476e-06
Iter: 471 loss: 1.63326229e-06
Iter: 472 loss: 1.63192749e-06
Iter: 473 loss: 1.63188417e-06
Iter: 474 loss: 1.63049754e-06
Iter: 475 loss: 1.65206006e-06
Iter: 476 loss: 1.63049344e-06
Iter: 477 loss: 1.62946799e-06
Iter: 478 loss: 1.63007303e-06
Iter: 479 loss: 1.62882679e-06
Iter: 480 loss: 1.62798574e-06
Iter: 481 loss: 1.64153857e-06
Iter: 482 loss: 1.62798517e-06
Iter: 483 loss: 1.62733443e-06
Iter: 484 loss: 1.62594097e-06
Iter: 485 loss: 1.64548214e-06
Iter: 486 loss: 1.62583e-06
Iter: 487 loss: 1.62491312e-06
Iter: 488 loss: 1.6248656e-06
Iter: 489 loss: 1.62392644e-06
Iter: 490 loss: 1.62442529e-06
Iter: 491 loss: 1.62338233e-06
Iter: 492 loss: 1.62259198e-06
Iter: 493 loss: 1.62249182e-06
Iter: 494 loss: 1.62192975e-06
Iter: 495 loss: 1.62076913e-06
Iter: 496 loss: 1.62077993e-06
Iter: 497 loss: 1.62026777e-06
Iter: 498 loss: 1.61852608e-06
Iter: 499 loss: 1.62558524e-06
Iter: 500 loss: 1.61787659e-06
Iter: 501 loss: 1.6161132e-06
Iter: 502 loss: 1.62318292e-06
Iter: 503 loss: 1.61570711e-06
Iter: 504 loss: 1.61400544e-06
Iter: 505 loss: 1.61889147e-06
Iter: 506 loss: 1.61347771e-06
Iter: 507 loss: 1.61271976e-06
Iter: 508 loss: 1.61248613e-06
Iter: 509 loss: 1.61196215e-06
Iter: 510 loss: 1.61214825e-06
Iter: 511 loss: 1.61158891e-06
Iter: 512 loss: 1.61072194e-06
Iter: 513 loss: 1.61012917e-06
Iter: 514 loss: 1.60973639e-06
Iter: 515 loss: 1.60888681e-06
Iter: 516 loss: 1.62191247e-06
Iter: 517 loss: 1.60889454e-06
Iter: 518 loss: 1.6079789e-06
Iter: 519 loss: 1.60680315e-06
Iter: 520 loss: 1.60668537e-06
Iter: 521 loss: 1.6056207e-06
Iter: 522 loss: 1.60560558e-06
Iter: 523 loss: 1.60475156e-06
Iter: 524 loss: 1.6042718e-06
Iter: 525 loss: 1.6039844e-06
Iter: 526 loss: 1.60324578e-06
Iter: 527 loss: 1.60143134e-06
Iter: 528 loss: 1.61931848e-06
Iter: 529 loss: 1.60122181e-06
Iter: 530 loss: 1.6003695e-06
Iter: 531 loss: 1.60032766e-06
Iter: 532 loss: 1.59942215e-06
Iter: 533 loss: 1.60059608e-06
Iter: 534 loss: 1.59890556e-06
Iter: 535 loss: 1.59823185e-06
Iter: 536 loss: 1.59675324e-06
Iter: 537 loss: 1.61915648e-06
Iter: 538 loss: 1.59667547e-06
Iter: 539 loss: 1.59523313e-06
Iter: 540 loss: 1.60181605e-06
Iter: 541 loss: 1.59500826e-06
Iter: 542 loss: 1.59415902e-06
Iter: 543 loss: 1.59408e-06
Iter: 544 loss: 1.59333899e-06
Iter: 545 loss: 1.59197589e-06
Iter: 546 loss: 1.62328433e-06
Iter: 547 loss: 1.59197123e-06
Iter: 548 loss: 1.59109811e-06
Iter: 549 loss: 1.59103138e-06
Iter: 550 loss: 1.59044839e-06
Iter: 551 loss: 1.59061096e-06
Iter: 552 loss: 1.58997875e-06
Iter: 553 loss: 1.58919556e-06
Iter: 554 loss: 1.5936464e-06
Iter: 555 loss: 1.58907289e-06
Iter: 556 loss: 1.58843125e-06
Iter: 557 loss: 1.58709508e-06
Iter: 558 loss: 1.61121068e-06
Iter: 559 loss: 1.58709668e-06
Iter: 560 loss: 1.58618195e-06
Iter: 561 loss: 1.59944909e-06
Iter: 562 loss: 1.58614557e-06
Iter: 563 loss: 1.58507396e-06
Iter: 564 loss: 1.58792318e-06
Iter: 565 loss: 1.58476598e-06
Iter: 566 loss: 1.58405305e-06
Iter: 567 loss: 1.58243688e-06
Iter: 568 loss: 1.59930232e-06
Iter: 569 loss: 1.58227613e-06
Iter: 570 loss: 1.58296234e-06
Iter: 571 loss: 1.58170656e-06
Iter: 572 loss: 1.58111584e-06
Iter: 573 loss: 1.58028911e-06
Iter: 574 loss: 1.58029911e-06
Iter: 575 loss: 1.57961404e-06
Iter: 576 loss: 1.58349269e-06
Iter: 577 loss: 1.57950694e-06
Iter: 578 loss: 1.57868953e-06
Iter: 579 loss: 1.57836018e-06
Iter: 580 loss: 1.57802538e-06
Iter: 581 loss: 1.57723912e-06
Iter: 582 loss: 1.58568162e-06
Iter: 583 loss: 1.57719455e-06
Iter: 584 loss: 1.57658019e-06
Iter: 585 loss: 1.57562079e-06
Iter: 586 loss: 1.57560783e-06
Iter: 587 loss: 1.57489046e-06
Iter: 588 loss: 1.58596322e-06
Iter: 589 loss: 1.57490649e-06
Iter: 590 loss: 1.57411603e-06
Iter: 591 loss: 1.57394641e-06
Iter: 592 loss: 1.57342754e-06
Iter: 593 loss: 1.57277816e-06
Iter: 594 loss: 1.57268823e-06
Iter: 595 loss: 1.57219711e-06
Iter: 596 loss: 1.57172587e-06
Iter: 597 loss: 1.57167187e-06
Iter: 598 loss: 1.57129466e-06
Iter: 599 loss: 1.57019326e-06
Iter: 600 loss: 1.57611794e-06
Iter: 601 loss: 1.56991632e-06
Iter: 602 loss: 1.56885267e-06
Iter: 603 loss: 1.5748742e-06
Iter: 604 loss: 1.56880287e-06
Iter: 605 loss: 1.56815315e-06
Iter: 606 loss: 1.56808153e-06
Iter: 607 loss: 1.56780789e-06
Iter: 608 loss: 1.56671138e-06
Iter: 609 loss: 1.57024215e-06
Iter: 610 loss: 1.56618603e-06
Iter: 611 loss: 1.56512874e-06
Iter: 612 loss: 1.56514932e-06
Iter: 613 loss: 1.56410852e-06
Iter: 614 loss: 1.56777037e-06
Iter: 615 loss: 1.56383521e-06
Iter: 616 loss: 1.56325257e-06
Iter: 617 loss: 1.56586918e-06
Iter: 618 loss: 1.56317356e-06
Iter: 619 loss: 1.56249314e-06
Iter: 620 loss: 1.56240583e-06
Iter: 621 loss: 1.56194415e-06
Iter: 622 loss: 1.5613266e-06
Iter: 623 loss: 1.56788747e-06
Iter: 624 loss: 1.56132523e-06
Iter: 625 loss: 1.56076919e-06
Iter: 626 loss: 1.56067813e-06
Iter: 627 loss: 1.56032297e-06
Iter: 628 loss: 1.55970008e-06
Iter: 629 loss: 1.55911403e-06
Iter: 630 loss: 1.55898942e-06
Iter: 631 loss: 1.55862847e-06
Iter: 632 loss: 1.55840212e-06
Iter: 633 loss: 1.55791588e-06
Iter: 634 loss: 1.55681801e-06
Iter: 635 loss: 1.56548424e-06
Iter: 636 loss: 1.5565596e-06
Iter: 637 loss: 1.55582416e-06
Iter: 638 loss: 1.55585337e-06
Iter: 639 loss: 1.55510247e-06
Iter: 640 loss: 1.55871453e-06
Iter: 641 loss: 1.55499583e-06
Iter: 642 loss: 1.5545595e-06
Iter: 643 loss: 1.55332646e-06
Iter: 644 loss: 1.56042961e-06
Iter: 645 loss: 1.55296448e-06
Iter: 646 loss: 1.55187581e-06
Iter: 647 loss: 1.56274939e-06
Iter: 648 loss: 1.55184273e-06
Iter: 649 loss: 1.55110149e-06
Iter: 650 loss: 1.55110615e-06
Iter: 651 loss: 1.55062821e-06
Iter: 652 loss: 1.5499329e-06
Iter: 653 loss: 1.54993927e-06
Iter: 654 loss: 1.54933059e-06
Iter: 655 loss: 1.54926772e-06
Iter: 656 loss: 1.54893496e-06
Iter: 657 loss: 1.54849499e-06
Iter: 658 loss: 1.54849204e-06
Iter: 659 loss: 1.54755912e-06
Iter: 660 loss: 1.55021212e-06
Iter: 661 loss: 1.54735665e-06
Iter: 662 loss: 1.54668351e-06
Iter: 663 loss: 1.5458686e-06
Iter: 664 loss: 1.54576333e-06
Iter: 665 loss: 1.54548866e-06
Iter: 666 loss: 1.54522672e-06
Iter: 667 loss: 1.54475083e-06
Iter: 668 loss: 1.54362988e-06
Iter: 669 loss: 1.55559314e-06
Iter: 670 loss: 1.54359213e-06
Iter: 671 loss: 1.54312613e-06
Iter: 672 loss: 1.54309805e-06
Iter: 673 loss: 1.54245879e-06
Iter: 674 loss: 1.54162501e-06
Iter: 675 loss: 1.54158965e-06
Iter: 676 loss: 1.5407993e-06
Iter: 677 loss: 1.53987548e-06
Iter: 678 loss: 1.53982751e-06
Iter: 679 loss: 1.53850522e-06
Iter: 680 loss: 1.54026236e-06
Iter: 681 loss: 1.53785732e-06
Iter: 682 loss: 1.53683391e-06
Iter: 683 loss: 1.54522206e-06
Iter: 684 loss: 1.53672318e-06
Iter: 685 loss: 1.53605674e-06
Iter: 686 loss: 1.53602764e-06
Iter: 687 loss: 1.53560973e-06
Iter: 688 loss: 1.53448866e-06
Iter: 689 loss: 1.5421092e-06
Iter: 690 loss: 1.53419e-06
Iter: 691 loss: 1.53293286e-06
Iter: 692 loss: 1.53577025e-06
Iter: 693 loss: 1.5323194e-06
Iter: 694 loss: 1.53151734e-06
Iter: 695 loss: 1.53150836e-06
Iter: 696 loss: 1.53071858e-06
Iter: 697 loss: 1.5316632e-06
Iter: 698 loss: 1.53034784e-06
Iter: 699 loss: 1.52961115e-06
Iter: 700 loss: 1.53577798e-06
Iter: 701 loss: 1.52959478e-06
Iter: 702 loss: 1.52903181e-06
Iter: 703 loss: 1.52804455e-06
Iter: 704 loss: 1.55081921e-06
Iter: 705 loss: 1.52808752e-06
Iter: 706 loss: 1.52722737e-06
Iter: 707 loss: 1.5289919e-06
Iter: 708 loss: 1.52693053e-06
Iter: 709 loss: 1.52654582e-06
Iter: 710 loss: 1.52633845e-06
Iter: 711 loss: 1.52604207e-06
Iter: 712 loss: 1.52526536e-06
Iter: 713 loss: 1.53312135e-06
Iter: 714 loss: 1.5251718e-06
Iter: 715 loss: 1.52483585e-06
Iter: 716 loss: 1.52466373e-06
Iter: 717 loss: 1.52437576e-06
Iter: 718 loss: 1.52356279e-06
Iter: 719 loss: 1.52594794e-06
Iter: 720 loss: 1.52314215e-06
Iter: 721 loss: 1.52286225e-06
Iter: 722 loss: 1.52253278e-06
Iter: 723 loss: 1.52204655e-06
Iter: 724 loss: 1.52114296e-06
Iter: 725 loss: 1.5398806e-06
Iter: 726 loss: 1.52114046e-06
Iter: 727 loss: 1.52012876e-06
Iter: 728 loss: 1.52042526e-06
Iter: 729 loss: 1.51936797e-06
Iter: 730 loss: 1.5185708e-06
Iter: 731 loss: 1.53015344e-06
Iter: 732 loss: 1.51856455e-06
Iter: 733 loss: 1.51767199e-06
Iter: 734 loss: 1.52012717e-06
Iter: 735 loss: 1.51740244e-06
Iter: 736 loss: 1.51665381e-06
Iter: 737 loss: 1.51684833e-06
Iter: 738 loss: 1.51612699e-06
Iter: 739 loss: 1.51540712e-06
Iter: 740 loss: 1.51640938e-06
Iter: 741 loss: 1.51510858e-06
Iter: 742 loss: 1.51460029e-06
Iter: 743 loss: 1.51458175e-06
Iter: 744 loss: 1.51406903e-06
Iter: 745 loss: 1.51381744e-06
Iter: 746 loss: 1.51358358e-06
Iter: 747 loss: 1.51326162e-06
Iter: 748 loss: 1.51323115e-06
Iter: 749 loss: 1.51291511e-06
Iter: 750 loss: 1.51214772e-06
Iter: 751 loss: 1.51942254e-06
Iter: 752 loss: 1.51207075e-06
Iter: 753 loss: 1.51107156e-06
Iter: 754 loss: 1.5121185e-06
Iter: 755 loss: 1.51050074e-06
Iter: 756 loss: 1.51011386e-06
Iter: 757 loss: 1.50986205e-06
Iter: 758 loss: 1.50955429e-06
Iter: 759 loss: 1.50871892e-06
Iter: 760 loss: 1.51444635e-06
Iter: 761 loss: 1.50859114e-06
Iter: 762 loss: 1.50763924e-06
Iter: 763 loss: 1.51027314e-06
Iter: 764 loss: 1.5073781e-06
Iter: 765 loss: 1.50704784e-06
Iter: 766 loss: 1.50689925e-06
Iter: 767 loss: 1.50651749e-06
Iter: 768 loss: 1.50625408e-06
Iter: 769 loss: 1.50607934e-06
Iter: 770 loss: 1.50550272e-06
Iter: 771 loss: 1.50750452e-06
Iter: 772 loss: 1.50530582e-06
Iter: 773 loss: 1.50481515e-06
Iter: 774 loss: 1.50410665e-06
Iter: 775 loss: 1.50403741e-06
Iter: 776 loss: 1.50361984e-06
Iter: 777 loss: 1.50340679e-06
Iter: 778 loss: 1.50313167e-06
Iter: 779 loss: 1.50287917e-06
Iter: 780 loss: 1.50278061e-06
Iter: 781 loss: 1.50220285e-06
Iter: 782 loss: 1.50517724e-06
Iter: 783 loss: 1.50211906e-06
Iter: 784 loss: 1.50173219e-06
Iter: 785 loss: 1.50095866e-06
Iter: 786 loss: 1.51570987e-06
Iter: 787 loss: 1.50095934e-06
Iter: 788 loss: 1.50070173e-06
Iter: 789 loss: 1.50056349e-06
Iter: 790 loss: 1.50028382e-06
Iter: 791 loss: 1.50003416e-06
Iter: 792 loss: 1.49995492e-06
Iter: 793 loss: 1.49947698e-06
Iter: 794 loss: 1.49874154e-06
Iter: 795 loss: 1.49875223e-06
Iter: 796 loss: 1.49792265e-06
Iter: 797 loss: 1.50348342e-06
Iter: 798 loss: 1.49786899e-06
Iter: 799 loss: 1.49732045e-06
Iter: 800 loss: 1.50125823e-06
Iter: 801 loss: 1.49732796e-06
Iter: 802 loss: 1.49689936e-06
Iter: 803 loss: 1.4968773e-06
Iter: 804 loss: 1.49650236e-06
Iter: 805 loss: 1.49595633e-06
Iter: 806 loss: 1.49642301e-06
Iter: 807 loss: 1.4955848e-06
Iter: 808 loss: 1.49522816e-06
Iter: 809 loss: 1.4951903e-06
Iter: 810 loss: 1.49479831e-06
Iter: 811 loss: 1.49429434e-06
Iter: 812 loss: 1.49430332e-06
Iter: 813 loss: 1.49386983e-06
Iter: 814 loss: 1.49387245e-06
Iter: 815 loss: 1.49346693e-06
Iter: 816 loss: 1.49270227e-06
Iter: 817 loss: 1.50460346e-06
Iter: 818 loss: 1.49270704e-06
Iter: 819 loss: 1.49197751e-06
Iter: 820 loss: 1.4959802e-06
Iter: 821 loss: 1.49188395e-06
Iter: 822 loss: 1.49110019e-06
Iter: 823 loss: 1.49681796e-06
Iter: 824 loss: 1.49103175e-06
Iter: 825 loss: 1.49067205e-06
Iter: 826 loss: 1.49023e-06
Iter: 827 loss: 1.49022026e-06
Iter: 828 loss: 1.48947868e-06
Iter: 829 loss: 1.49433515e-06
Iter: 830 loss: 1.48944787e-06
Iter: 831 loss: 1.48889308e-06
Iter: 832 loss: 1.48958486e-06
Iter: 833 loss: 1.4886175e-06
Iter: 834 loss: 1.48806862e-06
Iter: 835 loss: 1.49251173e-06
Iter: 836 loss: 1.48803133e-06
Iter: 837 loss: 1.48759318e-06
Iter: 838 loss: 1.48708159e-06
Iter: 839 loss: 1.48702907e-06
Iter: 840 loss: 1.48662275e-06
Iter: 841 loss: 1.48660536e-06
Iter: 842 loss: 1.48619324e-06
Iter: 843 loss: 1.48608558e-06
Iter: 844 loss: 1.48581205e-06
Iter: 845 loss: 1.48540448e-06
Iter: 846 loss: 1.48655965e-06
Iter: 847 loss: 1.48522031e-06
Iter: 848 loss: 1.48456502e-06
Iter: 849 loss: 1.48552931e-06
Iter: 850 loss: 1.48426784e-06
Iter: 851 loss: 1.4837824e-06
Iter: 852 loss: 1.48357117e-06
Iter: 853 loss: 1.48334561e-06
Iter: 854 loss: 1.48291588e-06
Iter: 855 loss: 1.48289087e-06
Iter: 856 loss: 1.48258653e-06
Iter: 857 loss: 1.48206072e-06
Iter: 858 loss: 1.48209097e-06
Iter: 859 loss: 1.48153549e-06
Iter: 860 loss: 1.48779804e-06
Iter: 861 loss: 1.48155789e-06
Iter: 862 loss: 1.48114725e-06
Iter: 863 loss: 1.48128584e-06
Iter: 864 loss: 1.48080551e-06
Iter: 865 loss: 1.48030267e-06
Iter: 866 loss: 1.48337722e-06
Iter: 867 loss: 1.48023446e-06
Iter: 868 loss: 1.47979154e-06
Iter: 869 loss: 1.47947276e-06
Iter: 870 loss: 1.47924266e-06
Iter: 871 loss: 1.47884475e-06
Iter: 872 loss: 1.48469042e-06
Iter: 873 loss: 1.47880303e-06
Iter: 874 loss: 1.47834e-06
Iter: 875 loss: 1.4795296e-06
Iter: 876 loss: 1.47819992e-06
Iter: 877 loss: 1.47774881e-06
Iter: 878 loss: 1.47768333e-06
Iter: 879 loss: 1.47739593e-06
Iter: 880 loss: 1.47684136e-06
Iter: 881 loss: 1.48224149e-06
Iter: 882 loss: 1.47674223e-06
Iter: 883 loss: 1.47648007e-06
Iter: 884 loss: 1.47586138e-06
Iter: 885 loss: 1.48764309e-06
Iter: 886 loss: 1.47585024e-06
Iter: 887 loss: 1.47555147e-06
Iter: 888 loss: 1.47551873e-06
Iter: 889 loss: 1.47518097e-06
Iter: 890 loss: 1.47463174e-06
Iter: 891 loss: 1.47468108e-06
Iter: 892 loss: 1.47409014e-06
Iter: 893 loss: 1.47842843e-06
Iter: 894 loss: 1.4741189e-06
Iter: 895 loss: 1.47360436e-06
Iter: 896 loss: 1.47433411e-06
Iter: 897 loss: 1.47336993e-06
Iter: 898 loss: 1.47290098e-06
Iter: 899 loss: 1.47503806e-06
Iter: 900 loss: 1.47277092e-06
Iter: 901 loss: 1.47239211e-06
Iter: 902 loss: 1.47200569e-06
Iter: 903 loss: 1.47187779e-06
Iter: 904 loss: 1.47125184e-06
Iter: 905 loss: 1.47618312e-06
Iter: 906 loss: 1.47119863e-06
Iter: 907 loss: 1.47069341e-06
Iter: 908 loss: 1.47491073e-06
Iter: 909 loss: 1.47069613e-06
Iter: 910 loss: 1.47030835e-06
Iter: 911 loss: 1.46992761e-06
Iter: 912 loss: 1.46985155e-06
Iter: 913 loss: 1.46937646e-06
Iter: 914 loss: 1.46938942e-06
Iter: 915 loss: 1.46910497e-06
Iter: 916 loss: 1.4684224e-06
Iter: 917 loss: 1.474589e-06
Iter: 918 loss: 1.46830189e-06
Iter: 919 loss: 1.46785976e-06
Iter: 920 loss: 1.4678393e-06
Iter: 921 loss: 1.46719549e-06
Iter: 922 loss: 1.46695515e-06
Iter: 923 loss: 1.46666764e-06
Iter: 924 loss: 1.46609659e-06
Iter: 925 loss: 1.4686841e-06
Iter: 926 loss: 1.46597267e-06
Iter: 927 loss: 1.46533569e-06
Iter: 928 loss: 1.46864352e-06
Iter: 929 loss: 1.4652561e-06
Iter: 930 loss: 1.46474895e-06
Iter: 931 loss: 1.46607647e-06
Iter: 932 loss: 1.46453135e-06
Iter: 933 loss: 1.4641264e-06
Iter: 934 loss: 1.4638764e-06
Iter: 935 loss: 1.46367961e-06
Iter: 936 loss: 1.46308093e-06
Iter: 937 loss: 1.46625757e-06
Iter: 938 loss: 1.46298271e-06
Iter: 939 loss: 1.46256616e-06
Iter: 940 loss: 1.46257344e-06
Iter: 941 loss: 1.46227728e-06
Iter: 942 loss: 1.46171737e-06
Iter: 943 loss: 1.47274147e-06
Iter: 944 loss: 1.46167213e-06
Iter: 945 loss: 1.46124933e-06
Iter: 946 loss: 1.46122795e-06
Iter: 947 loss: 1.460975e-06
Iter: 948 loss: 1.46024945e-06
Iter: 949 loss: 1.47017113e-06
Iter: 950 loss: 1.46023729e-06
Iter: 951 loss: 1.45955767e-06
Iter: 952 loss: 1.45985359e-06
Iter: 953 loss: 1.45919603e-06
Iter: 954 loss: 1.45872434e-06
Iter: 955 loss: 1.45867818e-06
Iter: 956 loss: 1.45828346e-06
Iter: 957 loss: 1.45764352e-06
Iter: 958 loss: 1.4681184e-06
Iter: 959 loss: 1.45763011e-06
Iter: 960 loss: 1.45704189e-06
Iter: 961 loss: 1.46474724e-06
Iter: 962 loss: 1.45705599e-06
Iter: 963 loss: 1.45661647e-06
Iter: 964 loss: 1.45897536e-06
Iter: 965 loss: 1.45648721e-06
Iter: 966 loss: 1.45610147e-06
Iter: 967 loss: 1.45549427e-06
Iter: 968 loss: 1.45545414e-06
Iter: 969 loss: 1.45480021e-06
Iter: 970 loss: 1.45602098e-06
Iter: 971 loss: 1.45448655e-06
Iter: 972 loss: 1.45421041e-06
Iter: 973 loss: 1.4540492e-06
Iter: 974 loss: 1.45376237e-06
Iter: 975 loss: 1.45323793e-06
Iter: 976 loss: 1.46696993e-06
Iter: 977 loss: 1.45325066e-06
Iter: 978 loss: 1.4527709e-06
Iter: 979 loss: 1.45926992e-06
Iter: 980 loss: 1.45275055e-06
Iter: 981 loss: 1.45225249e-06
Iter: 982 loss: 1.45198351e-06
Iter: 983 loss: 1.45178649e-06
Iter: 984 loss: 1.45129934e-06
Iter: 985 loss: 1.45126785e-06
Iter: 986 loss: 1.45088984e-06
Iter: 987 loss: 1.45046727e-06
Iter: 988 loss: 1.45648664e-06
Iter: 989 loss: 1.45043509e-06
Iter: 990 loss: 1.44991111e-06
Iter: 991 loss: 1.45019135e-06
Iter: 992 loss: 1.44960654e-06
Iter: 993 loss: 1.44916476e-06
Iter: 994 loss: 1.44878868e-06
Iter: 995 loss: 1.44869887e-06
Iter: 996 loss: 1.44809007e-06
Iter: 997 loss: 1.45809827e-06
Iter: 998 loss: 1.44803903e-06
Iter: 999 loss: 1.44757928e-06
Iter: 1000 loss: 1.4491435e-06
Iter: 1001 loss: 1.4474499e-06
Iter: 1002 loss: 1.44707201e-06
Iter: 1003 loss: 1.44631406e-06
Iter: 1004 loss: 1.46172977e-06
Iter: 1005 loss: 1.44629405e-06
Iter: 1006 loss: 1.44566854e-06
Iter: 1007 loss: 1.45458648e-06
Iter: 1008 loss: 1.44565331e-06
Iter: 1009 loss: 1.4450676e-06
Iter: 1010 loss: 1.44853084e-06
Iter: 1011 loss: 1.44498847e-06
Iter: 1012 loss: 1.44462638e-06
Iter: 1013 loss: 1.443982e-06
Iter: 1014 loss: 1.44400678e-06
Iter: 1015 loss: 1.44339015e-06
Iter: 1016 loss: 1.44340481e-06
Iter: 1017 loss: 1.44308319e-06
Iter: 1018 loss: 1.44253306e-06
Iter: 1019 loss: 1.44252749e-06
Iter: 1020 loss: 1.44191904e-06
Iter: 1021 loss: 1.44245769e-06
Iter: 1022 loss: 1.44156957e-06
Iter: 1023 loss: 1.44119485e-06
Iter: 1024 loss: 1.44119588e-06
Iter: 1025 loss: 1.44070714e-06
Iter: 1026 loss: 1.44017849e-06
Iter: 1027 loss: 1.44011869e-06
Iter: 1028 loss: 1.43957936e-06
Iter: 1029 loss: 1.44008493e-06
Iter: 1030 loss: 1.43926513e-06
Iter: 1031 loss: 1.43877219e-06
Iter: 1032 loss: 1.43875934e-06
Iter: 1033 loss: 1.43838975e-06
Iter: 1034 loss: 1.43885313e-06
Iter: 1035 loss: 1.43822422e-06
Iter: 1036 loss: 1.4376742e-06
Iter: 1037 loss: 1.43734042e-06
Iter: 1038 loss: 1.43720888e-06
Iter: 1039 loss: 1.43662044e-06
Iter: 1040 loss: 1.43884927e-06
Iter: 1041 loss: 1.4364972e-06
Iter: 1042 loss: 1.43595923e-06
Iter: 1043 loss: 1.43595094e-06
Iter: 1044 loss: 1.43577222e-06
Iter: 1045 loss: 1.4353509e-06
Iter: 1046 loss: 1.44446517e-06
Iter: 1047 loss: 1.43537159e-06
Iter: 1048 loss: 1.43488353e-06
Iter: 1049 loss: 1.43489547e-06
Iter: 1050 loss: 1.43464183e-06
Iter: 1051 loss: 1.43417242e-06
Iter: 1052 loss: 1.43420948e-06
Iter: 1053 loss: 1.43375905e-06
Iter: 1054 loss: 1.43523346e-06
Iter: 1055 loss: 1.43362172e-06
Iter: 1056 loss: 1.433178e-06
Iter: 1057 loss: 1.43941293e-06
Iter: 1058 loss: 1.43317902e-06
Iter: 1059 loss: 1.43280079e-06
Iter: 1060 loss: 1.43211105e-06
Iter: 1061 loss: 1.44630962e-06
Iter: 1062 loss: 1.43207296e-06
Iter: 1063 loss: 1.4313689e-06
Iter: 1064 loss: 1.43532145e-06
Iter: 1065 loss: 1.43124555e-06
Iter: 1066 loss: 1.43035254e-06
Iter: 1067 loss: 1.43384375e-06
Iter: 1068 loss: 1.43022771e-06
Iter: 1069 loss: 1.42959038e-06
Iter: 1070 loss: 1.43157513e-06
Iter: 1071 loss: 1.4293696e-06
Iter: 1072 loss: 1.428725e-06
Iter: 1073 loss: 1.42783199e-06
Iter: 1074 loss: 1.42780164e-06
Iter: 1075 loss: 1.42773274e-06
Iter: 1076 loss: 1.42733279e-06
Iter: 1077 loss: 1.42688077e-06
Iter: 1078 loss: 1.42600584e-06
Iter: 1079 loss: 1.43741943e-06
Iter: 1080 loss: 1.42595343e-06
Iter: 1081 loss: 1.42544e-06
Iter: 1082 loss: 1.42533736e-06
Iter: 1083 loss: 1.42474744e-06
Iter: 1084 loss: 1.42429383e-06
Iter: 1085 loss: 1.42410613e-06
Iter: 1086 loss: 1.42343265e-06
Iter: 1087 loss: 1.42401882e-06
Iter: 1088 loss: 1.42309261e-06
Iter: 1089 loss: 1.42262206e-06
Iter: 1090 loss: 1.42897488e-06
Iter: 1091 loss: 1.42260137e-06
Iter: 1092 loss: 1.42219801e-06
Iter: 1093 loss: 1.42226224e-06
Iter: 1094 loss: 1.42184786e-06
Iter: 1095 loss: 1.42133081e-06
Iter: 1096 loss: 1.42118233e-06
Iter: 1097 loss: 1.42089743e-06
Iter: 1098 loss: 1.42054114e-06
Iter: 1099 loss: 1.42045224e-06
Iter: 1100 loss: 1.42007684e-06
Iter: 1101 loss: 1.41992734e-06
Iter: 1102 loss: 1.41977785e-06
Iter: 1103 loss: 1.41925693e-06
Iter: 1104 loss: 1.42038016e-06
Iter: 1105 loss: 1.41904e-06
Iter: 1106 loss: 1.41858618e-06
Iter: 1107 loss: 1.42004956e-06
Iter: 1108 loss: 1.418495e-06
Iter: 1109 loss: 1.41780197e-06
Iter: 1110 loss: 1.41994167e-06
Iter: 1111 loss: 1.41770192e-06
Iter: 1112 loss: 1.41724922e-06
Iter: 1113 loss: 1.41658097e-06
Iter: 1114 loss: 1.43241311e-06
Iter: 1115 loss: 1.41659939e-06
Iter: 1116 loss: 1.41587202e-06
Iter: 1117 loss: 1.41587395e-06
Iter: 1118 loss: 1.41548753e-06
Iter: 1119 loss: 1.41454711e-06
Iter: 1120 loss: 1.42634826e-06
Iter: 1121 loss: 1.41449755e-06
Iter: 1122 loss: 1.41387738e-06
Iter: 1123 loss: 1.4138576e-06
Iter: 1124 loss: 1.41337887e-06
Iter: 1125 loss: 1.41506564e-06
Iter: 1126 loss: 1.41327291e-06
Iter: 1127 loss: 1.41282476e-06
Iter: 1128 loss: 1.41224859e-06
Iter: 1129 loss: 1.41224223e-06
Iter: 1130 loss: 1.41150122e-06
Iter: 1131 loss: 1.41652527e-06
Iter: 1132 loss: 1.41147768e-06
Iter: 1133 loss: 1.41073474e-06
Iter: 1134 loss: 1.41508826e-06
Iter: 1135 loss: 1.41064152e-06
Iter: 1136 loss: 1.41029113e-06
Iter: 1137 loss: 1.41055943e-06
Iter: 1138 loss: 1.41006763e-06
Iter: 1139 loss: 1.40951113e-06
Iter: 1140 loss: 1.40930058e-06
Iter: 1141 loss: 1.40898237e-06
Iter: 1142 loss: 1.4088173e-06
Iter: 1143 loss: 1.40863813e-06
Iter: 1144 loss: 1.40834356e-06
Iter: 1145 loss: 1.40759971e-06
Iter: 1146 loss: 1.41368423e-06
Iter: 1147 loss: 1.40741531e-06
Iter: 1148 loss: 1.40687303e-06
Iter: 1149 loss: 1.41521127e-06
Iter: 1150 loss: 1.40685484e-06
Iter: 1151 loss: 1.40623115e-06
Iter: 1152 loss: 1.40642032e-06
Iter: 1153 loss: 1.40577163e-06
Iter: 1154 loss: 1.40523025e-06
Iter: 1155 loss: 1.40555198e-06
Iter: 1156 loss: 1.40492614e-06
Iter: 1157 loss: 1.40445673e-06
Iter: 1158 loss: 1.4044731e-06
Iter: 1159 loss: 1.40417501e-06
Iter: 1160 loss: 1.403787e-06
Iter: 1161 loss: 1.40374937e-06
Iter: 1162 loss: 1.40320526e-06
Iter: 1163 loss: 1.40307372e-06
Iter: 1164 loss: 1.4026765e-06
Iter: 1165 loss: 1.40216309e-06
Iter: 1166 loss: 1.40758698e-06
Iter: 1167 loss: 1.40216571e-06
Iter: 1168 loss: 1.40159909e-06
Iter: 1169 loss: 1.40484e-06
Iter: 1170 loss: 1.4014e-06
Iter: 1171 loss: 1.40115947e-06
Iter: 1172 loss: 1.40031477e-06
Iter: 1173 loss: 1.40905991e-06
Iter: 1174 loss: 1.40027419e-06
Iter: 1175 loss: 1.40016266e-06
Iter: 1176 loss: 1.3998731e-06
Iter: 1177 loss: 1.39956887e-06
Iter: 1178 loss: 1.39989675e-06
Iter: 1179 loss: 1.39933036e-06
Iter: 1180 loss: 1.39890517e-06
Iter: 1181 loss: 1.39951908e-06
Iter: 1182 loss: 1.39863255e-06
Iter: 1183 loss: 1.39821509e-06
Iter: 1184 loss: 1.39905069e-06
Iter: 1185 loss: 1.39809401e-06
Iter: 1186 loss: 1.39746783e-06
Iter: 1187 loss: 1.39958547e-06
Iter: 1188 loss: 1.39730128e-06
Iter: 1189 loss: 1.39701194e-06
Iter: 1190 loss: 1.39644908e-06
Iter: 1191 loss: 1.39647045e-06
Iter: 1192 loss: 1.39619681e-06
Iter: 1193 loss: 1.39607721e-06
Iter: 1194 loss: 1.3958047e-06
Iter: 1195 loss: 1.39515828e-06
Iter: 1196 loss: 1.4035154e-06
Iter: 1197 loss: 1.3950895e-06
Iter: 1198 loss: 1.39452277e-06
Iter: 1199 loss: 1.39722761e-06
Iter: 1200 loss: 1.39435451e-06
Iter: 1201 loss: 1.3939989e-06
Iter: 1202 loss: 1.39394865e-06
Iter: 1203 loss: 1.39356848e-06
Iter: 1204 loss: 1.39286794e-06
Iter: 1205 loss: 1.41036412e-06
Iter: 1206 loss: 1.39287658e-06
Iter: 1207 loss: 1.39240205e-06
Iter: 1208 loss: 1.39353779e-06
Iter: 1209 loss: 1.39223198e-06
Iter: 1210 loss: 1.3921333e-06
Iter: 1211 loss: 1.39199972e-06
Iter: 1212 loss: 1.39179485e-06
Iter: 1213 loss: 1.39160022e-06
Iter: 1214 loss: 1.39150234e-06
Iter: 1215 loss: 1.39107647e-06
Iter: 1216 loss: 1.39139843e-06
Iter: 1217 loss: 1.39088047e-06
Iter: 1218 loss: 1.39056874e-06
Iter: 1219 loss: 1.39495467e-06
Iter: 1220 loss: 1.39057761e-06
Iter: 1221 loss: 1.39023007e-06
Iter: 1222 loss: 1.38982432e-06
Iter: 1223 loss: 1.38978749e-06
Iter: 1224 loss: 1.38937548e-06
Iter: 1225 loss: 1.3905285e-06
Iter: 1226 loss: 1.38923167e-06
Iter: 1227 loss: 1.38865801e-06
Iter: 1228 loss: 1.39201461e-06
Iter: 1229 loss: 1.38860742e-06
Iter: 1230 loss: 1.38832911e-06
Iter: 1231 loss: 1.38787846e-06
Iter: 1232 loss: 1.39620624e-06
Iter: 1233 loss: 1.38787232e-06
Iter: 1234 loss: 1.38731036e-06
Iter: 1235 loss: 1.38797316e-06
Iter: 1236 loss: 1.38698192e-06
Iter: 1237 loss: 1.38686664e-06
Iter: 1238 loss: 1.38662074e-06
Iter: 1239 loss: 1.38638825e-06
Iter: 1240 loss: 1.38593055e-06
Iter: 1241 loss: 1.38593441e-06
Iter: 1242 loss: 1.38554549e-06
Iter: 1243 loss: 1.38707367e-06
Iter: 1244 loss: 1.38543987e-06
Iter: 1245 loss: 1.38500127e-06
Iter: 1246 loss: 1.38730809e-06
Iter: 1247 loss: 1.38491237e-06
Iter: 1248 loss: 1.38453765e-06
Iter: 1249 loss: 1.38516134e-06
Iter: 1250 loss: 1.38444352e-06
Iter: 1251 loss: 1.38409337e-06
Iter: 1252 loss: 1.38441578e-06
Iter: 1253 loss: 1.38389373e-06
Iter: 1254 loss: 1.38362805e-06
Iter: 1255 loss: 1.38361497e-06
Iter: 1256 loss: 1.38338737e-06
Iter: 1257 loss: 1.38278165e-06
Iter: 1258 loss: 1.38642326e-06
Iter: 1259 loss: 1.38271139e-06
Iter: 1260 loss: 1.38255177e-06
Iter: 1261 loss: 1.38229689e-06
Iter: 1262 loss: 1.38208338e-06
Iter: 1263 loss: 1.38169776e-06
Iter: 1264 loss: 1.38167616e-06
Iter: 1265 loss: 1.38127643e-06
Iter: 1266 loss: 1.38080327e-06
Iter: 1267 loss: 1.38075927e-06
Iter: 1268 loss: 1.38018618e-06
Iter: 1269 loss: 1.38725545e-06
Iter: 1270 loss: 1.38015741e-06
Iter: 1271 loss: 1.37967231e-06
Iter: 1272 loss: 1.38531209e-06
Iter: 1273 loss: 1.37965878e-06
Iter: 1274 loss: 1.37943312e-06
Iter: 1275 loss: 1.37886627e-06
Iter: 1276 loss: 1.38789335e-06
Iter: 1277 loss: 1.37885013e-06
Iter: 1278 loss: 1.37870359e-06
Iter: 1279 loss: 1.37852294e-06
Iter: 1280 loss: 1.37832399e-06
Iter: 1281 loss: 1.37799088e-06
Iter: 1282 loss: 1.3779395e-06
Iter: 1283 loss: 1.37757206e-06
Iter: 1284 loss: 1.37953589e-06
Iter: 1285 loss: 1.37749498e-06
Iter: 1286 loss: 1.37716506e-06
Iter: 1287 loss: 1.37863378e-06
Iter: 1288 loss: 1.3771695e-06
Iter: 1289 loss: 1.37682434e-06
Iter: 1290 loss: 1.37687266e-06
Iter: 1291 loss: 1.37654683e-06
Iter: 1292 loss: 1.37614097e-06
Iter: 1293 loss: 1.37651659e-06
Iter: 1294 loss: 1.3759601e-06
Iter: 1295 loss: 1.37541349e-06
Iter: 1296 loss: 1.37866982e-06
Iter: 1297 loss: 1.37524489e-06
Iter: 1298 loss: 1.37497557e-06
Iter: 1299 loss: 1.37445318e-06
Iter: 1300 loss: 1.3869394e-06
Iter: 1301 loss: 1.37447012e-06
Iter: 1302 loss: 1.37381267e-06
Iter: 1303 loss: 1.37487109e-06
Iter: 1304 loss: 1.37351685e-06
Iter: 1305 loss: 1.37365112e-06
Iter: 1306 loss: 1.37323275e-06
Iter: 1307 loss: 1.37301652e-06
Iter: 1308 loss: 1.37270365e-06
Iter: 1309 loss: 1.381483e-06
Iter: 1310 loss: 1.37271059e-06
Iter: 1311 loss: 1.37236623e-06
Iter: 1312 loss: 1.37430266e-06
Iter: 1313 loss: 1.3722854e-06
Iter: 1314 loss: 1.37186453e-06
Iter: 1315 loss: 1.37299526e-06
Iter: 1316 loss: 1.37181212e-06
Iter: 1317 loss: 1.37152301e-06
Iter: 1318 loss: 1.37203619e-06
Iter: 1319 loss: 1.37141012e-06
Iter: 1320 loss: 1.37119559e-06
Iter: 1321 loss: 1.37160896e-06
Iter: 1322 loss: 1.37107008e-06
Iter: 1323 loss: 1.37072095e-06
Iter: 1324 loss: 1.37252493e-06
Iter: 1325 loss: 1.37064501e-06
Iter: 1326 loss: 1.37040604e-06
Iter: 1327 loss: 1.37000961e-06
Iter: 1328 loss: 1.37001462e-06
Iter: 1329 loss: 1.36974631e-06
Iter: 1330 loss: 1.36966082e-06
Iter: 1331 loss: 1.36949836e-06
Iter: 1332 loss: 1.36914969e-06
Iter: 1333 loss: 1.37635027e-06
Iter: 1334 loss: 1.36911342e-06
Iter: 1335 loss: 1.36870199e-06
Iter: 1336 loss: 1.36856215e-06
Iter: 1337 loss: 1.36833796e-06
Iter: 1338 loss: 1.36779568e-06
Iter: 1339 loss: 1.37393772e-06
Iter: 1340 loss: 1.36781375e-06
Iter: 1341 loss: 1.36726408e-06
Iter: 1342 loss: 1.37098868e-06
Iter: 1343 loss: 1.36725976e-06
Iter: 1344 loss: 1.3670001e-06
Iter: 1345 loss: 1.36668382e-06
Iter: 1346 loss: 1.36669269e-06
Iter: 1347 loss: 1.36627796e-06
Iter: 1348 loss: 1.36628455e-06
Iter: 1349 loss: 1.36606388e-06
Iter: 1350 loss: 1.36579035e-06
Iter: 1351 loss: 1.36575215e-06
Iter: 1352 loss: 1.3653729e-06
Iter: 1353 loss: 1.36709127e-06
Iter: 1354 loss: 1.36531139e-06
Iter: 1355 loss: 1.36505355e-06
Iter: 1356 loss: 1.3681738e-06
Iter: 1357 loss: 1.36503411e-06
Iter: 1358 loss: 1.36479446e-06
Iter: 1359 loss: 1.36432357e-06
Iter: 1360 loss: 1.37277402e-06
Iter: 1361 loss: 1.36429458e-06
Iter: 1362 loss: 1.3639326e-06
Iter: 1363 loss: 1.36392168e-06
Iter: 1364 loss: 1.36356061e-06
Iter: 1365 loss: 1.36322831e-06
Iter: 1366 loss: 1.36311314e-06
Iter: 1367 loss: 1.36272206e-06
Iter: 1368 loss: 1.36265987e-06
Iter: 1369 loss: 1.36234235e-06
Iter: 1370 loss: 1.36189738e-06
Iter: 1371 loss: 1.36536573e-06
Iter: 1372 loss: 1.36172116e-06
Iter: 1373 loss: 1.36150379e-06
Iter: 1374 loss: 1.3614856e-06
Iter: 1375 loss: 1.36130711e-06
Iter: 1376 loss: 1.36092456e-06
Iter: 1377 loss: 1.36666608e-06
Iter: 1378 loss: 1.36087431e-06
Iter: 1379 loss: 1.36080894e-06
Iter: 1380 loss: 1.36063113e-06
Iter: 1381 loss: 1.36050221e-06
Iter: 1382 loss: 1.36014717e-06
Iter: 1383 loss: 1.3661097e-06
Iter: 1384 loss: 1.36010976e-06
Iter: 1385 loss: 1.35971698e-06
Iter: 1386 loss: 1.36317283e-06
Iter: 1387 loss: 1.35971072e-06
Iter: 1388 loss: 1.35946027e-06
Iter: 1389 loss: 1.35954497e-06
Iter: 1390 loss: 1.35929633e-06
Iter: 1391 loss: 1.35893265e-06
Iter: 1392 loss: 1.36115318e-06
Iter: 1393 loss: 1.35887058e-06
Iter: 1394 loss: 1.35863536e-06
Iter: 1395 loss: 1.35863695e-06
Iter: 1396 loss: 1.35846767e-06
Iter: 1397 loss: 1.35798336e-06
Iter: 1398 loss: 1.3590826e-06
Iter: 1399 loss: 1.35785069e-06
Iter: 1400 loss: 1.35755886e-06
Iter: 1401 loss: 1.35713435e-06
Iter: 1402 loss: 1.35714151e-06
Iter: 1403 loss: 1.35664732e-06
Iter: 1404 loss: 1.35859409e-06
Iter: 1405 loss: 1.35656569e-06
Iter: 1406 loss: 1.35653136e-06
Iter: 1407 loss: 1.35632581e-06
Iter: 1408 loss: 1.35613163e-06
Iter: 1409 loss: 1.35581286e-06
Iter: 1410 loss: 1.35581877e-06
Iter: 1411 loss: 1.35553648e-06
Iter: 1412 loss: 1.35746257e-06
Iter: 1413 loss: 1.35550806e-06
Iter: 1414 loss: 1.3551853e-06
Iter: 1415 loss: 1.35530081e-06
Iter: 1416 loss: 1.35497726e-06
Iter: 1417 loss: 1.35467099e-06
Iter: 1418 loss: 1.35539437e-06
Iter: 1419 loss: 1.35455161e-06
Iter: 1420 loss: 1.35415723e-06
Iter: 1421 loss: 1.35417645e-06
Iter: 1422 loss: 1.35382891e-06
Iter: 1423 loss: 1.35358584e-06
Iter: 1424 loss: 1.35357982e-06
Iter: 1425 loss: 1.35336109e-06
Iter: 1426 loss: 1.35285779e-06
Iter: 1427 loss: 1.36177e-06
Iter: 1428 loss: 1.35283778e-06
Iter: 1429 loss: 1.35254936e-06
Iter: 1430 loss: 1.35252856e-06
Iter: 1431 loss: 1.35228504e-06
Iter: 1432 loss: 1.35171058e-06
Iter: 1433 loss: 1.3547633e-06
Iter: 1434 loss: 1.35151777e-06
Iter: 1435 loss: 1.35077357e-06
Iter: 1436 loss: 1.35219352e-06
Iter: 1437 loss: 1.35044797e-06
Iter: 1438 loss: 1.34997549e-06
Iter: 1439 loss: 1.34991899e-06
Iter: 1440 loss: 1.34953711e-06
Iter: 1441 loss: 1.35293863e-06
Iter: 1442 loss: 1.34944446e-06
Iter: 1443 loss: 1.34921379e-06
Iter: 1444 loss: 1.34880474e-06
Iter: 1445 loss: 1.35633684e-06
Iter: 1446 loss: 1.34879076e-06
Iter: 1447 loss: 1.34841298e-06
Iter: 1448 loss: 1.34837978e-06
Iter: 1449 loss: 1.34813251e-06
Iter: 1450 loss: 1.34744369e-06
Iter: 1451 loss: 1.35145058e-06
Iter: 1452 loss: 1.34725792e-06
Iter: 1453 loss: 1.34675179e-06
Iter: 1454 loss: 1.35159348e-06
Iter: 1455 loss: 1.34672052e-06
Iter: 1456 loss: 1.34638481e-06
Iter: 1457 loss: 1.3462211e-06
Iter: 1458 loss: 1.34605011e-06
Iter: 1459 loss: 1.34561628e-06
Iter: 1460 loss: 1.34811239e-06
Iter: 1461 loss: 1.34555933e-06
Iter: 1462 loss: 1.34517381e-06
Iter: 1463 loss: 1.34552465e-06
Iter: 1464 loss: 1.34494746e-06
Iter: 1465 loss: 1.34463039e-06
Iter: 1466 loss: 1.34460765e-06
Iter: 1467 loss: 1.34430616e-06
Iter: 1468 loss: 1.34370907e-06
Iter: 1469 loss: 1.34996299e-06
Iter: 1470 loss: 1.34361198e-06
Iter: 1471 loss: 1.34301854e-06
Iter: 1472 loss: 1.34448749e-06
Iter: 1473 loss: 1.3427391e-06
Iter: 1474 loss: 1.34243737e-06
Iter: 1475 loss: 1.34245442e-06
Iter: 1476 loss: 1.34217589e-06
Iter: 1477 loss: 1.3418437e-06
Iter: 1478 loss: 1.34191725e-06
Iter: 1479 loss: 1.34163611e-06
Iter: 1480 loss: 1.34160609e-06
Iter: 1481 loss: 1.34134712e-06
Iter: 1482 loss: 1.34079926e-06
Iter: 1483 loss: 1.3480327e-06
Iter: 1484 loss: 1.34072411e-06
Iter: 1485 loss: 1.34030324e-06
Iter: 1486 loss: 1.34031984e-06
Iter: 1487 loss: 1.33998367e-06
Iter: 1488 loss: 1.3400861e-06
Iter: 1489 loss: 1.33972571e-06
Iter: 1490 loss: 1.33944968e-06
Iter: 1491 loss: 1.34032689e-06
Iter: 1492 loss: 1.33933554e-06
Iter: 1493 loss: 1.33905132e-06
Iter: 1494 loss: 1.33982917e-06
Iter: 1495 loss: 1.3389074e-06
Iter: 1496 loss: 1.33861545e-06
Iter: 1497 loss: 1.33955928e-06
Iter: 1498 loss: 1.33860158e-06
Iter: 1499 loss: 1.33823812e-06
Iter: 1500 loss: 1.33987305e-06
Iter: 1501 loss: 1.33821482e-06
Iter: 1502 loss: 1.3379555e-06
Iter: 1503 loss: 1.33740912e-06
Iter: 1504 loss: 1.3437475e-06
Iter: 1505 loss: 1.33735921e-06
Iter: 1506 loss: 1.33694903e-06
Iter: 1507 loss: 1.3369031e-06
Iter: 1508 loss: 1.33647791e-06
Iter: 1509 loss: 1.3364471e-06
Iter: 1510 loss: 1.33615822e-06
Iter: 1511 loss: 1.33577532e-06
Iter: 1512 loss: 1.33612161e-06
Iter: 1513 loss: 1.33560104e-06
Iter: 1514 loss: 1.33509764e-06
Iter: 1515 loss: 1.34184177e-06
Iter: 1516 loss: 1.33506933e-06
Iter: 1517 loss: 1.33488902e-06
Iter: 1518 loss: 1.33455126e-06
Iter: 1519 loss: 1.33456911e-06
Iter: 1520 loss: 1.33426352e-06
Iter: 1521 loss: 1.33427284e-06
Iter: 1522 loss: 1.3340242e-06
Iter: 1523 loss: 1.33372259e-06
Iter: 1524 loss: 1.33365688e-06
Iter: 1525 loss: 1.3332542e-06
Iter: 1526 loss: 1.33540482e-06
Iter: 1527 loss: 1.33322294e-06
Iter: 1528 loss: 1.33276376e-06
Iter: 1529 loss: 1.33258072e-06
Iter: 1530 loss: 1.3324094e-06
Iter: 1531 loss: 1.33194305e-06
Iter: 1532 loss: 1.3357444e-06
Iter: 1533 loss: 1.33194681e-06
Iter: 1534 loss: 1.33140759e-06
Iter: 1535 loss: 1.33286574e-06
Iter: 1536 loss: 1.33125309e-06
Iter: 1537 loss: 1.33094454e-06
Iter: 1538 loss: 1.33061462e-06
Iter: 1539 loss: 1.33057267e-06
Iter: 1540 loss: 1.33007177e-06
Iter: 1541 loss: 1.33645642e-06
Iter: 1542 loss: 1.33003789e-06
Iter: 1543 loss: 1.32973025e-06
Iter: 1544 loss: 1.32933792e-06
Iter: 1545 loss: 1.32929301e-06
Iter: 1546 loss: 1.32882622e-06
Iter: 1547 loss: 1.32910213e-06
Iter: 1548 loss: 1.32859282e-06
Iter: 1549 loss: 1.32850016e-06
Iter: 1550 loss: 1.32831269e-06
Iter: 1551 loss: 1.3281491e-06
Iter: 1552 loss: 1.3277172e-06
Iter: 1553 loss: 1.33222852e-06
Iter: 1554 loss: 1.32769878e-06
Iter: 1555 loss: 1.32741411e-06
Iter: 1556 loss: 1.32738433e-06
Iter: 1557 loss: 1.32717241e-06
Iter: 1558 loss: 1.32703144e-06
Iter: 1559 loss: 1.32697664e-06
Iter: 1560 loss: 1.32663263e-06
Iter: 1561 loss: 1.32849823e-06
Iter: 1562 loss: 1.32666776e-06
Iter: 1563 loss: 1.32639639e-06
Iter: 1564 loss: 1.32720129e-06
Iter: 1565 loss: 1.32628236e-06
Iter: 1566 loss: 1.32606544e-06
Iter: 1567 loss: 1.32779962e-06
Iter: 1568 loss: 1.32605123e-06
Iter: 1569 loss: 1.32591492e-06
Iter: 1570 loss: 1.32548894e-06
Iter: 1571 loss: 1.32783475e-06
Iter: 1572 loss: 1.32535547e-06
Iter: 1573 loss: 1.32474406e-06
Iter: 1574 loss: 1.32854257e-06
Iter: 1575 loss: 1.32475839e-06
Iter: 1576 loss: 1.32399623e-06
Iter: 1577 loss: 1.33024719e-06
Iter: 1578 loss: 1.32399532e-06
Iter: 1579 loss: 1.32365494e-06
Iter: 1580 loss: 1.32315154e-06
Iter: 1581 loss: 1.33572621e-06
Iter: 1582 loss: 1.32320292e-06
Iter: 1583 loss: 1.32259981e-06
Iter: 1584 loss: 1.32271714e-06
Iter: 1585 loss: 1.32213063e-06
Iter: 1586 loss: 1.32149444e-06
Iter: 1587 loss: 1.32743855e-06
Iter: 1588 loss: 1.32142679e-06
Iter: 1589 loss: 1.32129662e-06
Iter: 1590 loss: 1.32113587e-06
Iter: 1591 loss: 1.32097171e-06
Iter: 1592 loss: 1.32043101e-06
Iter: 1593 loss: 1.32260175e-06
Iter: 1594 loss: 1.32018431e-06
Iter: 1595 loss: 1.31987144e-06
Iter: 1596 loss: 1.31979164e-06
Iter: 1597 loss: 1.31947729e-06
Iter: 1598 loss: 1.31942897e-06
Iter: 1599 loss: 1.31921638e-06
Iter: 1600 loss: 1.31872855e-06
Iter: 1601 loss: 1.32116963e-06
Iter: 1602 loss: 1.31864101e-06
Iter: 1603 loss: 1.31828642e-06
Iter: 1604 loss: 1.3177297e-06
Iter: 1605 loss: 1.31774073e-06
Iter: 1606 loss: 1.31721663e-06
Iter: 1607 loss: 1.31722686e-06
Iter: 1608 loss: 1.31672573e-06
Iter: 1609 loss: 1.31674233e-06
Iter: 1610 loss: 1.3163592e-06
Iter: 1611 loss: 1.3160211e-06
Iter: 1612 loss: 1.31602519e-06
Iter: 1613 loss: 1.31582078e-06
Iter: 1614 loss: 1.31535762e-06
Iter: 1615 loss: 1.31864977e-06
Iter: 1616 loss: 1.3152428e-06
Iter: 1617 loss: 1.31463673e-06
Iter: 1618 loss: 1.3157894e-06
Iter: 1619 loss: 1.31439356e-06
Iter: 1620 loss: 1.3139711e-06
Iter: 1621 loss: 1.31979243e-06
Iter: 1622 loss: 1.31397098e-06
Iter: 1623 loss: 1.31344746e-06
Iter: 1624 loss: 1.3135982e-06
Iter: 1625 loss: 1.31310253e-06
Iter: 1626 loss: 1.31276329e-06
Iter: 1627 loss: 1.31226784e-06
Iter: 1628 loss: 1.31223271e-06
Iter: 1629 loss: 1.31196907e-06
Iter: 1630 loss: 1.31180093e-06
Iter: 1631 loss: 1.3115615e-06
Iter: 1632 loss: 1.31104719e-06
Iter: 1633 loss: 1.32264802e-06
Iter: 1634 loss: 1.31103877e-06
Iter: 1635 loss: 1.31037518e-06
Iter: 1636 loss: 1.314614e-06
Iter: 1637 loss: 1.31029572e-06
Iter: 1638 loss: 1.30992328e-06
Iter: 1639 loss: 1.30956892e-06
Iter: 1640 loss: 1.30949786e-06
Iter: 1641 loss: 1.30892909e-06
Iter: 1642 loss: 1.30890203e-06
Iter: 1643 loss: 1.30855506e-06
Iter: 1644 loss: 1.30830358e-06
Iter: 1645 loss: 1.3082182e-06
Iter: 1646 loss: 1.30793626e-06
Iter: 1647 loss: 1.30794933e-06
Iter: 1648 loss: 1.30765011e-06
Iter: 1649 loss: 1.30714477e-06
Iter: 1650 loss: 1.30714636e-06
Iter: 1651 loss: 1.30681883e-06
Iter: 1652 loss: 1.30681747e-06
Iter: 1653 loss: 1.3063916e-06
Iter: 1654 loss: 1.30771923e-06
Iter: 1655 loss: 1.30634e-06
Iter: 1656 loss: 1.3060029e-06
Iter: 1657 loss: 1.30571902e-06
Iter: 1658 loss: 1.30563853e-06
Iter: 1659 loss: 1.30519584e-06
Iter: 1660 loss: 1.3079158e-06
Iter: 1661 loss: 1.30519129e-06
Iter: 1662 loss: 1.3047495e-06
Iter: 1663 loss: 1.30734395e-06
Iter: 1664 loss: 1.30467185e-06
Iter: 1665 loss: 1.30443937e-06
Iter: 1666 loss: 1.30442322e-06
Iter: 1667 loss: 1.30425519e-06
Iter: 1668 loss: 1.30381204e-06
Iter: 1669 loss: 1.30445142e-06
Iter: 1670 loss: 1.30365152e-06
Iter: 1671 loss: 1.30336502e-06
Iter: 1672 loss: 1.30575245e-06
Iter: 1673 loss: 1.30333513e-06
Iter: 1674 loss: 1.30298167e-06
Iter: 1675 loss: 1.30271962e-06
Iter: 1676 loss: 1.30258013e-06
Iter: 1677 loss: 1.30226431e-06
Iter: 1678 loss: 1.30419346e-06
Iter: 1679 loss: 1.30220519e-06
Iter: 1680 loss: 1.3018107e-06
Iter: 1681 loss: 1.30326134e-06
Iter: 1682 loss: 1.30179217e-06
Iter: 1683 loss: 1.30151489e-06
Iter: 1684 loss: 1.30119076e-06
Iter: 1685 loss: 1.30112892e-06
Iter: 1686 loss: 1.30076478e-06
Iter: 1687 loss: 1.30074432e-06
Iter: 1688 loss: 1.30047476e-06
Iter: 1689 loss: 1.30005628e-06
Iter: 1690 loss: 1.31000411e-06
Iter: 1691 loss: 1.30003332e-06
Iter: 1692 loss: 1.29969203e-06
Iter: 1693 loss: 1.30202102e-06
Iter: 1694 loss: 1.29961472e-06
Iter: 1695 loss: 1.2993454e-06
Iter: 1696 loss: 1.30245041e-06
Iter: 1697 loss: 1.29933937e-06
Iter: 1698 loss: 1.29912428e-06
Iter: 1699 loss: 1.29880982e-06
Iter: 1700 loss: 1.29875139e-06
Iter: 1701 loss: 1.29840623e-06
Iter: 1702 loss: 1.30068645e-06
Iter: 1703 loss: 1.29836485e-06
Iter: 1704 loss: 1.2980953e-06
Iter: 1705 loss: 1.29835325e-06
Iter: 1706 loss: 1.29795171e-06
Iter: 1707 loss: 1.29750595e-06
Iter: 1708 loss: 1.2993504e-06
Iter: 1709 loss: 1.29742466e-06
Iter: 1710 loss: 1.2971775e-06
Iter: 1711 loss: 1.29690227e-06
Iter: 1712 loss: 1.29684327e-06
Iter: 1713 loss: 1.29645548e-06
Iter: 1714 loss: 1.30236208e-06
Iter: 1715 loss: 1.29644627e-06
Iter: 1716 loss: 1.29616478e-06
Iter: 1717 loss: 1.29570026e-06
Iter: 1718 loss: 1.29566547e-06
Iter: 1719 loss: 1.29537602e-06
Iter: 1720 loss: 1.29536522e-06
Iter: 1721 loss: 1.29513455e-06
Iter: 1722 loss: 1.29468503e-06
Iter: 1723 loss: 1.30322564e-06
Iter: 1724 loss: 1.29471937e-06
Iter: 1725 loss: 1.29429441e-06
Iter: 1726 loss: 1.29585464e-06
Iter: 1727 loss: 1.2942005e-06
Iter: 1728 loss: 1.2938001e-06
Iter: 1729 loss: 1.29383898e-06
Iter: 1730 loss: 1.29364196e-06
Iter: 1731 loss: 1.29353725e-06
Iter: 1732 loss: 1.29336274e-06
Iter: 1733 loss: 1.29302668e-06
Iter: 1734 loss: 1.29446857e-06
Iter: 1735 loss: 1.29297746e-06
Iter: 1736 loss: 1.29261252e-06
Iter: 1737 loss: 1.2927203e-06
Iter: 1738 loss: 1.29231159e-06
Iter: 1739 loss: 1.29187276e-06
Iter: 1740 loss: 1.29650255e-06
Iter: 1741 loss: 1.29184855e-06
Iter: 1742 loss: 1.29155546e-06
Iter: 1743 loss: 1.29103069e-06
Iter: 1744 loss: 1.30388696e-06
Iter: 1745 loss: 1.29103216e-06
Iter: 1746 loss: 1.29061266e-06
Iter: 1747 loss: 1.29061391e-06
Iter: 1748 loss: 1.29029422e-06
Iter: 1749 loss: 1.28955526e-06
Iter: 1750 loss: 1.30206479e-06
Iter: 1751 loss: 1.28957015e-06
Iter: 1752 loss: 1.28936517e-06
Iter: 1753 loss: 1.28923443e-06
Iter: 1754 loss: 1.28882516e-06
Iter: 1755 loss: 1.28859642e-06
Iter: 1756 loss: 1.28847012e-06
Iter: 1757 loss: 1.28813372e-06
Iter: 1758 loss: 1.28838246e-06
Iter: 1759 loss: 1.2879018e-06
Iter: 1760 loss: 1.2876435e-06
Iter: 1761 loss: 1.28758734e-06
Iter: 1762 loss: 1.28741112e-06
Iter: 1763 loss: 1.2871717e-06
Iter: 1764 loss: 1.28712406e-06
Iter: 1765 loss: 1.28665124e-06
Iter: 1766 loss: 1.28736815e-06
Iter: 1767 loss: 1.2863751e-06
Iter: 1768 loss: 1.28604972e-06
Iter: 1769 loss: 1.28594115e-06
Iter: 1770 loss: 1.28564807e-06
Iter: 1771 loss: 1.28555769e-06
Iter: 1772 loss: 1.28542115e-06
Iter: 1773 loss: 1.28524357e-06
Iter: 1774 loss: 1.28487272e-06
Iter: 1775 loss: 1.29316493e-06
Iter: 1776 loss: 1.28488557e-06
Iter: 1777 loss: 1.28457737e-06
Iter: 1778 loss: 1.28458737e-06
Iter: 1779 loss: 1.28430486e-06
Iter: 1780 loss: 1.28416389e-06
Iter: 1781 loss: 1.28407078e-06
Iter: 1782 loss: 1.28368617e-06
Iter: 1783 loss: 1.28300519e-06
Iter: 1784 loss: 1.29399882e-06
Iter: 1785 loss: 1.28294096e-06
Iter: 1786 loss: 1.28274928e-06
Iter: 1787 loss: 1.28267436e-06
Iter: 1788 loss: 1.28229226e-06
Iter: 1789 loss: 1.28376178e-06
Iter: 1790 loss: 1.28221245e-06
Iter: 1791 loss: 1.28203692e-06
Iter: 1792 loss: 1.28155489e-06
Iter: 1793 loss: 1.28452598e-06
Iter: 1794 loss: 1.28140482e-06
Iter: 1795 loss: 1.28104671e-06
Iter: 1796 loss: 1.28101703e-06
Iter: 1797 loss: 1.28069325e-06
Iter: 1798 loss: 1.28400097e-06
Iter: 1799 loss: 1.28067336e-06
Iter: 1800 loss: 1.28047475e-06
Iter: 1801 loss: 1.28064141e-06
Iter: 1802 loss: 1.28030774e-06
Iter: 1803 loss: 1.27999976e-06
Iter: 1804 loss: 1.28007275e-06
Iter: 1805 loss: 1.27971657e-06
Iter: 1806 loss: 1.27934231e-06
Iter: 1807 loss: 1.28037357e-06
Iter: 1808 loss: 1.27918065e-06
Iter: 1809 loss: 1.27873955e-06
Iter: 1810 loss: 1.28130944e-06
Iter: 1811 loss: 1.2786636e-06
Iter: 1812 loss: 1.2783521e-06
Iter: 1813 loss: 1.27789531e-06
Iter: 1814 loss: 1.27788508e-06
Iter: 1815 loss: 1.27752082e-06
Iter: 1816 loss: 1.27752435e-06
Iter: 1817 loss: 1.277254e-06
Iter: 1818 loss: 1.27678936e-06
Iter: 1819 loss: 1.28355487e-06
Iter: 1820 loss: 1.27680687e-06
Iter: 1821 loss: 1.27637168e-06
Iter: 1822 loss: 1.27923704e-06
Iter: 1823 loss: 1.27636361e-06
Iter: 1824 loss: 1.27578176e-06
Iter: 1825 loss: 1.27692272e-06
Iter: 1826 loss: 1.27552312e-06
Iter: 1827 loss: 1.27526039e-06
Iter: 1828 loss: 1.27512203e-06
Iter: 1829 loss: 1.27499857e-06
Iter: 1830 loss: 1.27473027e-06
Iter: 1831 loss: 1.27821249e-06
Iter: 1832 loss: 1.27471162e-06
Iter: 1833 loss: 1.27430269e-06
Iter: 1834 loss: 1.27423436e-06
Iter: 1835 loss: 1.27402632e-06
Iter: 1836 loss: 1.27357589e-06
Iter: 1837 loss: 1.27653038e-06
Iter: 1838 loss: 1.27353121e-06
Iter: 1839 loss: 1.27326166e-06
Iter: 1840 loss: 1.27306566e-06
Iter: 1841 loss: 1.27293265e-06
Iter: 1842 loss: 1.27275973e-06
Iter: 1843 loss: 1.27276746e-06
Iter: 1844 loss: 1.27256772e-06
Iter: 1845 loss: 1.27219369e-06
Iter: 1846 loss: 1.27221278e-06
Iter: 1847 loss: 1.27202316e-06
Iter: 1848 loss: 1.27195653e-06
Iter: 1849 loss: 1.27175e-06
Iter: 1850 loss: 1.27141334e-06
Iter: 1851 loss: 1.27912358e-06
Iter: 1852 loss: 1.27140765e-06
Iter: 1853 loss: 1.2710027e-06
Iter: 1854 loss: 1.27175292e-06
Iter: 1855 loss: 1.27078476e-06
Iter: 1856 loss: 1.27069802e-06
Iter: 1857 loss: 1.27058456e-06
Iter: 1858 loss: 1.27041881e-06
Iter: 1859 loss: 1.26998316e-06
Iter: 1860 loss: 1.27285807e-06
Iter: 1861 loss: 1.26989357e-06
Iter: 1862 loss: 1.26960788e-06
Iter: 1863 loss: 1.26962107e-06
Iter: 1864 loss: 1.26940938e-06
Iter: 1865 loss: 1.26964619e-06
Iter: 1866 loss: 1.26924988e-06
Iter: 1867 loss: 1.26900022e-06
Iter: 1868 loss: 1.26969701e-06
Iter: 1869 loss: 1.26890563e-06
Iter: 1870 loss: 1.26857549e-06
Iter: 1871 loss: 1.26865223e-06
Iter: 1872 loss: 1.26832629e-06
Iter: 1873 loss: 1.26803184e-06
Iter: 1874 loss: 1.26898033e-06
Iter: 1875 loss: 1.26792247e-06
Iter: 1876 loss: 1.26754389e-06
Iter: 1877 loss: 1.26959026e-06
Iter: 1878 loss: 1.26750183e-06
Iter: 1879 loss: 1.2672914e-06
Iter: 1880 loss: 1.2669193e-06
Iter: 1881 loss: 1.26690247e-06
Iter: 1882 loss: 1.26640566e-06
Iter: 1883 loss: 1.27268061e-06
Iter: 1884 loss: 1.26640293e-06
Iter: 1885 loss: 1.26617454e-06
Iter: 1886 loss: 1.26573491e-06
Iter: 1887 loss: 1.2751907e-06
Iter: 1888 loss: 1.2657631e-06
Iter: 1889 loss: 1.2655438e-06
Iter: 1890 loss: 1.26550708e-06
Iter: 1891 loss: 1.265327e-06
Iter: 1892 loss: 1.26503642e-06
Iter: 1893 loss: 1.26500515e-06
Iter: 1894 loss: 1.26472037e-06
Iter: 1895 loss: 1.26574969e-06
Iter: 1896 loss: 1.26472253e-06
Iter: 1897 loss: 1.26450914e-06
Iter: 1898 loss: 1.2645271e-06
Iter: 1899 loss: 1.26437067e-06
Iter: 1900 loss: 1.26409645e-06
Iter: 1901 loss: 1.2640462e-06
Iter: 1902 loss: 1.26385874e-06
Iter: 1903 loss: 1.26345685e-06
Iter: 1904 loss: 1.26755276e-06
Iter: 1905 loss: 1.26345276e-06
Iter: 1906 loss: 1.26317013e-06
Iter: 1907 loss: 1.26285818e-06
Iter: 1908 loss: 1.26277018e-06
Iter: 1909 loss: 1.2625552e-06
Iter: 1910 loss: 1.26509258e-06
Iter: 1911 loss: 1.26259704e-06
Iter: 1912 loss: 1.26230839e-06
Iter: 1913 loss: 1.26214161e-06
Iter: 1914 loss: 1.2620103e-06
Iter: 1915 loss: 1.26169357e-06
Iter: 1916 loss: 1.26198688e-06
Iter: 1917 loss: 1.26153907e-06
Iter: 1918 loss: 1.26141481e-06
Iter: 1919 loss: 1.26138298e-06
Iter: 1920 loss: 1.26118084e-06
Iter: 1921 loss: 1.26088696e-06
Iter: 1922 loss: 1.26826831e-06
Iter: 1923 loss: 1.26089526e-06
Iter: 1924 loss: 1.26071939e-06
Iter: 1925 loss: 1.26067584e-06
Iter: 1926 loss: 1.260516e-06
Iter: 1927 loss: 1.26039197e-06
Iter: 1928 loss: 1.26035434e-06
Iter: 1929 loss: 1.26017653e-06
Iter: 1930 loss: 1.26152645e-06
Iter: 1931 loss: 1.26016153e-06
Iter: 1932 loss: 1.25994563e-06
Iter: 1933 loss: 1.25980591e-06
Iter: 1934 loss: 1.25972394e-06
Iter: 1935 loss: 1.25945417e-06
Iter: 1936 loss: 1.26003476e-06
Iter: 1937 loss: 1.25936504e-06
Iter: 1938 loss: 1.25910208e-06
Iter: 1939 loss: 1.26087559e-06
Iter: 1940 loss: 1.25904148e-06
Iter: 1941 loss: 1.25882252e-06
Iter: 1942 loss: 1.25890881e-06
Iter: 1943 loss: 1.25873112e-06
Iter: 1944 loss: 1.25858924e-06
Iter: 1945 loss: 1.25859958e-06
Iter: 1946 loss: 1.25844576e-06
Iter: 1947 loss: 1.25811107e-06
Iter: 1948 loss: 1.26164241e-06
Iter: 1949 loss: 1.2580756e-06
Iter: 1950 loss: 1.25768452e-06
Iter: 1951 loss: 1.25831298e-06
Iter: 1952 loss: 1.25751615e-06
Iter: 1953 loss: 1.257333e-06
Iter: 1954 loss: 1.25733129e-06
Iter: 1955 loss: 1.25705787e-06
Iter: 1956 loss: 1.25673637e-06
Iter: 1957 loss: 1.25673319e-06
Iter: 1958 loss: 1.25646079e-06
Iter: 1959 loss: 1.2582409e-06
Iter: 1960 loss: 1.25641554e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.4/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi0.8
+ date
Sun Nov  8 16:47:55 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.8/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.8/300_100_100_100_1 ']'
+ LOAD='--load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.4/300_100_100_100_1'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.4/300_100_100_100_1 --function f1 --psi 2 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.8/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08e0c76a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08e0c77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08e197d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08e0eeae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08e0dad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08dfcdf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08e089c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08dfcdd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08df4f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08df4f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08dfa7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08ded6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08dee18c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08e00fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08df1dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08df26bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc08df47400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc06c9bbb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc06c950ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc06c950ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc06ca1c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc06c990a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc06c99b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0481946a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc048194a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0481f1378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc06c9278c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc06c9288c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc06c9287b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0480d1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc04810c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc04809a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc04809b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc04809a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc04803ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc048070e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0041641467
test_loss: 0.004008673
train_loss: 0.0035974556
test_loss: 0.0035062903
train_loss: 0.0031791565
test_loss: 0.0034917654
train_loss: 0.0034274017
test_loss: 0.0035417022
train_loss: 0.0031121778
test_loss: 0.0037364373
train_loss: 0.0030840277
test_loss: 0.003408077
train_loss: 0.0031015936
test_loss: 0.003379436
train_loss: 0.0033675414
test_loss: 0.0036326002
train_loss: 0.0030639542
test_loss: 0.0034908256
train_loss: 0.0031411038
test_loss: 0.003363359
train_loss: 0.0030629775
test_loss: 0.0035093536
train_loss: 0.003185182
test_loss: 0.0034309912
train_loss: 0.0028789064
test_loss: 0.003409074
train_loss: 0.0027053277
test_loss: 0.0033037786
train_loss: 0.0030004557
test_loss: 0.003507028
train_loss: 0.003040484
test_loss: 0.003561616
train_loss: 0.0029392815
test_loss: 0.0032774033
train_loss: 0.0029189943
test_loss: 0.0034029244
train_loss: 0.0030380615
test_loss: 0.0033956638
train_loss: 0.0029593313
test_loss: 0.0033281338
train_loss: 0.002889791
test_loss: 0.003521802
train_loss: 0.0030033598
test_loss: 0.003377319
train_loss: 0.0031338609
test_loss: 0.0035030562
train_loss: 0.0032279487
test_loss: 0.0034400157
train_loss: 0.002874753
test_loss: 0.0033471743
train_loss: 0.003030397
test_loss: 0.003521099
train_loss: 0.002875303
test_loss: 0.0032960435
train_loss: 0.002896036
test_loss: 0.0033698273
train_loss: 0.0032468694
test_loss: 0.0035006793
train_loss: 0.0029880079
test_loss: 0.0036424636
train_loss: 0.0030540805
test_loss: 0.0033772744
train_loss: 0.003043159
test_loss: 0.0033358512
train_loss: 0.0032916623
test_loss: 0.0033189044
train_loss: 0.0032138983
test_loss: 0.0032993534
train_loss: 0.0029426394
test_loss: 0.003352512
train_loss: 0.003051646
test_loss: 0.0032828888
train_loss: 0.0034231963
test_loss: 0.0033306049
train_loss: 0.0028753402
test_loss: 0.0032769446
train_loss: 0.0030614897
test_loss: 0.003476971
train_loss: 0.0029671877
test_loss: 0.0032297322
train_loss: 0.0031444137
test_loss: 0.003282558
train_loss: 0.0028467735
test_loss: 0.0031615363
train_loss: 0.0031160978
test_loss: 0.0034632736
train_loss: 0.002819207
test_loss: 0.003159933
train_loss: 0.0029711528
test_loss: 0.0032328474
train_loss: 0.0029885822
test_loss: 0.0035768747
train_loss: 0.002674204
test_loss: 0.003210091
train_loss: 0.0030644238
test_loss: 0.0032648207
train_loss: 0.002892324
test_loss: 0.0032612642
train_loss: 0.0030496984
test_loss: 0.003274016
train_loss: 0.0027945507
test_loss: 0.003132729
train_loss: 0.0029475123
test_loss: 0.0033299287
train_loss: 0.003005832
test_loss: 0.00323795
train_loss: 0.002900593
test_loss: 0.0033428888
train_loss: 0.0028722151
test_loss: 0.003192628
train_loss: 0.0028636386
test_loss: 0.003150466
train_loss: 0.0029595178
test_loss: 0.0035029044
train_loss: 0.0030008634
test_loss: 0.00317484
train_loss: 0.0029015588
test_loss: 0.0032551517
train_loss: 0.0027990066
test_loss: 0.0035584455
train_loss: 0.0032291985
test_loss: 0.0032966502
train_loss: 0.0027359806
test_loss: 0.003099753
train_loss: 0.0029685285
test_loss: 0.0031671077
train_loss: 0.0029870106
test_loss: 0.0034263863
train_loss: 0.0027628671
test_loss: 0.0033275152
train_loss: 0.003027915
test_loss: 0.0032546306
train_loss: 0.002623282
test_loss: 0.0030567497
train_loss: 0.0030677458
test_loss: 0.0032553112
train_loss: 0.0026700152
test_loss: 0.0032634526
train_loss: 0.0029554926
test_loss: 0.0031953452
train_loss: 0.0027790517
test_loss: 0.0032415648
train_loss: 0.0027535826
test_loss: 0.0033320563
train_loss: 0.0028596846
test_loss: 0.0031823458
train_loss: 0.0026597055
test_loss: 0.0030814575
train_loss: 0.0027736684
test_loss: 0.0032186287
train_loss: 0.0027811406
test_loss: 0.0032110862
train_loss: 0.0027846568
test_loss: 0.003334218
train_loss: 0.0025978158
test_loss: 0.0030684727
train_loss: 0.0026078657
test_loss: 0.0030822337
train_loss: 0.0028761502
test_loss: 0.003245993
train_loss: 0.0027902557
test_loss: 0.0033832064
train_loss: 0.0028014297
test_loss: 0.0031910716
train_loss: 0.0026690708
test_loss: 0.003140293
train_loss: 0.0025980533
test_loss: 0.0031801343
train_loss: 0.0027915752
test_loss: 0.0031583058
train_loss: 0.0027759715
test_loss: 0.0033115668
train_loss: 0.0027672865
test_loss: 0.0035022853
train_loss: 0.0027934038
test_loss: 0.003239416
train_loss: 0.0030373458
test_loss: 0.0031894047
train_loss: 0.003030267
test_loss: 0.0034796696
train_loss: 0.0029283194
test_loss: 0.0032436573
train_loss: 0.0026412697
test_loss: 0.0030415973
train_loss: 0.0026303895
test_loss: 0.003119105
train_loss: 0.0027811525
test_loss: 0.003112183
train_loss: 0.0026765594
test_loss: 0.0032020318
train_loss: 0.0027968073
test_loss: 0.0031938874
train_loss: 0.002715073
test_loss: 0.003266606
train_loss: 0.002659965
test_loss: 0.0031179
train_loss: 0.002770539
test_loss: 0.0030310757
train_loss: 0.0026713938
test_loss: 0.0031557127
train_loss: 0.002810261
test_loss: 0.0032619387
train_loss: 0.0028648241
test_loss: 0.0032135998
train_loss: 0.0026649707
test_loss: 0.003134639
train_loss: 0.0026381994
test_loss: 0.0030607549
train_loss: 0.002859623
test_loss: 0.0033032023
train_loss: 0.002820992
test_loss: 0.0031736244
train_loss: 0.002698889
test_loss: 0.0030915986
train_loss: 0.0027137864
test_loss: 0.0031683238
train_loss: 0.0026869604
test_loss: 0.00304562
train_loss: 0.0026830693
test_loss: 0.0029867052
train_loss: 0.0028849053
test_loss: 0.003153294
train_loss: 0.0027581295
test_loss: 0.0032242525
train_loss: 0.002796752
test_loss: 0.0031187893
train_loss: 0.002707987
test_loss: 0.0032409232
train_loss: 0.0026458392
test_loss: 0.0030847352
train_loss: 0.0028241528
test_loss: 0.003078161
train_loss: 0.0025348929
test_loss: 0.0029837892
train_loss: 0.002577487
test_loss: 0.0029494914
train_loss: 0.0027037845
test_loss: 0.0031526217
train_loss: 0.0027764705
test_loss: 0.0031097261
train_loss: 0.0026788104
test_loss: 0.003174795
train_loss: 0.0026950468
test_loss: 0.0031582855
train_loss: 0.002897272
test_loss: 0.0032390412
train_loss: 0.0027824985
test_loss: 0.00312972
train_loss: 0.0025741295
test_loss: 0.003014312
train_loss: 0.0025381036
test_loss: 0.0029827983
train_loss: 0.0027569062
test_loss: 0.0032976244
train_loss: 0.0027092914
test_loss: 0.00307618
train_loss: 0.0027374893
test_loss: 0.0030690397
train_loss: 0.0025950803
test_loss: 0.003162669
train_loss: 0.0026727272
test_loss: 0.0031396972
train_loss: 0.0026227343
test_loss: 0.0031366171
train_loss: 0.0025468601
test_loss: 0.0030534703
train_loss: 0.002649264
test_loss: 0.0030654322
train_loss: 0.0028646006
test_loss: 0.00320405
train_loss: 0.0028125555
test_loss: 0.003216275
train_loss: 0.0026469615
test_loss: 0.0030455955
train_loss: 0.0026682676
test_loss: 0.0029963667
train_loss: 0.002655163
test_loss: 0.0031167842
train_loss: 0.0026552677
test_loss: 0.0031036336
train_loss: 0.00247389
test_loss: 0.0029368731
train_loss: 0.0026037467
test_loss: 0.003257164
train_loss: 0.0026707489
test_loss: 0.003094484
train_loss: 0.0024218338
test_loss: 0.0030098085
train_loss: 0.002733667
test_loss: 0.0031987184
train_loss: 0.0025785961
test_loss: 0.0030743843
train_loss: 0.0025778704
test_loss: 0.0030127906
train_loss: 0.0025560062
test_loss: 0.0029994894
train_loss: 0.0026922298
test_loss: 0.0031477513
train_loss: 0.00259587
test_loss: 0.003111448
train_loss: 0.0023403312
test_loss: 0.0030036923
train_loss: 0.0026650603
test_loss: 0.0030566934
train_loss: 0.0026959516
test_loss: 0.0031592003
train_loss: 0.0025043222
test_loss: 0.0030080588
train_loss: 0.002847784
test_loss: 0.003237509
train_loss: 0.002786188
test_loss: 0.0030461722
train_loss: 0.0025617513
test_loss: 0.0031905272
train_loss: 0.0024740966
test_loss: 0.0029689125
train_loss: 0.0026188132
test_loss: 0.0030327048
train_loss: 0.002648627
test_loss: 0.0030003511
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi0.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.8/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi0.8/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b4541e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b4541e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b45409f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b453ef048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b452a0ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b452a0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b45287bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b45287950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b45238620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b452382f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b451fb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b451bdd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b451b7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b4518a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b451399d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b45152d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b4514f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b4514f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b450c69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b450c6ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b45080ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b450c6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b44fe0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b45004048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b45004620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b44fa7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b44f6c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b44f6c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b44f90620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b44f90950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b23530ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b234e1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b234f6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b2351ebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b234be8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2b234bcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.04794617e-05
Iter: 2 loss: 9.74093291e-06
Iter: 3 loss: 8.83981102e-06
Iter: 4 loss: 8.33676677e-06
Iter: 5 loss: 7.555926e-06
Iter: 6 loss: 7.54536086e-06
Iter: 7 loss: 7.14001862e-06
Iter: 8 loss: 7.10253698e-06
Iter: 9 loss: 6.80263e-06
Iter: 10 loss: 6.27704958e-06
Iter: 11 loss: 6.27700911e-06
Iter: 12 loss: 6.17466685e-06
Iter: 13 loss: 6.1053679e-06
Iter: 14 loss: 5.9605859e-06
Iter: 15 loss: 5.7946354e-06
Iter: 16 loss: 5.77412766e-06
Iter: 17 loss: 5.56492432e-06
Iter: 18 loss: 6.99489647e-06
Iter: 19 loss: 5.54492362e-06
Iter: 20 loss: 5.34668743e-06
Iter: 21 loss: 5.13681698e-06
Iter: 22 loss: 5.10155769e-06
Iter: 23 loss: 5.00517217e-06
Iter: 24 loss: 4.94675e-06
Iter: 25 loss: 4.85804094e-06
Iter: 26 loss: 4.66218125e-06
Iter: 27 loss: 7.49222681e-06
Iter: 28 loss: 4.65293351e-06
Iter: 29 loss: 4.4719809e-06
Iter: 30 loss: 4.40643362e-06
Iter: 31 loss: 4.30567525e-06
Iter: 32 loss: 4.14433771e-06
Iter: 33 loss: 4.85286819e-06
Iter: 34 loss: 4.11198425e-06
Iter: 35 loss: 4.0017303e-06
Iter: 36 loss: 3.90562127e-06
Iter: 37 loss: 3.87638602e-06
Iter: 38 loss: 3.70971134e-06
Iter: 39 loss: 3.90767809e-06
Iter: 40 loss: 3.62166702e-06
Iter: 41 loss: 3.69787745e-06
Iter: 42 loss: 3.55475345e-06
Iter: 43 loss: 3.5185958e-06
Iter: 44 loss: 3.4319462e-06
Iter: 45 loss: 4.3899e-06
Iter: 46 loss: 3.42314115e-06
Iter: 47 loss: 3.35541881e-06
Iter: 48 loss: 3.35440245e-06
Iter: 49 loss: 3.29388149e-06
Iter: 50 loss: 3.21016228e-06
Iter: 51 loss: 3.20652407e-06
Iter: 52 loss: 3.13143164e-06
Iter: 53 loss: 3.25533233e-06
Iter: 54 loss: 3.09715892e-06
Iter: 55 loss: 3.03509205e-06
Iter: 56 loss: 3.03459728e-06
Iter: 57 loss: 2.99283329e-06
Iter: 58 loss: 2.99948169e-06
Iter: 59 loss: 2.96124836e-06
Iter: 60 loss: 2.89387162e-06
Iter: 61 loss: 3.2432622e-06
Iter: 62 loss: 2.88312253e-06
Iter: 63 loss: 2.84835323e-06
Iter: 64 loss: 2.82021597e-06
Iter: 65 loss: 2.80992936e-06
Iter: 66 loss: 2.78411085e-06
Iter: 67 loss: 2.7794797e-06
Iter: 68 loss: 2.75213051e-06
Iter: 69 loss: 2.70147984e-06
Iter: 70 loss: 3.85676231e-06
Iter: 71 loss: 2.70141504e-06
Iter: 72 loss: 2.65984158e-06
Iter: 73 loss: 2.80119866e-06
Iter: 74 loss: 2.64871414e-06
Iter: 75 loss: 2.61939294e-06
Iter: 76 loss: 2.61856167e-06
Iter: 77 loss: 2.6036887e-06
Iter: 78 loss: 2.57089755e-06
Iter: 79 loss: 3.04177547e-06
Iter: 80 loss: 2.56929661e-06
Iter: 81 loss: 2.53457165e-06
Iter: 82 loss: 2.53451549e-06
Iter: 83 loss: 2.51518372e-06
Iter: 84 loss: 2.48224524e-06
Iter: 85 loss: 2.48217748e-06
Iter: 86 loss: 2.44872808e-06
Iter: 87 loss: 2.48515516e-06
Iter: 88 loss: 2.43044906e-06
Iter: 89 loss: 2.40661757e-06
Iter: 90 loss: 2.40432109e-06
Iter: 91 loss: 2.38694156e-06
Iter: 92 loss: 2.40796044e-06
Iter: 93 loss: 2.37779341e-06
Iter: 94 loss: 2.35461243e-06
Iter: 95 loss: 2.43215345e-06
Iter: 96 loss: 2.34835306e-06
Iter: 97 loss: 2.33457399e-06
Iter: 98 loss: 2.31132503e-06
Iter: 99 loss: 2.31131912e-06
Iter: 100 loss: 2.29575016e-06
Iter: 101 loss: 2.29229272e-06
Iter: 102 loss: 2.27859164e-06
Iter: 103 loss: 2.25470421e-06
Iter: 104 loss: 2.25471967e-06
Iter: 105 loss: 2.23759571e-06
Iter: 106 loss: 2.349836e-06
Iter: 107 loss: 2.23588631e-06
Iter: 108 loss: 2.21705568e-06
Iter: 109 loss: 2.31198987e-06
Iter: 110 loss: 2.21400569e-06
Iter: 111 loss: 2.20296602e-06
Iter: 112 loss: 2.19379308e-06
Iter: 113 loss: 2.19065623e-06
Iter: 114 loss: 2.17243542e-06
Iter: 115 loss: 2.38934422e-06
Iter: 116 loss: 2.17212755e-06
Iter: 117 loss: 2.16432704e-06
Iter: 118 loss: 2.14938177e-06
Iter: 119 loss: 2.4726869e-06
Iter: 120 loss: 2.14934425e-06
Iter: 121 loss: 2.13044063e-06
Iter: 122 loss: 2.1433334e-06
Iter: 123 loss: 2.11850238e-06
Iter: 124 loss: 2.11385714e-06
Iter: 125 loss: 2.10781809e-06
Iter: 126 loss: 2.10030566e-06
Iter: 127 loss: 2.09862969e-06
Iter: 128 loss: 2.09365976e-06
Iter: 129 loss: 2.08029678e-06
Iter: 130 loss: 2.11696124e-06
Iter: 131 loss: 2.07588982e-06
Iter: 132 loss: 2.06531058e-06
Iter: 133 loss: 2.05020478e-06
Iter: 134 loss: 2.04969501e-06
Iter: 135 loss: 2.05258539e-06
Iter: 136 loss: 2.04270555e-06
Iter: 137 loss: 2.03723903e-06
Iter: 138 loss: 2.02538786e-06
Iter: 139 loss: 2.20922584e-06
Iter: 140 loss: 2.02497745e-06
Iter: 141 loss: 2.01311559e-06
Iter: 142 loss: 2.06005552e-06
Iter: 143 loss: 2.01042099e-06
Iter: 144 loss: 1.99616034e-06
Iter: 145 loss: 2.08000847e-06
Iter: 146 loss: 1.99434294e-06
Iter: 147 loss: 1.98880616e-06
Iter: 148 loss: 1.98802309e-06
Iter: 149 loss: 1.98408293e-06
Iter: 150 loss: 1.97462077e-06
Iter: 151 loss: 2.03713125e-06
Iter: 152 loss: 1.97362942e-06
Iter: 153 loss: 1.96824817e-06
Iter: 154 loss: 1.95676557e-06
Iter: 155 loss: 2.13968269e-06
Iter: 156 loss: 1.95635653e-06
Iter: 157 loss: 1.94599602e-06
Iter: 158 loss: 2.04894332e-06
Iter: 159 loss: 1.9456877e-06
Iter: 160 loss: 1.93880305e-06
Iter: 161 loss: 2.03864283e-06
Iter: 162 loss: 1.93874621e-06
Iter: 163 loss: 1.93370715e-06
Iter: 164 loss: 1.93110782e-06
Iter: 165 loss: 1.92870721e-06
Iter: 166 loss: 1.9195154e-06
Iter: 167 loss: 1.94443692e-06
Iter: 168 loss: 1.91643e-06
Iter: 169 loss: 1.90982541e-06
Iter: 170 loss: 1.90422065e-06
Iter: 171 loss: 1.90245009e-06
Iter: 172 loss: 1.90040339e-06
Iter: 173 loss: 1.8970004e-06
Iter: 174 loss: 1.89385355e-06
Iter: 175 loss: 1.88528e-06
Iter: 176 loss: 1.94738914e-06
Iter: 177 loss: 1.88342506e-06
Iter: 178 loss: 1.87797127e-06
Iter: 179 loss: 1.87750391e-06
Iter: 180 loss: 1.87163039e-06
Iter: 181 loss: 1.87285127e-06
Iter: 182 loss: 1.86731984e-06
Iter: 183 loss: 1.86338298e-06
Iter: 184 loss: 1.88106219e-06
Iter: 185 loss: 1.86254874e-06
Iter: 186 loss: 1.8572216e-06
Iter: 187 loss: 1.85951728e-06
Iter: 188 loss: 1.85354452e-06
Iter: 189 loss: 1.84860789e-06
Iter: 190 loss: 1.84054647e-06
Iter: 191 loss: 1.84048019e-06
Iter: 192 loss: 1.83289626e-06
Iter: 193 loss: 1.89311243e-06
Iter: 194 loss: 1.83246971e-06
Iter: 195 loss: 1.82752296e-06
Iter: 196 loss: 1.82746e-06
Iter: 197 loss: 1.8234424e-06
Iter: 198 loss: 1.8209488e-06
Iter: 199 loss: 1.81930784e-06
Iter: 200 loss: 1.81419182e-06
Iter: 201 loss: 1.8533824e-06
Iter: 202 loss: 1.81373559e-06
Iter: 203 loss: 1.81049336e-06
Iter: 204 loss: 1.80622101e-06
Iter: 205 loss: 1.80595532e-06
Iter: 206 loss: 1.80364145e-06
Iter: 207 loss: 1.80284519e-06
Iter: 208 loss: 1.79968868e-06
Iter: 209 loss: 1.79292033e-06
Iter: 210 loss: 1.89896832e-06
Iter: 211 loss: 1.79267067e-06
Iter: 212 loss: 1.78941855e-06
Iter: 213 loss: 1.78929781e-06
Iter: 214 loss: 1.78585515e-06
Iter: 215 loss: 1.78624771e-06
Iter: 216 loss: 1.78318226e-06
Iter: 217 loss: 1.77936784e-06
Iter: 218 loss: 1.77970742e-06
Iter: 219 loss: 1.77648405e-06
Iter: 220 loss: 1.77165589e-06
Iter: 221 loss: 1.84268799e-06
Iter: 222 loss: 1.77165043e-06
Iter: 223 loss: 1.76946833e-06
Iter: 224 loss: 1.76354501e-06
Iter: 225 loss: 1.7952367e-06
Iter: 226 loss: 1.76169374e-06
Iter: 227 loss: 1.75383775e-06
Iter: 228 loss: 1.78471805e-06
Iter: 229 loss: 1.75206117e-06
Iter: 230 loss: 1.75033165e-06
Iter: 231 loss: 1.74918716e-06
Iter: 232 loss: 1.74629599e-06
Iter: 233 loss: 1.74732622e-06
Iter: 234 loss: 1.7442178e-06
Iter: 235 loss: 1.74098136e-06
Iter: 236 loss: 1.73725516e-06
Iter: 237 loss: 1.73684202e-06
Iter: 238 loss: 1.73340709e-06
Iter: 239 loss: 1.73334251e-06
Iter: 240 loss: 1.72999489e-06
Iter: 241 loss: 1.74179e-06
Iter: 242 loss: 1.72910416e-06
Iter: 243 loss: 1.72643468e-06
Iter: 244 loss: 1.7385621e-06
Iter: 245 loss: 1.72589648e-06
Iter: 246 loss: 1.72331727e-06
Iter: 247 loss: 1.7195656e-06
Iter: 248 loss: 1.71947408e-06
Iter: 249 loss: 1.71668546e-06
Iter: 250 loss: 1.71653323e-06
Iter: 251 loss: 1.71412785e-06
Iter: 252 loss: 1.71068746e-06
Iter: 253 loss: 1.71057843e-06
Iter: 254 loss: 1.70758676e-06
Iter: 255 loss: 1.71598947e-06
Iter: 256 loss: 1.70660269e-06
Iter: 257 loss: 1.70274927e-06
Iter: 258 loss: 1.7335683e-06
Iter: 259 loss: 1.70253679e-06
Iter: 260 loss: 1.70074532e-06
Iter: 261 loss: 1.69663599e-06
Iter: 262 loss: 1.75365687e-06
Iter: 263 loss: 1.69635211e-06
Iter: 264 loss: 1.69329292e-06
Iter: 265 loss: 1.69321504e-06
Iter: 266 loss: 1.69001373e-06
Iter: 267 loss: 1.69827536e-06
Iter: 268 loss: 1.6888763e-06
Iter: 269 loss: 1.68613963e-06
Iter: 270 loss: 1.68154747e-06
Iter: 271 loss: 1.68152042e-06
Iter: 272 loss: 1.67831558e-06
Iter: 273 loss: 1.67825203e-06
Iter: 274 loss: 1.67540543e-06
Iter: 275 loss: 1.69490613e-06
Iter: 276 loss: 1.67520409e-06
Iter: 277 loss: 1.67320832e-06
Iter: 278 loss: 1.67653343e-06
Iter: 279 loss: 1.67240273e-06
Iter: 280 loss: 1.66995733e-06
Iter: 281 loss: 1.66955942e-06
Iter: 282 loss: 1.6678905e-06
Iter: 283 loss: 1.66568634e-06
Iter: 284 loss: 1.66566986e-06
Iter: 285 loss: 1.66404391e-06
Iter: 286 loss: 1.66111636e-06
Iter: 287 loss: 1.66114478e-06
Iter: 288 loss: 1.65788936e-06
Iter: 289 loss: 1.66114239e-06
Iter: 290 loss: 1.65617541e-06
Iter: 291 loss: 1.65251697e-06
Iter: 292 loss: 1.65245501e-06
Iter: 293 loss: 1.65100778e-06
Iter: 294 loss: 1.64749088e-06
Iter: 295 loss: 1.68679276e-06
Iter: 296 loss: 1.6472319e-06
Iter: 297 loss: 1.64474386e-06
Iter: 298 loss: 1.64469179e-06
Iter: 299 loss: 1.64196217e-06
Iter: 300 loss: 1.64238418e-06
Iter: 301 loss: 1.63985874e-06
Iter: 302 loss: 1.63725372e-06
Iter: 303 loss: 1.63605478e-06
Iter: 304 loss: 1.63481582e-06
Iter: 305 loss: 1.63185234e-06
Iter: 306 loss: 1.64559538e-06
Iter: 307 loss: 1.63137952e-06
Iter: 308 loss: 1.62841809e-06
Iter: 309 loss: 1.66475e-06
Iter: 310 loss: 1.62838865e-06
Iter: 311 loss: 1.62675133e-06
Iter: 312 loss: 1.62839024e-06
Iter: 313 loss: 1.6258366e-06
Iter: 314 loss: 1.62398e-06
Iter: 315 loss: 1.62800586e-06
Iter: 316 loss: 1.62323897e-06
Iter: 317 loss: 1.62177469e-06
Iter: 318 loss: 1.63491268e-06
Iter: 319 loss: 1.62170556e-06
Iter: 320 loss: 1.62036258e-06
Iter: 321 loss: 1.61825528e-06
Iter: 322 loss: 1.61822732e-06
Iter: 323 loss: 1.61583534e-06
Iter: 324 loss: 1.61701519e-06
Iter: 325 loss: 1.6142825e-06
Iter: 326 loss: 1.61341745e-06
Iter: 327 loss: 1.61273124e-06
Iter: 328 loss: 1.61163211e-06
Iter: 329 loss: 1.60848197e-06
Iter: 330 loss: 1.61966159e-06
Iter: 331 loss: 1.60710454e-06
Iter: 332 loss: 1.60466993e-06
Iter: 333 loss: 1.60453396e-06
Iter: 334 loss: 1.6021562e-06
Iter: 335 loss: 1.61230719e-06
Iter: 336 loss: 1.60165519e-06
Iter: 337 loss: 1.60047841e-06
Iter: 338 loss: 1.59818069e-06
Iter: 339 loss: 1.64738265e-06
Iter: 340 loss: 1.59815932e-06
Iter: 341 loss: 1.59508409e-06
Iter: 342 loss: 1.59873673e-06
Iter: 343 loss: 1.59351748e-06
Iter: 344 loss: 1.59396279e-06
Iter: 345 loss: 1.59225851e-06
Iter: 346 loss: 1.59120327e-06
Iter: 347 loss: 1.58991077e-06
Iter: 348 loss: 1.5897424e-06
Iter: 349 loss: 1.58765931e-06
Iter: 350 loss: 1.59524609e-06
Iter: 351 loss: 1.58709076e-06
Iter: 352 loss: 1.58548517e-06
Iter: 353 loss: 1.59017804e-06
Iter: 354 loss: 1.58498187e-06
Iter: 355 loss: 1.5832228e-06
Iter: 356 loss: 1.59294927e-06
Iter: 357 loss: 1.58295711e-06
Iter: 358 loss: 1.58186697e-06
Iter: 359 loss: 1.57966576e-06
Iter: 360 loss: 1.62398101e-06
Iter: 361 loss: 1.5796702e-06
Iter: 362 loss: 1.57752936e-06
Iter: 363 loss: 1.59426304e-06
Iter: 364 loss: 1.57741169e-06
Iter: 365 loss: 1.57496959e-06
Iter: 366 loss: 1.58506737e-06
Iter: 367 loss: 1.57446891e-06
Iter: 368 loss: 1.57320801e-06
Iter: 369 loss: 1.5712867e-06
Iter: 370 loss: 1.57124919e-06
Iter: 371 loss: 1.57012914e-06
Iter: 372 loss: 1.56985709e-06
Iter: 373 loss: 1.56868327e-06
Iter: 374 loss: 1.56640624e-06
Iter: 375 loss: 1.60110744e-06
Iter: 376 loss: 1.56627561e-06
Iter: 377 loss: 1.56400199e-06
Iter: 378 loss: 1.56892543e-06
Iter: 379 loss: 1.56318094e-06
Iter: 380 loss: 1.56313467e-06
Iter: 381 loss: 1.56230044e-06
Iter: 382 loss: 1.56162378e-06
Iter: 383 loss: 1.55966359e-06
Iter: 384 loss: 1.5704295e-06
Iter: 385 loss: 1.55905457e-06
Iter: 386 loss: 1.55740486e-06
Iter: 387 loss: 1.55733483e-06
Iter: 388 loss: 1.55633006e-06
Iter: 389 loss: 1.55864382e-06
Iter: 390 loss: 1.55595717e-06
Iter: 391 loss: 1.55467592e-06
Iter: 392 loss: 1.55585644e-06
Iter: 393 loss: 1.55396401e-06
Iter: 394 loss: 1.55275188e-06
Iter: 395 loss: 1.55050213e-06
Iter: 396 loss: 1.59897104e-06
Iter: 397 loss: 1.55043938e-06
Iter: 398 loss: 1.54966892e-06
Iter: 399 loss: 1.54921827e-06
Iter: 400 loss: 1.54787426e-06
Iter: 401 loss: 1.54813961e-06
Iter: 402 loss: 1.54691e-06
Iter: 403 loss: 1.54574536e-06
Iter: 404 loss: 1.54384247e-06
Iter: 405 loss: 1.54383497e-06
Iter: 406 loss: 1.54310374e-06
Iter: 407 loss: 1.54241991e-06
Iter: 408 loss: 1.54173028e-06
Iter: 409 loss: 1.54033467e-06
Iter: 410 loss: 1.56591955e-06
Iter: 411 loss: 1.54028726e-06
Iter: 412 loss: 1.53940346e-06
Iter: 413 loss: 1.53935571e-06
Iter: 414 loss: 1.53850556e-06
Iter: 415 loss: 1.53641759e-06
Iter: 416 loss: 1.55678038e-06
Iter: 417 loss: 1.53615827e-06
Iter: 418 loss: 1.53471524e-06
Iter: 419 loss: 1.5346734e-06
Iter: 420 loss: 1.53331212e-06
Iter: 421 loss: 1.54223676e-06
Iter: 422 loss: 1.53323526e-06
Iter: 423 loss: 1.53233213e-06
Iter: 424 loss: 1.53469398e-06
Iter: 425 loss: 1.53196356e-06
Iter: 426 loss: 1.53050905e-06
Iter: 427 loss: 1.52896962e-06
Iter: 428 loss: 1.52870462e-06
Iter: 429 loss: 1.52717507e-06
Iter: 430 loss: 1.52679127e-06
Iter: 431 loss: 1.52585847e-06
Iter: 432 loss: 1.52572306e-06
Iter: 433 loss: 1.52486439e-06
Iter: 434 loss: 1.52413111e-06
Iter: 435 loss: 1.52239602e-06
Iter: 436 loss: 1.54498878e-06
Iter: 437 loss: 1.52225812e-06
Iter: 438 loss: 1.520507e-06
Iter: 439 loss: 1.52604264e-06
Iter: 440 loss: 1.52004861e-06
Iter: 441 loss: 1.51920449e-06
Iter: 442 loss: 1.51904965e-06
Iter: 443 loss: 1.51835695e-06
Iter: 444 loss: 1.51674476e-06
Iter: 445 loss: 1.53520068e-06
Iter: 446 loss: 1.51658855e-06
Iter: 447 loss: 1.5163331e-06
Iter: 448 loss: 1.51577046e-06
Iter: 449 loss: 1.5151968e-06
Iter: 450 loss: 1.51381357e-06
Iter: 451 loss: 1.52517919e-06
Iter: 452 loss: 1.51351821e-06
Iter: 453 loss: 1.51214522e-06
Iter: 454 loss: 1.51890913e-06
Iter: 455 loss: 1.51201061e-06
Iter: 456 loss: 1.51053268e-06
Iter: 457 loss: 1.51995073e-06
Iter: 458 loss: 1.51038239e-06
Iter: 459 loss: 1.50935443e-06
Iter: 460 loss: 1.51005406e-06
Iter: 461 loss: 1.50876303e-06
Iter: 462 loss: 1.50722235e-06
Iter: 463 loss: 1.51331e-06
Iter: 464 loss: 1.50690676e-06
Iter: 465 loss: 1.50596418e-06
Iter: 466 loss: 1.50445771e-06
Iter: 467 loss: 1.50445749e-06
Iter: 468 loss: 1.50362928e-06
Iter: 469 loss: 1.50337905e-06
Iter: 470 loss: 1.50245171e-06
Iter: 471 loss: 1.50090864e-06
Iter: 472 loss: 1.53919393e-06
Iter: 473 loss: 1.50090045e-06
Iter: 474 loss: 1.49987193e-06
Iter: 475 loss: 1.51124618e-06
Iter: 476 loss: 1.49982191e-06
Iter: 477 loss: 1.49871096e-06
Iter: 478 loss: 1.50075493e-06
Iter: 479 loss: 1.49819471e-06
Iter: 480 loss: 1.49730317e-06
Iter: 481 loss: 1.49918651e-06
Iter: 482 loss: 1.49695313e-06
Iter: 483 loss: 1.4956081e-06
Iter: 484 loss: 1.49975108e-06
Iter: 485 loss: 1.49521452e-06
Iter: 486 loss: 1.49450125e-06
Iter: 487 loss: 1.49321795e-06
Iter: 488 loss: 1.52388452e-06
Iter: 489 loss: 1.4932275e-06
Iter: 490 loss: 1.49220909e-06
Iter: 491 loss: 1.49219397e-06
Iter: 492 loss: 1.49103175e-06
Iter: 493 loss: 1.49216498e-06
Iter: 494 loss: 1.4903851e-06
Iter: 495 loss: 1.48952597e-06
Iter: 496 loss: 1.49504194e-06
Iter: 497 loss: 1.48940967e-06
Iter: 498 loss: 1.4885e-06
Iter: 499 loss: 1.48823563e-06
Iter: 500 loss: 1.48768072e-06
Iter: 501 loss: 1.4865077e-06
Iter: 502 loss: 1.49074583e-06
Iter: 503 loss: 1.48624554e-06
Iter: 504 loss: 1.48498793e-06
Iter: 505 loss: 1.49243238e-06
Iter: 506 loss: 1.48480876e-06
Iter: 507 loss: 1.48410754e-06
Iter: 508 loss: 1.48296169e-06
Iter: 509 loss: 1.48297147e-06
Iter: 510 loss: 1.48237086e-06
Iter: 511 loss: 1.48216611e-06
Iter: 512 loss: 1.48165202e-06
Iter: 513 loss: 1.48068921e-06
Iter: 514 loss: 1.48075253e-06
Iter: 515 loss: 1.47977812e-06
Iter: 516 loss: 1.47980961e-06
Iter: 517 loss: 1.47911601e-06
Iter: 518 loss: 1.47787318e-06
Iter: 519 loss: 1.50643723e-06
Iter: 520 loss: 1.47780543e-06
Iter: 521 loss: 1.47649405e-06
Iter: 522 loss: 1.48214372e-06
Iter: 523 loss: 1.47621665e-06
Iter: 524 loss: 1.47484127e-06
Iter: 525 loss: 1.49615926e-06
Iter: 526 loss: 1.474844e-06
Iter: 527 loss: 1.47400067e-06
Iter: 528 loss: 1.47337903e-06
Iter: 529 loss: 1.47314142e-06
Iter: 530 loss: 1.4718953e-06
Iter: 531 loss: 1.4826262e-06
Iter: 532 loss: 1.47181549e-06
Iter: 533 loss: 1.47099115e-06
Iter: 534 loss: 1.47043102e-06
Iter: 535 loss: 1.47012031e-06
Iter: 536 loss: 1.4693785e-06
Iter: 537 loss: 1.46936736e-06
Iter: 538 loss: 1.46872821e-06
Iter: 539 loss: 1.46752143e-06
Iter: 540 loss: 1.49552329e-06
Iter: 541 loss: 1.46758305e-06
Iter: 542 loss: 1.46676018e-06
Iter: 543 loss: 1.46670436e-06
Iter: 544 loss: 1.46587331e-06
Iter: 545 loss: 1.46491311e-06
Iter: 546 loss: 1.46482284e-06
Iter: 547 loss: 1.46386105e-06
Iter: 548 loss: 1.46891e-06
Iter: 549 loss: 1.46366267e-06
Iter: 550 loss: 1.46252387e-06
Iter: 551 loss: 1.46748016e-06
Iter: 552 loss: 1.46232799e-06
Iter: 553 loss: 1.46171351e-06
Iter: 554 loss: 1.46051559e-06
Iter: 555 loss: 1.48781851e-06
Iter: 556 loss: 1.46053526e-06
Iter: 557 loss: 1.45951026e-06
Iter: 558 loss: 1.47101173e-06
Iter: 559 loss: 1.459484e-06
Iter: 560 loss: 1.45850879e-06
Iter: 561 loss: 1.46181173e-06
Iter: 562 loss: 1.45820547e-06
Iter: 563 loss: 1.45742888e-06
Iter: 564 loss: 1.45786385e-06
Iter: 565 loss: 1.45700369e-06
Iter: 566 loss: 1.45594879e-06
Iter: 567 loss: 1.45970694e-06
Iter: 568 loss: 1.45562899e-06
Iter: 569 loss: 1.45490912e-06
Iter: 570 loss: 1.45746958e-06
Iter: 571 loss: 1.45474451e-06
Iter: 572 loss: 1.45390243e-06
Iter: 573 loss: 1.45516594e-06
Iter: 574 loss: 1.45346439e-06
Iter: 575 loss: 1.45279296e-06
Iter: 576 loss: 1.45320121e-06
Iter: 577 loss: 1.45234378e-06
Iter: 578 loss: 1.4515299e-06
Iter: 579 loss: 1.46282673e-06
Iter: 580 loss: 1.45152035e-06
Iter: 581 loss: 1.45103888e-06
Iter: 582 loss: 1.44992805e-06
Iter: 583 loss: 1.46885122e-06
Iter: 584 loss: 1.44988985e-06
Iter: 585 loss: 1.44962019e-06
Iter: 586 loss: 1.44939361e-06
Iter: 587 loss: 1.44886712e-06
Iter: 588 loss: 1.44779517e-06
Iter: 589 loss: 1.46626974e-06
Iter: 590 loss: 1.44772093e-06
Iter: 591 loss: 1.44693672e-06
Iter: 592 loss: 1.4484192e-06
Iter: 593 loss: 1.44649334e-06
Iter: 594 loss: 1.44553678e-06
Iter: 595 loss: 1.45427975e-06
Iter: 596 loss: 1.44547039e-06
Iter: 597 loss: 1.44472347e-06
Iter: 598 loss: 1.44448745e-06
Iter: 599 loss: 1.44406567e-06
Iter: 600 loss: 1.44338742e-06
Iter: 601 loss: 1.45215154e-06
Iter: 602 loss: 1.4434321e-06
Iter: 603 loss: 1.44286605e-06
Iter: 604 loss: 1.44240471e-06
Iter: 605 loss: 1.44230296e-06
Iter: 606 loss: 1.44153864e-06
Iter: 607 loss: 1.4415881e-06
Iter: 608 loss: 1.44103e-06
Iter: 609 loss: 1.44003911e-06
Iter: 610 loss: 1.462775e-06
Iter: 611 loss: 1.44003218e-06
Iter: 612 loss: 1.43949114e-06
Iter: 613 loss: 1.43944e-06
Iter: 614 loss: 1.43891634e-06
Iter: 615 loss: 1.43904799e-06
Iter: 616 loss: 1.43847626e-06
Iter: 617 loss: 1.43790612e-06
Iter: 618 loss: 1.43711873e-06
Iter: 619 loss: 1.43706052e-06
Iter: 620 loss: 1.43645025e-06
Iter: 621 loss: 1.4362497e-06
Iter: 622 loss: 1.43595162e-06
Iter: 623 loss: 1.43520629e-06
Iter: 624 loss: 1.44340083e-06
Iter: 625 loss: 1.43509806e-06
Iter: 626 loss: 1.4341723e-06
Iter: 627 loss: 1.43847421e-06
Iter: 628 loss: 1.43393561e-06
Iter: 629 loss: 1.43276066e-06
Iter: 630 loss: 1.44007072e-06
Iter: 631 loss: 1.43263969e-06
Iter: 632 loss: 1.43203852e-06
Iter: 633 loss: 1.43271814e-06
Iter: 634 loss: 1.43174498e-06
Iter: 635 loss: 1.4310275e-06
Iter: 636 loss: 1.43153488e-06
Iter: 637 loss: 1.43053148e-06
Iter: 638 loss: 1.42976364e-06
Iter: 639 loss: 1.43397165e-06
Iter: 640 loss: 1.42962119e-06
Iter: 641 loss: 1.42871545e-06
Iter: 642 loss: 1.43053069e-06
Iter: 643 loss: 1.42834983e-06
Iter: 644 loss: 1.42763133e-06
Iter: 645 loss: 1.42841031e-06
Iter: 646 loss: 1.42725662e-06
Iter: 647 loss: 1.42645695e-06
Iter: 648 loss: 1.43397301e-06
Iter: 649 loss: 1.42642489e-06
Iter: 650 loss: 1.42589317e-06
Iter: 651 loss: 1.42503382e-06
Iter: 652 loss: 1.42500471e-06
Iter: 653 loss: 1.42437011e-06
Iter: 654 loss: 1.43308569e-06
Iter: 655 loss: 1.42436238e-06
Iter: 656 loss: 1.42355179e-06
Iter: 657 loss: 1.42490296e-06
Iter: 658 loss: 1.42314821e-06
Iter: 659 loss: 1.42268846e-06
Iter: 660 loss: 1.42164981e-06
Iter: 661 loss: 1.4393263e-06
Iter: 662 loss: 1.42167175e-06
Iter: 663 loss: 1.42088652e-06
Iter: 664 loss: 1.42085923e-06
Iter: 665 loss: 1.42001579e-06
Iter: 666 loss: 1.42106899e-06
Iter: 667 loss: 1.4195806e-06
Iter: 668 loss: 1.41894441e-06
Iter: 669 loss: 1.42000886e-06
Iter: 670 loss: 1.41858391e-06
Iter: 671 loss: 1.41772989e-06
Iter: 672 loss: 1.41876149e-06
Iter: 673 loss: 1.41727264e-06
Iter: 674 loss: 1.41647297e-06
Iter: 675 loss: 1.41867235e-06
Iter: 676 loss: 1.41622081e-06
Iter: 677 loss: 1.41516057e-06
Iter: 678 loss: 1.42116119e-06
Iter: 679 loss: 1.41504916e-06
Iter: 680 loss: 1.41455848e-06
Iter: 681 loss: 1.41440864e-06
Iter: 682 loss: 1.4140694e-06
Iter: 683 loss: 1.4133725e-06
Iter: 684 loss: 1.42097429e-06
Iter: 685 loss: 1.41335863e-06
Iter: 686 loss: 1.41284272e-06
Iter: 687 loss: 1.41171665e-06
Iter: 688 loss: 1.42842327e-06
Iter: 689 loss: 1.41164992e-06
Iter: 690 loss: 1.41059218e-06
Iter: 691 loss: 1.41609485e-06
Iter: 692 loss: 1.41044052e-06
Iter: 693 loss: 1.40998736e-06
Iter: 694 loss: 1.40983241e-06
Iter: 695 loss: 1.40942848e-06
Iter: 696 loss: 1.40826137e-06
Iter: 697 loss: 1.41466876e-06
Iter: 698 loss: 1.40789962e-06
Iter: 699 loss: 1.40669317e-06
Iter: 700 loss: 1.41429564e-06
Iter: 701 loss: 1.4066261e-06
Iter: 702 loss: 1.40583711e-06
Iter: 703 loss: 1.40578777e-06
Iter: 704 loss: 1.40536167e-06
Iter: 705 loss: 1.40443876e-06
Iter: 706 loss: 1.41888052e-06
Iter: 707 loss: 1.40434906e-06
Iter: 708 loss: 1.40335783e-06
Iter: 709 loss: 1.40640691e-06
Iter: 710 loss: 1.40305201e-06
Iter: 711 loss: 1.40244333e-06
Iter: 712 loss: 1.40242332e-06
Iter: 713 loss: 1.40204565e-06
Iter: 714 loss: 1.40220391e-06
Iter: 715 loss: 1.40181726e-06
Iter: 716 loss: 1.40114093e-06
Iter: 717 loss: 1.40108295e-06
Iter: 718 loss: 1.4005775e-06
Iter: 719 loss: 1.39993062e-06
Iter: 720 loss: 1.40476459e-06
Iter: 721 loss: 1.39989288e-06
Iter: 722 loss: 1.39913948e-06
Iter: 723 loss: 1.40000861e-06
Iter: 724 loss: 1.39876238e-06
Iter: 725 loss: 1.39807389e-06
Iter: 726 loss: 1.39737097e-06
Iter: 727 loss: 1.39724443e-06
Iter: 728 loss: 1.39676445e-06
Iter: 729 loss: 1.39670692e-06
Iter: 730 loss: 1.39604231e-06
Iter: 731 loss: 1.39591089e-06
Iter: 732 loss: 1.39546933e-06
Iter: 733 loss: 1.39493591e-06
Iter: 734 loss: 1.39450503e-06
Iter: 735 loss: 1.39433109e-06
Iter: 736 loss: 1.39402664e-06
Iter: 737 loss: 1.39385895e-06
Iter: 738 loss: 1.39341887e-06
Iter: 739 loss: 1.39257679e-06
Iter: 740 loss: 1.40915131e-06
Iter: 741 loss: 1.39255962e-06
Iter: 742 loss: 1.39178985e-06
Iter: 743 loss: 1.39284964e-06
Iter: 744 loss: 1.39138717e-06
Iter: 745 loss: 1.39105236e-06
Iter: 746 loss: 1.39093277e-06
Iter: 747 loss: 1.3905094e-06
Iter: 748 loss: 1.38972268e-06
Iter: 749 loss: 1.38971654e-06
Iter: 750 loss: 1.38904227e-06
Iter: 751 loss: 1.39415954e-06
Iter: 752 loss: 1.38901737e-06
Iter: 753 loss: 1.38833047e-06
Iter: 754 loss: 1.39015265e-06
Iter: 755 loss: 1.38813812e-06
Iter: 756 loss: 1.387677e-06
Iter: 757 loss: 1.38965083e-06
Iter: 758 loss: 1.38751705e-06
Iter: 759 loss: 1.38696055e-06
Iter: 760 loss: 1.38597591e-06
Iter: 761 loss: 1.38599114e-06
Iter: 762 loss: 1.38493067e-06
Iter: 763 loss: 1.38666405e-06
Iter: 764 loss: 1.38447535e-06
Iter: 765 loss: 1.38393102e-06
Iter: 766 loss: 1.38384598e-06
Iter: 767 loss: 1.38315067e-06
Iter: 768 loss: 1.3828419e-06
Iter: 769 loss: 1.38247219e-06
Iter: 770 loss: 1.38182861e-06
Iter: 771 loss: 1.38278608e-06
Iter: 772 loss: 1.38151654e-06
Iter: 773 loss: 1.38074267e-06
Iter: 774 loss: 1.38905466e-06
Iter: 775 loss: 1.38068731e-06
Iter: 776 loss: 1.38029043e-06
Iter: 777 loss: 1.37938355e-06
Iter: 778 loss: 1.39010604e-06
Iter: 779 loss: 1.37930044e-06
Iter: 780 loss: 1.3782917e-06
Iter: 781 loss: 1.38452026e-06
Iter: 782 loss: 1.3781995e-06
Iter: 783 loss: 1.37740858e-06
Iter: 784 loss: 1.37742438e-06
Iter: 785 loss: 1.37704296e-06
Iter: 786 loss: 1.37618395e-06
Iter: 787 loss: 1.38558642e-06
Iter: 788 loss: 1.37605775e-06
Iter: 789 loss: 1.37556322e-06
Iter: 790 loss: 1.37551638e-06
Iter: 791 loss: 1.37501422e-06
Iter: 792 loss: 1.37480515e-06
Iter: 793 loss: 1.37449285e-06
Iter: 794 loss: 1.3738329e-06
Iter: 795 loss: 1.37842403e-06
Iter: 796 loss: 1.37380823e-06
Iter: 797 loss: 1.3732265e-06
Iter: 798 loss: 1.37277016e-06
Iter: 799 loss: 1.37264738e-06
Iter: 800 loss: 1.37181269e-06
Iter: 801 loss: 1.37078769e-06
Iter: 802 loss: 1.37073471e-06
Iter: 803 loss: 1.36951121e-06
Iter: 804 loss: 1.37820416e-06
Iter: 805 loss: 1.36937717e-06
Iter: 806 loss: 1.3692179e-06
Iter: 807 loss: 1.36891458e-06
Iter: 808 loss: 1.36859376e-06
Iter: 809 loss: 1.36771155e-06
Iter: 810 loss: 1.37527331e-06
Iter: 811 loss: 1.36762242e-06
Iter: 812 loss: 1.36703545e-06
Iter: 813 loss: 1.36705967e-06
Iter: 814 loss: 1.36651488e-06
Iter: 815 loss: 1.36644053e-06
Iter: 816 loss: 1.36611811e-06
Iter: 817 loss: 1.3655058e-06
Iter: 818 loss: 1.36470635e-06
Iter: 819 loss: 1.36464053e-06
Iter: 820 loss: 1.36395715e-06
Iter: 821 loss: 1.36391122e-06
Iter: 822 loss: 1.36319841e-06
Iter: 823 loss: 1.36361746e-06
Iter: 824 loss: 1.36275162e-06
Iter: 825 loss: 1.36222945e-06
Iter: 826 loss: 1.36294034e-06
Iter: 827 loss: 1.36193057e-06
Iter: 828 loss: 1.36119934e-06
Iter: 829 loss: 1.36581377e-06
Iter: 830 loss: 1.3610836e-06
Iter: 831 loss: 1.36058463e-06
Iter: 832 loss: 1.36019389e-06
Iter: 833 loss: 1.36004576e-06
Iter: 834 loss: 1.35948471e-06
Iter: 835 loss: 1.35945209e-06
Iter: 836 loss: 1.35915548e-06
Iter: 837 loss: 1.35832761e-06
Iter: 838 loss: 1.36343033e-06
Iter: 839 loss: 1.35809807e-06
Iter: 840 loss: 1.35772507e-06
Iter: 841 loss: 1.35761252e-06
Iter: 842 loss: 1.35701464e-06
Iter: 843 loss: 1.35690743e-06
Iter: 844 loss: 1.35651953e-06
Iter: 845 loss: 1.35603159e-06
Iter: 846 loss: 1.35688902e-06
Iter: 847 loss: 1.35578784e-06
Iter: 848 loss: 1.35522e-06
Iter: 849 loss: 1.35922051e-06
Iter: 850 loss: 1.35516166e-06
Iter: 851 loss: 1.35468724e-06
Iter: 852 loss: 1.35412597e-06
Iter: 853 loss: 1.35405958e-06
Iter: 854 loss: 1.35337757e-06
Iter: 855 loss: 1.36464155e-06
Iter: 856 loss: 1.35337541e-06
Iter: 857 loss: 1.35285507e-06
Iter: 858 loss: 1.35228765e-06
Iter: 859 loss: 1.35217761e-06
Iter: 860 loss: 1.35143864e-06
Iter: 861 loss: 1.35164805e-06
Iter: 862 loss: 1.35088976e-06
Iter: 863 loss: 1.35036623e-06
Iter: 864 loss: 1.3503377e-06
Iter: 865 loss: 1.34982292e-06
Iter: 866 loss: 1.34959964e-06
Iter: 867 loss: 1.34932611e-06
Iter: 868 loss: 1.34889615e-06
Iter: 869 loss: 1.34889274e-06
Iter: 870 loss: 1.3484962e-06
Iter: 871 loss: 1.34752815e-06
Iter: 872 loss: 1.35433606e-06
Iter: 873 loss: 1.34738855e-06
Iter: 874 loss: 1.34670836e-06
Iter: 875 loss: 1.34667062e-06
Iter: 876 loss: 1.34595507e-06
Iter: 877 loss: 1.34772597e-06
Iter: 878 loss: 1.34570132e-06
Iter: 879 loss: 1.34525033e-06
Iter: 880 loss: 1.34435413e-06
Iter: 881 loss: 1.36259609e-06
Iter: 882 loss: 1.34435561e-06
Iter: 883 loss: 1.34389597e-06
Iter: 884 loss: 1.34379547e-06
Iter: 885 loss: 1.34323273e-06
Iter: 886 loss: 1.3426295e-06
Iter: 887 loss: 1.34251582e-06
Iter: 888 loss: 1.34194715e-06
Iter: 889 loss: 1.34191737e-06
Iter: 890 loss: 1.34153197e-06
Iter: 891 loss: 1.34101424e-06
Iter: 892 loss: 1.34100617e-06
Iter: 893 loss: 1.34032109e-06
Iter: 894 loss: 1.34018842e-06
Iter: 895 loss: 1.33975709e-06
Iter: 896 loss: 1.33894036e-06
Iter: 897 loss: 1.34042489e-06
Iter: 898 loss: 1.33860067e-06
Iter: 899 loss: 1.33781339e-06
Iter: 900 loss: 1.33777485e-06
Iter: 901 loss: 1.33740309e-06
Iter: 902 loss: 1.33814945e-06
Iter: 903 loss: 1.33726348e-06
Iter: 904 loss: 1.3366315e-06
Iter: 905 loss: 1.33566687e-06
Iter: 906 loss: 1.33560388e-06
Iter: 907 loss: 1.33484673e-06
Iter: 908 loss: 1.33636854e-06
Iter: 909 loss: 1.33452977e-06
Iter: 910 loss: 1.33436856e-06
Iter: 911 loss: 1.33406184e-06
Iter: 912 loss: 1.33371759e-06
Iter: 913 loss: 1.33275933e-06
Iter: 914 loss: 1.34249274e-06
Iter: 915 loss: 1.33266303e-06
Iter: 916 loss: 1.33176025e-06
Iter: 917 loss: 1.33420212e-06
Iter: 918 loss: 1.33152116e-06
Iter: 919 loss: 1.33129902e-06
Iter: 920 loss: 1.3310937e-06
Iter: 921 loss: 1.33075969e-06
Iter: 922 loss: 1.32984928e-06
Iter: 923 loss: 1.33505046e-06
Iter: 924 loss: 1.32953119e-06
Iter: 925 loss: 1.32870127e-06
Iter: 926 loss: 1.34064044e-06
Iter: 927 loss: 1.3287048e-06
Iter: 928 loss: 1.32807577e-06
Iter: 929 loss: 1.33385697e-06
Iter: 930 loss: 1.32805917e-06
Iter: 931 loss: 1.32768241e-06
Iter: 932 loss: 1.32733919e-06
Iter: 933 loss: 1.32721186e-06
Iter: 934 loss: 1.32668515e-06
Iter: 935 loss: 1.33113394e-06
Iter: 936 loss: 1.32666742e-06
Iter: 937 loss: 1.326031e-06
Iter: 938 loss: 1.32550031e-06
Iter: 939 loss: 1.32536104e-06
Iter: 940 loss: 1.32471041e-06
Iter: 941 loss: 1.32567993e-06
Iter: 942 loss: 1.32443006e-06
Iter: 943 loss: 1.32358389e-06
Iter: 944 loss: 1.3300446e-06
Iter: 945 loss: 1.32353352e-06
Iter: 946 loss: 1.32291211e-06
Iter: 947 loss: 1.32486e-06
Iter: 948 loss: 1.3227459e-06
Iter: 949 loss: 1.32199966e-06
Iter: 950 loss: 1.32395849e-06
Iter: 951 loss: 1.32176547e-06
Iter: 952 loss: 1.32131845e-06
Iter: 953 loss: 1.32054902e-06
Iter: 954 loss: 1.32054595e-06
Iter: 955 loss: 1.32007199e-06
Iter: 956 loss: 1.32002276e-06
Iter: 957 loss: 1.31947399e-06
Iter: 958 loss: 1.31884576e-06
Iter: 959 loss: 1.31877937e-06
Iter: 960 loss: 1.31825493e-06
Iter: 961 loss: 1.31798561e-06
Iter: 962 loss: 1.31781167e-06
Iter: 963 loss: 1.31723391e-06
Iter: 964 loss: 1.31722618e-06
Iter: 965 loss: 1.31675256e-06
Iter: 966 loss: 1.31594538e-06
Iter: 967 loss: 1.31593083e-06
Iter: 968 loss: 1.31522893e-06
Iter: 969 loss: 1.31958086e-06
Iter: 970 loss: 1.31521097e-06
Iter: 971 loss: 1.31454203e-06
Iter: 972 loss: 1.31825777e-06
Iter: 973 loss: 1.31447689e-06
Iter: 974 loss: 1.31397678e-06
Iter: 975 loss: 1.31297384e-06
Iter: 976 loss: 1.32852892e-06
Iter: 977 loss: 1.31290369e-06
Iter: 978 loss: 1.31225511e-06
Iter: 979 loss: 1.31226886e-06
Iter: 980 loss: 1.31176193e-06
Iter: 981 loss: 1.31334548e-06
Iter: 982 loss: 1.31164086e-06
Iter: 983 loss: 1.31116269e-06
Iter: 984 loss: 1.31062302e-06
Iter: 985 loss: 1.31060028e-06
Iter: 986 loss: 1.30967328e-06
Iter: 987 loss: 1.31678246e-06
Iter: 988 loss: 1.30960609e-06
Iter: 989 loss: 1.30908847e-06
Iter: 990 loss: 1.30946501e-06
Iter: 991 loss: 1.30873218e-06
Iter: 992 loss: 1.30807746e-06
Iter: 993 loss: 1.31563252e-06
Iter: 994 loss: 1.30811054e-06
Iter: 995 loss: 1.30770979e-06
Iter: 996 loss: 1.30690751e-06
Iter: 997 loss: 1.31418574e-06
Iter: 998 loss: 1.30675619e-06
Iter: 999 loss: 1.30554918e-06
Iter: 1000 loss: 1.30890396e-06
Iter: 1001 loss: 1.3051399e-06
Iter: 1002 loss: 1.30479702e-06
Iter: 1003 loss: 1.30472858e-06
Iter: 1004 loss: 1.30412013e-06
Iter: 1005 loss: 1.30328976e-06
Iter: 1006 loss: 1.30325577e-06
Iter: 1007 loss: 1.30254728e-06
Iter: 1008 loss: 1.30867397e-06
Iter: 1009 loss: 1.30250828e-06
Iter: 1010 loss: 1.30181377e-06
Iter: 1011 loss: 1.30334774e-06
Iter: 1012 loss: 1.30151398e-06
Iter: 1013 loss: 1.30107719e-06
Iter: 1014 loss: 1.30008732e-06
Iter: 1015 loss: 1.31916295e-06
Iter: 1016 loss: 1.30013916e-06
Iter: 1017 loss: 1.29906175e-06
Iter: 1018 loss: 1.30495141e-06
Iter: 1019 loss: 1.29893624e-06
Iter: 1020 loss: 1.29848809e-06
Iter: 1021 loss: 1.29837804e-06
Iter: 1022 loss: 1.29804471e-06
Iter: 1023 loss: 1.2973403e-06
Iter: 1024 loss: 1.30632111e-06
Iter: 1025 loss: 1.29731802e-06
Iter: 1026 loss: 1.2967289e-06
Iter: 1027 loss: 1.29679984e-06
Iter: 1028 loss: 1.29624141e-06
Iter: 1029 loss: 1.29606894e-06
Iter: 1030 loss: 1.29589921e-06
Iter: 1031 loss: 1.29506066e-06
Iter: 1032 loss: 1.29818898e-06
Iter: 1033 loss: 1.29485466e-06
Iter: 1034 loss: 1.29451269e-06
Iter: 1035 loss: 1.29369505e-06
Iter: 1036 loss: 1.30934e-06
Iter: 1037 loss: 1.29370233e-06
Iter: 1038 loss: 1.29263185e-06
Iter: 1039 loss: 1.29593025e-06
Iter: 1040 loss: 1.29232262e-06
Iter: 1041 loss: 1.29163323e-06
Iter: 1042 loss: 1.2972e-06
Iter: 1043 loss: 1.29155194e-06
Iter: 1044 loss: 1.29090859e-06
Iter: 1045 loss: 1.29798093e-06
Iter: 1046 loss: 1.29089472e-06
Iter: 1047 loss: 1.29056411e-06
Iter: 1048 loss: 1.28980764e-06
Iter: 1049 loss: 1.29875093e-06
Iter: 1050 loss: 1.28969214e-06
Iter: 1051 loss: 1.28925365e-06
Iter: 1052 loss: 1.28921397e-06
Iter: 1053 loss: 1.28867566e-06
Iter: 1054 loss: 1.28911734e-06
Iter: 1055 loss: 1.2883512e-06
Iter: 1056 loss: 1.28781471e-06
Iter: 1057 loss: 1.28694785e-06
Iter: 1058 loss: 1.28693148e-06
Iter: 1059 loss: 1.28683382e-06
Iter: 1060 loss: 1.2865363e-06
Iter: 1061 loss: 1.28614386e-06
Iter: 1062 loss: 1.28584338e-06
Iter: 1063 loss: 1.28571094e-06
Iter: 1064 loss: 1.28527824e-06
Iter: 1065 loss: 1.28528677e-06
Iter: 1066 loss: 1.28487841e-06
Iter: 1067 loss: 1.28404179e-06
Iter: 1068 loss: 1.28963268e-06
Iter: 1069 loss: 1.28391855e-06
Iter: 1070 loss: 1.28351292e-06
Iter: 1071 loss: 1.28322222e-06
Iter: 1072 loss: 1.28307795e-06
Iter: 1073 loss: 1.28261252e-06
Iter: 1074 loss: 1.28687543e-06
Iter: 1075 loss: 1.28261422e-06
Iter: 1076 loss: 1.28221404e-06
Iter: 1077 loss: 1.28196825e-06
Iter: 1078 loss: 1.28170495e-06
Iter: 1079 loss: 1.28134889e-06
Iter: 1080 loss: 1.28653244e-06
Iter: 1081 loss: 1.28134309e-06
Iter: 1082 loss: 1.28093325e-06
Iter: 1083 loss: 1.28105353e-06
Iter: 1084 loss: 1.28064687e-06
Iter: 1085 loss: 1.28024863e-06
Iter: 1086 loss: 1.2801944e-06
Iter: 1087 loss: 1.279868e-06
Iter: 1088 loss: 1.27946964e-06
Iter: 1089 loss: 1.27945214e-06
Iter: 1090 loss: 1.27916451e-06
Iter: 1091 loss: 1.27864382e-06
Iter: 1092 loss: 1.2912044e-06
Iter: 1093 loss: 1.27862734e-06
Iter: 1094 loss: 1.27840963e-06
Iter: 1095 loss: 1.2783571e-06
Iter: 1096 loss: 1.2780838e-06
Iter: 1097 loss: 1.27766566e-06
Iter: 1098 loss: 1.27764872e-06
Iter: 1099 loss: 1.277249e-06
Iter: 1100 loss: 1.28106194e-06
Iter: 1101 loss: 1.27725275e-06
Iter: 1102 loss: 1.27682267e-06
Iter: 1103 loss: 1.27678777e-06
Iter: 1104 loss: 1.27648821e-06
Iter: 1105 loss: 1.27607564e-06
Iter: 1106 loss: 1.27578721e-06
Iter: 1107 loss: 1.27565409e-06
Iter: 1108 loss: 1.27519206e-06
Iter: 1109 loss: 1.28185286e-06
Iter: 1110 loss: 1.27516432e-06
Iter: 1111 loss: 1.2747463e-06
Iter: 1112 loss: 1.27434328e-06
Iter: 1113 loss: 1.27426551e-06
Iter: 1114 loss: 1.2739456e-06
Iter: 1115 loss: 1.27386352e-06
Iter: 1116 loss: 1.27349858e-06
Iter: 1117 loss: 1.27326632e-06
Iter: 1118 loss: 1.27312455e-06
Iter: 1119 loss: 1.2727445e-06
Iter: 1120 loss: 1.27514136e-06
Iter: 1121 loss: 1.27269016e-06
Iter: 1122 loss: 1.27219778e-06
Iter: 1123 loss: 1.27256271e-06
Iter: 1124 loss: 1.27198314e-06
Iter: 1125 loss: 1.27155397e-06
Iter: 1126 loss: 1.27325529e-06
Iter: 1127 loss: 1.27144926e-06
Iter: 1128 loss: 1.27098383e-06
Iter: 1129 loss: 1.27211354e-06
Iter: 1130 loss: 1.27082285e-06
Iter: 1131 loss: 1.2705168e-06
Iter: 1132 loss: 1.27061651e-06
Iter: 1133 loss: 1.27038106e-06
Iter: 1134 loss: 1.26991551e-06
Iter: 1135 loss: 1.27229259e-06
Iter: 1136 loss: 1.26984628e-06
Iter: 1137 loss: 1.26948134e-06
Iter: 1138 loss: 1.26888892e-06
Iter: 1139 loss: 1.26888824e-06
Iter: 1140 loss: 1.26832219e-06
Iter: 1141 loss: 1.273105e-06
Iter: 1142 loss: 1.26834288e-06
Iter: 1143 loss: 1.26786222e-06
Iter: 1144 loss: 1.26946509e-06
Iter: 1145 loss: 1.26776172e-06
Iter: 1146 loss: 1.26735654e-06
Iter: 1147 loss: 1.26747341e-06
Iter: 1148 loss: 1.26707766e-06
Iter: 1149 loss: 1.26649797e-06
Iter: 1150 loss: 1.27161434e-06
Iter: 1151 loss: 1.2665115e-06
Iter: 1152 loss: 1.26619625e-06
Iter: 1153 loss: 1.2658387e-06
Iter: 1154 loss: 1.26576629e-06
Iter: 1155 loss: 1.2653702e-06
Iter: 1156 loss: 1.2722121e-06
Iter: 1157 loss: 1.26534087e-06
Iter: 1158 loss: 1.26502971e-06
Iter: 1159 loss: 1.26499e-06
Iter: 1160 loss: 1.26476186e-06
Iter: 1161 loss: 1.26425175e-06
Iter: 1162 loss: 1.26701332e-06
Iter: 1163 loss: 1.26418627e-06
Iter: 1164 loss: 1.26382361e-06
Iter: 1165 loss: 1.26316195e-06
Iter: 1166 loss: 1.26317877e-06
Iter: 1167 loss: 1.26285192e-06
Iter: 1168 loss: 1.26279758e-06
Iter: 1169 loss: 1.2624713e-06
Iter: 1170 loss: 1.26184909e-06
Iter: 1171 loss: 1.2730759e-06
Iter: 1172 loss: 1.26188252e-06
Iter: 1173 loss: 1.26125315e-06
Iter: 1174 loss: 1.26532689e-06
Iter: 1175 loss: 1.26118459e-06
Iter: 1176 loss: 1.26073746e-06
Iter: 1177 loss: 1.26167936e-06
Iter: 1178 loss: 1.26057034e-06
Iter: 1179 loss: 1.25999816e-06
Iter: 1180 loss: 1.26048894e-06
Iter: 1181 loss: 1.25969223e-06
Iter: 1182 loss: 1.25933934e-06
Iter: 1183 loss: 1.25928818e-06
Iter: 1184 loss: 1.2590167e-06
Iter: 1185 loss: 1.25829752e-06
Iter: 1186 loss: 1.26950499e-06
Iter: 1187 loss: 1.25832366e-06
Iter: 1188 loss: 1.25775796e-06
Iter: 1189 loss: 1.26502277e-06
Iter: 1190 loss: 1.2577575e-06
Iter: 1191 loss: 1.25719714e-06
Iter: 1192 loss: 1.25802103e-06
Iter: 1193 loss: 1.25694237e-06
Iter: 1194 loss: 1.25654e-06
Iter: 1195 loss: 1.2603191e-06
Iter: 1196 loss: 1.25656948e-06
Iter: 1197 loss: 1.25616907e-06
Iter: 1198 loss: 1.25549775e-06
Iter: 1199 loss: 1.25548559e-06
Iter: 1200 loss: 1.25493011e-06
Iter: 1201 loss: 1.25915017e-06
Iter: 1202 loss: 1.25485849e-06
Iter: 1203 loss: 1.25429779e-06
Iter: 1204 loss: 1.2574551e-06
Iter: 1205 loss: 1.25425993e-06
Iter: 1206 loss: 1.25395718e-06
Iter: 1207 loss: 1.25359247e-06
Iter: 1208 loss: 1.25360657e-06
Iter: 1209 loss: 1.25301744e-06
Iter: 1210 loss: 1.25564077e-06
Iter: 1211 loss: 1.25293786e-06
Iter: 1212 loss: 1.25237909e-06
Iter: 1213 loss: 1.25348311e-06
Iter: 1214 loss: 1.25216104e-06
Iter: 1215 loss: 1.25164536e-06
Iter: 1216 loss: 1.25314341e-06
Iter: 1217 loss: 1.25150962e-06
Iter: 1218 loss: 1.25090151e-06
Iter: 1219 loss: 1.25295412e-06
Iter: 1220 loss: 1.2506755e-06
Iter: 1221 loss: 1.25022416e-06
Iter: 1222 loss: 1.25002964e-06
Iter: 1223 loss: 1.24980238e-06
Iter: 1224 loss: 1.24930034e-06
Iter: 1225 loss: 1.24927737e-06
Iter: 1226 loss: 1.24893052e-06
Iter: 1227 loss: 1.24877533e-06
Iter: 1228 loss: 1.24854807e-06
Iter: 1229 loss: 1.2480225e-06
Iter: 1230 loss: 1.25281849e-06
Iter: 1231 loss: 1.248e-06
Iter: 1232 loss: 1.24763085e-06
Iter: 1233 loss: 1.24701387e-06
Iter: 1234 loss: 1.26036923e-06
Iter: 1235 loss: 1.24700159e-06
Iter: 1236 loss: 1.24644112e-06
Iter: 1237 loss: 1.2464028e-06
Iter: 1238 loss: 1.24608437e-06
Iter: 1239 loss: 1.24556664e-06
Iter: 1240 loss: 1.2455688e-06
Iter: 1241 loss: 1.2449043e-06
Iter: 1242 loss: 1.24522239e-06
Iter: 1243 loss: 1.24445023e-06
Iter: 1244 loss: 1.24378414e-06
Iter: 1245 loss: 1.24377186e-06
Iter: 1246 loss: 1.24332746e-06
Iter: 1247 loss: 1.2435919e-06
Iter: 1248 loss: 1.24304256e-06
Iter: 1249 loss: 1.24260464e-06
Iter: 1250 loss: 1.24807082e-06
Iter: 1251 loss: 1.24260509e-06
Iter: 1252 loss: 1.2422114e-06
Iter: 1253 loss: 1.24180235e-06
Iter: 1254 loss: 1.24169208e-06
Iter: 1255 loss: 1.24117128e-06
Iter: 1256 loss: 1.24313465e-06
Iter: 1257 loss: 1.24102564e-06
Iter: 1258 loss: 1.24044868e-06
Iter: 1259 loss: 1.24418875e-06
Iter: 1260 loss: 1.24035728e-06
Iter: 1261 loss: 1.24005851e-06
Iter: 1262 loss: 1.24081123e-06
Iter: 1263 loss: 1.23992913e-06
Iter: 1264 loss: 1.23950804e-06
Iter: 1265 loss: 1.24048802e-06
Iter: 1266 loss: 1.23932318e-06
Iter: 1267 loss: 1.23904522e-06
Iter: 1268 loss: 1.23874293e-06
Iter: 1269 loss: 1.23868313e-06
Iter: 1270 loss: 1.23816085e-06
Iter: 1271 loss: 1.24509802e-06
Iter: 1272 loss: 1.23815948e-06
Iter: 1273 loss: 1.23785844e-06
Iter: 1274 loss: 1.23728148e-06
Iter: 1275 loss: 1.24638098e-06
Iter: 1276 loss: 1.2372235e-06
Iter: 1277 loss: 1.23640791e-06
Iter: 1278 loss: 1.23974075e-06
Iter: 1279 loss: 1.23636391e-06
Iter: 1280 loss: 1.23574625e-06
Iter: 1281 loss: 1.2417803e-06
Iter: 1282 loss: 1.23574887e-06
Iter: 1283 loss: 1.23531436e-06
Iter: 1284 loss: 1.23525069e-06
Iter: 1285 loss: 1.23492964e-06
Iter: 1286 loss: 1.23433244e-06
Iter: 1287 loss: 1.24081271e-06
Iter: 1288 loss: 1.23427208e-06
Iter: 1289 loss: 1.2339251e-06
Iter: 1290 loss: 1.23310929e-06
Iter: 1291 loss: 1.24671237e-06
Iter: 1292 loss: 1.2330845e-06
Iter: 1293 loss: 1.23266886e-06
Iter: 1294 loss: 1.2326318e-06
Iter: 1295 loss: 1.23216796e-06
Iter: 1296 loss: 1.23197538e-06
Iter: 1297 loss: 1.23171139e-06
Iter: 1298 loss: 1.2313692e-06
Iter: 1299 loss: 1.23692098e-06
Iter: 1300 loss: 1.23137704e-06
Iter: 1301 loss: 1.23090877e-06
Iter: 1302 loss: 1.2305743e-06
Iter: 1303 loss: 1.23047062e-06
Iter: 1304 loss: 1.22999757e-06
Iter: 1305 loss: 1.23007044e-06
Iter: 1306 loss: 1.22962388e-06
Iter: 1307 loss: 1.22939673e-06
Iter: 1308 loss: 1.22924325e-06
Iter: 1309 loss: 1.22904908e-06
Iter: 1310 loss: 1.22842653e-06
Iter: 1311 loss: 1.23413952e-06
Iter: 1312 loss: 1.2283831e-06
Iter: 1313 loss: 1.2277585e-06
Iter: 1314 loss: 1.23161522e-06
Iter: 1315 loss: 1.22770734e-06
Iter: 1316 loss: 1.22710389e-06
Iter: 1317 loss: 1.22856102e-06
Iter: 1318 loss: 1.2268722e-06
Iter: 1319 loss: 1.22643382e-06
Iter: 1320 loss: 1.22704125e-06
Iter: 1321 loss: 1.22620213e-06
Iter: 1322 loss: 1.22565041e-06
Iter: 1323 loss: 1.23024961e-06
Iter: 1324 loss: 1.22554911e-06
Iter: 1325 loss: 1.22516474e-06
Iter: 1326 loss: 1.22440656e-06
Iter: 1327 loss: 1.23675045e-06
Iter: 1328 loss: 1.22441566e-06
Iter: 1329 loss: 1.2242408e-06
Iter: 1330 loss: 1.22404526e-06
Iter: 1331 loss: 1.2236967e-06
Iter: 1332 loss: 1.22339884e-06
Iter: 1333 loss: 1.22331221e-06
Iter: 1334 loss: 1.22281472e-06
Iter: 1335 loss: 1.22720689e-06
Iter: 1336 loss: 1.22269466e-06
Iter: 1337 loss: 1.22238828e-06
Iter: 1338 loss: 1.22167467e-06
Iter: 1339 loss: 1.23479822e-06
Iter: 1340 loss: 1.22162965e-06
Iter: 1341 loss: 1.22108349e-06
Iter: 1342 loss: 1.22408619e-06
Iter: 1343 loss: 1.22104063e-06
Iter: 1344 loss: 1.22045299e-06
Iter: 1345 loss: 1.22629149e-06
Iter: 1346 loss: 1.22046094e-06
Iter: 1347 loss: 1.22017582e-06
Iter: 1348 loss: 1.21955497e-06
Iter: 1349 loss: 1.22832466e-06
Iter: 1350 loss: 1.21954008e-06
Iter: 1351 loss: 1.21893288e-06
Iter: 1352 loss: 1.2252749e-06
Iter: 1353 loss: 1.21892026e-06
Iter: 1354 loss: 1.21846085e-06
Iter: 1355 loss: 1.22129768e-06
Iter: 1356 loss: 1.21831795e-06
Iter: 1357 loss: 1.21797609e-06
Iter: 1358 loss: 1.21812457e-06
Iter: 1359 loss: 1.21773428e-06
Iter: 1360 loss: 1.21708138e-06
Iter: 1361 loss: 1.21968787e-06
Iter: 1362 loss: 1.21698167e-06
Iter: 1363 loss: 1.21660514e-06
Iter: 1364 loss: 1.21600749e-06
Iter: 1365 loss: 1.21604546e-06
Iter: 1366 loss: 1.21575169e-06
Iter: 1367 loss: 1.21568269e-06
Iter: 1368 loss: 1.21525704e-06
Iter: 1369 loss: 1.21478593e-06
Iter: 1370 loss: 1.21472328e-06
Iter: 1371 loss: 1.21434448e-06
Iter: 1372 loss: 1.21794449e-06
Iter: 1373 loss: 1.21427684e-06
Iter: 1374 loss: 1.21387268e-06
Iter: 1375 loss: 1.21664175e-06
Iter: 1376 loss: 1.21384801e-06
Iter: 1377 loss: 1.21363621e-06
Iter: 1378 loss: 1.21305266e-06
Iter: 1379 loss: 1.21743778e-06
Iter: 1380 loss: 1.21295523e-06
Iter: 1381 loss: 1.21220842e-06
Iter: 1382 loss: 1.21262906e-06
Iter: 1383 loss: 1.21167022e-06
Iter: 1384 loss: 1.21102528e-06
Iter: 1385 loss: 1.22090387e-06
Iter: 1386 loss: 1.211023e-06
Iter: 1387 loss: 1.21040182e-06
Iter: 1388 loss: 1.21584924e-06
Iter: 1389 loss: 1.21040284e-06
Iter: 1390 loss: 1.21002449e-06
Iter: 1391 loss: 1.20930508e-06
Iter: 1392 loss: 1.22517645e-06
Iter: 1393 loss: 1.20929667e-06
Iter: 1394 loss: 1.20890309e-06
Iter: 1395 loss: 1.20890877e-06
Iter: 1396 loss: 1.20846948e-06
Iter: 1397 loss: 1.20838502e-06
Iter: 1398 loss: 1.20816412e-06
Iter: 1399 loss: 1.20769312e-06
Iter: 1400 loss: 1.20713594e-06
Iter: 1401 loss: 1.20700804e-06
Iter: 1402 loss: 1.20666994e-06
Iter: 1403 loss: 1.20656409e-06
Iter: 1404 loss: 1.20616096e-06
Iter: 1405 loss: 1.20586549e-06
Iter: 1406 loss: 1.20575646e-06
Iter: 1407 loss: 1.20511891e-06
Iter: 1408 loss: 1.209434e-06
Iter: 1409 loss: 1.20508844e-06
Iter: 1410 loss: 1.2047451e-06
Iter: 1411 loss: 1.20535015e-06
Iter: 1412 loss: 1.20455934e-06
Iter: 1413 loss: 1.20402058e-06
Iter: 1414 loss: 1.2064861e-06
Iter: 1415 loss: 1.20390632e-06
Iter: 1416 loss: 1.20366326e-06
Iter: 1417 loss: 1.20310165e-06
Iter: 1418 loss: 1.21280198e-06
Iter: 1419 loss: 1.20307823e-06
Iter: 1420 loss: 1.20260415e-06
Iter: 1421 loss: 1.20832055e-06
Iter: 1422 loss: 1.202573e-06
Iter: 1423 loss: 1.20203504e-06
Iter: 1424 loss: 1.20316747e-06
Iter: 1425 loss: 1.20182085e-06
Iter: 1426 loss: 1.20150037e-06
Iter: 1427 loss: 1.20104755e-06
Iter: 1428 loss: 1.20101322e-06
Iter: 1429 loss: 1.20072275e-06
Iter: 1430 loss: 1.20067216e-06
Iter: 1431 loss: 1.20032826e-06
Iter: 1432 loss: 1.19973561e-06
Iter: 1433 loss: 1.19973481e-06
Iter: 1434 loss: 1.19924323e-06
Iter: 1435 loss: 1.20018444e-06
Iter: 1436 loss: 1.19902325e-06
Iter: 1437 loss: 1.19841081e-06
Iter: 1438 loss: 1.20467951e-06
Iter: 1439 loss: 1.19839081e-06
Iter: 1440 loss: 1.19808055e-06
Iter: 1441 loss: 1.19770152e-06
Iter: 1442 loss: 1.1976399e-06
Iter: 1443 loss: 1.19707784e-06
Iter: 1444 loss: 1.2032931e-06
Iter: 1445 loss: 1.1970817e-06
Iter: 1446 loss: 1.19670153e-06
Iter: 1447 loss: 1.19716867e-06
Iter: 1448 loss: 1.19650281e-06
Iter: 1449 loss: 1.19602385e-06
Iter: 1450 loss: 1.19795914e-06
Iter: 1451 loss: 1.19589765e-06
Iter: 1452 loss: 1.19553613e-06
Iter: 1453 loss: 1.19484957e-06
Iter: 1454 loss: 1.20603477e-06
Iter: 1455 loss: 1.19482456e-06
Iter: 1456 loss: 1.19464437e-06
Iter: 1457 loss: 1.19441188e-06
Iter: 1458 loss: 1.19412539e-06
Iter: 1459 loss: 1.19367405e-06
Iter: 1460 loss: 1.19363267e-06
Iter: 1461 loss: 1.19326955e-06
Iter: 1462 loss: 1.19619199e-06
Iter: 1463 loss: 1.19321908e-06
Iter: 1464 loss: 1.19279616e-06
Iter: 1465 loss: 1.19273614e-06
Iter: 1466 loss: 1.19242657e-06
Iter: 1467 loss: 1.19189644e-06
Iter: 1468 loss: 1.19177571e-06
Iter: 1469 loss: 1.19139645e-06
Iter: 1470 loss: 1.19089168e-06
Iter: 1471 loss: 1.1909317e-06
Iter: 1472 loss: 1.19057961e-06
Iter: 1473 loss: 1.19013555e-06
Iter: 1474 loss: 1.19013851e-06
Iter: 1475 loss: 1.18949788e-06
Iter: 1476 loss: 1.19055221e-06
Iter: 1477 loss: 1.18930279e-06
Iter: 1478 loss: 1.18871321e-06
Iter: 1479 loss: 1.19692515e-06
Iter: 1480 loss: 1.18872151e-06
Iter: 1481 loss: 1.18847925e-06
Iter: 1482 loss: 1.18876335e-06
Iter: 1483 loss: 1.18829473e-06
Iter: 1484 loss: 1.18784874e-06
Iter: 1485 loss: 1.18847242e-06
Iter: 1486 loss: 1.18767707e-06
Iter: 1487 loss: 1.1872977e-06
Iter: 1488 loss: 1.18691014e-06
Iter: 1489 loss: 1.18681305e-06
Iter: 1490 loss: 1.18627077e-06
Iter: 1491 loss: 1.1863076e-06
Iter: 1492 loss: 1.18602838e-06
Iter: 1493 loss: 1.18562025e-06
Iter: 1494 loss: 1.18560069e-06
Iter: 1495 loss: 1.18528737e-06
Iter: 1496 loss: 1.18525224e-06
Iter: 1497 loss: 1.18493324e-06
Iter: 1498 loss: 1.18439903e-06
Iter: 1499 loss: 1.19731533e-06
Iter: 1500 loss: 1.18440425e-06
Iter: 1501 loss: 1.18391017e-06
Iter: 1502 loss: 1.18731941e-06
Iter: 1503 loss: 1.18385867e-06
Iter: 1504 loss: 1.18327682e-06
Iter: 1505 loss: 1.18450134e-06
Iter: 1506 loss: 1.18308185e-06
Iter: 1507 loss: 1.18259254e-06
Iter: 1508 loss: 1.18176354e-06
Iter: 1509 loss: 1.18176922e-06
Iter: 1510 loss: 1.18070625e-06
Iter: 1511 loss: 1.1850243e-06
Iter: 1512 loss: 1.18045841e-06
Iter: 1513 loss: 1.18045887e-06
Iter: 1514 loss: 1.18017192e-06
Iter: 1515 loss: 1.17988543e-06
Iter: 1516 loss: 1.17936781e-06
Iter: 1517 loss: 1.19152855e-06
Iter: 1518 loss: 1.17937361e-06
Iter: 1519 loss: 1.1787738e-06
Iter: 1520 loss: 1.18448395e-06
Iter: 1521 loss: 1.1787572e-06
Iter: 1522 loss: 1.17836703e-06
Iter: 1523 loss: 1.17781428e-06
Iter: 1524 loss: 1.17780291e-06
Iter: 1525 loss: 1.17750267e-06
Iter: 1526 loss: 1.17745537e-06
Iter: 1527 loss: 1.17708032e-06
Iter: 1528 loss: 1.17638444e-06
Iter: 1529 loss: 1.19108881e-06
Iter: 1530 loss: 1.17639092e-06
Iter: 1531 loss: 1.17577281e-06
Iter: 1532 loss: 1.17950833e-06
Iter: 1533 loss: 1.17575928e-06
Iter: 1534 loss: 1.17520153e-06
Iter: 1535 loss: 1.17880006e-06
Iter: 1536 loss: 1.17515572e-06
Iter: 1537 loss: 1.17482068e-06
Iter: 1538 loss: 1.17442119e-06
Iter: 1539 loss: 1.17438731e-06
Iter: 1540 loss: 1.17376032e-06
Iter: 1541 loss: 1.17859668e-06
Iter: 1542 loss: 1.17364834e-06
Iter: 1543 loss: 1.17320451e-06
Iter: 1544 loss: 1.17256479e-06
Iter: 1545 loss: 1.17258105e-06
Iter: 1546 loss: 1.17197919e-06
Iter: 1547 loss: 1.18024229e-06
Iter: 1548 loss: 1.17199011e-06
Iter: 1549 loss: 1.17157197e-06
Iter: 1550 loss: 1.17115155e-06
Iter: 1551 loss: 1.1711013e-06
Iter: 1552 loss: 1.17047784e-06
Iter: 1553 loss: 1.17000059e-06
Iter: 1554 loss: 1.16974331e-06
Iter: 1555 loss: 1.16930187e-06
Iter: 1556 loss: 1.16916681e-06
Iter: 1557 loss: 1.16864658e-06
Iter: 1558 loss: 1.16961564e-06
Iter: 1559 loss: 1.16844546e-06
Iter: 1560 loss: 1.16797582e-06
Iter: 1561 loss: 1.16754927e-06
Iter: 1562 loss: 1.1674565e-06
Iter: 1563 loss: 1.16731167e-06
Iter: 1564 loss: 1.16709248e-06
Iter: 1565 loss: 1.16683873e-06
Iter: 1566 loss: 1.16631543e-06
Iter: 1567 loss: 1.17332934e-06
Iter: 1568 loss: 1.16630736e-06
Iter: 1569 loss: 1.16579849e-06
Iter: 1570 loss: 1.1681675e-06
Iter: 1571 loss: 1.16571368e-06
Iter: 1572 loss: 1.16505316e-06
Iter: 1573 loss: 1.16800743e-06
Iter: 1574 loss: 1.16498086e-06
Iter: 1575 loss: 1.16447427e-06
Iter: 1576 loss: 1.16476872e-06
Iter: 1577 loss: 1.16412787e-06
Iter: 1578 loss: 1.16370938e-06
Iter: 1579 loss: 1.16994806e-06
Iter: 1580 loss: 1.16370222e-06
Iter: 1581 loss: 1.16337503e-06
Iter: 1582 loss: 1.16280671e-06
Iter: 1583 loss: 1.16280467e-06
Iter: 1584 loss: 1.16229216e-06
Iter: 1585 loss: 1.16231797e-06
Iter: 1586 loss: 1.16184754e-06
Iter: 1587 loss: 1.16239653e-06
Iter: 1588 loss: 1.16161118e-06
Iter: 1589 loss: 1.16127831e-06
Iter: 1590 loss: 1.16099761e-06
Iter: 1591 loss: 1.16096612e-06
Iter: 1592 loss: 1.16044544e-06
Iter: 1593 loss: 1.16693832e-06
Iter: 1594 loss: 1.16044362e-06
Iter: 1595 loss: 1.16007277e-06
Iter: 1596 loss: 1.15949422e-06
Iter: 1597 loss: 1.15945534e-06
Iter: 1598 loss: 1.15889702e-06
Iter: 1599 loss: 1.16033902e-06
Iter: 1600 loss: 1.15864088e-06
Iter: 1601 loss: 1.1584109e-06
Iter: 1602 loss: 1.15828936e-06
Iter: 1603 loss: 1.158033e-06
Iter: 1604 loss: 1.15733758e-06
Iter: 1605 loss: 1.16479623e-06
Iter: 1606 loss: 1.15732871e-06
Iter: 1607 loss: 1.15674538e-06
Iter: 1608 loss: 1.16180934e-06
Iter: 1609 loss: 1.1566558e-06
Iter: 1610 loss: 1.15615364e-06
Iter: 1611 loss: 1.15930891e-06
Iter: 1612 loss: 1.15604621e-06
Iter: 1613 loss: 1.15571038e-06
Iter: 1614 loss: 1.15508965e-06
Iter: 1615 loss: 1.15506441e-06
Iter: 1616 loss: 1.15467924e-06
Iter: 1617 loss: 1.15463433e-06
Iter: 1618 loss: 1.15421767e-06
Iter: 1619 loss: 1.15434409e-06
Iter: 1620 loss: 1.1538425e-06
Iter: 1621 loss: 1.1534197e-06
Iter: 1622 loss: 1.15346188e-06
Iter: 1623 loss: 1.15314015e-06
Iter: 1624 loss: 1.15270279e-06
Iter: 1625 loss: 1.15270166e-06
Iter: 1626 loss: 1.15240414e-06
Iter: 1627 loss: 1.15211469e-06
Iter: 1628 loss: 1.15199111e-06
Iter: 1629 loss: 1.1514212e-06
Iter: 1630 loss: 1.15522721e-06
Iter: 1631 loss: 1.15134173e-06
Iter: 1632 loss: 1.15105513e-06
Iter: 1633 loss: 1.15047237e-06
Iter: 1634 loss: 1.15046237e-06
Iter: 1635 loss: 1.14980105e-06
Iter: 1636 loss: 1.15280068e-06
Iter: 1637 loss: 1.149627e-06
Iter: 1638 loss: 1.1489368e-06
Iter: 1639 loss: 1.15469948e-06
Iter: 1640 loss: 1.14887882e-06
Iter: 1641 loss: 1.14847501e-06
Iter: 1642 loss: 1.14775924e-06
Iter: 1643 loss: 1.15976513e-06
Iter: 1644 loss: 1.14775742e-06
Iter: 1645 loss: 1.14699924e-06
Iter: 1646 loss: 1.14842987e-06
Iter: 1647 loss: 1.14667637e-06
Iter: 1648 loss: 1.146513e-06
Iter: 1649 loss: 1.14627881e-06
Iter: 1650 loss: 1.14592171e-06
Iter: 1651 loss: 1.14540546e-06
Iter: 1652 loss: 1.14538147e-06
Iter: 1653 loss: 1.14506008e-06
Iter: 1654 loss: 1.14913632e-06
Iter: 1655 loss: 1.14504712e-06
Iter: 1656 loss: 1.14462796e-06
Iter: 1657 loss: 1.14455554e-06
Iter: 1658 loss: 1.14429508e-06
Iter: 1659 loss: 1.14387865e-06
Iter: 1660 loss: 1.14388388e-06
Iter: 1661 loss: 1.14350405e-06
Iter: 1662 loss: 1.14314446e-06
Iter: 1663 loss: 1.1431656e-06
Iter: 1664 loss: 1.14287775e-06
Iter: 1665 loss: 1.14251452e-06
Iter: 1666 loss: 1.14245245e-06
Iter: 1667 loss: 1.1419213e-06
Iter: 1668 loss: 1.14610009e-06
Iter: 1669 loss: 1.1418831e-06
Iter: 1670 loss: 1.14146542e-06
Iter: 1671 loss: 1.14099714e-06
Iter: 1672 loss: 1.14097429e-06
Iter: 1673 loss: 1.14032628e-06
Iter: 1674 loss: 1.14122736e-06
Iter: 1675 loss: 1.13998044e-06
Iter: 1676 loss: 1.13955048e-06
Iter: 1677 loss: 1.13954684e-06
Iter: 1678 loss: 1.1390083e-06
Iter: 1679 loss: 1.13865212e-06
Iter: 1680 loss: 1.13844112e-06
Iter: 1681 loss: 1.13808983e-06
Iter: 1682 loss: 1.13830401e-06
Iter: 1683 loss: 1.13784677e-06
Iter: 1684 loss: 1.13736087e-06
Iter: 1685 loss: 1.13828958e-06
Iter: 1686 loss: 1.13716044e-06
Iter: 1687 loss: 1.1364657e-06
Iter: 1688 loss: 1.14197542e-06
Iter: 1689 loss: 1.13639294e-06
Iter: 1690 loss: 1.1360296e-06
Iter: 1691 loss: 1.13582951e-06
Iter: 1692 loss: 1.13571741e-06
Iter: 1693 loss: 1.1352497e-06
Iter: 1694 loss: 1.14155137e-06
Iter: 1695 loss: 1.13523674e-06
Iter: 1696 loss: 1.13490728e-06
Iter: 1697 loss: 1.13462841e-06
Iter: 1698 loss: 1.13451779e-06
Iter: 1699 loss: 1.1339979e-06
Iter: 1700 loss: 1.14011141e-06
Iter: 1701 loss: 1.13399506e-06
Iter: 1702 loss: 1.13371e-06
Iter: 1703 loss: 1.13375017e-06
Iter: 1704 loss: 1.13352189e-06
Iter: 1705 loss: 1.13301076e-06
Iter: 1706 loss: 1.13455155e-06
Iter: 1707 loss: 1.13291117e-06
Iter: 1708 loss: 1.13253168e-06
Iter: 1709 loss: 1.13199724e-06
Iter: 1710 loss: 1.13198666e-06
Iter: 1711 loss: 1.13168153e-06
Iter: 1712 loss: 1.13163435e-06
Iter: 1713 loss: 1.13128795e-06
Iter: 1714 loss: 1.13109365e-06
Iter: 1715 loss: 1.13088095e-06
Iter: 1716 loss: 1.13047145e-06
Iter: 1717 loss: 1.13019405e-06
Iter: 1718 loss: 1.1301006e-06
Iter: 1719 loss: 1.12961402e-06
Iter: 1720 loss: 1.1295956e-06
Iter: 1721 loss: 1.12915541e-06
Iter: 1722 loss: 1.12957468e-06
Iter: 1723 loss: 1.1288671e-06
Iter: 1724 loss: 1.12855048e-06
Iter: 1725 loss: 1.12850432e-06
Iter: 1726 loss: 1.12821863e-06
Iter: 1727 loss: 1.12764485e-06
Iter: 1728 loss: 1.13410658e-06
Iter: 1729 loss: 1.12759278e-06
Iter: 1730 loss: 1.1272989e-06
Iter: 1731 loss: 1.12799671e-06
Iter: 1732 loss: 1.12716407e-06
Iter: 1733 loss: 1.12674218e-06
Iter: 1734 loss: 1.12682687e-06
Iter: 1735 loss: 1.12644921e-06
Iter: 1736 loss: 1.12610655e-06
Iter: 1737 loss: 1.1300167e-06
Iter: 1738 loss: 1.12608132e-06
Iter: 1739 loss: 1.12575424e-06
Iter: 1740 loss: 1.12533098e-06
Iter: 1741 loss: 1.1253054e-06
Iter: 1742 loss: 1.12484418e-06
Iter: 1743 loss: 1.12703617e-06
Iter: 1744 loss: 1.12478369e-06
Iter: 1745 loss: 1.12427949e-06
Iter: 1746 loss: 1.12717339e-06
Iter: 1747 loss: 1.12424493e-06
Iter: 1748 loss: 1.12388057e-06
Iter: 1749 loss: 1.12337773e-06
Iter: 1750 loss: 1.12337818e-06
Iter: 1751 loss: 1.12296561e-06
Iter: 1752 loss: 1.12413227e-06
Iter: 1753 loss: 1.12283567e-06
Iter: 1754 loss: 1.12233624e-06
Iter: 1755 loss: 1.12852968e-06
Iter: 1756 loss: 1.1223276e-06
Iter: 1757 loss: 1.12205498e-06
Iter: 1758 loss: 1.12147939e-06
Iter: 1759 loss: 1.13569126e-06
Iter: 1760 loss: 1.12149235e-06
Iter: 1761 loss: 1.12115117e-06
Iter: 1762 loss: 1.12113707e-06
Iter: 1763 loss: 1.12077078e-06
Iter: 1764 loss: 1.12087605e-06
Iter: 1765 loss: 1.1205625e-06
Iter: 1766 loss: 1.1202153e-06
Iter: 1767 loss: 1.1241217e-06
Iter: 1768 loss: 1.12019336e-06
Iter: 1769 loss: 1.11998531e-06
Iter: 1770 loss: 1.11959741e-06
Iter: 1771 loss: 1.11959162e-06
Iter: 1772 loss: 1.11921372e-06
Iter: 1773 loss: 1.12535963e-06
Iter: 1774 loss: 1.11923941e-06
Iter: 1775 loss: 1.11903501e-06
Iter: 1776 loss: 1.11883435e-06
Iter: 1777 loss: 1.1187492e-06
Iter: 1778 loss: 1.11833708e-06
Iter: 1779 loss: 1.11807719e-06
Iter: 1780 loss: 1.11791144e-06
Iter: 1781 loss: 1.11751558e-06
Iter: 1782 loss: 1.11743259e-06
Iter: 1783 loss: 1.11718543e-06
Iter: 1784 loss: 1.1165024e-06
Iter: 1785 loss: 1.12100849e-06
Iter: 1786 loss: 1.11640043e-06
Iter: 1787 loss: 1.11597274e-06
Iter: 1788 loss: 1.1159749e-06
Iter: 1789 loss: 1.11550867e-06
Iter: 1790 loss: 1.11548979e-06
Iter: 1791 loss: 1.1151684e-06
Iter: 1792 loss: 1.1148087e-06
Iter: 1793 loss: 1.11571057e-06
Iter: 1794 loss: 1.11467352e-06
Iter: 1795 loss: 1.11427892e-06
Iter: 1796 loss: 1.11761278e-06
Iter: 1797 loss: 1.11426198e-06
Iter: 1798 loss: 1.11399174e-06
Iter: 1799 loss: 1.11472195e-06
Iter: 1800 loss: 1.11395832e-06
Iter: 1801 loss: 1.11363329e-06
Iter: 1802 loss: 1.11341956e-06
Iter: 1803 loss: 1.11329337e-06
Iter: 1804 loss: 1.11292195e-06
Iter: 1805 loss: 1.1133659e-06
Iter: 1806 loss: 1.11274915e-06
Iter: 1807 loss: 1.11239558e-06
Iter: 1808 loss: 1.11696227e-06
Iter: 1809 loss: 1.11239433e-06
Iter: 1810 loss: 1.11216366e-06
Iter: 1811 loss: 1.11154668e-06
Iter: 1812 loss: 1.1164384e-06
Iter: 1813 loss: 1.11144959e-06
Iter: 1814 loss: 1.11107636e-06
Iter: 1815 loss: 1.11103986e-06
Iter: 1816 loss: 1.11062764e-06
Iter: 1817 loss: 1.1120402e-06
Iter: 1818 loss: 1.11053589e-06
Iter: 1819 loss: 1.11025508e-06
Iter: 1820 loss: 1.10961275e-06
Iter: 1821 loss: 1.11663655e-06
Iter: 1822 loss: 1.10952556e-06
Iter: 1823 loss: 1.10902101e-06
Iter: 1824 loss: 1.10902135e-06
Iter: 1825 loss: 1.10848396e-06
Iter: 1826 loss: 1.11045324e-06
Iter: 1827 loss: 1.1083747e-06
Iter: 1828 loss: 1.10806536e-06
Iter: 1829 loss: 1.10756775e-06
Iter: 1830 loss: 1.12083058e-06
Iter: 1831 loss: 1.1075665e-06
Iter: 1832 loss: 1.107054e-06
Iter: 1833 loss: 1.10701069e-06
Iter: 1834 loss: 1.10673352e-06
Iter: 1835 loss: 1.10665951e-06
Iter: 1836 loss: 1.1064742e-06
Iter: 1837 loss: 1.10600217e-06
Iter: 1838 loss: 1.10841529e-06
Iter: 1839 loss: 1.10593567e-06
Iter: 1840 loss: 1.10568647e-06
Iter: 1841 loss: 1.10644805e-06
Iter: 1842 loss: 1.10562473e-06
Iter: 1843 loss: 1.10530334e-06
Iter: 1844 loss: 1.10507335e-06
Iter: 1845 loss: 1.10500366e-06
Iter: 1846 loss: 1.10464032e-06
Iter: 1847 loss: 1.10548922e-06
Iter: 1848 loss: 1.10451379e-06
Iter: 1849 loss: 1.10427754e-06
Iter: 1850 loss: 1.10424821e-06
Iter: 1851 loss: 1.10406256e-06
Iter: 1852 loss: 1.10355529e-06
Iter: 1853 loss: 1.10747965e-06
Iter: 1854 loss: 1.10342137e-06
Iter: 1855 loss: 1.10291103e-06
Iter: 1856 loss: 1.10387259e-06
Iter: 1857 loss: 1.10269571e-06
Iter: 1858 loss: 1.10222391e-06
Iter: 1859 loss: 1.10675842e-06
Iter: 1860 loss: 1.10222186e-06
Iter: 1861 loss: 1.10183464e-06
Iter: 1862 loss: 1.10441533e-06
Iter: 1863 loss: 1.10179349e-06
Iter: 1864 loss: 1.10153383e-06
Iter: 1865 loss: 1.10087956e-06
Iter: 1866 loss: 1.10998303e-06
Iter: 1867 loss: 1.10082487e-06
Iter: 1868 loss: 1.10070539e-06
Iter: 1869 loss: 1.10046267e-06
Iter: 1870 loss: 1.10017049e-06
Iter: 1871 loss: 1.09995108e-06
Iter: 1872 loss: 1.09988787e-06
Iter: 1873 loss: 1.09940538e-06
Iter: 1874 loss: 1.10336111e-06
Iter: 1875 loss: 1.09937855e-06
Iter: 1876 loss: 1.09911809e-06
Iter: 1877 loss: 1.09879875e-06
Iter: 1878 loss: 1.09874509e-06
Iter: 1879 loss: 1.09826124e-06
Iter: 1880 loss: 1.10334588e-06
Iter: 1881 loss: 1.09823179e-06
Iter: 1882 loss: 1.09798316e-06
Iter: 1883 loss: 1.0976205e-06
Iter: 1884 loss: 1.09756081e-06
Iter: 1885 loss: 1.09741245e-06
Iter: 1886 loss: 1.09736607e-06
Iter: 1887 loss: 1.09710891e-06
Iter: 1888 loss: 1.0967949e-06
Iter: 1889 loss: 1.09680013e-06
Iter: 1890 loss: 1.09640393e-06
Iter: 1891 loss: 1.09732764e-06
Iter: 1892 loss: 1.09632197e-06
Iter: 1893 loss: 1.09592293e-06
Iter: 1894 loss: 1.09983773e-06
Iter: 1895 loss: 1.09595101e-06
Iter: 1896 loss: 1.09562006e-06
Iter: 1897 loss: 1.09562814e-06
Iter: 1898 loss: 1.0953313e-06
Iter: 1899 loss: 1.09497932e-06
Iter: 1900 loss: 1.09490475e-06
Iter: 1901 loss: 1.09466419e-06
Iter: 1902 loss: 1.09402731e-06
Iter: 1903 loss: 1.10134852e-06
Iter: 1904 loss: 1.09404561e-06
Iter: 1905 loss: 1.09374628e-06
Iter: 1906 loss: 1.09441589e-06
Iter: 1907 loss: 1.09365737e-06
Iter: 1908 loss: 1.09333e-06
Iter: 1909 loss: 1.09422376e-06
Iter: 1910 loss: 1.09320979e-06
Iter: 1911 loss: 1.09296434e-06
Iter: 1912 loss: 1.09341704e-06
Iter: 1913 loss: 1.0928668e-06
Iter: 1914 loss: 1.09251459e-06
Iter: 1915 loss: 1.09213192e-06
Iter: 1916 loss: 1.09204984e-06
Iter: 1917 loss: 1.09157008e-06
Iter: 1918 loss: 1.09312748e-06
Iter: 1919 loss: 1.0914116e-06
Iter: 1920 loss: 1.09102825e-06
Iter: 1921 loss: 1.09099051e-06
Iter: 1922 loss: 1.09077143e-06
Iter: 1923 loss: 1.09032612e-06
Iter: 1924 loss: 1.09667644e-06
Iter: 1925 loss: 1.09025802e-06
Iter: 1926 loss: 1.08983272e-06
Iter: 1927 loss: 1.08982908e-06
Iter: 1928 loss: 1.08944221e-06
Iter: 1929 loss: 1.0899588e-06
Iter: 1930 loss: 1.08925678e-06
Iter: 1931 loss: 1.0889446e-06
Iter: 1932 loss: 1.08891095e-06
Iter: 1933 loss: 1.08869665e-06
Iter: 1934 loss: 1.08837787e-06
Iter: 1935 loss: 1.08837412e-06
Iter: 1936 loss: 1.08811037e-06
Iter: 1937 loss: 1.08770632e-06
Iter: 1938 loss: 1.08772633e-06
Iter: 1939 loss: 1.0872385e-06
Iter: 1940 loss: 1.09231291e-06
Iter: 1941 loss: 1.08723543e-06
Iter: 1942 loss: 1.08694212e-06
Iter: 1943 loss: 1.08683616e-06
Iter: 1944 loss: 1.08670986e-06
Iter: 1945 loss: 1.08630184e-06
Iter: 1946 loss: 1.08863605e-06
Iter: 1947 loss: 1.08622464e-06
Iter: 1948 loss: 1.08586198e-06
Iter: 1949 loss: 1.08553206e-06
Iter: 1950 loss: 1.08543838e-06
Iter: 1951 loss: 1.0852018e-06
Iter: 1952 loss: 1.085176e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.8/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi1.2
+ date
Sun Nov  8 17:55:12 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.2/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.2/300_100_100_100_1 ']'
+ LOAD='--load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.8/300_100_100_100_1'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi0.8/300_100_100_100_1 --function f1 --psi 2 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f462328d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f462328dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f46231166a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f462311d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4623116378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4623043620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f462304e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4622f98840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4622f98158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f462304eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4622f98378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e5848c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4622f98950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e54b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4622fd02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4622fd2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4622ff2400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4622f54950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e47dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e47dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e4ff620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e4ffea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e4546a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e3bf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e3bfb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e4b2378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e3bf598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e33e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e2e7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e30dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e371158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e2c87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e2c8b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e2d8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e28a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f461e407c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.003813505
test_loss: 0.0041647726
train_loss: 0.003276753
test_loss: 0.0036058598
train_loss: 0.003118829
test_loss: 0.0036273059
train_loss: 0.0033923704
test_loss: 0.003628347
train_loss: 0.0032135518
test_loss: 0.0035490284
train_loss: 0.0029956938
test_loss: 0.0034939493
train_loss: 0.003157744
test_loss: 0.0034797287
train_loss: 0.0029686643
test_loss: 0.0034197555
train_loss: 0.0029210648
test_loss: 0.0033389167
train_loss: 0.0028989965
test_loss: 0.0033576512
train_loss: 0.0030686092
test_loss: 0.0035659121
train_loss: 0.0031765997
test_loss: 0.0035736659
train_loss: 0.00308056
test_loss: 0.0036123656
train_loss: 0.0028343722
test_loss: 0.0034640948
train_loss: 0.0031908895
test_loss: 0.0035068332
train_loss: 0.0030151545
test_loss: 0.003492087
train_loss: 0.002927036
test_loss: 0.0033612647
train_loss: 0.0028374437
test_loss: 0.0034219006
train_loss: 0.0030148786
test_loss: 0.0034166083
train_loss: 0.0028440182
test_loss: 0.003344426
train_loss: 0.0031992863
test_loss: 0.003459294
train_loss: 0.0032004
test_loss: 0.003667094
train_loss: 0.0028779055
test_loss: 0.0035280702
train_loss: 0.0031269179
test_loss: 0.0036154566
train_loss: 0.0028320947
test_loss: 0.0034997065
train_loss: 0.0029225755
test_loss: 0.0032878811
train_loss: 0.003000693
test_loss: 0.0035364572
train_loss: 0.0028923983
test_loss: 0.0034057
train_loss: 0.0027232217
test_loss: 0.0034061733
train_loss: 0.0030425698
test_loss: 0.0036619212
train_loss: 0.0027815714
test_loss: 0.0033434713
train_loss: 0.0027926287
test_loss: 0.003362819
train_loss: 0.00296436
test_loss: 0.003523602
train_loss: 0.0027432796
test_loss: 0.0032508175
train_loss: 0.0029905736
test_loss: 0.0033278437
train_loss: 0.0027265279
test_loss: 0.0033292715
train_loss: 0.002756748
test_loss: 0.0032926481
train_loss: 0.0028696056
test_loss: 0.0033307304
train_loss: 0.0026987495
test_loss: 0.0032638141
train_loss: 0.0028781109
test_loss: 0.0033624424
train_loss: 0.0029704806
test_loss: 0.003465665
train_loss: 0.0028618956
test_loss: 0.0034345635
train_loss: 0.0029027741
test_loss: 0.0034055286
train_loss: 0.002922048
test_loss: 0.0034422344
train_loss: 0.002919308
test_loss: 0.0033498902
train_loss: 0.002902682
test_loss: 0.0033499997
train_loss: 0.0028751958
test_loss: 0.0033377467
train_loss: 0.00300763
test_loss: 0.003502785
train_loss: 0.0030108984
test_loss: 0.00342808
train_loss: 0.0029139512
test_loss: 0.0033529138
train_loss: 0.0028583596
test_loss: 0.003378325
train_loss: 0.0029967283
test_loss: 0.003447017
train_loss: 0.0027534687
test_loss: 0.0034118153
train_loss: 0.002894035
test_loss: 0.0032084375
train_loss: 0.0028331457
test_loss: 0.00351484
train_loss: 0.0028754761
test_loss: 0.003370017
train_loss: 0.0027968478
test_loss: 0.0034256876
train_loss: 0.0030175277
test_loss: 0.0032994044
train_loss: 0.0027694895
test_loss: 0.0034247611
train_loss: 0.0029362498
test_loss: 0.0032625832
train_loss: 0.0026213664
test_loss: 0.003173489
train_loss: 0.002881269
test_loss: 0.0032068368
train_loss: 0.0027635752
test_loss: 0.0032085306
train_loss: 0.0027339174
test_loss: 0.003408005
train_loss: 0.0029756352
test_loss: 0.0035042195
train_loss: 0.00280532
test_loss: 0.0033185794
train_loss: 0.002709904
test_loss: 0.003289832
train_loss: 0.0025501756
test_loss: 0.0033007243
train_loss: 0.0028278164
test_loss: 0.0034634422
train_loss: 0.0027792894
test_loss: 0.0033199617
train_loss: 0.0028672782
test_loss: 0.0034075761
train_loss: 0.0027959414
test_loss: 0.0033174898
train_loss: 0.0027808505
test_loss: 0.0033698725
train_loss: 0.0026769976
test_loss: 0.0032899023
train_loss: 0.0027964842
test_loss: 0.003291348
train_loss: 0.0027712919
test_loss: 0.0033472073
train_loss: 0.002925437
test_loss: 0.0035279463
train_loss: 0.003066116
test_loss: 0.0033273436
train_loss: 0.0030963426
test_loss: 0.0033627283
train_loss: 0.0026496642
test_loss: 0.003300526
train_loss: 0.0026952552
test_loss: 0.0032505104
train_loss: 0.0028722165
test_loss: 0.003722846
train_loss: 0.00293085
test_loss: 0.0032682666
train_loss: 0.002687827
test_loss: 0.0032695495
train_loss: 0.0028837137
test_loss: 0.003347046
train_loss: 0.00294481
test_loss: 0.0032299145
train_loss: 0.0026650624
test_loss: 0.0033238563
train_loss: 0.002764375
test_loss: 0.0034185378
train_loss: 0.0028447784
test_loss: 0.0033790648
train_loss: 0.0028036074
test_loss: 0.0032545538
train_loss: 0.0026230002
test_loss: 0.0033504828
train_loss: 0.0029667327
test_loss: 0.0032829437
train_loss: 0.00282913
test_loss: 0.003329687
train_loss: 0.002929775
test_loss: 0.003408209
train_loss: 0.0026885376
test_loss: 0.0034979796
train_loss: 0.0027922485
test_loss: 0.0032964528
train_loss: 0.0026931048
test_loss: 0.0032340526
train_loss: 0.0026850349
test_loss: 0.0032615343
train_loss: 0.0028849659
test_loss: 0.0034076883
train_loss: 0.0026629327
test_loss: 0.0032568404
train_loss: 0.0028052055
test_loss: 0.0032919832
train_loss: 0.0027047778
test_loss: 0.0033165761
train_loss: 0.0028316546
test_loss: 0.0032652835
train_loss: 0.002775979
test_loss: 0.0033342303
train_loss: 0.0026236614
test_loss: 0.0031914942
train_loss: 0.0027916995
test_loss: 0.0032540716
train_loss: 0.0027574631
test_loss: 0.0033253094
train_loss: 0.002671309
test_loss: 0.0032436377
train_loss: 0.002517642
test_loss: 0.0032265869
train_loss: 0.0025256465
test_loss: 0.003119979
train_loss: 0.002642545
test_loss: 0.0032291391
train_loss: 0.0028136706
test_loss: 0.0033921427
train_loss: 0.0028289063
test_loss: 0.0033233583
train_loss: 0.002564613
test_loss: 0.0032293082
train_loss: 0.0027607975
test_loss: 0.003205163
train_loss: 0.00265656
test_loss: 0.003291368
train_loss: 0.0026869385
test_loss: 0.003260324
train_loss: 0.0026429638
test_loss: 0.003199319
train_loss: 0.0026164756
test_loss: 0.0031987305
train_loss: 0.0027961954
test_loss: 0.0033269834
train_loss: 0.0029506774
test_loss: 0.003271409
train_loss: 0.0029033253
test_loss: 0.0033060738
train_loss: 0.0030953726
test_loss: 0.0034340783
train_loss: 0.0025595408
test_loss: 0.00322397
train_loss: 0.0025875417
test_loss: 0.0031563502
train_loss: 0.0025938977
test_loss: 0.0034307842
train_loss: 0.0026081428
test_loss: 0.0032763393
train_loss: 0.0027343356
test_loss: 0.003253066
train_loss: 0.002552995
test_loss: 0.0032317808
train_loss: 0.002653998
test_loss: 0.0032697832
train_loss: 0.0025371858
test_loss: 0.0032206294
train_loss: 0.0027323898
test_loss: 0.0033474509
train_loss: 0.002761117
test_loss: 0.003245268
train_loss: 0.0028071383
test_loss: 0.0032979185
train_loss: 0.0025878171
test_loss: 0.0035066626
train_loss: 0.0026798546
test_loss: 0.0032902746
train_loss: 0.0025841247
test_loss: 0.0031082334
train_loss: 0.002456803
test_loss: 0.0034506528
train_loss: 0.0024863398
test_loss: 0.003274806
train_loss: 0.0028936858
test_loss: 0.0033782087
train_loss: 0.0024703504
test_loss: 0.0031733732
train_loss: 0.0024647673
test_loss: 0.003062329
train_loss: 0.0024472547
test_loss: 0.003187368
train_loss: 0.0028116344
test_loss: 0.0035941128
train_loss: 0.0026731833
test_loss: 0.0033648727
train_loss: 0.0024787246
test_loss: 0.0031285859
train_loss: 0.0025534742
test_loss: 0.003185651
train_loss: 0.002529519
test_loss: 0.0033183577
train_loss: 0.0026719896
test_loss: 0.0031851805
train_loss: 0.0025785135
test_loss: 0.0031779944
train_loss: 0.0029812234
test_loss: 0.0032686184
train_loss: 0.002631457
test_loss: 0.0032768347
train_loss: 0.0026512851
test_loss: 0.0031647873
train_loss: 0.0025319024
test_loss: 0.0030935076
train_loss: 0.0026829531
test_loss: 0.0031571765
train_loss: 0.0027157352
test_loss: 0.0031169732
train_loss: 0.0025572209
test_loss: 0.003123693
train_loss: 0.0027694814
test_loss: 0.0033405514
train_loss: 0.0025837556
test_loss: 0.0031246415
train_loss: 0.0026663002
test_loss: 0.0031865002
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi1.2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.2/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi1.2/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd6d6950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd60f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd62b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd62b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd58e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd4e1b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd4cec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd487840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd4877b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd487488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd440598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd403e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd41c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd3d5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd382a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd399d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd39a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd39a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd30b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38dd30b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38cb119598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38cb119400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38cb0be8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38cb0a2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38cb09bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38cb040b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38cb01a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38cb028730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38cb028b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38cafd48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38cafd4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38cafd4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38caf4a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38caf7c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38caf16950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38caf18d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.08361983e-05
Iter: 2 loss: 9.20642378e-06
Iter: 3 loss: 9.16116551e-06
Iter: 4 loss: 8.42255758e-06
Iter: 5 loss: 8.91002219e-06
Iter: 6 loss: 7.95607775e-06
Iter: 7 loss: 7.42000975e-06
Iter: 8 loss: 9.28637564e-06
Iter: 9 loss: 7.28051e-06
Iter: 10 loss: 6.79368895e-06
Iter: 11 loss: 7.73988904e-06
Iter: 12 loss: 6.59111356e-06
Iter: 13 loss: 6.33625859e-06
Iter: 14 loss: 6.33636864e-06
Iter: 15 loss: 6.10591087e-06
Iter: 16 loss: 5.95253277e-06
Iter: 17 loss: 5.86601254e-06
Iter: 18 loss: 5.68675341e-06
Iter: 19 loss: 5.68258474e-06
Iter: 20 loss: 5.52389383e-06
Iter: 21 loss: 5.16565296e-06
Iter: 22 loss: 9.99925942e-06
Iter: 23 loss: 5.14407748e-06
Iter: 24 loss: 4.98914596e-06
Iter: 25 loss: 4.97451674e-06
Iter: 26 loss: 4.80277231e-06
Iter: 27 loss: 4.70776331e-06
Iter: 28 loss: 4.63214747e-06
Iter: 29 loss: 4.43668159e-06
Iter: 30 loss: 4.44557327e-06
Iter: 31 loss: 4.28306566e-06
Iter: 32 loss: 4.07141079e-06
Iter: 33 loss: 4.44917532e-06
Iter: 34 loss: 3.97912936e-06
Iter: 35 loss: 3.83815768e-06
Iter: 36 loss: 5.13646137e-06
Iter: 37 loss: 3.83204406e-06
Iter: 38 loss: 3.71688338e-06
Iter: 39 loss: 5.23372091e-06
Iter: 40 loss: 3.71621e-06
Iter: 41 loss: 3.63702588e-06
Iter: 42 loss: 3.55991165e-06
Iter: 43 loss: 3.5427131e-06
Iter: 44 loss: 3.45848957e-06
Iter: 45 loss: 4.48819128e-06
Iter: 46 loss: 3.457503e-06
Iter: 47 loss: 3.39242e-06
Iter: 48 loss: 3.37358915e-06
Iter: 49 loss: 3.33416074e-06
Iter: 50 loss: 3.22546748e-06
Iter: 51 loss: 4.04392767e-06
Iter: 52 loss: 3.21705784e-06
Iter: 53 loss: 3.16479e-06
Iter: 54 loss: 3.29013255e-06
Iter: 55 loss: 3.145753e-06
Iter: 56 loss: 3.08079552e-06
Iter: 57 loss: 3.2390758e-06
Iter: 58 loss: 3.05758522e-06
Iter: 59 loss: 3.01265163e-06
Iter: 60 loss: 2.97720635e-06
Iter: 61 loss: 2.96330427e-06
Iter: 62 loss: 2.90958906e-06
Iter: 63 loss: 2.908346e-06
Iter: 64 loss: 2.87835519e-06
Iter: 65 loss: 2.81820894e-06
Iter: 66 loss: 3.95136567e-06
Iter: 67 loss: 2.81745042e-06
Iter: 68 loss: 2.75865477e-06
Iter: 69 loss: 2.83184499e-06
Iter: 70 loss: 2.72822808e-06
Iter: 71 loss: 2.67907944e-06
Iter: 72 loss: 3.4319678e-06
Iter: 73 loss: 2.67905398e-06
Iter: 74 loss: 2.63961192e-06
Iter: 75 loss: 3.00874854e-06
Iter: 76 loss: 2.63804782e-06
Iter: 77 loss: 2.61685955e-06
Iter: 78 loss: 2.58568116e-06
Iter: 79 loss: 2.58488785e-06
Iter: 80 loss: 2.55794748e-06
Iter: 81 loss: 2.5578579e-06
Iter: 82 loss: 2.53608687e-06
Iter: 83 loss: 2.52839641e-06
Iter: 84 loss: 2.51611118e-06
Iter: 85 loss: 2.48109654e-06
Iter: 86 loss: 2.7525341e-06
Iter: 87 loss: 2.47860294e-06
Iter: 88 loss: 2.46096488e-06
Iter: 89 loss: 2.50405083e-06
Iter: 90 loss: 2.45455749e-06
Iter: 91 loss: 2.43116642e-06
Iter: 92 loss: 2.44966895e-06
Iter: 93 loss: 2.41717953e-06
Iter: 94 loss: 2.3933203e-06
Iter: 95 loss: 2.38633902e-06
Iter: 96 loss: 2.37201084e-06
Iter: 97 loss: 2.34501431e-06
Iter: 98 loss: 2.34485424e-06
Iter: 99 loss: 2.32817e-06
Iter: 100 loss: 2.29344391e-06
Iter: 101 loss: 2.87860667e-06
Iter: 102 loss: 2.29251123e-06
Iter: 103 loss: 2.26284988e-06
Iter: 104 loss: 2.33456217e-06
Iter: 105 loss: 2.25228382e-06
Iter: 106 loss: 2.23032566e-06
Iter: 107 loss: 2.23008874e-06
Iter: 108 loss: 2.21305868e-06
Iter: 109 loss: 2.33928586e-06
Iter: 110 loss: 2.2116019e-06
Iter: 111 loss: 2.20322954e-06
Iter: 112 loss: 2.18481841e-06
Iter: 113 loss: 2.44724424e-06
Iter: 114 loss: 2.18384594e-06
Iter: 115 loss: 2.17291881e-06
Iter: 116 loss: 2.17127035e-06
Iter: 117 loss: 2.16050807e-06
Iter: 118 loss: 2.15631121e-06
Iter: 119 loss: 2.15051728e-06
Iter: 120 loss: 2.13296221e-06
Iter: 121 loss: 2.2389122e-06
Iter: 122 loss: 2.13079534e-06
Iter: 123 loss: 2.12111104e-06
Iter: 124 loss: 2.15237606e-06
Iter: 125 loss: 2.11842325e-06
Iter: 126 loss: 2.10640314e-06
Iter: 127 loss: 2.10428675e-06
Iter: 128 loss: 2.09605969e-06
Iter: 129 loss: 2.08312053e-06
Iter: 130 loss: 2.11304132e-06
Iter: 131 loss: 2.07822927e-06
Iter: 132 loss: 2.06538107e-06
Iter: 133 loss: 2.20091761e-06
Iter: 134 loss: 2.06499544e-06
Iter: 135 loss: 2.05674178e-06
Iter: 136 loss: 2.03993432e-06
Iter: 137 loss: 2.34364666e-06
Iter: 138 loss: 2.03966511e-06
Iter: 139 loss: 2.02463934e-06
Iter: 140 loss: 2.04020375e-06
Iter: 141 loss: 2.01630155e-06
Iter: 142 loss: 2.01819194e-06
Iter: 143 loss: 2.00930276e-06
Iter: 144 loss: 2.00315094e-06
Iter: 145 loss: 1.99534634e-06
Iter: 146 loss: 1.99477881e-06
Iter: 147 loss: 1.98430553e-06
Iter: 148 loss: 1.98208795e-06
Iter: 149 loss: 1.97530562e-06
Iter: 150 loss: 1.9680781e-06
Iter: 151 loss: 1.96671749e-06
Iter: 152 loss: 1.96038945e-06
Iter: 153 loss: 1.95992334e-06
Iter: 154 loss: 1.95524945e-06
Iter: 155 loss: 1.94587574e-06
Iter: 156 loss: 1.99358419e-06
Iter: 157 loss: 1.94437371e-06
Iter: 158 loss: 1.93817505e-06
Iter: 159 loss: 1.95342477e-06
Iter: 160 loss: 1.93601909e-06
Iter: 161 loss: 1.92818652e-06
Iter: 162 loss: 1.93093865e-06
Iter: 163 loss: 1.92276821e-06
Iter: 164 loss: 1.91513118e-06
Iter: 165 loss: 1.93330743e-06
Iter: 166 loss: 1.91241975e-06
Iter: 167 loss: 1.90334458e-06
Iter: 168 loss: 1.95811344e-06
Iter: 169 loss: 1.90223068e-06
Iter: 170 loss: 1.895987e-06
Iter: 171 loss: 1.88387241e-06
Iter: 172 loss: 2.12726195e-06
Iter: 173 loss: 1.88385366e-06
Iter: 174 loss: 1.87117121e-06
Iter: 175 loss: 1.91154459e-06
Iter: 176 loss: 1.86755699e-06
Iter: 177 loss: 1.86415627e-06
Iter: 178 loss: 1.8617668e-06
Iter: 179 loss: 1.85649128e-06
Iter: 180 loss: 1.85000192e-06
Iter: 181 loss: 1.84945497e-06
Iter: 182 loss: 1.84197791e-06
Iter: 183 loss: 1.83931593e-06
Iter: 184 loss: 1.83511429e-06
Iter: 185 loss: 1.83431484e-06
Iter: 186 loss: 1.83040902e-06
Iter: 187 loss: 1.82752217e-06
Iter: 188 loss: 1.82509143e-06
Iter: 189 loss: 1.82440363e-06
Iter: 190 loss: 1.81819757e-06
Iter: 191 loss: 1.82895224e-06
Iter: 192 loss: 1.81552025e-06
Iter: 193 loss: 1.80920597e-06
Iter: 194 loss: 1.82807469e-06
Iter: 195 loss: 1.80727557e-06
Iter: 196 loss: 1.80143707e-06
Iter: 197 loss: 1.82071915e-06
Iter: 198 loss: 1.79963706e-06
Iter: 199 loss: 1.7956861e-06
Iter: 200 loss: 1.79764049e-06
Iter: 201 loss: 1.79300343e-06
Iter: 202 loss: 1.78580456e-06
Iter: 203 loss: 1.81043902e-06
Iter: 204 loss: 1.7839086e-06
Iter: 205 loss: 1.77980701e-06
Iter: 206 loss: 1.77318077e-06
Iter: 207 loss: 1.77315883e-06
Iter: 208 loss: 1.76524577e-06
Iter: 209 loss: 1.80491452e-06
Iter: 210 loss: 1.76397725e-06
Iter: 211 loss: 1.7609392e-06
Iter: 212 loss: 1.76045728e-06
Iter: 213 loss: 1.75672517e-06
Iter: 214 loss: 1.75081027e-06
Iter: 215 loss: 1.75076821e-06
Iter: 216 loss: 1.74602189e-06
Iter: 217 loss: 1.75443199e-06
Iter: 218 loss: 1.74397485e-06
Iter: 219 loss: 1.73970454e-06
Iter: 220 loss: 1.73968067e-06
Iter: 221 loss: 1.7364631e-06
Iter: 222 loss: 1.73316607e-06
Iter: 223 loss: 1.73258252e-06
Iter: 224 loss: 1.72738226e-06
Iter: 225 loss: 1.77449806e-06
Iter: 226 loss: 1.72708951e-06
Iter: 227 loss: 1.72389923e-06
Iter: 228 loss: 1.7261716e-06
Iter: 229 loss: 1.72195143e-06
Iter: 230 loss: 1.71839713e-06
Iter: 231 loss: 1.73685271e-06
Iter: 232 loss: 1.71781073e-06
Iter: 233 loss: 1.71493684e-06
Iter: 234 loss: 1.71206636e-06
Iter: 235 loss: 1.71146246e-06
Iter: 236 loss: 1.70545832e-06
Iter: 237 loss: 1.75130708e-06
Iter: 238 loss: 1.70502608e-06
Iter: 239 loss: 1.70276689e-06
Iter: 240 loss: 1.69775706e-06
Iter: 241 loss: 1.76500976e-06
Iter: 242 loss: 1.69746113e-06
Iter: 243 loss: 1.69161922e-06
Iter: 244 loss: 1.71619695e-06
Iter: 245 loss: 1.6904587e-06
Iter: 246 loss: 1.6869476e-06
Iter: 247 loss: 1.68683812e-06
Iter: 248 loss: 1.68321731e-06
Iter: 249 loss: 1.68623842e-06
Iter: 250 loss: 1.68117413e-06
Iter: 251 loss: 1.67850067e-06
Iter: 252 loss: 1.67581538e-06
Iter: 253 loss: 1.67529583e-06
Iter: 254 loss: 1.67289159e-06
Iter: 255 loss: 1.6723211e-06
Iter: 256 loss: 1.67066412e-06
Iter: 257 loss: 1.66831637e-06
Iter: 258 loss: 1.66823884e-06
Iter: 259 loss: 1.66499137e-06
Iter: 260 loss: 1.69344833e-06
Iter: 261 loss: 1.66479458e-06
Iter: 262 loss: 1.66285668e-06
Iter: 263 loss: 1.66095185e-06
Iter: 264 loss: 1.6606084e-06
Iter: 265 loss: 1.65684287e-06
Iter: 266 loss: 1.67284611e-06
Iter: 267 loss: 1.65602751e-06
Iter: 268 loss: 1.65244057e-06
Iter: 269 loss: 1.65774622e-06
Iter: 270 loss: 1.65069605e-06
Iter: 271 loss: 1.64765379e-06
Iter: 272 loss: 1.6852739e-06
Iter: 273 loss: 1.64768153e-06
Iter: 274 loss: 1.64568939e-06
Iter: 275 loss: 1.64073583e-06
Iter: 276 loss: 1.68164843e-06
Iter: 277 loss: 1.63981906e-06
Iter: 278 loss: 1.63535935e-06
Iter: 279 loss: 1.66576592e-06
Iter: 280 loss: 1.63497236e-06
Iter: 281 loss: 1.63160848e-06
Iter: 282 loss: 1.65570259e-06
Iter: 283 loss: 1.63129414e-06
Iter: 284 loss: 1.6290843e-06
Iter: 285 loss: 1.62908634e-06
Iter: 286 loss: 1.62773131e-06
Iter: 287 loss: 1.6240341e-06
Iter: 288 loss: 1.64665971e-06
Iter: 289 loss: 1.6231287e-06
Iter: 290 loss: 1.62054471e-06
Iter: 291 loss: 1.62055903e-06
Iter: 292 loss: 1.61782714e-06
Iter: 293 loss: 1.6294789e-06
Iter: 294 loss: 1.61726155e-06
Iter: 295 loss: 1.61550008e-06
Iter: 296 loss: 1.61650132e-06
Iter: 297 loss: 1.61443882e-06
Iter: 298 loss: 1.61159915e-06
Iter: 299 loss: 1.61851892e-06
Iter: 300 loss: 1.61062928e-06
Iter: 301 loss: 1.60846048e-06
Iter: 302 loss: 1.60649768e-06
Iter: 303 loss: 1.60588615e-06
Iter: 304 loss: 1.60378363e-06
Iter: 305 loss: 1.60363652e-06
Iter: 306 loss: 1.60209925e-06
Iter: 307 loss: 1.60049103e-06
Iter: 308 loss: 1.60020181e-06
Iter: 309 loss: 1.59692115e-06
Iter: 310 loss: 1.61022945e-06
Iter: 311 loss: 1.59622527e-06
Iter: 312 loss: 1.59430442e-06
Iter: 313 loss: 1.59150568e-06
Iter: 314 loss: 1.59144633e-06
Iter: 315 loss: 1.58787975e-06
Iter: 316 loss: 1.6046356e-06
Iter: 317 loss: 1.58725538e-06
Iter: 318 loss: 1.58399757e-06
Iter: 319 loss: 1.58659395e-06
Iter: 320 loss: 1.581996e-06
Iter: 321 loss: 1.57871341e-06
Iter: 322 loss: 1.58366765e-06
Iter: 323 loss: 1.57713873e-06
Iter: 324 loss: 1.57673969e-06
Iter: 325 loss: 1.57540148e-06
Iter: 326 loss: 1.57381032e-06
Iter: 327 loss: 1.57478814e-06
Iter: 328 loss: 1.57285626e-06
Iter: 329 loss: 1.57133036e-06
Iter: 330 loss: 1.5742e-06
Iter: 331 loss: 1.57073021e-06
Iter: 332 loss: 1.56841134e-06
Iter: 333 loss: 1.57082786e-06
Iter: 334 loss: 1.56717965e-06
Iter: 335 loss: 1.56543547e-06
Iter: 336 loss: 1.5666576e-06
Iter: 337 loss: 1.56430497e-06
Iter: 338 loss: 1.56152487e-06
Iter: 339 loss: 1.57851014e-06
Iter: 340 loss: 1.56116835e-06
Iter: 341 loss: 1.55989051e-06
Iter: 342 loss: 1.56195415e-06
Iter: 343 loss: 1.55929547e-06
Iter: 344 loss: 1.55729708e-06
Iter: 345 loss: 1.5574401e-06
Iter: 346 loss: 1.5557888e-06
Iter: 347 loss: 1.55381849e-06
Iter: 348 loss: 1.57730597e-06
Iter: 349 loss: 1.55383327e-06
Iter: 350 loss: 1.55224211e-06
Iter: 351 loss: 1.55020598e-06
Iter: 352 loss: 1.550055e-06
Iter: 353 loss: 1.54767645e-06
Iter: 354 loss: 1.55037503e-06
Iter: 355 loss: 1.54644636e-06
Iter: 356 loss: 1.54355655e-06
Iter: 357 loss: 1.54493148e-06
Iter: 358 loss: 1.54159716e-06
Iter: 359 loss: 1.53981159e-06
Iter: 360 loss: 1.53975384e-06
Iter: 361 loss: 1.53810026e-06
Iter: 362 loss: 1.55064845e-06
Iter: 363 loss: 1.53792689e-06
Iter: 364 loss: 1.53677968e-06
Iter: 365 loss: 1.53453936e-06
Iter: 366 loss: 1.57956538e-06
Iter: 367 loss: 1.53447445e-06
Iter: 368 loss: 1.53305848e-06
Iter: 369 loss: 1.53282883e-06
Iter: 370 loss: 1.53194742e-06
Iter: 371 loss: 1.53033852e-06
Iter: 372 loss: 1.56865224e-06
Iter: 373 loss: 1.53037e-06
Iter: 374 loss: 1.52909899e-06
Iter: 375 loss: 1.52905034e-06
Iter: 376 loss: 1.52785697e-06
Iter: 377 loss: 1.52643247e-06
Iter: 378 loss: 1.52620828e-06
Iter: 379 loss: 1.524553e-06
Iter: 380 loss: 1.53381097e-06
Iter: 381 loss: 1.52430857e-06
Iter: 382 loss: 1.52247128e-06
Iter: 383 loss: 1.52592361e-06
Iter: 384 loss: 1.52171049e-06
Iter: 385 loss: 1.52039956e-06
Iter: 386 loss: 1.53184078e-06
Iter: 387 loss: 1.52029884e-06
Iter: 388 loss: 1.51912025e-06
Iter: 389 loss: 1.51665563e-06
Iter: 390 loss: 1.56006013e-06
Iter: 391 loss: 1.51664653e-06
Iter: 392 loss: 1.51437393e-06
Iter: 393 loss: 1.52203393e-06
Iter: 394 loss: 1.51371046e-06
Iter: 395 loss: 1.51196014e-06
Iter: 396 loss: 1.52200505e-06
Iter: 397 loss: 1.51166978e-06
Iter: 398 loss: 1.51040297e-06
Iter: 399 loss: 1.51034169e-06
Iter: 400 loss: 1.50946812e-06
Iter: 401 loss: 1.50745518e-06
Iter: 402 loss: 1.53542101e-06
Iter: 403 loss: 1.50735877e-06
Iter: 404 loss: 1.50665369e-06
Iter: 405 loss: 1.50622554e-06
Iter: 406 loss: 1.5055698e-06
Iter: 407 loss: 1.50384608e-06
Iter: 408 loss: 1.52219457e-06
Iter: 409 loss: 1.50369124e-06
Iter: 410 loss: 1.50209632e-06
Iter: 411 loss: 1.50206392e-06
Iter: 412 loss: 1.50067103e-06
Iter: 413 loss: 1.50053279e-06
Iter: 414 loss: 1.49957464e-06
Iter: 415 loss: 1.49821335e-06
Iter: 416 loss: 1.49783955e-06
Iter: 417 loss: 1.49701884e-06
Iter: 418 loss: 1.49556888e-06
Iter: 419 loss: 1.49558491e-06
Iter: 420 loss: 1.49445577e-06
Iter: 421 loss: 1.49507707e-06
Iter: 422 loss: 1.49376956e-06
Iter: 423 loss: 1.49208074e-06
Iter: 424 loss: 1.49334574e-06
Iter: 425 loss: 1.49106279e-06
Iter: 426 loss: 1.48978563e-06
Iter: 427 loss: 1.48893719e-06
Iter: 428 loss: 1.48845129e-06
Iter: 429 loss: 1.4877495e-06
Iter: 430 loss: 1.48755748e-06
Iter: 431 loss: 1.48649451e-06
Iter: 432 loss: 1.48545837e-06
Iter: 433 loss: 1.48528648e-06
Iter: 434 loss: 1.48402864e-06
Iter: 435 loss: 1.4856148e-06
Iter: 436 loss: 1.48338711e-06
Iter: 437 loss: 1.48207369e-06
Iter: 438 loss: 1.50283347e-06
Iter: 439 loss: 1.48204253e-06
Iter: 440 loss: 1.48132131e-06
Iter: 441 loss: 1.48000674e-06
Iter: 442 loss: 1.51142285e-06
Iter: 443 loss: 1.48001709e-06
Iter: 444 loss: 1.47864148e-06
Iter: 445 loss: 1.47864807e-06
Iter: 446 loss: 1.47774563e-06
Iter: 447 loss: 1.47630931e-06
Iter: 448 loss: 1.47630567e-06
Iter: 449 loss: 1.47475362e-06
Iter: 450 loss: 1.47579806e-06
Iter: 451 loss: 1.47383025e-06
Iter: 452 loss: 1.4727998e-06
Iter: 453 loss: 1.47261051e-06
Iter: 454 loss: 1.47178343e-06
Iter: 455 loss: 1.47140531e-06
Iter: 456 loss: 1.47098785e-06
Iter: 457 loss: 1.46961725e-06
Iter: 458 loss: 1.47335322e-06
Iter: 459 loss: 1.46917239e-06
Iter: 460 loss: 1.46817661e-06
Iter: 461 loss: 1.46838579e-06
Iter: 462 loss: 1.46754599e-06
Iter: 463 loss: 1.46645175e-06
Iter: 464 loss: 1.46645857e-06
Iter: 465 loss: 1.46535911e-06
Iter: 466 loss: 1.46443062e-06
Iter: 467 loss: 1.46408411e-06
Iter: 468 loss: 1.46308753e-06
Iter: 469 loss: 1.46834338e-06
Iter: 470 loss: 1.46301318e-06
Iter: 471 loss: 1.46180435e-06
Iter: 472 loss: 1.46386765e-06
Iter: 473 loss: 1.46128718e-06
Iter: 474 loss: 1.4604002e-06
Iter: 475 loss: 1.45995614e-06
Iter: 476 loss: 1.45952094e-06
Iter: 477 loss: 1.45804825e-06
Iter: 478 loss: 1.475609e-06
Iter: 479 loss: 1.45804108e-06
Iter: 480 loss: 1.45722595e-06
Iter: 481 loss: 1.45601939e-06
Iter: 482 loss: 1.45602962e-06
Iter: 483 loss: 1.45473541e-06
Iter: 484 loss: 1.45563081e-06
Iter: 485 loss: 1.45388e-06
Iter: 486 loss: 1.45285071e-06
Iter: 487 loss: 1.4527202e-06
Iter: 488 loss: 1.45200238e-06
Iter: 489 loss: 1.45101649e-06
Iter: 490 loss: 1.45101967e-06
Iter: 491 loss: 1.44963883e-06
Iter: 492 loss: 1.45797799e-06
Iter: 493 loss: 1.44951423e-06
Iter: 494 loss: 1.44854369e-06
Iter: 495 loss: 1.44819887e-06
Iter: 496 loss: 1.44767819e-06
Iter: 497 loss: 1.44633918e-06
Iter: 498 loss: 1.44635953e-06
Iter: 499 loss: 1.44557021e-06
Iter: 500 loss: 1.4444505e-06
Iter: 501 loss: 1.44443129e-06
Iter: 502 loss: 1.44356898e-06
Iter: 503 loss: 1.44355295e-06
Iter: 504 loss: 1.44278886e-06
Iter: 505 loss: 1.44121054e-06
Iter: 506 loss: 1.46713955e-06
Iter: 507 loss: 1.44111573e-06
Iter: 508 loss: 1.44039655e-06
Iter: 509 loss: 1.44026626e-06
Iter: 510 loss: 1.43959028e-06
Iter: 511 loss: 1.43880209e-06
Iter: 512 loss: 1.43867669e-06
Iter: 513 loss: 1.43774639e-06
Iter: 514 loss: 1.43727425e-06
Iter: 515 loss: 1.43682075e-06
Iter: 516 loss: 1.43591069e-06
Iter: 517 loss: 1.43667103e-06
Iter: 518 loss: 1.43540751e-06
Iter: 519 loss: 1.43392595e-06
Iter: 520 loss: 1.43650186e-06
Iter: 521 loss: 1.43332477e-06
Iter: 522 loss: 1.43218608e-06
Iter: 523 loss: 1.43296188e-06
Iter: 524 loss: 1.43146599e-06
Iter: 525 loss: 1.43012323e-06
Iter: 526 loss: 1.44109049e-06
Iter: 527 loss: 1.43003558e-06
Iter: 528 loss: 1.42895522e-06
Iter: 529 loss: 1.43313105e-06
Iter: 530 loss: 1.42864519e-06
Iter: 531 loss: 1.42782596e-06
Iter: 532 loss: 1.43789771e-06
Iter: 533 loss: 1.42783222e-06
Iter: 534 loss: 1.42727049e-06
Iter: 535 loss: 1.42583383e-06
Iter: 536 loss: 1.43881061e-06
Iter: 537 loss: 1.42560214e-06
Iter: 538 loss: 1.42490467e-06
Iter: 539 loss: 1.4247571e-06
Iter: 540 loss: 1.42392287e-06
Iter: 541 loss: 1.42289412e-06
Iter: 542 loss: 1.42275258e-06
Iter: 543 loss: 1.42170211e-06
Iter: 544 loss: 1.42236865e-06
Iter: 545 loss: 1.4210035e-06
Iter: 546 loss: 1.42024214e-06
Iter: 547 loss: 1.42019951e-06
Iter: 548 loss: 1.41947e-06
Iter: 549 loss: 1.41847499e-06
Iter: 550 loss: 1.41839746e-06
Iter: 551 loss: 1.41750832e-06
Iter: 552 loss: 1.42768624e-06
Iter: 553 loss: 1.41749842e-06
Iter: 554 loss: 1.41662065e-06
Iter: 555 loss: 1.41688679e-06
Iter: 556 loss: 1.41601799e-06
Iter: 557 loss: 1.41506689e-06
Iter: 558 loss: 1.41591931e-06
Iter: 559 loss: 1.41455348e-06
Iter: 560 loss: 1.41338614e-06
Iter: 561 loss: 1.41672922e-06
Iter: 562 loss: 1.41305873e-06
Iter: 563 loss: 1.41190026e-06
Iter: 564 loss: 1.41980763e-06
Iter: 565 loss: 1.41180544e-06
Iter: 566 loss: 1.41100554e-06
Iter: 567 loss: 1.41711939e-06
Iter: 568 loss: 1.41090993e-06
Iter: 569 loss: 1.41028204e-06
Iter: 570 loss: 1.40901921e-06
Iter: 571 loss: 1.43325872e-06
Iter: 572 loss: 1.40897419e-06
Iter: 573 loss: 1.407882e-06
Iter: 574 loss: 1.42147542e-06
Iter: 575 loss: 1.40789189e-06
Iter: 576 loss: 1.40684449e-06
Iter: 577 loss: 1.40967836e-06
Iter: 578 loss: 1.40652787e-06
Iter: 579 loss: 1.40581403e-06
Iter: 580 loss: 1.40471741e-06
Iter: 581 loss: 1.40470661e-06
Iter: 582 loss: 1.4038435e-06
Iter: 583 loss: 1.40381349e-06
Iter: 584 loss: 1.40280576e-06
Iter: 585 loss: 1.40370105e-06
Iter: 586 loss: 1.40225598e-06
Iter: 587 loss: 1.40154111e-06
Iter: 588 loss: 1.40288557e-06
Iter: 589 loss: 1.40124223e-06
Iter: 590 loss: 1.40034069e-06
Iter: 591 loss: 1.40586633e-06
Iter: 592 loss: 1.40015618e-06
Iter: 593 loss: 1.39948645e-06
Iter: 594 loss: 1.39823112e-06
Iter: 595 loss: 1.42770682e-06
Iter: 596 loss: 1.39826966e-06
Iter: 597 loss: 1.39705821e-06
Iter: 598 loss: 1.40780321e-06
Iter: 599 loss: 1.39704684e-06
Iter: 600 loss: 1.39603651e-06
Iter: 601 loss: 1.39985286e-06
Iter: 602 loss: 1.39582539e-06
Iter: 603 loss: 1.3951128e-06
Iter: 604 loss: 1.40206487e-06
Iter: 605 loss: 1.39508404e-06
Iter: 606 loss: 1.39443705e-06
Iter: 607 loss: 1.3946069e-06
Iter: 608 loss: 1.39393273e-06
Iter: 609 loss: 1.39317854e-06
Iter: 610 loss: 1.393003e-06
Iter: 611 loss: 1.39257543e-06
Iter: 612 loss: 1.39165297e-06
Iter: 613 loss: 1.39169924e-06
Iter: 614 loss: 1.39118788e-06
Iter: 615 loss: 1.39005351e-06
Iter: 616 loss: 1.40452119e-06
Iter: 617 loss: 1.38990538e-06
Iter: 618 loss: 1.38877181e-06
Iter: 619 loss: 1.39791666e-06
Iter: 620 loss: 1.38868245e-06
Iter: 621 loss: 1.38769087e-06
Iter: 622 loss: 1.39666486e-06
Iter: 623 loss: 1.3876413e-06
Iter: 624 loss: 1.38704968e-06
Iter: 625 loss: 1.38616861e-06
Iter: 626 loss: 1.38613188e-06
Iter: 627 loss: 1.38543294e-06
Iter: 628 loss: 1.38539963e-06
Iter: 629 loss: 1.38485632e-06
Iter: 630 loss: 1.38373593e-06
Iter: 631 loss: 1.40225666e-06
Iter: 632 loss: 1.38368932e-06
Iter: 633 loss: 1.38271434e-06
Iter: 634 loss: 1.38689336e-06
Iter: 635 loss: 1.38244468e-06
Iter: 636 loss: 1.38153e-06
Iter: 637 loss: 1.38947212e-06
Iter: 638 loss: 1.38148027e-06
Iter: 639 loss: 1.38077155e-06
Iter: 640 loss: 1.38354244e-06
Iter: 641 loss: 1.38062524e-06
Iter: 642 loss: 1.37999314e-06
Iter: 643 loss: 1.38447717e-06
Iter: 644 loss: 1.37987581e-06
Iter: 645 loss: 1.37946472e-06
Iter: 646 loss: 1.37863674e-06
Iter: 647 loss: 1.39590247e-06
Iter: 648 loss: 1.37865334e-06
Iter: 649 loss: 1.37789687e-06
Iter: 650 loss: 1.377947e-06
Iter: 651 loss: 1.37723305e-06
Iter: 652 loss: 1.37699385e-06
Iter: 653 loss: 1.37664733e-06
Iter: 654 loss: 1.37576023e-06
Iter: 655 loss: 1.37534039e-06
Iter: 656 loss: 1.37501775e-06
Iter: 657 loss: 1.37417055e-06
Iter: 658 loss: 1.37408904e-06
Iter: 659 loss: 1.37353788e-06
Iter: 660 loss: 1.37240909e-06
Iter: 661 loss: 1.3940263e-06
Iter: 662 loss: 1.37242341e-06
Iter: 663 loss: 1.37177904e-06
Iter: 664 loss: 1.37171878e-06
Iter: 665 loss: 1.37111931e-06
Iter: 666 loss: 1.37007851e-06
Iter: 667 loss: 1.37008101e-06
Iter: 668 loss: 1.36907101e-06
Iter: 669 loss: 1.37140842e-06
Iter: 670 loss: 1.36875201e-06
Iter: 671 loss: 1.36793619e-06
Iter: 672 loss: 1.37506595e-06
Iter: 673 loss: 1.36788947e-06
Iter: 674 loss: 1.36701397e-06
Iter: 675 loss: 1.36795575e-06
Iter: 676 loss: 1.36652432e-06
Iter: 677 loss: 1.36581025e-06
Iter: 678 loss: 1.37693087e-06
Iter: 679 loss: 1.36577842e-06
Iter: 680 loss: 1.36532401e-06
Iter: 681 loss: 1.36468407e-06
Iter: 682 loss: 1.36473705e-06
Iter: 683 loss: 1.36389758e-06
Iter: 684 loss: 1.36935569e-06
Iter: 685 loss: 1.36380584e-06
Iter: 686 loss: 1.36283802e-06
Iter: 687 loss: 1.36252618e-06
Iter: 688 loss: 1.36206154e-06
Iter: 689 loss: 1.36127892e-06
Iter: 690 loss: 1.36443623e-06
Iter: 691 loss: 1.36108486e-06
Iter: 692 loss: 1.36026529e-06
Iter: 693 loss: 1.36623805e-06
Iter: 694 loss: 1.36019889e-06
Iter: 695 loss: 1.35963887e-06
Iter: 696 loss: 1.35906691e-06
Iter: 697 loss: 1.35894561e-06
Iter: 698 loss: 1.35835398e-06
Iter: 699 loss: 1.35834102e-06
Iter: 700 loss: 1.35780022e-06
Iter: 701 loss: 1.35657911e-06
Iter: 702 loss: 1.37217e-06
Iter: 703 loss: 1.35648872e-06
Iter: 704 loss: 1.35534947e-06
Iter: 705 loss: 1.35956157e-06
Iter: 706 loss: 1.35505832e-06
Iter: 707 loss: 1.35422465e-06
Iter: 708 loss: 1.36654853e-06
Iter: 709 loss: 1.3541719e-06
Iter: 710 loss: 1.35349046e-06
Iter: 711 loss: 1.35455855e-06
Iter: 712 loss: 1.35313053e-06
Iter: 713 loss: 1.35247114e-06
Iter: 714 loss: 1.35889081e-06
Iter: 715 loss: 1.35242931e-06
Iter: 716 loss: 1.35196183e-06
Iter: 717 loss: 1.35147286e-06
Iter: 718 loss: 1.35140658e-06
Iter: 719 loss: 1.3506658e-06
Iter: 720 loss: 1.35951313e-06
Iter: 721 loss: 1.35068899e-06
Iter: 722 loss: 1.35011157e-06
Iter: 723 loss: 1.34900074e-06
Iter: 724 loss: 1.34900779e-06
Iter: 725 loss: 1.34814968e-06
Iter: 726 loss: 1.35935693e-06
Iter: 727 loss: 1.34814195e-06
Iter: 728 loss: 1.34743721e-06
Iter: 729 loss: 1.35026403e-06
Iter: 730 loss: 1.34727293e-06
Iter: 731 loss: 1.34674724e-06
Iter: 732 loss: 1.34582217e-06
Iter: 733 loss: 1.34578261e-06
Iter: 734 loss: 1.34495463e-06
Iter: 735 loss: 1.34490836e-06
Iter: 736 loss: 1.34436573e-06
Iter: 737 loss: 1.34335778e-06
Iter: 738 loss: 1.36194672e-06
Iter: 739 loss: 1.34337256e-06
Iter: 740 loss: 1.34232914e-06
Iter: 741 loss: 1.34530421e-06
Iter: 742 loss: 1.34201537e-06
Iter: 743 loss: 1.34126867e-06
Iter: 744 loss: 1.35156938e-06
Iter: 745 loss: 1.3412299e-06
Iter: 746 loss: 1.34046752e-06
Iter: 747 loss: 1.34173331e-06
Iter: 748 loss: 1.34009247e-06
Iter: 749 loss: 1.33951698e-06
Iter: 750 loss: 1.34451807e-06
Iter: 751 loss: 1.33946673e-06
Iter: 752 loss: 1.33894162e-06
Iter: 753 loss: 1.33813091e-06
Iter: 754 loss: 1.33809885e-06
Iter: 755 loss: 1.33750564e-06
Iter: 756 loss: 1.33751382e-06
Iter: 757 loss: 1.33707158e-06
Iter: 758 loss: 1.33625e-06
Iter: 759 loss: 1.35223718e-06
Iter: 760 loss: 1.33625042e-06
Iter: 761 loss: 1.33549008e-06
Iter: 762 loss: 1.34194215e-06
Iter: 763 loss: 1.33540163e-06
Iter: 764 loss: 1.33468097e-06
Iter: 765 loss: 1.33731123e-06
Iter: 766 loss: 1.3344436e-06
Iter: 767 loss: 1.33386675e-06
Iter: 768 loss: 1.33377216e-06
Iter: 769 loss: 1.33336243e-06
Iter: 770 loss: 1.33242588e-06
Iter: 771 loss: 1.33995036e-06
Iter: 772 loss: 1.33243338e-06
Iter: 773 loss: 1.33195044e-06
Iter: 774 loss: 1.33106209e-06
Iter: 775 loss: 1.34966876e-06
Iter: 776 loss: 1.33102776e-06
Iter: 777 loss: 1.32993114e-06
Iter: 778 loss: 1.33146295e-06
Iter: 779 loss: 1.32937635e-06
Iter: 780 loss: 1.32848459e-06
Iter: 781 loss: 1.33157744e-06
Iter: 782 loss: 1.328233e-06
Iter: 783 loss: 1.32756043e-06
Iter: 784 loss: 1.3275212e-06
Iter: 785 loss: 1.32709351e-06
Iter: 786 loss: 1.32723892e-06
Iter: 787 loss: 1.3268002e-06
Iter: 788 loss: 1.326095e-06
Iter: 789 loss: 1.32638058e-06
Iter: 790 loss: 1.32562286e-06
Iter: 791 loss: 1.32508967e-06
Iter: 792 loss: 1.32917535e-06
Iter: 793 loss: 1.32512571e-06
Iter: 794 loss: 1.32450168e-06
Iter: 795 loss: 1.32463356e-06
Iter: 796 loss: 1.32407308e-06
Iter: 797 loss: 1.32347486e-06
Iter: 798 loss: 1.32367973e-06
Iter: 799 loss: 1.32306241e-06
Iter: 800 loss: 1.32241462e-06
Iter: 801 loss: 1.32239927e-06
Iter: 802 loss: 1.32197306e-06
Iter: 803 loss: 1.32162188e-06
Iter: 804 loss: 1.32143975e-06
Iter: 805 loss: 1.32089849e-06
Iter: 806 loss: 1.32736943e-06
Iter: 807 loss: 1.32086655e-06
Iter: 808 loss: 1.320401e-06
Iter: 809 loss: 1.31956244e-06
Iter: 810 loss: 1.31954675e-06
Iter: 811 loss: 1.3186343e-06
Iter: 812 loss: 1.3196443e-06
Iter: 813 loss: 1.3181259e-06
Iter: 814 loss: 1.31721572e-06
Iter: 815 loss: 1.32019738e-06
Iter: 816 loss: 1.31695947e-06
Iter: 817 loss: 1.31642798e-06
Iter: 818 loss: 1.31639285e-06
Iter: 819 loss: 1.31595993e-06
Iter: 820 loss: 1.31544436e-06
Iter: 821 loss: 1.31537763e-06
Iter: 822 loss: 1.31472666e-06
Iter: 823 loss: 1.31977288e-06
Iter: 824 loss: 1.31467709e-06
Iter: 825 loss: 1.31416232e-06
Iter: 826 loss: 1.31462116e-06
Iter: 827 loss: 1.31392903e-06
Iter: 828 loss: 1.31319734e-06
Iter: 829 loss: 1.31507147e-06
Iter: 830 loss: 1.31289403e-06
Iter: 831 loss: 1.31241234e-06
Iter: 832 loss: 1.31163551e-06
Iter: 833 loss: 1.31159015e-06
Iter: 834 loss: 1.31132049e-06
Iter: 835 loss: 1.31121692e-06
Iter: 836 loss: 1.31075251e-06
Iter: 837 loss: 1.31001661e-06
Iter: 838 loss: 1.31000138e-06
Iter: 839 loss: 1.30934518e-06
Iter: 840 loss: 1.31679985e-06
Iter: 841 loss: 1.30932744e-06
Iter: 842 loss: 1.30868307e-06
Iter: 843 loss: 1.30861304e-06
Iter: 844 loss: 1.30816773e-06
Iter: 845 loss: 1.30743251e-06
Iter: 846 loss: 1.30758349e-06
Iter: 847 loss: 1.30685362e-06
Iter: 848 loss: 1.30589888e-06
Iter: 849 loss: 1.30650778e-06
Iter: 850 loss: 1.30527678e-06
Iter: 851 loss: 1.30462195e-06
Iter: 852 loss: 1.30459159e-06
Iter: 853 loss: 1.30390481e-06
Iter: 854 loss: 1.30588046e-06
Iter: 855 loss: 1.30365765e-06
Iter: 856 loss: 1.3031804e-06
Iter: 857 loss: 1.30268495e-06
Iter: 858 loss: 1.30257513e-06
Iter: 859 loss: 1.3019137e-06
Iter: 860 loss: 1.30192132e-06
Iter: 861 loss: 1.30145975e-06
Iter: 862 loss: 1.30117655e-06
Iter: 863 loss: 1.30100989e-06
Iter: 864 loss: 1.3001403e-06
Iter: 865 loss: 1.30310764e-06
Iter: 866 loss: 1.29992566e-06
Iter: 867 loss: 1.29947898e-06
Iter: 868 loss: 1.29934915e-06
Iter: 869 loss: 1.29896353e-06
Iter: 870 loss: 1.29819864e-06
Iter: 871 loss: 1.30507192e-06
Iter: 872 loss: 1.29815021e-06
Iter: 873 loss: 1.2976227e-06
Iter: 874 loss: 1.29762407e-06
Iter: 875 loss: 1.29726754e-06
Iter: 876 loss: 1.29667421e-06
Iter: 877 loss: 1.30257149e-06
Iter: 878 loss: 1.29668445e-06
Iter: 879 loss: 1.29619059e-06
Iter: 880 loss: 1.29543071e-06
Iter: 881 loss: 1.31334195e-06
Iter: 882 loss: 1.29540285e-06
Iter: 883 loss: 1.29455952e-06
Iter: 884 loss: 1.29483817e-06
Iter: 885 loss: 1.29394084e-06
Iter: 886 loss: 1.29352134e-06
Iter: 887 loss: 1.29347507e-06
Iter: 888 loss: 1.2928723e-06
Iter: 889 loss: 1.2931273e-06
Iter: 890 loss: 1.29246132e-06
Iter: 891 loss: 1.29183718e-06
Iter: 892 loss: 1.29110481e-06
Iter: 893 loss: 1.29108957e-06
Iter: 894 loss: 1.29053728e-06
Iter: 895 loss: 1.2905108e-06
Iter: 896 loss: 1.28996e-06
Iter: 897 loss: 1.28966121e-06
Iter: 898 loss: 1.28948568e-06
Iter: 899 loss: 1.28886711e-06
Iter: 900 loss: 1.29155319e-06
Iter: 901 loss: 1.28874683e-06
Iter: 902 loss: 1.28813724e-06
Iter: 903 loss: 1.28936404e-06
Iter: 904 loss: 1.28789679e-06
Iter: 905 loss: 1.28731426e-06
Iter: 906 loss: 1.28992338e-06
Iter: 907 loss: 1.28728016e-06
Iter: 908 loss: 1.28671832e-06
Iter: 909 loss: 1.28841839e-06
Iter: 910 loss: 1.28655779e-06
Iter: 911 loss: 1.28616693e-06
Iter: 912 loss: 1.28582087e-06
Iter: 913 loss: 1.28570571e-06
Iter: 914 loss: 1.2847845e-06
Iter: 915 loss: 1.28943805e-06
Iter: 916 loss: 1.28466729e-06
Iter: 917 loss: 1.28418606e-06
Iter: 918 loss: 1.28362808e-06
Iter: 919 loss: 1.28357794e-06
Iter: 920 loss: 1.28344618e-06
Iter: 921 loss: 1.28326587e-06
Iter: 922 loss: 1.28287388e-06
Iter: 923 loss: 1.28222462e-06
Iter: 924 loss: 1.29722389e-06
Iter: 925 loss: 1.28226111e-06
Iter: 926 loss: 1.28163583e-06
Iter: 927 loss: 1.28203794e-06
Iter: 928 loss: 1.28126601e-06
Iter: 929 loss: 1.28044053e-06
Iter: 930 loss: 1.28083e-06
Iter: 931 loss: 1.27997396e-06
Iter: 932 loss: 1.27964063e-06
Iter: 933 loss: 1.27942826e-06
Iter: 934 loss: 1.27894828e-06
Iter: 935 loss: 1.27826661e-06
Iter: 936 loss: 1.27825524e-06
Iter: 937 loss: 1.27764872e-06
Iter: 938 loss: 1.28013971e-06
Iter: 939 loss: 1.27752378e-06
Iter: 940 loss: 1.27685712e-06
Iter: 941 loss: 1.27928934e-06
Iter: 942 loss: 1.27669614e-06
Iter: 943 loss: 1.27620388e-06
Iter: 944 loss: 1.2777017e-06
Iter: 945 loss: 1.2760205e-06
Iter: 946 loss: 1.27546673e-06
Iter: 947 loss: 1.27643523e-06
Iter: 948 loss: 1.27519957e-06
Iter: 949 loss: 1.27471435e-06
Iter: 950 loss: 1.27766305e-06
Iter: 951 loss: 1.27468036e-06
Iter: 952 loss: 1.27423732e-06
Iter: 953 loss: 1.27411886e-06
Iter: 954 loss: 1.27383407e-06
Iter: 955 loss: 1.27324552e-06
Iter: 956 loss: 1.27317344e-06
Iter: 957 loss: 1.27273563e-06
Iter: 958 loss: 1.27228236e-06
Iter: 959 loss: 1.27224098e-06
Iter: 960 loss: 1.27171518e-06
Iter: 961 loss: 1.27151634e-06
Iter: 962 loss: 1.27129249e-06
Iter: 963 loss: 1.27075668e-06
Iter: 964 loss: 1.27032172e-06
Iter: 965 loss: 1.27022554e-06
Iter: 966 loss: 1.26955888e-06
Iter: 967 loss: 1.27931844e-06
Iter: 968 loss: 1.26958298e-06
Iter: 969 loss: 1.26900989e-06
Iter: 970 loss: 1.27051248e-06
Iter: 971 loss: 1.26875864e-06
Iter: 972 loss: 1.26829855e-06
Iter: 973 loss: 1.26744487e-06
Iter: 974 loss: 1.28336796e-06
Iter: 975 loss: 1.26742816e-06
Iter: 976 loss: 1.26685245e-06
Iter: 977 loss: 1.26678026e-06
Iter: 978 loss: 1.26630653e-06
Iter: 979 loss: 1.26596353e-06
Iter: 980 loss: 1.2657913e-06
Iter: 981 loss: 1.26508507e-06
Iter: 982 loss: 1.27380792e-06
Iter: 983 loss: 1.26510895e-06
Iter: 984 loss: 1.26468785e-06
Iter: 985 loss: 1.26435611e-06
Iter: 986 loss: 1.26426039e-06
Iter: 987 loss: 1.26355508e-06
Iter: 988 loss: 1.26891791e-06
Iter: 989 loss: 1.2635187e-06
Iter: 990 loss: 1.26295708e-06
Iter: 991 loss: 1.2624082e-06
Iter: 992 loss: 1.26229656e-06
Iter: 993 loss: 1.2618375e-06
Iter: 994 loss: 1.26180862e-06
Iter: 995 loss: 1.26130044e-06
Iter: 996 loss: 1.26082409e-06
Iter: 997 loss: 1.26066857e-06
Iter: 998 loss: 1.26010218e-06
Iter: 999 loss: 1.26089571e-06
Iter: 1000 loss: 1.25975839e-06
Iter: 1001 loss: 1.25917461e-06
Iter: 1002 loss: 1.26477039e-06
Iter: 1003 loss: 1.25914994e-06
Iter: 1004 loss: 1.25872668e-06
Iter: 1005 loss: 1.25808265e-06
Iter: 1006 loss: 1.25804286e-06
Iter: 1007 loss: 1.25723318e-06
Iter: 1008 loss: 1.25955955e-06
Iter: 1009 loss: 1.25700058e-06
Iter: 1010 loss: 1.25624888e-06
Iter: 1011 loss: 1.26563555e-06
Iter: 1012 loss: 1.25625274e-06
Iter: 1013 loss: 1.25570921e-06
Iter: 1014 loss: 1.25591941e-06
Iter: 1015 loss: 1.25530642e-06
Iter: 1016 loss: 1.25456029e-06
Iter: 1017 loss: 1.25965448e-06
Iter: 1018 loss: 1.25454847e-06
Iter: 1019 loss: 1.25403756e-06
Iter: 1020 loss: 1.25399197e-06
Iter: 1021 loss: 1.25359611e-06
Iter: 1022 loss: 1.25287715e-06
Iter: 1023 loss: 1.25691656e-06
Iter: 1024 loss: 1.2527953e-06
Iter: 1025 loss: 1.25220913e-06
Iter: 1026 loss: 1.25167401e-06
Iter: 1027 loss: 1.25157e-06
Iter: 1028 loss: 1.25110125e-06
Iter: 1029 loss: 1.2510468e-06
Iter: 1030 loss: 1.25058591e-06
Iter: 1031 loss: 1.24974508e-06
Iter: 1032 loss: 1.26573877e-06
Iter: 1033 loss: 1.24965879e-06
Iter: 1034 loss: 1.24900555e-06
Iter: 1035 loss: 1.24904454e-06
Iter: 1036 loss: 1.24848884e-06
Iter: 1037 loss: 1.24768405e-06
Iter: 1038 loss: 1.24766655e-06
Iter: 1039 loss: 1.24686676e-06
Iter: 1040 loss: 1.24752989e-06
Iter: 1041 loss: 1.24639485e-06
Iter: 1042 loss: 1.24569817e-06
Iter: 1043 loss: 1.25648705e-06
Iter: 1044 loss: 1.24568805e-06
Iter: 1045 loss: 1.24497774e-06
Iter: 1046 loss: 1.2467541e-06
Iter: 1047 loss: 1.2447922e-06
Iter: 1048 loss: 1.24413259e-06
Iter: 1049 loss: 1.24699159e-06
Iter: 1050 loss: 1.24400185e-06
Iter: 1051 loss: 1.24344365e-06
Iter: 1052 loss: 1.24493192e-06
Iter: 1053 loss: 1.2431974e-06
Iter: 1054 loss: 1.24275414e-06
Iter: 1055 loss: 1.24402015e-06
Iter: 1056 loss: 1.24262078e-06
Iter: 1057 loss: 1.24195731e-06
Iter: 1058 loss: 1.24138705e-06
Iter: 1059 loss: 1.24125268e-06
Iter: 1060 loss: 1.24066378e-06
Iter: 1061 loss: 1.24761277e-06
Iter: 1062 loss: 1.24067537e-06
Iter: 1063 loss: 1.24003054e-06
Iter: 1064 loss: 1.24048483e-06
Iter: 1065 loss: 1.23963105e-06
Iter: 1066 loss: 1.23896109e-06
Iter: 1067 loss: 1.24005646e-06
Iter: 1068 loss: 1.23868438e-06
Iter: 1069 loss: 1.23788686e-06
Iter: 1070 loss: 1.24217706e-06
Iter: 1071 loss: 1.23780262e-06
Iter: 1072 loss: 1.23728842e-06
Iter: 1073 loss: 1.23639188e-06
Iter: 1074 loss: 1.23640336e-06
Iter: 1075 loss: 1.23538894e-06
Iter: 1076 loss: 1.23618929e-06
Iter: 1077 loss: 1.23480231e-06
Iter: 1078 loss: 1.23402617e-06
Iter: 1079 loss: 1.23393784e-06
Iter: 1080 loss: 1.23329528e-06
Iter: 1081 loss: 1.23547147e-06
Iter: 1082 loss: 1.23310065e-06
Iter: 1083 loss: 1.23262123e-06
Iter: 1084 loss: 1.23495761e-06
Iter: 1085 loss: 1.23250152e-06
Iter: 1086 loss: 1.23195093e-06
Iter: 1087 loss: 1.23224822e-06
Iter: 1088 loss: 1.23162431e-06
Iter: 1089 loss: 1.23115353e-06
Iter: 1090 loss: 1.23462337e-06
Iter: 1091 loss: 1.2310993e-06
Iter: 1092 loss: 1.23069367e-06
Iter: 1093 loss: 1.23001917e-06
Iter: 1094 loss: 1.23005452e-06
Iter: 1095 loss: 1.22956567e-06
Iter: 1096 loss: 1.22948256e-06
Iter: 1097 loss: 1.22909967e-06
Iter: 1098 loss: 1.22870324e-06
Iter: 1099 loss: 1.22857693e-06
Iter: 1100 loss: 1.22803181e-06
Iter: 1101 loss: 1.2308742e-06
Iter: 1102 loss: 1.2280417e-06
Iter: 1103 loss: 1.2273822e-06
Iter: 1104 loss: 1.22806432e-06
Iter: 1105 loss: 1.22707547e-06
Iter: 1106 loss: 1.22659139e-06
Iter: 1107 loss: 1.22589097e-06
Iter: 1108 loss: 1.22593133e-06
Iter: 1109 loss: 1.22505276e-06
Iter: 1110 loss: 1.2287303e-06
Iter: 1111 loss: 1.22494441e-06
Iter: 1112 loss: 1.22421454e-06
Iter: 1113 loss: 1.22440122e-06
Iter: 1114 loss: 1.22370511e-06
Iter: 1115 loss: 1.22336871e-06
Iter: 1116 loss: 1.22320785e-06
Iter: 1117 loss: 1.22276663e-06
Iter: 1118 loss: 1.22219012e-06
Iter: 1119 loss: 1.22215397e-06
Iter: 1120 loss: 1.22159679e-06
Iter: 1121 loss: 1.23060613e-06
Iter: 1122 loss: 1.22159418e-06
Iter: 1123 loss: 1.22116489e-06
Iter: 1124 loss: 1.22084123e-06
Iter: 1125 loss: 1.22073675e-06
Iter: 1126 loss: 1.22016741e-06
Iter: 1127 loss: 1.22659139e-06
Iter: 1128 loss: 1.22014126e-06
Iter: 1129 loss: 1.21981725e-06
Iter: 1130 loss: 1.21949495e-06
Iter: 1131 loss: 1.21937546e-06
Iter: 1132 loss: 1.2189023e-06
Iter: 1133 loss: 1.22563506e-06
Iter: 1134 loss: 1.21887138e-06
Iter: 1135 loss: 1.21852952e-06
Iter: 1136 loss: 1.21775463e-06
Iter: 1137 loss: 1.23456334e-06
Iter: 1138 loss: 1.21780658e-06
Iter: 1139 loss: 1.21711435e-06
Iter: 1140 loss: 1.22036658e-06
Iter: 1141 loss: 1.21692415e-06
Iter: 1142 loss: 1.21627977e-06
Iter: 1143 loss: 1.22321785e-06
Iter: 1144 loss: 1.21632593e-06
Iter: 1145 loss: 1.21588494e-06
Iter: 1146 loss: 1.21540631e-06
Iter: 1147 loss: 1.21539665e-06
Iter: 1148 loss: 1.21471248e-06
Iter: 1149 loss: 1.21524226e-06
Iter: 1150 loss: 1.21435232e-06
Iter: 1151 loss: 1.21382686e-06
Iter: 1152 loss: 1.21373978e-06
Iter: 1153 loss: 1.2134044e-06
Iter: 1154 loss: 1.21292828e-06
Iter: 1155 loss: 1.21288497e-06
Iter: 1156 loss: 1.21227424e-06
Iter: 1157 loss: 1.21921676e-06
Iter: 1158 loss: 1.21224821e-06
Iter: 1159 loss: 1.21178232e-06
Iter: 1160 loss: 1.21117705e-06
Iter: 1161 loss: 1.21110872e-06
Iter: 1162 loss: 1.21056473e-06
Iter: 1163 loss: 1.2105595e-06
Iter: 1164 loss: 1.2101973e-06
Iter: 1165 loss: 1.20986124e-06
Iter: 1166 loss: 1.20972845e-06
Iter: 1167 loss: 1.20919117e-06
Iter: 1168 loss: 1.21475568e-06
Iter: 1169 loss: 1.20919265e-06
Iter: 1170 loss: 1.20878713e-06
Iter: 1171 loss: 1.20776667e-06
Iter: 1172 loss: 1.21980156e-06
Iter: 1173 loss: 1.20765799e-06
Iter: 1174 loss: 1.20686866e-06
Iter: 1175 loss: 1.21409209e-06
Iter: 1176 loss: 1.2068042e-06
Iter: 1177 loss: 1.20622701e-06
Iter: 1178 loss: 1.20623179e-06
Iter: 1179 loss: 1.20576465e-06
Iter: 1180 loss: 1.20473533e-06
Iter: 1181 loss: 1.22185679e-06
Iter: 1182 loss: 1.20469804e-06
Iter: 1183 loss: 1.2037915e-06
Iter: 1184 loss: 1.2054611e-06
Iter: 1185 loss: 1.20342474e-06
Iter: 1186 loss: 1.20253435e-06
Iter: 1187 loss: 1.21186144e-06
Iter: 1188 loss: 1.20252093e-06
Iter: 1189 loss: 1.20181892e-06
Iter: 1190 loss: 1.20780396e-06
Iter: 1191 loss: 1.20185155e-06
Iter: 1192 loss: 1.20145228e-06
Iter: 1193 loss: 1.20116692e-06
Iter: 1194 loss: 1.2010164e-06
Iter: 1195 loss: 1.20032144e-06
Iter: 1196 loss: 1.20553977e-06
Iter: 1197 loss: 1.20031655e-06
Iter: 1198 loss: 1.19991535e-06
Iter: 1199 loss: 1.19982383e-06
Iter: 1200 loss: 1.19952858e-06
Iter: 1201 loss: 1.19888477e-06
Iter: 1202 loss: 1.20418235e-06
Iter: 1203 loss: 1.19885669e-06
Iter: 1204 loss: 1.19833749e-06
Iter: 1205 loss: 1.1986798e-06
Iter: 1206 loss: 1.198051e-06
Iter: 1207 loss: 1.19742867e-06
Iter: 1208 loss: 1.19949675e-06
Iter: 1209 loss: 1.1972113e-06
Iter: 1210 loss: 1.19674223e-06
Iter: 1211 loss: 1.19592187e-06
Iter: 1212 loss: 1.2134617e-06
Iter: 1213 loss: 1.19589663e-06
Iter: 1214 loss: 1.19521678e-06
Iter: 1215 loss: 1.19522281e-06
Iter: 1216 loss: 1.19448623e-06
Iter: 1217 loss: 1.19546962e-06
Iter: 1218 loss: 1.19418428e-06
Iter: 1219 loss: 1.19375807e-06
Iter: 1220 loss: 1.19343747e-06
Iter: 1221 loss: 1.19323238e-06
Iter: 1222 loss: 1.19243646e-06
Iter: 1223 loss: 1.19294668e-06
Iter: 1224 loss: 1.19192919e-06
Iter: 1225 loss: 1.19143886e-06
Iter: 1226 loss: 1.19130482e-06
Iter: 1227 loss: 1.19086235e-06
Iter: 1228 loss: 1.19015544e-06
Iter: 1229 loss: 1.19015647e-06
Iter: 1230 loss: 1.18959576e-06
Iter: 1231 loss: 1.19707897e-06
Iter: 1232 loss: 1.18959179e-06
Iter: 1233 loss: 1.18903699e-06
Iter: 1234 loss: 1.1896866e-06
Iter: 1235 loss: 1.18884441e-06
Iter: 1236 loss: 1.18826506e-06
Iter: 1237 loss: 1.19017966e-06
Iter: 1238 loss: 1.18819071e-06
Iter: 1239 loss: 1.1875801e-06
Iter: 1240 loss: 1.18876926e-06
Iter: 1241 loss: 1.18727417e-06
Iter: 1242 loss: 1.18689798e-06
Iter: 1243 loss: 1.18883941e-06
Iter: 1244 loss: 1.18680123e-06
Iter: 1245 loss: 1.18631897e-06
Iter: 1246 loss: 1.18561275e-06
Iter: 1247 loss: 1.18560934e-06
Iter: 1248 loss: 1.18490834e-06
Iter: 1249 loss: 1.18894127e-06
Iter: 1250 loss: 1.18486332e-06
Iter: 1251 loss: 1.18436924e-06
Iter: 1252 loss: 1.19058711e-06
Iter: 1253 loss: 1.18433104e-06
Iter: 1254 loss: 1.18396656e-06
Iter: 1255 loss: 1.18321577e-06
Iter: 1256 loss: 1.19663423e-06
Iter: 1257 loss: 1.18317166e-06
Iter: 1258 loss: 1.18253161e-06
Iter: 1259 loss: 1.18329956e-06
Iter: 1260 loss: 1.18216713e-06
Iter: 1261 loss: 1.18175649e-06
Iter: 1262 loss: 1.1816594e-06
Iter: 1263 loss: 1.18121875e-06
Iter: 1264 loss: 1.18116964e-06
Iter: 1265 loss: 1.18088315e-06
Iter: 1266 loss: 1.1803038e-06
Iter: 1267 loss: 1.18024627e-06
Iter: 1268 loss: 1.17986872e-06
Iter: 1269 loss: 1.17932962e-06
Iter: 1270 loss: 1.17930097e-06
Iter: 1271 loss: 1.17886748e-06
Iter: 1272 loss: 1.17848663e-06
Iter: 1273 loss: 1.17841989e-06
Iter: 1274 loss: 1.17796185e-06
Iter: 1275 loss: 1.17794059e-06
Iter: 1276 loss: 1.17763261e-06
Iter: 1277 loss: 1.17710397e-06
Iter: 1278 loss: 1.17710624e-06
Iter: 1279 loss: 1.17651325e-06
Iter: 1280 loss: 1.18014873e-06
Iter: 1281 loss: 1.17647392e-06
Iter: 1282 loss: 1.1759156e-06
Iter: 1283 loss: 1.17595937e-06
Iter: 1284 loss: 1.17554168e-06
Iter: 1285 loss: 1.17506306e-06
Iter: 1286 loss: 1.17508989e-06
Iter: 1287 loss: 1.17467096e-06
Iter: 1288 loss: 1.17404932e-06
Iter: 1289 loss: 1.17406444e-06
Iter: 1290 loss: 1.17342017e-06
Iter: 1291 loss: 1.17357126e-06
Iter: 1292 loss: 1.17294928e-06
Iter: 1293 loss: 1.17216678e-06
Iter: 1294 loss: 1.17590207e-06
Iter: 1295 loss: 1.1719635e-06
Iter: 1296 loss: 1.17122488e-06
Iter: 1297 loss: 1.18083653e-06
Iter: 1298 loss: 1.17123432e-06
Iter: 1299 loss: 1.17094771e-06
Iter: 1300 loss: 1.17012155e-06
Iter: 1301 loss: 1.17627133e-06
Iter: 1302 loss: 1.17000582e-06
Iter: 1303 loss: 1.169803e-06
Iter: 1304 loss: 1.16944648e-06
Iter: 1305 loss: 1.16904903e-06
Iter: 1306 loss: 1.16849947e-06
Iter: 1307 loss: 1.16849253e-06
Iter: 1308 loss: 1.16801084e-06
Iter: 1309 loss: 1.16798719e-06
Iter: 1310 loss: 1.16748345e-06
Iter: 1311 loss: 1.16674528e-06
Iter: 1312 loss: 1.16674619e-06
Iter: 1313 loss: 1.16604895e-06
Iter: 1314 loss: 1.16701483e-06
Iter: 1315 loss: 1.1657628e-06
Iter: 1316 loss: 1.16485069e-06
Iter: 1317 loss: 1.16968761e-06
Iter: 1318 loss: 1.16461467e-06
Iter: 1319 loss: 1.1639022e-06
Iter: 1320 loss: 1.16714114e-06
Iter: 1321 loss: 1.16382625e-06
Iter: 1322 loss: 1.1630874e-06
Iter: 1323 loss: 1.16566105e-06
Iter: 1324 loss: 1.16297701e-06
Iter: 1325 loss: 1.16243336e-06
Iter: 1326 loss: 1.16182594e-06
Iter: 1327 loss: 1.16178626e-06
Iter: 1328 loss: 1.16076308e-06
Iter: 1329 loss: 1.16234901e-06
Iter: 1330 loss: 1.16032652e-06
Iter: 1331 loss: 1.16013871e-06
Iter: 1332 loss: 1.15985506e-06
Iter: 1333 loss: 1.15944226e-06
Iter: 1334 loss: 1.15880607e-06
Iter: 1335 loss: 1.15875173e-06
Iter: 1336 loss: 1.15815465e-06
Iter: 1337 loss: 1.1587e-06
Iter: 1338 loss: 1.15775913e-06
Iter: 1339 loss: 1.15738908e-06
Iter: 1340 loss: 1.1572854e-06
Iter: 1341 loss: 1.15693433e-06
Iter: 1342 loss: 1.15614898e-06
Iter: 1343 loss: 1.17304091e-06
Iter: 1344 loss: 1.1561882e-06
Iter: 1345 loss: 1.15568901e-06
Iter: 1346 loss: 1.1556408e-06
Iter: 1347 loss: 1.15528246e-06
Iter: 1348 loss: 1.15444345e-06
Iter: 1349 loss: 1.16841522e-06
Iter: 1350 loss: 1.15442731e-06
Iter: 1351 loss: 1.153629e-06
Iter: 1352 loss: 1.16036631e-06
Iter: 1353 loss: 1.15364696e-06
Iter: 1354 loss: 1.15293096e-06
Iter: 1355 loss: 1.1570271e-06
Iter: 1356 loss: 1.15283854e-06
Iter: 1357 loss: 1.15246257e-06
Iter: 1358 loss: 1.15414389e-06
Iter: 1359 loss: 1.15238481e-06
Iter: 1360 loss: 1.15193689e-06
Iter: 1361 loss: 1.15112584e-06
Iter: 1362 loss: 1.16816466e-06
Iter: 1363 loss: 1.15117007e-06
Iter: 1364 loss: 1.150255e-06
Iter: 1365 loss: 1.15203443e-06
Iter: 1366 loss: 1.14988882e-06
Iter: 1367 loss: 1.14927502e-06
Iter: 1368 loss: 1.15788362e-06
Iter: 1369 loss: 1.1492632e-06
Iter: 1370 loss: 1.14863633e-06
Iter: 1371 loss: 1.1514062e-06
Iter: 1372 loss: 1.14847353e-06
Iter: 1373 loss: 1.14806221e-06
Iter: 1374 loss: 1.1472348e-06
Iter: 1375 loss: 1.16177148e-06
Iter: 1376 loss: 1.1472232e-06
Iter: 1377 loss: 1.14640386e-06
Iter: 1378 loss: 1.1542329e-06
Iter: 1379 loss: 1.14640693e-06
Iter: 1380 loss: 1.14573061e-06
Iter: 1381 loss: 1.15352441e-06
Iter: 1382 loss: 1.14571208e-06
Iter: 1383 loss: 1.14532827e-06
Iter: 1384 loss: 1.14507282e-06
Iter: 1385 loss: 1.14490445e-06
Iter: 1386 loss: 1.14432328e-06
Iter: 1387 loss: 1.14912427e-06
Iter: 1388 loss: 1.14428758e-06
Iter: 1389 loss: 1.14389672e-06
Iter: 1390 loss: 1.14327975e-06
Iter: 1391 loss: 1.14325985e-06
Iter: 1392 loss: 1.14284421e-06
Iter: 1393 loss: 1.14284978e-06
Iter: 1394 loss: 1.14241266e-06
Iter: 1395 loss: 1.14212287e-06
Iter: 1396 loss: 1.14198588e-06
Iter: 1397 loss: 1.1414229e-06
Iter: 1398 loss: 1.1449124e-06
Iter: 1399 loss: 1.14135946e-06
Iter: 1400 loss: 1.14097065e-06
Iter: 1401 loss: 1.14054103e-06
Iter: 1402 loss: 1.14042166e-06
Iter: 1403 loss: 1.13972328e-06
Iter: 1404 loss: 1.14082377e-06
Iter: 1405 loss: 1.13945293e-06
Iter: 1406 loss: 1.13886404e-06
Iter: 1407 loss: 1.14095496e-06
Iter: 1408 loss: 1.13869157e-06
Iter: 1409 loss: 1.13819283e-06
Iter: 1410 loss: 1.13822523e-06
Iter: 1411 loss: 1.13784915e-06
Iter: 1412 loss: 1.13724695e-06
Iter: 1413 loss: 1.14917214e-06
Iter: 1414 loss: 1.13721592e-06
Iter: 1415 loss: 1.13695285e-06
Iter: 1416 loss: 1.13687622e-06
Iter: 1417 loss: 1.1365529e-06
Iter: 1418 loss: 1.13602505e-06
Iter: 1419 loss: 1.13599242e-06
Iter: 1420 loss: 1.13557132e-06
Iter: 1421 loss: 1.14167892e-06
Iter: 1422 loss: 1.13557348e-06
Iter: 1423 loss: 1.13525721e-06
Iter: 1424 loss: 1.13469673e-06
Iter: 1425 loss: 1.13467286e-06
Iter: 1426 loss: 1.13420685e-06
Iter: 1427 loss: 1.13991484e-06
Iter: 1428 loss: 1.13421243e-06
Iter: 1429 loss: 1.13370402e-06
Iter: 1430 loss: 1.13394822e-06
Iter: 1431 loss: 1.13337205e-06
Iter: 1432 loss: 1.13291799e-06
Iter: 1433 loss: 1.13436181e-06
Iter: 1434 loss: 1.13270676e-06
Iter: 1435 loss: 1.13212479e-06
Iter: 1436 loss: 1.13263025e-06
Iter: 1437 loss: 1.13175906e-06
Iter: 1438 loss: 1.13124975e-06
Iter: 1439 loss: 1.13153339e-06
Iter: 1440 loss: 1.13087219e-06
Iter: 1441 loss: 1.13030887e-06
Iter: 1442 loss: 1.13738236e-06
Iter: 1443 loss: 1.13028329e-06
Iter: 1444 loss: 1.129765e-06
Iter: 1445 loss: 1.13195e-06
Iter: 1446 loss: 1.1296745e-06
Iter: 1447 loss: 1.12934401e-06
Iter: 1448 loss: 1.1284443e-06
Iter: 1449 loss: 1.14130739e-06
Iter: 1450 loss: 1.12836176e-06
Iter: 1451 loss: 1.12798386e-06
Iter: 1452 loss: 1.12798875e-06
Iter: 1453 loss: 1.12736211e-06
Iter: 1454 loss: 1.12711359e-06
Iter: 1455 loss: 1.12681823e-06
Iter: 1456 loss: 1.12643193e-06
Iter: 1457 loss: 1.12687167e-06
Iter: 1458 loss: 1.1261518e-06
Iter: 1459 loss: 1.12541738e-06
Iter: 1460 loss: 1.12824114e-06
Iter: 1461 loss: 1.12522844e-06
Iter: 1462 loss: 1.12471582e-06
Iter: 1463 loss: 1.12448288e-06
Iter: 1464 loss: 1.12424198e-06
Iter: 1465 loss: 1.12359498e-06
Iter: 1466 loss: 1.12526504e-06
Iter: 1467 loss: 1.12340194e-06
Iter: 1468 loss: 1.12281577e-06
Iter: 1469 loss: 1.13246438e-06
Iter: 1470 loss: 1.12283215e-06
Iter: 1471 loss: 1.12247244e-06
Iter: 1472 loss: 1.12174007e-06
Iter: 1473 loss: 1.13608849e-06
Iter: 1474 loss: 1.12174007e-06
Iter: 1475 loss: 1.12113526e-06
Iter: 1476 loss: 1.12891325e-06
Iter: 1477 loss: 1.1211323e-06
Iter: 1478 loss: 1.12057273e-06
Iter: 1479 loss: 1.12015823e-06
Iter: 1480 loss: 1.11996667e-06
Iter: 1481 loss: 1.11946872e-06
Iter: 1482 loss: 1.11945383e-06
Iter: 1483 loss: 1.11894212e-06
Iter: 1484 loss: 1.11898476e-06
Iter: 1485 loss: 1.11853092e-06
Iter: 1486 loss: 1.11802967e-06
Iter: 1487 loss: 1.11765848e-06
Iter: 1488 loss: 1.11747465e-06
Iter: 1489 loss: 1.11693805e-06
Iter: 1490 loss: 1.12441717e-06
Iter: 1491 loss: 1.11694771e-06
Iter: 1492 loss: 1.11645443e-06
Iter: 1493 loss: 1.11863505e-06
Iter: 1494 loss: 1.11630061e-06
Iter: 1495 loss: 1.1159417e-06
Iter: 1496 loss: 1.11519921e-06
Iter: 1497 loss: 1.12905013e-06
Iter: 1498 loss: 1.11521263e-06
Iter: 1499 loss: 1.11474139e-06
Iter: 1500 loss: 1.1147315e-06
Iter: 1501 loss: 1.11413601e-06
Iter: 1502 loss: 1.11337931e-06
Iter: 1503 loss: 1.11332201e-06
Iter: 1504 loss: 1.11257691e-06
Iter: 1505 loss: 1.11451072e-06
Iter: 1506 loss: 1.11231384e-06
Iter: 1507 loss: 1.11190832e-06
Iter: 1508 loss: 1.11188331e-06
Iter: 1509 loss: 1.11152463e-06
Iter: 1510 loss: 1.11084944e-06
Iter: 1511 loss: 1.12172199e-06
Iter: 1512 loss: 1.11074371e-06
Iter: 1513 loss: 1.11017187e-06
Iter: 1514 loss: 1.11291649e-06
Iter: 1515 loss: 1.11001964e-06
Iter: 1516 loss: 1.10963765e-06
Iter: 1517 loss: 1.10961992e-06
Iter: 1518 loss: 1.10938026e-06
Iter: 1519 loss: 1.10881331e-06
Iter: 1520 loss: 1.1088e-06
Iter: 1521 loss: 1.10818746e-06
Iter: 1522 loss: 1.11282964e-06
Iter: 1523 loss: 1.10811447e-06
Iter: 1524 loss: 1.1076329e-06
Iter: 1525 loss: 1.10755309e-06
Iter: 1526 loss: 1.10715814e-06
Iter: 1527 loss: 1.10666724e-06
Iter: 1528 loss: 1.10886344e-06
Iter: 1529 loss: 1.10656867e-06
Iter: 1530 loss: 1.10606777e-06
Iter: 1531 loss: 1.11248619e-06
Iter: 1532 loss: 1.1060821e-06
Iter: 1533 loss: 1.10586041e-06
Iter: 1534 loss: 1.10515055e-06
Iter: 1535 loss: 1.11395411e-06
Iter: 1536 loss: 1.10513133e-06
Iter: 1537 loss: 1.10460155e-06
Iter: 1538 loss: 1.11085092e-06
Iter: 1539 loss: 1.10461497e-06
Iter: 1540 loss: 1.10415931e-06
Iter: 1541 loss: 1.10634335e-06
Iter: 1542 loss: 1.10409974e-06
Iter: 1543 loss: 1.103758e-06
Iter: 1544 loss: 1.10310111e-06
Iter: 1545 loss: 1.11351528e-06
Iter: 1546 loss: 1.10302881e-06
Iter: 1547 loss: 1.10252847e-06
Iter: 1548 loss: 1.10846224e-06
Iter: 1549 loss: 1.10254382e-06
Iter: 1550 loss: 1.10199653e-06
Iter: 1551 loss: 1.10773567e-06
Iter: 1552 loss: 1.10203405e-06
Iter: 1553 loss: 1.10172937e-06
Iter: 1554 loss: 1.10119777e-06
Iter: 1555 loss: 1.11114048e-06
Iter: 1556 loss: 1.10115275e-06
Iter: 1557 loss: 1.10075428e-06
Iter: 1558 loss: 1.10073847e-06
Iter: 1559 loss: 1.10038286e-06
Iter: 1560 loss: 1.10034989e-06
Iter: 1561 loss: 1.10000883e-06
Iter: 1562 loss: 1.09963503e-06
Iter: 1563 loss: 1.1005335e-06
Iter: 1564 loss: 1.09946291e-06
Iter: 1565 loss: 1.09891948e-06
Iter: 1566 loss: 1.09989799e-06
Iter: 1567 loss: 1.09867892e-06
Iter: 1568 loss: 1.09831262e-06
Iter: 1569 loss: 1.10322014e-06
Iter: 1570 loss: 1.0983274e-06
Iter: 1571 loss: 1.0979453e-06
Iter: 1572 loss: 1.09751136e-06
Iter: 1573 loss: 1.09750124e-06
Iter: 1574 loss: 1.09704206e-06
Iter: 1575 loss: 1.09715779e-06
Iter: 1576 loss: 1.09675261e-06
Iter: 1577 loss: 1.09624762e-06
Iter: 1578 loss: 1.10005749e-06
Iter: 1579 loss: 1.09625034e-06
Iter: 1580 loss: 1.09570328e-06
Iter: 1581 loss: 1.09708799e-06
Iter: 1582 loss: 1.09552229e-06
Iter: 1583 loss: 1.0951328e-06
Iter: 1584 loss: 1.09440975e-06
Iter: 1585 loss: 1.10996359e-06
Iter: 1586 loss: 1.09444522e-06
Iter: 1587 loss: 1.09433245e-06
Iter: 1588 loss: 1.09401697e-06
Iter: 1589 loss: 1.09375355e-06
Iter: 1590 loss: 1.09311782e-06
Iter: 1591 loss: 1.10085682e-06
Iter: 1592 loss: 1.09306347e-06
Iter: 1593 loss: 1.09254336e-06
Iter: 1594 loss: 1.09771474e-06
Iter: 1595 loss: 1.09247208e-06
Iter: 1596 loss: 1.09202574e-06
Iter: 1597 loss: 1.09422103e-06
Iter: 1598 loss: 1.09190694e-06
Iter: 1599 loss: 1.09156304e-06
Iter: 1600 loss: 1.09093446e-06
Iter: 1601 loss: 1.10618544e-06
Iter: 1602 loss: 1.0909414e-06
Iter: 1603 loss: 1.09027769e-06
Iter: 1604 loss: 1.0927381e-06
Iter: 1605 loss: 1.09015514e-06
Iter: 1606 loss: 1.08946938e-06
Iter: 1607 loss: 1.09843063e-06
Iter: 1608 loss: 1.08939571e-06
Iter: 1609 loss: 1.0890567e-06
Iter: 1610 loss: 1.09004804e-06
Iter: 1611 loss: 1.08897234e-06
Iter: 1612 loss: 1.08858012e-06
Iter: 1613 loss: 1.08810286e-06
Iter: 1614 loss: 1.08805216e-06
Iter: 1615 loss: 1.0875217e-06
Iter: 1616 loss: 1.09038228e-06
Iter: 1617 loss: 1.08745792e-06
Iter: 1618 loss: 1.08688721e-06
Iter: 1619 loss: 1.08932227e-06
Iter: 1620 loss: 1.08677682e-06
Iter: 1621 loss: 1.08633981e-06
Iter: 1622 loss: 1.08566917e-06
Iter: 1623 loss: 1.08562847e-06
Iter: 1624 loss: 1.08555105e-06
Iter: 1625 loss: 1.08531992e-06
Iter: 1626 loss: 1.08499444e-06
Iter: 1627 loss: 1.08430379e-06
Iter: 1628 loss: 1.09522796e-06
Iter: 1629 loss: 1.08429708e-06
Iter: 1630 loss: 1.0835804e-06
Iter: 1631 loss: 1.08414361e-06
Iter: 1632 loss: 1.08314168e-06
Iter: 1633 loss: 1.08279323e-06
Iter: 1634 loss: 1.08267454e-06
Iter: 1635 loss: 1.08229267e-06
Iter: 1636 loss: 1.08199492e-06
Iter: 1637 loss: 1.08185429e-06
Iter: 1638 loss: 1.08138647e-06
Iter: 1639 loss: 1.08125687e-06
Iter: 1640 loss: 1.08096356e-06
Iter: 1641 loss: 1.08054837e-06
Iter: 1642 loss: 1.0805245e-06
Iter: 1643 loss: 1.08006202e-06
Iter: 1644 loss: 1.07973574e-06
Iter: 1645 loss: 1.07956566e-06
Iter: 1646 loss: 1.07905441e-06
Iter: 1647 loss: 1.08044185e-06
Iter: 1648 loss: 1.07893231e-06
Iter: 1649 loss: 1.07835342e-06
Iter: 1650 loss: 1.08101176e-06
Iter: 1651 loss: 1.07826077e-06
Iter: 1652 loss: 1.07785695e-06
Iter: 1653 loss: 1.07857795e-06
Iter: 1654 loss: 1.07768028e-06
Iter: 1655 loss: 1.07710446e-06
Iter: 1656 loss: 1.07772462e-06
Iter: 1657 loss: 1.0767676e-06
Iter: 1658 loss: 1.07628523e-06
Iter: 1659 loss: 1.07768301e-06
Iter: 1660 loss: 1.07620872e-06
Iter: 1661 loss: 1.07568212e-06
Iter: 1662 loss: 1.07890651e-06
Iter: 1663 loss: 1.07565768e-06
Iter: 1664 loss: 1.07525079e-06
Iter: 1665 loss: 1.07454173e-06
Iter: 1666 loss: 1.09062171e-06
Iter: 1667 loss: 1.07451763e-06
Iter: 1668 loss: 1.07405299e-06
Iter: 1669 loss: 1.08002587e-06
Iter: 1670 loss: 1.07406231e-06
Iter: 1671 loss: 1.07353117e-06
Iter: 1672 loss: 1.07495316e-06
Iter: 1673 loss: 1.07333653e-06
Iter: 1674 loss: 1.07296887e-06
Iter: 1675 loss: 1.07237952e-06
Iter: 1676 loss: 1.07237065e-06
Iter: 1677 loss: 1.07198275e-06
Iter: 1678 loss: 1.0719441e-06
Iter: 1679 loss: 1.07152164e-06
Iter: 1680 loss: 1.07215396e-06
Iter: 1681 loss: 1.07133451e-06
Iter: 1682 loss: 1.07104e-06
Iter: 1683 loss: 1.07046867e-06
Iter: 1684 loss: 1.08200436e-06
Iter: 1685 loss: 1.0704905e-06
Iter: 1686 loss: 1.07000369e-06
Iter: 1687 loss: 1.06997584e-06
Iter: 1688 loss: 1.06964831e-06
Iter: 1689 loss: 1.0689937e-06
Iter: 1690 loss: 1.08000631e-06
Iter: 1691 loss: 1.06893867e-06
Iter: 1692 loss: 1.06849541e-06
Iter: 1693 loss: 1.07626897e-06
Iter: 1694 loss: 1.06851462e-06
Iter: 1695 loss: 1.06809421e-06
Iter: 1696 loss: 1.0679787e-06
Iter: 1697 loss: 1.06771415e-06
Iter: 1698 loss: 1.06724156e-06
Iter: 1699 loss: 1.0676365e-06
Iter: 1700 loss: 1.06694768e-06
Iter: 1701 loss: 1.06661014e-06
Iter: 1702 loss: 1.06661969e-06
Iter: 1703 loss: 1.06629341e-06
Iter: 1704 loss: 1.06569723e-06
Iter: 1705 loss: 1.06568928e-06
Iter: 1706 loss: 1.06522202e-06
Iter: 1707 loss: 1.06583252e-06
Iter: 1708 loss: 1.06498044e-06
Iter: 1709 loss: 1.06444645e-06
Iter: 1710 loss: 1.07220239e-06
Iter: 1711 loss: 1.06442712e-06
Iter: 1712 loss: 1.06414495e-06
Iter: 1713 loss: 1.06385289e-06
Iter: 1714 loss: 1.06374694e-06
Iter: 1715 loss: 1.06348716e-06
Iter: 1716 loss: 1.06342918e-06
Iter: 1717 loss: 1.06318066e-06
Iter: 1718 loss: 1.06255186e-06
Iter: 1719 loss: 1.06896e-06
Iter: 1720 loss: 1.06250889e-06
Iter: 1721 loss: 1.06206426e-06
Iter: 1722 loss: 1.06872778e-06
Iter: 1723 loss: 1.0620704e-06
Iter: 1724 loss: 1.06162838e-06
Iter: 1725 loss: 1.06222387e-06
Iter: 1726 loss: 1.06145217e-06
Iter: 1727 loss: 1.06105574e-06
Iter: 1728 loss: 1.06090272e-06
Iter: 1729 loss: 1.06063396e-06
Iter: 1730 loss: 1.06011612e-06
Iter: 1731 loss: 1.06725042e-06
Iter: 1732 loss: 1.06008156e-06
Iter: 1733 loss: 1.05977301e-06
Iter: 1734 loss: 1.05971071e-06
Iter: 1735 loss: 1.0594639e-06
Iter: 1736 loss: 1.05901472e-06
Iter: 1737 loss: 1.06344066e-06
Iter: 1738 loss: 1.05899721e-06
Iter: 1739 loss: 1.0586466e-06
Iter: 1740 loss: 1.05838649e-06
Iter: 1741 loss: 1.05828394e-06
Iter: 1742 loss: 1.05781373e-06
Iter: 1743 loss: 1.06152879e-06
Iter: 1744 loss: 1.05781305e-06
Iter: 1745 loss: 1.05739309e-06
Iter: 1746 loss: 1.05781555e-06
Iter: 1747 loss: 1.05715026e-06
Iter: 1748 loss: 1.05680874e-06
Iter: 1749 loss: 1.05818447e-06
Iter: 1750 loss: 1.05669699e-06
Iter: 1751 loss: 1.05617619e-06
Iter: 1752 loss: 1.05764559e-06
Iter: 1753 loss: 1.05608331e-06
Iter: 1754 loss: 1.05577931e-06
Iter: 1755 loss: 1.05523918e-06
Iter: 1756 loss: 1.06832817e-06
Iter: 1757 loss: 1.05522759e-06
Iter: 1758 loss: 1.05491858e-06
Iter: 1759 loss: 1.05488016e-06
Iter: 1760 loss: 1.05459208e-06
Iter: 1761 loss: 1.0541313e-06
Iter: 1762 loss: 1.06644052e-06
Iter: 1763 loss: 1.05411709e-06
Iter: 1764 loss: 1.05359766e-06
Iter: 1765 loss: 1.0543846e-06
Iter: 1766 loss: 1.05328229e-06
Iter: 1767 loss: 1.05285335e-06
Iter: 1768 loss: 1.05282152e-06
Iter: 1769 loss: 1.05256311e-06
Iter: 1770 loss: 1.05221807e-06
Iter: 1771 loss: 1.05215099e-06
Iter: 1772 loss: 1.05161416e-06
Iter: 1773 loss: 1.05580796e-06
Iter: 1774 loss: 1.0515331e-06
Iter: 1775 loss: 1.05117749e-06
Iter: 1776 loss: 1.05074776e-06
Iter: 1777 loss: 1.05067272e-06
Iter: 1778 loss: 1.05032188e-06
Iter: 1779 loss: 1.05026459e-06
Iter: 1780 loss: 1.05000129e-06
Iter: 1781 loss: 1.0496492e-06
Iter: 1782 loss: 1.04957098e-06
Iter: 1783 loss: 1.04912078e-06
Iter: 1784 loss: 1.04961737e-06
Iter: 1785 loss: 1.04888136e-06
Iter: 1786 loss: 1.04852006e-06
Iter: 1787 loss: 1.04848937e-06
Iter: 1788 loss: 1.04829246e-06
Iter: 1789 loss: 1.04782623e-06
Iter: 1790 loss: 1.05438573e-06
Iter: 1791 loss: 1.04781657e-06
Iter: 1792 loss: 1.04734227e-06
Iter: 1793 loss: 1.05174342e-06
Iter: 1794 loss: 1.04737046e-06
Iter: 1795 loss: 1.04693481e-06
Iter: 1796 loss: 1.04731237e-06
Iter: 1797 loss: 1.04665435e-06
Iter: 1798 loss: 1.0462868e-06
Iter: 1799 loss: 1.04592937e-06
Iter: 1800 loss: 1.04583432e-06
Iter: 1801 loss: 1.04552134e-06
Iter: 1802 loss: 1.0454628e-06
Iter: 1803 loss: 1.04513856e-06
Iter: 1804 loss: 1.04504988e-06
Iter: 1805 loss: 1.0448781e-06
Iter: 1806 loss: 1.04430842e-06
Iter: 1807 loss: 1.04614753e-06
Iter: 1808 loss: 1.04416063e-06
Iter: 1809 loss: 1.04382843e-06
Iter: 1810 loss: 1.0440325e-06
Iter: 1811 loss: 1.04358026e-06
Iter: 1812 loss: 1.04322305e-06
Iter: 1813 loss: 1.0432351e-06
Iter: 1814 loss: 1.04295952e-06
Iter: 1815 loss: 1.04243293e-06
Iter: 1816 loss: 1.04240939e-06
Iter: 1817 loss: 1.04203946e-06
Iter: 1818 loss: 1.04209391e-06
Iter: 1819 loss: 1.04170476e-06
Iter: 1820 loss: 1.04218987e-06
Iter: 1821 loss: 1.04150251e-06
Iter: 1822 loss: 1.04121386e-06
Iter: 1823 loss: 1.04100172e-06
Iter: 1824 loss: 1.04089327e-06
Iter: 1825 loss: 1.04052901e-06
Iter: 1826 loss: 1.04054084e-06
Iter: 1827 loss: 1.04016e-06
Iter: 1828 loss: 1.03963384e-06
Iter: 1829 loss: 1.03966067e-06
Iter: 1830 loss: 1.03910406e-06
Iter: 1831 loss: 1.04004368e-06
Iter: 1832 loss: 1.03893126e-06
Iter: 1833 loss: 1.03860521e-06
Iter: 1834 loss: 1.03861203e-06
Iter: 1835 loss: 1.03830916e-06
Iter: 1836 loss: 1.03828575e-06
Iter: 1837 loss: 1.03809884e-06
Iter: 1838 loss: 1.0376782e-06
Iter: 1839 loss: 1.03869957e-06
Iter: 1840 loss: 1.03754894e-06
Iter: 1841 loss: 1.037213e-06
Iter: 1842 loss: 1.03738182e-06
Iter: 1843 loss: 1.03697539e-06
Iter: 1844 loss: 1.03665207e-06
Iter: 1845 loss: 1.04143078e-06
Iter: 1846 loss: 1.03664161e-06
Iter: 1847 loss: 1.03632169e-06
Iter: 1848 loss: 1.03559364e-06
Iter: 1849 loss: 1.04278683e-06
Iter: 1850 loss: 1.03547575e-06
Iter: 1851 loss: 1.03486866e-06
Iter: 1852 loss: 1.0405613e-06
Iter: 1853 loss: 1.03488492e-06
Iter: 1854 loss: 1.03425509e-06
Iter: 1855 loss: 1.03742286e-06
Iter: 1856 loss: 1.03418131e-06
Iter: 1857 loss: 1.03389016e-06
Iter: 1858 loss: 1.03335094e-06
Iter: 1859 loss: 1.0459687e-06
Iter: 1860 loss: 1.03334128e-06
Iter: 1861 loss: 1.03283901e-06
Iter: 1862 loss: 1.03738773e-06
Iter: 1863 loss: 1.03279729e-06
Iter: 1864 loss: 1.03231264e-06
Iter: 1865 loss: 1.03445291e-06
Iter: 1866 loss: 1.0321703e-06
Iter: 1867 loss: 1.03179082e-06
Iter: 1868 loss: 1.03129048e-06
Iter: 1869 loss: 1.0312765e-06
Iter: 1870 loss: 1.03070033e-06
Iter: 1871 loss: 1.03621437e-06
Iter: 1872 loss: 1.03069169e-06
Iter: 1873 loss: 1.03006073e-06
Iter: 1874 loss: 1.03218019e-06
Iter: 1875 loss: 1.02993317e-06
Iter: 1876 loss: 1.02953481e-06
Iter: 1877 loss: 1.03043533e-06
Iter: 1878 loss: 1.02939805e-06
Iter: 1879 loss: 1.02890533e-06
Iter: 1880 loss: 1.02902777e-06
Iter: 1881 loss: 1.02856916e-06
Iter: 1882 loss: 1.02798879e-06
Iter: 1883 loss: 1.02878187e-06
Iter: 1884 loss: 1.02780837e-06
Iter: 1885 loss: 1.02750232e-06
Iter: 1886 loss: 1.02745321e-06
Iter: 1887 loss: 1.02719275e-06
Iter: 1888 loss: 1.02642537e-06
Iter: 1889 loss: 1.03181878e-06
Iter: 1890 loss: 1.02626382e-06
Iter: 1891 loss: 1.02560568e-06
Iter: 1892 loss: 1.02937906e-06
Iter: 1893 loss: 1.02545948e-06
Iter: 1894 loss: 1.02510216e-06
Iter: 1895 loss: 1.02504521e-06
Iter: 1896 loss: 1.02469926e-06
Iter: 1897 loss: 1.02407307e-06
Iter: 1898 loss: 1.03624302e-06
Iter: 1899 loss: 1.02404283e-06
Iter: 1900 loss: 1.02350543e-06
Iter: 1901 loss: 1.02488525e-06
Iter: 1902 loss: 1.02323952e-06
Iter: 1903 loss: 1.02274282e-06
Iter: 1904 loss: 1.02273691e-06
Iter: 1905 loss: 1.02244621e-06
Iter: 1906 loss: 1.02194133e-06
Iter: 1907 loss: 1.03398952e-06
Iter: 1908 loss: 1.02196066e-06
Iter: 1909 loss: 1.02161448e-06
Iter: 1910 loss: 1.02157867e-06
Iter: 1911 loss: 1.02125227e-06
Iter: 1912 loss: 1.02084709e-06
Iter: 1913 loss: 1.02083618e-06
Iter: 1914 loss: 1.02032368e-06
Iter: 1915 loss: 1.02356046e-06
Iter: 1916 loss: 1.02025626e-06
Iter: 1917 loss: 1.0197964e-06
Iter: 1918 loss: 1.01989781e-06
Iter: 1919 loss: 1.01942919e-06
Iter: 1920 loss: 1.01903311e-06
Iter: 1921 loss: 1.02286674e-06
Iter: 1922 loss: 1.01901799e-06
Iter: 1923 loss: 1.01861394e-06
Iter: 1924 loss: 1.01905255e-06
Iter: 1925 loss: 1.01832666e-06
Iter: 1926 loss: 1.01794626e-06
Iter: 1927 loss: 1.01754563e-06
Iter: 1928 loss: 1.01748446e-06
Iter: 1929 loss: 1.01714647e-06
Iter: 1930 loss: 1.0170786e-06
Iter: 1931 loss: 1.0166782e-06
Iter: 1932 loss: 1.0169914e-06
Iter: 1933 loss: 1.01639728e-06
Iter: 1934 loss: 1.01602268e-06
Iter: 1935 loss: 1.0155959e-06
Iter: 1936 loss: 1.01551882e-06
Iter: 1937 loss: 1.01527098e-06
Iter: 1938 loss: 1.01519277e-06
Iter: 1939 loss: 1.01489559e-06
Iter: 1940 loss: 1.01438457e-06
Iter: 1941 loss: 1.01442163e-06
Iter: 1942 loss: 1.01396654e-06
Iter: 1943 loss: 1.01994385e-06
Iter: 1944 loss: 1.01394528e-06
Iter: 1945 loss: 1.0135509e-06
Iter: 1946 loss: 1.01351679e-06
Iter: 1947 loss: 1.01317084e-06
Iter: 1948 loss: 1.01278602e-06
Iter: 1949 loss: 1.01244484e-06
Iter: 1950 loss: 1.01234514e-06
Iter: 1951 loss: 1.01180422e-06
Iter: 1952 loss: 1.01427395e-06
Iter: 1953 loss: 1.01172645e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.2/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi1.6
+ date
Sun Nov  8 19:02:11 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.6/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.6/300_100_100_100_1 ']'
+ LOAD='--load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.2/300_100_100_100_1'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.2/300_100_100_100_1 --function f1 --psi 2 --phi 1.6 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.6/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de447d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de449e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de44bdb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de44bdc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de440d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de440d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de43498c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de42bf6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de42bf488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de4349d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de4349ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de1065bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de1065840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de10af8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de10da400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de10ead90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de10e7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0f8d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0f366a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0f36c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de1013598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de1013b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0e7e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0e9b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0e9b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0e617b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0fd86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0fbb620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0fbb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0dd0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0dd00d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0da01e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0d7b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0da8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0d42a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0de0d32e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0048013115
test_loss: 0.005211431
train_loss: 0.0038915984
test_loss: 0.004287374
train_loss: 0.003787737
test_loss: 0.0039853365
train_loss: 0.0033337614
test_loss: 0.0040166043
train_loss: 0.0034674902
test_loss: 0.0041512395
train_loss: 0.003280221
test_loss: 0.0038386453
train_loss: 0.0035811942
test_loss: 0.003898358
train_loss: 0.003632083
test_loss: 0.003997799
train_loss: 0.0034587758
test_loss: 0.003855078
train_loss: 0.0035457278
test_loss: 0.0037853594
train_loss: 0.0035264515
test_loss: 0.004143899
train_loss: 0.0031626076
test_loss: 0.0039333696
train_loss: 0.0033798467
test_loss: 0.0038551658
train_loss: 0.0035091676
test_loss: 0.003939516
train_loss: 0.0033093612
test_loss: 0.0038605577
train_loss: 0.0032236036
test_loss: 0.0038702372
train_loss: 0.0033864528
test_loss: 0.0041274405
train_loss: 0.00316528
test_loss: 0.0038579183
train_loss: 0.003429918
test_loss: 0.0040061623
train_loss: 0.0031316553
test_loss: 0.0039768675
train_loss: 0.0031901859
test_loss: 0.0037774958
train_loss: 0.0032611853
test_loss: 0.003849938
train_loss: 0.0032403963
test_loss: 0.003881548
train_loss: 0.0030746874
test_loss: 0.0037285641
train_loss: 0.003187609
test_loss: 0.0038392406
train_loss: 0.0034147133
test_loss: 0.0038045852
train_loss: 0.0034703012
test_loss: 0.0037793622
train_loss: 0.0033255848
test_loss: 0.0040337895
train_loss: 0.0029553855
test_loss: 0.0037435533
train_loss: 0.0029952908
test_loss: 0.0037946438
train_loss: 0.0030622252
test_loss: 0.0037877522
train_loss: 0.0032018952
test_loss: 0.0037574694
train_loss: 0.0033108247
test_loss: 0.0039018118
train_loss: 0.0030836486
test_loss: 0.0039219763
train_loss: 0.0031545863
test_loss: 0.0038608864
train_loss: 0.0030674236
test_loss: 0.003628116
train_loss: 0.0032498245
test_loss: 0.0039303238
train_loss: 0.0030078748
test_loss: 0.003745877
train_loss: 0.0031517725
test_loss: 0.0037434
train_loss: 0.0030747307
test_loss: 0.0039619533
train_loss: 0.0030699237
test_loss: 0.0036839305
train_loss: 0.0031186095
test_loss: 0.003919816
train_loss: 0.0031178538
test_loss: 0.0037799492
train_loss: 0.003070205
test_loss: 0.0037931192
train_loss: 0.003043072
test_loss: 0.0036893191
train_loss: 0.003223201
test_loss: 0.0038261071
train_loss: 0.00291136
test_loss: 0.0036433346
train_loss: 0.0031124335
test_loss: 0.004055541
train_loss: 0.0032452603
test_loss: 0.0038539818
train_loss: 0.003016652
test_loss: 0.0038133916
train_loss: 0.0032692482
test_loss: 0.0036482248
train_loss: 0.0029401977
test_loss: 0.003681993
train_loss: 0.003002546
test_loss: 0.0036428275
train_loss: 0.0029927501
test_loss: 0.0038664145
train_loss: 0.0030422562
test_loss: 0.003725371
train_loss: 0.0030379398
test_loss: 0.003741221
train_loss: 0.002991597
test_loss: 0.0037535387
train_loss: 0.0030305893
test_loss: 0.0038109126
train_loss: 0.002886971
test_loss: 0.003675475
train_loss: 0.0032180771
test_loss: 0.0039607384
train_loss: 0.003021685
test_loss: 0.0037875401
train_loss: 0.0029515903
test_loss: 0.0036172867
train_loss: 0.0031619242
test_loss: 0.0037863043
train_loss: 0.0030270764
test_loss: 0.0035910935
train_loss: 0.002911786
test_loss: 0.003715083
train_loss: 0.0031080816
test_loss: 0.0037827755
train_loss: 0.0031002145
test_loss: 0.0038916226
train_loss: 0.0030216912
test_loss: 0.0036785589
train_loss: 0.0031863777
test_loss: 0.0037136106
train_loss: 0.002996167
test_loss: 0.0037533885
train_loss: 0.002817035
test_loss: 0.0035788254
train_loss: 0.0028643962
test_loss: 0.0039130286
train_loss: 0.0030195722
test_loss: 0.003758189
train_loss: 0.0029662494
test_loss: 0.0037274987
train_loss: 0.0029844844
test_loss: 0.003893455
train_loss: 0.0028562364
test_loss: 0.0035044798
train_loss: 0.0033537233
test_loss: 0.00372451
train_loss: 0.003060209
test_loss: 0.0035822245
train_loss: 0.0029734848
test_loss: 0.0036112396
train_loss: 0.0030494328
test_loss: 0.00362654
train_loss: 0.0031990847
test_loss: 0.0037265208
train_loss: 0.0031205192
test_loss: 0.003793647
train_loss: 0.003134426
test_loss: 0.0036503721
train_loss: 0.0028530671
test_loss: 0.0037472039
train_loss: 0.0028370523
test_loss: 0.0036678654
train_loss: 0.0027482866
test_loss: 0.0037140497
train_loss: 0.002996182
test_loss: 0.0036267412
train_loss: 0.0030336769
test_loss: 0.0036294318
train_loss: 0.003134611
test_loss: 0.00372899
train_loss: 0.0028936397
test_loss: 0.003783183
train_loss: 0.0027617114
test_loss: 0.003616539
train_loss: 0.0029254984
test_loss: 0.003601498
train_loss: 0.0030900808
test_loss: 0.0036836548
train_loss: 0.003043451
test_loss: 0.0037299371
train_loss: 0.003309333
test_loss: 0.0036435018
train_loss: 0.0028269254
test_loss: 0.003699966
train_loss: 0.0028264425
test_loss: 0.0036043155
train_loss: 0.0028737
test_loss: 0.0035867253
train_loss: 0.002878068
test_loss: 0.0036131954
train_loss: 0.0029329888
test_loss: 0.0036144604
train_loss: 0.0031169807
test_loss: 0.0037477356
train_loss: 0.0028919086
test_loss: 0.0036021469
train_loss: 0.0031150049
test_loss: 0.0036955806
train_loss: 0.0031226533
test_loss: 0.0038151608
train_loss: 0.0030687784
test_loss: 0.003697088
train_loss: 0.0032041292
test_loss: 0.0036699183
train_loss: 0.0029707437
test_loss: 0.0037398443
train_loss: 0.0030840943
test_loss: 0.003693113
train_loss: 0.0027824088
test_loss: 0.0035088297
train_loss: 0.003081373
test_loss: 0.003822301
train_loss: 0.002925781
test_loss: 0.0037135582
train_loss: 0.0029826118
test_loss: 0.003640134
train_loss: 0.0030881723
test_loss: 0.0036150631
train_loss: 0.0029694675
test_loss: 0.0035092453
train_loss: 0.0028552846
test_loss: 0.0035441804
train_loss: 0.002887643
test_loss: 0.003637943
train_loss: 0.003020418
test_loss: 0.003572563
train_loss: 0.0027311468
test_loss: 0.0036452669
train_loss: 0.0027576713
test_loss: 0.003727019
train_loss: 0.002816736
test_loss: 0.0036594733
train_loss: 0.002945841
test_loss: 0.0037401258
train_loss: 0.0029039485
test_loss: 0.0036260334
train_loss: 0.0029210474
test_loss: 0.003609905
train_loss: 0.003100318
test_loss: 0.0037892214
train_loss: 0.003229639
test_loss: 0.0038081026
train_loss: 0.002876083
test_loss: 0.0036082773
train_loss: 0.002855291
test_loss: 0.0036298647
train_loss: 0.002978675
test_loss: 0.003678823
train_loss: 0.002882198
test_loss: 0.0036451465
train_loss: 0.002966206
test_loss: 0.0035296052
train_loss: 0.0028882902
test_loss: 0.0037213853
train_loss: 0.0029269943
test_loss: 0.0036743083
train_loss: 0.0027464866
test_loss: 0.0036554087
train_loss: 0.0030240142
test_loss: 0.003676105
train_loss: 0.002669594
test_loss: 0.003534458
train_loss: 0.0027909596
test_loss: 0.0035990148
train_loss: 0.0028444338
test_loss: 0.0036543456
train_loss: 0.002809589
test_loss: 0.003654605
train_loss: 0.0028622139
test_loss: 0.0034840158
train_loss: 0.0028668134
test_loss: 0.0036197545
train_loss: 0.0029908363
test_loss: 0.0042080935
train_loss: 0.002593534
test_loss: 0.0035860469
train_loss: 0.002764425
test_loss: 0.0035136282
train_loss: 0.0029044596
test_loss: 0.0035594192
train_loss: 0.002822664
test_loss: 0.0035304758
train_loss: 0.0028680225
test_loss: 0.0035960218
train_loss: 0.0028910385
test_loss: 0.0036054247
train_loss: 0.0028694302
test_loss: 0.0036297757
train_loss: 0.0028192364
test_loss: 0.0034956783
train_loss: 0.002794561
test_loss: 0.003662532
train_loss: 0.002803808
test_loss: 0.0034759718
train_loss: 0.0031756652
test_loss: 0.0037784283
train_loss: 0.0026259897
test_loss: 0.0035722342
train_loss: 0.0028510697
test_loss: 0.0036378517
train_loss: 0.0027982858
test_loss: 0.003658423
train_loss: 0.0027236468
test_loss: 0.0035765949
train_loss: 0.0029151984
test_loss: 0.0036648917
train_loss: 0.0027958113
test_loss: 0.0035107478
train_loss: 0.0027532391
test_loss: 0.0036743032
train_loss: 0.0028366211
test_loss: 0.0036343269
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi1.6/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.6/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.6 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi1.6/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfe0dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfdabf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfcf3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfceef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfd1e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfd1e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfc558c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfc0a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfc0a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfc55d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfbbf488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfb9eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfb9e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfb56d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfafe9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfb1cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfb16510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfb166a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfa878c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfa870d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfa2cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfa2cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2ecfa53158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb1018598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb1018268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb0fbd7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb0f86840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb0fa66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb0faa6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb0f559d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb0ef4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb0f24268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb0eaf268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb0ee8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb0e858c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f2eb0e8df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.3763155e-05
Iter: 2 loss: 1.18581465e-05
Iter: 3 loss: 3.79124722e-05
Iter: 4 loss: 1.18508578e-05
Iter: 5 loss: 1.08514614e-05
Iter: 6 loss: 1.2507533e-05
Iter: 7 loss: 1.03961629e-05
Iter: 8 loss: 9.68399399e-06
Iter: 9 loss: 1.66492209e-05
Iter: 10 loss: 9.65904655e-06
Iter: 11 loss: 9.09127084e-06
Iter: 12 loss: 8.58172461e-06
Iter: 13 loss: 8.43809e-06
Iter: 14 loss: 7.94315201e-06
Iter: 15 loss: 1.2980724e-05
Iter: 16 loss: 7.92837272e-06
Iter: 17 loss: 7.5066032e-06
Iter: 18 loss: 7.960517e-06
Iter: 19 loss: 7.27503084e-06
Iter: 20 loss: 6.93509355e-06
Iter: 21 loss: 9.23488733e-06
Iter: 22 loss: 6.90142679e-06
Iter: 23 loss: 6.62065395e-06
Iter: 24 loss: 6.63460924e-06
Iter: 25 loss: 6.40010785e-06
Iter: 26 loss: 6.08500659e-06
Iter: 27 loss: 9.47209355e-06
Iter: 28 loss: 6.07767197e-06
Iter: 29 loss: 5.86349279e-06
Iter: 30 loss: 5.54654707e-06
Iter: 31 loss: 5.53907148e-06
Iter: 32 loss: 5.20839649e-06
Iter: 33 loss: 7.14861835e-06
Iter: 34 loss: 5.16545879e-06
Iter: 35 loss: 4.93279549e-06
Iter: 36 loss: 5.39089888e-06
Iter: 37 loss: 4.83692838e-06
Iter: 38 loss: 4.67339305e-06
Iter: 39 loss: 6.39515974e-06
Iter: 40 loss: 4.66925076e-06
Iter: 41 loss: 4.52096492e-06
Iter: 42 loss: 5.72649833e-06
Iter: 43 loss: 4.51153574e-06
Iter: 44 loss: 4.4293256e-06
Iter: 45 loss: 4.75466368e-06
Iter: 46 loss: 4.41033e-06
Iter: 47 loss: 4.31694934e-06
Iter: 48 loss: 4.23751271e-06
Iter: 49 loss: 4.21172354e-06
Iter: 50 loss: 4.0680261e-06
Iter: 51 loss: 4.22369112e-06
Iter: 52 loss: 3.98923657e-06
Iter: 53 loss: 3.84958184e-06
Iter: 54 loss: 5.91598518e-06
Iter: 55 loss: 3.84937221e-06
Iter: 56 loss: 3.77977585e-06
Iter: 57 loss: 3.7702946e-06
Iter: 58 loss: 3.72102772e-06
Iter: 59 loss: 3.63047934e-06
Iter: 60 loss: 4.26799897e-06
Iter: 61 loss: 3.62246101e-06
Iter: 62 loss: 3.56099326e-06
Iter: 63 loss: 3.63003528e-06
Iter: 64 loss: 3.52782536e-06
Iter: 65 loss: 3.44016439e-06
Iter: 66 loss: 3.57302588e-06
Iter: 67 loss: 3.39838925e-06
Iter: 68 loss: 3.3283477e-06
Iter: 69 loss: 3.38206701e-06
Iter: 70 loss: 3.28563101e-06
Iter: 71 loss: 3.20328854e-06
Iter: 72 loss: 3.42983185e-06
Iter: 73 loss: 3.17656531e-06
Iter: 74 loss: 3.10605697e-06
Iter: 75 loss: 3.2058e-06
Iter: 76 loss: 3.07128312e-06
Iter: 77 loss: 3.05994399e-06
Iter: 78 loss: 3.03123852e-06
Iter: 79 loss: 3.00580405e-06
Iter: 80 loss: 2.9968644e-06
Iter: 81 loss: 2.98274426e-06
Iter: 82 loss: 2.94318897e-06
Iter: 83 loss: 3.11562371e-06
Iter: 84 loss: 2.93525068e-06
Iter: 85 loss: 2.90111166e-06
Iter: 86 loss: 2.85277929e-06
Iter: 87 loss: 2.85103988e-06
Iter: 88 loss: 2.82296423e-06
Iter: 89 loss: 2.82089104e-06
Iter: 90 loss: 2.79297319e-06
Iter: 91 loss: 2.75853745e-06
Iter: 92 loss: 2.75555703e-06
Iter: 93 loss: 2.71783756e-06
Iter: 94 loss: 3.06236598e-06
Iter: 95 loss: 2.71608565e-06
Iter: 96 loss: 2.68644681e-06
Iter: 97 loss: 2.75113325e-06
Iter: 98 loss: 2.67503674e-06
Iter: 99 loss: 2.645731e-06
Iter: 100 loss: 2.68759095e-06
Iter: 101 loss: 2.63147535e-06
Iter: 102 loss: 2.59632816e-06
Iter: 103 loss: 2.69871225e-06
Iter: 104 loss: 2.58532464e-06
Iter: 105 loss: 2.55764326e-06
Iter: 106 loss: 2.56517728e-06
Iter: 107 loss: 2.53750568e-06
Iter: 108 loss: 2.50536777e-06
Iter: 109 loss: 2.53987855e-06
Iter: 110 loss: 2.48769265e-06
Iter: 111 loss: 2.45611454e-06
Iter: 112 loss: 2.90514845e-06
Iter: 113 loss: 2.4561075e-06
Iter: 114 loss: 2.43583372e-06
Iter: 115 loss: 2.75231923e-06
Iter: 116 loss: 2.43587237e-06
Iter: 117 loss: 2.42422311e-06
Iter: 118 loss: 2.4137189e-06
Iter: 119 loss: 2.41086491e-06
Iter: 120 loss: 2.38698385e-06
Iter: 121 loss: 2.47097432e-06
Iter: 122 loss: 2.38079e-06
Iter: 123 loss: 2.36647293e-06
Iter: 124 loss: 2.35112907e-06
Iter: 125 loss: 2.34859112e-06
Iter: 126 loss: 2.33507535e-06
Iter: 127 loss: 2.33326114e-06
Iter: 128 loss: 2.32419302e-06
Iter: 129 loss: 2.30274e-06
Iter: 130 loss: 2.55409577e-06
Iter: 131 loss: 2.30078876e-06
Iter: 132 loss: 2.28142289e-06
Iter: 133 loss: 2.28114686e-06
Iter: 134 loss: 2.26921384e-06
Iter: 135 loss: 2.27636383e-06
Iter: 136 loss: 2.26145676e-06
Iter: 137 loss: 2.24660266e-06
Iter: 138 loss: 2.27576038e-06
Iter: 139 loss: 2.24038831e-06
Iter: 140 loss: 2.22401741e-06
Iter: 141 loss: 2.26233055e-06
Iter: 142 loss: 2.2179147e-06
Iter: 143 loss: 2.20391166e-06
Iter: 144 loss: 2.2048323e-06
Iter: 145 loss: 2.19311664e-06
Iter: 146 loss: 2.1785952e-06
Iter: 147 loss: 2.32420234e-06
Iter: 148 loss: 2.17804177e-06
Iter: 149 loss: 2.16380226e-06
Iter: 150 loss: 2.26010479e-06
Iter: 151 loss: 2.1622327e-06
Iter: 152 loss: 2.15358386e-06
Iter: 153 loss: 2.16019089e-06
Iter: 154 loss: 2.148254e-06
Iter: 155 loss: 2.13796739e-06
Iter: 156 loss: 2.18987384e-06
Iter: 157 loss: 2.13617977e-06
Iter: 158 loss: 2.12784425e-06
Iter: 159 loss: 2.11054657e-06
Iter: 160 loss: 2.4177989e-06
Iter: 161 loss: 2.1101846e-06
Iter: 162 loss: 2.10953158e-06
Iter: 163 loss: 2.10275584e-06
Iter: 164 loss: 2.09731343e-06
Iter: 165 loss: 2.08311985e-06
Iter: 166 loss: 2.19089065e-06
Iter: 167 loss: 2.08025176e-06
Iter: 168 loss: 2.07182279e-06
Iter: 169 loss: 2.07032303e-06
Iter: 170 loss: 2.0629966e-06
Iter: 171 loss: 2.06007508e-06
Iter: 172 loss: 2.05605693e-06
Iter: 173 loss: 2.04594744e-06
Iter: 174 loss: 2.07982862e-06
Iter: 175 loss: 2.04320031e-06
Iter: 176 loss: 2.03318223e-06
Iter: 177 loss: 2.05438027e-06
Iter: 178 loss: 2.02932847e-06
Iter: 179 loss: 2.0190223e-06
Iter: 180 loss: 2.03048603e-06
Iter: 181 loss: 2.01343482e-06
Iter: 182 loss: 2.01180137e-06
Iter: 183 loss: 2.00900649e-06
Iter: 184 loss: 2.00499107e-06
Iter: 185 loss: 1.99970691e-06
Iter: 186 loss: 1.99930946e-06
Iter: 187 loss: 1.9924646e-06
Iter: 188 loss: 2.03114769e-06
Iter: 189 loss: 1.99154988e-06
Iter: 190 loss: 1.98452381e-06
Iter: 191 loss: 1.98443945e-06
Iter: 192 loss: 1.97892177e-06
Iter: 193 loss: 1.97153304e-06
Iter: 194 loss: 1.98241241e-06
Iter: 195 loss: 1.96803353e-06
Iter: 196 loss: 1.96097085e-06
Iter: 197 loss: 2.06910249e-06
Iter: 198 loss: 1.9609347e-06
Iter: 199 loss: 1.95650955e-06
Iter: 200 loss: 1.94785684e-06
Iter: 201 loss: 2.11599809e-06
Iter: 202 loss: 1.94776158e-06
Iter: 203 loss: 1.94282802e-06
Iter: 204 loss: 1.94212589e-06
Iter: 205 loss: 1.93766687e-06
Iter: 206 loss: 1.92960169e-06
Iter: 207 loss: 2.12560303e-06
Iter: 208 loss: 1.92958578e-06
Iter: 209 loss: 1.92185325e-06
Iter: 210 loss: 2.00085265e-06
Iter: 211 loss: 1.92159609e-06
Iter: 212 loss: 1.91539e-06
Iter: 213 loss: 1.91761228e-06
Iter: 214 loss: 1.91110144e-06
Iter: 215 loss: 1.90349806e-06
Iter: 216 loss: 1.94431e-06
Iter: 217 loss: 1.90234414e-06
Iter: 218 loss: 1.89829052e-06
Iter: 219 loss: 1.89792979e-06
Iter: 220 loss: 1.89562832e-06
Iter: 221 loss: 1.89038087e-06
Iter: 222 loss: 1.95884377e-06
Iter: 223 loss: 1.89008949e-06
Iter: 224 loss: 1.8834329e-06
Iter: 225 loss: 1.9462957e-06
Iter: 226 loss: 1.88310094e-06
Iter: 227 loss: 1.87911166e-06
Iter: 228 loss: 1.87572175e-06
Iter: 229 loss: 1.87460523e-06
Iter: 230 loss: 1.86883267e-06
Iter: 231 loss: 1.90529011e-06
Iter: 232 loss: 1.86824866e-06
Iter: 233 loss: 1.86221757e-06
Iter: 234 loss: 1.87674539e-06
Iter: 235 loss: 1.86008492e-06
Iter: 236 loss: 1.85566262e-06
Iter: 237 loss: 1.8543044e-06
Iter: 238 loss: 1.85170393e-06
Iter: 239 loss: 1.84528949e-06
Iter: 240 loss: 1.91667914e-06
Iter: 241 loss: 1.84527948e-06
Iter: 242 loss: 1.841261e-06
Iter: 243 loss: 1.83696659e-06
Iter: 244 loss: 1.83630607e-06
Iter: 245 loss: 1.83062639e-06
Iter: 246 loss: 1.8802632e-06
Iter: 247 loss: 1.83043915e-06
Iter: 248 loss: 1.82649296e-06
Iter: 249 loss: 1.83305849e-06
Iter: 250 loss: 1.82467966e-06
Iter: 251 loss: 1.82054191e-06
Iter: 252 loss: 1.84540738e-06
Iter: 253 loss: 1.82005965e-06
Iter: 254 loss: 1.81518931e-06
Iter: 255 loss: 1.82609676e-06
Iter: 256 loss: 1.81325208e-06
Iter: 257 loss: 1.81057567e-06
Iter: 258 loss: 1.81481153e-06
Iter: 259 loss: 1.8092735e-06
Iter: 260 loss: 1.80521829e-06
Iter: 261 loss: 1.8083324e-06
Iter: 262 loss: 1.80272946e-06
Iter: 263 loss: 1.79814674e-06
Iter: 264 loss: 1.7996972e-06
Iter: 265 loss: 1.79489325e-06
Iter: 266 loss: 1.79105814e-06
Iter: 267 loss: 1.79110748e-06
Iter: 268 loss: 1.78773394e-06
Iter: 269 loss: 1.7837699e-06
Iter: 270 loss: 1.783279e-06
Iter: 271 loss: 1.77881407e-06
Iter: 272 loss: 1.80323309e-06
Iter: 273 loss: 1.77822278e-06
Iter: 274 loss: 1.77353377e-06
Iter: 275 loss: 1.78982202e-06
Iter: 276 loss: 1.77227435e-06
Iter: 277 loss: 1.76922481e-06
Iter: 278 loss: 1.77111076e-06
Iter: 279 loss: 1.76725098e-06
Iter: 280 loss: 1.76296567e-06
Iter: 281 loss: 1.7711601e-06
Iter: 282 loss: 1.76127855e-06
Iter: 283 loss: 1.75672596e-06
Iter: 284 loss: 1.77900154e-06
Iter: 285 loss: 1.75596404e-06
Iter: 286 loss: 1.75383457e-06
Iter: 287 loss: 1.7536604e-06
Iter: 288 loss: 1.75184744e-06
Iter: 289 loss: 1.74783463e-06
Iter: 290 loss: 1.81039502e-06
Iter: 291 loss: 1.74771424e-06
Iter: 292 loss: 1.74447507e-06
Iter: 293 loss: 1.78695541e-06
Iter: 294 loss: 1.74441971e-06
Iter: 295 loss: 1.7413148e-06
Iter: 296 loss: 1.7387232e-06
Iter: 297 loss: 1.73784542e-06
Iter: 298 loss: 1.73474871e-06
Iter: 299 loss: 1.74925049e-06
Iter: 300 loss: 1.73418903e-06
Iter: 301 loss: 1.73085766e-06
Iter: 302 loss: 1.7455252e-06
Iter: 303 loss: 1.73021112e-06
Iter: 304 loss: 1.72720411e-06
Iter: 305 loss: 1.72602938e-06
Iter: 306 loss: 1.72454156e-06
Iter: 307 loss: 1.72179068e-06
Iter: 308 loss: 1.7489682e-06
Iter: 309 loss: 1.72175237e-06
Iter: 310 loss: 1.71864485e-06
Iter: 311 loss: 1.71349734e-06
Iter: 312 loss: 1.7135103e-06
Iter: 313 loss: 1.70924113e-06
Iter: 314 loss: 1.74502225e-06
Iter: 315 loss: 1.7090058e-06
Iter: 316 loss: 1.70527e-06
Iter: 317 loss: 1.71084889e-06
Iter: 318 loss: 1.70347141e-06
Iter: 319 loss: 1.70009957e-06
Iter: 320 loss: 1.73177375e-06
Iter: 321 loss: 1.6998689e-06
Iter: 322 loss: 1.69654891e-06
Iter: 323 loss: 1.71001466e-06
Iter: 324 loss: 1.69590771e-06
Iter: 325 loss: 1.69352836e-06
Iter: 326 loss: 1.6911215e-06
Iter: 327 loss: 1.69060627e-06
Iter: 328 loss: 1.68820247e-06
Iter: 329 loss: 1.68815632e-06
Iter: 330 loss: 1.68632289e-06
Iter: 331 loss: 1.68207407e-06
Iter: 332 loss: 1.73306057e-06
Iter: 333 loss: 1.68172721e-06
Iter: 334 loss: 1.67951805e-06
Iter: 335 loss: 1.67915698e-06
Iter: 336 loss: 1.67711096e-06
Iter: 337 loss: 1.6777276e-06
Iter: 338 loss: 1.67551502e-06
Iter: 339 loss: 1.6727472e-06
Iter: 340 loss: 1.67210885e-06
Iter: 341 loss: 1.67045346e-06
Iter: 342 loss: 1.66762766e-06
Iter: 343 loss: 1.66768427e-06
Iter: 344 loss: 1.66539235e-06
Iter: 345 loss: 1.6632174e-06
Iter: 346 loss: 1.66273026e-06
Iter: 347 loss: 1.65979304e-06
Iter: 348 loss: 1.6668763e-06
Iter: 349 loss: 1.65869744e-06
Iter: 350 loss: 1.65518236e-06
Iter: 351 loss: 1.67333724e-06
Iter: 352 loss: 1.65466122e-06
Iter: 353 loss: 1.6528553e-06
Iter: 354 loss: 1.6528146e-06
Iter: 355 loss: 1.6513294e-06
Iter: 356 loss: 1.6494954e-06
Iter: 357 loss: 1.64927769e-06
Iter: 358 loss: 1.64683911e-06
Iter: 359 loss: 1.65059168e-06
Iter: 360 loss: 1.64572111e-06
Iter: 361 loss: 1.64264679e-06
Iter: 362 loss: 1.66197765e-06
Iter: 363 loss: 1.64228288e-06
Iter: 364 loss: 1.64054302e-06
Iter: 365 loss: 1.63785592e-06
Iter: 366 loss: 1.63777929e-06
Iter: 367 loss: 1.63558411e-06
Iter: 368 loss: 1.63555569e-06
Iter: 369 loss: 1.63346704e-06
Iter: 370 loss: 1.63407447e-06
Iter: 371 loss: 1.63193204e-06
Iter: 372 loss: 1.62991228e-06
Iter: 373 loss: 1.63177242e-06
Iter: 374 loss: 1.62869492e-06
Iter: 375 loss: 1.62617312e-06
Iter: 376 loss: 1.64920255e-06
Iter: 377 loss: 1.62614208e-06
Iter: 378 loss: 1.62407298e-06
Iter: 379 loss: 1.62159267e-06
Iter: 380 loss: 1.62136871e-06
Iter: 381 loss: 1.61874505e-06
Iter: 382 loss: 1.62812694e-06
Iter: 383 loss: 1.61807293e-06
Iter: 384 loss: 1.61496075e-06
Iter: 385 loss: 1.63095706e-06
Iter: 386 loss: 1.61441653e-06
Iter: 387 loss: 1.61227763e-06
Iter: 388 loss: 1.64505411e-06
Iter: 389 loss: 1.61227581e-06
Iter: 390 loss: 1.61098944e-06
Iter: 391 loss: 1.60965874e-06
Iter: 392 loss: 1.60940954e-06
Iter: 393 loss: 1.60747095e-06
Iter: 394 loss: 1.61383969e-06
Iter: 395 loss: 1.60693389e-06
Iter: 396 loss: 1.60454169e-06
Iter: 397 loss: 1.61099047e-06
Iter: 398 loss: 1.60367688e-06
Iter: 399 loss: 1.60216757e-06
Iter: 400 loss: 1.59982142e-06
Iter: 401 loss: 1.59981641e-06
Iter: 402 loss: 1.5982589e-06
Iter: 403 loss: 1.59787919e-06
Iter: 404 loss: 1.59667184e-06
Iter: 405 loss: 1.59476588e-06
Iter: 406 loss: 1.59470733e-06
Iter: 407 loss: 1.59246247e-06
Iter: 408 loss: 1.60491413e-06
Iter: 409 loss: 1.5921305e-06
Iter: 410 loss: 1.59022386e-06
Iter: 411 loss: 1.60147169e-06
Iter: 412 loss: 1.58995454e-06
Iter: 413 loss: 1.58850935e-06
Iter: 414 loss: 1.58553348e-06
Iter: 415 loss: 1.6440672e-06
Iter: 416 loss: 1.58553758e-06
Iter: 417 loss: 1.58297678e-06
Iter: 418 loss: 1.61209596e-06
Iter: 419 loss: 1.58293892e-06
Iter: 420 loss: 1.581059e-06
Iter: 421 loss: 1.60042089e-06
Iter: 422 loss: 1.58094144e-06
Iter: 423 loss: 1.57930231e-06
Iter: 424 loss: 1.58331613e-06
Iter: 425 loss: 1.57865895e-06
Iter: 426 loss: 1.57717068e-06
Iter: 427 loss: 1.57551676e-06
Iter: 428 loss: 1.57528166e-06
Iter: 429 loss: 1.57340276e-06
Iter: 430 loss: 1.60318746e-06
Iter: 431 loss: 1.57337445e-06
Iter: 432 loss: 1.57188515e-06
Iter: 433 loss: 1.57121769e-06
Iter: 434 loss: 1.57044451e-06
Iter: 435 loss: 1.56866122e-06
Iter: 436 loss: 1.5709868e-06
Iter: 437 loss: 1.56772285e-06
Iter: 438 loss: 1.56647e-06
Iter: 439 loss: 1.56643125e-06
Iter: 440 loss: 1.5653934e-06
Iter: 441 loss: 1.56325257e-06
Iter: 442 loss: 1.59351464e-06
Iter: 443 loss: 1.56305339e-06
Iter: 444 loss: 1.56116153e-06
Iter: 445 loss: 1.58686e-06
Iter: 446 loss: 1.56109377e-06
Iter: 447 loss: 1.55929138e-06
Iter: 448 loss: 1.56240162e-06
Iter: 449 loss: 1.55849636e-06
Iter: 450 loss: 1.55662531e-06
Iter: 451 loss: 1.555165e-06
Iter: 452 loss: 1.55448231e-06
Iter: 453 loss: 1.55240423e-06
Iter: 454 loss: 1.56090573e-06
Iter: 455 loss: 1.55185239e-06
Iter: 456 loss: 1.55028965e-06
Iter: 457 loss: 1.55017938e-06
Iter: 458 loss: 1.54900329e-06
Iter: 459 loss: 1.54920656e-06
Iter: 460 loss: 1.54817576e-06
Iter: 461 loss: 1.54681504e-06
Iter: 462 loss: 1.54902455e-06
Iter: 463 loss: 1.54616237e-06
Iter: 464 loss: 1.54497866e-06
Iter: 465 loss: 1.55196403e-06
Iter: 466 loss: 1.54492227e-06
Iter: 467 loss: 1.54372333e-06
Iter: 468 loss: 1.54456666e-06
Iter: 469 loss: 1.54300051e-06
Iter: 470 loss: 1.54154873e-06
Iter: 471 loss: 1.54014856e-06
Iter: 472 loss: 1.53989038e-06
Iter: 473 loss: 1.53893495e-06
Iter: 474 loss: 1.53864653e-06
Iter: 475 loss: 1.53762846e-06
Iter: 476 loss: 1.53554254e-06
Iter: 477 loss: 1.56714634e-06
Iter: 478 loss: 1.53545921e-06
Iter: 479 loss: 1.53356723e-06
Iter: 480 loss: 1.5542837e-06
Iter: 481 loss: 1.53351766e-06
Iter: 482 loss: 1.53207429e-06
Iter: 483 loss: 1.53885571e-06
Iter: 484 loss: 1.53181759e-06
Iter: 485 loss: 1.53058943e-06
Iter: 486 loss: 1.5288847e-06
Iter: 487 loss: 1.52878033e-06
Iter: 488 loss: 1.526917e-06
Iter: 489 loss: 1.53075564e-06
Iter: 490 loss: 1.52611733e-06
Iter: 491 loss: 1.52567918e-06
Iter: 492 loss: 1.52500547e-06
Iter: 493 loss: 1.52425196e-06
Iter: 494 loss: 1.52285236e-06
Iter: 495 loss: 1.55444047e-06
Iter: 496 loss: 1.52285008e-06
Iter: 497 loss: 1.52112898e-06
Iter: 498 loss: 1.52695679e-06
Iter: 499 loss: 1.5206989e-06
Iter: 500 loss: 1.51928919e-06
Iter: 501 loss: 1.52780626e-06
Iter: 502 loss: 1.51915299e-06
Iter: 503 loss: 1.51806648e-06
Iter: 504 loss: 1.52144162e-06
Iter: 505 loss: 1.51776203e-06
Iter: 506 loss: 1.51667973e-06
Iter: 507 loss: 1.51515769e-06
Iter: 508 loss: 1.51506129e-06
Iter: 509 loss: 1.51422284e-06
Iter: 510 loss: 1.51408767e-06
Iter: 511 loss: 1.51319477e-06
Iter: 512 loss: 1.5119e-06
Iter: 513 loss: 1.51186077e-06
Iter: 514 loss: 1.510326e-06
Iter: 515 loss: 1.51424945e-06
Iter: 516 loss: 1.50979474e-06
Iter: 517 loss: 1.50795745e-06
Iter: 518 loss: 1.51999757e-06
Iter: 519 loss: 1.50784854e-06
Iter: 520 loss: 1.50690289e-06
Iter: 521 loss: 1.50700293e-06
Iter: 522 loss: 1.50612038e-06
Iter: 523 loss: 1.50480378e-06
Iter: 524 loss: 1.50328015e-06
Iter: 525 loss: 1.50312667e-06
Iter: 526 loss: 1.50467315e-06
Iter: 527 loss: 1.5023877e-06
Iter: 528 loss: 1.50177129e-06
Iter: 529 loss: 1.50020151e-06
Iter: 530 loss: 1.51469885e-06
Iter: 531 loss: 1.49997186e-06
Iter: 532 loss: 1.49830021e-06
Iter: 533 loss: 1.50953292e-06
Iter: 534 loss: 1.49818152e-06
Iter: 535 loss: 1.49698326e-06
Iter: 536 loss: 1.50324081e-06
Iter: 537 loss: 1.49673781e-06
Iter: 538 loss: 1.49565653e-06
Iter: 539 loss: 1.49778839e-06
Iter: 540 loss: 1.49521065e-06
Iter: 541 loss: 1.49404559e-06
Iter: 542 loss: 1.49484322e-06
Iter: 543 loss: 1.4932989e-06
Iter: 544 loss: 1.49211837e-06
Iter: 545 loss: 1.49663083e-06
Iter: 546 loss: 1.49184962e-06
Iter: 547 loss: 1.49030143e-06
Iter: 548 loss: 1.49315906e-06
Iter: 549 loss: 1.48962795e-06
Iter: 550 loss: 1.48851734e-06
Iter: 551 loss: 1.48836864e-06
Iter: 552 loss: 1.48753861e-06
Iter: 553 loss: 1.48630943e-06
Iter: 554 loss: 1.48630329e-06
Iter: 555 loss: 1.48539357e-06
Iter: 556 loss: 1.48466461e-06
Iter: 557 loss: 1.48437425e-06
Iter: 558 loss: 1.48302843e-06
Iter: 559 loss: 1.48234176e-06
Iter: 560 loss: 1.48169829e-06
Iter: 561 loss: 1.48225683e-06
Iter: 562 loss: 1.48086713e-06
Iter: 563 loss: 1.48024492e-06
Iter: 564 loss: 1.4787945e-06
Iter: 565 loss: 1.49501261e-06
Iter: 566 loss: 1.47863739e-06
Iter: 567 loss: 1.47710455e-06
Iter: 568 loss: 1.482421e-06
Iter: 569 loss: 1.47655533e-06
Iter: 570 loss: 1.47555011e-06
Iter: 571 loss: 1.49108689e-06
Iter: 572 loss: 1.47554579e-06
Iter: 573 loss: 1.47460651e-06
Iter: 574 loss: 1.47357014e-06
Iter: 575 loss: 1.47341859e-06
Iter: 576 loss: 1.47179503e-06
Iter: 577 loss: 1.48237564e-06
Iter: 578 loss: 1.47166543e-06
Iter: 579 loss: 1.47075718e-06
Iter: 580 loss: 1.47117657e-06
Iter: 581 loss: 1.47002697e-06
Iter: 582 loss: 1.46874459e-06
Iter: 583 loss: 1.4825182e-06
Iter: 584 loss: 1.4687771e-06
Iter: 585 loss: 1.46805587e-06
Iter: 586 loss: 1.46676825e-06
Iter: 587 loss: 1.49652669e-06
Iter: 588 loss: 1.46678849e-06
Iter: 589 loss: 1.46538127e-06
Iter: 590 loss: 1.48125537e-06
Iter: 591 loss: 1.46539878e-06
Iter: 592 loss: 1.46425577e-06
Iter: 593 loss: 1.46793946e-06
Iter: 594 loss: 1.46395985e-06
Iter: 595 loss: 1.46315631e-06
Iter: 596 loss: 1.46153559e-06
Iter: 597 loss: 1.49338473e-06
Iter: 598 loss: 1.46152763e-06
Iter: 599 loss: 1.45983154e-06
Iter: 600 loss: 1.47832452e-06
Iter: 601 loss: 1.45985359e-06
Iter: 602 loss: 1.45887e-06
Iter: 603 loss: 1.45881529e-06
Iter: 604 loss: 1.45835065e-06
Iter: 605 loss: 1.45711192e-06
Iter: 606 loss: 1.46665082e-06
Iter: 607 loss: 1.45694059e-06
Iter: 608 loss: 1.45533727e-06
Iter: 609 loss: 1.46271282e-06
Iter: 610 loss: 1.45512831e-06
Iter: 611 loss: 1.45396893e-06
Iter: 612 loss: 1.45396075e-06
Iter: 613 loss: 1.45328806e-06
Iter: 614 loss: 1.45215768e-06
Iter: 615 loss: 1.4786765e-06
Iter: 616 loss: 1.45218564e-06
Iter: 617 loss: 1.45065746e-06
Iter: 618 loss: 1.46159823e-06
Iter: 619 loss: 1.45053502e-06
Iter: 620 loss: 1.44972944e-06
Iter: 621 loss: 1.45221986e-06
Iter: 622 loss: 1.44947592e-06
Iter: 623 loss: 1.44832779e-06
Iter: 624 loss: 1.44750356e-06
Iter: 625 loss: 1.44701869e-06
Iter: 626 loss: 1.44577461e-06
Iter: 627 loss: 1.44847718e-06
Iter: 628 loss: 1.445213e-06
Iter: 629 loss: 1.44429896e-06
Iter: 630 loss: 1.44428134e-06
Iter: 631 loss: 1.44358194e-06
Iter: 632 loss: 1.44329294e-06
Iter: 633 loss: 1.44294484e-06
Iter: 634 loss: 1.44196258e-06
Iter: 635 loss: 1.44079104e-06
Iter: 636 loss: 1.44057174e-06
Iter: 637 loss: 1.43905072e-06
Iter: 638 loss: 1.44826117e-06
Iter: 639 loss: 1.43878538e-06
Iter: 640 loss: 1.43791374e-06
Iter: 641 loss: 1.4379026e-06
Iter: 642 loss: 1.43692523e-06
Iter: 643 loss: 1.43865282e-06
Iter: 644 loss: 1.43648776e-06
Iter: 645 loss: 1.43588363e-06
Iter: 646 loss: 1.43519333e-06
Iter: 647 loss: 1.435076e-06
Iter: 648 loss: 1.43376542e-06
Iter: 649 loss: 1.44381647e-06
Iter: 650 loss: 1.43372256e-06
Iter: 651 loss: 1.43280931e-06
Iter: 652 loss: 1.43213538e-06
Iter: 653 loss: 1.43182717e-06
Iter: 654 loss: 1.43067041e-06
Iter: 655 loss: 1.44242483e-06
Iter: 656 loss: 1.43063448e-06
Iter: 657 loss: 1.42984868e-06
Iter: 658 loss: 1.43138084e-06
Iter: 659 loss: 1.42955355e-06
Iter: 660 loss: 1.42855686e-06
Iter: 661 loss: 1.43059674e-06
Iter: 662 loss: 1.4281834e-06
Iter: 663 loss: 1.42722524e-06
Iter: 664 loss: 1.42740305e-06
Iter: 665 loss: 1.42652971e-06
Iter: 666 loss: 1.42562226e-06
Iter: 667 loss: 1.4347321e-06
Iter: 668 loss: 1.42562988e-06
Iter: 669 loss: 1.42463705e-06
Iter: 670 loss: 1.42363808e-06
Iter: 671 loss: 1.42346573e-06
Iter: 672 loss: 1.42206568e-06
Iter: 673 loss: 1.42387353e-06
Iter: 674 loss: 1.42133695e-06
Iter: 675 loss: 1.42009e-06
Iter: 676 loss: 1.42837177e-06
Iter: 677 loss: 1.41992518e-06
Iter: 678 loss: 1.41933924e-06
Iter: 679 loss: 1.41923783e-06
Iter: 680 loss: 1.41887688e-06
Iter: 681 loss: 1.41778651e-06
Iter: 682 loss: 1.42516978e-06
Iter: 683 loss: 1.41756516e-06
Iter: 684 loss: 1.41678538e-06
Iter: 685 loss: 1.41680175e-06
Iter: 686 loss: 1.41587088e-06
Iter: 687 loss: 1.41534815e-06
Iter: 688 loss: 1.41502801e-06
Iter: 689 loss: 1.41421128e-06
Iter: 690 loss: 1.41615499e-06
Iter: 691 loss: 1.41385294e-06
Iter: 692 loss: 1.41294527e-06
Iter: 693 loss: 1.4197368e-06
Iter: 694 loss: 1.41285432e-06
Iter: 695 loss: 1.41207079e-06
Iter: 696 loss: 1.41340388e-06
Iter: 697 loss: 1.41168516e-06
Iter: 698 loss: 1.41087946e-06
Iter: 699 loss: 1.41179225e-06
Iter: 700 loss: 1.41045939e-06
Iter: 701 loss: 1.40924408e-06
Iter: 702 loss: 1.40874317e-06
Iter: 703 loss: 1.40811085e-06
Iter: 704 loss: 1.40702207e-06
Iter: 705 loss: 1.41301439e-06
Iter: 706 loss: 1.40677014e-06
Iter: 707 loss: 1.40574593e-06
Iter: 708 loss: 1.41531814e-06
Iter: 709 loss: 1.40570103e-06
Iter: 710 loss: 1.40490329e-06
Iter: 711 loss: 1.40464476e-06
Iter: 712 loss: 1.40424345e-06
Iter: 713 loss: 1.40325619e-06
Iter: 714 loss: 1.40911834e-06
Iter: 715 loss: 1.40307247e-06
Iter: 716 loss: 1.4020668e-06
Iter: 717 loss: 1.40845145e-06
Iter: 718 loss: 1.4019397e-06
Iter: 719 loss: 1.40150019e-06
Iter: 720 loss: 1.4003673e-06
Iter: 721 loss: 1.40804173e-06
Iter: 722 loss: 1.40008387e-06
Iter: 723 loss: 1.39922383e-06
Iter: 724 loss: 1.39915983e-06
Iter: 725 loss: 1.39835299e-06
Iter: 726 loss: 1.39889983e-06
Iter: 727 loss: 1.39779956e-06
Iter: 728 loss: 1.39695499e-06
Iter: 729 loss: 1.39617737e-06
Iter: 730 loss: 1.39597387e-06
Iter: 731 loss: 1.39514418e-06
Iter: 732 loss: 1.39509234e-06
Iter: 733 loss: 1.39432325e-06
Iter: 734 loss: 1.39372196e-06
Iter: 735 loss: 1.39339988e-06
Iter: 736 loss: 1.39265853e-06
Iter: 737 loss: 1.39355416e-06
Iter: 738 loss: 1.39215979e-06
Iter: 739 loss: 1.39107976e-06
Iter: 740 loss: 1.40067357e-06
Iter: 741 loss: 1.39108352e-06
Iter: 742 loss: 1.39040912e-06
Iter: 743 loss: 1.39133624e-06
Iter: 744 loss: 1.3900974e-06
Iter: 745 loss: 1.3891904e-06
Iter: 746 loss: 1.39179474e-06
Iter: 747 loss: 1.38884138e-06
Iter: 748 loss: 1.38794519e-06
Iter: 749 loss: 1.3892095e-06
Iter: 750 loss: 1.38749874e-06
Iter: 751 loss: 1.38658527e-06
Iter: 752 loss: 1.38655855e-06
Iter: 753 loss: 1.38618759e-06
Iter: 754 loss: 1.38520954e-06
Iter: 755 loss: 1.39505323e-06
Iter: 756 loss: 1.38505038e-06
Iter: 757 loss: 1.38396126e-06
Iter: 758 loss: 1.38816176e-06
Iter: 759 loss: 1.38371524e-06
Iter: 760 loss: 1.38302039e-06
Iter: 761 loss: 1.38294081e-06
Iter: 762 loss: 1.38253813e-06
Iter: 763 loss: 1.38176574e-06
Iter: 764 loss: 1.38175e-06
Iter: 765 loss: 1.38075302e-06
Iter: 766 loss: 1.38307985e-06
Iter: 767 loss: 1.38038672e-06
Iter: 768 loss: 1.3790459e-06
Iter: 769 loss: 1.3866794e-06
Iter: 770 loss: 1.37888264e-06
Iter: 771 loss: 1.37824327e-06
Iter: 772 loss: 1.37733741e-06
Iter: 773 loss: 1.37724373e-06
Iter: 774 loss: 1.37621601e-06
Iter: 775 loss: 1.38422854e-06
Iter: 776 loss: 1.37612176e-06
Iter: 777 loss: 1.37523057e-06
Iter: 778 loss: 1.3821068e-06
Iter: 779 loss: 1.37523853e-06
Iter: 780 loss: 1.37458301e-06
Iter: 781 loss: 1.3749634e-06
Iter: 782 loss: 1.37409234e-06
Iter: 783 loss: 1.37353891e-06
Iter: 784 loss: 1.38154576e-06
Iter: 785 loss: 1.37352629e-06
Iter: 786 loss: 1.37302322e-06
Iter: 787 loss: 1.37361508e-06
Iter: 788 loss: 1.3726966e-06
Iter: 789 loss: 1.3719141e-06
Iter: 790 loss: 1.37151767e-06
Iter: 791 loss: 1.37124732e-06
Iter: 792 loss: 1.37056077e-06
Iter: 793 loss: 1.37042048e-06
Iter: 794 loss: 1.36989195e-06
Iter: 795 loss: 1.36894789e-06
Iter: 796 loss: 1.37992947e-06
Iter: 797 loss: 1.36893595e-06
Iter: 798 loss: 1.36808035e-06
Iter: 799 loss: 1.3712804e-06
Iter: 800 loss: 1.3678698e-06
Iter: 801 loss: 1.36720678e-06
Iter: 802 loss: 1.36630297e-06
Iter: 803 loss: 1.36629808e-06
Iter: 804 loss: 1.36559697e-06
Iter: 805 loss: 1.36554206e-06
Iter: 806 loss: 1.36490632e-06
Iter: 807 loss: 1.36399672e-06
Iter: 808 loss: 1.36391441e-06
Iter: 809 loss: 1.36299695e-06
Iter: 810 loss: 1.36402684e-06
Iter: 811 loss: 1.3625413e-06
Iter: 812 loss: 1.36155813e-06
Iter: 813 loss: 1.36904578e-06
Iter: 814 loss: 1.36150879e-06
Iter: 815 loss: 1.36073209e-06
Iter: 816 loss: 1.36791573e-06
Iter: 817 loss: 1.36071515e-06
Iter: 818 loss: 1.36023027e-06
Iter: 819 loss: 1.36034964e-06
Iter: 820 loss: 1.35989058e-06
Iter: 821 loss: 1.35893526e-06
Iter: 822 loss: 1.36160861e-06
Iter: 823 loss: 1.35864025e-06
Iter: 824 loss: 1.35818345e-06
Iter: 825 loss: 1.35818391e-06
Iter: 826 loss: 1.35782807e-06
Iter: 827 loss: 1.35694745e-06
Iter: 828 loss: 1.35735945e-06
Iter: 829 loss: 1.35634605e-06
Iter: 830 loss: 1.35544815e-06
Iter: 831 loss: 1.35899745e-06
Iter: 832 loss: 1.35524601e-06
Iter: 833 loss: 1.35433618e-06
Iter: 834 loss: 1.3620031e-06
Iter: 835 loss: 1.35433879e-06
Iter: 836 loss: 1.35374069e-06
Iter: 837 loss: 1.35305959e-06
Iter: 838 loss: 1.35291521e-06
Iter: 839 loss: 1.35217476e-06
Iter: 840 loss: 1.35617211e-06
Iter: 841 loss: 1.35207597e-06
Iter: 842 loss: 1.35123e-06
Iter: 843 loss: 1.35646656e-06
Iter: 844 loss: 1.35119512e-06
Iter: 845 loss: 1.35060714e-06
Iter: 846 loss: 1.34973743e-06
Iter: 847 loss: 1.34975312e-06
Iter: 848 loss: 1.34860113e-06
Iter: 849 loss: 1.35141363e-06
Iter: 850 loss: 1.34822437e-06
Iter: 851 loss: 1.34747143e-06
Iter: 852 loss: 1.35819687e-06
Iter: 853 loss: 1.34745881e-06
Iter: 854 loss: 1.34679055e-06
Iter: 855 loss: 1.34879986e-06
Iter: 856 loss: 1.34657955e-06
Iter: 857 loss: 1.34599554e-06
Iter: 858 loss: 1.34941888e-06
Iter: 859 loss: 1.34590823e-06
Iter: 860 loss: 1.34548168e-06
Iter: 861 loss: 1.34452227e-06
Iter: 862 loss: 1.36190783e-06
Iter: 863 loss: 1.34446816e-06
Iter: 864 loss: 1.34347852e-06
Iter: 865 loss: 1.34660513e-06
Iter: 866 loss: 1.34317679e-06
Iter: 867 loss: 1.34228048e-06
Iter: 868 loss: 1.34866923e-06
Iter: 869 loss: 1.34212075e-06
Iter: 870 loss: 1.34135894e-06
Iter: 871 loss: 1.34573781e-06
Iter: 872 loss: 1.34124662e-06
Iter: 873 loss: 1.3407298e-06
Iter: 874 loss: 1.34204993e-06
Iter: 875 loss: 1.34062293e-06
Iter: 876 loss: 1.33997332e-06
Iter: 877 loss: 1.33898448e-06
Iter: 878 loss: 1.33900562e-06
Iter: 879 loss: 1.33843855e-06
Iter: 880 loss: 1.33841797e-06
Iter: 881 loss: 1.33780782e-06
Iter: 882 loss: 1.33801575e-06
Iter: 883 loss: 1.33745277e-06
Iter: 884 loss: 1.33672938e-06
Iter: 885 loss: 1.33603976e-06
Iter: 886 loss: 1.33590504e-06
Iter: 887 loss: 1.33483684e-06
Iter: 888 loss: 1.33761705e-06
Iter: 889 loss: 1.33445587e-06
Iter: 890 loss: 1.33458389e-06
Iter: 891 loss: 1.33408162e-06
Iter: 892 loss: 1.33369724e-06
Iter: 893 loss: 1.33269646e-06
Iter: 894 loss: 1.34330116e-06
Iter: 895 loss: 1.33261483e-06
Iter: 896 loss: 1.33174842e-06
Iter: 897 loss: 1.3371988e-06
Iter: 898 loss: 1.33163019e-06
Iter: 899 loss: 1.33096796e-06
Iter: 900 loss: 1.33518631e-06
Iter: 901 loss: 1.33081744e-06
Iter: 902 loss: 1.33025014e-06
Iter: 903 loss: 1.32948719e-06
Iter: 904 loss: 1.32943569e-06
Iter: 905 loss: 1.32858111e-06
Iter: 906 loss: 1.33107255e-06
Iter: 907 loss: 1.32836487e-06
Iter: 908 loss: 1.3274082e-06
Iter: 909 loss: 1.33423327e-06
Iter: 910 loss: 1.32733567e-06
Iter: 911 loss: 1.32673722e-06
Iter: 912 loss: 1.32972946e-06
Iter: 913 loss: 1.32662854e-06
Iter: 914 loss: 1.32603213e-06
Iter: 915 loss: 1.32662035e-06
Iter: 916 loss: 1.32562866e-06
Iter: 917 loss: 1.32489345e-06
Iter: 918 loss: 1.32533569e-06
Iter: 919 loss: 1.32442744e-06
Iter: 920 loss: 1.32367154e-06
Iter: 921 loss: 1.33003903e-06
Iter: 922 loss: 1.32361458e-06
Iter: 923 loss: 1.32274056e-06
Iter: 924 loss: 1.32260743e-06
Iter: 925 loss: 1.32208777e-06
Iter: 926 loss: 1.32137211e-06
Iter: 927 loss: 1.32131561e-06
Iter: 928 loss: 1.32073023e-06
Iter: 929 loss: 1.32042715e-06
Iter: 930 loss: 1.3201352e-06
Iter: 931 loss: 1.31981483e-06
Iter: 932 loss: 1.31898287e-06
Iter: 933 loss: 1.32633079e-06
Iter: 934 loss: 1.31877948e-06
Iter: 935 loss: 1.31785123e-06
Iter: 936 loss: 1.3198794e-06
Iter: 937 loss: 1.31745719e-06
Iter: 938 loss: 1.31678041e-06
Iter: 939 loss: 1.31674972e-06
Iter: 940 loss: 1.31604611e-06
Iter: 941 loss: 1.31681759e-06
Iter: 942 loss: 1.31569641e-06
Iter: 943 loss: 1.31511229e-06
Iter: 944 loss: 1.31498678e-06
Iter: 945 loss: 1.31455567e-06
Iter: 946 loss: 1.31360878e-06
Iter: 947 loss: 1.31878721e-06
Iter: 948 loss: 1.3134636e-06
Iter: 949 loss: 1.31275408e-06
Iter: 950 loss: 1.31523666e-06
Iter: 951 loss: 1.31248987e-06
Iter: 952 loss: 1.31174795e-06
Iter: 953 loss: 1.31239756e-06
Iter: 954 loss: 1.31132742e-06
Iter: 955 loss: 1.31045476e-06
Iter: 956 loss: 1.32091623e-06
Iter: 957 loss: 1.31041168e-06
Iter: 958 loss: 1.30997773e-06
Iter: 959 loss: 1.30909655e-06
Iter: 960 loss: 1.32516789e-06
Iter: 961 loss: 1.30903663e-06
Iter: 962 loss: 1.30839351e-06
Iter: 963 loss: 1.31988259e-06
Iter: 964 loss: 1.30839476e-06
Iter: 965 loss: 1.30773287e-06
Iter: 966 loss: 1.31172885e-06
Iter: 967 loss: 1.30758269e-06
Iter: 968 loss: 1.3071791e-06
Iter: 969 loss: 1.3073161e-06
Iter: 970 loss: 1.30685316e-06
Iter: 971 loss: 1.30611602e-06
Iter: 972 loss: 1.30757269e-06
Iter: 973 loss: 1.30586341e-06
Iter: 974 loss: 1.30528895e-06
Iter: 975 loss: 1.30432545e-06
Iter: 976 loss: 1.30434671e-06
Iter: 977 loss: 1.30321951e-06
Iter: 978 loss: 1.30917215e-06
Iter: 979 loss: 1.30303692e-06
Iter: 980 loss: 1.30209753e-06
Iter: 981 loss: 1.30717308e-06
Iter: 982 loss: 1.30196588e-06
Iter: 983 loss: 1.30107651e-06
Iter: 984 loss: 1.30797332e-06
Iter: 985 loss: 1.30107946e-06
Iter: 986 loss: 1.30056833e-06
Iter: 987 loss: 1.30003377e-06
Iter: 988 loss: 1.29996693e-06
Iter: 989 loss: 1.29914918e-06
Iter: 990 loss: 1.30657259e-06
Iter: 991 loss: 1.29911291e-06
Iter: 992 loss: 1.29849e-06
Iter: 993 loss: 1.29815567e-06
Iter: 994 loss: 1.29784917e-06
Iter: 995 loss: 1.29729665e-06
Iter: 996 loss: 1.29723935e-06
Iter: 997 loss: 1.29689738e-06
Iter: 998 loss: 1.29607486e-06
Iter: 999 loss: 1.30390299e-06
Iter: 1000 loss: 1.29592559e-06
Iter: 1001 loss: 1.29518219e-06
Iter: 1002 loss: 1.29517946e-06
Iter: 1003 loss: 1.29454725e-06
Iter: 1004 loss: 1.2978262e-06
Iter: 1005 loss: 1.29440468e-06
Iter: 1006 loss: 1.29388047e-06
Iter: 1007 loss: 1.2947321e-06
Iter: 1008 loss: 1.29371256e-06
Iter: 1009 loss: 1.29313332e-06
Iter: 1010 loss: 1.29231682e-06
Iter: 1011 loss: 1.29222099e-06
Iter: 1012 loss: 1.29153329e-06
Iter: 1013 loss: 1.29665034e-06
Iter: 1014 loss: 1.29146042e-06
Iter: 1015 loss: 1.29084754e-06
Iter: 1016 loss: 1.29314162e-06
Iter: 1017 loss: 1.29071896e-06
Iter: 1018 loss: 1.28991132e-06
Iter: 1019 loss: 1.2908381e-06
Iter: 1020 loss: 1.28942293e-06
Iter: 1021 loss: 1.28871739e-06
Iter: 1022 loss: 1.28924603e-06
Iter: 1023 loss: 1.28822944e-06
Iter: 1024 loss: 1.28734132e-06
Iter: 1025 loss: 1.29010323e-06
Iter: 1026 loss: 1.28703778e-06
Iter: 1027 loss: 1.28604097e-06
Iter: 1028 loss: 1.29220837e-06
Iter: 1029 loss: 1.28591887e-06
Iter: 1030 loss: 1.28540159e-06
Iter: 1031 loss: 1.29048931e-06
Iter: 1032 loss: 1.28540159e-06
Iter: 1033 loss: 1.28489933e-06
Iter: 1034 loss: 1.28443344e-06
Iter: 1035 loss: 1.28437819e-06
Iter: 1036 loss: 1.28360421e-06
Iter: 1037 loss: 1.28653755e-06
Iter: 1038 loss: 1.28342231e-06
Iter: 1039 loss: 1.28243391e-06
Iter: 1040 loss: 1.28801048e-06
Iter: 1041 loss: 1.28231e-06
Iter: 1042 loss: 1.28190675e-06
Iter: 1043 loss: 1.28229567e-06
Iter: 1044 loss: 1.28159922e-06
Iter: 1045 loss: 1.28097167e-06
Iter: 1046 loss: 1.28231659e-06
Iter: 1047 loss: 1.28071565e-06
Iter: 1048 loss: 1.28013914e-06
Iter: 1049 loss: 1.27925318e-06
Iter: 1050 loss: 1.27926887e-06
Iter: 1051 loss: 1.27818771e-06
Iter: 1052 loss: 1.28580427e-06
Iter: 1053 loss: 1.27809017e-06
Iter: 1054 loss: 1.27753651e-06
Iter: 1055 loss: 1.28612737e-06
Iter: 1056 loss: 1.27752628e-06
Iter: 1057 loss: 1.27694807e-06
Iter: 1058 loss: 1.27594967e-06
Iter: 1059 loss: 1.3005457e-06
Iter: 1060 loss: 1.27597036e-06
Iter: 1061 loss: 1.27517501e-06
Iter: 1062 loss: 1.28367424e-06
Iter: 1063 loss: 1.27514329e-06
Iter: 1064 loss: 1.27459941e-06
Iter: 1065 loss: 1.2782059e-06
Iter: 1066 loss: 1.27457133e-06
Iter: 1067 loss: 1.27401131e-06
Iter: 1068 loss: 1.27446322e-06
Iter: 1069 loss: 1.27371891e-06
Iter: 1070 loss: 1.27300154e-06
Iter: 1071 loss: 1.27429621e-06
Iter: 1072 loss: 1.2726689e-06
Iter: 1073 loss: 1.27221551e-06
Iter: 1074 loss: 1.27734756e-06
Iter: 1075 loss: 1.27219641e-06
Iter: 1076 loss: 1.27163344e-06
Iter: 1077 loss: 1.27146234e-06
Iter: 1078 loss: 1.27108115e-06
Iter: 1079 loss: 1.27055046e-06
Iter: 1080 loss: 1.27310807e-06
Iter: 1081 loss: 1.27048e-06
Iter: 1082 loss: 1.26980126e-06
Iter: 1083 loss: 1.26966438e-06
Iter: 1084 loss: 1.26922828e-06
Iter: 1085 loss: 1.26850705e-06
Iter: 1086 loss: 1.2697368e-06
Iter: 1087 loss: 1.26817872e-06
Iter: 1088 loss: 1.2673529e-06
Iter: 1089 loss: 1.26692919e-06
Iter: 1090 loss: 1.26652992e-06
Iter: 1091 loss: 1.26634927e-06
Iter: 1092 loss: 1.26603265e-06
Iter: 1093 loss: 1.26557188e-06
Iter: 1094 loss: 1.26476425e-06
Iter: 1095 loss: 1.28300985e-06
Iter: 1096 loss: 1.2647572e-06
Iter: 1097 loss: 1.26387306e-06
Iter: 1098 loss: 1.26610644e-06
Iter: 1099 loss: 1.26353007e-06
Iter: 1100 loss: 1.26285727e-06
Iter: 1101 loss: 1.27347766e-06
Iter: 1102 loss: 1.26284772e-06
Iter: 1103 loss: 1.26220766e-06
Iter: 1104 loss: 1.26357929e-06
Iter: 1105 loss: 1.26204623e-06
Iter: 1106 loss: 1.26143698e-06
Iter: 1107 loss: 1.26200598e-06
Iter: 1108 loss: 1.26116936e-06
Iter: 1109 loss: 1.26049463e-06
Iter: 1110 loss: 1.26375608e-06
Iter: 1111 loss: 1.26040504e-06
Iter: 1112 loss: 1.25976806e-06
Iter: 1113 loss: 1.26440102e-06
Iter: 1114 loss: 1.25971428e-06
Iter: 1115 loss: 1.25940983e-06
Iter: 1116 loss: 1.25880047e-06
Iter: 1117 loss: 1.2588057e-06
Iter: 1118 loss: 1.25818394e-06
Iter: 1119 loss: 1.2660862e-06
Iter: 1120 loss: 1.25816814e-06
Iter: 1121 loss: 1.25769145e-06
Iter: 1122 loss: 1.2570124e-06
Iter: 1123 loss: 1.25700319e-06
Iter: 1124 loss: 1.25627309e-06
Iter: 1125 loss: 1.25745555e-06
Iter: 1126 loss: 1.25588565e-06
Iter: 1127 loss: 1.25503379e-06
Iter: 1128 loss: 1.25685767e-06
Iter: 1129 loss: 1.25472968e-06
Iter: 1130 loss: 1.25392478e-06
Iter: 1131 loss: 1.2604113e-06
Iter: 1132 loss: 1.25391557e-06
Iter: 1133 loss: 1.25311306e-06
Iter: 1134 loss: 1.25664087e-06
Iter: 1135 loss: 1.25300289e-06
Iter: 1136 loss: 1.2524024e-06
Iter: 1137 loss: 1.25244753e-06
Iter: 1138 loss: 1.25195584e-06
Iter: 1139 loss: 1.25146471e-06
Iter: 1140 loss: 1.25972065e-06
Iter: 1141 loss: 1.25145664e-06
Iter: 1142 loss: 1.25092788e-06
Iter: 1143 loss: 1.25024485e-06
Iter: 1144 loss: 1.25017698e-06
Iter: 1145 loss: 1.24956148e-06
Iter: 1146 loss: 1.25759129e-06
Iter: 1147 loss: 1.24958717e-06
Iter: 1148 loss: 1.24916642e-06
Iter: 1149 loss: 1.25141037e-06
Iter: 1150 loss: 1.2491098e-06
Iter: 1151 loss: 1.24869916e-06
Iter: 1152 loss: 1.24833127e-06
Iter: 1153 loss: 1.24814301e-06
Iter: 1154 loss: 1.24743542e-06
Iter: 1155 loss: 1.24885867e-06
Iter: 1156 loss: 1.24719065e-06
Iter: 1157 loss: 1.24660801e-06
Iter: 1158 loss: 1.25186887e-06
Iter: 1159 loss: 1.24657186e-06
Iter: 1160 loss: 1.2460456e-06
Iter: 1161 loss: 1.24540509e-06
Iter: 1162 loss: 1.2453736e-06
Iter: 1163 loss: 1.24453061e-06
Iter: 1164 loss: 1.24500093e-06
Iter: 1165 loss: 1.24404221e-06
Iter: 1166 loss: 1.24308e-06
Iter: 1167 loss: 1.24767416e-06
Iter: 1168 loss: 1.24289465e-06
Iter: 1169 loss: 1.24196038e-06
Iter: 1170 loss: 1.24606572e-06
Iter: 1171 loss: 1.24185942e-06
Iter: 1172 loss: 1.24117059e-06
Iter: 1173 loss: 1.25120278e-06
Iter: 1174 loss: 1.24120118e-06
Iter: 1175 loss: 1.24071971e-06
Iter: 1176 loss: 1.2399812e-06
Iter: 1177 loss: 1.25645022e-06
Iter: 1178 loss: 1.23991708e-06
Iter: 1179 loss: 1.23957443e-06
Iter: 1180 loss: 1.23945847e-06
Iter: 1181 loss: 1.2390791e-06
Iter: 1182 loss: 1.23870188e-06
Iter: 1183 loss: 1.2386563e-06
Iter: 1184 loss: 1.23798168e-06
Iter: 1185 loss: 1.23949394e-06
Iter: 1186 loss: 1.2377692e-06
Iter: 1187 loss: 1.23718314e-06
Iter: 1188 loss: 1.2395692e-06
Iter: 1189 loss: 1.23702159e-06
Iter: 1190 loss: 1.23651455e-06
Iter: 1191 loss: 1.24183975e-06
Iter: 1192 loss: 1.23649033e-06
Iter: 1193 loss: 1.23601433e-06
Iter: 1194 loss: 1.23595828e-06
Iter: 1195 loss: 1.23563791e-06
Iter: 1196 loss: 1.23519669e-06
Iter: 1197 loss: 1.23916163e-06
Iter: 1198 loss: 1.23517339e-06
Iter: 1199 loss: 1.23477605e-06
Iter: 1200 loss: 1.23413156e-06
Iter: 1201 loss: 1.23413895e-06
Iter: 1202 loss: 1.23338327e-06
Iter: 1203 loss: 1.23326208e-06
Iter: 1204 loss: 1.2328203e-06
Iter: 1205 loss: 1.23188227e-06
Iter: 1206 loss: 1.23592463e-06
Iter: 1207 loss: 1.23164511e-06
Iter: 1208 loss: 1.23092786e-06
Iter: 1209 loss: 1.23715949e-06
Iter: 1210 loss: 1.23079712e-06
Iter: 1211 loss: 1.23016378e-06
Iter: 1212 loss: 1.23628365e-06
Iter: 1213 loss: 1.23014229e-06
Iter: 1214 loss: 1.2298251e-06
Iter: 1215 loss: 1.2290003e-06
Iter: 1216 loss: 1.24391272e-06
Iter: 1217 loss: 1.22896245e-06
Iter: 1218 loss: 1.22831284e-06
Iter: 1219 loss: 1.22827873e-06
Iter: 1220 loss: 1.22775725e-06
Iter: 1221 loss: 1.22995436e-06
Iter: 1222 loss: 1.22766016e-06
Iter: 1223 loss: 1.2272576e-06
Iter: 1224 loss: 1.22677807e-06
Iter: 1225 loss: 1.22667655e-06
Iter: 1226 loss: 1.22589404e-06
Iter: 1227 loss: 1.2300668e-06
Iter: 1228 loss: 1.22572965e-06
Iter: 1229 loss: 1.22529173e-06
Iter: 1230 loss: 1.2253189e-06
Iter: 1231 loss: 1.22489257e-06
Iter: 1232 loss: 1.22422261e-06
Iter: 1233 loss: 1.23972518e-06
Iter: 1234 loss: 1.2241984e-06
Iter: 1235 loss: 1.22353822e-06
Iter: 1236 loss: 1.23026871e-06
Iter: 1237 loss: 1.22349e-06
Iter: 1238 loss: 1.22301958e-06
Iter: 1239 loss: 1.22339191e-06
Iter: 1240 loss: 1.22265453e-06
Iter: 1241 loss: 1.22205643e-06
Iter: 1242 loss: 1.22197707e-06
Iter: 1243 loss: 1.22151528e-06
Iter: 1244 loss: 1.22084293e-06
Iter: 1245 loss: 1.22128949e-06
Iter: 1246 loss: 1.22041126e-06
Iter: 1247 loss: 1.21948642e-06
Iter: 1248 loss: 1.22481697e-06
Iter: 1249 loss: 1.2193791e-06
Iter: 1250 loss: 1.21864809e-06
Iter: 1251 loss: 1.22124493e-06
Iter: 1252 loss: 1.21850849e-06
Iter: 1253 loss: 1.21774292e-06
Iter: 1254 loss: 1.22514371e-06
Iter: 1255 loss: 1.21778385e-06
Iter: 1256 loss: 1.21724304e-06
Iter: 1257 loss: 1.21686071e-06
Iter: 1258 loss: 1.21670553e-06
Iter: 1259 loss: 1.21607854e-06
Iter: 1260 loss: 1.22409097e-06
Iter: 1261 loss: 1.21609219e-06
Iter: 1262 loss: 1.21571418e-06
Iter: 1263 loss: 1.21566268e-06
Iter: 1264 loss: 1.21532184e-06
Iter: 1265 loss: 1.21470521e-06
Iter: 1266 loss: 1.21904282e-06
Iter: 1267 loss: 1.21459141e-06
Iter: 1268 loss: 1.21421635e-06
Iter: 1269 loss: 1.21516564e-06
Iter: 1270 loss: 1.21400626e-06
Iter: 1271 loss: 1.21352718e-06
Iter: 1272 loss: 1.2129733e-06
Iter: 1273 loss: 1.21297603e-06
Iter: 1274 loss: 1.21212474e-06
Iter: 1275 loss: 1.2151113e-06
Iter: 1276 loss: 1.21196899e-06
Iter: 1277 loss: 1.21140852e-06
Iter: 1278 loss: 1.21635969e-06
Iter: 1279 loss: 1.21134883e-06
Iter: 1280 loss: 1.21082326e-06
Iter: 1281 loss: 1.21076482e-06
Iter: 1282 loss: 1.21034418e-06
Iter: 1283 loss: 1.20979826e-06
Iter: 1284 loss: 1.20985749e-06
Iter: 1285 loss: 1.20937489e-06
Iter: 1286 loss: 1.20850245e-06
Iter: 1287 loss: 1.20983964e-06
Iter: 1288 loss: 1.20806885e-06
Iter: 1289 loss: 1.20718664e-06
Iter: 1290 loss: 1.20962318e-06
Iter: 1291 loss: 1.20694767e-06
Iter: 1292 loss: 1.20676646e-06
Iter: 1293 loss: 1.20656091e-06
Iter: 1294 loss: 1.20622451e-06
Iter: 1295 loss: 1.20549453e-06
Iter: 1296 loss: 1.21617404e-06
Iter: 1297 loss: 1.20546974e-06
Iter: 1298 loss: 1.20485356e-06
Iter: 1299 loss: 1.21466837e-06
Iter: 1300 loss: 1.20488619e-06
Iter: 1301 loss: 1.2043663e-06
Iter: 1302 loss: 1.20670256e-06
Iter: 1303 loss: 1.20427467e-06
Iter: 1304 loss: 1.20386892e-06
Iter: 1305 loss: 1.2033687e-06
Iter: 1306 loss: 1.20329241e-06
Iter: 1307 loss: 1.20268419e-06
Iter: 1308 loss: 1.20484901e-06
Iter: 1309 loss: 1.20253844e-06
Iter: 1310 loss: 1.20201844e-06
Iter: 1311 loss: 1.20571053e-06
Iter: 1312 loss: 1.20195546e-06
Iter: 1313 loss: 1.2014724e-06
Iter: 1314 loss: 1.20100367e-06
Iter: 1315 loss: 1.20092238e-06
Iter: 1316 loss: 1.20020161e-06
Iter: 1317 loss: 1.20105688e-06
Iter: 1318 loss: 1.19982963e-06
Iter: 1319 loss: 1.19921e-06
Iter: 1320 loss: 1.19920037e-06
Iter: 1321 loss: 1.19865854e-06
Iter: 1322 loss: 1.19883066e-06
Iter: 1323 loss: 1.19826927e-06
Iter: 1324 loss: 1.19765082e-06
Iter: 1325 loss: 1.19731396e-06
Iter: 1326 loss: 1.19700587e-06
Iter: 1327 loss: 1.19624133e-06
Iter: 1328 loss: 1.20214099e-06
Iter: 1329 loss: 1.1961281e-06
Iter: 1330 loss: 1.19543142e-06
Iter: 1331 loss: 1.20353e-06
Iter: 1332 loss: 1.19544961e-06
Iter: 1333 loss: 1.19502e-06
Iter: 1334 loss: 1.19455831e-06
Iter: 1335 loss: 1.19452693e-06
Iter: 1336 loss: 1.19400875e-06
Iter: 1337 loss: 1.19398487e-06
Iter: 1338 loss: 1.19362278e-06
Iter: 1339 loss: 1.1935208e-06
Iter: 1340 loss: 1.19334152e-06
Iter: 1341 loss: 1.19286346e-06
Iter: 1342 loss: 1.1940167e-06
Iter: 1343 loss: 1.19274932e-06
Iter: 1344 loss: 1.19228559e-06
Iter: 1345 loss: 1.19183323e-06
Iter: 1346 loss: 1.19173455e-06
Iter: 1347 loss: 1.19102845e-06
Iter: 1348 loss: 1.20081768e-06
Iter: 1349 loss: 1.19099127e-06
Iter: 1350 loss: 1.19056506e-06
Iter: 1351 loss: 1.19014771e-06
Iter: 1352 loss: 1.19008314e-06
Iter: 1353 loss: 1.18937305e-06
Iter: 1354 loss: 1.19306742e-06
Iter: 1355 loss: 1.18934668e-06
Iter: 1356 loss: 1.18869252e-06
Iter: 1357 loss: 1.19274159e-06
Iter: 1358 loss: 1.18868445e-06
Iter: 1359 loss: 1.18824335e-06
Iter: 1360 loss: 1.18745334e-06
Iter: 1361 loss: 1.20555364e-06
Iter: 1362 loss: 1.18749847e-06
Iter: 1363 loss: 1.18671323e-06
Iter: 1364 loss: 1.19050139e-06
Iter: 1365 loss: 1.18655396e-06
Iter: 1366 loss: 1.18593709e-06
Iter: 1367 loss: 1.18594835e-06
Iter: 1368 loss: 1.18547405e-06
Iter: 1369 loss: 1.18557955e-06
Iter: 1370 loss: 1.18510525e-06
Iter: 1371 loss: 1.18472053e-06
Iter: 1372 loss: 1.1884249e-06
Iter: 1373 loss: 1.18471962e-06
Iter: 1374 loss: 1.18415517e-06
Iter: 1375 loss: 1.18394314e-06
Iter: 1376 loss: 1.1836853e-06
Iter: 1377 loss: 1.18318803e-06
Iter: 1378 loss: 1.18551384e-06
Iter: 1379 loss: 1.1830756e-06
Iter: 1380 loss: 1.18259607e-06
Iter: 1381 loss: 1.18215439e-06
Iter: 1382 loss: 1.18204775e-06
Iter: 1383 loss: 1.18147227e-06
Iter: 1384 loss: 1.18888943e-06
Iter: 1385 loss: 1.1814459e-06
Iter: 1386 loss: 1.180975e-06
Iter: 1387 loss: 1.18110177e-06
Iter: 1388 loss: 1.18059518e-06
Iter: 1389 loss: 1.18007415e-06
Iter: 1390 loss: 1.18095386e-06
Iter: 1391 loss: 1.17980721e-06
Iter: 1392 loss: 1.1793494e-06
Iter: 1393 loss: 1.18453545e-06
Iter: 1394 loss: 1.17931961e-06
Iter: 1395 loss: 1.17883656e-06
Iter: 1396 loss: 1.17845707e-06
Iter: 1397 loss: 1.17825243e-06
Iter: 1398 loss: 1.17766876e-06
Iter: 1399 loss: 1.17835123e-06
Iter: 1400 loss: 1.17736965e-06
Iter: 1401 loss: 1.17667412e-06
Iter: 1402 loss: 1.1780478e-06
Iter: 1403 loss: 1.17637228e-06
Iter: 1404 loss: 1.17598177e-06
Iter: 1405 loss: 1.17590707e-06
Iter: 1406 loss: 1.17550201e-06
Iter: 1407 loss: 1.17534637e-06
Iter: 1408 loss: 1.1751082e-06
Iter: 1409 loss: 1.1746539e-06
Iter: 1410 loss: 1.18099467e-06
Iter: 1411 loss: 1.1746497e-06
Iter: 1412 loss: 1.17431819e-06
Iter: 1413 loss: 1.17361549e-06
Iter: 1414 loss: 1.18333458e-06
Iter: 1415 loss: 1.17354602e-06
Iter: 1416 loss: 1.17289778e-06
Iter: 1417 loss: 1.18010223e-06
Iter: 1418 loss: 1.17286618e-06
Iter: 1419 loss: 1.17241e-06
Iter: 1420 loss: 1.17301715e-06
Iter: 1421 loss: 1.17211403e-06
Iter: 1422 loss: 1.1715465e-06
Iter: 1423 loss: 1.17288653e-06
Iter: 1424 loss: 1.17126456e-06
Iter: 1425 loss: 1.1705929e-06
Iter: 1426 loss: 1.17403545e-06
Iter: 1427 loss: 1.17052025e-06
Iter: 1428 loss: 1.17004311e-06
Iter: 1429 loss: 1.16917431e-06
Iter: 1430 loss: 1.1691352e-06
Iter: 1431 loss: 1.16894853e-06
Iter: 1432 loss: 1.16874378e-06
Iter: 1433 loss: 1.16835088e-06
Iter: 1434 loss: 1.16781393e-06
Iter: 1435 loss: 1.16773572e-06
Iter: 1436 loss: 1.16715444e-06
Iter: 1437 loss: 1.1684325e-06
Iter: 1438 loss: 1.16683702e-06
Iter: 1439 loss: 1.1663958e-06
Iter: 1440 loss: 1.17361446e-06
Iter: 1441 loss: 1.16641786e-06
Iter: 1442 loss: 1.16583885e-06
Iter: 1443 loss: 1.16585045e-06
Iter: 1444 loss: 1.16543981e-06
Iter: 1445 loss: 1.16502576e-06
Iter: 1446 loss: 1.16498779e-06
Iter: 1447 loss: 1.16469141e-06
Iter: 1448 loss: 1.16406045e-06
Iter: 1449 loss: 1.17388299e-06
Iter: 1450 loss: 1.16401077e-06
Iter: 1451 loss: 1.16334377e-06
Iter: 1452 loss: 1.16554872e-06
Iter: 1453 loss: 1.16317392e-06
Iter: 1454 loss: 1.16259673e-06
Iter: 1455 loss: 1.16486797e-06
Iter: 1456 loss: 1.16248589e-06
Iter: 1457 loss: 1.16188721e-06
Iter: 1458 loss: 1.16516685e-06
Iter: 1459 loss: 1.16177796e-06
Iter: 1460 loss: 1.16131832e-06
Iter: 1461 loss: 1.16191677e-06
Iter: 1462 loss: 1.16109413e-06
Iter: 1463 loss: 1.16049728e-06
Iter: 1464 loss: 1.16181059e-06
Iter: 1465 loss: 1.16029241e-06
Iter: 1466 loss: 1.15968044e-06
Iter: 1467 loss: 1.15937644e-06
Iter: 1468 loss: 1.15909495e-06
Iter: 1469 loss: 1.15841294e-06
Iter: 1470 loss: 1.16336741e-06
Iter: 1471 loss: 1.15839e-06
Iter: 1472 loss: 1.15774333e-06
Iter: 1473 loss: 1.15920102e-06
Iter: 1474 loss: 1.15759678e-06
Iter: 1475 loss: 1.15696844e-06
Iter: 1476 loss: 1.16244632e-06
Iter: 1477 loss: 1.15691034e-06
Iter: 1478 loss: 1.15660168e-06
Iter: 1479 loss: 1.15636192e-06
Iter: 1480 loss: 1.15624675e-06
Iter: 1481 loss: 1.15564455e-06
Iter: 1482 loss: 1.16004571e-06
Iter: 1483 loss: 1.1555868e-06
Iter: 1484 loss: 1.15514376e-06
Iter: 1485 loss: 1.15464127e-06
Iter: 1486 loss: 1.15465309e-06
Iter: 1487 loss: 1.15418618e-06
Iter: 1488 loss: 1.15413604e-06
Iter: 1489 loss: 1.15381522e-06
Iter: 1490 loss: 1.15327839e-06
Iter: 1491 loss: 1.15327464e-06
Iter: 1492 loss: 1.15260309e-06
Iter: 1493 loss: 1.15330545e-06
Iter: 1494 loss: 1.15228704e-06
Iter: 1495 loss: 1.15161265e-06
Iter: 1496 loss: 1.16099454e-06
Iter: 1497 loss: 1.15163743e-06
Iter: 1498 loss: 1.15114881e-06
Iter: 1499 loss: 1.15127114e-06
Iter: 1500 loss: 1.15078376e-06
Iter: 1501 loss: 1.15023749e-06
Iter: 1502 loss: 1.1516313e-06
Iter: 1503 loss: 1.1500864e-06
Iter: 1504 loss: 1.14946772e-06
Iter: 1505 loss: 1.15211628e-06
Iter: 1506 loss: 1.14937097e-06
Iter: 1507 loss: 1.14889872e-06
Iter: 1508 loss: 1.14843942e-06
Iter: 1509 loss: 1.14835598e-06
Iter: 1510 loss: 1.14791897e-06
Iter: 1511 loss: 1.14784e-06
Iter: 1512 loss: 1.14752447e-06
Iter: 1513 loss: 1.14710656e-06
Iter: 1514 loss: 1.14707041e-06
Iter: 1515 loss: 1.14661509e-06
Iter: 1516 loss: 1.14657757e-06
Iter: 1517 loss: 1.14632485e-06
Iter: 1518 loss: 1.14569957e-06
Iter: 1519 loss: 1.15755643e-06
Iter: 1520 loss: 1.14571594e-06
Iter: 1521 loss: 1.14519537e-06
Iter: 1522 loss: 1.14921727e-06
Iter: 1523 loss: 1.14517093e-06
Iter: 1524 loss: 1.14455383e-06
Iter: 1525 loss: 1.14498482e-06
Iter: 1526 loss: 1.1441341e-06
Iter: 1527 loss: 1.14362854e-06
Iter: 1528 loss: 1.14306454e-06
Iter: 1529 loss: 1.14302691e-06
Iter: 1530 loss: 1.14255567e-06
Iter: 1531 loss: 1.14245609e-06
Iter: 1532 loss: 1.1419927e-06
Iter: 1533 loss: 1.14352224e-06
Iter: 1534 loss: 1.14185264e-06
Iter: 1535 loss: 1.14145666e-06
Iter: 1536 loss: 1.14115426e-06
Iter: 1537 loss: 1.14103682e-06
Iter: 1538 loss: 1.14038437e-06
Iter: 1539 loss: 1.14367504e-06
Iter: 1540 loss: 1.14024874e-06
Iter: 1541 loss: 1.13961016e-06
Iter: 1542 loss: 1.1403971e-06
Iter: 1543 loss: 1.13927877e-06
Iter: 1544 loss: 1.13878491e-06
Iter: 1545 loss: 1.13878957e-06
Iter: 1546 loss: 1.13834676e-06
Iter: 1547 loss: 1.13827446e-06
Iter: 1548 loss: 1.13802082e-06
Iter: 1549 loss: 1.13740862e-06
Iter: 1550 loss: 1.14359636e-06
Iter: 1551 loss: 1.13741737e-06
Iter: 1552 loss: 1.1371784e-06
Iter: 1553 loss: 1.13651652e-06
Iter: 1554 loss: 1.1433815e-06
Iter: 1555 loss: 1.1364416e-06
Iter: 1556 loss: 1.13572935e-06
Iter: 1557 loss: 1.1411928e-06
Iter: 1558 loss: 1.13574185e-06
Iter: 1559 loss: 1.13519798e-06
Iter: 1560 loss: 1.13987778e-06
Iter: 1561 loss: 1.13511692e-06
Iter: 1562 loss: 1.13478814e-06
Iter: 1563 loss: 1.13397118e-06
Iter: 1564 loss: 1.14537897e-06
Iter: 1565 loss: 1.13394822e-06
Iter: 1566 loss: 1.13325359e-06
Iter: 1567 loss: 1.13765782e-06
Iter: 1568 loss: 1.13316378e-06
Iter: 1569 loss: 1.1325692e-06
Iter: 1570 loss: 1.13713577e-06
Iter: 1571 loss: 1.13250724e-06
Iter: 1572 loss: 1.1319529e-06
Iter: 1573 loss: 1.13278384e-06
Iter: 1574 loss: 1.13169915e-06
Iter: 1575 loss: 1.13116471e-06
Iter: 1576 loss: 1.13057172e-06
Iter: 1577 loss: 1.13048418e-06
Iter: 1578 loss: 1.12975931e-06
Iter: 1579 loss: 1.13549481e-06
Iter: 1580 loss: 1.1296836e-06
Iter: 1581 loss: 1.1290756e-06
Iter: 1582 loss: 1.1354872e-06
Iter: 1583 loss: 1.12909197e-06
Iter: 1584 loss: 1.12852342e-06
Iter: 1585 loss: 1.12941188e-06
Iter: 1586 loss: 1.12824489e-06
Iter: 1587 loss: 1.12788837e-06
Iter: 1588 loss: 1.13337023e-06
Iter: 1589 loss: 1.12791292e-06
Iter: 1590 loss: 1.12758812e-06
Iter: 1591 loss: 1.12671455e-06
Iter: 1592 loss: 1.13046019e-06
Iter: 1593 loss: 1.12642886e-06
Iter: 1594 loss: 1.12591124e-06
Iter: 1595 loss: 1.1258926e-06
Iter: 1596 loss: 1.12548912e-06
Iter: 1597 loss: 1.12693192e-06
Iter: 1598 loss: 1.12534508e-06
Iter: 1599 loss: 1.12485804e-06
Iter: 1600 loss: 1.12614521e-06
Iter: 1601 loss: 1.12475777e-06
Iter: 1602 loss: 1.12430644e-06
Iter: 1603 loss: 1.12440125e-06
Iter: 1604 loss: 1.12396151e-06
Iter: 1605 loss: 1.12337636e-06
Iter: 1606 loss: 1.12385442e-06
Iter: 1607 loss: 1.12302143e-06
Iter: 1608 loss: 1.1224945e-06
Iter: 1609 loss: 1.13085412e-06
Iter: 1610 loss: 1.12248904e-06
Iter: 1611 loss: 1.12200621e-06
Iter: 1612 loss: 1.12124042e-06
Iter: 1613 loss: 1.12122007e-06
Iter: 1614 loss: 1.12055034e-06
Iter: 1615 loss: 1.12144164e-06
Iter: 1616 loss: 1.12026635e-06
Iter: 1617 loss: 1.11994495e-06
Iter: 1618 loss: 1.11981785e-06
Iter: 1619 loss: 1.11947816e-06
Iter: 1620 loss: 1.11945803e-06
Iter: 1621 loss: 1.11916643e-06
Iter: 1622 loss: 1.11873601e-06
Iter: 1623 loss: 1.12298687e-06
Iter: 1624 loss: 1.11876409e-06
Iter: 1625 loss: 1.11840711e-06
Iter: 1626 loss: 1.11767281e-06
Iter: 1627 loss: 1.12643886e-06
Iter: 1628 loss: 1.11760983e-06
Iter: 1629 loss: 1.11695692e-06
Iter: 1630 loss: 1.11983832e-06
Iter: 1631 loss: 1.11682516e-06
Iter: 1632 loss: 1.11631016e-06
Iter: 1633 loss: 1.12328735e-06
Iter: 1634 loss: 1.11626423e-06
Iter: 1635 loss: 1.11583859e-06
Iter: 1636 loss: 1.11563907e-06
Iter: 1637 loss: 1.11552129e-06
Iter: 1638 loss: 1.11487691e-06
Iter: 1639 loss: 1.1173114e-06
Iter: 1640 loss: 1.1147099e-06
Iter: 1641 loss: 1.11417455e-06
Iter: 1642 loss: 1.11442932e-06
Iter: 1643 loss: 1.11380473e-06
Iter: 1644 loss: 1.11316945e-06
Iter: 1645 loss: 1.11741679e-06
Iter: 1646 loss: 1.11309396e-06
Iter: 1647 loss: 1.11241366e-06
Iter: 1648 loss: 1.11321083e-06
Iter: 1649 loss: 1.11212523e-06
Iter: 1650 loss: 1.11155612e-06
Iter: 1651 loss: 1.11141196e-06
Iter: 1652 loss: 1.11108534e-06
Iter: 1653 loss: 1.11052123e-06
Iter: 1654 loss: 1.1105376e-06
Iter: 1655 loss: 1.10999133e-06
Iter: 1656 loss: 1.11054032e-06
Iter: 1657 loss: 1.10966107e-06
Iter: 1658 loss: 1.10920416e-06
Iter: 1659 loss: 1.11229303e-06
Iter: 1660 loss: 1.10912606e-06
Iter: 1661 loss: 1.10871042e-06
Iter: 1662 loss: 1.10875692e-06
Iter: 1663 loss: 1.10834958e-06
Iter: 1664 loss: 1.10784367e-06
Iter: 1665 loss: 1.10697783e-06
Iter: 1666 loss: 1.12661371e-06
Iter: 1667 loss: 1.10699625e-06
Iter: 1668 loss: 1.10734027e-06
Iter: 1669 loss: 1.10660233e-06
Iter: 1670 loss: 1.10637529e-06
Iter: 1671 loss: 1.10573376e-06
Iter: 1672 loss: 1.11209465e-06
Iter: 1673 loss: 1.10573546e-06
Iter: 1674 loss: 1.1051186e-06
Iter: 1675 loss: 1.11269469e-06
Iter: 1676 loss: 1.10511166e-06
Iter: 1677 loss: 1.10475207e-06
Iter: 1678 loss: 1.10486133e-06
Iter: 1679 loss: 1.10439669e-06
Iter: 1680 loss: 1.10387577e-06
Iter: 1681 loss: 1.10682413e-06
Iter: 1682 loss: 1.10377107e-06
Iter: 1683 loss: 1.10327665e-06
Iter: 1684 loss: 1.10409974e-06
Iter: 1685 loss: 1.10305723e-06
Iter: 1686 loss: 1.10255019e-06
Iter: 1687 loss: 1.10302449e-06
Iter: 1688 loss: 1.10227927e-06
Iter: 1689 loss: 1.10173369e-06
Iter: 1690 loss: 1.10175552e-06
Iter: 1691 loss: 1.10140707e-06
Iter: 1692 loss: 1.10141173e-06
Iter: 1693 loss: 1.10118049e-06
Iter: 1694 loss: 1.10072256e-06
Iter: 1695 loss: 1.1025902e-06
Iter: 1696 loss: 1.10061865e-06
Iter: 1697 loss: 1.1002453e-06
Iter: 1698 loss: 1.09958637e-06
Iter: 1699 loss: 1.09954567e-06
Iter: 1700 loss: 1.09900543e-06
Iter: 1701 loss: 1.10506744e-06
Iter: 1702 loss: 1.09904659e-06
Iter: 1703 loss: 1.09849293e-06
Iter: 1704 loss: 1.09933342e-06
Iter: 1705 loss: 1.09819598e-06
Iter: 1706 loss: 1.09773714e-06
Iter: 1707 loss: 1.09765e-06
Iter: 1708 loss: 1.09734253e-06
Iter: 1709 loss: 1.09664529e-06
Iter: 1710 loss: 1.10048961e-06
Iter: 1711 loss: 1.0965199e-06
Iter: 1712 loss: 1.09595453e-06
Iter: 1713 loss: 1.09704149e-06
Iter: 1714 loss: 1.09571215e-06
Iter: 1715 loss: 1.09524524e-06
Iter: 1716 loss: 1.09943358e-06
Iter: 1717 loss: 1.09523569e-06
Iter: 1718 loss: 1.09478299e-06
Iter: 1719 loss: 1.09448229e-06
Iter: 1720 loss: 1.09435e-06
Iter: 1721 loss: 1.09388657e-06
Iter: 1722 loss: 1.09390146e-06
Iter: 1723 loss: 1.09354824e-06
Iter: 1724 loss: 1.09378311e-06
Iter: 1725 loss: 1.09334496e-06
Iter: 1726 loss: 1.09301959e-06
Iter: 1727 loss: 1.09462144e-06
Iter: 1728 loss: 1.09298594e-06
Iter: 1729 loss: 1.09260498e-06
Iter: 1730 loss: 1.09241034e-06
Iter: 1731 loss: 1.09225357e-06
Iter: 1732 loss: 1.09183861e-06
Iter: 1733 loss: 1.09255552e-06
Iter: 1734 loss: 1.09158805e-06
Iter: 1735 loss: 1.09126631e-06
Iter: 1736 loss: 1.09656412e-06
Iter: 1737 loss: 1.0912961e-06
Iter: 1738 loss: 1.09098778e-06
Iter: 1739 loss: 1.09027224e-06
Iter: 1740 loss: 1.10150359e-06
Iter: 1741 loss: 1.0903035e-06
Iter: 1742 loss: 1.08983636e-06
Iter: 1743 loss: 1.09624568e-06
Iter: 1744 loss: 1.08982681e-06
Iter: 1745 loss: 1.0893807e-06
Iter: 1746 loss: 1.09003668e-06
Iter: 1747 loss: 1.0891797e-06
Iter: 1748 loss: 1.08872212e-06
Iter: 1749 loss: 1.09021357e-06
Iter: 1750 loss: 1.08862969e-06
Iter: 1751 loss: 1.08812674e-06
Iter: 1752 loss: 1.08963241e-06
Iter: 1753 loss: 1.08807103e-06
Iter: 1754 loss: 1.08758752e-06
Iter: 1755 loss: 1.08792131e-06
Iter: 1756 loss: 1.08727602e-06
Iter: 1757 loss: 1.08692802e-06
Iter: 1758 loss: 1.08691211e-06
Iter: 1759 loss: 1.08668098e-06
Iter: 1760 loss: 1.08613517e-06
Iter: 1761 loss: 1.09221378e-06
Iter: 1762 loss: 1.08606253e-06
Iter: 1763 loss: 1.08550432e-06
Iter: 1764 loss: 1.08549023e-06
Iter: 1765 loss: 1.08518918e-06
Iter: 1766 loss: 1.08494157e-06
Iter: 1767 loss: 1.08483096e-06
Iter: 1768 loss: 1.08441418e-06
Iter: 1769 loss: 1.08718496e-06
Iter: 1770 loss: 1.08443078e-06
Iter: 1771 loss: 1.08391805e-06
Iter: 1772 loss: 1.08415634e-06
Iter: 1773 loss: 1.08366169e-06
Iter: 1774 loss: 1.08330141e-06
Iter: 1775 loss: 1.08326026e-06
Iter: 1776 loss: 1.08295217e-06
Iter: 1777 loss: 1.08248128e-06
Iter: 1778 loss: 1.08845347e-06
Iter: 1779 loss: 1.08247104e-06
Iter: 1780 loss: 1.0821426e-06
Iter: 1781 loss: 1.08223855e-06
Iter: 1782 loss: 1.08183121e-06
Iter: 1783 loss: 1.08128927e-06
Iter: 1784 loss: 1.0849426e-06
Iter: 1785 loss: 1.08130439e-06
Iter: 1786 loss: 1.08084646e-06
Iter: 1787 loss: 1.08159963e-06
Iter: 1788 loss: 1.080679e-06
Iter: 1789 loss: 1.08031111e-06
Iter: 1790 loss: 1.08402719e-06
Iter: 1791 loss: 1.08029985e-06
Iter: 1792 loss: 1.07990309e-06
Iter: 1793 loss: 1.07962808e-06
Iter: 1794 loss: 1.07949495e-06
Iter: 1795 loss: 1.07920118e-06
Iter: 1796 loss: 1.08237555e-06
Iter: 1797 loss: 1.07920891e-06
Iter: 1798 loss: 1.07881851e-06
Iter: 1799 loss: 1.07814628e-06
Iter: 1800 loss: 1.09244456e-06
Iter: 1801 loss: 1.07816e-06
Iter: 1802 loss: 1.07759558e-06
Iter: 1803 loss: 1.08288577e-06
Iter: 1804 loss: 1.07753226e-06
Iter: 1805 loss: 1.07712185e-06
Iter: 1806 loss: 1.08086579e-06
Iter: 1807 loss: 1.07713458e-06
Iter: 1808 loss: 1.07687902e-06
Iter: 1809 loss: 1.07617393e-06
Iter: 1810 loss: 1.0838254e-06
Iter: 1811 loss: 1.07615267e-06
Iter: 1812 loss: 1.07543019e-06
Iter: 1813 loss: 1.07985807e-06
Iter: 1814 loss: 1.07533992e-06
Iter: 1815 loss: 1.07477388e-06
Iter: 1816 loss: 1.07998449e-06
Iter: 1817 loss: 1.07467349e-06
Iter: 1818 loss: 1.0742807e-06
Iter: 1819 loss: 1.07459027e-06
Iter: 1820 loss: 1.07403048e-06
Iter: 1821 loss: 1.07360381e-06
Iter: 1822 loss: 1.07668325e-06
Iter: 1823 loss: 1.07351138e-06
Iter: 1824 loss: 1.07315793e-06
Iter: 1825 loss: 1.07370067e-06
Iter: 1826 loss: 1.07294909e-06
Iter: 1827 loss: 1.07250332e-06
Iter: 1828 loss: 1.07749634e-06
Iter: 1829 loss: 1.0725123e-06
Iter: 1830 loss: 1.07231222e-06
Iter: 1831 loss: 1.07188964e-06
Iter: 1832 loss: 1.0718752e-06
Iter: 1833 loss: 1.07146889e-06
Iter: 1834 loss: 1.07148753e-06
Iter: 1835 loss: 1.07125879e-06
Iter: 1836 loss: 1.07073402e-06
Iter: 1837 loss: 1.08090421e-06
Iter: 1838 loss: 1.0707405e-06
Iter: 1839 loss: 1.07041933e-06
Iter: 1840 loss: 1.07039978e-06
Iter: 1841 loss: 1.07011499e-06
Iter: 1842 loss: 1.06979405e-06
Iter: 1843 loss: 1.06977643e-06
Iter: 1844 loss: 1.06927246e-06
Iter: 1845 loss: 1.06934067e-06
Iter: 1846 loss: 1.06884738e-06
Iter: 1847 loss: 1.06847494e-06
Iter: 1848 loss: 1.06926518e-06
Iter: 1849 loss: 1.06826485e-06
Iter: 1850 loss: 1.06758239e-06
Iter: 1851 loss: 1.06847324e-06
Iter: 1852 loss: 1.06723633e-06
Iter: 1853 loss: 1.06670973e-06
Iter: 1854 loss: 1.07090273e-06
Iter: 1855 loss: 1.06676316e-06
Iter: 1856 loss: 1.06632592e-06
Iter: 1857 loss: 1.06841924e-06
Iter: 1858 loss: 1.06628931e-06
Iter: 1859 loss: 1.06594666e-06
Iter: 1860 loss: 1.06757443e-06
Iter: 1861 loss: 1.06588936e-06
Iter: 1862 loss: 1.06549714e-06
Iter: 1863 loss: 1.06623202e-06
Iter: 1864 loss: 1.0653705e-06
Iter: 1865 loss: 1.06506707e-06
Iter: 1866 loss: 1.06474045e-06
Iter: 1867 loss: 1.06469247e-06
Iter: 1868 loss: 1.06415803e-06
Iter: 1869 loss: 1.07041194e-06
Iter: 1870 loss: 1.06416951e-06
Iter: 1871 loss: 1.06392e-06
Iter: 1872 loss: 1.06353411e-06
Iter: 1873 loss: 1.06350535e-06
Iter: 1874 loss: 1.0631345e-06
Iter: 1875 loss: 1.06888172e-06
Iter: 1876 loss: 1.06307527e-06
Iter: 1877 loss: 1.06272091e-06
Iter: 1878 loss: 1.06275741e-06
Iter: 1879 loss: 1.06236303e-06
Iter: 1880 loss: 1.06207062e-06
Iter: 1881 loss: 1.06221296e-06
Iter: 1882 loss: 1.06177868e-06
Iter: 1883 loss: 1.06128482e-06
Iter: 1884 loss: 1.0643264e-06
Iter: 1885 loss: 1.06117932e-06
Iter: 1886 loss: 1.06084929e-06
Iter: 1887 loss: 1.06050265e-06
Iter: 1888 loss: 1.06044592e-06
Iter: 1889 loss: 1.05990819e-06
Iter: 1890 loss: 1.06468451e-06
Iter: 1891 loss: 1.05992547e-06
Iter: 1892 loss: 1.05955075e-06
Iter: 1893 loss: 1.06387779e-06
Iter: 1894 loss: 1.05954018e-06
Iter: 1895 loss: 1.05923959e-06
Iter: 1896 loss: 1.05982394e-06
Iter: 1897 loss: 1.05917672e-06
Iter: 1898 loss: 1.05878257e-06
Iter: 1899 loss: 1.05849688e-06
Iter: 1900 loss: 1.05838774e-06
Iter: 1901 loss: 1.05800132e-06
Iter: 1902 loss: 1.06038033e-06
Iter: 1903 loss: 1.05794174e-06
Iter: 1904 loss: 1.05749336e-06
Iter: 1905 loss: 1.05827473e-06
Iter: 1906 loss: 1.05735489e-06
Iter: 1907 loss: 1.05694221e-06
Iter: 1908 loss: 1.05686536e-06
Iter: 1909 loss: 1.05665549e-06
Iter: 1910 loss: 1.056217e-06
Iter: 1911 loss: 1.05624599e-06
Iter: 1912 loss: 1.05591789e-06
Iter: 1913 loss: 1.05556671e-06
Iter: 1914 loss: 1.0555566e-06
Iter: 1915 loss: 1.05506729e-06
Iter: 1916 loss: 1.0560907e-06
Iter: 1917 loss: 1.05483309e-06
Iter: 1918 loss: 1.05433571e-06
Iter: 1919 loss: 1.05854338e-06
Iter: 1920 loss: 1.05428785e-06
Iter: 1921 loss: 1.05393974e-06
Iter: 1922 loss: 1.05353115e-06
Iter: 1923 loss: 1.05354184e-06
Iter: 1924 loss: 1.05296908e-06
Iter: 1925 loss: 1.05918161e-06
Iter: 1926 loss: 1.05297715e-06
Iter: 1927 loss: 1.05260506e-06
Iter: 1928 loss: 1.05550134e-06
Iter: 1929 loss: 1.0525921e-06
Iter: 1930 loss: 1.05228742e-06
Iter: 1931 loss: 1.05295283e-06
Iter: 1932 loss: 1.0522308e-06
Iter: 1933 loss: 1.0518778e-06
Iter: 1934 loss: 1.05161519e-06
Iter: 1935 loss: 1.05150991e-06
Iter: 1936 loss: 1.05121148e-06
Iter: 1937 loss: 1.05534286e-06
Iter: 1938 loss: 1.05121694e-06
Iter: 1939 loss: 1.05085905e-06
Iter: 1940 loss: 1.05065101e-06
Iter: 1941 loss: 1.05047047e-06
Iter: 1942 loss: 1.05014874e-06
Iter: 1943 loss: 1.05243817e-06
Iter: 1944 loss: 1.05007757e-06
Iter: 1945 loss: 1.04963192e-06
Iter: 1946 loss: 1.04980325e-06
Iter: 1947 loss: 1.04933611e-06
Iter: 1948 loss: 1.04879712e-06
Iter: 1949 loss: 1.04921685e-06
Iter: 1950 loss: 1.04844867e-06
Iter: 1951 loss: 1.04800893e-06
Iter: 1952 loss: 1.05101367e-06
Iter: 1953 loss: 1.04796982e-06
Iter: 1954 loss: 1.04753349e-06
Iter: 1955 loss: 1.04786182e-06
Iter: 1956 loss: 1.04721198e-06
Iter: 1957 loss: 1.04678702e-06
Iter: 1958 loss: 1.04763171e-06
Iter: 1959 loss: 1.0465792e-06
Iter: 1960 loss: 1.04623007e-06
Iter: 1961 loss: 1.05241429e-06
Iter: 1962 loss: 1.04620176e-06
Iter: 1963 loss: 1.04586741e-06
Iter: 1964 loss: 1.04667743e-06
Iter: 1965 loss: 1.045781e-06
Iter: 1966 loss: 1.04550566e-06
Iter: 1967 loss: 1.04567175e-06
Iter: 1968 loss: 1.04532899e-06
Iter: 1969 loss: 1.04497667e-06
Iter: 1970 loss: 1.04483479e-06
Iter: 1971 loss: 1.04466608e-06
Iter: 1972 loss: 1.04428898e-06
Iter: 1973 loss: 1.04429023e-06
Iter: 1974 loss: 1.04401431e-06
Iter: 1975 loss: 1.04346032e-06
Iter: 1976 loss: 1.05226911e-06
Iter: 1977 loss: 1.04346873e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.6/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi2 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi2
+ date
Sun Nov  8 20:09:31 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi2/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi2/300_100_100_100_1 ']'
+ LOAD='--load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.6/300_100_100_100_1'
+ python biholoNN_train.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi1.6/300_100_100_100_1 --function f1 --psi 2 --phi 2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d8248c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d925a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d839730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d86ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d8f3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d8f3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d722840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d6f6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d6f6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d7dda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d7fb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d652158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d652a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d639378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d644e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d6ae510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d6ae598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976bedf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d7ae840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d5d5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97d5d5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976b2be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976b5f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976b9e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976b79620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976af9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976ac9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976ace510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976ace488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976bae9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9769e77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976a04510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976a04ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa9769cf598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa976a9dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa97698f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.00860243
test_loss: 0.009073542
train_loss: 0.0059921937
test_loss: 0.006532007
train_loss: 0.005596319
test_loss: 0.0058382694
train_loss: 0.0050208704
test_loss: 0.005823549
train_loss: 0.0047757765
test_loss: 0.0059675835
train_loss: 0.0044698105
test_loss: 0.005494687
train_loss: 0.0047539896
test_loss: 0.0053326422
train_loss: 0.004471825
test_loss: 0.005355299
train_loss: 0.004768863
test_loss: 0.0051831286
train_loss: 0.004671938
test_loss: 0.0053484133
train_loss: 0.004352338
test_loss: 0.0052264757
train_loss: 0.0045928136
test_loss: 0.005136606
train_loss: 0.004923923
test_loss: 0.005717608
train_loss: 0.004443922
test_loss: 0.005112891
train_loss: 0.0042887847
test_loss: 0.0052714036
train_loss: 0.0041512423
test_loss: 0.004934604
train_loss: 0.0046308273
test_loss: 0.0055293185
train_loss: 0.0043451837
test_loss: 0.0051255
train_loss: 0.004308567
test_loss: 0.005266453
train_loss: 0.004567944
test_loss: 0.005107965
train_loss: 0.003986846
test_loss: 0.0048915013
train_loss: 0.004454409
test_loss: 0.0049553574
train_loss: 0.0040309345
test_loss: 0.0050608027
train_loss: 0.004440738
test_loss: 0.0051337876
train_loss: 0.0042329365
test_loss: 0.0051888274
train_loss: 0.0040863864
test_loss: 0.0050554555
train_loss: 0.004062265
test_loss: 0.005045099
train_loss: 0.004239187
test_loss: 0.0049206624
train_loss: 0.004341801
test_loss: 0.005127366
train_loss: 0.003985624
test_loss: 0.0051674386
train_loss: 0.003921439
test_loss: 0.005029261
train_loss: 0.004037313
test_loss: 0.004838376
train_loss: 0.0039515635
test_loss: 0.0047586355
train_loss: 0.0040852455
test_loss: 0.0052972864
train_loss: 0.004103096
test_loss: 0.005238735
train_loss: 0.0041200276
test_loss: 0.005068496
train_loss: 0.0041834647
test_loss: 0.005049337
train_loss: 0.004320965
test_loss: 0.0053105857
train_loss: 0.0037013476
test_loss: 0.0050180177
train_loss: 0.0042738114
test_loss: 0.005068982
train_loss: 0.004370339
test_loss: 0.0052009583
train_loss: 0.0040329974
test_loss: 0.0048598624
train_loss: 0.004023859
test_loss: 0.004886615
train_loss: 0.0038036616
test_loss: 0.005065261
train_loss: 0.0041279634
test_loss: 0.005148816
train_loss: 0.0038030944
test_loss: 0.0048089107
train_loss: 0.00409775
test_loss: 0.0050486587
train_loss: 0.003877502
test_loss: 0.004749594
train_loss: 0.003984797
test_loss: 0.0048541813
train_loss: 0.004061907
test_loss: 0.00507988
train_loss: 0.0043005543
test_loss: 0.004894074
train_loss: 0.0038690409
test_loss: 0.004933941
train_loss: 0.004180334
test_loss: 0.0050213747
train_loss: 0.0041452916
test_loss: 0.004810639
train_loss: 0.004058865
test_loss: 0.0047362237
train_loss: 0.0039128684
test_loss: 0.004918581
train_loss: 0.004078341
test_loss: 0.004748144
train_loss: 0.003653257
test_loss: 0.004863885
train_loss: 0.003712362
test_loss: 0.004949815
train_loss: 0.0036967474
test_loss: 0.0049388316
train_loss: 0.003867303
test_loss: 0.0049348474
train_loss: 0.0039672204
test_loss: 0.0049477126
train_loss: 0.0037057018
test_loss: 0.0047717644
train_loss: 0.0037749773
test_loss: 0.004877392
train_loss: 0.004282506
test_loss: 0.0048408755
train_loss: 0.003890566
test_loss: 0.0049208268
train_loss: 0.003793323
test_loss: 0.0047126873
train_loss: 0.00398687
test_loss: 0.0053566233
train_loss: 0.003856768
test_loss: 0.005007588
train_loss: 0.0037965402
test_loss: 0.0046386793
train_loss: 0.0040743756
test_loss: 0.004766732
train_loss: 0.0042169085
test_loss: 0.005017472
train_loss: 0.0039775576
test_loss: 0.0049294173
train_loss: 0.0039774813
test_loss: 0.0053634904
train_loss: 0.0037627856
test_loss: 0.0048957425
train_loss: 0.003866318
test_loss: 0.0049191588
train_loss: 0.0038000061
test_loss: 0.0046508703
train_loss: 0.0040573976
test_loss: 0.004989947
train_loss: 0.0038657775
test_loss: 0.0047884965
train_loss: 0.004138486
test_loss: 0.004953554
train_loss: 0.0035712402
test_loss: 0.004641988
train_loss: 0.0038239714
test_loss: 0.0047770995
train_loss: 0.0038027158
test_loss: 0.0048236456
train_loss: 0.0041207382
test_loss: 0.005021645
train_loss: 0.0038707587
test_loss: 0.0049368627
train_loss: 0.0036925306
test_loss: 0.0048909215
train_loss: 0.003827778
test_loss: 0.0048940466
train_loss: 0.0036148462
test_loss: 0.0047781304
train_loss: 0.0036867885
test_loss: 0.004717873
train_loss: 0.0036977634
test_loss: 0.0048822057
train_loss: 0.003896243
test_loss: 0.0048630727
train_loss: 0.003722973
test_loss: 0.0048790835
train_loss: 0.0036369334
test_loss: 0.005153697
train_loss: 0.0037045444
test_loss: 0.0047621336
train_loss: 0.004010363
test_loss: 0.0049190386
train_loss: 0.0036628526
test_loss: 0.005047367
train_loss: 0.0038159634
test_loss: 0.0048623444
train_loss: 0.004136374
test_loss: 0.0050703697
train_loss: 0.0035654255
test_loss: 0.004820321
train_loss: 0.004107614
test_loss: 0.004815964
train_loss: 0.003790435
test_loss: 0.0047923666
train_loss: 0.0039192345
test_loss: 0.0051435367
train_loss: 0.0037634973
test_loss: 0.0048355376
train_loss: 0.0038545316
test_loss: 0.0048815315
train_loss: 0.003948049
test_loss: 0.004983021
train_loss: 0.0036022514
test_loss: 0.004744634
train_loss: 0.0040343315
test_loss: 0.0048176567
train_loss: 0.004024485
test_loss: 0.005041662
train_loss: 0.0036519584
test_loss: 0.005029171
train_loss: 0.003509515
test_loss: 0.004694948
train_loss: 0.0038730197
test_loss: 0.004755409
train_loss: 0.0036248239
test_loss: 0.004935298
train_loss: 0.003544199
test_loss: 0.0046969648
train_loss: 0.0036817784
test_loss: 0.0045999973
train_loss: 0.0037494805
test_loss: 0.0047124065
train_loss: 0.0035776822
test_loss: 0.0047645844
train_loss: 0.0033689816
test_loss: 0.0046945694
train_loss: 0.0037569662
test_loss: 0.0047086193
train_loss: 0.003764785
test_loss: 0.004753093
train_loss: 0.0034646967
test_loss: 0.0047174674
train_loss: 0.0035487534
test_loss: 0.004903555
train_loss: 0.0035195854
test_loss: 0.0047037373
train_loss: 0.0037814514
test_loss: 0.004901466
train_loss: 0.0036668007
test_loss: 0.0047532963
train_loss: 0.0034689014
test_loss: 0.0048058317
train_loss: 0.0038805925
test_loss: 0.004934885
train_loss: 0.003635291
test_loss: 0.0048480784
train_loss: 0.0038593635
test_loss: 0.0046879714
train_loss: 0.0035549756
test_loss: 0.004749235
train_loss: 0.0033591127
test_loss: 0.00481202
train_loss: 0.0037636564
test_loss: 0.0046660914
train_loss: 0.0036620842
test_loss: 0.0047715562
train_loss: 0.003519745
test_loss: 0.0046853824
train_loss: 0.003707002
test_loss: 0.00471855
train_loss: 0.0037869944
test_loss: 0.004870803
train_loss: 0.003659023
test_loss: 0.0046627563
train_loss: 0.0034565695
test_loss: 0.0047584004
train_loss: 0.0038733736
test_loss: 0.0048018135
train_loss: 0.0038102379
test_loss: 0.004737088
train_loss: 0.0035833544
test_loss: 0.0048467615
train_loss: 0.0035239344
test_loss: 0.004737946
train_loss: 0.0034753545
test_loss: 0.0047776825
train_loss: 0.0033740425
test_loss: 0.00468605
train_loss: 0.0036055208
test_loss: 0.0047325147
train_loss: 0.0036122652
test_loss: 0.0047379658
train_loss: 0.0035318017
test_loss: 0.0047645364
train_loss: 0.0036489563
test_loss: 0.004666443
train_loss: 0.0036532208
test_loss: 0.004815309
train_loss: 0.003368483
test_loss: 0.00460329
train_loss: 0.00376695
test_loss: 0.004780511
train_loss: 0.00370144
test_loss: 0.0049961405
train_loss: 0.003750098
test_loss: 0.004962178
train_loss: 0.003930951
test_loss: 0.0052601
train_loss: 0.0035622185
test_loss: 0.0047214874
train_loss: 0.0032560236
test_loss: 0.0047507584
train_loss: 0.0037969542
test_loss: 0.005338803
train_loss: 0.003518322
test_loss: 0.0053020697
train_loss: 0.0033405423
test_loss: 0.0045622797
train_loss: 0.0036747577
test_loss: 0.0047434974
train_loss: 0.0036037778
test_loss: 0.004664631
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi2_phi2/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 2 --phi 2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi2_phi2/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56c2c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56d1fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56d1ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56d40ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56d1fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56d1fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56bb68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56bd7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56b76400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56b76730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56b44e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56afdea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56afdb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56a9e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56afdd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56a6e048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56a6e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56a312f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a569ef8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56993378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56993268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a569c3d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56980048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56925598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a56925a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a568cd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a5688d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a568aa7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a568c0378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a39ebed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a39e62488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a39e8c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a39e8c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a39e4d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a39de4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9a39d98268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.29426587e-05
Iter: 2 loss: 1.81276082e-05
Iter: 3 loss: 6.46001863e-05
Iter: 4 loss: 1.79597537e-05
Iter: 5 loss: 1.68212719e-05
Iter: 6 loss: 1.66860391e-05
Iter: 7 loss: 1.60084637e-05
Iter: 8 loss: 1.47841074e-05
Iter: 9 loss: 4.43535246e-05
Iter: 10 loss: 1.47835726e-05
Iter: 11 loss: 1.34504935e-05
Iter: 12 loss: 1.9271436e-05
Iter: 13 loss: 1.31808774e-05
Iter: 14 loss: 1.22411166e-05
Iter: 15 loss: 1.23353148e-05
Iter: 16 loss: 1.15170897e-05
Iter: 17 loss: 1.05954823e-05
Iter: 18 loss: 2.29619764e-05
Iter: 19 loss: 1.05908694e-05
Iter: 20 loss: 9.99568e-06
Iter: 21 loss: 1.17578184e-05
Iter: 22 loss: 9.81416633e-06
Iter: 23 loss: 9.28676309e-06
Iter: 24 loss: 1.06578755e-05
Iter: 25 loss: 9.10813469e-06
Iter: 26 loss: 8.69253381e-06
Iter: 27 loss: 1.09496614e-05
Iter: 28 loss: 8.63167861e-06
Iter: 29 loss: 8.27234635e-06
Iter: 30 loss: 8.13451607e-06
Iter: 31 loss: 7.93851086e-06
Iter: 32 loss: 7.4566542e-06
Iter: 33 loss: 9.59252702e-06
Iter: 34 loss: 7.36113634e-06
Iter: 35 loss: 6.97270525e-06
Iter: 36 loss: 7.09874303e-06
Iter: 37 loss: 6.69691917e-06
Iter: 38 loss: 6.38313213e-06
Iter: 39 loss: 1.02783015e-05
Iter: 40 loss: 6.37998937e-06
Iter: 41 loss: 6.1865403e-06
Iter: 42 loss: 7.38540348e-06
Iter: 43 loss: 6.16347188e-06
Iter: 44 loss: 5.90304489e-06
Iter: 45 loss: 6.55164922e-06
Iter: 46 loss: 5.81109589e-06
Iter: 47 loss: 5.70337033e-06
Iter: 48 loss: 5.59905493e-06
Iter: 49 loss: 5.57586372e-06
Iter: 50 loss: 5.398339e-06
Iter: 51 loss: 6.34527532e-06
Iter: 52 loss: 5.37125788e-06
Iter: 53 loss: 5.19888363e-06
Iter: 54 loss: 5.411e-06
Iter: 55 loss: 5.10896734e-06
Iter: 56 loss: 4.98379313e-06
Iter: 57 loss: 5.61799425e-06
Iter: 58 loss: 4.9630512e-06
Iter: 59 loss: 4.84623615e-06
Iter: 60 loss: 5.53720929e-06
Iter: 61 loss: 4.83113308e-06
Iter: 62 loss: 4.74884928e-06
Iter: 63 loss: 4.69396127e-06
Iter: 64 loss: 4.66320353e-06
Iter: 65 loss: 4.52023369e-06
Iter: 66 loss: 5.37809274e-06
Iter: 67 loss: 4.50201787e-06
Iter: 68 loss: 4.40850499e-06
Iter: 69 loss: 4.55209693e-06
Iter: 70 loss: 4.36400569e-06
Iter: 71 loss: 4.26631e-06
Iter: 72 loss: 4.41093835e-06
Iter: 73 loss: 4.21908544e-06
Iter: 74 loss: 4.13391308e-06
Iter: 75 loss: 4.41585098e-06
Iter: 76 loss: 4.11042583e-06
Iter: 77 loss: 4.01485067e-06
Iter: 78 loss: 4.30759837e-06
Iter: 79 loss: 3.98667044e-06
Iter: 80 loss: 3.94168092e-06
Iter: 81 loss: 3.94172775e-06
Iter: 82 loss: 3.8861117e-06
Iter: 83 loss: 3.92752372e-06
Iter: 84 loss: 3.85205931e-06
Iter: 85 loss: 3.8086564e-06
Iter: 86 loss: 3.7621171e-06
Iter: 87 loss: 3.75446371e-06
Iter: 88 loss: 3.69639929e-06
Iter: 89 loss: 3.87934961e-06
Iter: 90 loss: 3.67967323e-06
Iter: 91 loss: 3.63313575e-06
Iter: 92 loss: 4.31872741e-06
Iter: 93 loss: 3.6331453e-06
Iter: 94 loss: 3.59776186e-06
Iter: 95 loss: 3.53014707e-06
Iter: 96 loss: 4.96628127e-06
Iter: 97 loss: 3.52989173e-06
Iter: 98 loss: 3.52176221e-06
Iter: 99 loss: 3.49813877e-06
Iter: 100 loss: 3.4777504e-06
Iter: 101 loss: 3.42351041e-06
Iter: 102 loss: 3.78837194e-06
Iter: 103 loss: 3.41057603e-06
Iter: 104 loss: 3.36756193e-06
Iter: 105 loss: 3.36453377e-06
Iter: 106 loss: 3.33928938e-06
Iter: 107 loss: 3.36272751e-06
Iter: 108 loss: 3.32480022e-06
Iter: 109 loss: 3.29038585e-06
Iter: 110 loss: 3.27346606e-06
Iter: 111 loss: 3.257152e-06
Iter: 112 loss: 3.21606149e-06
Iter: 113 loss: 3.39618214e-06
Iter: 114 loss: 3.20774575e-06
Iter: 115 loss: 3.16586375e-06
Iter: 116 loss: 3.24877601e-06
Iter: 117 loss: 3.1487034e-06
Iter: 118 loss: 3.14884232e-06
Iter: 119 loss: 3.13387e-06
Iter: 120 loss: 3.11931444e-06
Iter: 121 loss: 3.10933456e-06
Iter: 122 loss: 3.10396899e-06
Iter: 123 loss: 3.08324115e-06
Iter: 124 loss: 3.06181232e-06
Iter: 125 loss: 3.05782351e-06
Iter: 126 loss: 3.03647266e-06
Iter: 127 loss: 3.0976355e-06
Iter: 128 loss: 3.02985131e-06
Iter: 129 loss: 2.99879821e-06
Iter: 130 loss: 3.01620139e-06
Iter: 131 loss: 2.97874431e-06
Iter: 132 loss: 2.96088683e-06
Iter: 133 loss: 2.95739233e-06
Iter: 134 loss: 2.94606753e-06
Iter: 135 loss: 2.92594564e-06
Iter: 136 loss: 2.92595587e-06
Iter: 137 loss: 2.90277899e-06
Iter: 138 loss: 3.16897558e-06
Iter: 139 loss: 2.90255116e-06
Iter: 140 loss: 2.88404408e-06
Iter: 141 loss: 2.85962415e-06
Iter: 142 loss: 2.85825513e-06
Iter: 143 loss: 2.83810823e-06
Iter: 144 loss: 2.97066708e-06
Iter: 145 loss: 2.83582131e-06
Iter: 146 loss: 2.8155539e-06
Iter: 147 loss: 2.88929323e-06
Iter: 148 loss: 2.81043276e-06
Iter: 149 loss: 2.79280721e-06
Iter: 150 loss: 2.84987937e-06
Iter: 151 loss: 2.78786592e-06
Iter: 152 loss: 2.77513436e-06
Iter: 153 loss: 2.81109646e-06
Iter: 154 loss: 2.7711344e-06
Iter: 155 loss: 2.75399952e-06
Iter: 156 loss: 2.8795389e-06
Iter: 157 loss: 2.75253979e-06
Iter: 158 loss: 2.74607169e-06
Iter: 159 loss: 2.73192563e-06
Iter: 160 loss: 2.94578331e-06
Iter: 161 loss: 2.73124988e-06
Iter: 162 loss: 2.71086128e-06
Iter: 163 loss: 2.73295154e-06
Iter: 164 loss: 2.69967768e-06
Iter: 165 loss: 2.68540862e-06
Iter: 166 loss: 2.83463419e-06
Iter: 167 loss: 2.68496478e-06
Iter: 168 loss: 2.672645e-06
Iter: 169 loss: 2.68348913e-06
Iter: 170 loss: 2.66551297e-06
Iter: 171 loss: 2.64938853e-06
Iter: 172 loss: 2.77531603e-06
Iter: 173 loss: 2.64825394e-06
Iter: 174 loss: 2.6373034e-06
Iter: 175 loss: 2.63612264e-06
Iter: 176 loss: 2.62826029e-06
Iter: 177 loss: 2.61798778e-06
Iter: 178 loss: 2.76050287e-06
Iter: 179 loss: 2.61800778e-06
Iter: 180 loss: 2.60962406e-06
Iter: 181 loss: 2.59906437e-06
Iter: 182 loss: 2.59810963e-06
Iter: 183 loss: 2.58425e-06
Iter: 184 loss: 2.62154572e-06
Iter: 185 loss: 2.57965871e-06
Iter: 186 loss: 2.56869407e-06
Iter: 187 loss: 2.63418747e-06
Iter: 188 loss: 2.56740555e-06
Iter: 189 loss: 2.5540578e-06
Iter: 190 loss: 2.5696645e-06
Iter: 191 loss: 2.54706765e-06
Iter: 192 loss: 2.54096767e-06
Iter: 193 loss: 2.54059569e-06
Iter: 194 loss: 2.53298049e-06
Iter: 195 loss: 2.52303221e-06
Iter: 196 loss: 2.52232508e-06
Iter: 197 loss: 2.51364e-06
Iter: 198 loss: 2.52566224e-06
Iter: 199 loss: 2.50930361e-06
Iter: 200 loss: 2.50036737e-06
Iter: 201 loss: 2.48619494e-06
Iter: 202 loss: 2.48609604e-06
Iter: 203 loss: 2.47413641e-06
Iter: 204 loss: 2.47351272e-06
Iter: 205 loss: 2.46640093e-06
Iter: 206 loss: 2.46807599e-06
Iter: 207 loss: 2.46123864e-06
Iter: 208 loss: 2.44985245e-06
Iter: 209 loss: 2.52984751e-06
Iter: 210 loss: 2.44882381e-06
Iter: 211 loss: 2.44146349e-06
Iter: 212 loss: 2.4685005e-06
Iter: 213 loss: 2.43965678e-06
Iter: 214 loss: 2.43340196e-06
Iter: 215 loss: 2.43230306e-06
Iter: 216 loss: 2.42799524e-06
Iter: 217 loss: 2.41886528e-06
Iter: 218 loss: 2.49088362e-06
Iter: 219 loss: 2.41822249e-06
Iter: 220 loss: 2.41177531e-06
Iter: 221 loss: 2.40304053e-06
Iter: 222 loss: 2.40261807e-06
Iter: 223 loss: 2.39200335e-06
Iter: 224 loss: 2.46929244e-06
Iter: 225 loss: 2.39121823e-06
Iter: 226 loss: 2.38353732e-06
Iter: 227 loss: 2.40102577e-06
Iter: 228 loss: 2.38077291e-06
Iter: 229 loss: 2.37256677e-06
Iter: 230 loss: 2.42556234e-06
Iter: 231 loss: 2.37166751e-06
Iter: 232 loss: 2.36470419e-06
Iter: 233 loss: 2.36586766e-06
Iter: 234 loss: 2.35941525e-06
Iter: 235 loss: 2.35430662e-06
Iter: 236 loss: 2.35298762e-06
Iter: 237 loss: 2.35151356e-06
Iter: 238 loss: 2.34634717e-06
Iter: 239 loss: 2.34888307e-06
Iter: 240 loss: 2.34139861e-06
Iter: 241 loss: 2.33052242e-06
Iter: 242 loss: 2.40185818e-06
Iter: 243 loss: 2.32950583e-06
Iter: 244 loss: 2.32443654e-06
Iter: 245 loss: 2.32449065e-06
Iter: 246 loss: 2.31967601e-06
Iter: 247 loss: 2.31083709e-06
Iter: 248 loss: 2.51267738e-06
Iter: 249 loss: 2.31077729e-06
Iter: 250 loss: 2.3027219e-06
Iter: 251 loss: 2.38125904e-06
Iter: 252 loss: 2.30261981e-06
Iter: 253 loss: 2.29623629e-06
Iter: 254 loss: 2.31053355e-06
Iter: 255 loss: 2.29380589e-06
Iter: 256 loss: 2.28717772e-06
Iter: 257 loss: 2.31492913e-06
Iter: 258 loss: 2.28568479e-06
Iter: 259 loss: 2.28103067e-06
Iter: 260 loss: 2.27996134e-06
Iter: 261 loss: 2.27683972e-06
Iter: 262 loss: 2.26835027e-06
Iter: 263 loss: 2.28357885e-06
Iter: 264 loss: 2.2648278e-06
Iter: 265 loss: 2.25903523e-06
Iter: 266 loss: 2.28747626e-06
Iter: 267 loss: 2.25805752e-06
Iter: 268 loss: 2.25204963e-06
Iter: 269 loss: 2.28410727e-06
Iter: 270 loss: 2.25113195e-06
Iter: 271 loss: 2.24660789e-06
Iter: 272 loss: 2.26333395e-06
Iter: 273 loss: 2.24535052e-06
Iter: 274 loss: 2.23994766e-06
Iter: 275 loss: 2.27388523e-06
Iter: 276 loss: 2.23940879e-06
Iter: 277 loss: 2.23675897e-06
Iter: 278 loss: 2.23201323e-06
Iter: 279 loss: 2.34243612e-06
Iter: 280 loss: 2.23200141e-06
Iter: 281 loss: 2.226e-06
Iter: 282 loss: 2.2221534e-06
Iter: 283 loss: 2.21975438e-06
Iter: 284 loss: 2.21717391e-06
Iter: 285 loss: 2.21547316e-06
Iter: 286 loss: 2.21196842e-06
Iter: 287 loss: 2.21123128e-06
Iter: 288 loss: 2.20898414e-06
Iter: 289 loss: 2.20368429e-06
Iter: 290 loss: 2.20673019e-06
Iter: 291 loss: 2.20000038e-06
Iter: 292 loss: 2.19574667e-06
Iter: 293 loss: 2.2527397e-06
Iter: 294 loss: 2.19577078e-06
Iter: 295 loss: 2.19171397e-06
Iter: 296 loss: 2.18861533e-06
Iter: 297 loss: 2.18732316e-06
Iter: 298 loss: 2.18232867e-06
Iter: 299 loss: 2.20087327e-06
Iter: 300 loss: 2.18125683e-06
Iter: 301 loss: 2.17611273e-06
Iter: 302 loss: 2.18398554e-06
Iter: 303 loss: 2.17363822e-06
Iter: 304 loss: 2.16841318e-06
Iter: 305 loss: 2.18854439e-06
Iter: 306 loss: 2.16707554e-06
Iter: 307 loss: 2.16278659e-06
Iter: 308 loss: 2.20345e-06
Iter: 309 loss: 2.16267335e-06
Iter: 310 loss: 2.15944374e-06
Iter: 311 loss: 2.19171807e-06
Iter: 312 loss: 2.159323e-06
Iter: 313 loss: 2.15666842e-06
Iter: 314 loss: 2.15352497e-06
Iter: 315 loss: 2.15317118e-06
Iter: 316 loss: 2.14856277e-06
Iter: 317 loss: 2.14813622e-06
Iter: 318 loss: 2.14466127e-06
Iter: 319 loss: 2.14015222e-06
Iter: 320 loss: 2.14462057e-06
Iter: 321 loss: 2.13757949e-06
Iter: 322 loss: 2.1322694e-06
Iter: 323 loss: 2.19378012e-06
Iter: 324 loss: 2.13221097e-06
Iter: 325 loss: 2.12835016e-06
Iter: 326 loss: 2.14865941e-06
Iter: 327 loss: 2.12780083e-06
Iter: 328 loss: 2.12524537e-06
Iter: 329 loss: 2.12154646e-06
Iter: 330 loss: 2.12146801e-06
Iter: 331 loss: 2.11736437e-06
Iter: 332 loss: 2.17199977e-06
Iter: 333 loss: 2.11731776e-06
Iter: 334 loss: 2.11410702e-06
Iter: 335 loss: 2.11699034e-06
Iter: 336 loss: 2.11224346e-06
Iter: 337 loss: 2.10841495e-06
Iter: 338 loss: 2.10943517e-06
Iter: 339 loss: 2.10570238e-06
Iter: 340 loss: 2.10140092e-06
Iter: 341 loss: 2.10595385e-06
Iter: 342 loss: 2.09899213e-06
Iter: 343 loss: 2.0957184e-06
Iter: 344 loss: 2.09553718e-06
Iter: 345 loss: 2.09298651e-06
Iter: 346 loss: 2.11573865e-06
Iter: 347 loss: 2.09282575e-06
Iter: 348 loss: 2.09070367e-06
Iter: 349 loss: 2.08803885e-06
Iter: 350 loss: 2.08772985e-06
Iter: 351 loss: 2.0843e-06
Iter: 352 loss: 2.09163863e-06
Iter: 353 loss: 2.08296683e-06
Iter: 354 loss: 2.07963922e-06
Iter: 355 loss: 2.07942094e-06
Iter: 356 loss: 2.07689209e-06
Iter: 357 loss: 2.07229868e-06
Iter: 358 loss: 2.10113558e-06
Iter: 359 loss: 2.07167841e-06
Iter: 360 loss: 2.06949971e-06
Iter: 361 loss: 2.09627387e-06
Iter: 362 loss: 2.06937307e-06
Iter: 363 loss: 2.06687946e-06
Iter: 364 loss: 2.06127e-06
Iter: 365 loss: 2.1448634e-06
Iter: 366 loss: 2.06103209e-06
Iter: 367 loss: 2.05679703e-06
Iter: 368 loss: 2.11872202e-06
Iter: 369 loss: 2.05674041e-06
Iter: 370 loss: 2.05396236e-06
Iter: 371 loss: 2.06217919e-06
Iter: 372 loss: 2.05310539e-06
Iter: 373 loss: 2.04956496e-06
Iter: 374 loss: 2.05127049e-06
Iter: 375 loss: 2.04726507e-06
Iter: 376 loss: 2.04390517e-06
Iter: 377 loss: 2.04771595e-06
Iter: 378 loss: 2.04213939e-06
Iter: 379 loss: 2.03894638e-06
Iter: 380 loss: 2.06677305e-06
Iter: 381 loss: 2.03870582e-06
Iter: 382 loss: 2.03644299e-06
Iter: 383 loss: 2.03640411e-06
Iter: 384 loss: 2.03504874e-06
Iter: 385 loss: 2.03204081e-06
Iter: 386 loss: 2.07359244e-06
Iter: 387 loss: 2.03197374e-06
Iter: 388 loss: 2.02882575e-06
Iter: 389 loss: 2.05446395e-06
Iter: 390 loss: 2.0286443e-06
Iter: 391 loss: 2.02638603e-06
Iter: 392 loss: 2.02401861e-06
Iter: 393 loss: 2.02353476e-06
Iter: 394 loss: 2.0202574e-06
Iter: 395 loss: 2.03517925e-06
Iter: 396 loss: 2.01968896e-06
Iter: 397 loss: 2.01588864e-06
Iter: 398 loss: 2.03358695e-06
Iter: 399 loss: 2.01534249e-06
Iter: 400 loss: 2.01252283e-06
Iter: 401 loss: 2.02898332e-06
Iter: 402 loss: 2.01217244e-06
Iter: 403 loss: 2.01049238e-06
Iter: 404 loss: 2.0078844e-06
Iter: 405 loss: 2.00789327e-06
Iter: 406 loss: 2.00373051e-06
Iter: 407 loss: 2.02305682e-06
Iter: 408 loss: 2.0030111e-06
Iter: 409 loss: 2.00060072e-06
Iter: 410 loss: 2.03225864e-06
Iter: 411 loss: 2.0006155e-06
Iter: 412 loss: 1.99900114e-06
Iter: 413 loss: 1.99601982e-06
Iter: 414 loss: 2.06821e-06
Iter: 415 loss: 1.99605756e-06
Iter: 416 loss: 1.99338933e-06
Iter: 417 loss: 2.02335968e-06
Iter: 418 loss: 1.99335227e-06
Iter: 419 loss: 1.99155124e-06
Iter: 420 loss: 1.99144415e-06
Iter: 421 loss: 1.9903282e-06
Iter: 422 loss: 1.98772204e-06
Iter: 423 loss: 2.01351304e-06
Iter: 424 loss: 1.98727639e-06
Iter: 425 loss: 1.98458702e-06
Iter: 426 loss: 2.00024192e-06
Iter: 427 loss: 1.98419184e-06
Iter: 428 loss: 1.98153475e-06
Iter: 429 loss: 1.9870206e-06
Iter: 430 loss: 1.98043404e-06
Iter: 431 loss: 1.97791769e-06
Iter: 432 loss: 1.97970257e-06
Iter: 433 loss: 1.9764118e-06
Iter: 434 loss: 1.97345594e-06
Iter: 435 loss: 1.98827593e-06
Iter: 436 loss: 1.97294958e-06
Iter: 437 loss: 1.97021723e-06
Iter: 438 loss: 1.99172223e-06
Iter: 439 loss: 1.96994665e-06
Iter: 440 loss: 1.96800966e-06
Iter: 441 loss: 1.96385986e-06
Iter: 442 loss: 2.03420905e-06
Iter: 443 loss: 1.96384667e-06
Iter: 444 loss: 1.96147948e-06
Iter: 445 loss: 1.96130077e-06
Iter: 446 loss: 1.95963298e-06
Iter: 447 loss: 1.96315477e-06
Iter: 448 loss: 1.95897519e-06
Iter: 449 loss: 1.95643111e-06
Iter: 450 loss: 1.95537655e-06
Iter: 451 loss: 1.95405869e-06
Iter: 452 loss: 1.95175289e-06
Iter: 453 loss: 1.98096359e-06
Iter: 454 loss: 1.95171219e-06
Iter: 455 loss: 1.95051075e-06
Iter: 456 loss: 1.95052053e-06
Iter: 457 loss: 1.94948825e-06
Iter: 458 loss: 1.94679228e-06
Iter: 459 loss: 1.9671711e-06
Iter: 460 loss: 1.94632094e-06
Iter: 461 loss: 1.94356471e-06
Iter: 462 loss: 1.95055077e-06
Iter: 463 loss: 1.94265203e-06
Iter: 464 loss: 1.94012705e-06
Iter: 465 loss: 1.95611869e-06
Iter: 466 loss: 1.93967981e-06
Iter: 467 loss: 1.93774622e-06
Iter: 468 loss: 1.94467179e-06
Iter: 469 loss: 1.93708638e-06
Iter: 470 loss: 1.93460255e-06
Iter: 471 loss: 1.93255573e-06
Iter: 472 loss: 1.93181518e-06
Iter: 473 loss: 1.93050369e-06
Iter: 474 loss: 1.93015831e-06
Iter: 475 loss: 1.92876178e-06
Iter: 476 loss: 1.92855919e-06
Iter: 477 loss: 1.92751054e-06
Iter: 478 loss: 1.92567541e-06
Iter: 479 loss: 1.92692096e-06
Iter: 480 loss: 1.92460811e-06
Iter: 481 loss: 1.92201787e-06
Iter: 482 loss: 1.922293e-06
Iter: 483 loss: 1.92012953e-06
Iter: 484 loss: 1.91787512e-06
Iter: 485 loss: 1.91767367e-06
Iter: 486 loss: 1.91634922e-06
Iter: 487 loss: 1.9136362e-06
Iter: 488 loss: 1.96721408e-06
Iter: 489 loss: 1.91363733e-06
Iter: 490 loss: 1.91417939e-06
Iter: 491 loss: 1.9123554e-06
Iter: 492 loss: 1.91145455e-06
Iter: 493 loss: 1.90975788e-06
Iter: 494 loss: 1.94459494e-06
Iter: 495 loss: 1.90964556e-06
Iter: 496 loss: 1.90789706e-06
Iter: 497 loss: 1.90760943e-06
Iter: 498 loss: 1.90634341e-06
Iter: 499 loss: 1.90358037e-06
Iter: 500 loss: 1.91028857e-06
Iter: 501 loss: 1.90257799e-06
Iter: 502 loss: 1.89994057e-06
Iter: 503 loss: 1.91829577e-06
Iter: 504 loss: 1.89967432e-06
Iter: 505 loss: 1.89798311e-06
Iter: 506 loss: 1.90766173e-06
Iter: 507 loss: 1.89770753e-06
Iter: 508 loss: 1.89621392e-06
Iter: 509 loss: 1.89529396e-06
Iter: 510 loss: 1.89464629e-06
Iter: 511 loss: 1.89254604e-06
Iter: 512 loss: 1.90262563e-06
Iter: 513 loss: 1.89221191e-06
Iter: 514 loss: 1.89027855e-06
Iter: 515 loss: 1.90310436e-06
Iter: 516 loss: 1.89009324e-06
Iter: 517 loss: 1.8882381e-06
Iter: 518 loss: 1.88864965e-06
Iter: 519 loss: 1.88685488e-06
Iter: 520 loss: 1.8851083e-06
Iter: 521 loss: 1.88308604e-06
Iter: 522 loss: 1.8827966e-06
Iter: 523 loss: 1.88032607e-06
Iter: 524 loss: 1.88035119e-06
Iter: 525 loss: 1.87950366e-06
Iter: 526 loss: 1.87933131e-06
Iter: 527 loss: 1.87856676e-06
Iter: 528 loss: 1.87825481e-06
Iter: 529 loss: 1.87775754e-06
Iter: 530 loss: 1.87620776e-06
Iter: 531 loss: 1.87426167e-06
Iter: 532 loss: 1.8740825e-06
Iter: 533 loss: 1.87242358e-06
Iter: 534 loss: 1.87549654e-06
Iter: 535 loss: 1.87173e-06
Iter: 536 loss: 1.86958937e-06
Iter: 537 loss: 1.87155342e-06
Iter: 538 loss: 1.86836292e-06
Iter: 539 loss: 1.86604871e-06
Iter: 540 loss: 1.88173294e-06
Iter: 541 loss: 1.86580598e-06
Iter: 542 loss: 1.86416787e-06
Iter: 543 loss: 1.87138528e-06
Iter: 544 loss: 1.86378247e-06
Iter: 545 loss: 1.86222042e-06
Iter: 546 loss: 1.86763225e-06
Iter: 547 loss: 1.86176339e-06
Iter: 548 loss: 1.86013472e-06
Iter: 549 loss: 1.8628059e-06
Iter: 550 loss: 1.85930594e-06
Iter: 551 loss: 1.85771341e-06
Iter: 552 loss: 1.86747388e-06
Iter: 553 loss: 1.85747263e-06
Iter: 554 loss: 1.85604051e-06
Iter: 555 loss: 1.85669205e-06
Iter: 556 loss: 1.85501449e-06
Iter: 557 loss: 1.85334864e-06
Iter: 558 loss: 1.85607928e-06
Iter: 559 loss: 1.85260535e-06
Iter: 560 loss: 1.85112674e-06
Iter: 561 loss: 1.86467423e-06
Iter: 562 loss: 1.85104477e-06
Iter: 563 loss: 1.84906344e-06
Iter: 564 loss: 1.84939609e-06
Iter: 565 loss: 1.84750661e-06
Iter: 566 loss: 1.84640817e-06
Iter: 567 loss: 1.8467307e-06
Iter: 568 loss: 1.84552323e-06
Iter: 569 loss: 1.84382179e-06
Iter: 570 loss: 1.84877592e-06
Iter: 571 loss: 1.84321993e-06
Iter: 572 loss: 1.84121666e-06
Iter: 573 loss: 1.84328474e-06
Iter: 574 loss: 1.84016585e-06
Iter: 575 loss: 1.83825023e-06
Iter: 576 loss: 1.83856196e-06
Iter: 577 loss: 1.83687473e-06
Iter: 578 loss: 1.83457348e-06
Iter: 579 loss: 1.84342559e-06
Iter: 580 loss: 1.83396253e-06
Iter: 581 loss: 1.83194311e-06
Iter: 582 loss: 1.84751741e-06
Iter: 583 loss: 1.8317e-06
Iter: 584 loss: 1.83032512e-06
Iter: 585 loss: 1.84324722e-06
Iter: 586 loss: 1.83031193e-06
Iter: 587 loss: 1.82909491e-06
Iter: 588 loss: 1.82841234e-06
Iter: 589 loss: 1.82783378e-06
Iter: 590 loss: 1.82641554e-06
Iter: 591 loss: 1.8378696e-06
Iter: 592 loss: 1.8262358e-06
Iter: 593 loss: 1.82498059e-06
Iter: 594 loss: 1.82423344e-06
Iter: 595 loss: 1.82353403e-06
Iter: 596 loss: 1.82219219e-06
Iter: 597 loss: 1.82216081e-06
Iter: 598 loss: 1.82081021e-06
Iter: 599 loss: 1.82289318e-06
Iter: 600 loss: 1.82026872e-06
Iter: 601 loss: 1.81940618e-06
Iter: 602 loss: 1.81715507e-06
Iter: 603 loss: 1.83928364e-06
Iter: 604 loss: 1.81687267e-06
Iter: 605 loss: 1.81426105e-06
Iter: 606 loss: 1.84007195e-06
Iter: 607 loss: 1.81405994e-06
Iter: 608 loss: 1.81282826e-06
Iter: 609 loss: 1.81679297e-06
Iter: 610 loss: 1.81249482e-06
Iter: 611 loss: 1.81080463e-06
Iter: 612 loss: 1.81389578e-06
Iter: 613 loss: 1.81001951e-06
Iter: 614 loss: 1.80842414e-06
Iter: 615 loss: 1.80898519e-06
Iter: 616 loss: 1.80736538e-06
Iter: 617 loss: 1.80573602e-06
Iter: 618 loss: 1.81198959e-06
Iter: 619 loss: 1.80534221e-06
Iter: 620 loss: 1.80369079e-06
Iter: 621 loss: 1.8060166e-06
Iter: 622 loss: 1.80285645e-06
Iter: 623 loss: 1.80173356e-06
Iter: 624 loss: 1.80166387e-06
Iter: 625 loss: 1.80092536e-06
Iter: 626 loss: 1.79898097e-06
Iter: 627 loss: 1.81712016e-06
Iter: 628 loss: 1.79873575e-06
Iter: 629 loss: 1.79797871e-06
Iter: 630 loss: 1.79754829e-06
Iter: 631 loss: 1.79653421e-06
Iter: 632 loss: 1.79886661e-06
Iter: 633 loss: 1.79619815e-06
Iter: 634 loss: 1.7952184e-06
Iter: 635 loss: 1.79363417e-06
Iter: 636 loss: 1.79356584e-06
Iter: 637 loss: 1.79200094e-06
Iter: 638 loss: 1.80783275e-06
Iter: 639 loss: 1.79190545e-06
Iter: 640 loss: 1.79086737e-06
Iter: 641 loss: 1.79214931e-06
Iter: 642 loss: 1.79047925e-06
Iter: 643 loss: 1.78894834e-06
Iter: 644 loss: 1.78680318e-06
Iter: 645 loss: 1.78672622e-06
Iter: 646 loss: 1.78491382e-06
Iter: 647 loss: 1.8029657e-06
Iter: 648 loss: 1.78480252e-06
Iter: 649 loss: 1.78315793e-06
Iter: 650 loss: 1.78604273e-06
Iter: 651 loss: 1.78226287e-06
Iter: 652 loss: 1.78082405e-06
Iter: 653 loss: 1.79958784e-06
Iter: 654 loss: 1.78082735e-06
Iter: 655 loss: 1.7799216e-06
Iter: 656 loss: 1.77901688e-06
Iter: 657 loss: 1.77881361e-06
Iter: 658 loss: 1.77711047e-06
Iter: 659 loss: 1.77885511e-06
Iter: 660 loss: 1.77616744e-06
Iter: 661 loss: 1.77433753e-06
Iter: 662 loss: 1.78000766e-06
Iter: 663 loss: 1.77384186e-06
Iter: 664 loss: 1.7740532e-06
Iter: 665 loss: 1.77325273e-06
Iter: 666 loss: 1.77269726e-06
Iter: 667 loss: 1.77193283e-06
Iter: 668 loss: 1.77186189e-06
Iter: 669 loss: 1.77086258e-06
Iter: 670 loss: 1.7721776e-06
Iter: 671 loss: 1.77037373e-06
Iter: 672 loss: 1.76892286e-06
Iter: 673 loss: 1.77248603e-06
Iter: 674 loss: 1.76848562e-06
Iter: 675 loss: 1.76737717e-06
Iter: 676 loss: 1.76744607e-06
Iter: 677 loss: 1.76651065e-06
Iter: 678 loss: 1.76510673e-06
Iter: 679 loss: 1.77719323e-06
Iter: 680 loss: 1.76502533e-06
Iter: 681 loss: 1.76388698e-06
Iter: 682 loss: 1.76349613e-06
Iter: 683 loss: 1.76285835e-06
Iter: 684 loss: 1.7610522e-06
Iter: 685 loss: 1.76571e-06
Iter: 686 loss: 1.76047718e-06
Iter: 687 loss: 1.7590844e-06
Iter: 688 loss: 1.7652269e-06
Iter: 689 loss: 1.75878733e-06
Iter: 690 loss: 1.75759567e-06
Iter: 691 loss: 1.76157641e-06
Iter: 692 loss: 1.75724745e-06
Iter: 693 loss: 1.75572109e-06
Iter: 694 loss: 1.7593635e-06
Iter: 695 loss: 1.75537104e-06
Iter: 696 loss: 1.75418973e-06
Iter: 697 loss: 1.75429466e-06
Iter: 698 loss: 1.75325818e-06
Iter: 699 loss: 1.75262426e-06
Iter: 700 loss: 1.752358e-06
Iter: 701 loss: 1.75148341e-06
Iter: 702 loss: 1.75065838e-06
Iter: 703 loss: 1.75045307e-06
Iter: 704 loss: 1.74923571e-06
Iter: 705 loss: 1.74998854e-06
Iter: 706 loss: 1.74849617e-06
Iter: 707 loss: 1.74759225e-06
Iter: 708 loss: 1.75955506e-06
Iter: 709 loss: 1.74758452e-06
Iter: 710 loss: 1.74667821e-06
Iter: 711 loss: 1.74511501e-06
Iter: 712 loss: 1.74505487e-06
Iter: 713 loss: 1.74382251e-06
Iter: 714 loss: 1.75029106e-06
Iter: 715 loss: 1.74357569e-06
Iter: 716 loss: 1.74200045e-06
Iter: 717 loss: 1.74702382e-06
Iter: 718 loss: 1.74163085e-06
Iter: 719 loss: 1.74039667e-06
Iter: 720 loss: 1.74321735e-06
Iter: 721 loss: 1.73992555e-06
Iter: 722 loss: 1.73845456e-06
Iter: 723 loss: 1.73718115e-06
Iter: 724 loss: 1.73690842e-06
Iter: 725 loss: 1.73519834e-06
Iter: 726 loss: 1.73526348e-06
Iter: 727 loss: 1.73410808e-06
Iter: 728 loss: 1.73856915e-06
Iter: 729 loss: 1.73389651e-06
Iter: 730 loss: 1.73269575e-06
Iter: 731 loss: 1.73305261e-06
Iter: 732 loss: 1.73194928e-06
Iter: 733 loss: 1.73129274e-06
Iter: 734 loss: 1.73117201e-06
Iter: 735 loss: 1.73056412e-06
Iter: 736 loss: 1.72952412e-06
Iter: 737 loss: 1.72955561e-06
Iter: 738 loss: 1.72850582e-06
Iter: 739 loss: 1.72861098e-06
Iter: 740 loss: 1.72782052e-06
Iter: 741 loss: 1.72636874e-06
Iter: 742 loss: 1.73500143e-06
Iter: 743 loss: 1.72636965e-06
Iter: 744 loss: 1.72490365e-06
Iter: 745 loss: 1.72693808e-06
Iter: 746 loss: 1.724247e-06
Iter: 747 loss: 1.72325281e-06
Iter: 748 loss: 1.72524e-06
Iter: 749 loss: 1.72287832e-06
Iter: 750 loss: 1.72175464e-06
Iter: 751 loss: 1.72098953e-06
Iter: 752 loss: 1.72056821e-06
Iter: 753 loss: 1.71937177e-06
Iter: 754 loss: 1.71941633e-06
Iter: 755 loss: 1.71842839e-06
Iter: 756 loss: 1.71736895e-06
Iter: 757 loss: 1.71714123e-06
Iter: 758 loss: 1.71568774e-06
Iter: 759 loss: 1.72233763e-06
Iter: 760 loss: 1.71530939e-06
Iter: 761 loss: 1.71383897e-06
Iter: 762 loss: 1.7141042e-06
Iter: 763 loss: 1.71269403e-06
Iter: 764 loss: 1.71264469e-06
Iter: 765 loss: 1.71188276e-06
Iter: 766 loss: 1.71134957e-06
Iter: 767 loss: 1.71352212e-06
Iter: 768 loss: 1.71122986e-06
Iter: 769 loss: 1.71072725e-06
Iter: 770 loss: 1.7094892e-06
Iter: 771 loss: 1.72270813e-06
Iter: 772 loss: 1.70936914e-06
Iter: 773 loss: 1.70790167e-06
Iter: 774 loss: 1.7112219e-06
Iter: 775 loss: 1.70732028e-06
Iter: 776 loss: 1.70587941e-06
Iter: 777 loss: 1.71385079e-06
Iter: 778 loss: 1.70571423e-06
Iter: 779 loss: 1.70452586e-06
Iter: 780 loss: 1.71415536e-06
Iter: 781 loss: 1.70445367e-06
Iter: 782 loss: 1.70365126e-06
Iter: 783 loss: 1.70240833e-06
Iter: 784 loss: 1.70241196e-06
Iter: 785 loss: 1.70078943e-06
Iter: 786 loss: 1.70853014e-06
Iter: 787 loss: 1.70051942e-06
Iter: 788 loss: 1.69937516e-06
Iter: 789 loss: 1.69953307e-06
Iter: 790 loss: 1.69838268e-06
Iter: 791 loss: 1.69685836e-06
Iter: 792 loss: 1.70701071e-06
Iter: 793 loss: 1.69668976e-06
Iter: 794 loss: 1.6954516e-06
Iter: 795 loss: 1.69975749e-06
Iter: 796 loss: 1.69506063e-06
Iter: 797 loss: 1.69383463e-06
Iter: 798 loss: 1.69838893e-06
Iter: 799 loss: 1.69350483e-06
Iter: 800 loss: 1.69269106e-06
Iter: 801 loss: 1.69501186e-06
Iter: 802 loss: 1.69236762e-06
Iter: 803 loss: 1.69126599e-06
Iter: 804 loss: 1.70313865e-06
Iter: 805 loss: 1.69133546e-06
Iter: 806 loss: 1.69067221e-06
Iter: 807 loss: 1.68936083e-06
Iter: 808 loss: 1.70610838e-06
Iter: 809 loss: 1.68919894e-06
Iter: 810 loss: 1.68788188e-06
Iter: 811 loss: 1.69780833e-06
Iter: 812 loss: 1.68784277e-06
Iter: 813 loss: 1.68659608e-06
Iter: 814 loss: 1.6872541e-06
Iter: 815 loss: 1.68579072e-06
Iter: 816 loss: 1.68446206e-06
Iter: 817 loss: 1.68687984e-06
Iter: 818 loss: 1.68385156e-06
Iter: 819 loss: 1.68239831e-06
Iter: 820 loss: 1.69558496e-06
Iter: 821 loss: 1.68241013e-06
Iter: 822 loss: 1.68144265e-06
Iter: 823 loss: 1.68433826e-06
Iter: 824 loss: 1.6812362e-06
Iter: 825 loss: 1.68037218e-06
Iter: 826 loss: 1.67853045e-06
Iter: 827 loss: 1.70535054e-06
Iter: 828 loss: 1.67844416e-06
Iter: 829 loss: 1.67682515e-06
Iter: 830 loss: 1.67680605e-06
Iter: 831 loss: 1.67587496e-06
Iter: 832 loss: 1.67470193e-06
Iter: 833 loss: 1.67452117e-06
Iter: 834 loss: 1.6731351e-06
Iter: 835 loss: 1.67314067e-06
Iter: 836 loss: 1.67223357e-06
Iter: 837 loss: 1.67376038e-06
Iter: 838 loss: 1.67191388e-06
Iter: 839 loss: 1.67120174e-06
Iter: 840 loss: 1.6712321e-06
Iter: 841 loss: 1.6705709e-06
Iter: 842 loss: 1.66955306e-06
Iter: 843 loss: 1.69180191e-06
Iter: 844 loss: 1.66953919e-06
Iter: 845 loss: 1.66849418e-06
Iter: 846 loss: 1.66754808e-06
Iter: 847 loss: 1.66737721e-06
Iter: 848 loss: 1.66573034e-06
Iter: 849 loss: 1.6791796e-06
Iter: 850 loss: 1.66559573e-06
Iter: 851 loss: 1.66448388e-06
Iter: 852 loss: 1.67398616e-06
Iter: 853 loss: 1.66442135e-06
Iter: 854 loss: 1.66357279e-06
Iter: 855 loss: 1.66315317e-06
Iter: 856 loss: 1.66269376e-06
Iter: 857 loss: 1.66136306e-06
Iter: 858 loss: 1.66297536e-06
Iter: 859 loss: 1.66069e-06
Iter: 860 loss: 1.65976303e-06
Iter: 861 loss: 1.65971801e-06
Iter: 862 loss: 1.6591589e-06
Iter: 863 loss: 1.65773622e-06
Iter: 864 loss: 1.67229507e-06
Iter: 865 loss: 1.65757137e-06
Iter: 866 loss: 1.65562756e-06
Iter: 867 loss: 1.66150357e-06
Iter: 868 loss: 1.65497602e-06
Iter: 869 loss: 1.65365122e-06
Iter: 870 loss: 1.66018344e-06
Iter: 871 loss: 1.65341839e-06
Iter: 872 loss: 1.65210895e-06
Iter: 873 loss: 1.65406914e-06
Iter: 874 loss: 1.65137988e-06
Iter: 875 loss: 1.65052836e-06
Iter: 876 loss: 1.65054621e-06
Iter: 877 loss: 1.64956521e-06
Iter: 878 loss: 1.65131826e-06
Iter: 879 loss: 1.64922358e-06
Iter: 880 loss: 1.64841083e-06
Iter: 881 loss: 1.64793414e-06
Iter: 882 loss: 1.64760968e-06
Iter: 883 loss: 1.64655273e-06
Iter: 884 loss: 1.65093184e-06
Iter: 885 loss: 1.64631138e-06
Iter: 886 loss: 1.64529706e-06
Iter: 887 loss: 1.64471294e-06
Iter: 888 loss: 1.64425978e-06
Iter: 889 loss: 1.64302185e-06
Iter: 890 loss: 1.65085282e-06
Iter: 891 loss: 1.64286337e-06
Iter: 892 loss: 1.64176936e-06
Iter: 893 loss: 1.64624828e-06
Iter: 894 loss: 1.6414748e-06
Iter: 895 loss: 1.64040512e-06
Iter: 896 loss: 1.64289941e-06
Iter: 897 loss: 1.63995446e-06
Iter: 898 loss: 1.63886079e-06
Iter: 899 loss: 1.63906918e-06
Iter: 900 loss: 1.63809182e-06
Iter: 901 loss: 1.63719233e-06
Iter: 902 loss: 1.63716493e-06
Iter: 903 loss: 1.63655398e-06
Iter: 904 loss: 1.63543734e-06
Iter: 905 loss: 1.6571538e-06
Iter: 906 loss: 1.63543768e-06
Iter: 907 loss: 1.63381651e-06
Iter: 908 loss: 1.63681182e-06
Iter: 909 loss: 1.63308687e-06
Iter: 910 loss: 1.63452319e-06
Iter: 911 loss: 1.63273148e-06
Iter: 912 loss: 1.63244101e-06
Iter: 913 loss: 1.63164373e-06
Iter: 914 loss: 1.6342733e-06
Iter: 915 loss: 1.63125401e-06
Iter: 916 loss: 1.63001778e-06
Iter: 917 loss: 1.63566244e-06
Iter: 918 loss: 1.62979245e-06
Iter: 919 loss: 1.62878564e-06
Iter: 920 loss: 1.63485959e-06
Iter: 921 loss: 1.62868434e-06
Iter: 922 loss: 1.62776882e-06
Iter: 923 loss: 1.6270003e-06
Iter: 924 loss: 1.62687036e-06
Iter: 925 loss: 1.6256829e-06
Iter: 926 loss: 1.62910555e-06
Iter: 927 loss: 1.62529568e-06
Iter: 928 loss: 1.62423498e-06
Iter: 929 loss: 1.63599077e-06
Iter: 930 loss: 1.62427068e-06
Iter: 931 loss: 1.62339848e-06
Iter: 932 loss: 1.62360209e-06
Iter: 933 loss: 1.62286506e-06
Iter: 934 loss: 1.62184733e-06
Iter: 935 loss: 1.6264039e-06
Iter: 936 loss: 1.62169272e-06
Iter: 937 loss: 1.62062736e-06
Iter: 938 loss: 1.62339825e-06
Iter: 939 loss: 1.62026595e-06
Iter: 940 loss: 1.61920116e-06
Iter: 941 loss: 1.61913317e-06
Iter: 942 loss: 1.61845242e-06
Iter: 943 loss: 1.61751632e-06
Iter: 944 loss: 1.62860022e-06
Iter: 945 loss: 1.61752382e-06
Iter: 946 loss: 1.61713399e-06
Iter: 947 loss: 1.61709124e-06
Iter: 948 loss: 1.61677303e-06
Iter: 949 loss: 1.61570779e-06
Iter: 950 loss: 1.61924459e-06
Iter: 951 loss: 1.61526225e-06
Iter: 952 loss: 1.61407752e-06
Iter: 953 loss: 1.62225228e-06
Iter: 954 loss: 1.61394178e-06
Iter: 955 loss: 1.6130158e-06
Iter: 956 loss: 1.61466721e-06
Iter: 957 loss: 1.61260402e-06
Iter: 958 loss: 1.61127809e-06
Iter: 959 loss: 1.61483808e-06
Iter: 960 loss: 1.61073694e-06
Iter: 961 loss: 1.60965692e-06
Iter: 962 loss: 1.61093305e-06
Iter: 963 loss: 1.60906325e-06
Iter: 964 loss: 1.60810521e-06
Iter: 965 loss: 1.61316507e-06
Iter: 966 loss: 1.60793104e-06
Iter: 967 loss: 1.60691116e-06
Iter: 968 loss: 1.61269554e-06
Iter: 969 loss: 1.60674938e-06
Iter: 970 loss: 1.60581612e-06
Iter: 971 loss: 1.60578861e-06
Iter: 972 loss: 1.60509796e-06
Iter: 973 loss: 1.60431898e-06
Iter: 974 loss: 1.61622893e-06
Iter: 975 loss: 1.60431455e-06
Iter: 976 loss: 1.60351124e-06
Iter: 977 loss: 1.60226273e-06
Iter: 978 loss: 1.60222407e-06
Iter: 979 loss: 1.60097306e-06
Iter: 980 loss: 1.61048342e-06
Iter: 981 loss: 1.60088211e-06
Iter: 982 loss: 1.60034915e-06
Iter: 983 loss: 1.60023092e-06
Iter: 984 loss: 1.59992919e-06
Iter: 985 loss: 1.59880187e-06
Iter: 986 loss: 1.60335912e-06
Iter: 987 loss: 1.59834917e-06
Iter: 988 loss: 1.59712852e-06
Iter: 989 loss: 1.59820456e-06
Iter: 990 loss: 1.59638125e-06
Iter: 991 loss: 1.59481556e-06
Iter: 992 loss: 1.61012747e-06
Iter: 993 loss: 1.59476201e-06
Iter: 994 loss: 1.59395131e-06
Iter: 995 loss: 1.5977e-06
Iter: 996 loss: 1.59383103e-06
Iter: 997 loss: 1.59294279e-06
Iter: 998 loss: 1.59399156e-06
Iter: 999 loss: 1.59239698e-06
Iter: 1000 loss: 1.59141632e-06
Iter: 1001 loss: 1.59204342e-06
Iter: 1002 loss: 1.59086426e-06
Iter: 1003 loss: 1.58953912e-06
Iter: 1004 loss: 1.59513456e-06
Iter: 1005 loss: 1.58917396e-06
Iter: 1006 loss: 1.58824332e-06
Iter: 1007 loss: 1.59087972e-06
Iter: 1008 loss: 1.58795206e-06
Iter: 1009 loss: 1.58682701e-06
Iter: 1010 loss: 1.59180331e-06
Iter: 1011 loss: 1.58656712e-06
Iter: 1012 loss: 1.58582861e-06
Iter: 1013 loss: 1.58788202e-06
Iter: 1014 loss: 1.58551279e-06
Iter: 1015 loss: 1.58475677e-06
Iter: 1016 loss: 1.5874291e-06
Iter: 1017 loss: 1.58438809e-06
Iter: 1018 loss: 1.58359853e-06
Iter: 1019 loss: 1.58380203e-06
Iter: 1020 loss: 1.58287537e-06
Iter: 1021 loss: 1.58279317e-06
Iter: 1022 loss: 1.5824678e-06
Iter: 1023 loss: 1.58209559e-06
Iter: 1024 loss: 1.58104535e-06
Iter: 1025 loss: 1.58345188e-06
Iter: 1026 loss: 1.58045168e-06
Iter: 1027 loss: 1.57945044e-06
Iter: 1028 loss: 1.58769853e-06
Iter: 1029 loss: 1.57939292e-06
Iter: 1030 loss: 1.57836973e-06
Iter: 1031 loss: 1.57746376e-06
Iter: 1032 loss: 1.57716e-06
Iter: 1033 loss: 1.57635077e-06
Iter: 1034 loss: 1.57614977e-06
Iter: 1035 loss: 1.57566387e-06
Iter: 1036 loss: 1.57575914e-06
Iter: 1037 loss: 1.5750835e-06
Iter: 1038 loss: 1.57436557e-06
Iter: 1039 loss: 1.57441264e-06
Iter: 1040 loss: 1.57367765e-06
Iter: 1041 loss: 1.5726921e-06
Iter: 1042 loss: 1.58226408e-06
Iter: 1043 loss: 1.57262252e-06
Iter: 1044 loss: 1.57185445e-06
Iter: 1045 loss: 1.57524983e-06
Iter: 1046 loss: 1.57168665e-06
Iter: 1047 loss: 1.57099748e-06
Iter: 1048 loss: 1.57187014e-06
Iter: 1049 loss: 1.57065324e-06
Iter: 1050 loss: 1.56969418e-06
Iter: 1051 loss: 1.57113982e-06
Iter: 1052 loss: 1.56924818e-06
Iter: 1053 loss: 1.56853389e-06
Iter: 1054 loss: 1.5784882e-06
Iter: 1055 loss: 1.56846318e-06
Iter: 1056 loss: 1.56809278e-06
Iter: 1057 loss: 1.57145735e-06
Iter: 1058 loss: 1.56807846e-06
Iter: 1059 loss: 1.56761871e-06
Iter: 1060 loss: 1.56640829e-06
Iter: 1061 loss: 1.58100102e-06
Iter: 1062 loss: 1.56650526e-06
Iter: 1063 loss: 1.56554017e-06
Iter: 1064 loss: 1.56521355e-06
Iter: 1065 loss: 1.56474334e-06
Iter: 1066 loss: 1.56331498e-06
Iter: 1067 loss: 1.57024624e-06
Iter: 1068 loss: 1.56296505e-06
Iter: 1069 loss: 1.56202373e-06
Iter: 1070 loss: 1.5651101e-06
Iter: 1071 loss: 1.56171427e-06
Iter: 1072 loss: 1.56066039e-06
Iter: 1073 loss: 1.56939063e-06
Iter: 1074 loss: 1.56061049e-06
Iter: 1075 loss: 1.55995633e-06
Iter: 1076 loss: 1.56007991e-06
Iter: 1077 loss: 1.55940404e-06
Iter: 1078 loss: 1.55839109e-06
Iter: 1079 loss: 1.55951852e-06
Iter: 1080 loss: 1.55789166e-06
Iter: 1081 loss: 1.55688292e-06
Iter: 1082 loss: 1.56220131e-06
Iter: 1083 loss: 1.55668818e-06
Iter: 1084 loss: 1.5556368e-06
Iter: 1085 loss: 1.56089754e-06
Iter: 1086 loss: 1.55538282e-06
Iter: 1087 loss: 1.55469047e-06
Iter: 1088 loss: 1.55725593e-06
Iter: 1089 loss: 1.55468138e-06
Iter: 1090 loss: 1.55399096e-06
Iter: 1091 loss: 1.555322e-06
Iter: 1092 loss: 1.55376824e-06
Iter: 1093 loss: 1.5529306e-06
Iter: 1094 loss: 1.5581038e-06
Iter: 1095 loss: 1.55284988e-06
Iter: 1096 loss: 1.55227121e-06
Iter: 1097 loss: 1.55236899e-06
Iter: 1098 loss: 1.55178691e-06
Iter: 1099 loss: 1.55114878e-06
Iter: 1100 loss: 1.55062321e-06
Iter: 1101 loss: 1.55045029e-06
Iter: 1102 loss: 1.54942791e-06
Iter: 1103 loss: 1.55272119e-06
Iter: 1104 loss: 1.5491853e-06
Iter: 1105 loss: 1.54801353e-06
Iter: 1106 loss: 1.54852557e-06
Iter: 1107 loss: 1.54729742e-06
Iter: 1108 loss: 1.54624354e-06
Iter: 1109 loss: 1.55467569e-06
Iter: 1110 loss: 1.54627082e-06
Iter: 1111 loss: 1.54537429e-06
Iter: 1112 loss: 1.5494536e-06
Iter: 1113 loss: 1.54524469e-06
Iter: 1114 loss: 1.5442356e-06
Iter: 1115 loss: 1.54536e-06
Iter: 1116 loss: 1.54376676e-06
Iter: 1117 loss: 1.54287227e-06
Iter: 1118 loss: 1.54367899e-06
Iter: 1119 loss: 1.54243412e-06
Iter: 1120 loss: 1.54163035e-06
Iter: 1121 loss: 1.5528949e-06
Iter: 1122 loss: 1.54161648e-06
Iter: 1123 loss: 1.54084864e-06
Iter: 1124 loss: 1.54079044e-06
Iter: 1125 loss: 1.54014447e-06
Iter: 1126 loss: 1.53935548e-06
Iter: 1127 loss: 1.54751206e-06
Iter: 1128 loss: 1.539354e-06
Iter: 1129 loss: 1.53865597e-06
Iter: 1130 loss: 1.54398037e-06
Iter: 1131 loss: 1.53864903e-06
Iter: 1132 loss: 1.53826511e-06
Iter: 1133 loss: 1.53821304e-06
Iter: 1134 loss: 1.53791848e-06
Iter: 1135 loss: 1.53726455e-06
Iter: 1136 loss: 1.5361004e-06
Iter: 1137 loss: 1.53613541e-06
Iter: 1138 loss: 1.53498775e-06
Iter: 1139 loss: 1.54089776e-06
Iter: 1140 loss: 1.5347598e-06
Iter: 1141 loss: 1.53383269e-06
Iter: 1142 loss: 1.53645988e-06
Iter: 1143 loss: 1.53343626e-06
Iter: 1144 loss: 1.53257e-06
Iter: 1145 loss: 1.53528538e-06
Iter: 1146 loss: 1.53229712e-06
Iter: 1147 loss: 1.53123528e-06
Iter: 1148 loss: 1.53227415e-06
Iter: 1149 loss: 1.53068095e-06
Iter: 1150 loss: 1.5298782e-06
Iter: 1151 loss: 1.53639576e-06
Iter: 1152 loss: 1.52976349e-06
Iter: 1153 loss: 1.52888515e-06
Iter: 1154 loss: 1.53229303e-06
Iter: 1155 loss: 1.52867597e-06
Iter: 1156 loss: 1.52796019e-06
Iter: 1157 loss: 1.52701534e-06
Iter: 1158 loss: 1.52694668e-06
Iter: 1159 loss: 1.52673988e-06
Iter: 1160 loss: 1.52633356e-06
Iter: 1161 loss: 1.52597022e-06
Iter: 1162 loss: 1.52562677e-06
Iter: 1163 loss: 1.52544771e-06
Iter: 1164 loss: 1.52478276e-06
Iter: 1165 loss: 1.53243514e-06
Iter: 1166 loss: 1.5247731e-06
Iter: 1167 loss: 1.52416567e-06
Iter: 1168 loss: 1.52349071e-06
Iter: 1169 loss: 1.52339169e-06
Iter: 1170 loss: 1.52278108e-06
Iter: 1171 loss: 1.5266935e-06
Iter: 1172 loss: 1.52258588e-06
Iter: 1173 loss: 1.52190387e-06
Iter: 1174 loss: 1.52077223e-06
Iter: 1175 loss: 1.52072664e-06
Iter: 1176 loss: 1.51958216e-06
Iter: 1177 loss: 1.52663711e-06
Iter: 1178 loss: 1.51947972e-06
Iter: 1179 loss: 1.51853374e-06
Iter: 1180 loss: 1.5200178e-06
Iter: 1181 loss: 1.51819086e-06
Iter: 1182 loss: 1.51720383e-06
Iter: 1183 loss: 1.51866448e-06
Iter: 1184 loss: 1.51674215e-06
Iter: 1185 loss: 1.51575159e-06
Iter: 1186 loss: 1.52702944e-06
Iter: 1187 loss: 1.51568611e-06
Iter: 1188 loss: 1.51493896e-06
Iter: 1189 loss: 1.51749077e-06
Iter: 1190 loss: 1.51471045e-06
Iter: 1191 loss: 1.51395784e-06
Iter: 1192 loss: 1.51415702e-06
Iter: 1193 loss: 1.51348809e-06
Iter: 1194 loss: 1.51271331e-06
Iter: 1195 loss: 1.52215193e-06
Iter: 1196 loss: 1.5126551e-06
Iter: 1197 loss: 1.51195559e-06
Iter: 1198 loss: 1.51335632e-06
Iter: 1199 loss: 1.51171832e-06
Iter: 1200 loss: 1.51110157e-06
Iter: 1201 loss: 1.51934432e-06
Iter: 1202 loss: 1.51111237e-06
Iter: 1203 loss: 1.51075835e-06
Iter: 1204 loss: 1.50997153e-06
Iter: 1205 loss: 1.52011626e-06
Iter: 1206 loss: 1.50996493e-06
Iter: 1207 loss: 1.50921051e-06
Iter: 1208 loss: 1.51453628e-06
Iter: 1209 loss: 1.50905043e-06
Iter: 1210 loss: 1.5083009e-06
Iter: 1211 loss: 1.5097512e-06
Iter: 1212 loss: 1.50810683e-06
Iter: 1213 loss: 1.5073449e-06
Iter: 1214 loss: 1.50774304e-06
Iter: 1215 loss: 1.50693018e-06
Iter: 1216 loss: 1.50607048e-06
Iter: 1217 loss: 1.50872859e-06
Iter: 1218 loss: 1.50581684e-06
Iter: 1219 loss: 1.50485278e-06
Iter: 1220 loss: 1.50491792e-06
Iter: 1221 loss: 1.50406458e-06
Iter: 1222 loss: 1.50311735e-06
Iter: 1223 loss: 1.51294523e-06
Iter: 1224 loss: 1.50307403e-06
Iter: 1225 loss: 1.50222036e-06
Iter: 1226 loss: 1.50201083e-06
Iter: 1227 loss: 1.50147457e-06
Iter: 1228 loss: 1.5005669e-06
Iter: 1229 loss: 1.50059009e-06
Iter: 1230 loss: 1.49991797e-06
Iter: 1231 loss: 1.5000287e-06
Iter: 1232 loss: 1.49938842e-06
Iter: 1233 loss: 1.49877985e-06
Iter: 1234 loss: 1.49874791e-06
Iter: 1235 loss: 1.49829964e-06
Iter: 1236 loss: 1.49834693e-06
Iter: 1237 loss: 1.49789435e-06
Iter: 1238 loss: 1.49740049e-06
Iter: 1239 loss: 1.4983716e-06
Iter: 1240 loss: 1.49704374e-06
Iter: 1241 loss: 1.49627431e-06
Iter: 1242 loss: 1.49533787e-06
Iter: 1243 loss: 1.49525454e-06
Iter: 1244 loss: 1.49450216e-06
Iter: 1245 loss: 1.50371898e-06
Iter: 1246 loss: 1.49450216e-06
Iter: 1247 loss: 1.49372977e-06
Iter: 1248 loss: 1.49483844e-06
Iter: 1249 loss: 1.49332936e-06
Iter: 1250 loss: 1.49258608e-06
Iter: 1251 loss: 1.49333368e-06
Iter: 1252 loss: 1.49208427e-06
Iter: 1253 loss: 1.49118637e-06
Iter: 1254 loss: 1.49324228e-06
Iter: 1255 loss: 1.49078573e-06
Iter: 1256 loss: 1.48993854e-06
Iter: 1257 loss: 1.49134598e-06
Iter: 1258 loss: 1.48938318e-06
Iter: 1259 loss: 1.48824336e-06
Iter: 1260 loss: 1.49378593e-06
Iter: 1261 loss: 1.48799768e-06
Iter: 1262 loss: 1.48726713e-06
Iter: 1263 loss: 1.48831396e-06
Iter: 1264 loss: 1.48673917e-06
Iter: 1265 loss: 1.48606443e-06
Iter: 1266 loss: 1.48603988e-06
Iter: 1267 loss: 1.48550419e-06
Iter: 1268 loss: 1.49027028e-06
Iter: 1269 loss: 1.48542699e-06
Iter: 1270 loss: 1.48499316e-06
Iter: 1271 loss: 1.48469519e-06
Iter: 1272 loss: 1.48452068e-06
Iter: 1273 loss: 1.48372567e-06
Iter: 1274 loss: 1.48548702e-06
Iter: 1275 loss: 1.48344793e-06
Iter: 1276 loss: 1.48288825e-06
Iter: 1277 loss: 1.48258835e-06
Iter: 1278 loss: 1.48234199e-06
Iter: 1279 loss: 1.48133336e-06
Iter: 1280 loss: 1.48787512e-06
Iter: 1281 loss: 1.48121421e-06
Iter: 1282 loss: 1.48065408e-06
Iter: 1283 loss: 1.48088679e-06
Iter: 1284 loss: 1.4802215e-06
Iter: 1285 loss: 1.47940409e-06
Iter: 1286 loss: 1.48132494e-06
Iter: 1287 loss: 1.47909407e-06
Iter: 1288 loss: 1.47822061e-06
Iter: 1289 loss: 1.48303684e-06
Iter: 1290 loss: 1.47817809e-06
Iter: 1291 loss: 1.47722403e-06
Iter: 1292 loss: 1.47610399e-06
Iter: 1293 loss: 1.47598394e-06
Iter: 1294 loss: 1.47503715e-06
Iter: 1295 loss: 1.48391405e-06
Iter: 1296 loss: 1.47492e-06
Iter: 1297 loss: 1.47413971e-06
Iter: 1298 loss: 1.4754055e-06
Iter: 1299 loss: 1.47371043e-06
Iter: 1300 loss: 1.47279422e-06
Iter: 1301 loss: 1.47723426e-06
Iter: 1302 loss: 1.47268838e-06
Iter: 1303 loss: 1.47204764e-06
Iter: 1304 loss: 1.47194953e-06
Iter: 1305 loss: 1.47165122e-06
Iter: 1306 loss: 1.47102332e-06
Iter: 1307 loss: 1.48617107e-06
Iter: 1308 loss: 1.47096523e-06
Iter: 1309 loss: 1.47031153e-06
Iter: 1310 loss: 1.4736404e-06
Iter: 1311 loss: 1.47018818e-06
Iter: 1312 loss: 1.46940374e-06
Iter: 1313 loss: 1.4700862e-06
Iter: 1314 loss: 1.46891693e-06
Iter: 1315 loss: 1.46811863e-06
Iter: 1316 loss: 1.4689341e-06
Iter: 1317 loss: 1.46778939e-06
Iter: 1318 loss: 1.46691059e-06
Iter: 1319 loss: 1.47012634e-06
Iter: 1320 loss: 1.46674722e-06
Iter: 1321 loss: 1.46583614e-06
Iter: 1322 loss: 1.46963544e-06
Iter: 1323 loss: 1.46564616e-06
Iter: 1324 loss: 1.46490856e-06
Iter: 1325 loss: 1.46441607e-06
Iter: 1326 loss: 1.46419654e-06
Iter: 1327 loss: 1.46328e-06
Iter: 1328 loss: 1.47102378e-06
Iter: 1329 loss: 1.46317313e-06
Iter: 1330 loss: 1.46253535e-06
Iter: 1331 loss: 1.46468869e-06
Iter: 1332 loss: 1.46234788e-06
Iter: 1333 loss: 1.46145362e-06
Iter: 1334 loss: 1.46150501e-06
Iter: 1335 loss: 1.46071739e-06
Iter: 1336 loss: 1.45986951e-06
Iter: 1337 loss: 1.46378989e-06
Iter: 1338 loss: 1.45972956e-06
Iter: 1339 loss: 1.45939021e-06
Iter: 1340 loss: 1.45918204e-06
Iter: 1341 loss: 1.45892568e-06
Iter: 1342 loss: 1.4582472e-06
Iter: 1343 loss: 1.46215177e-06
Iter: 1344 loss: 1.45795389e-06
Iter: 1345 loss: 1.45690728e-06
Iter: 1346 loss: 1.45992567e-06
Iter: 1347 loss: 1.45663944e-06
Iter: 1348 loss: 1.45600552e-06
Iter: 1349 loss: 1.45599836e-06
Iter: 1350 loss: 1.45549916e-06
Iter: 1351 loss: 1.45452839e-06
Iter: 1352 loss: 1.46142781e-06
Iter: 1353 loss: 1.45423e-06
Iter: 1354 loss: 1.45323872e-06
Iter: 1355 loss: 1.4670369e-06
Iter: 1356 loss: 1.45326339e-06
Iter: 1357 loss: 1.45234844e-06
Iter: 1358 loss: 1.45584727e-06
Iter: 1359 loss: 1.45212698e-06
Iter: 1360 loss: 1.45140245e-06
Iter: 1361 loss: 1.45356842e-06
Iter: 1362 loss: 1.4511885e-06
Iter: 1363 loss: 1.45058459e-06
Iter: 1364 loss: 1.45005788e-06
Iter: 1365 loss: 1.44979413e-06
Iter: 1366 loss: 1.44892897e-06
Iter: 1367 loss: 1.45992203e-06
Iter: 1368 loss: 1.44893227e-06
Iter: 1369 loss: 1.44831324e-06
Iter: 1370 loss: 1.44929572e-06
Iter: 1371 loss: 1.44805915e-06
Iter: 1372 loss: 1.44726209e-06
Iter: 1373 loss: 1.45079366e-06
Iter: 1374 loss: 1.44718774e-06
Iter: 1375 loss: 1.44650403e-06
Iter: 1376 loss: 1.4541896e-06
Iter: 1377 loss: 1.44648232e-06
Iter: 1378 loss: 1.44606634e-06
Iter: 1379 loss: 1.44522824e-06
Iter: 1380 loss: 1.45604872e-06
Iter: 1381 loss: 1.4451914e-06
Iter: 1382 loss: 1.44414696e-06
Iter: 1383 loss: 1.4477971e-06
Iter: 1384 loss: 1.44390879e-06
Iter: 1385 loss: 1.44327544e-06
Iter: 1386 loss: 1.44541366e-06
Iter: 1387 loss: 1.4430309e-06
Iter: 1388 loss: 1.44198134e-06
Iter: 1389 loss: 1.44638341e-06
Iter: 1390 loss: 1.44186095e-06
Iter: 1391 loss: 1.44132e-06
Iter: 1392 loss: 1.44061198e-06
Iter: 1393 loss: 1.44057083e-06
Iter: 1394 loss: 1.43959574e-06
Iter: 1395 loss: 1.44437922e-06
Iter: 1396 loss: 1.43935608e-06
Iter: 1397 loss: 1.43843135e-06
Iter: 1398 loss: 1.44289174e-06
Iter: 1399 loss: 1.43825e-06
Iter: 1400 loss: 1.43746877e-06
Iter: 1401 loss: 1.43823502e-06
Iter: 1402 loss: 1.43687532e-06
Iter: 1403 loss: 1.43612328e-06
Iter: 1404 loss: 1.43628813e-06
Iter: 1405 loss: 1.43555189e-06
Iter: 1406 loss: 1.43482202e-06
Iter: 1407 loss: 1.4347911e-06
Iter: 1408 loss: 1.43433317e-06
Iter: 1409 loss: 1.43904128e-06
Iter: 1410 loss: 1.43435273e-06
Iter: 1411 loss: 1.43388615e-06
Iter: 1412 loss: 1.43369493e-06
Iter: 1413 loss: 1.43343334e-06
Iter: 1414 loss: 1.43286093e-06
Iter: 1415 loss: 1.4322942e-06
Iter: 1416 loss: 1.43224042e-06
Iter: 1417 loss: 1.43131376e-06
Iter: 1418 loss: 1.43200782e-06
Iter: 1419 loss: 1.43088982e-06
Iter: 1420 loss: 1.42997237e-06
Iter: 1421 loss: 1.43000671e-06
Iter: 1422 loss: 1.4293588e-06
Iter: 1423 loss: 1.43068519e-06
Iter: 1424 loss: 1.42910483e-06
Iter: 1425 loss: 1.42851309e-06
Iter: 1426 loss: 1.42733302e-06
Iter: 1427 loss: 1.44700857e-06
Iter: 1428 loss: 1.42740396e-06
Iter: 1429 loss: 1.42639146e-06
Iter: 1430 loss: 1.44010619e-06
Iter: 1431 loss: 1.42632871e-06
Iter: 1432 loss: 1.42548629e-06
Iter: 1433 loss: 1.42972806e-06
Iter: 1434 loss: 1.42536965e-06
Iter: 1435 loss: 1.4245594e-06
Iter: 1436 loss: 1.42553677e-06
Iter: 1437 loss: 1.42417912e-06
Iter: 1438 loss: 1.42336364e-06
Iter: 1439 loss: 1.42280226e-06
Iter: 1440 loss: 1.42252259e-06
Iter: 1441 loss: 1.42165254e-06
Iter: 1442 loss: 1.43456305e-06
Iter: 1443 loss: 1.42168733e-06
Iter: 1444 loss: 1.42114072e-06
Iter: 1445 loss: 1.42113333e-06
Iter: 1446 loss: 1.42081683e-06
Iter: 1447 loss: 1.42021827e-06
Iter: 1448 loss: 1.42019599e-06
Iter: 1449 loss: 1.41956366e-06
Iter: 1450 loss: 1.42136753e-06
Iter: 1451 loss: 1.41932787e-06
Iter: 1452 loss: 1.41867486e-06
Iter: 1453 loss: 1.41873488e-06
Iter: 1454 loss: 1.41823102e-06
Iter: 1455 loss: 1.41750866e-06
Iter: 1456 loss: 1.42239992e-06
Iter: 1457 loss: 1.4174941e-06
Iter: 1458 loss: 1.41676946e-06
Iter: 1459 loss: 1.41824603e-06
Iter: 1460 loss: 1.41647388e-06
Iter: 1461 loss: 1.41579346e-06
Iter: 1462 loss: 1.41787973e-06
Iter: 1463 loss: 1.41555051e-06
Iter: 1464 loss: 1.41496957e-06
Iter: 1465 loss: 1.41430792e-06
Iter: 1466 loss: 1.41413625e-06
Iter: 1467 loss: 1.41318333e-06
Iter: 1468 loss: 1.41438045e-06
Iter: 1469 loss: 1.41274791e-06
Iter: 1470 loss: 1.41161649e-06
Iter: 1471 loss: 1.42477347e-06
Iter: 1472 loss: 1.41169039e-06
Iter: 1473 loss: 1.41103692e-06
Iter: 1474 loss: 1.4162988e-06
Iter: 1475 loss: 1.41104988e-06
Iter: 1476 loss: 1.41057103e-06
Iter: 1477 loss: 1.40986788e-06
Iter: 1478 loss: 1.40980808e-06
Iter: 1479 loss: 1.40972293e-06
Iter: 1480 loss: 1.40933957e-06
Iter: 1481 loss: 1.4089652e-06
Iter: 1482 loss: 1.40828047e-06
Iter: 1483 loss: 1.41952671e-06
Iter: 1484 loss: 1.40820339e-06
Iter: 1485 loss: 1.40757038e-06
Iter: 1486 loss: 1.40764507e-06
Iter: 1487 loss: 1.40708914e-06
Iter: 1488 loss: 1.40623729e-06
Iter: 1489 loss: 1.40837665e-06
Iter: 1490 loss: 1.40587792e-06
Iter: 1491 loss: 1.40502948e-06
Iter: 1492 loss: 1.4150155e-06
Iter: 1493 loss: 1.40503676e-06
Iter: 1494 loss: 1.4043884e-06
Iter: 1495 loss: 1.40591931e-06
Iter: 1496 loss: 1.40414886e-06
Iter: 1497 loss: 1.40351699e-06
Iter: 1498 loss: 1.40423913e-06
Iter: 1499 loss: 1.40313e-06
Iter: 1500 loss: 1.40226325e-06
Iter: 1501 loss: 1.40507632e-06
Iter: 1502 loss: 1.40197562e-06
Iter: 1503 loss: 1.40139423e-06
Iter: 1504 loss: 1.40176803e-06
Iter: 1505 loss: 1.40095563e-06
Iter: 1506 loss: 1.40022303e-06
Iter: 1507 loss: 1.40382122e-06
Iter: 1508 loss: 1.40010218e-06
Iter: 1509 loss: 1.39946201e-06
Iter: 1510 loss: 1.39870053e-06
Iter: 1511 loss: 1.39861277e-06
Iter: 1512 loss: 1.39818462e-06
Iter: 1513 loss: 1.3979859e-06
Iter: 1514 loss: 1.39750614e-06
Iter: 1515 loss: 1.39967563e-06
Iter: 1516 loss: 1.39744486e-06
Iter: 1517 loss: 1.39692088e-06
Iter: 1518 loss: 1.39639769e-06
Iter: 1519 loss: 1.3963288e-06
Iter: 1520 loss: 1.39561246e-06
Iter: 1521 loss: 1.39589338e-06
Iter: 1522 loss: 1.39513e-06
Iter: 1523 loss: 1.39425197e-06
Iter: 1524 loss: 1.39427198e-06
Iter: 1525 loss: 1.39350459e-06
Iter: 1526 loss: 1.39243389e-06
Iter: 1527 loss: 1.39663166e-06
Iter: 1528 loss: 1.3922006e-06
Iter: 1529 loss: 1.39128383e-06
Iter: 1530 loss: 1.40531654e-06
Iter: 1531 loss: 1.39126325e-06
Iter: 1532 loss: 1.39053077e-06
Iter: 1533 loss: 1.39095641e-06
Iter: 1534 loss: 1.3901423e-06
Iter: 1535 loss: 1.38943483e-06
Iter: 1536 loss: 1.3905485e-06
Iter: 1537 loss: 1.38911128e-06
Iter: 1538 loss: 1.38843222e-06
Iter: 1539 loss: 1.39468284e-06
Iter: 1540 loss: 1.38834071e-06
Iter: 1541 loss: 1.38785583e-06
Iter: 1542 loss: 1.38671146e-06
Iter: 1543 loss: 1.40696534e-06
Iter: 1544 loss: 1.38663518e-06
Iter: 1545 loss: 1.38568271e-06
Iter: 1546 loss: 1.39751774e-06
Iter: 1547 loss: 1.385735e-06
Iter: 1548 loss: 1.3852391e-06
Iter: 1549 loss: 1.3852507e-06
Iter: 1550 loss: 1.38460882e-06
Iter: 1551 loss: 1.3853014e-06
Iter: 1552 loss: 1.38435951e-06
Iter: 1553 loss: 1.38383166e-06
Iter: 1554 loss: 1.38481926e-06
Iter: 1555 loss: 1.3836127e-06
Iter: 1556 loss: 1.38305381e-06
Iter: 1557 loss: 1.382248e-06
Iter: 1558 loss: 1.38229507e-06
Iter: 1559 loss: 1.38135169e-06
Iter: 1560 loss: 1.38339578e-06
Iter: 1561 loss: 1.38103303e-06
Iter: 1562 loss: 1.37985955e-06
Iter: 1563 loss: 1.38243865e-06
Iter: 1564 loss: 1.37938923e-06
Iter: 1565 loss: 1.37870506e-06
Iter: 1566 loss: 1.37865811e-06
Iter: 1567 loss: 1.37818893e-06
Iter: 1568 loss: 1.37811617e-06
Iter: 1569 loss: 1.37776851e-06
Iter: 1570 loss: 1.37707434e-06
Iter: 1571 loss: 1.37784059e-06
Iter: 1572 loss: 1.37657685e-06
Iter: 1573 loss: 1.37584811e-06
Iter: 1574 loss: 1.37963639e-06
Iter: 1575 loss: 1.37573795e-06
Iter: 1576 loss: 1.37499569e-06
Iter: 1577 loss: 1.37640427e-06
Iter: 1578 loss: 1.37466304e-06
Iter: 1579 loss: 1.37398251e-06
Iter: 1580 loss: 1.37634993e-06
Iter: 1581 loss: 1.37379288e-06
Iter: 1582 loss: 1.3731393e-06
Iter: 1583 loss: 1.3765906e-06
Iter: 1584 loss: 1.37304039e-06
Iter: 1585 loss: 1.37227323e-06
Iter: 1586 loss: 1.37363918e-06
Iter: 1587 loss: 1.37189954e-06
Iter: 1588 loss: 1.37149459e-06
Iter: 1589 loss: 1.37129757e-06
Iter: 1590 loss: 1.37105053e-06
Iter: 1591 loss: 1.37046163e-06
Iter: 1592 loss: 1.37455299e-06
Iter: 1593 loss: 1.37039353e-06
Iter: 1594 loss: 1.36983795e-06
Iter: 1595 loss: 1.36921574e-06
Iter: 1596 loss: 1.36911149e-06
Iter: 1597 loss: 1.36829101e-06
Iter: 1598 loss: 1.37291647e-06
Iter: 1599 loss: 1.36810422e-06
Iter: 1600 loss: 1.36741278e-06
Iter: 1601 loss: 1.37055156e-06
Iter: 1602 loss: 1.36730932e-06
Iter: 1603 loss: 1.36663732e-06
Iter: 1604 loss: 1.36892925e-06
Iter: 1605 loss: 1.36636652e-06
Iter: 1606 loss: 1.36575602e-06
Iter: 1607 loss: 1.36612584e-06
Iter: 1608 loss: 1.36539461e-06
Iter: 1609 loss: 1.36467224e-06
Iter: 1610 loss: 1.36637709e-06
Iter: 1611 loss: 1.36440576e-06
Iter: 1612 loss: 1.36369943e-06
Iter: 1613 loss: 1.3692345e-06
Iter: 1614 loss: 1.36370568e-06
Iter: 1615 loss: 1.36316123e-06
Iter: 1616 loss: 1.36225083e-06
Iter: 1617 loss: 1.36223548e-06
Iter: 1618 loss: 1.36256176e-06
Iter: 1619 loss: 1.36174208e-06
Iter: 1620 loss: 1.36150425e-06
Iter: 1621 loss: 1.36087306e-06
Iter: 1622 loss: 1.37125471e-06
Iter: 1623 loss: 1.36095355e-06
Iter: 1624 loss: 1.36026779e-06
Iter: 1625 loss: 1.36054678e-06
Iter: 1626 loss: 1.35982145e-06
Iter: 1627 loss: 1.35905225e-06
Iter: 1628 loss: 1.36644951e-06
Iter: 1629 loss: 1.35902951e-06
Iter: 1630 loss: 1.35847404e-06
Iter: 1631 loss: 1.35797768e-06
Iter: 1632 loss: 1.35778646e-06
Iter: 1633 loss: 1.35699474e-06
Iter: 1634 loss: 1.36128892e-06
Iter: 1635 loss: 1.35688663e-06
Iter: 1636 loss: 1.35626362e-06
Iter: 1637 loss: 1.35580535e-06
Iter: 1638 loss: 1.35559378e-06
Iter: 1639 loss: 1.35479672e-06
Iter: 1640 loss: 1.35471737e-06
Iter: 1641 loss: 1.35438415e-06
Iter: 1642 loss: 1.35360472e-06
Iter: 1643 loss: 1.36874883e-06
Iter: 1644 loss: 1.35353548e-06
Iter: 1645 loss: 1.35271921e-06
Iter: 1646 loss: 1.35675771e-06
Iter: 1647 loss: 1.35257017e-06
Iter: 1648 loss: 1.35173536e-06
Iter: 1649 loss: 1.35614982e-06
Iter: 1650 loss: 1.35156733e-06
Iter: 1651 loss: 1.35076777e-06
Iter: 1652 loss: 1.3542965e-06
Iter: 1653 loss: 1.35066136e-06
Iter: 1654 loss: 1.35005598e-06
Iter: 1655 loss: 1.35015989e-06
Iter: 1656 loss: 1.34965796e-06
Iter: 1657 loss: 1.34890547e-06
Iter: 1658 loss: 1.35097707e-06
Iter: 1659 loss: 1.34865923e-06
Iter: 1660 loss: 1.34837569e-06
Iter: 1661 loss: 1.34827133e-06
Iter: 1662 loss: 1.34795823e-06
Iter: 1663 loss: 1.34726861e-06
Iter: 1664 loss: 1.35824689e-06
Iter: 1665 loss: 1.34726724e-06
Iter: 1666 loss: 1.346712e-06
Iter: 1667 loss: 1.35006496e-06
Iter: 1668 loss: 1.34654078e-06
Iter: 1669 loss: 1.34598849e-06
Iter: 1670 loss: 1.34589845e-06
Iter: 1671 loss: 1.3454428e-06
Iter: 1672 loss: 1.3447883e-06
Iter: 1673 loss: 1.34976949e-06
Iter: 1674 loss: 1.34458583e-06
Iter: 1675 loss: 1.34404399e-06
Iter: 1676 loss: 1.34655158e-06
Iter: 1677 loss: 1.34395214e-06
Iter: 1678 loss: 1.34339371e-06
Iter: 1679 loss: 1.34353138e-06
Iter: 1680 loss: 1.3429792e-06
Iter: 1681 loss: 1.34218044e-06
Iter: 1682 loss: 1.34324682e-06
Iter: 1683 loss: 1.34188929e-06
Iter: 1684 loss: 1.34095399e-06
Iter: 1685 loss: 1.34131074e-06
Iter: 1686 loss: 1.3403137e-06
Iter: 1687 loss: 1.33944354e-06
Iter: 1688 loss: 1.34612901e-06
Iter: 1689 loss: 1.33938056e-06
Iter: 1690 loss: 1.338478e-06
Iter: 1691 loss: 1.34228878e-06
Iter: 1692 loss: 1.33833032e-06
Iter: 1693 loss: 1.33759272e-06
Iter: 1694 loss: 1.33887613e-06
Iter: 1695 loss: 1.33737592e-06
Iter: 1696 loss: 1.33696199e-06
Iter: 1697 loss: 1.33697199e-06
Iter: 1698 loss: 1.33639151e-06
Iter: 1699 loss: 1.33558626e-06
Iter: 1700 loss: 1.33560889e-06
Iter: 1701 loss: 1.33506694e-06
Iter: 1702 loss: 1.33418405e-06
Iter: 1703 loss: 1.3341853e-06
Iter: 1704 loss: 1.33406502e-06
Iter: 1705 loss: 1.33371191e-06
Iter: 1706 loss: 1.33341825e-06
Iter: 1707 loss: 1.3325074e-06
Iter: 1708 loss: 1.34285551e-06
Iter: 1709 loss: 1.33255503e-06
Iter: 1710 loss: 1.33158039e-06
Iter: 1711 loss: 1.33565686e-06
Iter: 1712 loss: 1.33142328e-06
Iter: 1713 loss: 1.33082369e-06
Iter: 1714 loss: 1.33075582e-06
Iter: 1715 loss: 1.33037872e-06
Iter: 1716 loss: 1.33005415e-06
Iter: 1717 loss: 1.32992204e-06
Iter: 1718 loss: 1.32921321e-06
Iter: 1719 loss: 1.32964112e-06
Iter: 1720 loss: 1.32866671e-06
Iter: 1721 loss: 1.32787318e-06
Iter: 1722 loss: 1.3326985e-06
Iter: 1723 loss: 1.3277737e-06
Iter: 1724 loss: 1.32715718e-06
Iter: 1725 loss: 1.32855268e-06
Iter: 1726 loss: 1.32695754e-06
Iter: 1727 loss: 1.32639047e-06
Iter: 1728 loss: 1.32979585e-06
Iter: 1729 loss: 1.32624791e-06
Iter: 1730 loss: 1.32555192e-06
Iter: 1731 loss: 1.33097956e-06
Iter: 1732 loss: 1.32555169e-06
Iter: 1733 loss: 1.32516971e-06
Iter: 1734 loss: 1.32486934e-06
Iter: 1735 loss: 1.32477953e-06
Iter: 1736 loss: 1.32424225e-06
Iter: 1737 loss: 1.32383366e-06
Iter: 1738 loss: 1.32367279e-06
Iter: 1739 loss: 1.32293712e-06
Iter: 1740 loss: 1.3265676e-06
Iter: 1741 loss: 1.32273249e-06
Iter: 1742 loss: 1.3219036e-06
Iter: 1743 loss: 1.32423963e-06
Iter: 1744 loss: 1.32162677e-06
Iter: 1745 loss: 1.32102878e-06
Iter: 1746 loss: 1.32176217e-06
Iter: 1747 loss: 1.3207989e-06
Iter: 1748 loss: 1.32013497e-06
Iter: 1749 loss: 1.32318144e-06
Iter: 1750 loss: 1.31994557e-06
Iter: 1751 loss: 1.31926561e-06
Iter: 1752 loss: 1.32285049e-06
Iter: 1753 loss: 1.31918023e-06
Iter: 1754 loss: 1.31880529e-06
Iter: 1755 loss: 1.31831689e-06
Iter: 1756 loss: 1.31829051e-06
Iter: 1757 loss: 1.31755951e-06
Iter: 1758 loss: 1.32014088e-06
Iter: 1759 loss: 1.31736954e-06
Iter: 1760 loss: 1.31660738e-06
Iter: 1761 loss: 1.31901879e-06
Iter: 1762 loss: 1.31643014e-06
Iter: 1763 loss: 1.31599882e-06
Iter: 1764 loss: 1.31591537e-06
Iter: 1765 loss: 1.31563661e-06
Iter: 1766 loss: 1.31579418e-06
Iter: 1767 loss: 1.31533875e-06
Iter: 1768 loss: 1.31493687e-06
Iter: 1769 loss: 1.31396189e-06
Iter: 1770 loss: 1.32893035e-06
Iter: 1771 loss: 1.31397587e-06
Iter: 1772 loss: 1.31317302e-06
Iter: 1773 loss: 1.31667537e-06
Iter: 1774 loss: 1.31295894e-06
Iter: 1775 loss: 1.31201227e-06
Iter: 1776 loss: 1.31558784e-06
Iter: 1777 loss: 1.31179081e-06
Iter: 1778 loss: 1.3113123e-06
Iter: 1779 loss: 1.31634772e-06
Iter: 1780 loss: 1.31133504e-06
Iter: 1781 loss: 1.31086153e-06
Iter: 1782 loss: 1.30988849e-06
Iter: 1783 loss: 1.32832861e-06
Iter: 1784 loss: 1.3098828e-06
Iter: 1785 loss: 1.30934427e-06
Iter: 1786 loss: 1.30932335e-06
Iter: 1787 loss: 1.30885064e-06
Iter: 1788 loss: 1.30966623e-06
Iter: 1789 loss: 1.30864169e-06
Iter: 1790 loss: 1.30820638e-06
Iter: 1791 loss: 1.30775823e-06
Iter: 1792 loss: 1.30768399e-06
Iter: 1793 loss: 1.30698106e-06
Iter: 1794 loss: 1.3068526e-06
Iter: 1795 loss: 1.30632361e-06
Iter: 1796 loss: 1.30629974e-06
Iter: 1797 loss: 1.30596936e-06
Iter: 1798 loss: 1.30553087e-06
Iter: 1799 loss: 1.30597152e-06
Iter: 1800 loss: 1.30530225e-06
Iter: 1801 loss: 1.304752e-06
Iter: 1802 loss: 1.30518424e-06
Iter: 1803 loss: 1.30448166e-06
Iter: 1804 loss: 1.30382978e-06
Iter: 1805 loss: 1.30437206e-06
Iter: 1806 loss: 1.30352123e-06
Iter: 1807 loss: 1.30298849e-06
Iter: 1808 loss: 1.30359683e-06
Iter: 1809 loss: 1.30263379e-06
Iter: 1810 loss: 1.30202557e-06
Iter: 1811 loss: 1.3061383e-06
Iter: 1812 loss: 1.30202216e-06
Iter: 1813 loss: 1.30137914e-06
Iter: 1814 loss: 1.30181024e-06
Iter: 1815 loss: 1.30092963e-06
Iter: 1816 loss: 1.30020862e-06
Iter: 1817 loss: 1.30296746e-06
Iter: 1818 loss: 1.30007879e-06
Iter: 1819 loss: 1.29956493e-06
Iter: 1820 loss: 1.30098147e-06
Iter: 1821 loss: 1.29937825e-06
Iter: 1822 loss: 1.29872615e-06
Iter: 1823 loss: 1.29974728e-06
Iter: 1824 loss: 1.29834098e-06
Iter: 1825 loss: 1.29762577e-06
Iter: 1826 loss: 1.29911734e-06
Iter: 1827 loss: 1.29726141e-06
Iter: 1828 loss: 1.29658542e-06
Iter: 1829 loss: 1.29720502e-06
Iter: 1830 loss: 1.29622742e-06
Iter: 1831 loss: 1.29561e-06
Iter: 1832 loss: 1.29563614e-06
Iter: 1833 loss: 1.2951034e-06
Iter: 1834 loss: 1.29730483e-06
Iter: 1835 loss: 1.29502052e-06
Iter: 1836 loss: 1.2946947e-06
Iter: 1837 loss: 1.29398165e-06
Iter: 1838 loss: 1.30709623e-06
Iter: 1839 loss: 1.29404657e-06
Iter: 1840 loss: 1.29328646e-06
Iter: 1841 loss: 1.30174544e-06
Iter: 1842 loss: 1.29325463e-06
Iter: 1843 loss: 1.29283478e-06
Iter: 1844 loss: 1.29217892e-06
Iter: 1845 loss: 1.29215118e-06
Iter: 1846 loss: 1.2912958e-06
Iter: 1847 loss: 1.29883801e-06
Iter: 1848 loss: 1.29124646e-06
Iter: 1849 loss: 1.29070816e-06
Iter: 1850 loss: 1.29379055e-06
Iter: 1851 loss: 1.29057071e-06
Iter: 1852 loss: 1.29004184e-06
Iter: 1853 loss: 1.29085311e-06
Iter: 1854 loss: 1.28970589e-06
Iter: 1855 loss: 1.28917861e-06
Iter: 1856 loss: 1.28951774e-06
Iter: 1857 loss: 1.28887064e-06
Iter: 1858 loss: 1.28804515e-06
Iter: 1859 loss: 1.2932868e-06
Iter: 1860 loss: 1.28798479e-06
Iter: 1861 loss: 1.28751e-06
Iter: 1862 loss: 1.28802799e-06
Iter: 1863 loss: 1.28736656e-06
Iter: 1864 loss: 1.2868154e-06
Iter: 1865 loss: 1.28658303e-06
Iter: 1866 loss: 1.28634588e-06
Iter: 1867 loss: 1.28596753e-06
Iter: 1868 loss: 1.2859565e-06
Iter: 1869 loss: 1.28554871e-06
Iter: 1870 loss: 1.28589204e-06
Iter: 1871 loss: 1.28532588e-06
Iter: 1872 loss: 1.2850337e-06
Iter: 1873 loss: 1.28443583e-06
Iter: 1874 loss: 1.29664159e-06
Iter: 1875 loss: 1.28444094e-06
Iter: 1876 loss: 1.28380475e-06
Iter: 1877 loss: 1.28922375e-06
Iter: 1878 loss: 1.28368538e-06
Iter: 1879 loss: 1.28314218e-06
Iter: 1880 loss: 1.28314036e-06
Iter: 1881 loss: 1.28274473e-06
Iter: 1882 loss: 1.28208808e-06
Iter: 1883 loss: 1.28399006e-06
Iter: 1884 loss: 1.28187128e-06
Iter: 1885 loss: 1.28125851e-06
Iter: 1886 loss: 1.28511408e-06
Iter: 1887 loss: 1.28119723e-06
Iter: 1888 loss: 1.28045099e-06
Iter: 1889 loss: 1.28116471e-06
Iter: 1890 loss: 1.2802185e-06
Iter: 1891 loss: 1.27950057e-06
Iter: 1892 loss: 1.27966723e-06
Iter: 1893 loss: 1.2790747e-06
Iter: 1894 loss: 1.27848807e-06
Iter: 1895 loss: 1.27851024e-06
Iter: 1896 loss: 1.278083e-06
Iter: 1897 loss: 1.27771636e-06
Iter: 1898 loss: 1.27757846e-06
Iter: 1899 loss: 1.27683177e-06
Iter: 1900 loss: 1.27673138e-06
Iter: 1901 loss: 1.2761725e-06
Iter: 1902 loss: 1.275415e-06
Iter: 1903 loss: 1.28001716e-06
Iter: 1904 loss: 1.2753809e-06
Iter: 1905 loss: 1.2748344e-06
Iter: 1906 loss: 1.27481508e-06
Iter: 1907 loss: 1.27442661e-06
Iter: 1908 loss: 1.27373175e-06
Iter: 1909 loss: 1.27373278e-06
Iter: 1910 loss: 1.27322824e-06
Iter: 1911 loss: 1.27311557e-06
Iter: 1912 loss: 1.27272e-06
Iter: 1913 loss: 1.27188923e-06
Iter: 1914 loss: 1.27634542e-06
Iter: 1915 loss: 1.27176747e-06
Iter: 1916 loss: 1.27110252e-06
Iter: 1917 loss: 1.27311932e-06
Iter: 1918 loss: 1.27083808e-06
Iter: 1919 loss: 1.270231e-06
Iter: 1920 loss: 1.27372948e-06
Iter: 1921 loss: 1.27017256e-06
Iter: 1922 loss: 1.26965051e-06
Iter: 1923 loss: 1.26888347e-06
Iter: 1924 loss: 1.26894327e-06
Iter: 1925 loss: 1.26812699e-06
Iter: 1926 loss: 1.2789485e-06
Iter: 1927 loss: 1.26813859e-06
Iter: 1928 loss: 1.26752627e-06
Iter: 1929 loss: 1.27029284e-06
Iter: 1930 loss: 1.26743475e-06
Iter: 1931 loss: 1.26685723e-06
Iter: 1932 loss: 1.26674718e-06
Iter: 1933 loss: 1.26636132e-06
Iter: 1934 loss: 1.2657265e-06
Iter: 1935 loss: 1.26731402e-06
Iter: 1936 loss: 1.26545729e-06
Iter: 1937 loss: 1.26455757e-06
Iter: 1938 loss: 1.27144449e-06
Iter: 1939 loss: 1.26449822e-06
Iter: 1940 loss: 1.26404632e-06
Iter: 1941 loss: 1.26738917e-06
Iter: 1942 loss: 1.26402256e-06
Iter: 1943 loss: 1.26360737e-06
Iter: 1944 loss: 1.2653295e-06
Iter: 1945 loss: 1.26354121e-06
Iter: 1946 loss: 1.26329337e-06
Iter: 1947 loss: 1.26248074e-06
Iter: 1948 loss: 1.26654072e-06
Iter: 1949 loss: 1.26220129e-06
Iter: 1950 loss: 1.26145665e-06
Iter: 1951 loss: 1.26788007e-06
Iter: 1952 loss: 1.26147904e-06
Iter: 1953 loss: 1.2607552e-06
Iter: 1954 loss: 1.26056261e-06
Iter: 1955 loss: 1.26012742e-06
Iter: 1956 loss: 1.25942847e-06
Iter: 1957 loss: 1.2700167e-06
Iter: 1958 loss: 1.2594262e-06
Iter: 1959 loss: 1.25883639e-06
Iter: 1960 loss: 1.25978659e-06
Iter: 1961 loss: 1.25853944e-06
Iter: 1962 loss: 1.25807856e-06
Iter: 1963 loss: 1.26185341e-06
Iter: 1964 loss: 1.25801512e-06
Iter: 1965 loss: 1.25749e-06
Iter: 1966 loss: 1.25709857e-06
Iter: 1967 loss: 1.25692145e-06
Iter: 1968 loss: 1.25630163e-06
Iter: 1969 loss: 1.26220289e-06
Iter: 1970 loss: 1.25626207e-06
Iter: 1971 loss: 1.25574479e-06
Iter: 1972 loss: 1.25625479e-06
Iter: 1973 loss: 1.25535394e-06
Iter: 1974 loss: 1.25480199e-06
Iter: 1975 loss: 1.25596944e-06
Iter: 1976 loss: 1.25457291e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ exit 1
