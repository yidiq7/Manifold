+ RUN=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ LAYERS=300_300_300_1
+ case $RUN in
+ PSI=2
+ OPTIONS='			 --optimizer adam 				 --n_pairs 50000 				 --batch_size 5000 				 --max_epochs 200 				 --learning_rate 0.001 				 --decay_rate 0.98 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output78
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output79
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL='--load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi0
+ date
Sat Oct 31 17:27:30 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30536d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b3054b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30536730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b3049a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b3049a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b3049aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30447d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30163a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30163bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b301ed048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30207bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30251f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30251d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30251ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30250158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30250ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b302bcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30096620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30049ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ae477e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ae477e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ae4780bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ae4743b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ae476ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ae47bf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ae47bfd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ae47dda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30235a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30235840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b300e41e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b300e91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30221d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b30221730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ae46799d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ae4674840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6ae4647d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0027604438
test_loss: 0.0027993263
train_loss: 0.0025998305
test_loss: 0.0028554876
train_loss: 0.0022750774
test_loss: 0.0022105114
train_loss: 0.0020306038
test_loss: 0.0021281037
train_loss: 0.0021059373
test_loss: 0.0021140543
train_loss: 0.0017852054
test_loss: 0.0019360001
train_loss: 0.001739403
test_loss: 0.0018019069
train_loss: 0.0015849904
test_loss: 0.0016934851
train_loss: 0.0015707044
test_loss: 0.0015890944
train_loss: 0.0015622717
test_loss: 0.0016035193
train_loss: 0.0013883507
test_loss: 0.0015275056
train_loss: 0.0013583144
test_loss: 0.0014618083
train_loss: 0.001279147
test_loss: 0.0014269646
train_loss: 0.0012563198
test_loss: 0.0013651722
train_loss: 0.0012532205
test_loss: 0.0013520229
train_loss: 0.0012093945
test_loss: 0.0013266891
train_loss: 0.0011776058
test_loss: 0.0012920523
train_loss: 0.001161025
test_loss: 0.0012917558
train_loss: 0.001103247
test_loss: 0.001274559
train_loss: 0.001070855
test_loss: 0.0012566121
train_loss: 0.0010995099
test_loss: 0.0012361647
train_loss: 0.001037623
test_loss: 0.0012183577
train_loss: 0.0010068659
test_loss: 0.0011967875
train_loss: 0.0010057513
test_loss: 0.0011815962
train_loss: 0.0010171158
test_loss: 0.0011775902
train_loss: 0.0010039856
test_loss: 0.0011679482
train_loss: 0.0009802033
test_loss: 0.0011601973
train_loss: 0.0009840559
test_loss: 0.0011534776
train_loss: 0.0009299889
test_loss: 0.0011518283
train_loss: 0.00090500317
test_loss: 0.0011390704
train_loss: 0.0009431654
test_loss: 0.0011381494
train_loss: 0.00090163073
test_loss: 0.0011306145
train_loss: 0.00090830016
test_loss: 0.001129314
train_loss: 0.0009136374
test_loss: 0.0011184736
train_loss: 0.00095161307
test_loss: 0.0011197391
train_loss: 0.00090397947
test_loss: 0.0011164071
train_loss: 0.0008645932
test_loss: 0.0011089457
train_loss: 0.00090150564
test_loss: 0.0011091153
train_loss: 0.000922092
test_loss: 0.0011087697
train_loss: 0.0008541322
test_loss: 0.0011052609
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi0/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe42a42a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe42a488b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe42a488488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe42a42a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3fe738598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3fe716d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3fe749e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3fe749598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3fe7168c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3fe64f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3fe64fae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3fe6a1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3fe6a1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3fe6b7ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3fe6b7e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3fe6b7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d85fc488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d85fc268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d85fc950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d85ec730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d85ec6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d86ca8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d850db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d851cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d851f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d855c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d84948c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d855c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d84b3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d8460400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d84b3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d83d0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d83f2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d837dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d837dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fe3d837d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.50332244e-06
Iter: 2 loss: 1.49313644e-06
Iter: 3 loss: 1.49251559e-06
Iter: 4 loss: 1.48638355e-06
Iter: 5 loss: 1.48693164e-06
Iter: 6 loss: 1.48166077e-06
Iter: 7 loss: 1.478748e-06
Iter: 8 loss: 1.52353016e-06
Iter: 9 loss: 1.4787413e-06
Iter: 10 loss: 1.47560502e-06
Iter: 11 loss: 1.46929517e-06
Iter: 12 loss: 1.58822741e-06
Iter: 13 loss: 1.46921366e-06
Iter: 14 loss: 1.46685306e-06
Iter: 15 loss: 1.46682737e-06
Iter: 16 loss: 1.46393677e-06
Iter: 17 loss: 1.45907006e-06
Iter: 18 loss: 1.45905108e-06
Iter: 19 loss: 1.45412605e-06
Iter: 20 loss: 1.45672698e-06
Iter: 21 loss: 1.45085767e-06
Iter: 22 loss: 1.44466412e-06
Iter: 23 loss: 1.49793402e-06
Iter: 24 loss: 1.44430703e-06
Iter: 25 loss: 1.44199248e-06
Iter: 26 loss: 1.4531397e-06
Iter: 27 loss: 1.44158753e-06
Iter: 28 loss: 1.43921193e-06
Iter: 29 loss: 1.45729405e-06
Iter: 30 loss: 1.43904367e-06
Iter: 31 loss: 1.43752914e-06
Iter: 32 loss: 1.43323348e-06
Iter: 33 loss: 1.45328249e-06
Iter: 34 loss: 1.43171701e-06
Iter: 35 loss: 1.42597082e-06
Iter: 36 loss: 1.44400019e-06
Iter: 37 loss: 1.42431236e-06
Iter: 38 loss: 1.42453518e-06
Iter: 39 loss: 1.42224337e-06
Iter: 40 loss: 1.4200441e-06
Iter: 41 loss: 1.41655664e-06
Iter: 42 loss: 1.41653288e-06
Iter: 43 loss: 1.41652174e-06
Iter: 44 loss: 1.41543489e-06
Iter: 45 loss: 1.41460532e-06
Iter: 46 loss: 1.41245891e-06
Iter: 47 loss: 1.42935028e-06
Iter: 48 loss: 1.41202815e-06
Iter: 49 loss: 1.41024202e-06
Iter: 50 loss: 1.43153079e-06
Iter: 51 loss: 1.41023247e-06
Iter: 52 loss: 1.40806287e-06
Iter: 53 loss: 1.40695283e-06
Iter: 54 loss: 1.40589282e-06
Iter: 55 loss: 1.4039922e-06
Iter: 56 loss: 1.40282748e-06
Iter: 57 loss: 1.40204634e-06
Iter: 58 loss: 1.4005459e-06
Iter: 59 loss: 1.40047393e-06
Iter: 60 loss: 1.39927647e-06
Iter: 61 loss: 1.39893723e-06
Iter: 62 loss: 1.39820372e-06
Iter: 63 loss: 1.3971088e-06
Iter: 64 loss: 1.39701172e-06
Iter: 65 loss: 1.39643339e-06
Iter: 66 loss: 1.39498616e-06
Iter: 67 loss: 1.40824636e-06
Iter: 68 loss: 1.39476435e-06
Iter: 69 loss: 1.39343751e-06
Iter: 70 loss: 1.39343547e-06
Iter: 71 loss: 1.39211443e-06
Iter: 72 loss: 1.39181043e-06
Iter: 73 loss: 1.39098404e-06
Iter: 74 loss: 1.39003453e-06
Iter: 75 loss: 1.38995074e-06
Iter: 76 loss: 1.38926976e-06
Iter: 77 loss: 1.38942096e-06
Iter: 78 loss: 1.38870394e-06
Iter: 79 loss: 1.38841801e-06
Iter: 80 loss: 1.38766404e-06
Iter: 81 loss: 1.3925121e-06
Iter: 82 loss: 1.387466e-06
Iter: 83 loss: 1.38698351e-06
Iter: 84 loss: 1.38685277e-06
Iter: 85 loss: 1.38626842e-06
Iter: 86 loss: 1.38477617e-06
Iter: 87 loss: 1.39852932e-06
Iter: 88 loss: 1.38457744e-06
Iter: 89 loss: 1.38302573e-06
Iter: 90 loss: 1.38354483e-06
Iter: 91 loss: 1.38189978e-06
Iter: 92 loss: 1.38058147e-06
Iter: 93 loss: 1.3943569e-06
Iter: 94 loss: 1.3805643e-06
Iter: 95 loss: 1.37985819e-06
Iter: 96 loss: 1.37985933e-06
Iter: 97 loss: 1.37930977e-06
Iter: 98 loss: 1.38100722e-06
Iter: 99 loss: 1.37914503e-06
Iter: 100 loss: 1.37848565e-06
Iter: 101 loss: 1.37730922e-06
Iter: 102 loss: 1.40581517e-06
Iter: 103 loss: 1.37730649e-06
Iter: 104 loss: 1.37681673e-06
Iter: 105 loss: 1.376672e-06
Iter: 106 loss: 1.37607162e-06
Iter: 107 loss: 1.37493407e-06
Iter: 108 loss: 1.39773306e-06
Iter: 109 loss: 1.37492395e-06
Iter: 110 loss: 1.3743221e-06
Iter: 111 loss: 1.37429515e-06
Iter: 112 loss: 1.37367749e-06
Iter: 113 loss: 1.3738927e-06
Iter: 114 loss: 1.37324287e-06
Iter: 115 loss: 1.37285656e-06
Iter: 116 loss: 1.372219e-06
Iter: 117 loss: 1.37222878e-06
Iter: 118 loss: 1.37205484e-06
Iter: 119 loss: 1.37180814e-06
Iter: 120 loss: 1.37150403e-06
Iter: 121 loss: 1.37058191e-06
Iter: 122 loss: 1.37497773e-06
Iter: 123 loss: 1.37026291e-06
Iter: 124 loss: 1.36898075e-06
Iter: 125 loss: 1.37061284e-06
Iter: 126 loss: 1.36830909e-06
Iter: 127 loss: 1.36725305e-06
Iter: 128 loss: 1.37300776e-06
Iter: 129 loss: 1.36705262e-06
Iter: 130 loss: 1.36680433e-06
Iter: 131 loss: 1.36654887e-06
Iter: 132 loss: 1.36629092e-06
Iter: 133 loss: 1.36662322e-06
Iter: 134 loss: 1.36616e-06
Iter: 135 loss: 1.36586902e-06
Iter: 136 loss: 1.36533936e-06
Iter: 137 loss: 1.36533185e-06
Iter: 138 loss: 1.36464428e-06
Iter: 139 loss: 1.37318705e-06
Iter: 140 loss: 1.36464223e-06
Iter: 141 loss: 1.36422159e-06
Iter: 142 loss: 1.36372569e-06
Iter: 143 loss: 1.36364815e-06
Iter: 144 loss: 1.36331539e-06
Iter: 145 loss: 1.36325434e-06
Iter: 146 loss: 1.36299366e-06
Iter: 147 loss: 1.36229551e-06
Iter: 148 loss: 1.36497317e-06
Iter: 149 loss: 1.36198219e-06
Iter: 150 loss: 1.36158417e-06
Iter: 151 loss: 1.36154051e-06
Iter: 152 loss: 1.36105587e-06
Iter: 153 loss: 1.36162112e-06
Iter: 154 loss: 1.36077654e-06
Iter: 155 loss: 1.36042433e-06
Iter: 156 loss: 1.35995845e-06
Iter: 157 loss: 1.35991286e-06
Iter: 158 loss: 1.35924461e-06
Iter: 159 loss: 1.35851951e-06
Iter: 160 loss: 1.35842311e-06
Iter: 161 loss: 1.35802361e-06
Iter: 162 loss: 1.35797541e-06
Iter: 163 loss: 1.35751338e-06
Iter: 164 loss: 1.35853202e-06
Iter: 165 loss: 1.35732171e-06
Iter: 166 loss: 1.35689891e-06
Iter: 167 loss: 1.35705e-06
Iter: 168 loss: 1.35663618e-06
Iter: 169 loss: 1.35604205e-06
Iter: 170 loss: 1.35919072e-06
Iter: 171 loss: 1.35596974e-06
Iter: 172 loss: 1.35553546e-06
Iter: 173 loss: 1.35879429e-06
Iter: 174 loss: 1.35549612e-06
Iter: 175 loss: 1.35520486e-06
Iter: 176 loss: 1.35479286e-06
Iter: 177 loss: 1.35478899e-06
Iter: 178 loss: 1.35437494e-06
Iter: 179 loss: 1.35435562e-06
Iter: 180 loss: 1.35415871e-06
Iter: 181 loss: 1.35365553e-06
Iter: 182 loss: 1.35791925e-06
Iter: 183 loss: 1.35358641e-06
Iter: 184 loss: 1.35350433e-06
Iter: 185 loss: 1.35343384e-06
Iter: 186 loss: 1.35322375e-06
Iter: 187 loss: 1.35291702e-06
Iter: 188 loss: 1.35290043e-06
Iter: 189 loss: 1.3525073e-06
Iter: 190 loss: 1.35166965e-06
Iter: 191 loss: 1.36626613e-06
Iter: 192 loss: 1.35167227e-06
Iter: 193 loss: 1.3511135e-06
Iter: 194 loss: 1.35108121e-06
Iter: 195 loss: 1.35074811e-06
Iter: 196 loss: 1.35416269e-06
Iter: 197 loss: 1.3507356e-06
Iter: 198 loss: 1.35048845e-06
Iter: 199 loss: 1.35032121e-06
Iter: 200 loss: 1.35025209e-06
Iter: 201 loss: 1.34997197e-06
Iter: 202 loss: 1.35156267e-06
Iter: 203 loss: 1.34991899e-06
Iter: 204 loss: 1.34965376e-06
Iter: 205 loss: 1.35162861e-06
Iter: 206 loss: 1.34962841e-06
Iter: 207 loss: 1.34945878e-06
Iter: 208 loss: 1.34965842e-06
Iter: 209 loss: 1.34935237e-06
Iter: 210 loss: 1.34915115e-06
Iter: 211 loss: 1.34938637e-06
Iter: 212 loss: 1.34901552e-06
Iter: 213 loss: 1.34857839e-06
Iter: 214 loss: 1.34827667e-06
Iter: 215 loss: 1.34812774e-06
Iter: 216 loss: 1.34776565e-06
Iter: 217 loss: 1.34772154e-06
Iter: 218 loss: 1.34744778e-06
Iter: 219 loss: 1.34748962e-06
Iter: 220 loss: 1.34725929e-06
Iter: 221 loss: 1.34713173e-06
Iter: 222 loss: 1.34682466e-06
Iter: 223 loss: 1.34937545e-06
Iter: 224 loss: 1.34678169e-06
Iter: 225 loss: 1.34644199e-06
Iter: 226 loss: 1.34771562e-06
Iter: 227 loss: 1.34635661e-06
Iter: 228 loss: 1.34609297e-06
Iter: 229 loss: 1.34997163e-06
Iter: 230 loss: 1.3460733e-06
Iter: 231 loss: 1.34583354e-06
Iter: 232 loss: 1.34542847e-06
Iter: 233 loss: 1.34541551e-06
Iter: 234 loss: 1.34481911e-06
Iter: 235 loss: 1.34479808e-06
Iter: 236 loss: 1.34436709e-06
Iter: 237 loss: 1.34414427e-06
Iter: 238 loss: 1.34395918e-06
Iter: 239 loss: 1.34372499e-06
Iter: 240 loss: 1.3441495e-06
Iter: 241 loss: 1.34362449e-06
Iter: 242 loss: 1.3434518e-06
Iter: 243 loss: 1.34401967e-06
Iter: 244 loss: 1.34341485e-06
Iter: 245 loss: 1.34325569e-06
Iter: 246 loss: 1.34440211e-06
Iter: 247 loss: 1.34323363e-06
Iter: 248 loss: 1.34308584e-06
Iter: 249 loss: 1.34268134e-06
Iter: 250 loss: 1.34434947e-06
Iter: 251 loss: 1.34251297e-06
Iter: 252 loss: 1.34218203e-06
Iter: 253 loss: 1.34219817e-06
Iter: 254 loss: 1.34182551e-06
Iter: 255 loss: 1.34269612e-06
Iter: 256 loss: 1.34171e-06
Iter: 257 loss: 1.34148775e-06
Iter: 258 loss: 1.34116135e-06
Iter: 259 loss: 1.34113679e-06
Iter: 260 loss: 1.34100242e-06
Iter: 261 loss: 1.340989e-06
Iter: 262 loss: 1.3408046e-06
Iter: 263 loss: 1.34106972e-06
Iter: 264 loss: 1.34073457e-06
Iter: 265 loss: 1.3405438e-06
Iter: 266 loss: 1.34020797e-06
Iter: 267 loss: 1.34022127e-06
Iter: 268 loss: 1.33981803e-06
Iter: 269 loss: 1.34178117e-06
Iter: 270 loss: 1.33975254e-06
Iter: 271 loss: 1.33933656e-06
Iter: 272 loss: 1.34289087e-06
Iter: 273 loss: 1.33929427e-06
Iter: 274 loss: 1.33909566e-06
Iter: 275 loss: 1.33963204e-06
Iter: 276 loss: 1.33902358e-06
Iter: 277 loss: 1.33884134e-06
Iter: 278 loss: 1.33999538e-06
Iter: 279 loss: 1.33883498e-06
Iter: 280 loss: 1.33863409e-06
Iter: 281 loss: 1.33856281e-06
Iter: 282 loss: 1.33846834e-06
Iter: 283 loss: 1.33828019e-06
Iter: 284 loss: 1.33817093e-06
Iter: 285 loss: 1.33808135e-06
Iter: 286 loss: 1.33797562e-06
Iter: 287 loss: 1.33795118e-06
Iter: 288 loss: 1.33777894e-06
Iter: 289 loss: 1.33734693e-06
Iter: 290 loss: 1.34026641e-06
Iter: 291 loss: 1.3372312e-06
Iter: 292 loss: 1.3368267e-06
Iter: 293 loss: 1.33839467e-06
Iter: 294 loss: 1.33673348e-06
Iter: 295 loss: 1.33675871e-06
Iter: 296 loss: 1.33659546e-06
Iter: 297 loss: 1.336487e-06
Iter: 298 loss: 1.33628794e-06
Iter: 299 loss: 1.33942899e-06
Iter: 300 loss: 1.33627566e-06
Iter: 301 loss: 1.33604863e-06
Iter: 302 loss: 1.33591925e-06
Iter: 303 loss: 1.33582466e-06
Iter: 304 loss: 1.33552726e-06
Iter: 305 loss: 1.33990795e-06
Iter: 306 loss: 1.33553658e-06
Iter: 307 loss: 1.33522485e-06
Iter: 308 loss: 1.33633557e-06
Iter: 309 loss: 1.33514118e-06
Iter: 310 loss: 1.33497701e-06
Iter: 311 loss: 1.33513731e-06
Iter: 312 loss: 1.3348556e-06
Iter: 313 loss: 1.33460344e-06
Iter: 314 loss: 1.33609888e-06
Iter: 315 loss: 1.33458298e-06
Iter: 316 loss: 1.33437322e-06
Iter: 317 loss: 1.33468359e-06
Iter: 318 loss: 1.33425385e-06
Iter: 319 loss: 1.33415028e-06
Iter: 320 loss: 1.33396475e-06
Iter: 321 loss: 1.33396361e-06
Iter: 322 loss: 1.33376204e-06
Iter: 323 loss: 1.33375556e-06
Iter: 324 loss: 1.33353296e-06
Iter: 325 loss: 1.33309368e-06
Iter: 326 loss: 1.34064726e-06
Iter: 327 loss: 1.33307333e-06
Iter: 328 loss: 1.33278127e-06
Iter: 329 loss: 1.33382969e-06
Iter: 330 loss: 1.33270578e-06
Iter: 331 loss: 1.33255912e-06
Iter: 332 loss: 1.33254014e-06
Iter: 333 loss: 1.33241849e-06
Iter: 334 loss: 1.33218828e-06
Iter: 335 loss: 1.33492517e-06
Iter: 336 loss: 1.33214326e-06
Iter: 337 loss: 1.33192225e-06
Iter: 338 loss: 1.33248216e-06
Iter: 339 loss: 1.33183266e-06
Iter: 340 loss: 1.3316594e-06
Iter: 341 loss: 1.33165645e-06
Iter: 342 loss: 1.33145784e-06
Iter: 343 loss: 1.33123092e-06
Iter: 344 loss: 1.33117487e-06
Iter: 345 loss: 1.33092897e-06
Iter: 346 loss: 1.33264791e-06
Iter: 347 loss: 1.33089111e-06
Iter: 348 loss: 1.33067294e-06
Iter: 349 loss: 1.33262529e-06
Iter: 350 loss: 1.33066521e-06
Iter: 351 loss: 1.33056278e-06
Iter: 352 loss: 1.33075525e-06
Iter: 353 loss: 1.33049787e-06
Iter: 354 loss: 1.33042499e-06
Iter: 355 loss: 1.33023741e-06
Iter: 356 loss: 1.33285016e-06
Iter: 357 loss: 1.33020581e-06
Iter: 358 loss: 1.33013896e-06
Iter: 359 loss: 1.33007916e-06
Iter: 360 loss: 1.32997161e-06
Iter: 361 loss: 1.32972934e-06
Iter: 362 loss: 1.33377807e-06
Iter: 363 loss: 1.32972764e-06
Iter: 364 loss: 1.32951891e-06
Iter: 365 loss: 1.33007916e-06
Iter: 366 loss: 1.32942944e-06
Iter: 367 loss: 1.32911941e-06
Iter: 368 loss: 1.33135677e-06
Iter: 369 loss: 1.32910577e-06
Iter: 370 loss: 1.32898015e-06
Iter: 371 loss: 1.32878677e-06
Iter: 372 loss: 1.32879859e-06
Iter: 373 loss: 1.32858918e-06
Iter: 374 loss: 1.32874641e-06
Iter: 375 loss: 1.32847617e-06
Iter: 376 loss: 1.32821651e-06
Iter: 377 loss: 1.32903665e-06
Iter: 378 loss: 1.32814785e-06
Iter: 379 loss: 1.32792752e-06
Iter: 380 loss: 1.32793366e-06
Iter: 381 loss: 1.3277878e-06
Iter: 382 loss: 1.32778337e-06
Iter: 383 loss: 1.32768196e-06
Iter: 384 loss: 1.32750142e-06
Iter: 385 loss: 1.32778882e-06
Iter: 386 loss: 1.32742014e-06
Iter: 387 loss: 1.32724881e-06
Iter: 388 loss: 1.32717673e-06
Iter: 389 loss: 1.32706953e-06
Iter: 390 loss: 1.32710716e-06
Iter: 391 loss: 1.32696732e-06
Iter: 392 loss: 1.32691525e-06
Iter: 393 loss: 1.32674336e-06
Iter: 394 loss: 1.32783066e-06
Iter: 395 loss: 1.32671835e-06
Iter: 396 loss: 1.32659568e-06
Iter: 397 loss: 1.32658397e-06
Iter: 398 loss: 1.32644971e-06
Iter: 399 loss: 1.32617492e-06
Iter: 400 loss: 1.33043557e-06
Iter: 401 loss: 1.32616947e-06
Iter: 402 loss: 1.32608272e-06
Iter: 403 loss: 1.32601554e-06
Iter: 404 loss: 1.32594448e-06
Iter: 405 loss: 1.32575258e-06
Iter: 406 loss: 1.3302357e-06
Iter: 407 loss: 1.32576724e-06
Iter: 408 loss: 1.32561422e-06
Iter: 409 loss: 1.32567334e-06
Iter: 410 loss: 1.32549792e-06
Iter: 411 loss: 1.32539424e-06
Iter: 412 loss: 1.32638138e-06
Iter: 413 loss: 1.32538844e-06
Iter: 414 loss: 1.32524008e-06
Iter: 415 loss: 1.3253084e-06
Iter: 416 loss: 1.32516629e-06
Iter: 417 loss: 1.32503442e-06
Iter: 418 loss: 1.32490629e-06
Iter: 419 loss: 1.32487867e-06
Iter: 420 loss: 1.32466448e-06
Iter: 421 loss: 1.32461014e-06
Iter: 422 loss: 1.32447803e-06
Iter: 423 loss: 1.32417608e-06
Iter: 424 loss: 1.32832633e-06
Iter: 425 loss: 1.32417426e-06
Iter: 426 loss: 1.32398986e-06
Iter: 427 loss: 1.32397327e-06
Iter: 428 loss: 1.32386117e-06
Iter: 429 loss: 1.32376101e-06
Iter: 430 loss: 1.3237651e-06
Iter: 431 loss: 1.32361708e-06
Iter: 432 loss: 1.32349328e-06
Iter: 433 loss: 1.32346815e-06
Iter: 434 loss: 1.32332286e-06
Iter: 435 loss: 1.32400385e-06
Iter: 436 loss: 1.32329501e-06
Iter: 437 loss: 1.32310151e-06
Iter: 438 loss: 1.32340142e-06
Iter: 439 loss: 1.32301432e-06
Iter: 440 loss: 1.32288869e-06
Iter: 441 loss: 1.32263199e-06
Iter: 442 loss: 1.32769287e-06
Iter: 443 loss: 1.32264222e-06
Iter: 444 loss: 1.32253012e-06
Iter: 445 loss: 1.32249966e-06
Iter: 446 loss: 1.32238438e-06
Iter: 447 loss: 1.32276625e-06
Iter: 448 loss: 1.32232e-06
Iter: 449 loss: 1.32224613e-06
Iter: 450 loss: 1.32267974e-06
Iter: 451 loss: 1.32220885e-06
Iter: 452 loss: 1.32209902e-06
Iter: 453 loss: 1.32188552e-06
Iter: 454 loss: 1.32187722e-06
Iter: 455 loss: 1.32175e-06
Iter: 456 loss: 1.32173534e-06
Iter: 457 loss: 1.32160199e-06
Iter: 458 loss: 1.3214908e-06
Iter: 459 loss: 1.32144032e-06
Iter: 460 loss: 1.32126684e-06
Iter: 461 loss: 1.3221545e-06
Iter: 462 loss: 1.32124069e-06
Iter: 463 loss: 1.32114837e-06
Iter: 464 loss: 1.3211195e-06
Iter: 465 loss: 1.32105879e-06
Iter: 466 loss: 1.32087894e-06
Iter: 467 loss: 1.3224942e-06
Iter: 468 loss: 1.32086393e-06
Iter: 469 loss: 1.32071023e-06
Iter: 470 loss: 1.32251262e-06
Iter: 471 loss: 1.3207009e-06
Iter: 472 loss: 1.32052332e-06
Iter: 473 loss: 1.3205688e-06
Iter: 474 loss: 1.32039963e-06
Iter: 475 loss: 1.32023183e-06
Iter: 476 loss: 1.31998627e-06
Iter: 477 loss: 1.31999082e-06
Iter: 478 loss: 1.32013429e-06
Iter: 479 loss: 1.31993284e-06
Iter: 480 loss: 1.31987099e-06
Iter: 481 loss: 1.31979107e-06
Iter: 482 loss: 1.31979095e-06
Iter: 483 loss: 1.31970626e-06
Iter: 484 loss: 1.32051662e-06
Iter: 485 loss: 1.31970626e-06
Iter: 486 loss: 1.31963634e-06
Iter: 487 loss: 1.31955721e-06
Iter: 488 loss: 1.31957131e-06
Iter: 489 loss: 1.31945649e-06
Iter: 490 loss: 1.31937031e-06
Iter: 491 loss: 1.3193511e-06
Iter: 492 loss: 1.31920331e-06
Iter: 493 loss: 1.31918569e-06
Iter: 494 loss: 1.31908018e-06
Iter: 495 loss: 1.31891898e-06
Iter: 496 loss: 1.32042237e-06
Iter: 497 loss: 1.31891306e-06
Iter: 498 loss: 1.31882427e-06
Iter: 499 loss: 1.31941897e-06
Iter: 500 loss: 1.3188037e-06
Iter: 501 loss: 1.31875049e-06
Iter: 502 loss: 1.31856746e-06
Iter: 503 loss: 1.32052128e-06
Iter: 504 loss: 1.31854483e-06
Iter: 505 loss: 1.31838942e-06
Iter: 506 loss: 1.31969227e-06
Iter: 507 loss: 1.31837965e-06
Iter: 508 loss: 1.31821696e-06
Iter: 509 loss: 1.31910679e-06
Iter: 510 loss: 1.31818319e-06
Iter: 511 loss: 1.31806132e-06
Iter: 512 loss: 1.3179075e-06
Iter: 513 loss: 1.31789534e-06
Iter: 514 loss: 1.3177995e-06
Iter: 515 loss: 1.31778211e-06
Iter: 516 loss: 1.31769082e-06
Iter: 517 loss: 1.317635e-06
Iter: 518 loss: 1.31758316e-06
Iter: 519 loss: 1.31753e-06
Iter: 520 loss: 1.31751835e-06
Iter: 521 loss: 1.3174747e-06
Iter: 522 loss: 1.31734203e-06
Iter: 523 loss: 1.31936724e-06
Iter: 524 loss: 1.31735021e-06
Iter: 525 loss: 1.31723971e-06
Iter: 526 loss: 1.31802494e-06
Iter: 527 loss: 1.31722641e-06
Iter: 528 loss: 1.31710499e-06
Iter: 529 loss: 1.31725665e-06
Iter: 530 loss: 1.31703507e-06
Iter: 531 loss: 1.31695106e-06
Iter: 532 loss: 1.31696447e-06
Iter: 533 loss: 1.31684533e-06
Iter: 534 loss: 1.31672618e-06
Iter: 535 loss: 1.31730519e-06
Iter: 536 loss: 1.31670674e-06
Iter: 537 loss: 1.31660659e-06
Iter: 538 loss: 1.31764875e-06
Iter: 539 loss: 1.31660954e-06
Iter: 540 loss: 1.31651507e-06
Iter: 541 loss: 1.31657896e-06
Iter: 542 loss: 1.31646016e-06
Iter: 543 loss: 1.31638626e-06
Iter: 544 loss: 1.31750971e-06
Iter: 545 loss: 1.31637637e-06
Iter: 546 loss: 1.31630713e-06
Iter: 547 loss: 1.3160975e-06
Iter: 548 loss: 1.31684931e-06
Iter: 549 loss: 1.3160145e-06
Iter: 550 loss: 1.3160377e-06
Iter: 551 loss: 1.31591514e-06
Iter: 552 loss: 1.3158317e-06
Iter: 553 loss: 1.31569539e-06
Iter: 554 loss: 1.3157e-06
Iter: 555 loss: 1.31561535e-06
Iter: 556 loss: 1.31560273e-06
Iter: 557 loss: 1.31552315e-06
Iter: 558 loss: 1.3154588e-06
Iter: 559 loss: 1.31543652e-06
Iter: 560 loss: 1.31531453e-06
Iter: 561 loss: 1.31556192e-06
Iter: 562 loss: 1.31528896e-06
Iter: 563 loss: 1.31514071e-06
Iter: 564 loss: 1.3160145e-06
Iter: 565 loss: 1.31511899e-06
Iter: 566 loss: 1.31506033e-06
Iter: 567 loss: 1.31500724e-06
Iter: 568 loss: 1.31497495e-06
Iter: 569 loss: 1.31483682e-06
Iter: 570 loss: 1.3150252e-06
Iter: 571 loss: 1.31477498e-06
Iter: 572 loss: 1.31465561e-06
Iter: 573 loss: 1.31582794e-06
Iter: 574 loss: 1.31465254e-06
Iter: 575 loss: 1.31458091e-06
Iter: 576 loss: 1.3146655e-06
Iter: 577 loss: 1.31456034e-06
Iter: 578 loss: 1.31447814e-06
Iter: 579 loss: 1.31505419e-06
Iter: 580 loss: 1.31446359e-06
Iter: 581 loss: 1.3144097e-06
Iter: 582 loss: 1.31431034e-06
Iter: 583 loss: 1.31431329e-06
Iter: 584 loss: 1.31424213e-06
Iter: 585 loss: 1.31423621e-06
Iter: 586 loss: 1.31419915e-06
Iter: 587 loss: 1.31410457e-06
Iter: 588 loss: 1.31560773e-06
Iter: 589 loss: 1.3141e-06
Iter: 590 loss: 1.31390061e-06
Iter: 591 loss: 1.31429647e-06
Iter: 592 loss: 1.3138432e-06
Iter: 593 loss: 1.31372713e-06
Iter: 594 loss: 1.31385241e-06
Iter: 595 loss: 1.31366096e-06
Iter: 596 loss: 1.31359945e-06
Iter: 597 loss: 1.31358922e-06
Iter: 598 loss: 1.31353954e-06
Iter: 599 loss: 1.3134304e-06
Iter: 600 loss: 1.31341835e-06
Iter: 601 loss: 1.31331922e-06
Iter: 602 loss: 1.31443767e-06
Iter: 603 loss: 1.31332422e-06
Iter: 604 loss: 1.31324725e-06
Iter: 605 loss: 1.31370552e-06
Iter: 606 loss: 1.31322008e-06
Iter: 607 loss: 1.31314619e-06
Iter: 608 loss: 1.31304023e-06
Iter: 609 loss: 1.31303887e-06
Iter: 610 loss: 1.31291142e-06
Iter: 611 loss: 1.31291267e-06
Iter: 612 loss: 1.31281968e-06
Iter: 613 loss: 1.31268735e-06
Iter: 614 loss: 1.31267393e-06
Iter: 615 loss: 1.31257877e-06
Iter: 616 loss: 1.31257229e-06
Iter: 617 loss: 1.3125125e-06
Iter: 618 loss: 1.31243473e-06
Iter: 619 loss: 1.31240859e-06
Iter: 620 loss: 1.31236277e-06
Iter: 621 loss: 1.31236357e-06
Iter: 622 loss: 1.31229456e-06
Iter: 623 loss: 1.3121354e-06
Iter: 624 loss: 1.31347974e-06
Iter: 625 loss: 1.31210516e-06
Iter: 626 loss: 1.31195804e-06
Iter: 627 loss: 1.31196509e-06
Iter: 628 loss: 1.31188381e-06
Iter: 629 loss: 1.31173795e-06
Iter: 630 loss: 1.31173169e-06
Iter: 631 loss: 1.31153729e-06
Iter: 632 loss: 1.31145384e-06
Iter: 633 loss: 1.31137949e-06
Iter: 634 loss: 1.31118395e-06
Iter: 635 loss: 1.3113538e-06
Iter: 636 loss: 1.31106526e-06
Iter: 637 loss: 1.31092963e-06
Iter: 638 loss: 1.31123215e-06
Iter: 639 loss: 1.31091031e-06
Iter: 640 loss: 1.31081129e-06
Iter: 641 loss: 1.31109482e-06
Iter: 642 loss: 1.31080355e-06
Iter: 643 loss: 1.31071533e-06
Iter: 644 loss: 1.31105151e-06
Iter: 645 loss: 1.31068487e-06
Iter: 646 loss: 1.31058698e-06
Iter: 647 loss: 1.31070385e-06
Iter: 648 loss: 1.31053207e-06
Iter: 649 loss: 1.31041838e-06
Iter: 650 loss: 1.31121874e-06
Iter: 651 loss: 1.31040338e-06
Iter: 652 loss: 1.31033812e-06
Iter: 653 loss: 1.31035051e-06
Iter: 654 loss: 1.31029026e-06
Iter: 655 loss: 1.31015281e-06
Iter: 656 loss: 1.31035256e-06
Iter: 657 loss: 1.31010211e-06
Iter: 658 loss: 1.31003765e-06
Iter: 659 loss: 1.31053253e-06
Iter: 660 loss: 1.3100173e-06
Iter: 661 loss: 1.30993817e-06
Iter: 662 loss: 1.31007118e-06
Iter: 663 loss: 1.30990611e-06
Iter: 664 loss: 1.30983619e-06
Iter: 665 loss: 1.31014417e-06
Iter: 666 loss: 1.30983585e-06
Iter: 667 loss: 1.30975297e-06
Iter: 668 loss: 1.30959893e-06
Iter: 669 loss: 1.3124228e-06
Iter: 670 loss: 1.30959438e-06
Iter: 671 loss: 1.3094417e-06
Iter: 672 loss: 1.30956892e-06
Iter: 673 loss: 1.30933779e-06
Iter: 674 loss: 1.30919148e-06
Iter: 675 loss: 1.31126626e-06
Iter: 676 loss: 1.30918897e-06
Iter: 677 loss: 1.30908791e-06
Iter: 678 loss: 1.30963042e-06
Iter: 679 loss: 1.30906449e-06
Iter: 680 loss: 1.30898479e-06
Iter: 681 loss: 1.30954993e-06
Iter: 682 loss: 1.30898172e-06
Iter: 683 loss: 1.30893284e-06
Iter: 684 loss: 1.30909416e-06
Iter: 685 loss: 1.30890089e-06
Iter: 686 loss: 1.30885041e-06
Iter: 687 loss: 1.30888475e-06
Iter: 688 loss: 1.30881358e-06
Iter: 689 loss: 1.30872309e-06
Iter: 690 loss: 1.30904255e-06
Iter: 691 loss: 1.30870308e-06
Iter: 692 loss: 1.3086252e-06
Iter: 693 loss: 1.30856313e-06
Iter: 694 loss: 1.30854846e-06
Iter: 695 loss: 1.30843659e-06
Iter: 696 loss: 1.3099118e-06
Iter: 697 loss: 1.30842113e-06
Iter: 698 loss: 1.30836679e-06
Iter: 699 loss: 1.30847047e-06
Iter: 700 loss: 1.30833723e-06
Iter: 701 loss: 1.30824844e-06
Iter: 702 loss: 1.30835758e-06
Iter: 703 loss: 1.30821127e-06
Iter: 704 loss: 1.30817284e-06
Iter: 705 loss: 1.30799845e-06
Iter: 706 loss: 1.3099617e-06
Iter: 707 loss: 1.30797764e-06
Iter: 708 loss: 1.30781154e-06
Iter: 709 loss: 1.30987053e-06
Iter: 710 loss: 1.30780109e-06
Iter: 711 loss: 1.30768967e-06
Iter: 712 loss: 1.30869114e-06
Iter: 713 loss: 1.30767648e-06
Iter: 714 loss: 1.30757223e-06
Iter: 715 loss: 1.30792705e-06
Iter: 716 loss: 1.30754927e-06
Iter: 717 loss: 1.30747094e-06
Iter: 718 loss: 1.30785656e-06
Iter: 719 loss: 1.30745252e-06
Iter: 720 loss: 1.30739329e-06
Iter: 721 loss: 1.30763044e-06
Iter: 722 loss: 1.30738204e-06
Iter: 723 loss: 1.30733838e-06
Iter: 724 loss: 1.3076708e-06
Iter: 725 loss: 1.30733497e-06
Iter: 726 loss: 1.30729336e-06
Iter: 727 loss: 1.3072289e-06
Iter: 728 loss: 1.30722356e-06
Iter: 729 loss: 1.30714602e-06
Iter: 730 loss: 1.30818967e-06
Iter: 731 loss: 1.30714761e-06
Iter: 732 loss: 1.30707315e-06
Iter: 733 loss: 1.30701574e-06
Iter: 734 loss: 1.30700994e-06
Iter: 735 loss: 1.30689898e-06
Iter: 736 loss: 1.30737e-06
Iter: 737 loss: 1.30687499e-06
Iter: 738 loss: 1.30678336e-06
Iter: 739 loss: 1.30669014e-06
Iter: 740 loss: 1.30667081e-06
Iter: 741 loss: 1.30656281e-06
Iter: 742 loss: 1.30669923e-06
Iter: 743 loss: 1.3065154e-06
Iter: 744 loss: 1.30643502e-06
Iter: 745 loss: 1.30642729e-06
Iter: 746 loss: 1.306385e-06
Iter: 747 loss: 1.30647823e-06
Iter: 748 loss: 1.30635522e-06
Iter: 749 loss: 1.30626029e-06
Iter: 750 loss: 1.30652938e-06
Iter: 751 loss: 1.30623994e-06
Iter: 752 loss: 1.30615035e-06
Iter: 753 loss: 1.30615194e-06
Iter: 754 loss: 1.30609305e-06
Iter: 755 loss: 1.30598278e-06
Iter: 756 loss: 1.30720923e-06
Iter: 757 loss: 1.30597141e-06
Iter: 758 loss: 1.30590865e-06
Iter: 759 loss: 1.30595095e-06
Iter: 760 loss: 1.30588501e-06
Iter: 761 loss: 1.30582453e-06
Iter: 762 loss: 1.30620663e-06
Iter: 763 loss: 1.30580668e-06
Iter: 764 loss: 1.30577087e-06
Iter: 765 loss: 1.30571129e-06
Iter: 766 loss: 1.3056989e-06
Iter: 767 loss: 1.30556509e-06
Iter: 768 loss: 1.30605827e-06
Iter: 769 loss: 1.30556191e-06
Iter: 770 loss: 1.30545436e-06
Iter: 771 loss: 1.30532862e-06
Iter: 772 loss: 1.30531112e-06
Iter: 773 loss: 1.30516401e-06
Iter: 774 loss: 1.30544242e-06
Iter: 775 loss: 1.30507465e-06
Iter: 776 loss: 1.30497733e-06
Iter: 777 loss: 1.30497892e-06
Iter: 778 loss: 1.30489559e-06
Iter: 779 loss: 1.30516014e-06
Iter: 780 loss: 1.30488627e-06
Iter: 781 loss: 1.30478577e-06
Iter: 782 loss: 1.30523017e-06
Iter: 783 loss: 1.30477213e-06
Iter: 784 loss: 1.30469289e-06
Iter: 785 loss: 1.30470858e-06
Iter: 786 loss: 1.30464196e-06
Iter: 787 loss: 1.30451849e-06
Iter: 788 loss: 1.30534613e-06
Iter: 789 loss: 1.30451099e-06
Iter: 790 loss: 1.30442265e-06
Iter: 791 loss: 1.30434159e-06
Iter: 792 loss: 1.30429635e-06
Iter: 793 loss: 1.3041772e-06
Iter: 794 loss: 1.30493061e-06
Iter: 795 loss: 1.30416981e-06
Iter: 796 loss: 1.3040958e-06
Iter: 797 loss: 1.30436979e-06
Iter: 798 loss: 1.30405147e-06
Iter: 799 loss: 1.30396199e-06
Iter: 800 loss: 1.30426338e-06
Iter: 801 loss: 1.3039471e-06
Iter: 802 loss: 1.30385843e-06
Iter: 803 loss: 1.30368653e-06
Iter: 804 loss: 1.30367107e-06
Iter: 805 loss: 1.30351395e-06
Iter: 806 loss: 1.30375906e-06
Iter: 807 loss: 1.30344347e-06
Iter: 808 loss: 1.30325691e-06
Iter: 809 loss: 1.30399758e-06
Iter: 810 loss: 1.30323292e-06
Iter: 811 loss: 1.30309763e-06
Iter: 812 loss: 1.30514809e-06
Iter: 813 loss: 1.30308649e-06
Iter: 814 loss: 1.30299429e-06
Iter: 815 loss: 1.30343551e-06
Iter: 816 loss: 1.30298827e-06
Iter: 817 loss: 1.30288e-06
Iter: 818 loss: 1.30288186e-06
Iter: 819 loss: 1.30280728e-06
Iter: 820 loss: 1.30274418e-06
Iter: 821 loss: 1.30362616e-06
Iter: 822 loss: 1.30273281e-06
Iter: 823 loss: 1.30264971e-06
Iter: 824 loss: 1.30266039e-06
Iter: 825 loss: 1.30257524e-06
Iter: 826 loss: 1.30246656e-06
Iter: 827 loss: 1.30248941e-06
Iter: 828 loss: 1.30240835e-06
Iter: 829 loss: 1.3022202e-06
Iter: 830 loss: 1.30311537e-06
Iter: 831 loss: 1.30220235e-06
Iter: 832 loss: 1.30208286e-06
Iter: 833 loss: 1.30251499e-06
Iter: 834 loss: 1.30204387e-06
Iter: 835 loss: 1.30193962e-06
Iter: 836 loss: 1.30202829e-06
Iter: 837 loss: 1.30190563e-06
Iter: 838 loss: 1.30178626e-06
Iter: 839 loss: 1.3016122e-06
Iter: 840 loss: 1.30161038e-06
Iter: 841 loss: 1.3013921e-06
Iter: 842 loss: 1.30238652e-06
Iter: 843 loss: 1.30136061e-06
Iter: 844 loss: 1.30122135e-06
Iter: 845 loss: 1.30121748e-06
Iter: 846 loss: 1.30110868e-06
Iter: 847 loss: 1.30119884e-06
Iter: 848 loss: 1.30105286e-06
Iter: 849 loss: 1.30088279e-06
Iter: 850 loss: 1.30131275e-06
Iter: 851 loss: 1.30081071e-06
Iter: 852 loss: 1.30070293e-06
Iter: 853 loss: 1.3013198e-06
Iter: 854 loss: 1.30068406e-06
Iter: 855 loss: 1.300549e-06
Iter: 856 loss: 1.30070987e-06
Iter: 857 loss: 1.30048284e-06
Iter: 858 loss: 1.3003712e-06
Iter: 859 loss: 1.30036267e-06
Iter: 860 loss: 1.30025489e-06
Iter: 861 loss: 1.30010085e-06
Iter: 862 loss: 1.30144417e-06
Iter: 863 loss: 1.30005367e-06
Iter: 864 loss: 1.2999069e-06
Iter: 865 loss: 1.29987654e-06
Iter: 866 loss: 1.29980458e-06
Iter: 867 loss: 1.29956356e-06
Iter: 868 loss: 1.30004821e-06
Iter: 869 loss: 1.29944249e-06
Iter: 870 loss: 1.29925729e-06
Iter: 871 loss: 1.29935847e-06
Iter: 872 loss: 1.2991494e-06
Iter: 873 loss: 1.29898388e-06
Iter: 874 loss: 1.29900366e-06
Iter: 875 loss: 1.29885552e-06
Iter: 876 loss: 1.29870432e-06
Iter: 877 loss: 1.29868283e-06
Iter: 878 loss: 1.29854016e-06
Iter: 879 loss: 1.2986477e-06
Iter: 880 loss: 1.29846967e-06
Iter: 881 loss: 1.29830107e-06
Iter: 882 loss: 1.29905277e-06
Iter: 883 loss: 1.2982473e-06
Iter: 884 loss: 1.29811065e-06
Iter: 885 loss: 1.29810257e-06
Iter: 886 loss: 1.29798696e-06
Iter: 887 loss: 1.29772809e-06
Iter: 888 loss: 1.29932994e-06
Iter: 889 loss: 1.29771945e-06
Iter: 890 loss: 1.29758882e-06
Iter: 891 loss: 1.29755676e-06
Iter: 892 loss: 1.29746286e-06
Iter: 893 loss: 1.29732825e-06
Iter: 894 loss: 1.29919295e-06
Iter: 895 loss: 1.29729938e-06
Iter: 896 loss: 1.29721946e-06
Iter: 897 loss: 1.29720218e-06
Iter: 898 loss: 1.29713135e-06
Iter: 899 loss: 1.29698537e-06
Iter: 900 loss: 1.29741227e-06
Iter: 901 loss: 1.29692046e-06
Iter: 902 loss: 1.29677619e-06
Iter: 903 loss: 1.29652551e-06
Iter: 904 loss: 1.29653722e-06
Iter: 905 loss: 1.29628052e-06
Iter: 906 loss: 1.29725743e-06
Iter: 907 loss: 1.29622231e-06
Iter: 908 loss: 1.2960711e-06
Iter: 909 loss: 1.29606701e-06
Iter: 910 loss: 1.2959149e-06
Iter: 911 loss: 1.29605462e-06
Iter: 912 loss: 1.29580235e-06
Iter: 913 loss: 1.29567911e-06
Iter: 914 loss: 1.29728608e-06
Iter: 915 loss: 1.29564683e-06
Iter: 916 loss: 1.29553791e-06
Iter: 917 loss: 1.2953526e-06
Iter: 918 loss: 1.29535886e-06
Iter: 919 loss: 1.2951291e-06
Iter: 920 loss: 1.29514342e-06
Iter: 921 loss: 1.295012e-06
Iter: 922 loss: 1.29488876e-06
Iter: 923 loss: 1.29487751e-06
Iter: 924 loss: 1.29471914e-06
Iter: 925 loss: 1.29655757e-06
Iter: 926 loss: 1.29472073e-06
Iter: 927 loss: 1.29458158e-06
Iter: 928 loss: 1.29468094e-06
Iter: 929 loss: 1.2945178e-06
Iter: 930 loss: 1.29437581e-06
Iter: 931 loss: 1.29497857e-06
Iter: 932 loss: 1.29436398e-06
Iter: 933 loss: 1.2942279e-06
Iter: 934 loss: 1.29403884e-06
Iter: 935 loss: 1.29872865e-06
Iter: 936 loss: 1.29403293e-06
Iter: 937 loss: 1.2937129e-06
Iter: 938 loss: 1.29349451e-06
Iter: 939 loss: 1.29340742e-06
Iter: 940 loss: 1.29322723e-06
Iter: 941 loss: 1.29317505e-06
Iter: 942 loss: 1.2929695e-06
Iter: 943 loss: 1.29397733e-06
Iter: 944 loss: 1.29295267e-06
Iter: 945 loss: 1.29281625e-06
Iter: 946 loss: 1.2933308e-06
Iter: 947 loss: 1.29279783e-06
Iter: 948 loss: 1.29266698e-06
Iter: 949 loss: 1.29252976e-06
Iter: 950 loss: 1.29247621e-06
Iter: 951 loss: 1.29241494e-06
Iter: 952 loss: 1.29237333e-06
Iter: 953 loss: 1.29231432e-06
Iter: 954 loss: 1.29215709e-06
Iter: 955 loss: 1.2942171e-06
Iter: 956 loss: 1.29213549e-06
Iter: 957 loss: 1.29192892e-06
Iter: 958 loss: 1.29333159e-06
Iter: 959 loss: 1.29190732e-06
Iter: 960 loss: 1.29170326e-06
Iter: 961 loss: 1.29204057e-06
Iter: 962 loss: 1.29160674e-06
Iter: 963 loss: 1.29143814e-06
Iter: 964 loss: 1.29210707e-06
Iter: 965 loss: 1.29137709e-06
Iter: 966 loss: 1.291221e-06
Iter: 967 loss: 1.29110663e-06
Iter: 968 loss: 1.29105285e-06
Iter: 969 loss: 1.2908323e-06
Iter: 970 loss: 1.29079058e-06
Iter: 971 loss: 1.29063471e-06
Iter: 972 loss: 1.29034458e-06
Iter: 973 loss: 1.29283785e-06
Iter: 974 loss: 1.29030855e-06
Iter: 975 loss: 1.29015189e-06
Iter: 976 loss: 1.29014757e-06
Iter: 977 loss: 1.29004525e-06
Iter: 978 loss: 1.29021987e-06
Iter: 979 loss: 1.28997192e-06
Iter: 980 loss: 1.28983288e-06
Iter: 981 loss: 1.28980798e-06
Iter: 982 loss: 1.28965644e-06
Iter: 983 loss: 1.28956731e-06
Iter: 984 loss: 1.28957777e-06
Iter: 985 loss: 1.28946044e-06
Iter: 986 loss: 1.28936927e-06
Iter: 987 loss: 1.28932402e-06
Iter: 988 loss: 1.28919464e-06
Iter: 989 loss: 1.28984107e-06
Iter: 990 loss: 1.28916327e-06
Iter: 991 loss: 1.2889991e-06
Iter: 992 loss: 1.28940189e-06
Iter: 993 loss: 1.28894681e-06
Iter: 994 loss: 1.28879128e-06
Iter: 995 loss: 1.28900956e-06
Iter: 996 loss: 1.28868601e-06
Iter: 997 loss: 1.2884924e-06
Iter: 998 loss: 1.28852639e-06
Iter: 999 loss: 1.28831493e-06
Iter: 1000 loss: 1.28812553e-06
Iter: 1001 loss: 1.28831596e-06
Iter: 1002 loss: 1.28800411e-06
Iter: 1003 loss: 1.28779391e-06
Iter: 1004 loss: 1.2883047e-06
Iter: 1005 loss: 1.28773604e-06
Iter: 1006 loss: 1.28756892e-06
Iter: 1007 loss: 1.28756983e-06
Iter: 1008 loss: 1.28742204e-06
Iter: 1009 loss: 1.28754471e-06
Iter: 1010 loss: 1.28734177e-06
Iter: 1011 loss: 1.28716476e-06
Iter: 1012 loss: 1.28761985e-06
Iter: 1013 loss: 1.28707438e-06
Iter: 1014 loss: 1.28693785e-06
Iter: 1015 loss: 1.28731813e-06
Iter: 1016 loss: 1.28689021e-06
Iter: 1017 loss: 1.28670513e-06
Iter: 1018 loss: 1.28706938e-06
Iter: 1019 loss: 1.28659576e-06
Iter: 1020 loss: 1.28648264e-06
Iter: 1021 loss: 1.28672264e-06
Iter: 1022 loss: 1.28646718e-06
Iter: 1023 loss: 1.28629915e-06
Iter: 1024 loss: 1.28703255e-06
Iter: 1025 loss: 1.28626391e-06
Iter: 1026 loss: 1.2861683e-06
Iter: 1027 loss: 1.2863195e-06
Iter: 1028 loss: 1.28612123e-06
Iter: 1029 loss: 1.28598572e-06
Iter: 1030 loss: 1.28589522e-06
Iter: 1031 loss: 1.2858452e-06
Iter: 1032 loss: 1.2856226e-06
Iter: 1033 loss: 1.28559259e-06
Iter: 1034 loss: 1.28543729e-06
Iter: 1035 loss: 1.28523516e-06
Iter: 1036 loss: 1.28662657e-06
Iter: 1037 loss: 1.28519082e-06
Iter: 1038 loss: 1.2851234e-06
Iter: 1039 loss: 1.28513182e-06
Iter: 1040 loss: 1.285032e-06
Iter: 1041 loss: 1.28501961e-06
Iter: 1042 loss: 1.28493434e-06
Iter: 1043 loss: 1.2848094e-06
Iter: 1044 loss: 1.28558418e-06
Iter: 1045 loss: 1.28480656e-06
Iter: 1046 loss: 1.2846906e-06
Iter: 1047 loss: 1.28466843e-06
Iter: 1048 loss: 1.28460601e-06
Iter: 1049 loss: 1.2844373e-06
Iter: 1050 loss: 1.28571207e-06
Iter: 1051 loss: 1.28442457e-06
Iter: 1052 loss: 1.2843268e-06
Iter: 1053 loss: 1.28421175e-06
Iter: 1054 loss: 1.28419288e-06
Iter: 1055 loss: 1.28401791e-06
Iter: 1056 loss: 1.28563238e-06
Iter: 1057 loss: 1.28401234e-06
Iter: 1058 loss: 1.28390093e-06
Iter: 1059 loss: 1.28415718e-06
Iter: 1060 loss: 1.28385841e-06
Iter: 1061 loss: 1.28374757e-06
Iter: 1062 loss: 1.28377701e-06
Iter: 1063 loss: 1.28366719e-06
Iter: 1064 loss: 1.28351599e-06
Iter: 1065 loss: 1.28340321e-06
Iter: 1066 loss: 1.28335682e-06
Iter: 1067 loss: 1.28310148e-06
Iter: 1068 loss: 1.28341367e-06
Iter: 1069 loss: 1.28300053e-06
Iter: 1070 loss: 1.28287e-06
Iter: 1071 loss: 1.28282954e-06
Iter: 1072 loss: 1.282722e-06
Iter: 1073 loss: 1.28295164e-06
Iter: 1074 loss: 1.28265219e-06
Iter: 1075 loss: 1.28252736e-06
Iter: 1076 loss: 1.28315105e-06
Iter: 1077 loss: 1.28249258e-06
Iter: 1078 loss: 1.28239151e-06
Iter: 1079 loss: 1.28222348e-06
Iter: 1080 loss: 1.28222064e-06
Iter: 1081 loss: 1.28207228e-06
Iter: 1082 loss: 1.28206932e-06
Iter: 1083 loss: 1.28196461e-06
Iter: 1084 loss: 1.28176384e-06
Iter: 1085 loss: 1.28504894e-06
Iter: 1086 loss: 1.28177737e-06
Iter: 1087 loss: 1.28155989e-06
Iter: 1088 loss: 1.28456168e-06
Iter: 1089 loss: 1.28155773e-06
Iter: 1090 loss: 1.28142904e-06
Iter: 1091 loss: 1.28175247e-06
Iter: 1092 loss: 1.28137549e-06
Iter: 1093 loss: 1.28122679e-06
Iter: 1094 loss: 1.28128829e-06
Iter: 1095 loss: 1.28113425e-06
Iter: 1096 loss: 1.28096383e-06
Iter: 1097 loss: 1.28115369e-06
Iter: 1098 loss: 1.28086162e-06
Iter: 1099 loss: 1.28071702e-06
Iter: 1100 loss: 1.28060401e-06
Iter: 1101 loss: 1.28054307e-06
Iter: 1102 loss: 1.28030047e-06
Iter: 1103 loss: 1.28261343e-06
Iter: 1104 loss: 1.28029842e-06
Iter: 1105 loss: 1.28008423e-06
Iter: 1106 loss: 1.28162583e-06
Iter: 1107 loss: 1.28004717e-06
Iter: 1108 loss: 1.27993849e-06
Iter: 1109 loss: 1.28072713e-06
Iter: 1110 loss: 1.27992462e-06
Iter: 1111 loss: 1.27981696e-06
Iter: 1112 loss: 1.27964415e-06
Iter: 1113 loss: 1.27966e-06
Iter: 1114 loss: 1.27956673e-06
Iter: 1115 loss: 1.27955116e-06
Iter: 1116 loss: 1.27946146e-06
Iter: 1117 loss: 1.27927979e-06
Iter: 1118 loss: 1.282222e-06
Iter: 1119 loss: 1.27928661e-06
Iter: 1120 loss: 1.27911983e-06
Iter: 1121 loss: 1.28109639e-06
Iter: 1122 loss: 1.27912483e-06
Iter: 1123 loss: 1.27899307e-06
Iter: 1124 loss: 1.27917065e-06
Iter: 1125 loss: 1.27892804e-06
Iter: 1126 loss: 1.27875069e-06
Iter: 1127 loss: 1.27888563e-06
Iter: 1128 loss: 1.27864723e-06
Iter: 1129 loss: 1.27847443e-06
Iter: 1130 loss: 1.2783911e-06
Iter: 1131 loss: 1.2783114e-06
Iter: 1132 loss: 1.2780622e-06
Iter: 1133 loss: 1.27883641e-06
Iter: 1134 loss: 1.27797557e-06
Iter: 1135 loss: 1.27781811e-06
Iter: 1136 loss: 1.27925477e-06
Iter: 1137 loss: 1.27780845e-06
Iter: 1138 loss: 1.27766896e-06
Iter: 1139 loss: 1.27867168e-06
Iter: 1140 loss: 1.27766668e-06
Iter: 1141 loss: 1.27754356e-06
Iter: 1142 loss: 1.27768612e-06
Iter: 1143 loss: 1.2774907e-06
Iter: 1144 loss: 1.27733074e-06
Iter: 1145 loss: 1.27732119e-06
Iter: 1146 loss: 1.27718363e-06
Iter: 1147 loss: 1.27708e-06
Iter: 1148 loss: 1.27706744e-06
Iter: 1149 loss: 1.2769766e-06
Iter: 1150 loss: 1.27683211e-06
Iter: 1151 loss: 1.27681108e-06
Iter: 1152 loss: 1.27671694e-06
Iter: 1153 loss: 1.27762928e-06
Iter: 1154 loss: 1.27668443e-06
Iter: 1155 loss: 1.27656222e-06
Iter: 1156 loss: 1.27668386e-06
Iter: 1157 loss: 1.27650685e-06
Iter: 1158 loss: 1.27631597e-06
Iter: 1159 loss: 1.27682961e-06
Iter: 1160 loss: 1.27627095e-06
Iter: 1161 loss: 1.27615658e-06
Iter: 1162 loss: 1.27605404e-06
Iter: 1163 loss: 1.27601766e-06
Iter: 1164 loss: 1.27578869e-06
Iter: 1165 loss: 1.27611941e-06
Iter: 1166 loss: 1.27568023e-06
Iter: 1167 loss: 1.27554961e-06
Iter: 1168 loss: 1.27667363e-06
Iter: 1169 loss: 1.27552551e-06
Iter: 1170 loss: 1.27546105e-06
Iter: 1171 loss: 1.27544911e-06
Iter: 1172 loss: 1.27537714e-06
Iter: 1173 loss: 1.27531848e-06
Iter: 1174 loss: 1.27530404e-06
Iter: 1175 loss: 1.27516728e-06
Iter: 1176 loss: 1.27541375e-06
Iter: 1177 loss: 1.27511009e-06
Iter: 1178 loss: 1.27501664e-06
Iter: 1179 loss: 1.27537578e-06
Iter: 1180 loss: 1.27499743e-06
Iter: 1181 loss: 1.2748485e-06
Iter: 1182 loss: 1.27514795e-06
Iter: 1183 loss: 1.27478825e-06
Iter: 1184 loss: 1.27474414e-06
Iter: 1185 loss: 1.27489454e-06
Iter: 1186 loss: 1.27468979e-06
Iter: 1187 loss: 1.27459339e-06
Iter: 1188 loss: 1.27491285e-06
Iter: 1189 loss: 1.27459418e-06
Iter: 1190 loss: 1.27451881e-06
Iter: 1191 loss: 1.27471083e-06
Iter: 1192 loss: 1.27449755e-06
Iter: 1193 loss: 1.27440728e-06
Iter: 1194 loss: 1.27431451e-06
Iter: 1195 loss: 1.27431349e-06
Iter: 1196 loss: 1.27415842e-06
Iter: 1197 loss: 1.27441729e-06
Iter: 1198 loss: 1.27412045e-06
Iter: 1199 loss: 1.27397686e-06
Iter: 1200 loss: 1.2740129e-06
Iter: 1201 loss: 1.27387898e-06
Iter: 1202 loss: 1.27386556e-06
Iter: 1203 loss: 1.27381622e-06
Iter: 1204 loss: 1.27371538e-06
Iter: 1205 loss: 1.27368207e-06
Iter: 1206 loss: 1.27367889e-06
Iter: 1207 loss: 1.27359e-06
Iter: 1208 loss: 1.27414989e-06
Iter: 1209 loss: 1.27356861e-06
Iter: 1210 loss: 1.27353178e-06
Iter: 1211 loss: 1.27339308e-06
Iter: 1212 loss: 1.27539806e-06
Iter: 1213 loss: 1.27341355e-06
Iter: 1214 loss: 1.27328849e-06
Iter: 1215 loss: 1.27328e-06
Iter: 1216 loss: 1.27322369e-06
Iter: 1217 loss: 1.2731216e-06
Iter: 1218 loss: 1.27494854e-06
Iter: 1219 loss: 1.27311637e-06
Iter: 1220 loss: 1.27296039e-06
Iter: 1221 loss: 1.27442149e-06
Iter: 1222 loss: 1.27296744e-06
Iter: 1223 loss: 1.272909e-06
Iter: 1224 loss: 1.27280873e-06
Iter: 1225 loss: 1.27434919e-06
Iter: 1226 loss: 1.27279316e-06
Iter: 1227 loss: 1.27280214e-06
Iter: 1228 loss: 1.27271392e-06
Iter: 1229 loss: 1.27265093e-06
Iter: 1230 loss: 1.27261524e-06
Iter: 1231 loss: 1.27262456e-06
Iter: 1232 loss: 1.27254896e-06
Iter: 1233 loss: 1.27255691e-06
Iter: 1234 loss: 1.272502e-06
Iter: 1235 loss: 1.27244755e-06
Iter: 1236 loss: 1.27266526e-06
Iter: 1237 loss: 1.2724355e-06
Iter: 1238 loss: 1.27240469e-06
Iter: 1239 loss: 1.27287115e-06
Iter: 1240 loss: 1.27239446e-06
Iter: 1241 loss: 1.27234762e-06
Iter: 1242 loss: 1.27232488e-06
Iter: 1243 loss: 1.27231328e-06
Iter: 1244 loss: 1.27226212e-06
Iter: 1245 loss: 1.27226917e-06
Iter: 1246 loss: 1.27220324e-06
Iter: 1247 loss: 1.27214787e-06
Iter: 1248 loss: 1.27250064e-06
Iter: 1249 loss: 1.27211945e-06
Iter: 1250 loss: 1.27203657e-06
Iter: 1251 loss: 1.27187741e-06
Iter: 1252 loss: 1.27186706e-06
Iter: 1253 loss: 1.27178919e-06
Iter: 1254 loss: 1.27178578e-06
Iter: 1255 loss: 1.2717029e-06
Iter: 1256 loss: 1.27155158e-06
Iter: 1257 loss: 1.27539329e-06
Iter: 1258 loss: 1.27155067e-06
Iter: 1259 loss: 1.27142562e-06
Iter: 1260 loss: 1.27141232e-06
Iter: 1261 loss: 1.27132603e-06
Iter: 1262 loss: 1.2711522e-06
Iter: 1263 loss: 1.27157818e-06
Iter: 1264 loss: 1.2711231e-06
Iter: 1265 loss: 1.27109683e-06
Iter: 1266 loss: 1.27106e-06
Iter: 1267 loss: 1.27099611e-06
Iter: 1268 loss: 1.27090482e-06
Iter: 1269 loss: 1.27092449e-06
Iter: 1270 loss: 1.27081842e-06
Iter: 1271 loss: 1.27149951e-06
Iter: 1272 loss: 1.27081228e-06
Iter: 1273 loss: 1.27073156e-06
Iter: 1274 loss: 1.27085559e-06
Iter: 1275 loss: 1.27071348e-06
Iter: 1276 loss: 1.27058297e-06
Iter: 1277 loss: 1.27061321e-06
Iter: 1278 loss: 1.27048804e-06
Iter: 1279 loss: 1.27040994e-06
Iter: 1280 loss: 1.27042972e-06
Iter: 1281 loss: 1.2703465e-06
Iter: 1282 loss: 1.27030933e-06
Iter: 1283 loss: 1.27027442e-06
Iter: 1284 loss: 1.27023668e-06
Iter: 1285 loss: 1.27013016e-06
Iter: 1286 loss: 1.27258409e-06
Iter: 1287 loss: 1.27012595e-06
Iter: 1288 loss: 1.27008661e-06
Iter: 1289 loss: 1.27007922e-06
Iter: 1290 loss: 1.27004864e-06
Iter: 1291 loss: 1.26991949e-06
Iter: 1292 loss: 1.27029068e-06
Iter: 1293 loss: 1.26986038e-06
Iter: 1294 loss: 1.26971963e-06
Iter: 1295 loss: 1.26987698e-06
Iter: 1296 loss: 1.26961936e-06
Iter: 1297 loss: 1.26951034e-06
Iter: 1298 loss: 1.26966154e-06
Iter: 1299 loss: 1.26941768e-06
Iter: 1300 loss: 1.26941472e-06
Iter: 1301 loss: 1.26932446e-06
Iter: 1302 loss: 1.26928262e-06
Iter: 1303 loss: 1.26920554e-06
Iter: 1304 loss: 1.2704927e-06
Iter: 1305 loss: 1.2691869e-06
Iter: 1306 loss: 1.26913051e-06
Iter: 1307 loss: 1.26982286e-06
Iter: 1308 loss: 1.2691371e-06
Iter: 1309 loss: 1.26906195e-06
Iter: 1310 loss: 1.26912551e-06
Iter: 1311 loss: 1.2690324e-06
Iter: 1312 loss: 1.26892155e-06
Iter: 1313 loss: 1.26902842e-06
Iter: 1314 loss: 1.26888233e-06
Iter: 1315 loss: 1.26881991e-06
Iter: 1316 loss: 1.26874158e-06
Iter: 1317 loss: 1.26873442e-06
Iter: 1318 loss: 1.26867735e-06
Iter: 1319 loss: 1.26866325e-06
Iter: 1320 loss: 1.26861369e-06
Iter: 1321 loss: 1.268613e-06
Iter: 1322 loss: 1.26857e-06
Iter: 1323 loss: 1.26850591e-06
Iter: 1324 loss: 1.26884117e-06
Iter: 1325 loss: 1.26847e-06
Iter: 1326 loss: 1.26843065e-06
Iter: 1327 loss: 1.26826535e-06
Iter: 1328 loss: 1.26908503e-06
Iter: 1329 loss: 1.26824045e-06
Iter: 1330 loss: 1.26801876e-06
Iter: 1331 loss: 1.26887699e-06
Iter: 1332 loss: 1.26796715e-06
Iter: 1333 loss: 1.26777263e-06
Iter: 1334 loss: 1.26876466e-06
Iter: 1335 loss: 1.26774307e-06
Iter: 1336 loss: 1.26756845e-06
Iter: 1337 loss: 1.26997043e-06
Iter: 1338 loss: 1.26757163e-06
Iter: 1339 loss: 1.26751047e-06
Iter: 1340 loss: 1.26750888e-06
Iter: 1341 loss: 1.26744521e-06
Iter: 1342 loss: 1.26735847e-06
Iter: 1343 loss: 1.26847851e-06
Iter: 1344 loss: 1.26736313e-06
Iter: 1345 loss: 1.26731413e-06
Iter: 1346 loss: 1.26726184e-06
Iter: 1347 loss: 1.26725024e-06
Iter: 1348 loss: 1.26709892e-06
Iter: 1349 loss: 1.26811779e-06
Iter: 1350 loss: 1.26709642e-06
Iter: 1351 loss: 1.26702525e-06
Iter: 1352 loss: 1.26707164e-06
Iter: 1353 loss: 1.26697137e-06
Iter: 1354 loss: 1.26682312e-06
Iter: 1355 loss: 1.26714212e-06
Iter: 1356 loss: 1.26676071e-06
Iter: 1357 loss: 1.2666942e-06
Iter: 1358 loss: 1.26765701e-06
Iter: 1359 loss: 1.26668431e-06
Iter: 1360 loss: 1.26661917e-06
Iter: 1361 loss: 1.26652799e-06
Iter: 1362 loss: 1.26878501e-06
Iter: 1363 loss: 1.26651628e-06
Iter: 1364 loss: 1.26638406e-06
Iter: 1365 loss: 1.26635803e-06
Iter: 1366 loss: 1.26627015e-06
Iter: 1367 loss: 1.26607074e-06
Iter: 1368 loss: 1.26652208e-06
Iter: 1369 loss: 1.2660023e-06
Iter: 1370 loss: 1.26590771e-06
Iter: 1371 loss: 1.26589475e-06
Iter: 1372 loss: 1.26578493e-06
Iter: 1373 loss: 1.26590044e-06
Iter: 1374 loss: 1.26573252e-06
Iter: 1375 loss: 1.26566692e-06
Iter: 1376 loss: 1.2655687e-06
Iter: 1377 loss: 1.2655662e-06
Iter: 1378 loss: 1.26543e-06
Iter: 1379 loss: 1.26549742e-06
Iter: 1380 loss: 1.26532382e-06
Iter: 1381 loss: 1.26524048e-06
Iter: 1382 loss: 1.26520399e-06
Iter: 1383 loss: 1.2651185e-06
Iter: 1384 loss: 1.26515022e-06
Iter: 1385 loss: 1.26506711e-06
Iter: 1386 loss: 1.264925e-06
Iter: 1387 loss: 1.26519899e-06
Iter: 1388 loss: 1.26483314e-06
Iter: 1389 loss: 1.26476368e-06
Iter: 1390 loss: 1.26525492e-06
Iter: 1391 loss: 1.2647406e-06
Iter: 1392 loss: 1.26463453e-06
Iter: 1393 loss: 1.26481132e-06
Iter: 1394 loss: 1.26459815e-06
Iter: 1395 loss: 1.26451403e-06
Iter: 1396 loss: 1.26439966e-06
Iter: 1397 loss: 1.26440432e-06
Iter: 1398 loss: 1.26427472e-06
Iter: 1399 loss: 1.26427187e-06
Iter: 1400 loss: 1.26411442e-06
Iter: 1401 loss: 1.26416626e-06
Iter: 1402 loss: 1.26401324e-06
Iter: 1403 loss: 1.26389477e-06
Iter: 1404 loss: 1.26381269e-06
Iter: 1405 loss: 1.26378018e-06
Iter: 1406 loss: 1.26363852e-06
Iter: 1407 loss: 1.2642829e-06
Iter: 1408 loss: 1.26361238e-06
Iter: 1409 loss: 1.26352813e-06
Iter: 1410 loss: 1.26352847e-06
Iter: 1411 loss: 1.26345731e-06
Iter: 1412 loss: 1.26330883e-06
Iter: 1413 loss: 1.26419991e-06
Iter: 1414 loss: 1.26328041e-06
Iter: 1415 loss: 1.2631466e-06
Iter: 1416 loss: 1.26313137e-06
Iter: 1417 loss: 1.26302245e-06
Iter: 1418 loss: 1.26306486e-06
Iter: 1419 loss: 1.26294822e-06
Iter: 1420 loss: 1.26279497e-06
Iter: 1421 loss: 1.26317934e-06
Iter: 1422 loss: 1.26274017e-06
Iter: 1423 loss: 1.26258863e-06
Iter: 1424 loss: 1.26333828e-06
Iter: 1425 loss: 1.26257191e-06
Iter: 1426 loss: 1.26244959e-06
Iter: 1427 loss: 1.26245936e-06
Iter: 1428 loss: 1.26236273e-06
Iter: 1429 loss: 1.26225893e-06
Iter: 1430 loss: 1.26218561e-06
Iter: 1431 loss: 1.26213968e-06
Iter: 1432 loss: 1.26211773e-06
Iter: 1433 loss: 1.2620709e-06
Iter: 1434 loss: 1.26196289e-06
Iter: 1435 loss: 1.26176542e-06
Iter: 1436 loss: 1.26341615e-06
Iter: 1437 loss: 1.26169778e-06
Iter: 1438 loss: 1.26146438e-06
Iter: 1439 loss: 1.26170573e-06
Iter: 1440 loss: 1.26135353e-06
Iter: 1441 loss: 1.26124212e-06
Iter: 1442 loss: 1.26121859e-06
Iter: 1443 loss: 1.26108057e-06
Iter: 1444 loss: 1.26158784e-06
Iter: 1445 loss: 1.26104214e-06
Iter: 1446 loss: 1.26095131e-06
Iter: 1447 loss: 1.26084115e-06
Iter: 1448 loss: 1.26084137e-06
Iter: 1449 loss: 1.26069062e-06
Iter: 1450 loss: 1.26068937e-06
Iter: 1451 loss: 1.26060058e-06
Iter: 1452 loss: 1.26050077e-06
Iter: 1453 loss: 1.26047144e-06
Iter: 1454 loss: 1.26023951e-06
Iter: 1455 loss: 1.26084637e-06
Iter: 1456 loss: 1.26016755e-06
Iter: 1457 loss: 1.25997894e-06
Iter: 1458 loss: 1.26064947e-06
Iter: 1459 loss: 1.25990096e-06
Iter: 1460 loss: 1.25972656e-06
Iter: 1461 loss: 1.25969882e-06
Iter: 1462 loss: 1.25959684e-06
Iter: 1463 loss: 1.25945724e-06
Iter: 1464 loss: 1.2601264e-06
Iter: 1465 loss: 1.25946917e-06
Iter: 1466 loss: 1.25932274e-06
Iter: 1467 loss: 1.26047973e-06
Iter: 1468 loss: 1.25930376e-06
Iter: 1469 loss: 1.25925567e-06
Iter: 1470 loss: 1.25909207e-06
Iter: 1471 loss: 1.26032774e-06
Iter: 1472 loss: 1.25904853e-06
Iter: 1473 loss: 1.2588207e-06
Iter: 1474 loss: 1.25870145e-06
Iter: 1475 loss: 1.2585881e-06
Iter: 1476 loss: 1.25830366e-06
Iter: 1477 loss: 1.26077032e-06
Iter: 1478 loss: 1.25827341e-06
Iter: 1479 loss: 1.25826909e-06
Iter: 1480 loss: 1.25818394e-06
Iter: 1481 loss: 1.25812016e-06
Iter: 1482 loss: 1.25799818e-06
Iter: 1483 loss: 1.2597086e-06
Iter: 1484 loss: 1.25799238e-06
Iter: 1485 loss: 1.25783936e-06
Iter: 1486 loss: 1.25776478e-06
Iter: 1487 loss: 1.25768292e-06
Iter: 1488 loss: 1.2574651e-06
Iter: 1489 loss: 1.25746465e-06
Iter: 1490 loss: 1.25733686e-06
Iter: 1491 loss: 1.25713382e-06
Iter: 1492 loss: 1.26147313e-06
Iter: 1493 loss: 1.25713575e-06
Iter: 1494 loss: 1.25694044e-06
Iter: 1495 loss: 1.25974043e-06
Iter: 1496 loss: 1.25693407e-06
Iter: 1497 loss: 1.2568272e-06
Iter: 1498 loss: 1.25811459e-06
Iter: 1499 loss: 1.25682368e-06
Iter: 1500 loss: 1.25675979e-06
Iter: 1501 loss: 1.25666816e-06
Iter: 1502 loss: 1.25665588e-06
Iter: 1503 loss: 1.25662723e-06
Iter: 1504 loss: 1.25662e-06
Iter: 1505 loss: 1.25656413e-06
Iter: 1506 loss: 1.25641645e-06
Iter: 1507 loss: 1.25760255e-06
Iter: 1508 loss: 1.25641941e-06
Iter: 1509 loss: 1.25627344e-06
Iter: 1510 loss: 1.25613212e-06
Iter: 1511 loss: 1.25610154e-06
Iter: 1512 loss: 1.25592442e-06
Iter: 1513 loss: 1.25850045e-06
Iter: 1514 loss: 1.25592658e-06
Iter: 1515 loss: 1.25576503e-06
Iter: 1516 loss: 1.25692077e-06
Iter: 1517 loss: 1.25574081e-06
Iter: 1518 loss: 1.2556676e-06
Iter: 1519 loss: 1.25550639e-06
Iter: 1520 loss: 1.25925862e-06
Iter: 1521 loss: 1.25551333e-06
Iter: 1522 loss: 1.25540328e-06
Iter: 1523 loss: 1.25539987e-06
Iter: 1524 loss: 1.25525526e-06
Iter: 1525 loss: 1.25532097e-06
Iter: 1526 loss: 1.25517636e-06
Iter: 1527 loss: 1.25506813e-06
Iter: 1528 loss: 1.25497411e-06
Iter: 1529 loss: 1.25493511e-06
Iter: 1530 loss: 1.25475412e-06
Iter: 1531 loss: 1.25476481e-06
Iter: 1532 loss: 1.25463635e-06
Iter: 1533 loss: 1.25481029e-06
Iter: 1534 loss: 1.25459087e-06
Iter: 1535 loss: 1.25446547e-06
Iter: 1536 loss: 1.25459735e-06
Iter: 1537 loss: 1.25440351e-06
Iter: 1538 loss: 1.25425754e-06
Iter: 1539 loss: 1.2554292e-06
Iter: 1540 loss: 1.25425368e-06
Iter: 1541 loss: 1.2541841e-06
Iter: 1542 loss: 1.25400311e-06
Iter: 1543 loss: 1.25543659e-06
Iter: 1544 loss: 1.25397719e-06
Iter: 1545 loss: 1.25380791e-06
Iter: 1546 loss: 1.25440533e-06
Iter: 1547 loss: 1.25373549e-06
Iter: 1548 loss: 1.25358724e-06
Iter: 1549 loss: 1.25357201e-06
Iter: 1550 loss: 1.25346332e-06
Iter: 1551 loss: 1.2532322e-06
Iter: 1552 loss: 1.25846225e-06
Iter: 1553 loss: 1.25322651e-06
Iter: 1554 loss: 1.25303268e-06
Iter: 1555 loss: 1.25387851e-06
Iter: 1556 loss: 1.25301449e-06
Iter: 1557 loss: 1.25286692e-06
Iter: 1558 loss: 1.25286192e-06
Iter: 1559 loss: 1.25280019e-06
Iter: 1560 loss: 1.2526267e-06
Iter: 1561 loss: 1.25469592e-06
Iter: 1562 loss: 1.25261954e-06
Iter: 1563 loss: 1.2525021e-06
Iter: 1564 loss: 1.2524979e-06
Iter: 1565 loss: 1.25235238e-06
Iter: 1566 loss: 1.25247038e-06
Iter: 1567 loss: 1.25225802e-06
Iter: 1568 loss: 1.25209942e-06
Iter: 1569 loss: 1.25239183e-06
Iter: 1570 loss: 1.25203371e-06
Iter: 1571 loss: 1.25192173e-06
Iter: 1572 loss: 1.25191809e-06
Iter: 1573 loss: 1.25184204e-06
Iter: 1574 loss: 1.2516806e-06
Iter: 1575 loss: 1.25435008e-06
Iter: 1576 loss: 1.25167492e-06
Iter: 1577 loss: 1.25154838e-06
Iter: 1578 loss: 1.2518334e-06
Iter: 1579 loss: 1.25149e-06
Iter: 1580 loss: 1.25138752e-06
Iter: 1581 loss: 1.25139354e-06
Iter: 1582 loss: 1.25130964e-06
Iter: 1583 loss: 1.25115673e-06
Iter: 1584 loss: 1.2550754e-06
Iter: 1585 loss: 1.25117208e-06
Iter: 1586 loss: 1.25101133e-06
Iter: 1587 loss: 1.25118288e-06
Iter: 1588 loss: 1.2509239e-06
Iter: 1589 loss: 1.25087797e-06
Iter: 1590 loss: 1.2508267e-06
Iter: 1591 loss: 1.25077179e-06
Iter: 1592 loss: 1.25064321e-06
Iter: 1593 loss: 1.25247698e-06
Iter: 1594 loss: 1.25062388e-06
Iter: 1595 loss: 1.25048791e-06
Iter: 1596 loss: 1.25088627e-06
Iter: 1597 loss: 1.25045131e-06
Iter: 1598 loss: 1.25033307e-06
Iter: 1599 loss: 1.25210011e-06
Iter: 1600 loss: 1.25033739e-06
Iter: 1601 loss: 1.25026418e-06
Iter: 1602 loss: 1.25020165e-06
Iter: 1603 loss: 1.25017618e-06
Iter: 1604 loss: 1.25004294e-06
Iter: 1605 loss: 1.25075962e-06
Iter: 1606 loss: 1.25001361e-06
Iter: 1607 loss: 1.24985468e-06
Iter: 1608 loss: 1.24970938e-06
Iter: 1609 loss: 1.24963685e-06
Iter: 1610 loss: 1.24948247e-06
Iter: 1611 loss: 1.24947678e-06
Iter: 1612 loss: 1.24935843e-06
Iter: 1613 loss: 1.24926066e-06
Iter: 1614 loss: 1.24923213e-06
Iter: 1615 loss: 1.24912424e-06
Iter: 1616 loss: 1.24912208e-06
Iter: 1617 loss: 1.24903545e-06
Iter: 1618 loss: 1.24890494e-06
Iter: 1619 loss: 1.24877852e-06
Iter: 1620 loss: 1.24874077e-06
Iter: 1621 loss: 1.24865642e-06
Iter: 1622 loss: 1.24863823e-06
Iter: 1623 loss: 1.24851931e-06
Iter: 1624 loss: 1.24828432e-06
Iter: 1625 loss: 1.25287477e-06
Iter: 1626 loss: 1.24829432e-06
Iter: 1627 loss: 1.2480557e-06
Iter: 1628 loss: 1.24835947e-06
Iter: 1629 loss: 1.24795451e-06
Iter: 1630 loss: 1.24792632e-06
Iter: 1631 loss: 1.24785902e-06
Iter: 1632 loss: 1.2477974e-06
Iter: 1633 loss: 1.24767189e-06
Iter: 1634 loss: 1.24767485e-06
Iter: 1635 loss: 1.2475507e-06
Iter: 1636 loss: 1.24849907e-06
Iter: 1637 loss: 1.24754683e-06
Iter: 1638 loss: 1.24743474e-06
Iter: 1639 loss: 1.24761584e-06
Iter: 1640 loss: 1.24738949e-06
Iter: 1641 loss: 1.24726012e-06
Iter: 1642 loss: 1.24700375e-06
Iter: 1643 loss: 1.25112911e-06
Iter: 1644 loss: 1.24699693e-06
Iter: 1645 loss: 1.24678149e-06
Iter: 1646 loss: 1.24906205e-06
Iter: 1647 loss: 1.24678718e-06
Iter: 1648 loss: 1.24664166e-06
Iter: 1649 loss: 1.24664143e-06
Iter: 1650 loss: 1.24654139e-06
Iter: 1651 loss: 1.24634562e-06
Iter: 1652 loss: 1.24821872e-06
Iter: 1653 loss: 1.24633107e-06
Iter: 1654 loss: 1.24610244e-06
Iter: 1655 loss: 1.24877351e-06
Iter: 1656 loss: 1.24611131e-06
Iter: 1657 loss: 1.24589951e-06
Iter: 1658 loss: 1.24636404e-06
Iter: 1659 loss: 1.24580151e-06
Iter: 1660 loss: 1.24564281e-06
Iter: 1661 loss: 1.24531482e-06
Iter: 1662 loss: 1.2514397e-06
Iter: 1663 loss: 1.24532835e-06
Iter: 1664 loss: 1.24509972e-06
Iter: 1665 loss: 1.24509222e-06
Iter: 1666 loss: 1.24491817e-06
Iter: 1667 loss: 1.24627286e-06
Iter: 1668 loss: 1.24490339e-06
Iter: 1669 loss: 1.24479766e-06
Iter: 1670 loss: 1.24491658e-06
Iter: 1671 loss: 1.24476389e-06
Iter: 1672 loss: 1.2446385e-06
Iter: 1673 loss: 1.24501332e-06
Iter: 1674 loss: 1.24461098e-06
Iter: 1675 loss: 1.24447843e-06
Iter: 1676 loss: 1.24427299e-06
Iter: 1677 loss: 1.24966823e-06
Iter: 1678 loss: 1.24427436e-06
Iter: 1679 loss: 1.24403846e-06
Iter: 1680 loss: 1.24424321e-06
Iter: 1681 loss: 1.2439242e-06
Iter: 1682 loss: 1.24391977e-06
Iter: 1683 loss: 1.24381586e-06
Iter: 1684 loss: 1.24372536e-06
Iter: 1685 loss: 1.2435894e-06
Iter: 1686 loss: 1.24357894e-06
Iter: 1687 loss: 1.24346298e-06
Iter: 1688 loss: 1.24365579e-06
Iter: 1689 loss: 1.24339681e-06
Iter: 1690 loss: 1.24330109e-06
Iter: 1691 loss: 1.24327903e-06
Iter: 1692 loss: 1.24323446e-06
Iter: 1693 loss: 1.24302619e-06
Iter: 1694 loss: 1.24417534e-06
Iter: 1695 loss: 1.24300618e-06
Iter: 1696 loss: 1.24271673e-06
Iter: 1697 loss: 1.24277017e-06
Iter: 1698 loss: 1.24250528e-06
Iter: 1699 loss: 1.24240728e-06
Iter: 1700 loss: 1.24233043e-06
Iter: 1701 loss: 1.24218275e-06
Iter: 1702 loss: 1.24234987e-06
Iter: 1703 loss: 1.24205758e-06
Iter: 1704 loss: 1.24197902e-06
Iter: 1705 loss: 1.24290398e-06
Iter: 1706 loss: 1.24196845e-06
Iter: 1707 loss: 1.24185885e-06
Iter: 1708 loss: 1.24189319e-06
Iter: 1709 loss: 1.24180428e-06
Iter: 1710 loss: 1.24165172e-06
Iter: 1711 loss: 1.24163125e-06
Iter: 1712 loss: 1.24155679e-06
Iter: 1713 loss: 1.24139876e-06
Iter: 1714 loss: 1.2413027e-06
Iter: 1715 loss: 1.24125859e-06
Iter: 1716 loss: 1.24125881e-06
Iter: 1717 loss: 1.24114194e-06
Iter: 1718 loss: 1.24106145e-06
Iter: 1719 loss: 1.24089274e-06
Iter: 1720 loss: 1.24408075e-06
Iter: 1721 loss: 1.24089752e-06
Iter: 1722 loss: 1.24074631e-06
Iter: 1723 loss: 1.24087637e-06
Iter: 1724 loss: 1.24066787e-06
Iter: 1725 loss: 1.24063911e-06
Iter: 1726 loss: 1.24057942e-06
Iter: 1727 loss: 1.2404879e-06
Iter: 1728 loss: 1.24026724e-06
Iter: 1729 loss: 1.24246765e-06
Iter: 1730 loss: 1.2402362e-06
Iter: 1731 loss: 1.24002872e-06
Iter: 1732 loss: 1.24026246e-06
Iter: 1733 loss: 1.23991367e-06
Iter: 1734 loss: 1.23969733e-06
Iter: 1735 loss: 1.24024245e-06
Iter: 1736 loss: 1.23961092e-06
Iter: 1737 loss: 1.2396e-06
Iter: 1738 loss: 1.23949872e-06
Iter: 1739 loss: 1.23943596e-06
Iter: 1740 loss: 1.23936638e-06
Iter: 1741 loss: 1.23937116e-06
Iter: 1742 loss: 1.23924724e-06
Iter: 1743 loss: 1.24018061e-06
Iter: 1744 loss: 1.23923769e-06
Iter: 1745 loss: 1.23916561e-06
Iter: 1746 loss: 1.23922359e-06
Iter: 1747 loss: 1.23911377e-06
Iter: 1748 loss: 1.23902714e-06
Iter: 1749 loss: 1.23884092e-06
Iter: 1750 loss: 1.24286441e-06
Iter: 1751 loss: 1.23882387e-06
Iter: 1752 loss: 1.23863663e-06
Iter: 1753 loss: 1.23904283e-06
Iter: 1754 loss: 1.23857069e-06
Iter: 1755 loss: 1.23858013e-06
Iter: 1756 loss: 1.23847519e-06
Iter: 1757 loss: 1.23841812e-06
Iter: 1758 loss: 1.23822633e-06
Iter: 1759 loss: 1.23961263e-06
Iter: 1760 loss: 1.23820564e-06
Iter: 1761 loss: 1.23806535e-06
Iter: 1762 loss: 1.23958239e-06
Iter: 1763 loss: 1.23804773e-06
Iter: 1764 loss: 1.23787868e-06
Iter: 1765 loss: 1.23839959e-06
Iter: 1766 loss: 1.23784264e-06
Iter: 1767 loss: 1.23771497e-06
Iter: 1768 loss: 1.23745303e-06
Iter: 1769 loss: 1.24049075e-06
Iter: 1770 loss: 1.23745315e-06
Iter: 1771 loss: 1.23713983e-06
Iter: 1772 loss: 1.23924633e-06
Iter: 1773 loss: 1.23711357e-06
Iter: 1774 loss: 1.23693303e-06
Iter: 1775 loss: 1.23782206e-06
Iter: 1776 loss: 1.23689176e-06
Iter: 1777 loss: 1.23672442e-06
Iter: 1778 loss: 1.23907739e-06
Iter: 1779 loss: 1.23671884e-06
Iter: 1780 loss: 1.23667007e-06
Iter: 1781 loss: 1.23685095e-06
Iter: 1782 loss: 1.23664063e-06
Iter: 1783 loss: 1.2365465e-06
Iter: 1784 loss: 1.23633595e-06
Iter: 1785 loss: 1.23909581e-06
Iter: 1786 loss: 1.23631685e-06
Iter: 1787 loss: 1.23607538e-06
Iter: 1788 loss: 1.23647e-06
Iter: 1789 loss: 1.23598556e-06
Iter: 1790 loss: 1.23566417e-06
Iter: 1791 loss: 1.23711573e-06
Iter: 1792 loss: 1.23562654e-06
Iter: 1793 loss: 1.23544714e-06
Iter: 1794 loss: 1.2360465e-06
Iter: 1795 loss: 1.23540281e-06
Iter: 1796 loss: 1.23525911e-06
Iter: 1797 loss: 1.23749771e-06
Iter: 1798 loss: 1.23525524e-06
Iter: 1799 loss: 1.23519226e-06
Iter: 1800 loss: 1.2349949e-06
Iter: 1801 loss: 1.23567474e-06
Iter: 1802 loss: 1.23490713e-06
Iter: 1803 loss: 1.23485984e-06
Iter: 1804 loss: 1.23479037e-06
Iter: 1805 loss: 1.23464326e-06
Iter: 1806 loss: 1.23455015e-06
Iter: 1807 loss: 1.23452639e-06
Iter: 1808 loss: 1.23436507e-06
Iter: 1809 loss: 1.23422296e-06
Iter: 1810 loss: 1.23419136e-06
Iter: 1811 loss: 1.2339882e-06
Iter: 1812 loss: 1.235073e-06
Iter: 1813 loss: 1.23399218e-06
Iter: 1814 loss: 1.23390896e-06
Iter: 1815 loss: 1.23388304e-06
Iter: 1816 loss: 1.23384211e-06
Iter: 1817 loss: 1.23370864e-06
Iter: 1818 loss: 1.23481993e-06
Iter: 1819 loss: 1.23369659e-06
Iter: 1820 loss: 1.23351765e-06
Iter: 1821 loss: 1.23510085e-06
Iter: 1822 loss: 1.23350651e-06
Iter: 1823 loss: 1.23342465e-06
Iter: 1824 loss: 1.23319614e-06
Iter: 1825 loss: 1.23513178e-06
Iter: 1826 loss: 1.23317227e-06
Iter: 1827 loss: 1.23291261e-06
Iter: 1828 loss: 1.23429027e-06
Iter: 1829 loss: 1.23285315e-06
Iter: 1830 loss: 1.23275868e-06
Iter: 1831 loss: 1.23271116e-06
Iter: 1832 loss: 1.23261873e-06
Iter: 1833 loss: 1.23265363e-06
Iter: 1834 loss: 1.23254767e-06
Iter: 1835 loss: 1.23242512e-06
Iter: 1836 loss: 1.23232508e-06
Iter: 1837 loss: 1.23230461e-06
Iter: 1838 loss: 1.23227278e-06
Iter: 1839 loss: 1.23221707e-06
Iter: 1840 loss: 1.23217478e-06
Iter: 1841 loss: 1.23200084e-06
Iter: 1842 loss: 1.23241239e-06
Iter: 1843 loss: 1.23187829e-06
Iter: 1844 loss: 1.231601e-06
Iter: 1845 loss: 1.23264533e-06
Iter: 1846 loss: 1.23156724e-06
Iter: 1847 loss: 1.23129041e-06
Iter: 1848 loss: 1.23146197e-06
Iter: 1849 loss: 1.23114273e-06
Iter: 1850 loss: 1.23143559e-06
Iter: 1851 loss: 1.23107282e-06
Iter: 1852 loss: 1.23098903e-06
Iter: 1853 loss: 1.23083043e-06
Iter: 1854 loss: 1.2323386e-06
Iter: 1855 loss: 1.23080133e-06
Iter: 1856 loss: 1.23066616e-06
Iter: 1857 loss: 1.23066661e-06
Iter: 1858 loss: 1.23054303e-06
Iter: 1859 loss: 1.23046698e-06
Iter: 1860 loss: 1.23041627e-06
Iter: 1861 loss: 1.23025939e-06
Iter: 1862 loss: 1.23136738e-06
Iter: 1863 loss: 1.23023483e-06
Iter: 1864 loss: 1.23009954e-06
Iter: 1865 loss: 1.23020186e-06
Iter: 1866 loss: 1.23001541e-06
Iter: 1867 loss: 1.22982919e-06
Iter: 1868 loss: 1.23046652e-06
Iter: 1869 loss: 1.22977588e-06
Iter: 1870 loss: 1.2296033e-06
Iter: 1871 loss: 1.23093309e-06
Iter: 1872 loss: 1.22959318e-06
Iter: 1873 loss: 1.22945812e-06
Iter: 1874 loss: 1.23029463e-06
Iter: 1875 loss: 1.22943106e-06
Iter: 1876 loss: 1.22931874e-06
Iter: 1877 loss: 1.22911888e-06
Iter: 1878 loss: 1.23335985e-06
Iter: 1879 loss: 1.22912331e-06
Iter: 1880 loss: 1.22886399e-06
Iter: 1881 loss: 1.22891697e-06
Iter: 1882 loss: 1.22870074e-06
Iter: 1883 loss: 1.22843835e-06
Iter: 1884 loss: 1.23038762e-06
Iter: 1885 loss: 1.22842505e-06
Iter: 1886 loss: 1.22849883e-06
Iter: 1887 loss: 1.22835854e-06
Iter: 1888 loss: 1.22834285e-06
Iter: 1889 loss: 1.22825486e-06
Iter: 1890 loss: 1.22857614e-06
Iter: 1891 loss: 1.22822689e-06
Iter: 1892 loss: 1.22806182e-06
Iter: 1893 loss: 1.22937263e-06
Iter: 1894 loss: 1.22806307e-06
Iter: 1895 loss: 1.22796268e-06
Iter: 1896 loss: 1.22780602e-06
Iter: 1897 loss: 1.23112045e-06
Iter: 1898 loss: 1.22780182e-06
Iter: 1899 loss: 1.22770405e-06
Iter: 1900 loss: 1.22768586e-06
Iter: 1901 loss: 1.22758968e-06
Iter: 1902 loss: 1.22746917e-06
Iter: 1903 loss: 1.22747008e-06
Iter: 1904 loss: 1.22730637e-06
Iter: 1905 loss: 1.22771507e-06
Iter: 1906 loss: 1.2272385e-06
Iter: 1907 loss: 1.22716e-06
Iter: 1908 loss: 1.2275184e-06
Iter: 1909 loss: 1.22713845e-06
Iter: 1910 loss: 1.22703477e-06
Iter: 1911 loss: 1.22749702e-06
Iter: 1912 loss: 1.22699385e-06
Iter: 1913 loss: 1.226893e-06
Iter: 1914 loss: 1.22704773e-06
Iter: 1915 loss: 1.22682695e-06
Iter: 1916 loss: 1.22673566e-06
Iter: 1917 loss: 1.22651159e-06
Iter: 1918 loss: 1.23071209e-06
Iter: 1919 loss: 1.22650761e-06
Iter: 1920 loss: 1.2263979e-06
Iter: 1921 loss: 1.22638892e-06
Iter: 1922 loss: 1.22630217e-06
Iter: 1923 loss: 1.22609356e-06
Iter: 1924 loss: 1.22942288e-06
Iter: 1925 loss: 1.22608913e-06
Iter: 1926 loss: 1.22605263e-06
Iter: 1927 loss: 1.22602501e-06
Iter: 1928 loss: 1.22594702e-06
Iter: 1929 loss: 1.22597396e-06
Iter: 1930 loss: 1.22587664e-06
Iter: 1931 loss: 1.22579092e-06
Iter: 1932 loss: 1.22565166e-06
Iter: 1933 loss: 1.22740562e-06
Iter: 1934 loss: 1.2256196e-06
Iter: 1935 loss: 1.22554297e-06
Iter: 1936 loss: 1.22548693e-06
Iter: 1937 loss: 1.22539132e-06
Iter: 1938 loss: 1.22528991e-06
Iter: 1939 loss: 1.22526569e-06
Iter: 1940 loss: 1.22510289e-06
Iter: 1941 loss: 1.22544225e-06
Iter: 1942 loss: 1.22505423e-06
Iter: 1943 loss: 1.22495544e-06
Iter: 1944 loss: 1.22518236e-06
Iter: 1945 loss: 1.22491019e-06
Iter: 1946 loss: 1.22482061e-06
Iter: 1947 loss: 1.22530867e-06
Iter: 1948 loss: 1.22479946e-06
Iter: 1949 loss: 1.22469419e-06
Iter: 1950 loss: 1.22481231e-06
Iter: 1951 loss: 1.22467122e-06
Iter: 1952 loss: 1.22456788e-06
Iter: 1953 loss: 1.22509e-06
Iter: 1954 loss: 1.22456959e-06
Iter: 1955 loss: 1.2244775e-06
Iter: 1956 loss: 1.22429242e-06
Iter: 1957 loss: 1.22429219e-06
Iter: 1958 loss: 1.22413303e-06
Iter: 1959 loss: 1.22413451e-06
Iter: 1960 loss: 1.22400081e-06
Iter: 1961 loss: 1.22414599e-06
Iter: 1962 loss: 1.22394385e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi0.4
+ date
Sat Oct 31 18:27:29 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7405751e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa740575510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa740563c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7405639d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa740610b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa740621510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7402cce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7402ec730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7402ec378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74044cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7403ed510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7403e2f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74041b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa740296950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7402808c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74016ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa740144620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74013c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7402582f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa740258620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa740275950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa740275ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74018a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6f00bd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6f00bda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6f00db378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7401156a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa740115488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7401fe730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7401fe488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7400c0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6dc7d3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6dc7d31e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6dc73d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa6dc772510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74008dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0046783863
test_loss: 0.004485743
train_loss: 0.0036664933
test_loss: 0.00373405
train_loss: 0.0030615926
test_loss: 0.0030514353
train_loss: 0.0026889592
test_loss: 0.002765384
train_loss: 0.0024289791
test_loss: 0.002499044
train_loss: 0.0023428537
test_loss: 0.0024594262
train_loss: 0.002293941
test_loss: 0.002338197
train_loss: 0.0020690733
test_loss: 0.0021473556
train_loss: 0.001954084
test_loss: 0.0020709394
train_loss: 0.0018465497
test_loss: 0.0020576762
train_loss: 0.0018280408
test_loss: 0.0019451648
train_loss: 0.0016453787
test_loss: 0.0018698038
train_loss: 0.0016287398
test_loss: 0.0018076332
train_loss: 0.0016746034
test_loss: 0.0017861774
train_loss: 0.0014821081
test_loss: 0.0016881066
train_loss: 0.0015147324
test_loss: 0.0016779192
train_loss: 0.0014983722
test_loss: 0.0016355832
train_loss: 0.001455351
test_loss: 0.0016003633
train_loss: 0.0014392445
test_loss: 0.0015834282
train_loss: 0.0013775898
test_loss: 0.0015505741
train_loss: 0.001426458
test_loss: 0.001533132
train_loss: 0.0013211675
test_loss: 0.001526014
train_loss: 0.001318042
test_loss: 0.0015074436
train_loss: 0.0012901496
test_loss: 0.0014844442
train_loss: 0.0012762448
test_loss: 0.0014724813
train_loss: 0.0012605146
test_loss: 0.0014574194
train_loss: 0.0012969289
test_loss: 0.001449206
train_loss: 0.0012657184
test_loss: 0.001436752
train_loss: 0.0012078169
test_loss: 0.0014356492
train_loss: 0.0012290804
test_loss: 0.0014220334
train_loss: 0.0011633375
test_loss: 0.0014097493
train_loss: 0.0011677655
test_loss: 0.0014089873
train_loss: 0.0011666081
test_loss: 0.0013987651
train_loss: 0.0011466068
test_loss: 0.0013957648
train_loss: 0.0011046333
test_loss: 0.0013918256
train_loss: 0.0011617816
test_loss: 0.0013845847
train_loss: 0.0010945899
test_loss: 0.0013849873
train_loss: 0.0011810517
test_loss: 0.0013803872
train_loss: 0.0011442576
test_loss: 0.0013790994
train_loss: 0.0011536104
test_loss: 0.001376885
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi0.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi0.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f744820e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f744820e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f744822aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f741f4ecf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f741f503730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f741f46d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f741f4a69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f741f3e5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f741f3e5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f741f40d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f741f44e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f82f4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f82f4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f832dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f832d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f82a1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f82c5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f82837b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f81f4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f81f4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f819f1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f81f4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f8229950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f818d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f818d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f80ee378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f80bd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f80cc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f8126730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f80cc268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f802fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73e00fa598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73e00f71e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73e012cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73e00a3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73f80570d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.45286e-06
Iter: 2 loss: 2.4368087e-06
Iter: 3 loss: 2.43480054e-06
Iter: 4 loss: 2.42537e-06
Iter: 5 loss: 2.43806517e-06
Iter: 6 loss: 2.42064175e-06
Iter: 7 loss: 2.41346083e-06
Iter: 8 loss: 2.42347892e-06
Iter: 9 loss: 2.40988788e-06
Iter: 10 loss: 2.40277723e-06
Iter: 11 loss: 2.46224909e-06
Iter: 12 loss: 2.40234317e-06
Iter: 13 loss: 2.39642372e-06
Iter: 14 loss: 2.3883124e-06
Iter: 15 loss: 2.3879129e-06
Iter: 16 loss: 2.38160737e-06
Iter: 17 loss: 2.41804082e-06
Iter: 18 loss: 2.38077473e-06
Iter: 19 loss: 2.37245536e-06
Iter: 20 loss: 2.38942175e-06
Iter: 21 loss: 2.3691257e-06
Iter: 22 loss: 2.3628761e-06
Iter: 23 loss: 2.35425796e-06
Iter: 24 loss: 2.35389052e-06
Iter: 25 loss: 2.35088464e-06
Iter: 26 loss: 2.34987374e-06
Iter: 27 loss: 2.34553522e-06
Iter: 28 loss: 2.34631079e-06
Iter: 29 loss: 2.34229174e-06
Iter: 30 loss: 2.3382845e-06
Iter: 31 loss: 2.32908064e-06
Iter: 32 loss: 2.45078195e-06
Iter: 33 loss: 2.32850289e-06
Iter: 34 loss: 2.31828494e-06
Iter: 35 loss: 2.38009738e-06
Iter: 36 loss: 2.31704644e-06
Iter: 37 loss: 2.31837771e-06
Iter: 38 loss: 2.31377498e-06
Iter: 39 loss: 2.31111835e-06
Iter: 40 loss: 2.3050261e-06
Iter: 41 loss: 2.38847838e-06
Iter: 42 loss: 2.30471187e-06
Iter: 43 loss: 2.3000448e-06
Iter: 44 loss: 2.29999796e-06
Iter: 45 loss: 2.29718535e-06
Iter: 46 loss: 2.30977571e-06
Iter: 47 loss: 2.29664511e-06
Iter: 48 loss: 2.2942545e-06
Iter: 49 loss: 2.28949648e-06
Iter: 50 loss: 2.38102803e-06
Iter: 51 loss: 2.28947897e-06
Iter: 52 loss: 2.28590034e-06
Iter: 53 loss: 2.28588215e-06
Iter: 54 loss: 2.28240651e-06
Iter: 55 loss: 2.28806243e-06
Iter: 56 loss: 2.28082035e-06
Iter: 57 loss: 2.27898954e-06
Iter: 58 loss: 2.27580517e-06
Iter: 59 loss: 2.27580131e-06
Iter: 60 loss: 2.27393321e-06
Iter: 61 loss: 2.27352075e-06
Iter: 62 loss: 2.27120859e-06
Iter: 63 loss: 2.26880297e-06
Iter: 64 loss: 2.26833208e-06
Iter: 65 loss: 2.26600923e-06
Iter: 66 loss: 2.26314364e-06
Iter: 67 loss: 2.26291263e-06
Iter: 68 loss: 2.25987105e-06
Iter: 69 loss: 2.30342e-06
Iter: 70 loss: 2.25988833e-06
Iter: 71 loss: 2.2590898e-06
Iter: 72 loss: 2.25865915e-06
Iter: 73 loss: 2.25799954e-06
Iter: 74 loss: 2.25619829e-06
Iter: 75 loss: 2.26564293e-06
Iter: 76 loss: 2.25563736e-06
Iter: 77 loss: 2.25393524e-06
Iter: 78 loss: 2.25387953e-06
Iter: 79 loss: 2.25259555e-06
Iter: 80 loss: 2.25168287e-06
Iter: 81 loss: 2.25121926e-06
Iter: 82 loss: 2.24907399e-06
Iter: 83 loss: 2.25246549e-06
Iter: 84 loss: 2.24801306e-06
Iter: 85 loss: 2.24654264e-06
Iter: 86 loss: 2.24653331e-06
Iter: 87 loss: 2.2453587e-06
Iter: 88 loss: 2.24362e-06
Iter: 89 loss: 2.24359565e-06
Iter: 90 loss: 2.24183987e-06
Iter: 91 loss: 2.24267706e-06
Iter: 92 loss: 2.24067753e-06
Iter: 93 loss: 2.24060841e-06
Iter: 94 loss: 2.23973507e-06
Iter: 95 loss: 2.23908319e-06
Iter: 96 loss: 2.23716688e-06
Iter: 97 loss: 2.24377618e-06
Iter: 98 loss: 2.23628558e-06
Iter: 99 loss: 2.23340294e-06
Iter: 100 loss: 2.24105816e-06
Iter: 101 loss: 2.23246593e-06
Iter: 102 loss: 2.23206598e-06
Iter: 103 loss: 2.23159282e-06
Iter: 104 loss: 2.23069128e-06
Iter: 105 loss: 2.2315578e-06
Iter: 106 loss: 2.23014649e-06
Iter: 107 loss: 2.22937433e-06
Iter: 108 loss: 2.22896779e-06
Iter: 109 loss: 2.22861217e-06
Iter: 110 loss: 2.22727772e-06
Iter: 111 loss: 2.23982215e-06
Iter: 112 loss: 2.22723065e-06
Iter: 113 loss: 2.22648032e-06
Iter: 114 loss: 2.22575659e-06
Iter: 115 loss: 2.22559606e-06
Iter: 116 loss: 2.22403969e-06
Iter: 117 loss: 2.22838617e-06
Iter: 118 loss: 2.22353719e-06
Iter: 119 loss: 2.22195513e-06
Iter: 120 loss: 2.23103439e-06
Iter: 121 loss: 2.22174094e-06
Iter: 122 loss: 2.22068593e-06
Iter: 123 loss: 2.21918845e-06
Iter: 124 loss: 2.21913797e-06
Iter: 125 loss: 2.21808477e-06
Iter: 126 loss: 2.22837571e-06
Iter: 127 loss: 2.21803907e-06
Iter: 128 loss: 2.21691789e-06
Iter: 129 loss: 2.22192807e-06
Iter: 130 loss: 2.21666505e-06
Iter: 131 loss: 2.21595565e-06
Iter: 132 loss: 2.21423852e-06
Iter: 133 loss: 2.23542111e-06
Iter: 134 loss: 2.21411847e-06
Iter: 135 loss: 2.2122972e-06
Iter: 136 loss: 2.21765072e-06
Iter: 137 loss: 2.21172377e-06
Iter: 138 loss: 2.21213077e-06
Iter: 139 loss: 2.21100163e-06
Iter: 140 loss: 2.2104241e-06
Iter: 141 loss: 2.20922311e-06
Iter: 142 loss: 2.22712515e-06
Iter: 143 loss: 2.20914239e-06
Iter: 144 loss: 2.20882202e-06
Iter: 145 loss: 2.20867196e-06
Iter: 146 loss: 2.20826246e-06
Iter: 147 loss: 2.20762286e-06
Iter: 148 loss: 2.20762627e-06
Iter: 149 loss: 2.20696052e-06
Iter: 150 loss: 2.20964466e-06
Iter: 151 loss: 2.20680295e-06
Iter: 152 loss: 2.2059628e-06
Iter: 153 loss: 2.2072727e-06
Iter: 154 loss: 2.2055367e-06
Iter: 155 loss: 2.20439551e-06
Iter: 156 loss: 2.20546917e-06
Iter: 157 loss: 2.20375614e-06
Iter: 158 loss: 2.20277229e-06
Iter: 159 loss: 2.20209404e-06
Iter: 160 loss: 2.20172751e-06
Iter: 161 loss: 2.20160609e-06
Iter: 162 loss: 2.2011568e-06
Iter: 163 loss: 2.20062793e-06
Iter: 164 loss: 2.19984804e-06
Iter: 165 loss: 2.19980757e-06
Iter: 166 loss: 2.19921549e-06
Iter: 167 loss: 2.19939557e-06
Iter: 168 loss: 2.19879166e-06
Iter: 169 loss: 2.19795902e-06
Iter: 170 loss: 2.20022321e-06
Iter: 171 loss: 2.19766298e-06
Iter: 172 loss: 2.19637445e-06
Iter: 173 loss: 2.20515676e-06
Iter: 174 loss: 2.196281e-06
Iter: 175 loss: 2.19576532e-06
Iter: 176 loss: 2.19497451e-06
Iter: 177 loss: 2.19496087e-06
Iter: 178 loss: 2.19424146e-06
Iter: 179 loss: 2.19418234e-06
Iter: 180 loss: 2.19382719e-06
Iter: 181 loss: 2.19296612e-06
Iter: 182 loss: 2.20502511e-06
Iter: 183 loss: 2.19288404e-06
Iter: 184 loss: 2.19266485e-06
Iter: 185 loss: 2.19246317e-06
Iter: 186 loss: 2.19217145e-06
Iter: 187 loss: 2.19232288e-06
Iter: 188 loss: 2.1919866e-06
Iter: 189 loss: 2.19157778e-06
Iter: 190 loss: 2.19077401e-06
Iter: 191 loss: 2.20763e-06
Iter: 192 loss: 2.19080562e-06
Iter: 193 loss: 2.18975674e-06
Iter: 194 loss: 2.19296339e-06
Iter: 195 loss: 2.18947298e-06
Iter: 196 loss: 2.18882678e-06
Iter: 197 loss: 2.18875221e-06
Iter: 198 loss: 2.18845776e-06
Iter: 199 loss: 2.18775631e-06
Iter: 200 loss: 2.19302729e-06
Iter: 201 loss: 2.18757623e-06
Iter: 202 loss: 2.18678247e-06
Iter: 203 loss: 2.18913237e-06
Iter: 204 loss: 2.18654623e-06
Iter: 205 loss: 2.1861897e-06
Iter: 206 loss: 2.1861797e-06
Iter: 207 loss: 2.1857104e-06
Iter: 208 loss: 2.18474224e-06
Iter: 209 loss: 2.20368429e-06
Iter: 210 loss: 2.18474406e-06
Iter: 211 loss: 2.18408331e-06
Iter: 212 loss: 2.18945775e-06
Iter: 213 loss: 2.18403648e-06
Iter: 214 loss: 2.18328478e-06
Iter: 215 loss: 2.18451305e-06
Iter: 216 loss: 2.182966e-06
Iter: 217 loss: 2.18253672e-06
Iter: 218 loss: 2.18295077e-06
Iter: 219 loss: 2.18228888e-06
Iter: 220 loss: 2.18190871e-06
Iter: 221 loss: 2.18189507e-06
Iter: 222 loss: 2.18167088e-06
Iter: 223 loss: 2.18130617e-06
Iter: 224 loss: 2.1813039e-06
Iter: 225 loss: 2.18072023e-06
Iter: 226 loss: 2.18207833e-06
Iter: 227 loss: 2.18051628e-06
Iter: 228 loss: 2.18006426e-06
Iter: 229 loss: 2.18253899e-06
Iter: 230 loss: 2.18000378e-06
Iter: 231 loss: 2.17938759e-06
Iter: 232 loss: 2.17970864e-06
Iter: 233 loss: 2.17897059e-06
Iter: 234 loss: 2.17840125e-06
Iter: 235 loss: 2.17763682e-06
Iter: 236 loss: 2.17760271e-06
Iter: 237 loss: 2.17747424e-06
Iter: 238 loss: 2.17733759e-06
Iter: 239 loss: 2.17702495e-06
Iter: 240 loss: 2.1773667e-06
Iter: 241 loss: 2.17686966e-06
Iter: 242 loss: 2.17661136e-06
Iter: 243 loss: 2.17609067e-06
Iter: 244 loss: 2.1869223e-06
Iter: 245 loss: 2.17608249e-06
Iter: 246 loss: 2.17567549e-06
Iter: 247 loss: 2.1756216e-06
Iter: 248 loss: 2.17541537e-06
Iter: 249 loss: 2.17490219e-06
Iter: 250 loss: 2.17978527e-06
Iter: 251 loss: 2.17481443e-06
Iter: 252 loss: 2.17450815e-06
Iter: 253 loss: 2.17447132e-06
Iter: 254 loss: 2.17413981e-06
Iter: 255 loss: 2.17368597e-06
Iter: 256 loss: 2.17367824e-06
Iter: 257 loss: 2.17324714e-06
Iter: 258 loss: 2.17576144e-06
Iter: 259 loss: 2.17320667e-06
Iter: 260 loss: 2.17285833e-06
Iter: 261 loss: 2.1740766e-06
Iter: 262 loss: 2.17274192e-06
Iter: 263 loss: 2.17235652e-06
Iter: 264 loss: 2.1743831e-06
Iter: 265 loss: 2.17231764e-06
Iter: 266 loss: 2.17197248e-06
Iter: 267 loss: 2.17129832e-06
Iter: 268 loss: 2.18333844e-06
Iter: 269 loss: 2.17128718e-06
Iter: 270 loss: 2.1708845e-06
Iter: 271 loss: 2.17088541e-06
Iter: 272 loss: 2.1705373e-06
Iter: 273 loss: 2.17267643e-06
Iter: 274 loss: 2.17048364e-06
Iter: 275 loss: 2.17022966e-06
Iter: 276 loss: 2.16984176e-06
Iter: 277 loss: 2.17893717e-06
Iter: 278 loss: 2.16984108e-06
Iter: 279 loss: 2.16968465e-06
Iter: 280 loss: 2.16957233e-06
Iter: 281 loss: 2.16936041e-06
Iter: 282 loss: 2.16883927e-06
Iter: 283 loss: 2.17526167e-06
Iter: 284 loss: 2.16880835e-06
Iter: 285 loss: 2.16842386e-06
Iter: 286 loss: 2.17281968e-06
Iter: 287 loss: 2.16840294e-06
Iter: 288 loss: 2.16795092e-06
Iter: 289 loss: 2.16791932e-06
Iter: 290 loss: 2.16758826e-06
Iter: 291 loss: 2.16718058e-06
Iter: 292 loss: 2.16768944e-06
Iter: 293 loss: 2.16699164e-06
Iter: 294 loss: 2.16655371e-06
Iter: 295 loss: 2.1684657e-06
Iter: 296 loss: 2.16645117e-06
Iter: 297 loss: 2.1660826e-06
Iter: 298 loss: 2.16986041e-06
Iter: 299 loss: 2.16607214e-06
Iter: 300 loss: 2.16576063e-06
Iter: 301 loss: 2.16520789e-06
Iter: 302 loss: 2.1775395e-06
Iter: 303 loss: 2.16522562e-06
Iter: 304 loss: 2.16469789e-06
Iter: 305 loss: 2.16754029e-06
Iter: 306 loss: 2.16458966e-06
Iter: 307 loss: 2.16426588e-06
Iter: 308 loss: 2.1675437e-06
Iter: 309 loss: 2.16428907e-06
Iter: 310 loss: 2.16408444e-06
Iter: 311 loss: 2.16366789e-06
Iter: 312 loss: 2.17119714e-06
Iter: 313 loss: 2.16365288e-06
Iter: 314 loss: 2.16337958e-06
Iter: 315 loss: 2.16336275e-06
Iter: 316 loss: 2.16306512e-06
Iter: 317 loss: 2.16288981e-06
Iter: 318 loss: 2.16271064e-06
Iter: 319 loss: 2.16241278e-06
Iter: 320 loss: 2.16284388e-06
Iter: 321 loss: 2.16229409e-06
Iter: 322 loss: 2.16186754e-06
Iter: 323 loss: 2.16465151e-06
Iter: 324 loss: 2.16184071e-06
Iter: 325 loss: 2.16155377e-06
Iter: 326 loss: 2.16105809e-06
Iter: 327 loss: 2.17254683e-06
Iter: 328 loss: 2.16103672e-06
Iter: 329 loss: 2.16052422e-06
Iter: 330 loss: 2.16400804e-06
Iter: 331 loss: 2.16049466e-06
Iter: 332 loss: 2.16015428e-06
Iter: 333 loss: 2.16532453e-06
Iter: 334 loss: 2.16013154e-06
Iter: 335 loss: 2.15981527e-06
Iter: 336 loss: 2.15966452e-06
Iter: 337 loss: 2.15953605e-06
Iter: 338 loss: 2.15917134e-06
Iter: 339 loss: 2.16008948e-06
Iter: 340 loss: 2.15904902e-06
Iter: 341 loss: 2.15877958e-06
Iter: 342 loss: 2.15876662e-06
Iter: 343 loss: 2.15856971e-06
Iter: 344 loss: 2.15812724e-06
Iter: 345 loss: 2.16424951e-06
Iter: 346 loss: 2.15810405e-06
Iter: 347 loss: 2.15772388e-06
Iter: 348 loss: 2.16151193e-06
Iter: 349 loss: 2.15772616e-06
Iter: 350 loss: 2.15737828e-06
Iter: 351 loss: 2.15900104e-06
Iter: 352 loss: 2.15726823e-06
Iter: 353 loss: 2.15706905e-06
Iter: 354 loss: 2.15674072e-06
Iter: 355 loss: 2.15673299e-06
Iter: 356 loss: 2.15650425e-06
Iter: 357 loss: 2.15647697e-06
Iter: 358 loss: 2.15629507e-06
Iter: 359 loss: 2.15584714e-06
Iter: 360 loss: 2.16034186e-06
Iter: 361 loss: 2.15580803e-06
Iter: 362 loss: 2.15524551e-06
Iter: 363 loss: 2.15681848e-06
Iter: 364 loss: 2.15511068e-06
Iter: 365 loss: 2.15466844e-06
Iter: 366 loss: 2.16054468e-06
Iter: 367 loss: 2.1546291e-06
Iter: 368 loss: 2.15434306e-06
Iter: 369 loss: 2.15589853e-06
Iter: 370 loss: 2.15424348e-06
Iter: 371 loss: 2.15398609e-06
Iter: 372 loss: 2.15362024e-06
Iter: 373 loss: 2.15360478e-06
Iter: 374 loss: 2.15348882e-06
Iter: 375 loss: 2.15339423e-06
Iter: 376 loss: 2.1531755e-06
Iter: 377 loss: 2.15285854e-06
Iter: 378 loss: 2.15283399e-06
Iter: 379 loss: 2.15251634e-06
Iter: 380 loss: 2.15342607e-06
Iter: 381 loss: 2.15240789e-06
Iter: 382 loss: 2.15209207e-06
Iter: 383 loss: 2.1568485e-06
Iter: 384 loss: 2.15209e-06
Iter: 385 loss: 2.15191267e-06
Iter: 386 loss: 2.1515516e-06
Iter: 387 loss: 2.15851333e-06
Iter: 388 loss: 2.15156206e-06
Iter: 389 loss: 2.15132104e-06
Iter: 390 loss: 2.15129876e-06
Iter: 391 loss: 2.15111982e-06
Iter: 392 loss: 2.15071486e-06
Iter: 393 loss: 2.1600531e-06
Iter: 394 loss: 2.15070622e-06
Iter: 395 loss: 2.15036471e-06
Iter: 396 loss: 2.15110254e-06
Iter: 397 loss: 2.15023783e-06
Iter: 398 loss: 2.14978627e-06
Iter: 399 loss: 2.15059322e-06
Iter: 400 loss: 2.14963347e-06
Iter: 401 loss: 2.14916304e-06
Iter: 402 loss: 2.15574664e-06
Iter: 403 loss: 2.14917145e-06
Iter: 404 loss: 2.14895908e-06
Iter: 405 loss: 2.14866805e-06
Iter: 406 loss: 2.14864917e-06
Iter: 407 loss: 2.14833562e-06
Iter: 408 loss: 2.1519063e-06
Iter: 409 loss: 2.14835291e-06
Iter: 410 loss: 2.14803094e-06
Iter: 411 loss: 2.14868805e-06
Iter: 412 loss: 2.14791976e-06
Iter: 413 loss: 2.14771808e-06
Iter: 414 loss: 2.14773809e-06
Iter: 415 loss: 2.14756983e-06
Iter: 416 loss: 2.14737793e-06
Iter: 417 loss: 2.14736838e-06
Iter: 418 loss: 2.14726697e-06
Iter: 419 loss: 2.14693455e-06
Iter: 420 loss: 2.15109822e-06
Iter: 421 loss: 2.14691227e-06
Iter: 422 loss: 2.14670013e-06
Iter: 423 loss: 2.1466858e-06
Iter: 424 loss: 2.14646798e-06
Iter: 425 loss: 2.14627789e-06
Iter: 426 loss: 2.14619286e-06
Iter: 427 loss: 2.14593456e-06
Iter: 428 loss: 2.14599731e-06
Iter: 429 loss: 2.14580496e-06
Iter: 430 loss: 2.14540978e-06
Iter: 431 loss: 2.14711804e-06
Iter: 432 loss: 2.14533793e-06
Iter: 433 loss: 2.14512966e-06
Iter: 434 loss: 2.14850593e-06
Iter: 435 loss: 2.1451408e-06
Iter: 436 loss: 2.1448991e-06
Iter: 437 loss: 2.14450438e-06
Iter: 438 loss: 2.14451279e-06
Iter: 439 loss: 2.14420561e-06
Iter: 440 loss: 2.14738748e-06
Iter: 441 loss: 2.14420197e-06
Iter: 442 loss: 2.14398e-06
Iter: 443 loss: 2.14518604e-06
Iter: 444 loss: 2.14394254e-06
Iter: 445 loss: 2.14374427e-06
Iter: 446 loss: 2.14341208e-06
Iter: 447 loss: 2.14342026e-06
Iter: 448 loss: 2.14322154e-06
Iter: 449 loss: 2.14319198e-06
Iter: 450 loss: 2.14298575e-06
Iter: 451 loss: 2.14279316e-06
Iter: 452 loss: 2.14274223e-06
Iter: 453 loss: 2.14246984e-06
Iter: 454 loss: 2.1436972e-06
Iter: 455 loss: 2.1424612e-06
Iter: 456 loss: 2.14209649e-06
Iter: 457 loss: 2.14251895e-06
Iter: 458 loss: 2.14195416e-06
Iter: 459 loss: 2.14171337e-06
Iter: 460 loss: 2.14141164e-06
Iter: 461 loss: 2.14135957e-06
Iter: 462 loss: 2.14103125e-06
Iter: 463 loss: 2.14423653e-06
Iter: 464 loss: 2.14098918e-06
Iter: 465 loss: 2.14071042e-06
Iter: 466 loss: 2.14271017e-06
Iter: 467 loss: 2.14070951e-06
Iter: 468 loss: 2.14043121e-06
Iter: 469 loss: 2.14133979e-06
Iter: 470 loss: 2.14036208e-06
Iter: 471 loss: 2.14020201e-06
Iter: 472 loss: 2.13982162e-06
Iter: 473 loss: 2.14789225e-06
Iter: 474 loss: 2.13983026e-06
Iter: 475 loss: 2.13971748e-06
Iter: 476 loss: 2.13962676e-06
Iter: 477 loss: 2.13944031e-06
Iter: 478 loss: 2.13915109e-06
Iter: 479 loss: 2.14504871e-06
Iter: 480 loss: 2.13915041e-06
Iter: 481 loss: 2.13883209e-06
Iter: 482 loss: 2.13988255e-06
Iter: 483 loss: 2.13873636e-06
Iter: 484 loss: 2.13836233e-06
Iter: 485 loss: 2.14120564e-06
Iter: 486 loss: 2.13834846e-06
Iter: 487 loss: 2.13820977e-06
Iter: 488 loss: 2.13810335e-06
Iter: 489 loss: 2.13804833e-06
Iter: 490 loss: 2.13779458e-06
Iter: 491 loss: 2.14081092e-06
Iter: 492 loss: 2.13779731e-06
Iter: 493 loss: 2.13761041e-06
Iter: 494 loss: 2.13724661e-06
Iter: 495 loss: 2.14227703e-06
Iter: 496 loss: 2.13724115e-06
Iter: 497 loss: 2.13689646e-06
Iter: 498 loss: 2.13937233e-06
Iter: 499 loss: 2.13690555e-06
Iter: 500 loss: 2.13661133e-06
Iter: 501 loss: 2.13855242e-06
Iter: 502 loss: 2.13660041e-06
Iter: 503 loss: 2.13641079e-06
Iter: 504 loss: 2.13826956e-06
Iter: 505 loss: 2.13641806e-06
Iter: 506 loss: 2.13627936e-06
Iter: 507 loss: 2.13597e-06
Iter: 508 loss: 2.14254396e-06
Iter: 509 loss: 2.13596672e-06
Iter: 510 loss: 2.1358187e-06
Iter: 511 loss: 2.13578392e-06
Iter: 512 loss: 2.13559565e-06
Iter: 513 loss: 2.13535668e-06
Iter: 514 loss: 2.13533485e-06
Iter: 515 loss: 2.1350786e-06
Iter: 516 loss: 2.13502381e-06
Iter: 517 loss: 2.13482326e-06
Iter: 518 loss: 2.13470707e-06
Iter: 519 loss: 2.13463272e-06
Iter: 520 loss: 2.13451631e-06
Iter: 521 loss: 2.13424664e-06
Iter: 522 loss: 2.13981957e-06
Iter: 523 loss: 2.134238e-06
Iter: 524 loss: 2.13414455e-06
Iter: 525 loss: 2.13413523e-06
Iter: 526 loss: 2.13399562e-06
Iter: 527 loss: 2.13370572e-06
Iter: 528 loss: 2.13708654e-06
Iter: 529 loss: 2.13365183e-06
Iter: 530 loss: 2.13333897e-06
Iter: 531 loss: 2.13322505e-06
Iter: 532 loss: 2.13301109e-06
Iter: 533 loss: 2.13267413e-06
Iter: 534 loss: 2.13490148e-06
Iter: 535 loss: 2.13260091e-06
Iter: 536 loss: 2.13226235e-06
Iter: 537 loss: 2.13581029e-06
Iter: 538 loss: 2.13226849e-06
Iter: 539 loss: 2.13206408e-06
Iter: 540 loss: 2.13308158e-06
Iter: 541 loss: 2.13202838e-06
Iter: 542 loss: 2.13191129e-06
Iter: 543 loss: 2.13184603e-06
Iter: 544 loss: 2.13181829e-06
Iter: 545 loss: 2.13159979e-06
Iter: 546 loss: 2.13314979e-06
Iter: 547 loss: 2.1315841e-06
Iter: 548 loss: 2.13145154e-06
Iter: 549 loss: 2.13117505e-06
Iter: 550 loss: 2.13534804e-06
Iter: 551 loss: 2.13115391e-06
Iter: 552 loss: 2.13097655e-06
Iter: 553 loss: 2.1309645e-06
Iter: 554 loss: 2.13074463e-06
Iter: 555 loss: 2.13062822e-06
Iter: 556 loss: 2.13052544e-06
Iter: 557 loss: 2.13032718e-06
Iter: 558 loss: 2.13075032e-06
Iter: 559 loss: 2.13025078e-06
Iter: 560 loss: 2.13001431e-06
Iter: 561 loss: 2.13307521e-06
Iter: 562 loss: 2.12999066e-06
Iter: 563 loss: 2.12986561e-06
Iter: 564 loss: 2.12961254e-06
Iter: 565 loss: 2.13262274e-06
Iter: 566 loss: 2.12958116e-06
Iter: 567 loss: 2.1292708e-06
Iter: 568 loss: 2.12999e-06
Iter: 569 loss: 2.12918121e-06
Iter: 570 loss: 2.12878331e-06
Iter: 571 loss: 2.13018984e-06
Iter: 572 loss: 2.128696e-06
Iter: 573 loss: 2.12832e-06
Iter: 574 loss: 2.12832674e-06
Iter: 575 loss: 2.12815871e-06
Iter: 576 loss: 2.12836312e-06
Iter: 577 loss: 2.12806458e-06
Iter: 578 loss: 2.12790201e-06
Iter: 579 loss: 2.12775035e-06
Iter: 580 loss: 2.12766804e-06
Iter: 581 loss: 2.12758596e-06
Iter: 582 loss: 2.1275464e-06
Iter: 583 loss: 2.12742634e-06
Iter: 584 loss: 2.12715327e-06
Iter: 585 loss: 2.13103885e-06
Iter: 586 loss: 2.12714122e-06
Iter: 587 loss: 2.12696636e-06
Iter: 588 loss: 2.12862e-06
Iter: 589 loss: 2.1269725e-06
Iter: 590 loss: 2.12673694e-06
Iter: 591 loss: 2.12730811e-06
Iter: 592 loss: 2.12664099e-06
Iter: 593 loss: 2.1264982e-06
Iter: 594 loss: 2.12628629e-06
Iter: 595 loss: 2.12629106e-06
Iter: 596 loss: 2.12619216e-06
Iter: 597 loss: 2.1261751e-06
Iter: 598 loss: 2.12607029e-06
Iter: 599 loss: 2.12591362e-06
Iter: 600 loss: 2.12590885e-06
Iter: 601 loss: 2.12577697e-06
Iter: 602 loss: 2.12542818e-06
Iter: 603 loss: 2.12939244e-06
Iter: 604 loss: 2.12539635e-06
Iter: 605 loss: 2.12503096e-06
Iter: 606 loss: 2.12914028e-06
Iter: 607 loss: 2.125018e-06
Iter: 608 loss: 2.12481291e-06
Iter: 609 loss: 2.12480109e-06
Iter: 610 loss: 2.12470786e-06
Iter: 611 loss: 2.12441637e-06
Iter: 612 loss: 2.12885971e-06
Iter: 613 loss: 2.12437476e-06
Iter: 614 loss: 2.12419627e-06
Iter: 615 loss: 2.12418809e-06
Iter: 616 loss: 2.12403347e-06
Iter: 617 loss: 2.12500117e-06
Iter: 618 loss: 2.12400573e-06
Iter: 619 loss: 2.12389841e-06
Iter: 620 loss: 2.12377449e-06
Iter: 621 loss: 2.12374607e-06
Iter: 622 loss: 2.12349e-06
Iter: 623 loss: 2.12318218e-06
Iter: 624 loss: 2.12317627e-06
Iter: 625 loss: 2.12287978e-06
Iter: 626 loss: 2.12285295e-06
Iter: 627 loss: 2.12267173e-06
Iter: 628 loss: 2.12221812e-06
Iter: 629 loss: 2.12646e-06
Iter: 630 loss: 2.12212353e-06
Iter: 631 loss: 2.12184659e-06
Iter: 632 loss: 2.12417785e-06
Iter: 633 loss: 2.12184182e-06
Iter: 634 loss: 2.12151e-06
Iter: 635 loss: 2.1237156e-06
Iter: 636 loss: 2.12147e-06
Iter: 637 loss: 2.12128225e-06
Iter: 638 loss: 2.12092027e-06
Iter: 639 loss: 2.12937e-06
Iter: 640 loss: 2.12092891e-06
Iter: 641 loss: 2.12054465e-06
Iter: 642 loss: 2.12123859e-06
Iter: 643 loss: 2.12042551e-06
Iter: 644 loss: 2.12007603e-06
Iter: 645 loss: 2.12007535e-06
Iter: 646 loss: 2.11984275e-06
Iter: 647 loss: 2.11961174e-06
Iter: 648 loss: 2.11955353e-06
Iter: 649 loss: 2.11933684e-06
Iter: 650 loss: 2.11932274e-06
Iter: 651 loss: 2.11912447e-06
Iter: 652 loss: 2.11896281e-06
Iter: 653 loss: 2.11890483e-06
Iter: 654 loss: 2.11857719e-06
Iter: 655 loss: 2.11927727e-06
Iter: 656 loss: 2.11849465e-06
Iter: 657 loss: 2.1181886e-06
Iter: 658 loss: 2.11815518e-06
Iter: 659 loss: 2.11793576e-06
Iter: 660 loss: 2.11765087e-06
Iter: 661 loss: 2.11762176e-06
Iter: 662 loss: 2.11749057e-06
Iter: 663 loss: 2.1171229e-06
Iter: 664 loss: 2.12151076e-06
Iter: 665 loss: 2.11708175e-06
Iter: 666 loss: 2.11673478e-06
Iter: 667 loss: 2.11687211e-06
Iter: 668 loss: 2.11650149e-06
Iter: 669 loss: 2.11615179e-06
Iter: 670 loss: 2.11994688e-06
Iter: 671 loss: 2.11616293e-06
Iter: 672 loss: 2.11589554e-06
Iter: 673 loss: 2.11993438e-06
Iter: 674 loss: 2.11588667e-06
Iter: 675 loss: 2.11569704e-06
Iter: 676 loss: 2.11524775e-06
Iter: 677 loss: 2.11930751e-06
Iter: 678 loss: 2.11518272e-06
Iter: 679 loss: 2.11495399e-06
Iter: 680 loss: 2.11491988e-06
Iter: 681 loss: 2.1146277e-06
Iter: 682 loss: 2.11446422e-06
Iter: 683 loss: 2.11433758e-06
Iter: 684 loss: 2.11406632e-06
Iter: 685 loss: 2.11396946e-06
Iter: 686 loss: 2.11379506e-06
Iter: 687 loss: 2.11374709e-06
Iter: 688 loss: 2.11364295e-06
Iter: 689 loss: 2.11345514e-06
Iter: 690 loss: 2.11316433e-06
Iter: 691 loss: 2.12084069e-06
Iter: 692 loss: 2.11317911e-06
Iter: 693 loss: 2.11283714e-06
Iter: 694 loss: 2.11474935e-06
Iter: 695 loss: 2.11281304e-06
Iter: 696 loss: 2.11252382e-06
Iter: 697 loss: 2.11361089e-06
Iter: 698 loss: 2.11247584e-06
Iter: 699 loss: 2.11219435e-06
Iter: 700 loss: 2.11207566e-06
Iter: 701 loss: 2.11194697e-06
Iter: 702 loss: 2.11160523e-06
Iter: 703 loss: 2.11177394e-06
Iter: 704 loss: 2.11145652e-06
Iter: 705 loss: 2.111105e-06
Iter: 706 loss: 2.1112678e-06
Iter: 707 loss: 2.11082465e-06
Iter: 708 loss: 2.11049883e-06
Iter: 709 loss: 2.11048246e-06
Iter: 710 loss: 2.11028373e-06
Iter: 711 loss: 2.10991129e-06
Iter: 712 loss: 2.11597762e-06
Iter: 713 loss: 2.10990856e-06
Iter: 714 loss: 2.10961571e-06
Iter: 715 loss: 2.11226643e-06
Iter: 716 loss: 2.10960116e-06
Iter: 717 loss: 2.10923918e-06
Iter: 718 loss: 2.10962071e-06
Iter: 719 loss: 2.10900907e-06
Iter: 720 loss: 2.10883331e-06
Iter: 721 loss: 2.10847679e-06
Iter: 722 loss: 2.10849385e-06
Iter: 723 loss: 2.10821918e-06
Iter: 724 loss: 2.10822168e-06
Iter: 725 loss: 2.1078904e-06
Iter: 726 loss: 2.108643e-06
Iter: 727 loss: 2.10777625e-06
Iter: 728 loss: 2.10756184e-06
Iter: 729 loss: 2.1073206e-06
Iter: 730 loss: 2.10731e-06
Iter: 731 loss: 2.10710095e-06
Iter: 732 loss: 2.10708049e-06
Iter: 733 loss: 2.10696749e-06
Iter: 734 loss: 2.10674671e-06
Iter: 735 loss: 2.1067558e-06
Iter: 736 loss: 2.10648386e-06
Iter: 737 loss: 2.10702501e-06
Iter: 738 loss: 2.10635835e-06
Iter: 739 loss: 2.10614371e-06
Iter: 740 loss: 2.10628104e-06
Iter: 741 loss: 2.10598637e-06
Iter: 742 loss: 2.10573467e-06
Iter: 743 loss: 2.10572216e-06
Iter: 744 loss: 2.10554208e-06
Iter: 745 loss: 2.10506e-06
Iter: 746 loss: 2.10881444e-06
Iter: 747 loss: 2.10496842e-06
Iter: 748 loss: 2.10459575e-06
Iter: 749 loss: 2.10762028e-06
Iter: 750 loss: 2.10459712e-06
Iter: 751 loss: 2.10443523e-06
Iter: 752 loss: 2.10440794e-06
Iter: 753 loss: 2.10429198e-06
Iter: 754 loss: 2.10395319e-06
Iter: 755 loss: 2.105985e-06
Iter: 756 loss: 2.10383087e-06
Iter: 757 loss: 2.10351573e-06
Iter: 758 loss: 2.10625331e-06
Iter: 759 loss: 2.10346639e-06
Iter: 760 loss: 2.10315102e-06
Iter: 761 loss: 2.10665462e-06
Iter: 762 loss: 2.10314556e-06
Iter: 763 loss: 2.10298049e-06
Iter: 764 loss: 2.10256803e-06
Iter: 765 loss: 2.10559551e-06
Iter: 766 loss: 2.10242843e-06
Iter: 767 loss: 2.10249209e-06
Iter: 768 loss: 2.10221924e-06
Iter: 769 loss: 2.10204462e-06
Iter: 770 loss: 2.10171265e-06
Iter: 771 loss: 2.10611665e-06
Iter: 772 loss: 2.10167354e-06
Iter: 773 loss: 2.10139319e-06
Iter: 774 loss: 2.10372446e-06
Iter: 775 loss: 2.10136159e-06
Iter: 776 loss: 2.10110989e-06
Iter: 777 loss: 2.101473e-06
Iter: 778 loss: 2.10097619e-06
Iter: 779 loss: 2.10071562e-06
Iter: 780 loss: 2.10071448e-06
Iter: 781 loss: 2.10057487e-06
Iter: 782 loss: 2.10029793e-06
Iter: 783 loss: 2.10647977e-06
Iter: 784 loss: 2.1002943e-06
Iter: 785 loss: 2.09994869e-06
Iter: 786 loss: 2.09963582e-06
Iter: 787 loss: 2.09954987e-06
Iter: 788 loss: 2.09915606e-06
Iter: 789 loss: 2.10302869e-06
Iter: 790 loss: 2.09913264e-06
Iter: 791 loss: 2.09877771e-06
Iter: 792 loss: 2.09907375e-06
Iter: 793 loss: 2.09858172e-06
Iter: 794 loss: 2.09844188e-06
Iter: 795 loss: 2.09830796e-06
Iter: 796 loss: 2.09816949e-06
Iter: 797 loss: 2.09782775e-06
Iter: 798 loss: 2.10224971e-06
Iter: 799 loss: 2.09780319e-06
Iter: 800 loss: 2.0975308e-06
Iter: 801 loss: 2.10091662e-06
Iter: 802 loss: 2.09753625e-06
Iter: 803 loss: 2.09717109e-06
Iter: 804 loss: 2.09759946e-06
Iter: 805 loss: 2.09702603e-06
Iter: 806 loss: 2.09686368e-06
Iter: 807 loss: 2.0978714e-06
Iter: 808 loss: 2.0968082e-06
Iter: 809 loss: 2.0966163e-06
Iter: 810 loss: 2.09649647e-06
Iter: 811 loss: 2.09642303e-06
Iter: 812 loss: 2.09613427e-06
Iter: 813 loss: 2.09647055e-06
Iter: 814 loss: 2.09596919e-06
Iter: 815 loss: 2.09566133e-06
Iter: 816 loss: 2.09794507e-06
Iter: 817 loss: 2.09562086e-06
Iter: 818 loss: 2.09531345e-06
Iter: 819 loss: 2.09578866e-06
Iter: 820 loss: 2.09519249e-06
Iter: 821 loss: 2.09492327e-06
Iter: 822 loss: 2.0947507e-06
Iter: 823 loss: 2.09462587e-06
Iter: 824 loss: 2.09434211e-06
Iter: 825 loss: 2.09479e-06
Iter: 826 loss: 2.09424934e-06
Iter: 827 loss: 2.093896e-06
Iter: 828 loss: 2.09488212e-06
Iter: 829 loss: 2.09379868e-06
Iter: 830 loss: 2.09370091e-06
Iter: 831 loss: 2.09364407e-06
Iter: 832 loss: 2.09349355e-06
Iter: 833 loss: 2.09310156e-06
Iter: 834 loss: 2.09814471e-06
Iter: 835 loss: 2.0930936e-06
Iter: 836 loss: 2.09282416e-06
Iter: 837 loss: 2.09473092e-06
Iter: 838 loss: 2.09279165e-06
Iter: 839 loss: 2.09252812e-06
Iter: 840 loss: 2.09579389e-06
Iter: 841 loss: 2.09250379e-06
Iter: 842 loss: 2.09239988e-06
Iter: 843 loss: 2.09231803e-06
Iter: 844 loss: 2.09226255e-06
Iter: 845 loss: 2.09205109e-06
Iter: 846 loss: 2.09256405e-06
Iter: 847 loss: 2.09195468e-06
Iter: 848 loss: 2.09176937e-06
Iter: 849 loss: 2.09159043e-06
Iter: 850 loss: 2.09155883e-06
Iter: 851 loss: 2.09130121e-06
Iter: 852 loss: 2.09515224e-06
Iter: 853 loss: 2.09129939e-06
Iter: 854 loss: 2.09107543e-06
Iter: 855 loss: 2.0915304e-06
Iter: 856 loss: 2.09105156e-06
Iter: 857 loss: 2.090826e-06
Iter: 858 loss: 2.09059772e-06
Iter: 859 loss: 2.09057816e-06
Iter: 860 loss: 2.0902387e-06
Iter: 861 loss: 2.09032032e-06
Iter: 862 loss: 2.08999791e-06
Iter: 863 loss: 2.08962547e-06
Iter: 864 loss: 2.0905436e-06
Iter: 865 loss: 2.08949791e-06
Iter: 866 loss: 2.08909273e-06
Iter: 867 loss: 2.09175232e-06
Iter: 868 loss: 2.08905021e-06
Iter: 869 loss: 2.08877896e-06
Iter: 870 loss: 2.08877282e-06
Iter: 871 loss: 2.0886564e-06
Iter: 872 loss: 2.08839401e-06
Iter: 873 loss: 2.09172913e-06
Iter: 874 loss: 2.0883549e-06
Iter: 875 loss: 2.08833762e-06
Iter: 876 loss: 2.08820143e-06
Iter: 877 loss: 2.08809615e-06
Iter: 878 loss: 2.08786264e-06
Iter: 879 loss: 2.09197742e-06
Iter: 880 loss: 2.08787196e-06
Iter: 881 loss: 2.08776e-06
Iter: 882 loss: 2.0877053e-06
Iter: 883 loss: 2.08762867e-06
Iter: 884 loss: 2.08735446e-06
Iter: 885 loss: 2.0894754e-06
Iter: 886 loss: 2.0873149e-06
Iter: 887 loss: 2.08706524e-06
Iter: 888 loss: 2.08895381e-06
Iter: 889 loss: 2.08703477e-06
Iter: 890 loss: 2.0868481e-06
Iter: 891 loss: 2.08857773e-06
Iter: 892 loss: 2.08682377e-06
Iter: 893 loss: 2.08671509e-06
Iter: 894 loss: 2.08651e-06
Iter: 895 loss: 2.08649021e-06
Iter: 896 loss: 2.08629694e-06
Iter: 897 loss: 2.0864195e-06
Iter: 898 loss: 2.08613619e-06
Iter: 899 loss: 2.08589313e-06
Iter: 900 loss: 2.08635e-06
Iter: 901 loss: 2.08574238e-06
Iter: 902 loss: 2.08562642e-06
Iter: 903 loss: 2.08554798e-06
Iter: 904 loss: 2.08538131e-06
Iter: 905 loss: 2.08507845e-06
Iter: 906 loss: 2.09125301e-06
Iter: 907 loss: 2.08507663e-06
Iter: 908 loss: 2.08481197e-06
Iter: 909 loss: 2.08525262e-06
Iter: 910 loss: 2.08469055e-06
Iter: 911 loss: 2.08453775e-06
Iter: 912 loss: 2.0845132e-06
Iter: 913 loss: 2.08443521e-06
Iter: 914 loss: 2.08429128e-06
Iter: 915 loss: 2.08429628e-06
Iter: 916 loss: 2.08411848e-06
Iter: 917 loss: 2.08466986e-06
Iter: 918 loss: 2.08404344e-06
Iter: 919 loss: 2.08390202e-06
Iter: 920 loss: 2.0835821e-06
Iter: 921 loss: 2.08885831e-06
Iter: 922 loss: 2.08357596e-06
Iter: 923 loss: 2.08331699e-06
Iter: 924 loss: 2.08695315e-06
Iter: 925 loss: 2.08330857e-06
Iter: 926 loss: 2.08307029e-06
Iter: 927 loss: 2.08396204e-06
Iter: 928 loss: 2.08298889e-06
Iter: 929 loss: 2.08280903e-06
Iter: 930 loss: 2.08266511e-06
Iter: 931 loss: 2.08257256e-06
Iter: 932 loss: 2.08232927e-06
Iter: 933 loss: 2.08357824e-06
Iter: 934 loss: 2.08228312e-06
Iter: 935 loss: 2.0820753e-06
Iter: 936 loss: 2.08224924e-06
Iter: 937 loss: 2.08191068e-06
Iter: 938 loss: 2.08171514e-06
Iter: 939 loss: 2.08264078e-06
Iter: 940 loss: 2.08164874e-06
Iter: 941 loss: 2.08139318e-06
Iter: 942 loss: 2.08343727e-06
Iter: 943 loss: 2.08137681e-06
Iter: 944 loss: 2.08124084e-06
Iter: 945 loss: 2.08102642e-06
Iter: 946 loss: 2.08102279e-06
Iter: 947 loss: 2.08076676e-06
Iter: 948 loss: 2.08083475e-06
Iter: 949 loss: 2.08060237e-06
Iter: 950 loss: 2.0807463e-06
Iter: 951 loss: 2.0804614e-06
Iter: 952 loss: 2.08040774e-06
Iter: 953 loss: 2.08019014e-06
Iter: 954 loss: 2.08324172e-06
Iter: 955 loss: 2.08014944e-06
Iter: 956 loss: 2.08005326e-06
Iter: 957 loss: 2.08004076e-06
Iter: 958 loss: 2.07992434e-06
Iter: 959 loss: 2.07964831e-06
Iter: 960 loss: 2.0837183e-06
Iter: 961 loss: 2.07965422e-06
Iter: 962 loss: 2.07943549e-06
Iter: 963 loss: 2.07998937e-06
Iter: 964 loss: 2.07935318e-06
Iter: 965 loss: 2.07919447e-06
Iter: 966 loss: 2.08097117e-06
Iter: 967 loss: 2.07918083e-06
Iter: 968 loss: 2.07898029e-06
Iter: 969 loss: 2.07966264e-06
Iter: 970 loss: 2.07896232e-06
Iter: 971 loss: 2.07883954e-06
Iter: 972 loss: 2.07876428e-06
Iter: 973 loss: 2.07870426e-06
Iter: 974 loss: 2.07850212e-06
Iter: 975 loss: 2.07870107e-06
Iter: 976 loss: 2.07838821e-06
Iter: 977 loss: 2.07821881e-06
Iter: 978 loss: 2.078227e-06
Iter: 979 loss: 2.07810785e-06
Iter: 980 loss: 2.07775656e-06
Iter: 981 loss: 2.07989274e-06
Iter: 982 loss: 2.0776647e-06
Iter: 983 loss: 2.07741368e-06
Iter: 984 loss: 2.08055144e-06
Iter: 985 loss: 2.07741664e-06
Iter: 986 loss: 2.07734092e-06
Iter: 987 loss: 2.07730773e-06
Iter: 988 loss: 2.07723338e-06
Iter: 989 loss: 2.07701055e-06
Iter: 990 loss: 2.07865969e-06
Iter: 991 loss: 2.07697508e-06
Iter: 992 loss: 2.07685412e-06
Iter: 993 loss: 2.07682615e-06
Iter: 994 loss: 2.07668199e-06
Iter: 995 loss: 2.07638891e-06
Iter: 996 loss: 2.07917515e-06
Iter: 997 loss: 2.0763423e-06
Iter: 998 loss: 2.07604785e-06
Iter: 999 loss: 2.07701146e-06
Iter: 1000 loss: 2.07596804e-06
Iter: 1001 loss: 2.07560834e-06
Iter: 1002 loss: 2.07971334e-06
Iter: 1003 loss: 2.07560151e-06
Iter: 1004 loss: 2.07538415e-06
Iter: 1005 loss: 2.0756313e-06
Iter: 1006 loss: 2.07524522e-06
Iter: 1007 loss: 2.07504172e-06
Iter: 1008 loss: 2.0752118e-06
Iter: 1009 loss: 2.07495077e-06
Iter: 1010 loss: 2.07478979e-06
Iter: 1011 loss: 2.07651601e-06
Iter: 1012 loss: 2.07479525e-06
Iter: 1013 loss: 2.07462108e-06
Iter: 1014 loss: 2.07474477e-06
Iter: 1015 loss: 2.07452695e-06
Iter: 1016 loss: 2.07435551e-06
Iter: 1017 loss: 2.07403582e-06
Iter: 1018 loss: 2.08126175e-06
Iter: 1019 loss: 2.07403946e-06
Iter: 1020 loss: 2.07399034e-06
Iter: 1021 loss: 2.07386029e-06
Iter: 1022 loss: 2.07373159e-06
Iter: 1023 loss: 2.07347625e-06
Iter: 1024 loss: 2.07928906e-06
Iter: 1025 loss: 2.07347694e-06
Iter: 1026 loss: 2.07323751e-06
Iter: 1027 loss: 2.07419862e-06
Iter: 1028 loss: 2.07317385e-06
Iter: 1029 loss: 2.07295352e-06
Iter: 1030 loss: 2.07576386e-06
Iter: 1031 loss: 2.07293579e-06
Iter: 1032 loss: 2.07289531e-06
Iter: 1033 loss: 2.07274e-06
Iter: 1034 loss: 2.07445282e-06
Iter: 1035 loss: 2.07270114e-06
Iter: 1036 loss: 2.07246285e-06
Iter: 1037 loss: 2.07256835e-06
Iter: 1038 loss: 2.07228254e-06
Iter: 1039 loss: 2.0722191e-06
Iter: 1040 loss: 2.07212952e-06
Iter: 1041 loss: 2.07198718e-06
Iter: 1042 loss: 2.07177322e-06
Iter: 1043 loss: 2.07175458e-06
Iter: 1044 loss: 2.07153039e-06
Iter: 1045 loss: 2.07182256e-06
Iter: 1046 loss: 2.0713851e-06
Iter: 1047 loss: 2.07119024e-06
Iter: 1048 loss: 2.07117546e-06
Iter: 1049 loss: 2.07096764e-06
Iter: 1050 loss: 2.07124026e-06
Iter: 1051 loss: 2.07087783e-06
Iter: 1052 loss: 2.07073958e-06
Iter: 1053 loss: 2.07065159e-06
Iter: 1054 loss: 2.07056701e-06
Iter: 1055 loss: 2.07033963e-06
Iter: 1056 loss: 2.07369794e-06
Iter: 1057 loss: 2.07033531e-06
Iter: 1058 loss: 2.07022731e-06
Iter: 1059 loss: 2.06994537e-06
Iter: 1060 loss: 2.07481162e-06
Iter: 1061 loss: 2.06995128e-06
Iter: 1062 loss: 2.06985305e-06
Iter: 1063 loss: 2.06982031e-06
Iter: 1064 loss: 2.06967979e-06
Iter: 1065 loss: 2.06943787e-06
Iter: 1066 loss: 2.0752741e-06
Iter: 1067 loss: 2.06944196e-06
Iter: 1068 loss: 2.06922596e-06
Iter: 1069 loss: 2.06927098e-06
Iter: 1070 loss: 2.06905838e-06
Iter: 1071 loss: 2.06879531e-06
Iter: 1072 loss: 2.06980303e-06
Iter: 1073 loss: 2.06871573e-06
Iter: 1074 loss: 2.06852837e-06
Iter: 1075 loss: 2.06850973e-06
Iter: 1076 loss: 2.06835398e-06
Iter: 1077 loss: 2.06822642e-06
Iter: 1078 loss: 2.06819072e-06
Iter: 1079 loss: 2.06792265e-06
Iter: 1080 loss: 2.06822074e-06
Iter: 1081 loss: 2.06778168e-06
Iter: 1082 loss: 2.06760387e-06
Iter: 1083 loss: 2.06760751e-06
Iter: 1084 loss: 2.06748541e-06
Iter: 1085 loss: 2.06736877e-06
Iter: 1086 loss: 2.06728396e-06
Iter: 1087 loss: 2.0671223e-06
Iter: 1088 loss: 2.06780896e-06
Iter: 1089 loss: 2.06711456e-06
Iter: 1090 loss: 2.06685309e-06
Iter: 1091 loss: 2.06714753e-06
Iter: 1092 loss: 2.06667983e-06
Iter: 1093 loss: 2.06650861e-06
Iter: 1094 loss: 2.06652294e-06
Iter: 1095 loss: 2.06634195e-06
Iter: 1096 loss: 2.06623781e-06
Iter: 1097 loss: 2.06619143e-06
Iter: 1098 loss: 2.06610366e-06
Iter: 1099 loss: 2.06581649e-06
Iter: 1100 loss: 2.06754021e-06
Iter: 1101 loss: 2.06574282e-06
Iter: 1102 loss: 2.06544632e-06
Iter: 1103 loss: 2.06715322e-06
Iter: 1104 loss: 2.06541085e-06
Iter: 1105 loss: 2.06517689e-06
Iter: 1106 loss: 2.06722461e-06
Iter: 1107 loss: 2.06516415e-06
Iter: 1108 loss: 2.06495133e-06
Iter: 1109 loss: 2.06555933e-06
Iter: 1110 loss: 2.06486948e-06
Iter: 1111 loss: 2.06469463e-06
Iter: 1112 loss: 2.06457503e-06
Iter: 1113 loss: 2.06454615e-06
Iter: 1114 loss: 2.06429058e-06
Iter: 1115 loss: 2.06616414e-06
Iter: 1116 loss: 2.06428308e-06
Iter: 1117 loss: 2.0640914e-06
Iter: 1118 loss: 2.06531104e-06
Iter: 1119 loss: 2.06407117e-06
Iter: 1120 loss: 2.06396908e-06
Iter: 1121 loss: 2.06396771e-06
Iter: 1122 loss: 2.06385221e-06
Iter: 1123 loss: 2.06370555e-06
Iter: 1124 loss: 2.06542404e-06
Iter: 1125 loss: 2.06370169e-06
Iter: 1126 loss: 2.06356026e-06
Iter: 1127 loss: 2.06320874e-06
Iter: 1128 loss: 2.06650702e-06
Iter: 1129 loss: 2.06314644e-06
Iter: 1130 loss: 2.06298409e-06
Iter: 1131 loss: 2.06292862e-06
Iter: 1132 loss: 2.06275263e-06
Iter: 1133 loss: 2.06274399e-06
Iter: 1134 loss: 2.06258778e-06
Iter: 1135 loss: 2.06235609e-06
Iter: 1136 loss: 2.06203822e-06
Iter: 1137 loss: 2.06199661e-06
Iter: 1138 loss: 2.06180175e-06
Iter: 1139 loss: 2.06178493e-06
Iter: 1140 loss: 2.06161621e-06
Iter: 1141 loss: 2.06265804e-06
Iter: 1142 loss: 2.06157938e-06
Iter: 1143 loss: 2.06143341e-06
Iter: 1144 loss: 2.06123286e-06
Iter: 1145 loss: 2.06121831e-06
Iter: 1146 loss: 2.060925e-06
Iter: 1147 loss: 2.06237451e-06
Iter: 1148 loss: 2.06087611e-06
Iter: 1149 loss: 2.06068626e-06
Iter: 1150 loss: 2.06262189e-06
Iter: 1151 loss: 2.06064396e-06
Iter: 1152 loss: 2.06047594e-06
Iter: 1153 loss: 2.06042705e-06
Iter: 1154 loss: 2.06031746e-06
Iter: 1155 loss: 2.06015716e-06
Iter: 1156 loss: 2.06015602e-06
Iter: 1157 loss: 2.06001732e-06
Iter: 1158 loss: 2.05985452e-06
Iter: 1159 loss: 2.05984179e-06
Iter: 1160 loss: 2.05967945e-06
Iter: 1161 loss: 2.05985543e-06
Iter: 1162 loss: 2.05958577e-06
Iter: 1163 loss: 2.05942297e-06
Iter: 1164 loss: 2.05942388e-06
Iter: 1165 loss: 2.05935248e-06
Iter: 1166 loss: 2.05910101e-06
Iter: 1167 loss: 2.06025629e-06
Iter: 1168 loss: 2.05903416e-06
Iter: 1169 loss: 2.05872971e-06
Iter: 1170 loss: 2.0601567e-06
Iter: 1171 loss: 2.05867195e-06
Iter: 1172 loss: 2.05844731e-06
Iter: 1173 loss: 2.06134473e-06
Iter: 1174 loss: 2.05843071e-06
Iter: 1175 loss: 2.05827223e-06
Iter: 1176 loss: 2.05828974e-06
Iter: 1177 loss: 2.05813012e-06
Iter: 1178 loss: 2.05797e-06
Iter: 1179 loss: 2.05828769e-06
Iter: 1180 loss: 2.05789206e-06
Iter: 1181 loss: 2.05771721e-06
Iter: 1182 loss: 2.05947845e-06
Iter: 1183 loss: 2.05770721e-06
Iter: 1184 loss: 2.05755396e-06
Iter: 1185 loss: 2.0575626e-06
Iter: 1186 loss: 2.05742526e-06
Iter: 1187 loss: 2.05724632e-06
Iter: 1188 loss: 2.058277e-06
Iter: 1189 loss: 2.05721949e-06
Iter: 1190 loss: 2.0569978e-06
Iter: 1191 loss: 2.05715469e-06
Iter: 1192 loss: 2.0568516e-06
Iter: 1193 loss: 2.05664946e-06
Iter: 1194 loss: 2.05641754e-06
Iter: 1195 loss: 2.05638707e-06
Iter: 1196 loss: 2.05630977e-06
Iter: 1197 loss: 2.05622473e-06
Iter: 1198 loss: 2.05611536e-06
Iter: 1199 loss: 2.05589413e-06
Iter: 1200 loss: 2.06067125e-06
Iter: 1201 loss: 2.05590095e-06
Iter: 1202 loss: 2.05574e-06
Iter: 1203 loss: 2.05618198e-06
Iter: 1204 loss: 2.05569495e-06
Iter: 1205 loss: 2.05551123e-06
Iter: 1206 loss: 2.05677202e-06
Iter: 1207 loss: 2.05549122e-06
Iter: 1208 loss: 2.05530932e-06
Iter: 1209 loss: 2.0552543e-06
Iter: 1210 loss: 2.05514971e-06
Iter: 1211 loss: 2.0548905e-06
Iter: 1212 loss: 2.05533433e-06
Iter: 1213 loss: 2.05477045e-06
Iter: 1214 loss: 2.05459469e-06
Iter: 1215 loss: 2.05458309e-06
Iter: 1216 loss: 2.05448555e-06
Iter: 1217 loss: 2.05469405e-06
Iter: 1218 loss: 2.05440642e-06
Iter: 1219 loss: 2.05426022e-06
Iter: 1220 loss: 2.05456126e-06
Iter: 1221 loss: 2.05420133e-06
Iter: 1222 loss: 2.05407059e-06
Iter: 1223 loss: 2.05561855e-06
Iter: 1224 loss: 2.05406081e-06
Iter: 1225 loss: 2.05399e-06
Iter: 1226 loss: 2.05375795e-06
Iter: 1227 loss: 2.05547076e-06
Iter: 1228 loss: 2.05371407e-06
Iter: 1229 loss: 2.05345e-06
Iter: 1230 loss: 2.0564853e-06
Iter: 1231 loss: 2.05343849e-06
Iter: 1232 loss: 2.05318111e-06
Iter: 1233 loss: 2.05371384e-06
Iter: 1234 loss: 2.05306924e-06
Iter: 1235 loss: 2.0529169e-06
Iter: 1236 loss: 2.0526e-06
Iter: 1237 loss: 2.05261e-06
Iter: 1238 loss: 2.05242054e-06
Iter: 1239 loss: 2.05240235e-06
Iter: 1240 loss: 2.05224069e-06
Iter: 1241 loss: 2.05250581e-06
Iter: 1242 loss: 2.05212723e-06
Iter: 1243 loss: 2.05195624e-06
Iter: 1244 loss: 2.05180413e-06
Iter: 1245 loss: 2.05174865e-06
Iter: 1246 loss: 2.05151764e-06
Iter: 1247 loss: 2.05354081e-06
Iter: 1248 loss: 2.05148922e-06
Iter: 1249 loss: 2.0512598e-06
Iter: 1250 loss: 2.05277e-06
Iter: 1251 loss: 2.05123843e-06
Iter: 1252 loss: 2.05108518e-06
Iter: 1253 loss: 2.05098331e-06
Iter: 1254 loss: 2.0509533e-06
Iter: 1255 loss: 2.05071046e-06
Iter: 1256 loss: 2.05071728e-06
Iter: 1257 loss: 2.05056767e-06
Iter: 1258 loss: 2.05040988e-06
Iter: 1259 loss: 2.05037145e-06
Iter: 1260 loss: 2.05022388e-06
Iter: 1261 loss: 2.05033621e-06
Iter: 1262 loss: 2.0501293e-06
Iter: 1263 loss: 2.04995285e-06
Iter: 1264 loss: 2.04994353e-06
Iter: 1265 loss: 2.04982052e-06
Iter: 1266 loss: 2.04954563e-06
Iter: 1267 loss: 2.05258721e-06
Iter: 1268 loss: 2.04952084e-06
Iter: 1269 loss: 2.04934918e-06
Iter: 1270 loss: 2.0519924e-06
Iter: 1271 loss: 2.0493394e-06
Iter: 1272 loss: 2.04914977e-06
Iter: 1273 loss: 2.04967637e-06
Iter: 1274 loss: 2.04909543e-06
Iter: 1275 loss: 2.04890375e-06
Iter: 1276 loss: 2.04884873e-06
Iter: 1277 loss: 2.04874141e-06
Iter: 1278 loss: 2.04852586e-06
Iter: 1279 loss: 2.04896332e-06
Iter: 1280 loss: 2.0484531e-06
Iter: 1281 loss: 2.0482687e-06
Iter: 1282 loss: 2.04825164e-06
Iter: 1283 loss: 2.04812432e-06
Iter: 1284 loss: 2.04793423e-06
Iter: 1285 loss: 2.0479074e-06
Iter: 1286 loss: 2.04770481e-06
Iter: 1287 loss: 2.0499524e-06
Iter: 1288 loss: 2.04772891e-06
Iter: 1289 loss: 2.04752268e-06
Iter: 1290 loss: 2.04767366e-06
Iter: 1291 loss: 2.04739422e-06
Iter: 1292 loss: 2.04715798e-06
Iter: 1293 loss: 2.04697199e-06
Iter: 1294 loss: 2.04692196e-06
Iter: 1295 loss: 2.04685171e-06
Iter: 1296 loss: 2.04681214e-06
Iter: 1297 loss: 2.04667094e-06
Iter: 1298 loss: 2.04641128e-06
Iter: 1299 loss: 2.04642629e-06
Iter: 1300 loss: 2.04617368e-06
Iter: 1301 loss: 2.04647813e-06
Iter: 1302 loss: 2.04605749e-06
Iter: 1303 loss: 2.04591379e-06
Iter: 1304 loss: 2.04588423e-06
Iter: 1305 loss: 2.04576509e-06
Iter: 1306 loss: 2.04556363e-06
Iter: 1307 loss: 2.05055176e-06
Iter: 1308 loss: 2.04555818e-06
Iter: 1309 loss: 2.04525986e-06
Iter: 1310 loss: 2.04551316e-06
Iter: 1311 loss: 2.04509774e-06
Iter: 1312 loss: 2.04487787e-06
Iter: 1313 loss: 2.04794287e-06
Iter: 1314 loss: 2.04487651e-06
Iter: 1315 loss: 2.04465596e-06
Iter: 1316 loss: 2.04483058e-06
Iter: 1317 loss: 2.04450271e-06
Iter: 1318 loss: 2.04421895e-06
Iter: 1319 loss: 2.04445723e-06
Iter: 1320 loss: 2.04403477e-06
Iter: 1321 loss: 2.04379012e-06
Iter: 1322 loss: 2.04377784e-06
Iter: 1323 loss: 2.04360117e-06
Iter: 1324 loss: 2.04334083e-06
Iter: 1325 loss: 2.04333401e-06
Iter: 1326 loss: 2.04306912e-06
Iter: 1327 loss: 2.04350181e-06
Iter: 1328 loss: 2.04293019e-06
Iter: 1329 loss: 2.04281264e-06
Iter: 1330 loss: 2.04275261e-06
Iter: 1331 loss: 2.04264438e-06
Iter: 1332 loss: 2.04231901e-06
Iter: 1333 loss: 2.04345861e-06
Iter: 1334 loss: 2.04216667e-06
Iter: 1335 loss: 2.04191838e-06
Iter: 1336 loss: 2.04192975e-06
Iter: 1337 loss: 2.04165781e-06
Iter: 1338 loss: 2.04195271e-06
Iter: 1339 loss: 2.04153275e-06
Iter: 1340 loss: 2.04125126e-06
Iter: 1341 loss: 2.04104344e-06
Iter: 1342 loss: 2.0409941e-06
Iter: 1343 loss: 2.04066259e-06
Iter: 1344 loss: 2.04256276e-06
Iter: 1345 loss: 2.04062712e-06
Iter: 1346 loss: 2.04050116e-06
Iter: 1347 loss: 2.04050139e-06
Iter: 1348 loss: 2.04037e-06
Iter: 1349 loss: 2.04007574e-06
Iter: 1350 loss: 2.04472644e-06
Iter: 1351 loss: 2.04007029e-06
Iter: 1352 loss: 2.03984223e-06
Iter: 1353 loss: 2.03983222e-06
Iter: 1354 loss: 2.03963873e-06
Iter: 1355 loss: 2.03965556e-06
Iter: 1356 loss: 2.03947661e-06
Iter: 1357 loss: 2.03923651e-06
Iter: 1358 loss: 2.0389848e-06
Iter: 1359 loss: 2.03894115e-06
Iter: 1360 loss: 2.03864829e-06
Iter: 1361 loss: 2.04283242e-06
Iter: 1362 loss: 2.03864693e-06
Iter: 1363 loss: 2.03838931e-06
Iter: 1364 loss: 2.03965374e-06
Iter: 1365 loss: 2.0383568e-06
Iter: 1366 loss: 2.03823629e-06
Iter: 1367 loss: 2.03801096e-06
Iter: 1368 loss: 2.04207618e-06
Iter: 1369 loss: 2.03801528e-06
Iter: 1370 loss: 2.03779064e-06
Iter: 1371 loss: 2.03778472e-06
Iter: 1372 loss: 2.03762784e-06
Iter: 1373 loss: 2.03726358e-06
Iter: 1374 loss: 2.04334651e-06
Iter: 1375 loss: 2.03728791e-06
Iter: 1376 loss: 2.03684567e-06
Iter: 1377 loss: 2.03757554e-06
Iter: 1378 loss: 2.03665309e-06
Iter: 1379 loss: 2.0362952e-06
Iter: 1380 loss: 2.04016806e-06
Iter: 1381 loss: 2.03628679e-06
Iter: 1382 loss: 2.03601849e-06
Iter: 1383 loss: 2.0389416e-06
Iter: 1384 loss: 2.03599825e-06
Iter: 1385 loss: 2.03582704e-06
Iter: 1386 loss: 2.03569107e-06
Iter: 1387 loss: 2.03565946e-06
Iter: 1388 loss: 2.03544982e-06
Iter: 1389 loss: 2.03543595e-06
Iter: 1390 loss: 2.03529771e-06
Iter: 1391 loss: 2.03515128e-06
Iter: 1392 loss: 2.03512673e-06
Iter: 1393 loss: 2.03486e-06
Iter: 1394 loss: 2.03441368e-06
Iter: 1395 loss: 2.04548678e-06
Iter: 1396 loss: 2.03440459e-06
Iter: 1397 loss: 2.03429704e-06
Iter: 1398 loss: 2.03412742e-06
Iter: 1399 loss: 2.0338498e-06
Iter: 1400 loss: 2.03377022e-06
Iter: 1401 loss: 2.03363857e-06
Iter: 1402 loss: 2.03340778e-06
Iter: 1403 loss: 2.03366244e-06
Iter: 1404 loss: 2.03332115e-06
Iter: 1405 loss: 2.03301124e-06
Iter: 1406 loss: 2.03548484e-06
Iter: 1407 loss: 2.03298896e-06
Iter: 1408 loss: 2.03289392e-06
Iter: 1409 loss: 2.0326313e-06
Iter: 1410 loss: 2.03778973e-06
Iter: 1411 loss: 2.03263266e-06
Iter: 1412 loss: 2.03231798e-06
Iter: 1413 loss: 2.03269246e-06
Iter: 1414 loss: 2.03212448e-06
Iter: 1415 loss: 2.03180321e-06
Iter: 1416 loss: 2.03599552e-06
Iter: 1417 loss: 2.03183254e-06
Iter: 1418 loss: 2.03149875e-06
Iter: 1419 loss: 2.03244599e-06
Iter: 1420 loss: 2.03141713e-06
Iter: 1421 loss: 2.03119157e-06
Iter: 1422 loss: 2.03111131e-06
Iter: 1423 loss: 2.03100763e-06
Iter: 1424 loss: 2.03088666e-06
Iter: 1425 loss: 2.03084892e-06
Iter: 1426 loss: 2.03072477e-06
Iter: 1427 loss: 2.03049331e-06
Iter: 1428 loss: 2.03455875e-06
Iter: 1429 loss: 2.03048603e-06
Iter: 1430 loss: 2.03018453e-06
Iter: 1431 loss: 2.03026093e-06
Iter: 1432 loss: 2.03001082e-06
Iter: 1433 loss: 2.02971705e-06
Iter: 1434 loss: 2.03267905e-06
Iter: 1435 loss: 2.02972433e-06
Iter: 1436 loss: 2.02947308e-06
Iter: 1437 loss: 2.02990964e-06
Iter: 1438 loss: 2.02937485e-06
Iter: 1439 loss: 2.02914293e-06
Iter: 1440 loss: 2.02875322e-06
Iter: 1441 loss: 2.03768968e-06
Iter: 1442 loss: 2.0287664e-06
Iter: 1443 loss: 2.02842148e-06
Iter: 1444 loss: 2.03006061e-06
Iter: 1445 loss: 2.02838419e-06
Iter: 1446 loss: 2.02821639e-06
Iter: 1447 loss: 2.02821e-06
Iter: 1448 loss: 2.02805313e-06
Iter: 1449 loss: 2.02781439e-06
Iter: 1450 loss: 2.02781121e-06
Iter: 1451 loss: 2.02756928e-06
Iter: 1452 loss: 2.02799288e-06
Iter: 1453 loss: 2.02739943e-06
Iter: 1454 loss: 2.02722777e-06
Iter: 1455 loss: 2.02721867e-06
Iter: 1456 loss: 2.02708134e-06
Iter: 1457 loss: 2.02694423e-06
Iter: 1458 loss: 2.02695128e-06
Iter: 1459 loss: 2.02674028e-06
Iter: 1460 loss: 2.0294633e-06
Iter: 1461 loss: 2.02673709e-06
Iter: 1462 loss: 2.02660749e-06
Iter: 1463 loss: 2.02627689e-06
Iter: 1464 loss: 2.02941328e-06
Iter: 1465 loss: 2.02623823e-06
Iter: 1466 loss: 2.02589672e-06
Iter: 1467 loss: 2.0276816e-06
Iter: 1468 loss: 2.02586943e-06
Iter: 1469 loss: 2.02559636e-06
Iter: 1470 loss: 2.02693059e-06
Iter: 1471 loss: 2.02548244e-06
Iter: 1472 loss: 2.02529282e-06
Iter: 1473 loss: 2.02531578e-06
Iter: 1474 loss: 2.02518299e-06
Iter: 1475 loss: 2.02496312e-06
Iter: 1476 loss: 2.02838532e-06
Iter: 1477 loss: 2.02498541e-06
Iter: 1478 loss: 2.02472575e-06
Iter: 1479 loss: 2.02537285e-06
Iter: 1480 loss: 2.02462616e-06
Iter: 1481 loss: 2.02436127e-06
Iter: 1482 loss: 2.02753745e-06
Iter: 1483 loss: 2.02434421e-06
Iter: 1484 loss: 2.02423507e-06
Iter: 1485 loss: 2.02386082e-06
Iter: 1486 loss: 2.025806e-06
Iter: 1487 loss: 2.02377237e-06
Iter: 1488 loss: 2.02362753e-06
Iter: 1489 loss: 2.02356705e-06
Iter: 1490 loss: 2.02331807e-06
Iter: 1491 loss: 2.02352771e-06
Iter: 1492 loss: 2.02319984e-06
Iter: 1493 loss: 2.02303568e-06
Iter: 1494 loss: 2.02502451e-06
Iter: 1495 loss: 2.02301521e-06
Iter: 1496 loss: 2.02285401e-06
Iter: 1497 loss: 2.02257957e-06
Iter: 1498 loss: 2.0285961e-06
Iter: 1499 loss: 2.0225898e-06
Iter: 1500 loss: 2.02234e-06
Iter: 1501 loss: 2.02250976e-06
Iter: 1502 loss: 2.02215597e-06
Iter: 1503 loss: 2.02175124e-06
Iter: 1504 loss: 2.02358524e-06
Iter: 1505 loss: 2.02168394e-06
Iter: 1506 loss: 2.02137289e-06
Iter: 1507 loss: 2.02139222e-06
Iter: 1508 loss: 2.02117144e-06
Iter: 1509 loss: 2.02105275e-06
Iter: 1510 loss: 2.02099636e-06
Iter: 1511 loss: 2.02073738e-06
Iter: 1512 loss: 2.02059277e-06
Iter: 1513 loss: 2.02049523e-06
Iter: 1514 loss: 2.0203081e-06
Iter: 1515 loss: 2.02026877e-06
Iter: 1516 loss: 2.02013348e-06
Iter: 1517 loss: 2.01986722e-06
Iter: 1518 loss: 2.02438559e-06
Iter: 1519 loss: 2.01984403e-06
Iter: 1520 loss: 2.01954072e-06
Iter: 1521 loss: 2.02006e-06
Iter: 1522 loss: 2.01936473e-06
Iter: 1523 loss: 2.01902867e-06
Iter: 1524 loss: 2.02453793e-06
Iter: 1525 loss: 2.01902594e-06
Iter: 1526 loss: 2.01884541e-06
Iter: 1527 loss: 2.01883586e-06
Iter: 1528 loss: 2.01867215e-06
Iter: 1529 loss: 2.01834155e-06
Iter: 1530 loss: 2.0201287e-06
Iter: 1531 loss: 2.01830971e-06
Iter: 1532 loss: 2.01814419e-06
Iter: 1533 loss: 2.0177672e-06
Iter: 1534 loss: 2.02273645e-06
Iter: 1535 loss: 2.017724e-06
Iter: 1536 loss: 2.01738271e-06
Iter: 1537 loss: 2.02041474e-06
Iter: 1538 loss: 2.01732496e-06
Iter: 1539 loss: 2.01699959e-06
Iter: 1540 loss: 2.02018964e-06
Iter: 1541 loss: 2.01699822e-06
Iter: 1542 loss: 2.01677199e-06
Iter: 1543 loss: 2.01747594e-06
Iter: 1544 loss: 2.01668195e-06
Iter: 1545 loss: 2.01649891e-06
Iter: 1546 loss: 2.01619105e-06
Iter: 1547 loss: 2.01618468e-06
Iter: 1548 loss: 2.01597959e-06
Iter: 1549 loss: 2.01590638e-06
Iter: 1550 loss: 2.01573494e-06
Iter: 1551 loss: 2.01563876e-06
Iter: 1552 loss: 2.01553803e-06
Iter: 1553 loss: 2.01532862e-06
Iter: 1554 loss: 2.01502462e-06
Iter: 1555 loss: 2.01501916e-06
Iter: 1556 loss: 2.0149273e-06
Iter: 1557 loss: 2.0147877e-06
Iter: 1558 loss: 2.01465036e-06
Iter: 1559 loss: 2.01431567e-06
Iter: 1560 loss: 2.02056208e-06
Iter: 1561 loss: 2.01433568e-06
Iter: 1562 loss: 2.01409671e-06
Iter: 1563 loss: 2.01405874e-06
Iter: 1564 loss: 2.0139023e-06
Iter: 1565 loss: 2.01349849e-06
Iter: 1566 loss: 2.01743569e-06
Iter: 1567 loss: 2.01344346e-06
Iter: 1568 loss: 2.01297439e-06
Iter: 1569 loss: 2.01579746e-06
Iter: 1570 loss: 2.01292232e-06
Iter: 1571 loss: 2.01267721e-06
Iter: 1572 loss: 2.01267858e-06
Iter: 1573 loss: 2.01244211e-06
Iter: 1574 loss: 2.01257262e-06
Iter: 1575 loss: 2.01231569e-06
Iter: 1576 loss: 2.01207058e-06
Iter: 1577 loss: 2.01170883e-06
Iter: 1578 loss: 2.01170087e-06
Iter: 1579 loss: 2.01129478e-06
Iter: 1580 loss: 2.01129797e-06
Iter: 1581 loss: 2.01103694e-06
Iter: 1582 loss: 2.01091689e-06
Iter: 1583 loss: 2.01080138e-06
Iter: 1584 loss: 2.01045759e-06
Iter: 1585 loss: 2.01045168e-06
Iter: 1586 loss: 2.01015428e-06
Iter: 1587 loss: 2.00990507e-06
Iter: 1588 loss: 2.00991531e-06
Iter: 1589 loss: 2.00966019e-06
Iter: 1590 loss: 2.0095838e-06
Iter: 1591 loss: 2.00942918e-06
Iter: 1592 loss: 2.00916975e-06
Iter: 1593 loss: 2.01098374e-06
Iter: 1594 loss: 2.00915156e-06
Iter: 1595 loss: 2.00888553e-06
Iter: 1596 loss: 2.00862905e-06
Iter: 1597 loss: 2.00856834e-06
Iter: 1598 loss: 2.00823911e-06
Iter: 1599 loss: 2.00820523e-06
Iter: 1600 loss: 2.00798081e-06
Iter: 1601 loss: 2.00757609e-06
Iter: 1602 loss: 2.01077683e-06
Iter: 1603 loss: 2.00755744e-06
Iter: 1604 loss: 2.00735349e-06
Iter: 1605 loss: 2.00735349e-06
Iter: 1606 loss: 2.0072091e-06
Iter: 1607 loss: 2.00707291e-06
Iter: 1608 loss: 2.00702925e-06
Iter: 1609 loss: 2.00675777e-06
Iter: 1610 loss: 2.0067e-06
Iter: 1611 loss: 2.00656359e-06
Iter: 1612 loss: 2.00617046e-06
Iter: 1613 loss: 2.01060857e-06
Iter: 1614 loss: 2.00617183e-06
Iter: 1615 loss: 2.00594604e-06
Iter: 1616 loss: 2.00560157e-06
Iter: 1617 loss: 2.00559384e-06
Iter: 1618 loss: 2.00521981e-06
Iter: 1619 loss: 2.0078337e-06
Iter: 1620 loss: 2.00519685e-06
Iter: 1621 loss: 2.00481441e-06
Iter: 1622 loss: 2.00654358e-06
Iter: 1623 loss: 2.00476097e-06
Iter: 1624 loss: 2.00454178e-06
Iter: 1625 loss: 2.00436352e-06
Iter: 1626 loss: 2.00429486e-06
Iter: 1627 loss: 2.00419754e-06
Iter: 1628 loss: 2.00415684e-06
Iter: 1629 loss: 2.00400814e-06
Iter: 1630 loss: 2.00360455e-06
Iter: 1631 loss: 2.00605609e-06
Iter: 1632 loss: 2.00351633e-06
Iter: 1633 loss: 2.00312729e-06
Iter: 1634 loss: 2.00451791e-06
Iter: 1635 loss: 2.00307181e-06
Iter: 1636 loss: 2.0027403e-06
Iter: 1637 loss: 2.00590739e-06
Iter: 1638 loss: 2.00270324e-06
Iter: 1639 loss: 2.00248815e-06
Iter: 1640 loss: 2.00379327e-06
Iter: 1641 loss: 2.00238037e-06
Iter: 1642 loss: 2.00217937e-06
Iter: 1643 loss: 2.00198747e-06
Iter: 1644 loss: 2.00192153e-06
Iter: 1645 loss: 2.00171053e-06
Iter: 1646 loss: 2.00366367e-06
Iter: 1647 loss: 2.00171417e-06
Iter: 1648 loss: 2.00148042e-06
Iter: 1649 loss: 2.0017178e-06
Iter: 1650 loss: 2.00132035e-06
Iter: 1651 loss: 2.00113823e-06
Iter: 1652 loss: 2.00094291e-06
Iter: 1653 loss: 2.00087902e-06
Iter: 1654 loss: 2.00062436e-06
Iter: 1655 loss: 2.00062573e-06
Iter: 1656 loss: 2.00041541e-06
Iter: 1657 loss: 2.00008162e-06
Iter: 1658 loss: 2.00005934e-06
Iter: 1659 loss: 1.99978035e-06
Iter: 1660 loss: 1.99956708e-06
Iter: 1661 loss: 1.99947704e-06
Iter: 1662 loss: 1.9990166e-06
Iter: 1663 loss: 2.00171189e-06
Iter: 1664 loss: 1.99894066e-06
Iter: 1665 loss: 1.99877195e-06
Iter: 1666 loss: 1.99876445e-06
Iter: 1667 loss: 1.998475e-06
Iter: 1668 loss: 1.99832039e-06
Iter: 1669 loss: 1.99819897e-06
Iter: 1670 loss: 1.99787928e-06
Iter: 1671 loss: 1.99774377e-06
Iter: 1672 loss: 1.9975696e-06
Iter: 1673 loss: 1.99766691e-06
Iter: 1674 loss: 1.9974043e-06
Iter: 1675 loss: 1.99726878e-06
Iter: 1676 loss: 1.99689475e-06
Iter: 1677 loss: 1.99784563e-06
Iter: 1678 loss: 1.99668148e-06
Iter: 1679 loss: 1.99648866e-06
Iter: 1680 loss: 1.99641249e-06
Iter: 1681 loss: 1.99612805e-06
Iter: 1682 loss: 1.99628084e-06
Iter: 1683 loss: 1.99595638e-06
Iter: 1684 loss: 1.99569968e-06
Iter: 1685 loss: 1.99593501e-06
Iter: 1686 loss: 1.9955055e-06
Iter: 1687 loss: 1.99520355e-06
Iter: 1688 loss: 1.9963486e-06
Iter: 1689 loss: 1.99512533e-06
Iter: 1690 loss: 1.99486021e-06
Iter: 1691 loss: 1.99486567e-06
Iter: 1692 loss: 1.99474562e-06
Iter: 1693 loss: 1.99441502e-06
Iter: 1694 loss: 1.998058e-06
Iter: 1695 loss: 1.99432247e-06
Iter: 1696 loss: 1.99399119e-06
Iter: 1697 loss: 1.99424e-06
Iter: 1698 loss: 1.99376018e-06
Iter: 1699 loss: 1.99359192e-06
Iter: 1700 loss: 1.99358055e-06
Iter: 1701 loss: 1.99328269e-06
Iter: 1702 loss: 1.99304509e-06
Iter: 1703 loss: 1.99297756e-06
Iter: 1704 loss: 1.99283477e-06
Iter: 1705 loss: 1.99452506e-06
Iter: 1706 loss: 1.99281976e-06
Iter: 1707 loss: 1.99259125e-06
Iter: 1708 loss: 1.99247097e-06
Iter: 1709 loss: 1.99238502e-06
Iter: 1710 loss: 1.99217379e-06
Iter: 1711 loss: 1.99252736e-06
Iter: 1712 loss: 1.99205692e-06
Iter: 1713 loss: 1.99190026e-06
Iter: 1714 loss: 1.99189049e-06
Iter: 1715 loss: 1.9918084e-06
Iter: 1716 loss: 1.99155056e-06
Iter: 1717 loss: 1.99396436e-06
Iter: 1718 loss: 1.99152964e-06
Iter: 1719 loss: 1.99117585e-06
Iter: 1720 loss: 1.99247688e-06
Iter: 1721 loss: 1.99108626e-06
Iter: 1722 loss: 1.9907925e-06
Iter: 1723 loss: 1.99318129e-06
Iter: 1724 loss: 1.99078522e-06
Iter: 1725 loss: 1.99046576e-06
Iter: 1726 loss: 1.99046394e-06
Iter: 1727 loss: 1.99020724e-06
Iter: 1728 loss: 1.98997e-06
Iter: 1729 loss: 1.9899162e-06
Iter: 1730 loss: 1.98978296e-06
Iter: 1731 loss: 1.98947509e-06
Iter: 1732 loss: 1.99080068e-06
Iter: 1733 loss: 1.98942257e-06
Iter: 1734 loss: 1.98919361e-06
Iter: 1735 loss: 1.98917678e-06
Iter: 1736 loss: 1.98908447e-06
Iter: 1737 loss: 1.98889029e-06
Iter: 1738 loss: 1.99184024e-06
Iter: 1739 loss: 1.98887119e-06
Iter: 1740 loss: 1.98859061e-06
Iter: 1741 loss: 1.99207693e-06
Iter: 1742 loss: 1.98859971e-06
Iter: 1743 loss: 1.98845123e-06
Iter: 1744 loss: 1.98810358e-06
Iter: 1745 loss: 1.99401347e-06
Iter: 1746 loss: 1.98810835e-06
Iter: 1747 loss: 1.98801081e-06
Iter: 1748 loss: 1.98796806e-06
Iter: 1749 loss: 1.98783528e-06
Iter: 1750 loss: 1.98769612e-06
Iter: 1751 loss: 1.9876461e-06
Iter: 1752 loss: 1.98742646e-06
Iter: 1753 loss: 1.98739417e-06
Iter: 1754 loss: 1.98721455e-06
Iter: 1755 loss: 1.98696307e-06
Iter: 1756 loss: 1.99032979e-06
Iter: 1757 loss: 1.98695943e-06
Iter: 1758 loss: 1.98676435e-06
Iter: 1759 loss: 1.98772909e-06
Iter: 1760 loss: 1.98670568e-06
Iter: 1761 loss: 1.98654288e-06
Iter: 1762 loss: 1.98620683e-06
Iter: 1763 loss: 1.99239821e-06
Iter: 1764 loss: 1.98617386e-06
Iter: 1765 loss: 1.98586713e-06
Iter: 1766 loss: 1.98782891e-06
Iter: 1767 loss: 1.98581733e-06
Iter: 1768 loss: 1.98587486e-06
Iter: 1769 loss: 1.98571388e-06
Iter: 1770 loss: 1.98565704e-06
Iter: 1771 loss: 1.98544808e-06
Iter: 1772 loss: 1.98786233e-06
Iter: 1773 loss: 1.98541829e-06
Iter: 1774 loss: 1.98527596e-06
Iter: 1775 loss: 1.98527e-06
Iter: 1776 loss: 1.98513635e-06
Iter: 1777 loss: 1.98483849e-06
Iter: 1778 loss: 1.98817065e-06
Iter: 1779 loss: 1.98478551e-06
Iter: 1780 loss: 1.98450562e-06
Iter: 1781 loss: 1.98506905e-06
Iter: 1782 loss: 1.98434236e-06
Iter: 1783 loss: 1.98422549e-06
Iter: 1784 loss: 1.98417183e-06
Iter: 1785 loss: 1.98407452e-06
Iter: 1786 loss: 1.9838094e-06
Iter: 1787 loss: 1.9861186e-06
Iter: 1788 loss: 1.98378575e-06
Iter: 1789 loss: 1.98355337e-06
Iter: 1790 loss: 1.98580528e-06
Iter: 1791 loss: 1.98355406e-06
Iter: 1792 loss: 1.9834165e-06
Iter: 1793 loss: 1.9835802e-06
Iter: 1794 loss: 1.98329576e-06
Iter: 1795 loss: 1.98311409e-06
Iter: 1796 loss: 1.983532e-06
Iter: 1797 loss: 1.9830868e-06
Iter: 1798 loss: 1.98287239e-06
Iter: 1799 loss: 1.98247199e-06
Iter: 1800 loss: 1.99112765e-06
Iter: 1801 loss: 1.98247494e-06
Iter: 1802 loss: 1.98227553e-06
Iter: 1803 loss: 1.9822628e-06
Iter: 1804 loss: 1.98210546e-06
Iter: 1805 loss: 1.9825352e-06
Iter: 1806 loss: 1.98202724e-06
Iter: 1807 loss: 1.9818649e-06
Iter: 1808 loss: 1.98155067e-06
Iter: 1809 loss: 1.98627981e-06
Iter: 1810 loss: 1.98156886e-06
Iter: 1811 loss: 1.98166981e-06
Iter: 1812 loss: 1.98141788e-06
Iter: 1813 loss: 1.98136013e-06
Iter: 1814 loss: 1.98112753e-06
Iter: 1815 loss: 1.98311182e-06
Iter: 1816 loss: 1.98108955e-06
Iter: 1817 loss: 1.98093903e-06
Iter: 1818 loss: 1.98093926e-06
Iter: 1819 loss: 1.98081284e-06
Iter: 1820 loss: 1.98063253e-06
Iter: 1821 loss: 1.98060479e-06
Iter: 1822 loss: 1.98043631e-06
Iter: 1823 loss: 1.98224279e-06
Iter: 1824 loss: 1.98043062e-06
Iter: 1825 loss: 1.98026942e-06
Iter: 1826 loss: 1.98025555e-06
Iter: 1827 loss: 1.98016824e-06
Iter: 1828 loss: 1.97997565e-06
Iter: 1829 loss: 1.98105045e-06
Iter: 1830 loss: 1.97995291e-06
Iter: 1831 loss: 1.97979443e-06
Iter: 1832 loss: 1.97958207e-06
Iter: 1833 loss: 1.97956547e-06
Iter: 1834 loss: 1.97923146e-06
Iter: 1835 loss: 1.98036855e-06
Iter: 1836 loss: 1.97917e-06
Iter: 1837 loss: 1.97907138e-06
Iter: 1838 loss: 1.97904774e-06
Iter: 1839 loss: 1.9789336e-06
Iter: 1840 loss: 1.97865165e-06
Iter: 1841 loss: 1.98149951e-06
Iter: 1842 loss: 1.97863346e-06
Iter: 1843 loss: 1.97832537e-06
Iter: 1844 loss: 1.97891222e-06
Iter: 1845 loss: 1.97822192e-06
Iter: 1846 loss: 1.9782135e-06
Iter: 1847 loss: 1.97811551e-06
Iter: 1848 loss: 1.97802296e-06
Iter: 1849 loss: 1.97779218e-06
Iter: 1850 loss: 1.98032967e-06
Iter: 1851 loss: 1.97777285e-06
Iter: 1852 loss: 1.97764984e-06
Iter: 1853 loss: 1.97761869e-06
Iter: 1854 loss: 1.9775016e-06
Iter: 1855 loss: 1.97726922e-06
Iter: 1856 loss: 1.97728514e-06
Iter: 1857 loss: 1.97712552e-06
Iter: 1858 loss: 1.97869645e-06
Iter: 1859 loss: 1.97713962e-06
Iter: 1860 loss: 1.97698319e-06
Iter: 1861 loss: 1.97701343e-06
Iter: 1862 loss: 1.9768604e-06
Iter: 1863 loss: 1.97671557e-06
Iter: 1864 loss: 1.97701502e-06
Iter: 1865 loss: 1.97663394e-06
Iter: 1866 loss: 1.97639406e-06
Iter: 1867 loss: 1.97639474e-06
Iter: 1868 loss: 1.97620056e-06
Iter: 1869 loss: 1.97603845e-06
Iter: 1870 loss: 1.97832492e-06
Iter: 1871 loss: 1.97605368e-06
Iter: 1872 loss: 1.97590407e-06
Iter: 1873 loss: 1.97633631e-06
Iter: 1874 loss: 1.97582222e-06
Iter: 1875 loss: 1.97574968e-06
Iter: 1876 loss: 1.97551208e-06
Iter: 1877 loss: 1.97995405e-06
Iter: 1878 loss: 1.97551662e-06
Iter: 1879 loss: 1.97545887e-06
Iter: 1880 loss: 1.97543909e-06
Iter: 1881 loss: 1.97532609e-06
Iter: 1882 loss: 1.97515919e-06
Iter: 1883 loss: 1.97955569e-06
Iter: 1884 loss: 1.97514419e-06
Iter: 1885 loss: 1.97494023e-06
Iter: 1886 loss: 1.97617965e-06
Iter: 1887 loss: 1.97492614e-06
Iter: 1888 loss: 1.97472582e-06
Iter: 1889 loss: 1.97501231e-06
Iter: 1890 loss: 1.97462214e-06
Iter: 1891 loss: 1.97449071e-06
Iter: 1892 loss: 1.97447253e-06
Iter: 1893 loss: 1.97440158e-06
Iter: 1894 loss: 1.97426471e-06
Iter: 1895 loss: 1.97425038e-06
Iter: 1896 loss: 1.97416352e-06
Iter: 1897 loss: 1.97400482e-06
Iter: 1898 loss: 1.9740105e-06
Iter: 1899 loss: 1.97378449e-06
Iter: 1900 loss: 1.97462191e-06
Iter: 1901 loss: 1.97374447e-06
Iter: 1902 loss: 1.97363624e-06
Iter: 1903 loss: 1.97412737e-06
Iter: 1904 loss: 1.97361146e-06
Iter: 1905 loss: 1.97350982e-06
Iter: 1906 loss: 1.97414965e-06
Iter: 1907 loss: 1.97347299e-06
Iter: 1908 loss: 1.97335794e-06
Iter: 1909 loss: 1.97323652e-06
Iter: 1910 loss: 1.97323129e-06
Iter: 1911 loss: 1.97309464e-06
Iter: 1912 loss: 1.97438749e-06
Iter: 1913 loss: 1.9731e-06
Iter: 1914 loss: 1.97293593e-06
Iter: 1915 loss: 1.97346299e-06
Iter: 1916 loss: 1.97291638e-06
Iter: 1917 loss: 1.97281634e-06
Iter: 1918 loss: 1.97275858e-06
Iter: 1919 loss: 1.97271788e-06
Iter: 1920 loss: 1.97253621e-06
Iter: 1921 loss: 1.97359668e-06
Iter: 1922 loss: 1.97251438e-06
Iter: 1923 loss: 1.97239956e-06
Iter: 1924 loss: 1.97228474e-06
Iter: 1925 loss: 1.97227064e-06
Iter: 1926 loss: 1.97216491e-06
Iter: 1927 loss: 1.97215945e-06
Iter: 1928 loss: 1.97204508e-06
Iter: 1929 loss: 1.97194368e-06
Iter: 1930 loss: 1.97191662e-06
Iter: 1931 loss: 1.97176587e-06
Iter: 1932 loss: 1.97236977e-06
Iter: 1933 loss: 1.9717304e-06
Iter: 1934 loss: 1.97162649e-06
Iter: 1935 loss: 1.97169902e-06
Iter: 1936 loss: 1.97152758e-06
Iter: 1937 loss: 1.97138183e-06
Iter: 1938 loss: 1.97316717e-06
Iter: 1939 loss: 1.97137501e-06
Iter: 1940 loss: 1.97125974e-06
Iter: 1941 loss: 1.97106e-06
Iter: 1942 loss: 1.97553527e-06
Iter: 1943 loss: 1.97107101e-06
Iter: 1944 loss: 1.97091413e-06
Iter: 1945 loss: 1.97202962e-06
Iter: 1946 loss: 1.97089457e-06
Iter: 1947 loss: 1.97075292e-06
Iter: 1948 loss: 1.97194368e-06
Iter: 1949 loss: 1.9707436e-06
Iter: 1950 loss: 1.97064651e-06
Iter: 1951 loss: 1.97056238e-06
Iter: 1952 loss: 1.97054987e-06
Iter: 1953 loss: 1.97042255e-06
Iter: 1954 loss: 1.970423e-06
Iter: 1955 loss: 1.97036206e-06
Iter: 1956 loss: 1.970212e-06
Iter: 1957 loss: 1.97136546e-06
Iter: 1958 loss: 1.9701888e-06
Iter: 1959 loss: 1.97004874e-06
Iter: 1960 loss: 1.97135819e-06
Iter: 1961 loss: 1.97004374e-06
Iter: 1962 loss: 1.96983547e-06
Iter: 1963 loss: 1.97016561e-06
Iter: 1964 loss: 1.96975543e-06
Iter: 1965 loss: 1.96960036e-06
Iter: 1966 loss: 1.96984183e-06
Iter: 1967 loss: 1.96954898e-06
Iter: 1968 loss: 1.96940573e-06
Iter: 1969 loss: 1.96955307e-06
Iter: 1970 loss: 1.9693075e-06
Iter: 1971 loss: 1.96920064e-06
Iter: 1972 loss: 1.96919405e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi0.8
+ date
Sat Oct 31 19:27:13 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b02e71e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b02c4c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b02c4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b02c4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b042bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b03ef950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b042bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b042b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b002c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b002c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b027d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b010df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b01266a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b00948c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0860193950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08601938c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f086017c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08601a22f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f086022bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b01a7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b0193840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b0193950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08600aa6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b01abbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b0193bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08600b9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08b0150840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f085afad840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f085afad1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f085afa02f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f08601b5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f085afe2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f085afe21e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f085af289d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f085af36510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0860030d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0058949324
test_loss: 0.0060729696
train_loss: 0.0046366495
test_loss: 0.0044806674
train_loss: 0.0044987686
test_loss: 0.0040567014
train_loss: 0.003672948
test_loss: 0.003753
train_loss: 0.0033533643
test_loss: 0.0034172747
train_loss: 0.0032071876
test_loss: 0.0031869162
train_loss: 0.0028816683
test_loss: 0.002990128
train_loss: 0.0027759047
test_loss: 0.0028328418
train_loss: 0.002581688
test_loss: 0.0026989072
train_loss: 0.0024333515
test_loss: 0.0025974233
train_loss: 0.0022737777
test_loss: 0.0024490445
train_loss: 0.0022655549
test_loss: 0.0024068442
train_loss: 0.0022774863
test_loss: 0.0023990653
train_loss: 0.0021200613
test_loss: 0.0022614955
train_loss: 0.0020461357
test_loss: 0.002228756
train_loss: 0.0019464052
test_loss: 0.0021840774
train_loss: 0.0019279474
test_loss: 0.002130492
train_loss: 0.0018724079
test_loss: 0.0020651375
train_loss: 0.0018374028
test_loss: 0.002046983
train_loss: 0.0017864698
test_loss: 0.0020002585
train_loss: 0.001807332
test_loss: 0.0020185835
train_loss: 0.0018138895
test_loss: 0.001975355
train_loss: 0.0017524482
test_loss: 0.001931158
train_loss: 0.0017206531
test_loss: 0.001915708
train_loss: 0.001639842
test_loss: 0.0018948559
train_loss: 0.0016291908
test_loss: 0.0018684432
train_loss: 0.001663362
test_loss: 0.0018620847
train_loss: 0.0016486847
test_loss: 0.0018434572
train_loss: 0.0016030468
test_loss: 0.0018463471
train_loss: 0.0015596324
test_loss: 0.0018274208
train_loss: 0.0015986274
test_loss: 0.0018242717
train_loss: 0.0016073582
test_loss: 0.0018070319
train_loss: 0.0015743054
test_loss: 0.0018064096
train_loss: 0.0015694706
test_loss: 0.0017983265
train_loss: 0.0015643958
test_loss: 0.001785146
train_loss: 0.0015805734
test_loss: 0.0017827
train_loss: 0.0016041929
test_loss: 0.0017794125
train_loss: 0.0015601377
test_loss: 0.0017748273
train_loss: 0.0015063883
test_loss: 0.0017691174
train_loss: 0.0015761409
test_loss: 0.0017594715
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi0.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi0.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f4b0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f4b0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f4f9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f5bf950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f406e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f406d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f443c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f443950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58766f5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58766f5158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58766b3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f587663ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f587663fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f589f3e9ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f587667bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f587666cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5876699400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5876699730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58765ec6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58765ecd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58504ae598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58504ae950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5850439730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5850437950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5850437598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58504111e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58504898c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f585049b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f585037c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f585037e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f585030a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f585034b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5850350268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f585035a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f58502c8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5850274400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.37548715e-06
Iter: 2 loss: 4.35081074e-06
Iter: 3 loss: 4.60326419e-06
Iter: 4 loss: 4.35012953e-06
Iter: 5 loss: 4.3315822e-06
Iter: 6 loss: 4.38787811e-06
Iter: 7 loss: 4.32605839e-06
Iter: 8 loss: 4.31397166e-06
Iter: 9 loss: 4.31380522e-06
Iter: 10 loss: 4.3042819e-06
Iter: 11 loss: 4.28504427e-06
Iter: 12 loss: 4.40453596e-06
Iter: 13 loss: 4.28279782e-06
Iter: 14 loss: 4.26951101e-06
Iter: 15 loss: 4.28033854e-06
Iter: 16 loss: 4.26149927e-06
Iter: 17 loss: 4.24775817e-06
Iter: 18 loss: 4.27480882e-06
Iter: 19 loss: 4.24203427e-06
Iter: 20 loss: 4.2251113e-06
Iter: 21 loss: 4.33190144e-06
Iter: 22 loss: 4.22312678e-06
Iter: 23 loss: 4.21158211e-06
Iter: 24 loss: 4.1933572e-06
Iter: 25 loss: 4.19312619e-06
Iter: 26 loss: 4.19144817e-06
Iter: 27 loss: 4.18449099e-06
Iter: 28 loss: 4.17840238e-06
Iter: 29 loss: 4.16434386e-06
Iter: 30 loss: 4.34090452e-06
Iter: 31 loss: 4.16322337e-06
Iter: 32 loss: 4.15006934e-06
Iter: 33 loss: 4.16132389e-06
Iter: 34 loss: 4.14223859e-06
Iter: 35 loss: 4.12622921e-06
Iter: 36 loss: 4.17109277e-06
Iter: 37 loss: 4.12106237e-06
Iter: 38 loss: 4.10429857e-06
Iter: 39 loss: 4.1521962e-06
Iter: 40 loss: 4.09901304e-06
Iter: 41 loss: 4.10550547e-06
Iter: 42 loss: 4.09413224e-06
Iter: 43 loss: 4.09034465e-06
Iter: 44 loss: 4.08235519e-06
Iter: 45 loss: 4.21089953e-06
Iter: 46 loss: 4.08212782e-06
Iter: 47 loss: 4.07824518e-06
Iter: 48 loss: 4.07790776e-06
Iter: 49 loss: 4.07398693e-06
Iter: 50 loss: 4.0709283e-06
Iter: 51 loss: 4.06965773e-06
Iter: 52 loss: 4.0645341e-06
Iter: 53 loss: 4.07553171e-06
Iter: 54 loss: 4.06253366e-06
Iter: 55 loss: 4.055933e-06
Iter: 56 loss: 4.09049426e-06
Iter: 57 loss: 4.05489754e-06
Iter: 58 loss: 4.05061155e-06
Iter: 59 loss: 4.05772516e-06
Iter: 60 loss: 4.04867887e-06
Iter: 61 loss: 4.04418688e-06
Iter: 62 loss: 4.04766615e-06
Iter: 63 loss: 4.04148886e-06
Iter: 64 loss: 4.03578179e-06
Iter: 65 loss: 4.09652148e-06
Iter: 66 loss: 4.03564309e-06
Iter: 67 loss: 4.03349895e-06
Iter: 68 loss: 4.02935848e-06
Iter: 69 loss: 4.11622386e-06
Iter: 70 loss: 4.02934711e-06
Iter: 71 loss: 4.02418891e-06
Iter: 72 loss: 4.02526894e-06
Iter: 73 loss: 4.02038404e-06
Iter: 74 loss: 4.01251236e-06
Iter: 75 loss: 4.02958722e-06
Iter: 76 loss: 4.00944236e-06
Iter: 77 loss: 4.00434783e-06
Iter: 78 loss: 4.00429872e-06
Iter: 79 loss: 3.99980672e-06
Iter: 80 loss: 4.0386949e-06
Iter: 81 loss: 3.99958299e-06
Iter: 82 loss: 3.99782e-06
Iter: 83 loss: 3.99487226e-06
Iter: 84 loss: 3.99484088e-06
Iter: 85 loss: 3.99170585e-06
Iter: 86 loss: 3.99170131e-06
Iter: 87 loss: 3.99023338e-06
Iter: 88 loss: 3.98641441e-06
Iter: 89 loss: 4.01286388e-06
Iter: 90 loss: 3.98549764e-06
Iter: 91 loss: 3.98207249e-06
Iter: 92 loss: 3.98181419e-06
Iter: 93 loss: 3.97941312e-06
Iter: 94 loss: 3.97761687e-06
Iter: 95 loss: 3.97678377e-06
Iter: 96 loss: 3.97407939e-06
Iter: 97 loss: 3.99482042e-06
Iter: 98 loss: 3.97389522e-06
Iter: 99 loss: 3.97148096e-06
Iter: 100 loss: 3.9804554e-06
Iter: 101 loss: 3.97086342e-06
Iter: 102 loss: 3.96880205e-06
Iter: 103 loss: 3.96951373e-06
Iter: 104 loss: 3.96737869e-06
Iter: 105 loss: 3.9650231e-06
Iter: 106 loss: 3.96118776e-06
Iter: 107 loss: 3.96116138e-06
Iter: 108 loss: 3.95644338e-06
Iter: 109 loss: 3.97266422e-06
Iter: 110 loss: 3.95520055e-06
Iter: 111 loss: 3.95207462e-06
Iter: 112 loss: 3.95207917e-06
Iter: 113 loss: 3.94932613e-06
Iter: 114 loss: 3.97242911e-06
Iter: 115 loss: 3.94914287e-06
Iter: 116 loss: 3.94798781e-06
Iter: 117 loss: 3.94642211e-06
Iter: 118 loss: 3.94632752e-06
Iter: 119 loss: 3.94487597e-06
Iter: 120 loss: 3.94482186e-06
Iter: 121 loss: 3.94370591e-06
Iter: 122 loss: 3.94072504e-06
Iter: 123 loss: 3.96199493e-06
Iter: 124 loss: 3.94010885e-06
Iter: 125 loss: 3.93888968e-06
Iter: 126 loss: 3.9383408e-06
Iter: 127 loss: 3.93661776e-06
Iter: 128 loss: 3.93353321e-06
Iter: 129 loss: 3.93351365e-06
Iter: 130 loss: 3.93110622e-06
Iter: 131 loss: 3.9433e-06
Iter: 132 loss: 3.93066466e-06
Iter: 133 loss: 3.92844913e-06
Iter: 134 loss: 3.95214693e-06
Iter: 135 loss: 3.92841321e-06
Iter: 136 loss: 3.92744414e-06
Iter: 137 loss: 3.92733818e-06
Iter: 138 loss: 3.92662514e-06
Iter: 139 loss: 3.92509673e-06
Iter: 140 loss: 3.92405855e-06
Iter: 141 loss: 3.92352831e-06
Iter: 142 loss: 3.92081165e-06
Iter: 143 loss: 3.92673792e-06
Iter: 144 loss: 3.91978847e-06
Iter: 145 loss: 3.91804315e-06
Iter: 146 loss: 3.93732444e-06
Iter: 147 loss: 3.91800859e-06
Iter: 148 loss: 3.91590311e-06
Iter: 149 loss: 3.92043694e-06
Iter: 150 loss: 3.91508593e-06
Iter: 151 loss: 3.91373851e-06
Iter: 152 loss: 3.91314416e-06
Iter: 153 loss: 3.91244248e-06
Iter: 154 loss: 3.91107278e-06
Iter: 155 loss: 3.91104e-06
Iter: 156 loss: 3.91032108e-06
Iter: 157 loss: 3.9086176e-06
Iter: 158 loss: 3.92859602e-06
Iter: 159 loss: 3.90847799e-06
Iter: 160 loss: 3.90763671e-06
Iter: 161 loss: 3.90742935e-06
Iter: 162 loss: 3.90644618e-06
Iter: 163 loss: 3.90428522e-06
Iter: 164 loss: 3.9377469e-06
Iter: 165 loss: 3.90420973e-06
Iter: 166 loss: 3.90265768e-06
Iter: 167 loss: 3.92083348e-06
Iter: 168 loss: 3.90263e-06
Iter: 169 loss: 3.90088e-06
Iter: 170 loss: 3.90531841e-06
Iter: 171 loss: 3.90036485e-06
Iter: 172 loss: 3.89919933e-06
Iter: 173 loss: 3.89816387e-06
Iter: 174 loss: 3.89789329e-06
Iter: 175 loss: 3.89623347e-06
Iter: 176 loss: 3.91037702e-06
Iter: 177 loss: 3.89616e-06
Iter: 178 loss: 3.89518073e-06
Iter: 179 loss: 3.89526303e-06
Iter: 180 loss: 3.89437628e-06
Iter: 181 loss: 3.89339129e-06
Iter: 182 loss: 3.89337401e-06
Iter: 183 loss: 3.89219622e-06
Iter: 184 loss: 3.89098705e-06
Iter: 185 loss: 3.89078787e-06
Iter: 186 loss: 3.88949775e-06
Iter: 187 loss: 3.89628576e-06
Iter: 188 loss: 3.8893013e-06
Iter: 189 loss: 3.88778e-06
Iter: 190 loss: 3.89026127e-06
Iter: 191 loss: 3.88706394e-06
Iter: 192 loss: 3.88610351e-06
Iter: 193 loss: 3.88533317e-06
Iter: 194 loss: 3.88506e-06
Iter: 195 loss: 3.88407534e-06
Iter: 196 loss: 3.88398439e-06
Iter: 197 loss: 3.88341914e-06
Iter: 198 loss: 3.88219e-06
Iter: 199 loss: 3.89972411e-06
Iter: 200 loss: 3.88210537e-06
Iter: 201 loss: 3.88111584e-06
Iter: 202 loss: 3.88107628e-06
Iter: 203 loss: 3.88000444e-06
Iter: 204 loss: 3.87913315e-06
Iter: 205 loss: 3.87879118e-06
Iter: 206 loss: 3.87746604e-06
Iter: 207 loss: 3.87893624e-06
Iter: 208 loss: 3.87674663e-06
Iter: 209 loss: 3.87529e-06
Iter: 210 loss: 3.88231365e-06
Iter: 211 loss: 3.87503042e-06
Iter: 212 loss: 3.87392e-06
Iter: 213 loss: 3.8772414e-06
Iter: 214 loss: 3.87357886e-06
Iter: 215 loss: 3.87275759e-06
Iter: 216 loss: 3.87271575e-06
Iter: 217 loss: 3.87230557e-06
Iter: 218 loss: 3.87133196e-06
Iter: 219 loss: 3.88671697e-06
Iter: 220 loss: 3.8712833e-06
Iter: 221 loss: 3.87058162e-06
Iter: 222 loss: 3.87055479e-06
Iter: 223 loss: 3.86998818e-06
Iter: 224 loss: 3.86858483e-06
Iter: 225 loss: 3.88422814e-06
Iter: 226 loss: 3.86845113e-06
Iter: 227 loss: 3.866935e-06
Iter: 228 loss: 3.87396904e-06
Iter: 229 loss: 3.86663396e-06
Iter: 230 loss: 3.86546299e-06
Iter: 231 loss: 3.86549527e-06
Iter: 232 loss: 3.8649323e-06
Iter: 233 loss: 3.86368265e-06
Iter: 234 loss: 3.87637738e-06
Iter: 235 loss: 3.86355077e-06
Iter: 236 loss: 3.86320426e-06
Iter: 237 loss: 3.86284728e-06
Iter: 238 loss: 3.86227e-06
Iter: 239 loss: 3.86135753e-06
Iter: 240 loss: 3.88160606e-06
Iter: 241 loss: 3.86134252e-06
Iter: 242 loss: 3.8602434e-06
Iter: 243 loss: 3.85945168e-06
Iter: 244 loss: 3.85907651e-06
Iter: 245 loss: 3.85741532e-06
Iter: 246 loss: 3.8725e-06
Iter: 247 loss: 3.85732119e-06
Iter: 248 loss: 3.85637941e-06
Iter: 249 loss: 3.86862757e-06
Iter: 250 loss: 3.8563644e-06
Iter: 251 loss: 3.8554017e-06
Iter: 252 loss: 3.85749263e-06
Iter: 253 loss: 3.85511612e-06
Iter: 254 loss: 3.85450448e-06
Iter: 255 loss: 3.85446492e-06
Iter: 256 loss: 3.85405974e-06
Iter: 257 loss: 3.8529306e-06
Iter: 258 loss: 3.85804424e-06
Iter: 259 loss: 3.8526955e-06
Iter: 260 loss: 3.85205567e-06
Iter: 261 loss: 3.85130807e-06
Iter: 262 loss: 3.85124895e-06
Iter: 263 loss: 3.85039311e-06
Iter: 264 loss: 3.85040221e-06
Iter: 265 loss: 3.84957821e-06
Iter: 266 loss: 3.84829855e-06
Iter: 267 loss: 3.84824489e-06
Iter: 268 loss: 3.84738541e-06
Iter: 269 loss: 3.85446174e-06
Iter: 270 loss: 3.8473454e-06
Iter: 271 loss: 3.84655e-06
Iter: 272 loss: 3.85214162e-06
Iter: 273 loss: 3.84652867e-06
Iter: 274 loss: 3.8459757e-06
Iter: 275 loss: 3.84475152e-06
Iter: 276 loss: 3.85681415e-06
Iter: 277 loss: 3.84461964e-06
Iter: 278 loss: 3.84312807e-06
Iter: 279 loss: 3.85130079e-06
Iter: 280 loss: 3.84291889e-06
Iter: 281 loss: 3.84177e-06
Iter: 282 loss: 3.84725263e-06
Iter: 283 loss: 3.84155919e-06
Iter: 284 loss: 3.84057876e-06
Iter: 285 loss: 3.85443218e-06
Iter: 286 loss: 3.84059422e-06
Iter: 287 loss: 3.83984025e-06
Iter: 288 loss: 3.83955285e-06
Iter: 289 loss: 3.839079e-06
Iter: 290 loss: 3.83837687e-06
Iter: 291 loss: 3.84259e-06
Iter: 292 loss: 3.83828046e-06
Iter: 293 loss: 3.83747556e-06
Iter: 294 loss: 3.83887073e-06
Iter: 295 loss: 3.83711904e-06
Iter: 296 loss: 3.83664701e-06
Iter: 297 loss: 3.83600491e-06
Iter: 298 loss: 3.83593579e-06
Iter: 299 loss: 3.83515135e-06
Iter: 300 loss: 3.8351377e-06
Iter: 301 loss: 3.83451243e-06
Iter: 302 loss: 3.83340512e-06
Iter: 303 loss: 3.83342831e-06
Iter: 304 loss: 3.83266888e-06
Iter: 305 loss: 3.83269253e-06
Iter: 306 loss: 3.83190172e-06
Iter: 307 loss: 3.83164479e-06
Iter: 308 loss: 3.83117958e-06
Iter: 309 loss: 3.83050974e-06
Iter: 310 loss: 3.83019415e-06
Iter: 311 loss: 3.82985218e-06
Iter: 312 loss: 3.82898543e-06
Iter: 313 loss: 3.830477e-06
Iter: 314 loss: 3.82863709e-06
Iter: 315 loss: 3.82767348e-06
Iter: 316 loss: 3.83358611e-06
Iter: 317 loss: 3.82752614e-06
Iter: 318 loss: 3.82659346e-06
Iter: 319 loss: 3.83345787e-06
Iter: 320 loss: 3.82651388e-06
Iter: 321 loss: 3.82589496e-06
Iter: 322 loss: 3.82556664e-06
Iter: 323 loss: 3.82521375e-06
Iter: 324 loss: 3.82454118e-06
Iter: 325 loss: 3.83257975e-06
Iter: 326 loss: 3.82449707e-06
Iter: 327 loss: 3.82382041e-06
Iter: 328 loss: 3.82384042e-06
Iter: 329 loss: 3.82327789e-06
Iter: 330 loss: 3.82257167e-06
Iter: 331 loss: 3.82318922e-06
Iter: 332 loss: 3.82215649e-06
Iter: 333 loss: 3.82128201e-06
Iter: 334 loss: 3.83076713e-06
Iter: 335 loss: 3.82127746e-06
Iter: 336 loss: 3.82084772e-06
Iter: 337 loss: 3.82001599e-06
Iter: 338 loss: 3.83864881e-06
Iter: 339 loss: 3.82001826e-06
Iter: 340 loss: 3.81934387e-06
Iter: 341 loss: 3.8192934e-06
Iter: 342 loss: 3.81873451e-06
Iter: 343 loss: 3.8177127e-06
Iter: 344 loss: 3.84158693e-06
Iter: 345 loss: 3.81768768e-06
Iter: 346 loss: 3.81667678e-06
Iter: 347 loss: 3.81957489e-06
Iter: 348 loss: 3.81639438e-06
Iter: 349 loss: 3.81544032e-06
Iter: 350 loss: 3.81817335e-06
Iter: 351 loss: 3.81516611e-06
Iter: 352 loss: 3.81440532e-06
Iter: 353 loss: 3.81993823e-06
Iter: 354 loss: 3.81429777e-06
Iter: 355 loss: 3.81366181e-06
Iter: 356 loss: 3.82145117e-06
Iter: 357 loss: 3.81365589e-06
Iter: 358 loss: 3.81332552e-06
Iter: 359 loss: 3.81253176e-06
Iter: 360 loss: 3.82330063e-06
Iter: 361 loss: 3.81250175e-06
Iter: 362 loss: 3.81190375e-06
Iter: 363 loss: 3.81187442e-06
Iter: 364 loss: 3.81141763e-06
Iter: 365 loss: 3.81092559e-06
Iter: 366 loss: 3.81091058e-06
Iter: 367 loss: 3.8103974e-06
Iter: 368 loss: 3.81613654e-06
Iter: 369 loss: 3.81039899e-06
Iter: 370 loss: 3.8100402e-06
Iter: 371 loss: 3.81086284e-06
Iter: 372 loss: 3.80982988e-06
Iter: 373 loss: 3.80937672e-06
Iter: 374 loss: 3.80894494e-06
Iter: 375 loss: 3.8088765e-06
Iter: 376 loss: 3.80837696e-06
Iter: 377 loss: 3.80836468e-06
Iter: 378 loss: 3.80780239e-06
Iter: 379 loss: 3.80719348e-06
Iter: 380 loss: 3.80714982e-06
Iter: 381 loss: 3.80635402e-06
Iter: 382 loss: 3.80587517e-06
Iter: 383 loss: 3.80555548e-06
Iter: 384 loss: 3.8044027e-06
Iter: 385 loss: 3.81157724e-06
Iter: 386 loss: 3.80427764e-06
Iter: 387 loss: 3.80381903e-06
Iter: 388 loss: 3.80379242e-06
Iter: 389 loss: 3.80330607e-06
Iter: 390 loss: 3.80386882e-06
Iter: 391 loss: 3.80303186e-06
Iter: 392 loss: 3.80252413e-06
Iter: 393 loss: 3.80210167e-06
Iter: 394 loss: 3.80190727e-06
Iter: 395 loss: 3.80115603e-06
Iter: 396 loss: 3.81265181e-06
Iter: 397 loss: 3.80115398e-06
Iter: 398 loss: 3.80067445e-06
Iter: 399 loss: 3.80003416e-06
Iter: 400 loss: 3.80000233e-06
Iter: 401 loss: 3.79933181e-06
Iter: 402 loss: 3.80943948e-06
Iter: 403 loss: 3.79933726e-06
Iter: 404 loss: 3.79879452e-06
Iter: 405 loss: 3.79875883e-06
Iter: 406 loss: 3.79831909e-06
Iter: 407 loss: 3.79769403e-06
Iter: 408 loss: 3.79908624e-06
Iter: 409 loss: 3.7974512e-06
Iter: 410 loss: 3.79700737e-06
Iter: 411 loss: 3.80399752e-06
Iter: 412 loss: 3.79700896e-06
Iter: 413 loss: 3.79660605e-06
Iter: 414 loss: 3.79577705e-06
Iter: 415 loss: 3.81005907e-06
Iter: 416 loss: 3.7958016e-06
Iter: 417 loss: 3.79489234e-06
Iter: 418 loss: 3.79503558e-06
Iter: 419 loss: 3.7941918e-06
Iter: 420 loss: 3.7931959e-06
Iter: 421 loss: 3.80273377e-06
Iter: 422 loss: 3.79313315e-06
Iter: 423 loss: 3.79272751e-06
Iter: 424 loss: 3.79267021e-06
Iter: 425 loss: 3.79229959e-06
Iter: 426 loss: 3.79170706e-06
Iter: 427 loss: 3.79169819e-06
Iter: 428 loss: 3.79117364e-06
Iter: 429 loss: 3.79621179e-06
Iter: 430 loss: 3.79109952e-06
Iter: 431 loss: 3.79063e-06
Iter: 432 loss: 3.7914624e-06
Iter: 433 loss: 3.79036e-06
Iter: 434 loss: 3.78990694e-06
Iter: 435 loss: 3.7899872e-06
Iter: 436 loss: 3.78956338e-06
Iter: 437 loss: 3.78884761e-06
Iter: 438 loss: 3.7935522e-06
Iter: 439 loss: 3.78875257e-06
Iter: 440 loss: 3.78819368e-06
Iter: 441 loss: 3.78766958e-06
Iter: 442 loss: 3.78751065e-06
Iter: 443 loss: 3.78676259e-06
Iter: 444 loss: 3.79300559e-06
Iter: 445 loss: 3.78671598e-06
Iter: 446 loss: 3.7861023e-06
Iter: 447 loss: 3.79009225e-06
Iter: 448 loss: 3.7860741e-06
Iter: 449 loss: 3.78566347e-06
Iter: 450 loss: 3.78491131e-06
Iter: 451 loss: 3.79943026e-06
Iter: 452 loss: 3.78492041e-06
Iter: 453 loss: 3.78413779e-06
Iter: 454 loss: 3.78705499e-06
Iter: 455 loss: 3.78390246e-06
Iter: 456 loss: 3.78345499e-06
Iter: 457 loss: 3.79102721e-06
Iter: 458 loss: 3.78342816e-06
Iter: 459 loss: 3.78283812e-06
Iter: 460 loss: 3.78292839e-06
Iter: 461 loss: 3.78238747e-06
Iter: 462 loss: 3.78169784e-06
Iter: 463 loss: 3.78195045e-06
Iter: 464 loss: 3.78119444e-06
Iter: 465 loss: 3.78067284e-06
Iter: 466 loss: 3.78063896e-06
Iter: 467 loss: 3.78030836e-06
Iter: 468 loss: 3.7797472e-06
Iter: 469 loss: 3.77973879e-06
Iter: 470 loss: 3.77921606e-06
Iter: 471 loss: 3.78684945e-06
Iter: 472 loss: 3.77921469e-06
Iter: 473 loss: 3.77881815e-06
Iter: 474 loss: 3.77903689e-06
Iter: 475 loss: 3.77850256e-06
Iter: 476 loss: 3.77812012e-06
Iter: 477 loss: 3.77813194e-06
Iter: 478 loss: 3.77774404e-06
Iter: 479 loss: 3.77710057e-06
Iter: 480 loss: 3.78308869e-06
Iter: 481 loss: 3.77706374e-06
Iter: 482 loss: 3.77648166e-06
Iter: 483 loss: 3.77603351e-06
Iter: 484 loss: 3.77578112e-06
Iter: 485 loss: 3.7751463e-06
Iter: 486 loss: 3.77544711e-06
Iter: 487 loss: 3.77468314e-06
Iter: 488 loss: 3.77393235e-06
Iter: 489 loss: 3.7773882e-06
Iter: 490 loss: 3.77381662e-06
Iter: 491 loss: 3.77329889e-06
Iter: 492 loss: 3.77330821e-06
Iter: 493 loss: 3.77295487e-06
Iter: 494 loss: 3.77226e-06
Iter: 495 loss: 3.78370623e-06
Iter: 496 loss: 3.7722657e-06
Iter: 497 loss: 3.77167953e-06
Iter: 498 loss: 3.77961237e-06
Iter: 499 loss: 3.77170636e-06
Iter: 500 loss: 3.77114111e-06
Iter: 501 loss: 3.77147762e-06
Iter: 502 loss: 3.77076685e-06
Iter: 503 loss: 3.7701775e-06
Iter: 504 loss: 3.77105357e-06
Iter: 505 loss: 3.76991557e-06
Iter: 506 loss: 3.76920298e-06
Iter: 507 loss: 3.77452534e-06
Iter: 508 loss: 3.76912385e-06
Iter: 509 loss: 3.76870821e-06
Iter: 510 loss: 3.76849175e-06
Iter: 511 loss: 3.76828075e-06
Iter: 512 loss: 3.76773528e-06
Iter: 513 loss: 3.77130846e-06
Iter: 514 loss: 3.76769549e-06
Iter: 515 loss: 3.76713319e-06
Iter: 516 loss: 3.7683435e-06
Iter: 517 loss: 3.7668874e-06
Iter: 518 loss: 3.76631965e-06
Iter: 519 loss: 3.76535854e-06
Iter: 520 loss: 3.76534081e-06
Iter: 521 loss: 3.76427693e-06
Iter: 522 loss: 3.76915978e-06
Iter: 523 loss: 3.7640898e-06
Iter: 524 loss: 3.76380376e-06
Iter: 525 loss: 3.76367598e-06
Iter: 526 loss: 3.76323851e-06
Iter: 527 loss: 3.76271214e-06
Iter: 528 loss: 3.76269054e-06
Iter: 529 loss: 3.76218236e-06
Iter: 530 loss: 3.76406751e-06
Iter: 531 loss: 3.76205503e-06
Iter: 532 loss: 3.76164576e-06
Iter: 533 loss: 3.76631579e-06
Iter: 534 loss: 3.76167577e-06
Iter: 535 loss: 3.76136131e-06
Iter: 536 loss: 3.76090316e-06
Iter: 537 loss: 3.76089884e-06
Iter: 538 loss: 3.76038906e-06
Iter: 539 loss: 3.76637945e-06
Iter: 540 loss: 3.76038406e-06
Iter: 541 loss: 3.75993318e-06
Iter: 542 loss: 3.75943682e-06
Iter: 543 loss: 3.75934314e-06
Iter: 544 loss: 3.75868649e-06
Iter: 545 loss: 3.76094431e-06
Iter: 546 loss: 3.75854074e-06
Iter: 547 loss: 3.75804075e-06
Iter: 548 loss: 3.75804893e-06
Iter: 549 loss: 3.75769832e-06
Iter: 550 loss: 3.75723766e-06
Iter: 551 loss: 3.75720265e-06
Iter: 552 loss: 3.75666377e-06
Iter: 553 loss: 3.75672562e-06
Iter: 554 loss: 3.75618242e-06
Iter: 555 loss: 3.75547597e-06
Iter: 556 loss: 3.75988157e-06
Iter: 557 loss: 3.75538411e-06
Iter: 558 loss: 3.75477907e-06
Iter: 559 loss: 3.7632426e-06
Iter: 560 loss: 3.75476884e-06
Iter: 561 loss: 3.75441641e-06
Iter: 562 loss: 3.75354375e-06
Iter: 563 loss: 3.76567368e-06
Iter: 564 loss: 3.75354739e-06
Iter: 565 loss: 3.75326249e-06
Iter: 566 loss: 3.75312857e-06
Iter: 567 loss: 3.75278387e-06
Iter: 568 loss: 3.752559e-06
Iter: 569 loss: 3.75245e-06
Iter: 570 loss: 3.75205286e-06
Iter: 571 loss: 3.75383456e-06
Iter: 572 loss: 3.75190666e-06
Iter: 573 loss: 3.75147488e-06
Iter: 574 loss: 3.75269838e-06
Iter: 575 loss: 3.75132163e-06
Iter: 576 loss: 3.75091577e-06
Iter: 577 loss: 3.75058244e-06
Iter: 578 loss: 3.75044101e-06
Iter: 579 loss: 3.74990282e-06
Iter: 580 loss: 3.75605987e-06
Iter: 581 loss: 3.74986712e-06
Iter: 582 loss: 3.74928686e-06
Iter: 583 loss: 3.74956358e-06
Iter: 584 loss: 3.74888168e-06
Iter: 585 loss: 3.74833894e-06
Iter: 586 loss: 3.74829256e-06
Iter: 587 loss: 3.74785964e-06
Iter: 588 loss: 3.74731417e-06
Iter: 589 loss: 3.74972251e-06
Iter: 590 loss: 3.74718093e-06
Iter: 591 loss: 3.74689444e-06
Iter: 592 loss: 3.74690399e-06
Iter: 593 loss: 3.74656292e-06
Iter: 594 loss: 3.74603314e-06
Iter: 595 loss: 3.74602564e-06
Iter: 596 loss: 3.7455759e-06
Iter: 597 loss: 3.74728643e-06
Iter: 598 loss: 3.74547358e-06
Iter: 599 loss: 3.74489946e-06
Iter: 600 loss: 3.74689421e-06
Iter: 601 loss: 3.74473211e-06
Iter: 602 loss: 3.74423735e-06
Iter: 603 loss: 3.74405499e-06
Iter: 604 loss: 3.74384945e-06
Iter: 605 loss: 3.74331375e-06
Iter: 606 loss: 3.74329193e-06
Iter: 607 loss: 3.743e-06
Iter: 608 loss: 3.74271031e-06
Iter: 609 loss: 3.74262572e-06
Iter: 610 loss: 3.742226e-06
Iter: 611 loss: 3.74364845e-06
Iter: 612 loss: 3.74208071e-06
Iter: 613 loss: 3.74163983e-06
Iter: 614 loss: 3.74478827e-06
Iter: 615 loss: 3.7416271e-06
Iter: 616 loss: 3.74128012e-06
Iter: 617 loss: 3.74066622e-06
Iter: 618 loss: 3.74070669e-06
Iter: 619 loss: 3.7399584e-06
Iter: 620 loss: 3.74069396e-06
Iter: 621 loss: 3.73954344e-06
Iter: 622 loss: 3.73892613e-06
Iter: 623 loss: 3.74680758e-06
Iter: 624 loss: 3.73895318e-06
Iter: 625 loss: 3.73838566e-06
Iter: 626 loss: 3.74129058e-06
Iter: 627 loss: 3.73826288e-06
Iter: 628 loss: 3.73790704e-06
Iter: 629 loss: 3.73747775e-06
Iter: 630 loss: 3.73741022e-06
Iter: 631 loss: 3.7371683e-06
Iter: 632 loss: 3.7371567e-06
Iter: 633 loss: 3.7369e-06
Iter: 634 loss: 3.73642661e-06
Iter: 635 loss: 3.74606361e-06
Iter: 636 loss: 3.73643456e-06
Iter: 637 loss: 3.73604144e-06
Iter: 638 loss: 3.74041474e-06
Iter: 639 loss: 3.73602029e-06
Iter: 640 loss: 3.73562261e-06
Iter: 641 loss: 3.7354705e-06
Iter: 642 loss: 3.73524699e-06
Iter: 643 loss: 3.73467401e-06
Iter: 644 loss: 3.73528974e-06
Iter: 645 loss: 3.73438593e-06
Iter: 646 loss: 3.73390776e-06
Iter: 647 loss: 3.73389776e-06
Iter: 648 loss: 3.73351668e-06
Iter: 649 loss: 3.73327794e-06
Iter: 650 loss: 3.73315947e-06
Iter: 651 loss: 3.73259672e-06
Iter: 652 loss: 3.73294847e-06
Iter: 653 loss: 3.73228022e-06
Iter: 654 loss: 3.73165358e-06
Iter: 655 loss: 3.73249145e-06
Iter: 656 loss: 3.73138482e-06
Iter: 657 loss: 3.7310424e-06
Iter: 658 loss: 3.73094986e-06
Iter: 659 loss: 3.73057014e-06
Iter: 660 loss: 3.7297882e-06
Iter: 661 loss: 3.74261208e-06
Iter: 662 loss: 3.72976729e-06
Iter: 663 loss: 3.72931527e-06
Iter: 664 loss: 3.73691955e-06
Iter: 665 loss: 3.72931891e-06
Iter: 666 loss: 3.72883142e-06
Iter: 667 loss: 3.72940758e-06
Iter: 668 loss: 3.72858676e-06
Iter: 669 loss: 3.72820182e-06
Iter: 670 loss: 3.72854606e-06
Iter: 671 loss: 3.72799423e-06
Iter: 672 loss: 3.72752447e-06
Iter: 673 loss: 3.73247531e-06
Iter: 674 loss: 3.72754334e-06
Iter: 675 loss: 3.72722548e-06
Iter: 676 loss: 3.72680415e-06
Iter: 677 loss: 3.72680688e-06
Iter: 678 loss: 3.72638829e-06
Iter: 679 loss: 3.72919476e-06
Iter: 680 loss: 3.72631757e-06
Iter: 681 loss: 3.72577392e-06
Iter: 682 loss: 3.72733757e-06
Iter: 683 loss: 3.72562681e-06
Iter: 684 loss: 3.72516479e-06
Iter: 685 loss: 3.72466525e-06
Iter: 686 loss: 3.72461636e-06
Iter: 687 loss: 3.72389491e-06
Iter: 688 loss: 3.72675936e-06
Iter: 689 loss: 3.72380396e-06
Iter: 690 loss: 3.72341287e-06
Iter: 691 loss: 3.72894829e-06
Iter: 692 loss: 3.72338741e-06
Iter: 693 loss: 3.72294721e-06
Iter: 694 loss: 3.72313343e-06
Iter: 695 loss: 3.72264526e-06
Iter: 696 loss: 3.72226282e-06
Iter: 697 loss: 3.72248428e-06
Iter: 698 loss: 3.72204113e-06
Iter: 699 loss: 3.72150157e-06
Iter: 700 loss: 3.72698969e-06
Iter: 701 loss: 3.72153272e-06
Iter: 702 loss: 3.72116574e-06
Iter: 703 loss: 3.72053546e-06
Iter: 704 loss: 3.7341274e-06
Iter: 705 loss: 3.72051454e-06
Iter: 706 loss: 3.72007844e-06
Iter: 707 loss: 3.72003183e-06
Iter: 708 loss: 3.71965893e-06
Iter: 709 loss: 3.71931083e-06
Iter: 710 loss: 3.71920692e-06
Iter: 711 loss: 3.71879787e-06
Iter: 712 loss: 3.72031718e-06
Iter: 713 loss: 3.71869373e-06
Iter: 714 loss: 3.71830447e-06
Iter: 715 loss: 3.72132763e-06
Iter: 716 loss: 3.71829492e-06
Iter: 717 loss: 3.71790293e-06
Iter: 718 loss: 3.71745546e-06
Iter: 719 loss: 3.71746637e-06
Iter: 720 loss: 3.71681563e-06
Iter: 721 loss: 3.71742499e-06
Iter: 722 loss: 3.71645729e-06
Iter: 723 loss: 3.71581837e-06
Iter: 724 loss: 3.71965234e-06
Iter: 725 loss: 3.71571696e-06
Iter: 726 loss: 3.71525221e-06
Iter: 727 loss: 3.71525448e-06
Iter: 728 loss: 3.71497276e-06
Iter: 729 loss: 3.71446799e-06
Iter: 730 loss: 3.72340673e-06
Iter: 731 loss: 3.7144714e-06
Iter: 732 loss: 3.71415967e-06
Iter: 733 loss: 3.71410488e-06
Iter: 734 loss: 3.71386204e-06
Iter: 735 loss: 3.71339866e-06
Iter: 736 loss: 3.71341048e-06
Iter: 737 loss: 3.71295278e-06
Iter: 738 loss: 3.71482065e-06
Iter: 739 loss: 3.71286592e-06
Iter: 740 loss: 3.71224701e-06
Iter: 741 loss: 3.71369833e-06
Iter: 742 loss: 3.7120185e-06
Iter: 743 loss: 3.71146734e-06
Iter: 744 loss: 3.71105716e-06
Iter: 745 loss: 3.71092347e-06
Iter: 746 loss: 3.71030046e-06
Iter: 747 loss: 3.71030251e-06
Iter: 748 loss: 3.70982389e-06
Iter: 749 loss: 3.71012493e-06
Iter: 750 loss: 3.70951921e-06
Iter: 751 loss: 3.70907901e-06
Iter: 752 loss: 3.70925773e-06
Iter: 753 loss: 3.70876933e-06
Iter: 754 loss: 3.70823591e-06
Iter: 755 loss: 3.70929638e-06
Iter: 756 loss: 3.70802263e-06
Iter: 757 loss: 3.70769703e-06
Iter: 758 loss: 3.70765542e-06
Iter: 759 loss: 3.70731686e-06
Iter: 760 loss: 3.7066643e-06
Iter: 761 loss: 3.72001932e-06
Iter: 762 loss: 3.70666248e-06
Iter: 763 loss: 3.70608541e-06
Iter: 764 loss: 3.71019746e-06
Iter: 765 loss: 3.70602879e-06
Iter: 766 loss: 3.70539738e-06
Iter: 767 loss: 3.70748353e-06
Iter: 768 loss: 3.70525231e-06
Iter: 769 loss: 3.7048776e-06
Iter: 770 loss: 3.70470661e-06
Iter: 771 loss: 3.7045213e-06
Iter: 772 loss: 3.70408134e-06
Iter: 773 loss: 3.7040877e-06
Iter: 774 loss: 3.7037621e-06
Iter: 775 loss: 3.70325756e-06
Iter: 776 loss: 3.70325415e-06
Iter: 777 loss: 3.70277576e-06
Iter: 778 loss: 3.70596263e-06
Iter: 779 loss: 3.70276643e-06
Iter: 780 loss: 3.70221665e-06
Iter: 781 loss: 3.70381622e-06
Iter: 782 loss: 3.70209682e-06
Iter: 783 loss: 3.70156113e-06
Iter: 784 loss: 3.70110797e-06
Iter: 785 loss: 3.70098678e-06
Iter: 786 loss: 3.7002651e-06
Iter: 787 loss: 3.7025352e-06
Iter: 788 loss: 3.70002545e-06
Iter: 789 loss: 3.69954319e-06
Iter: 790 loss: 3.70489624e-06
Iter: 791 loss: 3.69953136e-06
Iter: 792 loss: 3.69899931e-06
Iter: 793 loss: 3.70099156e-06
Iter: 794 loss: 3.69892564e-06
Iter: 795 loss: 3.69859436e-06
Iter: 796 loss: 3.69813642e-06
Iter: 797 loss: 3.69815029e-06
Iter: 798 loss: 3.69778968e-06
Iter: 799 loss: 3.69776353e-06
Iter: 800 loss: 3.69746908e-06
Iter: 801 loss: 3.69679265e-06
Iter: 802 loss: 3.70463113e-06
Iter: 803 loss: 3.69673671e-06
Iter: 804 loss: 3.69614213e-06
Iter: 805 loss: 3.69614463e-06
Iter: 806 loss: 3.69556028e-06
Iter: 807 loss: 3.69581562e-06
Iter: 808 loss: 3.69518375e-06
Iter: 809 loss: 3.69467625e-06
Iter: 810 loss: 3.69502641e-06
Iter: 811 loss: 3.69442114e-06
Iter: 812 loss: 3.6940005e-06
Iter: 813 loss: 3.69400868e-06
Iter: 814 loss: 3.69360123e-06
Iter: 815 loss: 3.69322197e-06
Iter: 816 loss: 3.6931674e-06
Iter: 817 loss: 3.69260897e-06
Iter: 818 loss: 3.69276358e-06
Iter: 819 loss: 3.69211e-06
Iter: 820 loss: 3.69125951e-06
Iter: 821 loss: 3.69284203e-06
Iter: 822 loss: 3.69094641e-06
Iter: 823 loss: 3.69064583e-06
Iter: 824 loss: 3.69056852e-06
Iter: 825 loss: 3.69014015e-06
Iter: 826 loss: 3.68980909e-06
Iter: 827 loss: 3.68969927e-06
Iter: 828 loss: 3.68927226e-06
Iter: 829 loss: 3.69082045e-06
Iter: 830 loss: 3.68912811e-06
Iter: 831 loss: 3.688687e-06
Iter: 832 loss: 3.6921715e-06
Iter: 833 loss: 3.68864653e-06
Iter: 834 loss: 3.68836982e-06
Iter: 835 loss: 3.68763085e-06
Iter: 836 loss: 3.69545978e-06
Iter: 837 loss: 3.68756764e-06
Iter: 838 loss: 3.6871138e-06
Iter: 839 loss: 3.68704696e-06
Iter: 840 loss: 3.68660244e-06
Iter: 841 loss: 3.6859974e-06
Iter: 842 loss: 3.68594647e-06
Iter: 843 loss: 3.68531641e-06
Iter: 844 loss: 3.68597648e-06
Iter: 845 loss: 3.68489282e-06
Iter: 846 loss: 3.68453493e-06
Iter: 847 loss: 3.68446445e-06
Iter: 848 loss: 3.68415294e-06
Iter: 849 loss: 3.68381143e-06
Iter: 850 loss: 3.68373594e-06
Iter: 851 loss: 3.68330302e-06
Iter: 852 loss: 3.68362043e-06
Iter: 853 loss: 3.68306314e-06
Iter: 854 loss: 3.68243127e-06
Iter: 855 loss: 3.68246037e-06
Iter: 856 loss: 3.68194878e-06
Iter: 857 loss: 3.68144038e-06
Iter: 858 loss: 3.68141082e-06
Iter: 859 loss: 3.68085921e-06
Iter: 860 loss: 3.68167343e-06
Iter: 861 loss: 3.68060137e-06
Iter: 862 loss: 3.68027895e-06
Iter: 863 loss: 3.68017913e-06
Iter: 864 loss: 3.67990742e-06
Iter: 865 loss: 3.67952634e-06
Iter: 866 loss: 3.67955499e-06
Iter: 867 loss: 3.67919574e-06
Iter: 868 loss: 3.67871303e-06
Iter: 869 loss: 3.67869893e-06
Iter: 870 loss: 3.67831626e-06
Iter: 871 loss: 3.67830216e-06
Iter: 872 loss: 3.67788334e-06
Iter: 873 loss: 3.67768371e-06
Iter: 874 loss: 3.67749681e-06
Iter: 875 loss: 3.67695884e-06
Iter: 876 loss: 3.67670964e-06
Iter: 877 loss: 3.67643815e-06
Iter: 878 loss: 3.6759875e-06
Iter: 879 loss: 3.67595385e-06
Iter: 880 loss: 3.6754675e-06
Iter: 881 loss: 3.67635948e-06
Iter: 882 loss: 3.67532948e-06
Iter: 883 loss: 3.6750107e-06
Iter: 884 loss: 3.67501e-06
Iter: 885 loss: 3.67475286e-06
Iter: 886 loss: 3.67422308e-06
Iter: 887 loss: 3.67450252e-06
Iter: 888 loss: 3.67388247e-06
Iter: 889 loss: 3.673641e-06
Iter: 890 loss: 3.67359917e-06
Iter: 891 loss: 3.673234e-06
Iter: 892 loss: 3.67280336e-06
Iter: 893 loss: 3.67272332e-06
Iter: 894 loss: 3.6722422e-06
Iter: 895 loss: 3.67296275e-06
Iter: 896 loss: 3.67200983e-06
Iter: 897 loss: 3.67161056e-06
Iter: 898 loss: 3.67158714e-06
Iter: 899 loss: 3.67137454e-06
Iter: 900 loss: 3.67093526e-06
Iter: 901 loss: 3.67928351e-06
Iter: 902 loss: 3.67094231e-06
Iter: 903 loss: 3.67053076e-06
Iter: 904 loss: 3.67612438e-06
Iter: 905 loss: 3.67056214e-06
Iter: 906 loss: 3.67012854e-06
Iter: 907 loss: 3.67011035e-06
Iter: 908 loss: 3.66982454e-06
Iter: 909 loss: 3.66933909e-06
Iter: 910 loss: 3.6692e-06
Iter: 911 loss: 3.66892687e-06
Iter: 912 loss: 3.66862741e-06
Iter: 913 loss: 3.66853237e-06
Iter: 914 loss: 3.66827658e-06
Iter: 915 loss: 3.66756103e-06
Iter: 916 loss: 3.67644884e-06
Iter: 917 loss: 3.66749555e-06
Iter: 918 loss: 3.6668464e-06
Iter: 919 loss: 3.6691722e-06
Iter: 920 loss: 3.66669019e-06
Iter: 921 loss: 3.66632435e-06
Iter: 922 loss: 3.66632548e-06
Iter: 923 loss: 3.66595941e-06
Iter: 924 loss: 3.66624636e-06
Iter: 925 loss: 3.66574432e-06
Iter: 926 loss: 3.66532095e-06
Iter: 927 loss: 3.66519e-06
Iter: 928 loss: 3.66491145e-06
Iter: 929 loss: 3.66454242e-06
Iter: 930 loss: 3.66871632e-06
Iter: 931 loss: 3.66453219e-06
Iter: 932 loss: 3.66400945e-06
Iter: 933 loss: 3.6639567e-06
Iter: 934 loss: 3.66357926e-06
Iter: 935 loss: 3.66315635e-06
Iter: 936 loss: 3.66317136e-06
Iter: 937 loss: 3.66279801e-06
Iter: 938 loss: 3.66222412e-06
Iter: 939 loss: 3.66220911e-06
Iter: 940 loss: 3.66190034e-06
Iter: 941 loss: 3.66129893e-06
Iter: 942 loss: 3.67096527e-06
Iter: 943 loss: 3.66132281e-06
Iter: 944 loss: 3.66086124e-06
Iter: 945 loss: 3.66086761e-06
Iter: 946 loss: 3.66034874e-06
Iter: 947 loss: 3.65979258e-06
Iter: 948 loss: 3.65975166e-06
Iter: 949 loss: 3.65907681e-06
Iter: 950 loss: 3.65941924e-06
Iter: 951 loss: 3.65867709e-06
Iter: 952 loss: 3.65803703e-06
Iter: 953 loss: 3.66436871e-06
Iter: 954 loss: 3.65800815e-06
Iter: 955 loss: 3.65745223e-06
Iter: 956 loss: 3.66076893e-06
Iter: 957 loss: 3.65739743e-06
Iter: 958 loss: 3.65698952e-06
Iter: 959 loss: 3.6572028e-06
Iter: 960 loss: 3.6567435e-06
Iter: 961 loss: 3.65637607e-06
Iter: 962 loss: 3.65746087e-06
Iter: 963 loss: 3.65622236e-06
Iter: 964 loss: 3.65570077e-06
Iter: 965 loss: 3.65745655e-06
Iter: 966 loss: 3.65558071e-06
Iter: 967 loss: 3.65518372e-06
Iter: 968 loss: 3.65437677e-06
Iter: 969 loss: 3.66801123e-06
Iter: 970 loss: 3.65438154e-06
Iter: 971 loss: 3.65414189e-06
Iter: 972 loss: 3.65385699e-06
Iter: 973 loss: 3.65356391e-06
Iter: 974 loss: 3.65282358e-06
Iter: 975 loss: 3.6630031e-06
Iter: 976 loss: 3.65277492e-06
Iter: 977 loss: 3.65210758e-06
Iter: 978 loss: 3.65656751e-06
Iter: 979 loss: 3.6520878e-06
Iter: 980 loss: 3.65143978e-06
Iter: 981 loss: 3.65427013e-06
Iter: 982 loss: 3.65130313e-06
Iter: 983 loss: 3.65095616e-06
Iter: 984 loss: 3.65048027e-06
Iter: 985 loss: 3.65044798e-06
Iter: 986 loss: 3.64984362e-06
Iter: 987 loss: 3.65229607e-06
Iter: 988 loss: 3.649679e-06
Iter: 989 loss: 3.64908692e-06
Iter: 990 loss: 3.65113192e-06
Iter: 991 loss: 3.64892912e-06
Iter: 992 loss: 3.64834e-06
Iter: 993 loss: 3.64903326e-06
Iter: 994 loss: 3.64801281e-06
Iter: 995 loss: 3.64745893e-06
Iter: 996 loss: 3.64891048e-06
Iter: 997 loss: 3.64725793e-06
Iter: 998 loss: 3.64676544e-06
Iter: 999 loss: 3.65370533e-06
Iter: 1000 loss: 3.6467577e-06
Iter: 1001 loss: 3.64646348e-06
Iter: 1002 loss: 3.64568359e-06
Iter: 1003 loss: 3.65279675e-06
Iter: 1004 loss: 3.64550715e-06
Iter: 1005 loss: 3.64521225e-06
Iter: 1006 loss: 3.64504194e-06
Iter: 1007 loss: 3.64459265e-06
Iter: 1008 loss: 3.64394373e-06
Iter: 1009 loss: 3.64395601e-06
Iter: 1010 loss: 3.64322705e-06
Iter: 1011 loss: 3.64464131e-06
Iter: 1012 loss: 3.6430215e-06
Iter: 1013 loss: 3.6423221e-06
Iter: 1014 loss: 3.65034475e-06
Iter: 1015 loss: 3.64231664e-06
Iter: 1016 loss: 3.64187417e-06
Iter: 1017 loss: 3.64127527e-06
Iter: 1018 loss: 3.64128209e-06
Iter: 1019 loss: 3.64067546e-06
Iter: 1020 loss: 3.64596917e-06
Iter: 1021 loss: 3.64066e-06
Iter: 1022 loss: 3.64013067e-06
Iter: 1023 loss: 3.64396965e-06
Iter: 1024 loss: 3.64008474e-06
Iter: 1025 loss: 3.63971412e-06
Iter: 1026 loss: 3.63954541e-06
Iter: 1027 loss: 3.63933987e-06
Iter: 1028 loss: 3.63870049e-06
Iter: 1029 loss: 3.63954086e-06
Iter: 1030 loss: 3.63836944e-06
Iter: 1031 loss: 3.63783715e-06
Iter: 1032 loss: 3.63785239e-06
Iter: 1033 loss: 3.63742e-06
Iter: 1034 loss: 3.63654931e-06
Iter: 1035 loss: 3.65058622e-06
Iter: 1036 loss: 3.6364836e-06
Iter: 1037 loss: 3.63594427e-06
Iter: 1038 loss: 3.63592108e-06
Iter: 1039 loss: 3.63541426e-06
Iter: 1040 loss: 3.63635968e-06
Iter: 1041 loss: 3.63516119e-06
Iter: 1042 loss: 3.63472304e-06
Iter: 1043 loss: 3.63424692e-06
Iter: 1044 loss: 3.63416643e-06
Iter: 1045 loss: 3.6337874e-06
Iter: 1046 loss: 3.63374465e-06
Iter: 1047 loss: 3.63342588e-06
Iter: 1048 loss: 3.63263462e-06
Iter: 1049 loss: 3.64120115e-06
Iter: 1050 loss: 3.63254958e-06
Iter: 1051 loss: 3.63172171e-06
Iter: 1052 loss: 3.63497838e-06
Iter: 1053 loss: 3.63156687e-06
Iter: 1054 loss: 3.63096251e-06
Iter: 1055 loss: 3.63093841e-06
Iter: 1056 loss: 3.63057961e-06
Iter: 1057 loss: 3.63005211e-06
Iter: 1058 loss: 3.63005211e-06
Iter: 1059 loss: 3.62929677e-06
Iter: 1060 loss: 3.63194567e-06
Iter: 1061 loss: 3.6291467e-06
Iter: 1062 loss: 3.62869469e-06
Iter: 1063 loss: 3.63502841e-06
Iter: 1064 loss: 3.62866922e-06
Iter: 1065 loss: 3.62827268e-06
Iter: 1066 loss: 3.62781884e-06
Iter: 1067 loss: 3.62772471e-06
Iter: 1068 loss: 3.62719084e-06
Iter: 1069 loss: 3.62920946e-06
Iter: 1070 loss: 3.62696937e-06
Iter: 1071 loss: 3.62636592e-06
Iter: 1072 loss: 3.63155095e-06
Iter: 1073 loss: 3.62636229e-06
Iter: 1074 loss: 3.62598053e-06
Iter: 1075 loss: 3.62516494e-06
Iter: 1076 loss: 3.63711888e-06
Iter: 1077 loss: 3.62514584e-06
Iter: 1078 loss: 3.62490164e-06
Iter: 1079 loss: 3.62472974e-06
Iter: 1080 loss: 3.62435867e-06
Iter: 1081 loss: 3.62367155e-06
Iter: 1082 loss: 3.63625713e-06
Iter: 1083 loss: 3.62366563e-06
Iter: 1084 loss: 3.62300648e-06
Iter: 1085 loss: 3.62383093e-06
Iter: 1086 loss: 3.6226495e-06
Iter: 1087 loss: 3.6224269e-06
Iter: 1088 loss: 3.62219862e-06
Iter: 1089 loss: 3.62187848e-06
Iter: 1090 loss: 3.62126411e-06
Iter: 1091 loss: 3.63502204e-06
Iter: 1092 loss: 3.62125456e-06
Iter: 1093 loss: 3.62063111e-06
Iter: 1094 loss: 3.62382525e-06
Iter: 1095 loss: 3.62050241e-06
Iter: 1096 loss: 3.6200081e-06
Iter: 1097 loss: 3.62352853e-06
Iter: 1098 loss: 3.61995194e-06
Iter: 1099 loss: 3.61948855e-06
Iter: 1100 loss: 3.62037281e-06
Iter: 1101 loss: 3.61930074e-06
Iter: 1102 loss: 3.61890466e-06
Iter: 1103 loss: 3.61859838e-06
Iter: 1104 loss: 3.61849061e-06
Iter: 1105 loss: 3.61804496e-06
Iter: 1106 loss: 3.61804041e-06
Iter: 1107 loss: 3.61777393e-06
Iter: 1108 loss: 3.61710113e-06
Iter: 1109 loss: 3.62347373e-06
Iter: 1110 loss: 3.61701495e-06
Iter: 1111 loss: 3.61665661e-06
Iter: 1112 loss: 3.61658749e-06
Iter: 1113 loss: 3.6161955e-06
Iter: 1114 loss: 3.61561342e-06
Iter: 1115 loss: 3.61558386e-06
Iter: 1116 loss: 3.61500315e-06
Iter: 1117 loss: 3.61507e-06
Iter: 1118 loss: 3.61457319e-06
Iter: 1119 loss: 3.61430079e-06
Iter: 1120 loss: 3.61419643e-06
Iter: 1121 loss: 3.61373668e-06
Iter: 1122 loss: 3.6136812e-06
Iter: 1123 loss: 3.61342609e-06
Iter: 1124 loss: 3.61304069e-06
Iter: 1125 loss: 3.61345542e-06
Iter: 1126 loss: 3.61286379e-06
Iter: 1127 loss: 3.61238608e-06
Iter: 1128 loss: 3.61527054e-06
Iter: 1129 loss: 3.61232719e-06
Iter: 1130 loss: 3.61184584e-06
Iter: 1131 loss: 3.61321054e-06
Iter: 1132 loss: 3.61168509e-06
Iter: 1133 loss: 3.61134744e-06
Iter: 1134 loss: 3.61070397e-06
Iter: 1135 loss: 3.61067168e-06
Iter: 1136 loss: 3.61041566e-06
Iter: 1137 loss: 3.61029743e-06
Iter: 1138 loss: 3.60998138e-06
Iter: 1139 loss: 3.60933836e-06
Iter: 1140 loss: 3.61998309e-06
Iter: 1141 loss: 3.60928516e-06
Iter: 1142 loss: 3.60885315e-06
Iter: 1143 loss: 3.61537923e-06
Iter: 1144 loss: 3.60885815e-06
Iter: 1145 loss: 3.60839203e-06
Iter: 1146 loss: 3.60906256e-06
Iter: 1147 loss: 3.60817398e-06
Iter: 1148 loss: 3.60778381e-06
Iter: 1149 loss: 3.60725289e-06
Iter: 1150 loss: 3.60722652e-06
Iter: 1151 loss: 3.60655122e-06
Iter: 1152 loss: 3.61063712e-06
Iter: 1153 loss: 3.60650256e-06
Iter: 1154 loss: 3.60570789e-06
Iter: 1155 loss: 3.61099728e-06
Iter: 1156 loss: 3.6056415e-06
Iter: 1157 loss: 3.60528907e-06
Iter: 1158 loss: 3.6048591e-06
Iter: 1159 loss: 3.60483818e-06
Iter: 1160 loss: 3.60429067e-06
Iter: 1161 loss: 3.61029925e-06
Iter: 1162 loss: 3.60425361e-06
Iter: 1163 loss: 3.60387298e-06
Iter: 1164 loss: 3.60564195e-06
Iter: 1165 loss: 3.6037834e-06
Iter: 1166 loss: 3.60344802e-06
Iter: 1167 loss: 3.60315471e-06
Iter: 1168 loss: 3.60305557e-06
Iter: 1169 loss: 3.6027227e-06
Iter: 1170 loss: 3.60756553e-06
Iter: 1171 loss: 3.60273953e-06
Iter: 1172 loss: 3.60233685e-06
Iter: 1173 loss: 3.60200556e-06
Iter: 1174 loss: 3.60192848e-06
Iter: 1175 loss: 3.60150329e-06
Iter: 1176 loss: 3.60200329e-06
Iter: 1177 loss: 3.60126091e-06
Iter: 1178 loss: 3.60072431e-06
Iter: 1179 loss: 3.60636295e-06
Iter: 1180 loss: 3.60070021e-06
Iter: 1181 loss: 3.60034937e-06
Iter: 1182 loss: 3.59962905e-06
Iter: 1183 loss: 3.61094271e-06
Iter: 1184 loss: 3.59962269e-06
Iter: 1185 loss: 3.59893284e-06
Iter: 1186 loss: 3.60051877e-06
Iter: 1187 loss: 3.59866613e-06
Iter: 1188 loss: 3.59884075e-06
Iter: 1189 loss: 3.59836963e-06
Iter: 1190 loss: 3.59817932e-06
Iter: 1191 loss: 3.59769433e-06
Iter: 1192 loss: 3.60063314e-06
Iter: 1193 loss: 3.59756677e-06
Iter: 1194 loss: 3.59703927e-06
Iter: 1195 loss: 3.6012284e-06
Iter: 1196 loss: 3.59700243e-06
Iter: 1197 loss: 3.59650812e-06
Iter: 1198 loss: 3.60010381e-06
Iter: 1199 loss: 3.59648811e-06
Iter: 1200 loss: 3.59617798e-06
Iter: 1201 loss: 3.59632759e-06
Iter: 1202 loss: 3.59592e-06
Iter: 1203 loss: 3.59557362e-06
Iter: 1204 loss: 3.59587602e-06
Iter: 1205 loss: 3.59535716e-06
Iter: 1206 loss: 3.59486626e-06
Iter: 1207 loss: 3.59948854e-06
Iter: 1208 loss: 3.59484193e-06
Iter: 1209 loss: 3.59461e-06
Iter: 1210 loss: 3.59418095e-06
Iter: 1211 loss: 3.60241029e-06
Iter: 1212 loss: 3.59416936e-06
Iter: 1213 loss: 3.59385649e-06
Iter: 1214 loss: 3.59381556e-06
Iter: 1215 loss: 3.59352703e-06
Iter: 1216 loss: 3.59286628e-06
Iter: 1217 loss: 3.60287731e-06
Iter: 1218 loss: 3.5928681e-06
Iter: 1219 loss: 3.59217984e-06
Iter: 1220 loss: 3.59257638e-06
Iter: 1221 loss: 3.5917783e-06
Iter: 1222 loss: 3.59182309e-06
Iter: 1223 loss: 3.59148044e-06
Iter: 1224 loss: 3.59114438e-06
Iter: 1225 loss: 3.5907151e-06
Iter: 1226 loss: 3.59072101e-06
Iter: 1227 loss: 3.59025125e-06
Iter: 1228 loss: 3.59077762e-06
Iter: 1229 loss: 3.59003639e-06
Iter: 1230 loss: 3.58969191e-06
Iter: 1231 loss: 3.58968418e-06
Iter: 1232 loss: 3.58939815e-06
Iter: 1233 loss: 3.58917487e-06
Iter: 1234 loss: 3.58906345e-06
Iter: 1235 loss: 3.58859461e-06
Iter: 1236 loss: 3.58886359e-06
Iter: 1237 loss: 3.58829129e-06
Iter: 1238 loss: 3.58795978e-06
Iter: 1239 loss: 3.58796137e-06
Iter: 1240 loss: 3.58769512e-06
Iter: 1241 loss: 3.58715943e-06
Iter: 1242 loss: 3.59553519e-06
Iter: 1243 loss: 3.58713851e-06
Iter: 1244 loss: 3.58670923e-06
Iter: 1245 loss: 3.59051592e-06
Iter: 1246 loss: 3.58667262e-06
Iter: 1247 loss: 3.5862347e-06
Iter: 1248 loss: 3.58836269e-06
Iter: 1249 loss: 3.5861508e-06
Iter: 1250 loss: 3.58592388e-06
Iter: 1251 loss: 3.58534498e-06
Iter: 1252 loss: 3.59232422e-06
Iter: 1253 loss: 3.5853036e-06
Iter: 1254 loss: 3.58464558e-06
Iter: 1255 loss: 3.58602801e-06
Iter: 1256 loss: 3.58436364e-06
Iter: 1257 loss: 3.58429361e-06
Iter: 1258 loss: 3.58402394e-06
Iter: 1259 loss: 3.58374496e-06
Iter: 1260 loss: 3.58315287e-06
Iter: 1261 loss: 3.58860689e-06
Iter: 1262 loss: 3.58309035e-06
Iter: 1263 loss: 3.58264742e-06
Iter: 1264 loss: 3.58842499e-06
Iter: 1265 loss: 3.58265834e-06
Iter: 1266 loss: 3.58227817e-06
Iter: 1267 loss: 3.58352054e-06
Iter: 1268 loss: 3.58214584e-06
Iter: 1269 loss: 3.58188549e-06
Iter: 1270 loss: 3.58221587e-06
Iter: 1271 loss: 3.58174384e-06
Iter: 1272 loss: 3.58144825e-06
Iter: 1273 loss: 3.5824221e-06
Iter: 1274 loss: 3.58134048e-06
Iter: 1275 loss: 3.58099442e-06
Iter: 1276 loss: 3.58233524e-06
Iter: 1277 loss: 3.58089733e-06
Iter: 1278 loss: 3.58056332e-06
Iter: 1279 loss: 3.58012039e-06
Iter: 1280 loss: 3.58010038e-06
Iter: 1281 loss: 3.57980343e-06
Iter: 1282 loss: 3.57977433e-06
Iter: 1283 loss: 3.57944737e-06
Iter: 1284 loss: 3.57888689e-06
Iter: 1285 loss: 3.57890781e-06
Iter: 1286 loss: 3.57843192e-06
Iter: 1287 loss: 3.57880185e-06
Iter: 1288 loss: 3.57818658e-06
Iter: 1289 loss: 3.57790304e-06
Iter: 1290 loss: 3.57787212e-06
Iter: 1291 loss: 3.57753242e-06
Iter: 1292 loss: 3.57793647e-06
Iter: 1293 loss: 3.57737781e-06
Iter: 1294 loss: 3.57713452e-06
Iter: 1295 loss: 3.57660929e-06
Iter: 1296 loss: 3.5853368e-06
Iter: 1297 loss: 3.5765861e-06
Iter: 1298 loss: 3.57633917e-06
Iter: 1299 loss: 3.57622389e-06
Iter: 1300 loss: 3.57592421e-06
Iter: 1301 loss: 3.57556655e-06
Iter: 1302 loss: 3.57552426e-06
Iter: 1303 loss: 3.57513682e-06
Iter: 1304 loss: 3.57737758e-06
Iter: 1305 loss: 3.57510021e-06
Iter: 1306 loss: 3.57482759e-06
Iter: 1307 loss: 3.57785893e-06
Iter: 1308 loss: 3.57480303e-06
Iter: 1309 loss: 3.57456906e-06
Iter: 1310 loss: 3.57443651e-06
Iter: 1311 loss: 3.57433692e-06
Iter: 1312 loss: 3.57406179e-06
Iter: 1313 loss: 3.57422914e-06
Iter: 1314 loss: 3.57385852e-06
Iter: 1315 loss: 3.57348586e-06
Iter: 1316 loss: 3.57853423e-06
Iter: 1317 loss: 3.57347403e-06
Iter: 1318 loss: 3.57323302e-06
Iter: 1319 loss: 3.57262502e-06
Iter: 1320 loss: 3.57766748e-06
Iter: 1321 loss: 3.57250156e-06
Iter: 1322 loss: 3.57187e-06
Iter: 1323 loss: 3.57430372e-06
Iter: 1324 loss: 3.5717535e-06
Iter: 1325 loss: 3.57170484e-06
Iter: 1326 loss: 3.57153044e-06
Iter: 1327 loss: 3.57125782e-06
Iter: 1328 loss: 3.57088948e-06
Iter: 1329 loss: 3.5709204e-06
Iter: 1330 loss: 3.57062254e-06
Iter: 1331 loss: 3.57111639e-06
Iter: 1332 loss: 3.5705134e-06
Iter: 1333 loss: 3.5702692e-06
Iter: 1334 loss: 3.57025897e-06
Iter: 1335 loss: 3.57005797e-06
Iter: 1336 loss: 3.56954797e-06
Iter: 1337 loss: 3.57331396e-06
Iter: 1338 loss: 3.56941428e-06
Iter: 1339 loss: 3.56914325e-06
Iter: 1340 loss: 3.56911096e-06
Iter: 1341 loss: 3.56878718e-06
Iter: 1342 loss: 3.56954888e-06
Iter: 1343 loss: 3.56867e-06
Iter: 1344 loss: 3.56843634e-06
Iter: 1345 loss: 3.56867827e-06
Iter: 1346 loss: 3.5682649e-06
Iter: 1347 loss: 3.5679891e-06
Iter: 1348 loss: 3.5685182e-06
Iter: 1349 loss: 3.56788314e-06
Iter: 1350 loss: 3.56752571e-06
Iter: 1351 loss: 3.56946134e-06
Iter: 1352 loss: 3.56746682e-06
Iter: 1353 loss: 3.56725013e-06
Iter: 1354 loss: 3.56683313e-06
Iter: 1355 loss: 3.57513181e-06
Iter: 1356 loss: 3.56682222e-06
Iter: 1357 loss: 3.5664134e-06
Iter: 1358 loss: 3.5713872e-06
Iter: 1359 loss: 3.56641067e-06
Iter: 1360 loss: 3.56605051e-06
Iter: 1361 loss: 3.56839905e-06
Iter: 1362 loss: 3.56602959e-06
Iter: 1363 loss: 3.5658386e-06
Iter: 1364 loss: 3.56549435e-06
Iter: 1365 loss: 3.5721821e-06
Iter: 1366 loss: 3.56549936e-06
Iter: 1367 loss: 3.56524401e-06
Iter: 1368 loss: 3.56523833e-06
Iter: 1369 loss: 3.56493501e-06
Iter: 1370 loss: 3.56492274e-06
Iter: 1371 loss: 3.56474402e-06
Iter: 1372 loss: 3.56440614e-06
Iter: 1373 loss: 3.56410919e-06
Iter: 1374 loss: 3.5640403e-06
Iter: 1375 loss: 3.56376e-06
Iter: 1376 loss: 3.56371265e-06
Iter: 1377 loss: 3.56346891e-06
Iter: 1378 loss: 3.56319379e-06
Iter: 1379 loss: 3.56318151e-06
Iter: 1380 loss: 3.56277e-06
Iter: 1381 loss: 3.5639041e-06
Iter: 1382 loss: 3.56263e-06
Iter: 1383 loss: 3.56236433e-06
Iter: 1384 loss: 3.56555756e-06
Iter: 1385 loss: 3.56235751e-06
Iter: 1386 loss: 3.56213377e-06
Iter: 1387 loss: 3.56197597e-06
Iter: 1388 loss: 3.56190094e-06
Iter: 1389 loss: 3.56162218e-06
Iter: 1390 loss: 3.56122928e-06
Iter: 1391 loss: 3.56119062e-06
Iter: 1392 loss: 3.56111696e-06
Iter: 1393 loss: 3.56091368e-06
Iter: 1394 loss: 3.56066039e-06
Iter: 1395 loss: 3.56016972e-06
Iter: 1396 loss: 3.57082354e-06
Iter: 1397 loss: 3.56016881e-06
Iter: 1398 loss: 3.55971179e-06
Iter: 1399 loss: 3.56016835e-06
Iter: 1400 loss: 3.55942529e-06
Iter: 1401 loss: 3.55913198e-06
Iter: 1402 loss: 3.5590906e-06
Iter: 1403 loss: 3.5588937e-06
Iter: 1404 loss: 3.55844986e-06
Iter: 1405 loss: 3.56219198e-06
Iter: 1406 loss: 3.55831571e-06
Iter: 1407 loss: 3.55803832e-06
Iter: 1408 loss: 3.55805264e-06
Iter: 1409 loss: 3.55771544e-06
Iter: 1410 loss: 3.55784505e-06
Iter: 1411 loss: 3.55748489e-06
Iter: 1412 loss: 3.55712245e-06
Iter: 1413 loss: 3.55747579e-06
Iter: 1414 loss: 3.55686825e-06
Iter: 1415 loss: 3.55642601e-06
Iter: 1416 loss: 3.55993598e-06
Iter: 1417 loss: 3.55643488e-06
Iter: 1418 loss: 3.55602879e-06
Iter: 1419 loss: 3.55702923e-06
Iter: 1420 loss: 3.55591055e-06
Iter: 1421 loss: 3.55556585e-06
Iter: 1422 loss: 3.55504767e-06
Iter: 1423 loss: 3.55502016e-06
Iter: 1424 loss: 3.55465136e-06
Iter: 1425 loss: 3.55975385e-06
Iter: 1426 loss: 3.55462771e-06
Iter: 1427 loss: 3.55418797e-06
Iter: 1428 loss: 3.55619568e-06
Iter: 1429 loss: 3.55414386e-06
Iter: 1430 loss: 3.55388192e-06
Iter: 1431 loss: 3.55334373e-06
Iter: 1432 loss: 3.56095165e-06
Iter: 1433 loss: 3.55330531e-06
Iter: 1434 loss: 3.55313682e-06
Iter: 1435 loss: 3.55302018e-06
Iter: 1436 loss: 3.55273096e-06
Iter: 1437 loss: 3.5520975e-06
Iter: 1438 loss: 3.56339297e-06
Iter: 1439 loss: 3.55206657e-06
Iter: 1440 loss: 3.55155044e-06
Iter: 1441 loss: 3.55485713e-06
Iter: 1442 loss: 3.55144675e-06
Iter: 1443 loss: 3.55103634e-06
Iter: 1444 loss: 3.55614407e-06
Iter: 1445 loss: 3.55104385e-06
Iter: 1446 loss: 3.55077054e-06
Iter: 1447 loss: 3.55030556e-06
Iter: 1448 loss: 3.56201372e-06
Iter: 1449 loss: 3.55029533e-06
Iter: 1450 loss: 3.54976555e-06
Iter: 1451 loss: 3.55424027e-06
Iter: 1452 loss: 3.54973417e-06
Iter: 1453 loss: 3.54937856e-06
Iter: 1454 loss: 3.5517146e-06
Iter: 1455 loss: 3.54937788e-06
Iter: 1456 loss: 3.54900681e-06
Iter: 1457 loss: 3.54831445e-06
Iter: 1458 loss: 3.56341388e-06
Iter: 1459 loss: 3.54828921e-06
Iter: 1460 loss: 3.54767826e-06
Iter: 1461 loss: 3.55002408e-06
Iter: 1462 loss: 3.54754593e-06
Iter: 1463 loss: 3.54718895e-06
Iter: 1464 loss: 3.54713256e-06
Iter: 1465 loss: 3.54690133e-06
Iter: 1466 loss: 3.54631698e-06
Iter: 1467 loss: 3.55488419e-06
Iter: 1468 loss: 3.54631106e-06
Iter: 1469 loss: 3.54585063e-06
Iter: 1470 loss: 3.5485109e-06
Iter: 1471 loss: 3.54573945e-06
Iter: 1472 loss: 3.54529197e-06
Iter: 1473 loss: 3.5485109e-06
Iter: 1474 loss: 3.54526537e-06
Iter: 1475 loss: 3.54501117e-06
Iter: 1476 loss: 3.54447047e-06
Iter: 1477 loss: 3.55170687e-06
Iter: 1478 loss: 3.54441681e-06
Iter: 1479 loss: 3.54392705e-06
Iter: 1480 loss: 3.54390659e-06
Iter: 1481 loss: 3.54344684e-06
Iter: 1482 loss: 3.54314e-06
Iter: 1483 loss: 3.54296139e-06
Iter: 1484 loss: 3.54249573e-06
Iter: 1485 loss: 3.54485564e-06
Iter: 1486 loss: 3.54240274e-06
Iter: 1487 loss: 3.54199983e-06
Iter: 1488 loss: 3.54450503e-06
Iter: 1489 loss: 3.54197755e-06
Iter: 1490 loss: 3.54163967e-06
Iter: 1491 loss: 3.54208464e-06
Iter: 1492 loss: 3.54146778e-06
Iter: 1493 loss: 3.54117606e-06
Iter: 1494 loss: 3.54092185e-06
Iter: 1495 loss: 3.54084295e-06
Iter: 1496 loss: 3.54052304e-06
Iter: 1497 loss: 3.5404787e-06
Iter: 1498 loss: 3.54015265e-06
Iter: 1499 loss: 3.53958058e-06
Iter: 1500 loss: 3.53958671e-06
Iter: 1501 loss: 3.53908e-06
Iter: 1502 loss: 3.53922906e-06
Iter: 1503 loss: 3.5386679e-06
Iter: 1504 loss: 3.53851919e-06
Iter: 1505 loss: 3.53832229e-06
Iter: 1506 loss: 3.53809401e-06
Iter: 1507 loss: 3.53752648e-06
Iter: 1508 loss: 3.54382246e-06
Iter: 1509 loss: 3.53746145e-06
Iter: 1510 loss: 3.5371072e-06
Iter: 1511 loss: 3.53708583e-06
Iter: 1512 loss: 3.53673408e-06
Iter: 1513 loss: 3.53698124e-06
Iter: 1514 loss: 3.53653127e-06
Iter: 1515 loss: 3.53619112e-06
Iter: 1516 loss: 3.53623045e-06
Iter: 1517 loss: 3.53595055e-06
Iter: 1518 loss: 3.53539804e-06
Iter: 1519 loss: 3.53712062e-06
Iter: 1520 loss: 3.53518863e-06
Iter: 1521 loss: 3.53467772e-06
Iter: 1522 loss: 3.53696578e-06
Iter: 1523 loss: 3.53462246e-06
Iter: 1524 loss: 3.53417363e-06
Iter: 1525 loss: 3.53388191e-06
Iter: 1526 loss: 3.53377891e-06
Iter: 1527 loss: 3.53332689e-06
Iter: 1528 loss: 3.5333419e-06
Iter: 1529 loss: 3.53293444e-06
Iter: 1530 loss: 3.53348833e-06
Iter: 1531 loss: 3.53273322e-06
Iter: 1532 loss: 3.53248583e-06
Iter: 1533 loss: 3.53189898e-06
Iter: 1534 loss: 3.54304257e-06
Iter: 1535 loss: 3.53187488e-06
Iter: 1536 loss: 3.53167434e-06
Iter: 1537 loss: 3.53155201e-06
Iter: 1538 loss: 3.53120959e-06
Iter: 1539 loss: 3.53063933e-06
Iter: 1540 loss: 3.53063251e-06
Iter: 1541 loss: 3.53010364e-06
Iter: 1542 loss: 3.53271844e-06
Iter: 1543 loss: 3.53004225e-06
Iter: 1544 loss: 3.52955112e-06
Iter: 1545 loss: 3.53347559e-06
Iter: 1546 loss: 3.52950383e-06
Iter: 1547 loss: 3.5292162e-06
Iter: 1548 loss: 3.52889629e-06
Iter: 1549 loss: 3.52887173e-06
Iter: 1550 loss: 3.52838288e-06
Iter: 1551 loss: 3.53284531e-06
Iter: 1552 loss: 3.52835468e-06
Iter: 1553 loss: 3.5280807e-06
Iter: 1554 loss: 3.52885013e-06
Iter: 1555 loss: 3.5279545e-06
Iter: 1556 loss: 3.52753727e-06
Iter: 1557 loss: 3.52726602e-06
Iter: 1558 loss: 3.52714142e-06
Iter: 1559 loss: 3.5267e-06
Iter: 1560 loss: 3.52971529e-06
Iter: 1561 loss: 3.52662846e-06
Iter: 1562 loss: 3.52611187e-06
Iter: 1563 loss: 3.52957886e-06
Iter: 1564 loss: 3.52603865e-06
Iter: 1565 loss: 3.52573966e-06
Iter: 1566 loss: 3.52516827e-06
Iter: 1567 loss: 3.53522569e-06
Iter: 1568 loss: 3.52518714e-06
Iter: 1569 loss: 3.52465167e-06
Iter: 1570 loss: 3.5294961e-06
Iter: 1571 loss: 3.52462598e-06
Iter: 1572 loss: 3.52412508e-06
Iter: 1573 loss: 3.52700863e-06
Iter: 1574 loss: 3.5240289e-06
Iter: 1575 loss: 3.52371808e-06
Iter: 1576 loss: 3.52313964e-06
Iter: 1577 loss: 3.53441737e-06
Iter: 1578 loss: 3.52315533e-06
Iter: 1579 loss: 3.52273196e-06
Iter: 1580 loss: 3.52265397e-06
Iter: 1581 loss: 3.52235884e-06
Iter: 1582 loss: 3.52182883e-06
Iter: 1583 loss: 3.53417704e-06
Iter: 1584 loss: 3.52181655e-06
Iter: 1585 loss: 3.52131747e-06
Iter: 1586 loss: 3.52835559e-06
Iter: 1587 loss: 3.52131156e-06
Iter: 1588 loss: 3.52094435e-06
Iter: 1589 loss: 3.52146435e-06
Iter: 1590 loss: 3.52078087e-06
Iter: 1591 loss: 3.5203177e-06
Iter: 1592 loss: 3.52075017e-06
Iter: 1593 loss: 3.52009647e-06
Iter: 1594 loss: 3.51962717e-06
Iter: 1595 loss: 3.52030338e-06
Iter: 1596 loss: 3.51946846e-06
Iter: 1597 loss: 3.51922586e-06
Iter: 1598 loss: 3.51913241e-06
Iter: 1599 loss: 3.51896438e-06
Iter: 1600 loss: 3.51836093e-06
Iter: 1601 loss: 3.52146026e-06
Iter: 1602 loss: 3.51824269e-06
Iter: 1603 loss: 3.51757785e-06
Iter: 1604 loss: 3.52018287e-06
Iter: 1605 loss: 3.51739368e-06
Iter: 1606 loss: 3.51708422e-06
Iter: 1607 loss: 3.51702e-06
Iter: 1608 loss: 3.51677386e-06
Iter: 1609 loss: 3.51627227e-06
Iter: 1610 loss: 3.52372717e-06
Iter: 1611 loss: 3.51627114e-06
Iter: 1612 loss: 3.51601193e-06
Iter: 1613 loss: 3.515956e-06
Iter: 1614 loss: 3.51566086e-06
Iter: 1615 loss: 3.51515496e-06
Iter: 1616 loss: 3.51514086e-06
Iter: 1617 loss: 3.51469271e-06
Iter: 1618 loss: 3.51730091e-06
Iter: 1619 loss: 3.51460812e-06
Iter: 1620 loss: 3.51407471e-06
Iter: 1621 loss: 3.5160574e-06
Iter: 1622 loss: 3.51397e-06
Iter: 1623 loss: 3.51357153e-06
Iter: 1624 loss: 3.51407971e-06
Iter: 1625 loss: 3.51334529e-06
Iter: 1626 loss: 3.51281e-06
Iter: 1627 loss: 3.51269387e-06
Iter: 1628 loss: 3.51237395e-06
Iter: 1629 loss: 3.51223753e-06
Iter: 1630 loss: 3.51207518e-06
Iter: 1631 loss: 3.51178915e-06
Iter: 1632 loss: 3.51143e-06
Iter: 1633 loss: 3.51140307e-06
Iter: 1634 loss: 3.51101289e-06
Iter: 1635 loss: 3.51083759e-06
Iter: 1636 loss: 3.51060044e-06
Iter: 1637 loss: 3.51035715e-06
Iter: 1638 loss: 3.51032054e-06
Iter: 1639 loss: 3.50992264e-06
Iter: 1640 loss: 3.50943537e-06
Iter: 1641 loss: 3.50939899e-06
Iter: 1642 loss: 3.50896835e-06
Iter: 1643 loss: 3.51086055e-06
Iter: 1644 loss: 3.50891924e-06
Iter: 1645 loss: 3.50842538e-06
Iter: 1646 loss: 3.50961727e-06
Iter: 1647 loss: 3.50821915e-06
Iter: 1648 loss: 3.50787013e-06
Iter: 1649 loss: 3.50813434e-06
Iter: 1650 loss: 3.50767323e-06
Iter: 1651 loss: 3.50731375e-06
Iter: 1652 loss: 3.51119184e-06
Iter: 1653 loss: 3.50731239e-06
Iter: 1654 loss: 3.50705932e-06
Iter: 1655 loss: 3.5067676e-06
Iter: 1656 loss: 3.50672622e-06
Iter: 1657 loss: 3.5061878e-06
Iter: 1658 loss: 3.50785353e-06
Iter: 1659 loss: 3.50602159e-06
Iter: 1660 loss: 3.5055607e-06
Iter: 1661 loss: 3.50585583e-06
Iter: 1662 loss: 3.50529513e-06
Iter: 1663 loss: 3.50478058e-06
Iter: 1664 loss: 3.50478217e-06
Iter: 1665 loss: 3.50454502e-06
Iter: 1666 loss: 3.50398659e-06
Iter: 1667 loss: 3.50911887e-06
Iter: 1668 loss: 3.50390519e-06
Iter: 1669 loss: 3.50333448e-06
Iter: 1670 loss: 3.5037981e-06
Iter: 1671 loss: 3.50300979e-06
Iter: 1672 loss: 3.50348546e-06
Iter: 1673 loss: 3.5028213e-06
Iter: 1674 loss: 3.50273103e-06
Iter: 1675 loss: 3.50231448e-06
Iter: 1676 loss: 3.5026037e-06
Iter: 1677 loss: 3.50200116e-06
Iter: 1678 loss: 3.50160667e-06
Iter: 1679 loss: 3.50160599e-06
Iter: 1680 loss: 3.50118353e-06
Iter: 1681 loss: 3.50281039e-06
Iter: 1682 loss: 3.50109349e-06
Iter: 1683 loss: 3.50079767e-06
Iter: 1684 loss: 3.50019309e-06
Iter: 1685 loss: 3.50988012e-06
Iter: 1686 loss: 3.50018308e-06
Iter: 1687 loss: 3.5002538e-06
Iter: 1688 loss: 3.4999116e-06
Iter: 1689 loss: 3.49972379e-06
Iter: 1690 loss: 3.49930497e-06
Iter: 1691 loss: 3.50332698e-06
Iter: 1692 loss: 3.49925449e-06
Iter: 1693 loss: 3.49882976e-06
Iter: 1694 loss: 3.50291521e-06
Iter: 1695 loss: 3.49880247e-06
Iter: 1696 loss: 3.49850598e-06
Iter: 1697 loss: 3.49831748e-06
Iter: 1698 loss: 3.49821471e-06
Iter: 1699 loss: 3.4977279e-06
Iter: 1700 loss: 3.50461e-06
Iter: 1701 loss: 3.49772904e-06
Iter: 1702 loss: 3.49751508e-06
Iter: 1703 loss: 3.49704123e-06
Iter: 1704 loss: 3.50520668e-06
Iter: 1705 loss: 3.49704897e-06
Iter: 1706 loss: 3.49668289e-06
Iter: 1707 loss: 3.49841093e-06
Iter: 1708 loss: 3.49660877e-06
Iter: 1709 loss: 3.49620314e-06
Iter: 1710 loss: 3.49986726e-06
Iter: 1711 loss: 3.49620291e-06
Iter: 1712 loss: 3.49601419e-06
Iter: 1713 loss: 3.49557945e-06
Iter: 1714 loss: 3.50415758e-06
Iter: 1715 loss: 3.49558377e-06
Iter: 1716 loss: 3.49519541e-06
Iter: 1717 loss: 3.50042774e-06
Iter: 1718 loss: 3.49520496e-06
Iter: 1719 loss: 3.49479933e-06
Iter: 1720 loss: 3.49477136e-06
Iter: 1721 loss: 3.49445963e-06
Iter: 1722 loss: 3.49402308e-06
Iter: 1723 loss: 3.49510492e-06
Iter: 1724 loss: 3.49388074e-06
Iter: 1725 loss: 3.49339643e-06
Iter: 1726 loss: 3.49541847e-06
Iter: 1727 loss: 3.49332413e-06
Iter: 1728 loss: 3.49302286e-06
Iter: 1729 loss: 3.49328366e-06
Iter: 1730 loss: 3.49289439e-06
Iter: 1731 loss: 3.49254606e-06
Iter: 1732 loss: 3.49344282e-06
Iter: 1733 loss: 3.49241122e-06
Iter: 1734 loss: 3.49213246e-06
Iter: 1735 loss: 3.49361585e-06
Iter: 1736 loss: 3.49203287e-06
Iter: 1737 loss: 3.49168045e-06
Iter: 1738 loss: 3.4912141e-06
Iter: 1739 loss: 3.49117e-06
Iter: 1740 loss: 3.4906202e-06
Iter: 1741 loss: 3.4906941e-06
Iter: 1742 loss: 3.49020888e-06
Iter: 1743 loss: 3.49006973e-06
Iter: 1744 loss: 3.48992535e-06
Iter: 1745 loss: 3.4895761e-06
Iter: 1746 loss: 3.48919411e-06
Iter: 1747 loss: 3.48915637e-06
Iter: 1748 loss: 3.48875938e-06
Iter: 1749 loss: 3.48926233e-06
Iter: 1750 loss: 3.48858612e-06
Iter: 1751 loss: 3.48837e-06
Iter: 1752 loss: 3.48830645e-06
Iter: 1753 loss: 3.48812569e-06
Iter: 1754 loss: 3.48757862e-06
Iter: 1755 loss: 3.49233346e-06
Iter: 1756 loss: 3.48754679e-06
Iter: 1757 loss: 3.48721983e-06
Iter: 1758 loss: 3.48717799e-06
Iter: 1759 loss: 3.4868508e-06
Iter: 1760 loss: 3.48647518e-06
Iter: 1761 loss: 3.48642061e-06
Iter: 1762 loss: 3.48604863e-06
Iter: 1763 loss: 3.4880677e-06
Iter: 1764 loss: 3.48597951e-06
Iter: 1765 loss: 3.48565254e-06
Iter: 1766 loss: 3.48965978e-06
Iter: 1767 loss: 3.48564868e-06
Iter: 1768 loss: 3.48540584e-06
Iter: 1769 loss: 3.48575622e-06
Iter: 1770 loss: 3.48527647e-06
Iter: 1771 loss: 3.48507274e-06
Iter: 1772 loss: 3.48462618e-06
Iter: 1773 loss: 3.49166271e-06
Iter: 1774 loss: 3.48457024e-06
Iter: 1775 loss: 3.48408798e-06
Iter: 1776 loss: 3.48943809e-06
Iter: 1777 loss: 3.48407139e-06
Iter: 1778 loss: 3.48356593e-06
Iter: 1779 loss: 3.48500907e-06
Iter: 1780 loss: 3.48338654e-06
Iter: 1781 loss: 3.48308e-06
Iter: 1782 loss: 3.48274421e-06
Iter: 1783 loss: 3.48270942e-06
Iter: 1784 loss: 3.4824352e-06
Iter: 1785 loss: 3.48243225e-06
Iter: 1786 loss: 3.48215326e-06
Iter: 1787 loss: 3.48209483e-06
Iter: 1788 loss: 3.48195726e-06
Iter: 1789 loss: 3.48172216e-06
Iter: 1790 loss: 3.48223102e-06
Iter: 1791 loss: 3.48163621e-06
Iter: 1792 loss: 3.48130516e-06
Iter: 1793 loss: 3.482279e-06
Iter: 1794 loss: 3.48121898e-06
Iter: 1795 loss: 3.48092908e-06
Iter: 1796 loss: 3.48034155e-06
Iter: 1797 loss: 3.49098036e-06
Iter: 1798 loss: 3.48033268e-06
Iter: 1799 loss: 3.48010553e-06
Iter: 1800 loss: 3.47996593e-06
Iter: 1801 loss: 3.47967057e-06
Iter: 1802 loss: 3.47993796e-06
Iter: 1803 loss: 3.47949435e-06
Iter: 1804 loss: 3.47917239e-06
Iter: 1805 loss: 3.47894047e-06
Iter: 1806 loss: 3.47880768e-06
Iter: 1807 loss: 3.47843616e-06
Iter: 1808 loss: 3.47911691e-06
Iter: 1809 loss: 3.47830246e-06
Iter: 1810 loss: 3.47808032e-06
Iter: 1811 loss: 3.47806531e-06
Iter: 1812 loss: 3.47784112e-06
Iter: 1813 loss: 3.47760192e-06
Iter: 1814 loss: 3.4775378e-06
Iter: 1815 loss: 3.47721038e-06
Iter: 1816 loss: 3.47688933e-06
Iter: 1817 loss: 3.47681907e-06
Iter: 1818 loss: 3.47654395e-06
Iter: 1819 loss: 3.4764571e-06
Iter: 1820 loss: 3.47629475e-06
Iter: 1821 loss: 3.47589503e-06
Iter: 1822 loss: 3.48129333e-06
Iter: 1823 loss: 3.47586183e-06
Iter: 1824 loss: 3.47565629e-06
Iter: 1825 loss: 3.47565538e-06
Iter: 1826 loss: 3.47543755e-06
Iter: 1827 loss: 3.47509945e-06
Iter: 1828 loss: 3.47511696e-06
Iter: 1829 loss: 3.47487662e-06
Iter: 1830 loss: 3.47717878e-06
Iter: 1831 loss: 3.47486821e-06
Iter: 1832 loss: 3.4745633e-06
Iter: 1833 loss: 3.47547711e-06
Iter: 1834 loss: 3.4744935e-06
Iter: 1835 loss: 3.47427067e-06
Iter: 1836 loss: 3.47419791e-06
Iter: 1837 loss: 3.47407013e-06
Iter: 1838 loss: 3.47364494e-06
Iter: 1839 loss: 3.47337073e-06
Iter: 1840 loss: 3.47319315e-06
Iter: 1841 loss: 3.47272248e-06
Iter: 1842 loss: 3.47816831e-06
Iter: 1843 loss: 3.47272521e-06
Iter: 1844 loss: 3.47233845e-06
Iter: 1845 loss: 3.47476453e-06
Iter: 1846 loss: 3.47221703e-06
Iter: 1847 loss: 3.47207447e-06
Iter: 1848 loss: 3.47165587e-06
Iter: 1849 loss: 3.47920923e-06
Iter: 1850 loss: 3.47163314e-06
Iter: 1851 loss: 3.47152763e-06
Iter: 1852 loss: 3.47147079e-06
Iter: 1853 loss: 3.47125388e-06
Iter: 1854 loss: 3.47088235e-06
Iter: 1855 loss: 3.47934247e-06
Iter: 1856 loss: 3.4708919e-06
Iter: 1857 loss: 3.47055175e-06
Iter: 1858 loss: 3.47248215e-06
Iter: 1859 loss: 3.47047876e-06
Iter: 1860 loss: 3.47008063e-06
Iter: 1861 loss: 3.47101286e-06
Iter: 1862 loss: 3.46989327e-06
Iter: 1863 loss: 3.46961815e-06
Iter: 1864 loss: 3.46982915e-06
Iter: 1865 loss: 3.46938577e-06
Iter: 1866 loss: 3.46909337e-06
Iter: 1867 loss: 3.47333707e-06
Iter: 1868 loss: 3.46906745e-06
Iter: 1869 loss: 3.46892057e-06
Iter: 1870 loss: 3.46869501e-06
Iter: 1871 loss: 3.46867e-06
Iter: 1872 loss: 3.46829802e-06
Iter: 1873 loss: 3.46875e-06
Iter: 1874 loss: 3.46810793e-06
Iter: 1875 loss: 3.46773913e-06
Iter: 1876 loss: 3.46792376e-06
Iter: 1877 loss: 3.46747356e-06
Iter: 1878 loss: 3.46719116e-06
Iter: 1879 loss: 3.46717843e-06
Iter: 1880 loss: 3.46692468e-06
Iter: 1881 loss: 3.46635807e-06
Iter: 1882 loss: 3.47116202e-06
Iter: 1883 loss: 3.46625256e-06
Iter: 1884 loss: 3.46572551e-06
Iter: 1885 loss: 3.46831735e-06
Iter: 1886 loss: 3.46563706e-06
Iter: 1887 loss: 3.46529146e-06
Iter: 1888 loss: 3.46526849e-06
Iter: 1889 loss: 3.46507613e-06
Iter: 1890 loss: 3.46461343e-06
Iter: 1891 loss: 3.46915954e-06
Iter: 1892 loss: 3.46459365e-06
Iter: 1893 loss: 3.46432626e-06
Iter: 1894 loss: 3.46429283e-06
Iter: 1895 loss: 3.46405636e-06
Iter: 1896 loss: 3.46364891e-06
Iter: 1897 loss: 3.47362175e-06
Iter: 1898 loss: 3.46364232e-06
Iter: 1899 loss: 3.46323259e-06
Iter: 1900 loss: 3.46324236e-06
Iter: 1901 loss: 3.46299885e-06
Iter: 1902 loss: 3.46250363e-06
Iter: 1903 loss: 3.47224955e-06
Iter: 1904 loss: 3.46250181e-06
Iter: 1905 loss: 3.46191882e-06
Iter: 1906 loss: 3.46580191e-06
Iter: 1907 loss: 3.46188085e-06
Iter: 1908 loss: 3.46143815e-06
Iter: 1909 loss: 3.46285969e-06
Iter: 1910 loss: 3.46129264e-06
Iter: 1911 loss: 3.46100114e-06
Iter: 1912 loss: 3.46234833e-06
Iter: 1913 loss: 3.46093293e-06
Iter: 1914 loss: 3.46051775e-06
Iter: 1915 loss: 3.4608197e-06
Iter: 1916 loss: 3.46028764e-06
Iter: 1917 loss: 3.45993681e-06
Iter: 1918 loss: 3.459656e-06
Iter: 1919 loss: 3.45953799e-06
Iter: 1920 loss: 3.45924059e-06
Iter: 1921 loss: 3.45923672e-06
Iter: 1922 loss: 3.45890339e-06
Iter: 1923 loss: 3.45836497e-06
Iter: 1924 loss: 3.45836543e-06
Iter: 1925 loss: 3.45787157e-06
Iter: 1926 loss: 3.45986859e-06
Iter: 1927 loss: 3.45779836e-06
Iter: 1928 loss: 3.45724311e-06
Iter: 1929 loss: 3.46098068e-06
Iter: 1930 loss: 3.45719673e-06
Iter: 1931 loss: 3.45695e-06
Iter: 1932 loss: 3.45775379e-06
Iter: 1933 loss: 3.45690796e-06
Iter: 1934 loss: 3.456576e-06
Iter: 1935 loss: 3.45668354e-06
Iter: 1936 loss: 3.45632566e-06
Iter: 1937 loss: 3.45599346e-06
Iter: 1938 loss: 3.45547801e-06
Iter: 1939 loss: 3.45544868e-06
Iter: 1940 loss: 3.45479611e-06
Iter: 1941 loss: 3.46064417e-06
Iter: 1942 loss: 3.45477815e-06
Iter: 1943 loss: 3.45418857e-06
Iter: 1944 loss: 3.45482613e-06
Iter: 1945 loss: 3.45388867e-06
Iter: 1946 loss: 3.45379385e-06
Iter: 1947 loss: 3.45367016e-06
Iter: 1948 loss: 3.45347121e-06
Iter: 1949 loss: 3.45307012e-06
Iter: 1950 loss: 3.45906983e-06
Iter: 1951 loss: 3.45301873e-06
Iter: 1952 loss: 3.45254807e-06
Iter: 1953 loss: 3.45392414e-06
Iter: 1954 loss: 3.45241074e-06
Iter: 1955 loss: 3.4518921e-06
Iter: 1956 loss: 3.45750914e-06
Iter: 1957 loss: 3.45187073e-06
Iter: 1958 loss: 3.45156832e-06
Iter: 1959 loss: 3.45104081e-06
Iter: 1960 loss: 3.4607192e-06
Iter: 1961 loss: 3.45097669e-06
Iter: 1962 loss: 3.45053286e-06
Iter: 1963 loss: 3.45654189e-06
Iter: 1964 loss: 3.45051376e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi1.2
+ date
Sat Oct 31 20:28:50 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e446c01e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e446c3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e446c3f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e446c3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e4475f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e4475fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e445ca8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e44444840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e445caf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e445ca730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e44689730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e445acea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e445acc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e4448de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e444a3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e443ddd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e443f4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e44672510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e4453b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e4453bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e44557950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e445576a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e442aca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e4429c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e4421bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e44223620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e441e66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e44272840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e442721e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e4424f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e44197488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e445fc488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e445fc1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e4438ee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e443868c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f9e44123510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.007751612
test_loss: 0.008150692
train_loss: 0.006129532
test_loss: 0.00601844
train_loss: 0.0052209366
test_loss: 0.0054673604
train_loss: 0.0045714164
test_loss: 0.0047448543
train_loss: 0.004285848
test_loss: 0.00452069
train_loss: 0.0039553326
test_loss: 0.0040035895
train_loss: 0.0036589126
test_loss: 0.0037182085
train_loss: 0.003517535
test_loss: 0.0036147146
train_loss: 0.0031912024
test_loss: 0.003335569
train_loss: 0.0030534654
test_loss: 0.0031608702
train_loss: 0.0028193807
test_loss: 0.0030754548
train_loss: 0.002768922
test_loss: 0.0029177004
train_loss: 0.0027183746
test_loss: 0.002790397
train_loss: 0.0025416086
test_loss: 0.0027148896
train_loss: 0.002463133
test_loss: 0.0026646608
train_loss: 0.002387577
test_loss: 0.0025889461
train_loss: 0.0024298765
test_loss: 0.0025442047
train_loss: 0.0024134612
test_loss: 0.0024957613
train_loss: 0.0022139
test_loss: 0.0024793749
train_loss: 0.0021694666
test_loss: 0.0024082698
train_loss: 0.0021402014
test_loss: 0.002397591
train_loss: 0.002114954
test_loss: 0.0023483932
train_loss: 0.0020059915
test_loss: 0.0022871094
train_loss: 0.0021116985
test_loss: 0.002271051
train_loss: 0.0019330035
test_loss: 0.0022549795
train_loss: 0.0019957987
test_loss: 0.002231366
train_loss: 0.0020088041
test_loss: 0.0022227548
train_loss: 0.0019703228
test_loss: 0.0021988512
train_loss: 0.0019710225
test_loss: 0.002182976
train_loss: 0.0018656473
test_loss: 0.0021720275
train_loss: 0.0019116211
test_loss: 0.0021603315
train_loss: 0.0018990997
test_loss: 0.0021590125
train_loss: 0.0018183263
test_loss: 0.002146426
train_loss: 0.0018540437
test_loss: 0.0021354377
train_loss: 0.0018655568
test_loss: 0.0021274586
train_loss: 0.0017911468
test_loss: 0.0021140988
train_loss: 0.001870706
test_loss: 0.0021089166
train_loss: 0.0018624422
test_loss: 0.002111459
train_loss: 0.0018587541
test_loss: 0.002101166
train_loss: 0.0017698556
test_loss: 0.0020990022
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09c5aba510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09c5a8bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09c5a8b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09c5a8b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09c5b6b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f099ad33378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f099ad6de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09c5b5a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f099accd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f099acc8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f099ad26d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f099acc8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09743bd950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09744039d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09743ab510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0974386d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f097439b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f097439b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09742d41e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09742b2d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0974283400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0974283d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0974201510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f097421bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09742046a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09741db620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0974241840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09742412f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f097422f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0974138950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09740be598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0974076048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f097407c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09740a9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f09740439d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0974119400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6.2740387e-06
Iter: 2 loss: 6.23283722e-06
Iter: 3 loss: 6.22409698e-06
Iter: 4 loss: 6.20012906e-06
Iter: 5 loss: 6.25600114e-06
Iter: 6 loss: 6.19129105e-06
Iter: 7 loss: 6.17136811e-06
Iter: 8 loss: 6.14381042e-06
Iter: 9 loss: 6.14263e-06
Iter: 10 loss: 6.12123858e-06
Iter: 11 loss: 6.12073882e-06
Iter: 12 loss: 6.10155485e-06
Iter: 13 loss: 6.09148219e-06
Iter: 14 loss: 6.08278151e-06
Iter: 15 loss: 6.05763262e-06
Iter: 16 loss: 6.10996949e-06
Iter: 17 loss: 6.04767411e-06
Iter: 18 loss: 6.02438831e-06
Iter: 19 loss: 6.32271076e-06
Iter: 20 loss: 6.02423279e-06
Iter: 21 loss: 6.01103056e-06
Iter: 22 loss: 5.98210454e-06
Iter: 23 loss: 6.40924281e-06
Iter: 24 loss: 5.98085035e-06
Iter: 25 loss: 5.98209954e-06
Iter: 26 loss: 5.96895143e-06
Iter: 27 loss: 5.95951269e-06
Iter: 28 loss: 5.93873028e-06
Iter: 29 loss: 6.24349695e-06
Iter: 30 loss: 5.93777895e-06
Iter: 31 loss: 5.91786e-06
Iter: 32 loss: 5.92079459e-06
Iter: 33 loss: 5.90275067e-06
Iter: 34 loss: 5.8796486e-06
Iter: 35 loss: 6.04474099e-06
Iter: 36 loss: 5.87753038e-06
Iter: 37 loss: 5.86002e-06
Iter: 38 loss: 5.95087067e-06
Iter: 39 loss: 5.85718e-06
Iter: 40 loss: 5.842669e-06
Iter: 41 loss: 5.84267309e-06
Iter: 42 loss: 5.83841393e-06
Iter: 43 loss: 5.8307769e-06
Iter: 44 loss: 6.01435295e-06
Iter: 45 loss: 5.83075325e-06
Iter: 46 loss: 5.82148095e-06
Iter: 47 loss: 5.88750572e-06
Iter: 48 loss: 5.8206042e-06
Iter: 49 loss: 5.81251106e-06
Iter: 50 loss: 5.84905683e-06
Iter: 51 loss: 5.81083759e-06
Iter: 52 loss: 5.80429651e-06
Iter: 53 loss: 5.80122469e-06
Iter: 54 loss: 5.79793823e-06
Iter: 55 loss: 5.7904449e-06
Iter: 56 loss: 5.87565046e-06
Iter: 57 loss: 5.79033531e-06
Iter: 58 loss: 5.7828e-06
Iter: 59 loss: 5.77397668e-06
Iter: 60 loss: 5.77301034e-06
Iter: 61 loss: 5.7657162e-06
Iter: 62 loss: 5.84176132e-06
Iter: 63 loss: 5.76551156e-06
Iter: 64 loss: 5.75921831e-06
Iter: 65 loss: 5.7911866e-06
Iter: 66 loss: 5.75813829e-06
Iter: 67 loss: 5.75336117e-06
Iter: 68 loss: 5.74383239e-06
Iter: 69 loss: 5.92371453e-06
Iter: 70 loss: 5.74366368e-06
Iter: 71 loss: 5.73352418e-06
Iter: 72 loss: 5.75286913e-06
Iter: 73 loss: 5.72935e-06
Iter: 74 loss: 5.71896362e-06
Iter: 75 loss: 5.77336959e-06
Iter: 76 loss: 5.71738883e-06
Iter: 77 loss: 5.71780265e-06
Iter: 78 loss: 5.71377132e-06
Iter: 79 loss: 5.71134751e-06
Iter: 80 loss: 5.70488e-06
Iter: 81 loss: 5.74641081e-06
Iter: 82 loss: 5.70320572e-06
Iter: 83 loss: 5.69752319e-06
Iter: 84 loss: 5.75931335e-06
Iter: 85 loss: 5.69744543e-06
Iter: 86 loss: 5.69301e-06
Iter: 87 loss: 5.73455782e-06
Iter: 88 loss: 5.69283293e-06
Iter: 89 loss: 5.68988889e-06
Iter: 90 loss: 5.68441465e-06
Iter: 91 loss: 5.81016457e-06
Iter: 92 loss: 5.6844242e-06
Iter: 93 loss: 5.68033056e-06
Iter: 94 loss: 5.68025735e-06
Iter: 95 loss: 5.67703137e-06
Iter: 96 loss: 5.67651523e-06
Iter: 97 loss: 5.67418965e-06
Iter: 98 loss: 5.66941071e-06
Iter: 99 loss: 5.67828647e-06
Iter: 100 loss: 5.66732615e-06
Iter: 101 loss: 5.66406106e-06
Iter: 102 loss: 5.71396185e-06
Iter: 103 loss: 5.6641e-06
Iter: 104 loss: 5.66103836e-06
Iter: 105 loss: 5.65571372e-06
Iter: 106 loss: 5.65572191e-06
Iter: 107 loss: 5.65083519e-06
Iter: 108 loss: 5.64716174e-06
Iter: 109 loss: 5.6456156e-06
Iter: 110 loss: 5.6392264e-06
Iter: 111 loss: 5.70526754e-06
Iter: 112 loss: 5.63905e-06
Iter: 113 loss: 5.63608501e-06
Iter: 114 loss: 5.63578578e-06
Iter: 115 loss: 5.63287949e-06
Iter: 116 loss: 5.62849164e-06
Iter: 117 loss: 5.62838204e-06
Iter: 118 loss: 5.62478635e-06
Iter: 119 loss: 5.62300102e-06
Iter: 120 loss: 5.62125297e-06
Iter: 121 loss: 5.6188037e-06
Iter: 122 loss: 5.61774095e-06
Iter: 123 loss: 5.61622392e-06
Iter: 124 loss: 5.61268916e-06
Iter: 125 loss: 5.6550939e-06
Iter: 126 loss: 5.61238448e-06
Iter: 127 loss: 5.6096078e-06
Iter: 128 loss: 5.60952594e-06
Iter: 129 loss: 5.60697481e-06
Iter: 130 loss: 5.60416947e-06
Iter: 131 loss: 5.60379249e-06
Iter: 132 loss: 5.6001104e-06
Iter: 133 loss: 5.62953028e-06
Iter: 134 loss: 5.59988212e-06
Iter: 135 loss: 5.59727641e-06
Iter: 136 loss: 5.60800163e-06
Iter: 137 loss: 5.59667387e-06
Iter: 138 loss: 5.59407727e-06
Iter: 139 loss: 5.5921214e-06
Iter: 140 loss: 5.59126784e-06
Iter: 141 loss: 5.58727061e-06
Iter: 142 loss: 5.59023283e-06
Iter: 143 loss: 5.58481952e-06
Iter: 144 loss: 5.58074044e-06
Iter: 145 loss: 5.5984392e-06
Iter: 146 loss: 5.57990779e-06
Iter: 147 loss: 5.57825661e-06
Iter: 148 loss: 5.5776427e-06
Iter: 149 loss: 5.57643352e-06
Iter: 150 loss: 5.57317071e-06
Iter: 151 loss: 5.59622458e-06
Iter: 152 loss: 5.57247131e-06
Iter: 153 loss: 5.56904797e-06
Iter: 154 loss: 5.58805687e-06
Iter: 155 loss: 5.56850227e-06
Iter: 156 loss: 5.56513487e-06
Iter: 157 loss: 5.59802038e-06
Iter: 158 loss: 5.56500527e-06
Iter: 159 loss: 5.56291798e-06
Iter: 160 loss: 5.56016494e-06
Iter: 161 loss: 5.56002396e-06
Iter: 162 loss: 5.55783527e-06
Iter: 163 loss: 5.55780389e-06
Iter: 164 loss: 5.55566339e-06
Iter: 165 loss: 5.55425459e-06
Iter: 166 loss: 5.55347106e-06
Iter: 167 loss: 5.55127554e-06
Iter: 168 loss: 5.56495115e-06
Iter: 169 loss: 5.55101633e-06
Iter: 170 loss: 5.54851522e-06
Iter: 171 loss: 5.55346833e-06
Iter: 172 loss: 5.54748704e-06
Iter: 173 loss: 5.54554845e-06
Iter: 174 loss: 5.54651888e-06
Iter: 175 loss: 5.54426606e-06
Iter: 176 loss: 5.541674e-06
Iter: 177 loss: 5.54654889e-06
Iter: 178 loss: 5.5406972e-06
Iter: 179 loss: 5.53801874e-06
Iter: 180 loss: 5.5398823e-06
Iter: 181 loss: 5.53638392e-06
Iter: 182 loss: 5.53552e-06
Iter: 183 loss: 5.53457448e-06
Iter: 184 loss: 5.5337423e-06
Iter: 185 loss: 5.53120753e-06
Iter: 186 loss: 5.53858081e-06
Iter: 187 loss: 5.52993743e-06
Iter: 188 loss: 5.52724032e-06
Iter: 189 loss: 5.52724578e-06
Iter: 190 loss: 5.52508936e-06
Iter: 191 loss: 5.53802556e-06
Iter: 192 loss: 5.52480378e-06
Iter: 193 loss: 5.52346955e-06
Iter: 194 loss: 5.52064375e-06
Iter: 195 loss: 5.5699993e-06
Iter: 196 loss: 5.52060192e-06
Iter: 197 loss: 5.51987614e-06
Iter: 198 loss: 5.51919402e-06
Iter: 199 loss: 5.51796074e-06
Iter: 200 loss: 5.51567791e-06
Iter: 201 loss: 5.56838177e-06
Iter: 202 loss: 5.51565518e-06
Iter: 203 loss: 5.51419544e-06
Iter: 204 loss: 5.51416633e-06
Iter: 205 loss: 5.51250787e-06
Iter: 206 loss: 5.50999448e-06
Iter: 207 loss: 5.5099872e-06
Iter: 208 loss: 5.50768436e-06
Iter: 209 loss: 5.51766516e-06
Iter: 210 loss: 5.50723962e-06
Iter: 211 loss: 5.50502364e-06
Iter: 212 loss: 5.50754976e-06
Iter: 213 loss: 5.50387404e-06
Iter: 214 loss: 5.50174536e-06
Iter: 215 loss: 5.51983248e-06
Iter: 216 loss: 5.50160667e-06
Iter: 217 loss: 5.5e-06
Iter: 218 loss: 5.51870471e-06
Iter: 219 loss: 5.49994547e-06
Iter: 220 loss: 5.49895776e-06
Iter: 221 loss: 5.49641e-06
Iter: 222 loss: 5.51955554e-06
Iter: 223 loss: 5.49594097e-06
Iter: 224 loss: 5.49426795e-06
Iter: 225 loss: 5.51813582e-06
Iter: 226 loss: 5.49431752e-06
Iter: 227 loss: 5.49238484e-06
Iter: 228 loss: 5.4980992e-06
Iter: 229 loss: 5.49187735e-06
Iter: 230 loss: 5.49051947e-06
Iter: 231 loss: 5.48813023e-06
Iter: 232 loss: 5.48815296e-06
Iter: 233 loss: 5.48752132e-06
Iter: 234 loss: 5.48690696e-06
Iter: 235 loss: 5.4858474e-06
Iter: 236 loss: 5.48418393e-06
Iter: 237 loss: 5.4841812e-06
Iter: 238 loss: 5.48241587e-06
Iter: 239 loss: 5.49444167e-06
Iter: 240 loss: 5.48221669e-06
Iter: 241 loss: 5.48048456e-06
Iter: 242 loss: 5.48670596e-06
Iter: 243 loss: 5.48006892e-06
Iter: 244 loss: 5.47888794e-06
Iter: 245 loss: 5.47663421e-06
Iter: 246 loss: 5.52303027e-06
Iter: 247 loss: 5.4766324e-06
Iter: 248 loss: 5.47459331e-06
Iter: 249 loss: 5.49195556e-06
Iter: 250 loss: 5.47446689e-06
Iter: 251 loss: 5.47297532e-06
Iter: 252 loss: 5.4867196e-06
Iter: 253 loss: 5.47290892e-06
Iter: 254 loss: 5.47149648e-06
Iter: 255 loss: 5.4751813e-06
Iter: 256 loss: 5.47102718e-06
Iter: 257 loss: 5.46992669e-06
Iter: 258 loss: 5.46888896e-06
Iter: 259 loss: 5.46862e-06
Iter: 260 loss: 5.46703177e-06
Iter: 261 loss: 5.47172476e-06
Iter: 262 loss: 5.46655565e-06
Iter: 263 loss: 5.46495448e-06
Iter: 264 loss: 5.48439175e-06
Iter: 265 loss: 5.46491e-06
Iter: 266 loss: 5.46389492e-06
Iter: 267 loss: 5.46231513e-06
Iter: 268 loss: 5.46226e-06
Iter: 269 loss: 5.46085721e-06
Iter: 270 loss: 5.47379386e-06
Iter: 271 loss: 5.46076717e-06
Iter: 272 loss: 5.45920102e-06
Iter: 273 loss: 5.46224555e-06
Iter: 274 loss: 5.45851117e-06
Iter: 275 loss: 5.45736202e-06
Iter: 276 loss: 5.45798594e-06
Iter: 277 loss: 5.45659532e-06
Iter: 278 loss: 5.45516059e-06
Iter: 279 loss: 5.47212312e-06
Iter: 280 loss: 5.45513512e-06
Iter: 281 loss: 5.45436524e-06
Iter: 282 loss: 5.45216108e-06
Iter: 283 loss: 5.46570527e-06
Iter: 284 loss: 5.45156172e-06
Iter: 285 loss: 5.44957e-06
Iter: 286 loss: 5.44958e-06
Iter: 287 loss: 5.44835075e-06
Iter: 288 loss: 5.44839395e-06
Iter: 289 loss: 5.4475554e-06
Iter: 290 loss: 5.44623344e-06
Iter: 291 loss: 5.44621344e-06
Iter: 292 loss: 5.44481463e-06
Iter: 293 loss: 5.45037119e-06
Iter: 294 loss: 5.44445538e-06
Iter: 295 loss: 5.44322575e-06
Iter: 296 loss: 5.44575732e-06
Iter: 297 loss: 5.4427328e-06
Iter: 298 loss: 5.44117574e-06
Iter: 299 loss: 5.45198418e-06
Iter: 300 loss: 5.44103e-06
Iter: 301 loss: 5.4402044e-06
Iter: 302 loss: 5.43825536e-06
Iter: 303 loss: 5.45799503e-06
Iter: 304 loss: 5.4379916e-06
Iter: 305 loss: 5.43822534e-06
Iter: 306 loss: 5.43710394e-06
Iter: 307 loss: 5.43633814e-06
Iter: 308 loss: 5.43485294e-06
Iter: 309 loss: 5.46240335e-06
Iter: 310 loss: 5.43486385e-06
Iter: 311 loss: 5.43370743e-06
Iter: 312 loss: 5.44906e-06
Iter: 313 loss: 5.43368e-06
Iter: 314 loss: 5.43245551e-06
Iter: 315 loss: 5.43357646e-06
Iter: 316 loss: 5.43175156e-06
Iter: 317 loss: 5.43067063e-06
Iter: 318 loss: 5.43014312e-06
Iter: 319 loss: 5.4296097e-06
Iter: 320 loss: 5.4286138e-06
Iter: 321 loss: 5.44360228e-06
Iter: 322 loss: 5.42861744e-06
Iter: 323 loss: 5.42740236e-06
Iter: 324 loss: 5.42677e-06
Iter: 325 loss: 5.42615544e-06
Iter: 326 loss: 5.42488806e-06
Iter: 327 loss: 5.42824364e-06
Iter: 328 loss: 5.42442285e-06
Iter: 329 loss: 5.42324551e-06
Iter: 330 loss: 5.42403359e-06
Iter: 331 loss: 5.42255248e-06
Iter: 332 loss: 5.42137514e-06
Iter: 333 loss: 5.43980059e-06
Iter: 334 loss: 5.42135376e-06
Iter: 335 loss: 5.42030739e-06
Iter: 336 loss: 5.4194611e-06
Iter: 337 loss: 5.41913778e-06
Iter: 338 loss: 5.41797772e-06
Iter: 339 loss: 5.41901227e-06
Iter: 340 loss: 5.41726e-06
Iter: 341 loss: 5.41600957e-06
Iter: 342 loss: 5.41601821e-06
Iter: 343 loss: 5.41516374e-06
Iter: 344 loss: 5.4137904e-06
Iter: 345 loss: 5.41375903e-06
Iter: 346 loss: 5.41267673e-06
Iter: 347 loss: 5.41269674e-06
Iter: 348 loss: 5.41158624e-06
Iter: 349 loss: 5.41024974e-06
Iter: 350 loss: 5.41008967e-06
Iter: 351 loss: 5.408775e-06
Iter: 352 loss: 5.40996e-06
Iter: 353 loss: 5.40804467e-06
Iter: 354 loss: 5.40775409e-06
Iter: 355 loss: 5.40735e-06
Iter: 356 loss: 5.40665314e-06
Iter: 357 loss: 5.40530527e-06
Iter: 358 loss: 5.43200895e-06
Iter: 359 loss: 5.40532164e-06
Iter: 360 loss: 5.40410929e-06
Iter: 361 loss: 5.40628434e-06
Iter: 362 loss: 5.40360452e-06
Iter: 363 loss: 5.40228075e-06
Iter: 364 loss: 5.41091094e-06
Iter: 365 loss: 5.40213932e-06
Iter: 366 loss: 5.40106703e-06
Iter: 367 loss: 5.40910105e-06
Iter: 368 loss: 5.40096198e-06
Iter: 369 loss: 5.40012297e-06
Iter: 370 loss: 5.39926896e-06
Iter: 371 loss: 5.39914163e-06
Iter: 372 loss: 5.39775738e-06
Iter: 373 loss: 5.40020301e-06
Iter: 374 loss: 5.39718394e-06
Iter: 375 loss: 5.3961412e-06
Iter: 376 loss: 5.39604025e-06
Iter: 377 loss: 5.3955132e-06
Iter: 378 loss: 5.39405119e-06
Iter: 379 loss: 5.4090192e-06
Iter: 380 loss: 5.39394023e-06
Iter: 381 loss: 5.39328084e-06
Iter: 382 loss: 5.39299072e-06
Iter: 383 loss: 5.39237e-06
Iter: 384 loss: 5.39093435e-06
Iter: 385 loss: 5.41098325e-06
Iter: 386 loss: 5.39089888e-06
Iter: 387 loss: 5.3894255e-06
Iter: 388 loss: 5.3937847e-06
Iter: 389 loss: 5.38898621e-06
Iter: 390 loss: 5.38846962e-06
Iter: 391 loss: 5.38812537e-06
Iter: 392 loss: 5.38748e-06
Iter: 393 loss: 5.38609902e-06
Iter: 394 loss: 5.41224472e-06
Iter: 395 loss: 5.38602035e-06
Iter: 396 loss: 5.38484846e-06
Iter: 397 loss: 5.38380937e-06
Iter: 398 loss: 5.38345739e-06
Iter: 399 loss: 5.38350878e-06
Iter: 400 loss: 5.38262248e-06
Iter: 401 loss: 5.38196946e-06
Iter: 402 loss: 5.38155427e-06
Iter: 403 loss: 5.38131553e-06
Iter: 404 loss: 5.38011682e-06
Iter: 405 loss: 5.37947653e-06
Iter: 406 loss: 5.37894539e-06
Iter: 407 loss: 5.37784126e-06
Iter: 408 loss: 5.37781898e-06
Iter: 409 loss: 5.37699543e-06
Iter: 410 loss: 5.38000859e-06
Iter: 411 loss: 5.37677e-06
Iter: 412 loss: 5.3759768e-06
Iter: 413 loss: 5.37478445e-06
Iter: 414 loss: 5.37475171e-06
Iter: 415 loss: 5.37384767e-06
Iter: 416 loss: 5.37376582e-06
Iter: 417 loss: 5.37302958e-06
Iter: 418 loss: 5.37173355e-06
Iter: 419 loss: 5.40317342e-06
Iter: 420 loss: 5.37176e-06
Iter: 421 loss: 5.37049891e-06
Iter: 422 loss: 5.3741278e-06
Iter: 423 loss: 5.3701342e-06
Iter: 424 loss: 5.36920834e-06
Iter: 425 loss: 5.36912739e-06
Iter: 426 loss: 5.36862808e-06
Iter: 427 loss: 5.36710559e-06
Iter: 428 loss: 5.37600181e-06
Iter: 429 loss: 5.36669e-06
Iter: 430 loss: 5.3654735e-06
Iter: 431 loss: 5.380517e-06
Iter: 432 loss: 5.36552e-06
Iter: 433 loss: 5.36455673e-06
Iter: 434 loss: 5.37383039e-06
Iter: 435 loss: 5.36453808e-06
Iter: 436 loss: 5.36381367e-06
Iter: 437 loss: 5.36292e-06
Iter: 438 loss: 5.36281641e-06
Iter: 439 loss: 5.36173638e-06
Iter: 440 loss: 5.3669537e-06
Iter: 441 loss: 5.3615031e-06
Iter: 442 loss: 5.36041534e-06
Iter: 443 loss: 5.36777543e-06
Iter: 444 loss: 5.36036077e-06
Iter: 445 loss: 5.35940762e-06
Iter: 446 loss: 5.36030257e-06
Iter: 447 loss: 5.35885192e-06
Iter: 448 loss: 5.35794879e-06
Iter: 449 loss: 5.3587919e-06
Iter: 450 loss: 5.35739491e-06
Iter: 451 loss: 5.3563117e-06
Iter: 452 loss: 5.36870539e-06
Iter: 453 loss: 5.35629897e-06
Iter: 454 loss: 5.35578e-06
Iter: 455 loss: 5.35453955e-06
Iter: 456 loss: 5.36757943e-06
Iter: 457 loss: 5.35441768e-06
Iter: 458 loss: 5.35367462e-06
Iter: 459 loss: 5.35351865e-06
Iter: 460 loss: 5.35261461e-06
Iter: 461 loss: 5.35289109e-06
Iter: 462 loss: 5.35203526e-06
Iter: 463 loss: 5.35125218e-06
Iter: 464 loss: 5.35027038e-06
Iter: 465 loss: 5.35017e-06
Iter: 466 loss: 5.34921855e-06
Iter: 467 loss: 5.36420794e-06
Iter: 468 loss: 5.3492322e-06
Iter: 469 loss: 5.34804121e-06
Iter: 470 loss: 5.34902119e-06
Iter: 471 loss: 5.34737956e-06
Iter: 472 loss: 5.34656783e-06
Iter: 473 loss: 5.34642231e-06
Iter: 474 loss: 5.34579794e-06
Iter: 475 loss: 5.34483388e-06
Iter: 476 loss: 5.35683648e-06
Iter: 477 loss: 5.34483206e-06
Iter: 478 loss: 5.34414357e-06
Iter: 479 loss: 5.34674928e-06
Iter: 480 loss: 5.3439644e-06
Iter: 481 loss: 5.34321498e-06
Iter: 482 loss: 5.342446e-06
Iter: 483 loss: 5.34237e-06
Iter: 484 loss: 5.34134142e-06
Iter: 485 loss: 5.35514573e-06
Iter: 486 loss: 5.34135961e-06
Iter: 487 loss: 5.34052924e-06
Iter: 488 loss: 5.3413487e-06
Iter: 489 loss: 5.34010678e-06
Iter: 490 loss: 5.33930552e-06
Iter: 491 loss: 5.33777529e-06
Iter: 492 loss: 5.37318e-06
Iter: 493 loss: 5.3377953e-06
Iter: 494 loss: 5.33798629e-06
Iter: 495 loss: 5.33692946e-06
Iter: 496 loss: 5.33644243e-06
Iter: 497 loss: 5.33545881e-06
Iter: 498 loss: 5.34629271e-06
Iter: 499 loss: 5.33530965e-06
Iter: 500 loss: 5.33423645e-06
Iter: 501 loss: 5.33600451e-06
Iter: 502 loss: 5.33369e-06
Iter: 503 loss: 5.33304774e-06
Iter: 504 loss: 5.33291723e-06
Iter: 505 loss: 5.33230923e-06
Iter: 506 loss: 5.33110369e-06
Iter: 507 loss: 5.35471e-06
Iter: 508 loss: 5.33109778e-06
Iter: 509 loss: 5.33000048e-06
Iter: 510 loss: 5.33535695e-06
Iter: 511 loss: 5.3297631e-06
Iter: 512 loss: 5.32866761e-06
Iter: 513 loss: 5.33743832e-06
Iter: 514 loss: 5.32857666e-06
Iter: 515 loss: 5.32776403e-06
Iter: 516 loss: 5.32902413e-06
Iter: 517 loss: 5.32745071e-06
Iter: 518 loss: 5.3267313e-06
Iter: 519 loss: 5.32796048e-06
Iter: 520 loss: 5.32637296e-06
Iter: 521 loss: 5.32554077e-06
Iter: 522 loss: 5.33071579e-06
Iter: 523 loss: 5.32546619e-06
Iter: 524 loss: 5.3247627e-06
Iter: 525 loss: 5.32365175e-06
Iter: 526 loss: 5.32366494e-06
Iter: 527 loss: 5.32284139e-06
Iter: 528 loss: 5.32279319e-06
Iter: 529 loss: 5.32200147e-06
Iter: 530 loss: 5.32276e-06
Iter: 531 loss: 5.32146896e-06
Iter: 532 loss: 5.32072409e-06
Iter: 533 loss: 5.31969181e-06
Iter: 534 loss: 5.31956812e-06
Iter: 535 loss: 5.31851083e-06
Iter: 536 loss: 5.33152206e-06
Iter: 537 loss: 5.31850037e-06
Iter: 538 loss: 5.31760543e-06
Iter: 539 loss: 5.32503145e-06
Iter: 540 loss: 5.31749174e-06
Iter: 541 loss: 5.31704382e-06
Iter: 542 loss: 5.31596288e-06
Iter: 543 loss: 5.32604781e-06
Iter: 544 loss: 5.31583237e-06
Iter: 545 loss: 5.31483192e-06
Iter: 546 loss: 5.31478054e-06
Iter: 547 loss: 5.31397291e-06
Iter: 548 loss: 5.31536807e-06
Iter: 549 loss: 5.31364913e-06
Iter: 550 loss: 5.31296564e-06
Iter: 551 loss: 5.31356272e-06
Iter: 552 loss: 5.31253954e-06
Iter: 553 loss: 5.31175419e-06
Iter: 554 loss: 5.31725482e-06
Iter: 555 loss: 5.31164596e-06
Iter: 556 loss: 5.31092064e-06
Iter: 557 loss: 5.31166734e-06
Iter: 558 loss: 5.3105523e-06
Iter: 559 loss: 5.30979878e-06
Iter: 560 loss: 5.30971283e-06
Iter: 561 loss: 5.30920533e-06
Iter: 562 loss: 5.30859188e-06
Iter: 563 loss: 5.30853504e-06
Iter: 564 loss: 5.3080239e-06
Iter: 565 loss: 5.30670832e-06
Iter: 566 loss: 5.31339037e-06
Iter: 567 loss: 5.30621946e-06
Iter: 568 loss: 5.30491252e-06
Iter: 569 loss: 5.31621936e-06
Iter: 570 loss: 5.30481384e-06
Iter: 571 loss: 5.30421903e-06
Iter: 572 loss: 5.30420584e-06
Iter: 573 loss: 5.30371335e-06
Iter: 574 loss: 5.30276156e-06
Iter: 575 loss: 5.32300555e-06
Iter: 576 loss: 5.30275702e-06
Iter: 577 loss: 5.30190573e-06
Iter: 578 loss: 5.30556827e-06
Iter: 579 loss: 5.30169564e-06
Iter: 580 loss: 5.30076704e-06
Iter: 581 loss: 5.30931175e-06
Iter: 582 loss: 5.3007775e-06
Iter: 583 loss: 5.30025318e-06
Iter: 584 loss: 5.30000307e-06
Iter: 585 loss: 5.29976933e-06
Iter: 586 loss: 5.29897125e-06
Iter: 587 loss: 5.30122361e-06
Iter: 588 loss: 5.29863655e-06
Iter: 589 loss: 5.29766749e-06
Iter: 590 loss: 5.30063335e-06
Iter: 591 loss: 5.29733825e-06
Iter: 592 loss: 5.29643103e-06
Iter: 593 loss: 5.29639783e-06
Iter: 594 loss: 5.29568388e-06
Iter: 595 loss: 5.29502904e-06
Iter: 596 loss: 5.29503723e-06
Iter: 597 loss: 5.29429963e-06
Iter: 598 loss: 5.29393947e-06
Iter: 599 loss: 5.29367117e-06
Iter: 600 loss: 5.29292311e-06
Iter: 601 loss: 5.29247791e-06
Iter: 602 loss: 5.29213685e-06
Iter: 603 loss: 5.2913997e-06
Iter: 604 loss: 5.30165289e-06
Iter: 605 loss: 5.29143517e-06
Iter: 606 loss: 5.29055205e-06
Iter: 607 loss: 5.29102545e-06
Iter: 608 loss: 5.28998316e-06
Iter: 609 loss: 5.28917872e-06
Iter: 610 loss: 5.2890573e-06
Iter: 611 loss: 5.28854707e-06
Iter: 612 loss: 5.28779401e-06
Iter: 613 loss: 5.28780311e-06
Iter: 614 loss: 5.28707051e-06
Iter: 615 loss: 5.28623332e-06
Iter: 616 loss: 5.28611145e-06
Iter: 617 loss: 5.28527426e-06
Iter: 618 loss: 5.29209365e-06
Iter: 619 loss: 5.28515375e-06
Iter: 620 loss: 5.28442115e-06
Iter: 621 loss: 5.28753662e-06
Iter: 622 loss: 5.28424698e-06
Iter: 623 loss: 5.28364853e-06
Iter: 624 loss: 5.28406281e-06
Iter: 625 loss: 5.28324108e-06
Iter: 626 loss: 5.28249529e-06
Iter: 627 loss: 5.28399232e-06
Iter: 628 loss: 5.28221881e-06
Iter: 629 loss: 5.28130158e-06
Iter: 630 loss: 5.28861e-06
Iter: 631 loss: 5.28130568e-06
Iter: 632 loss: 5.28070541e-06
Iter: 633 loss: 5.27927841e-06
Iter: 634 loss: 5.29546696e-06
Iter: 635 loss: 5.27917655e-06
Iter: 636 loss: 5.27793509e-06
Iter: 637 loss: 5.284081e-06
Iter: 638 loss: 5.27771272e-06
Iter: 639 loss: 5.27719612e-06
Iter: 640 loss: 5.27708835e-06
Iter: 641 loss: 5.27656e-06
Iter: 642 loss: 5.2755413e-06
Iter: 643 loss: 5.29553927e-06
Iter: 644 loss: 5.27555721e-06
Iter: 645 loss: 5.27464636e-06
Iter: 646 loss: 5.27751808e-06
Iter: 647 loss: 5.2743635e-06
Iter: 648 loss: 5.2733958e-06
Iter: 649 loss: 5.2835976e-06
Iter: 650 loss: 5.27339671e-06
Iter: 651 loss: 5.2726873e-06
Iter: 652 loss: 5.27141401e-06
Iter: 653 loss: 5.30100579e-06
Iter: 654 loss: 5.27139764e-06
Iter: 655 loss: 5.27056545e-06
Iter: 656 loss: 5.27049269e-06
Iter: 657 loss: 5.2698324e-06
Iter: 658 loss: 5.26962913e-06
Iter: 659 loss: 5.26925714e-06
Iter: 660 loss: 5.26821259e-06
Iter: 661 loss: 5.26938675e-06
Iter: 662 loss: 5.26766098e-06
Iter: 663 loss: 5.26713211e-06
Iter: 664 loss: 5.26704662e-06
Iter: 665 loss: 5.26654185e-06
Iter: 666 loss: 5.26583608e-06
Iter: 667 loss: 5.26580334e-06
Iter: 668 loss: 5.26489748e-06
Iter: 669 loss: 5.26401709e-06
Iter: 670 loss: 5.26380973e-06
Iter: 671 loss: 5.26271424e-06
Iter: 672 loss: 5.27299471e-06
Iter: 673 loss: 5.2626574e-06
Iter: 674 loss: 5.26137774e-06
Iter: 675 loss: 5.26559234e-06
Iter: 676 loss: 5.26102122e-06
Iter: 677 loss: 5.26020267e-06
Iter: 678 loss: 5.25937685e-06
Iter: 679 loss: 5.25924042e-06
Iter: 680 loss: 5.2583523e-06
Iter: 681 loss: 5.25835776e-06
Iter: 682 loss: 5.25750693e-06
Iter: 683 loss: 5.25780297e-06
Iter: 684 loss: 5.25693758e-06
Iter: 685 loss: 5.25622e-06
Iter: 686 loss: 5.25712221e-06
Iter: 687 loss: 5.25584528e-06
Iter: 688 loss: 5.25482437e-06
Iter: 689 loss: 5.2604214e-06
Iter: 690 loss: 5.2547025e-06
Iter: 691 loss: 5.25398264e-06
Iter: 692 loss: 5.2536916e-06
Iter: 693 loss: 5.25329233e-06
Iter: 694 loss: 5.2522405e-06
Iter: 695 loss: 5.2599089e-06
Iter: 696 loss: 5.25217638e-06
Iter: 697 loss: 5.2512828e-06
Iter: 698 loss: 5.25667338e-06
Iter: 699 loss: 5.25118958e-06
Iter: 700 loss: 5.25072846e-06
Iter: 701 loss: 5.24963525e-06
Iter: 702 loss: 5.26502117e-06
Iter: 703 loss: 5.24957068e-06
Iter: 704 loss: 5.24827874e-06
Iter: 705 loss: 5.25296355e-06
Iter: 706 loss: 5.24791267e-06
Iter: 707 loss: 5.2473124e-06
Iter: 708 loss: 5.24723964e-06
Iter: 709 loss: 5.24655115e-06
Iter: 710 loss: 5.24527e-06
Iter: 711 loss: 5.27390648e-06
Iter: 712 loss: 5.24530287e-06
Iter: 713 loss: 5.24423e-06
Iter: 714 loss: 5.24684174e-06
Iter: 715 loss: 5.2438063e-06
Iter: 716 loss: 5.24276038e-06
Iter: 717 loss: 5.25946052e-06
Iter: 718 loss: 5.24278948e-06
Iter: 719 loss: 5.24219649e-06
Iter: 720 loss: 5.24137522e-06
Iter: 721 loss: 5.24131838e-06
Iter: 722 loss: 5.24045799e-06
Iter: 723 loss: 5.2524e-06
Iter: 724 loss: 5.24047209e-06
Iter: 725 loss: 5.23974086e-06
Iter: 726 loss: 5.23939389e-06
Iter: 727 loss: 5.23903509e-06
Iter: 728 loss: 5.23813242e-06
Iter: 729 loss: 5.24144252e-06
Iter: 730 loss: 5.23790095e-06
Iter: 731 loss: 5.2370815e-06
Iter: 732 loss: 5.24655252e-06
Iter: 733 loss: 5.23708059e-06
Iter: 734 loss: 5.23653671e-06
Iter: 735 loss: 5.2359992e-06
Iter: 736 loss: 5.2359178e-06
Iter: 737 loss: 5.23499693e-06
Iter: 738 loss: 5.234946e-06
Iter: 739 loss: 5.23422204e-06
Iter: 740 loss: 5.23337758e-06
Iter: 741 loss: 5.23990593e-06
Iter: 742 loss: 5.233303e-06
Iter: 743 loss: 5.23239169e-06
Iter: 744 loss: 5.23781137e-06
Iter: 745 loss: 5.23224935e-06
Iter: 746 loss: 5.23165454e-06
Iter: 747 loss: 5.2307596e-06
Iter: 748 loss: 5.23068366e-06
Iter: 749 loss: 5.22991104e-06
Iter: 750 loss: 5.23897052e-06
Iter: 751 loss: 5.22988921e-06
Iter: 752 loss: 5.22890696e-06
Iter: 753 loss: 5.22882237e-06
Iter: 754 loss: 5.22808568e-06
Iter: 755 loss: 5.22704704e-06
Iter: 756 loss: 5.22829123e-06
Iter: 757 loss: 5.22651862e-06
Iter: 758 loss: 5.22563914e-06
Iter: 759 loss: 5.2256155e-06
Iter: 760 loss: 5.22507798e-06
Iter: 761 loss: 5.22412347e-06
Iter: 762 loss: 5.22409709e-06
Iter: 763 loss: 5.22357095e-06
Iter: 764 loss: 5.22346681e-06
Iter: 765 loss: 5.22297341e-06
Iter: 766 loss: 5.22265373e-06
Iter: 767 loss: 5.2224309e-06
Iter: 768 loss: 5.22165374e-06
Iter: 769 loss: 5.22171422e-06
Iter: 770 loss: 5.22107803e-06
Iter: 771 loss: 5.21998e-06
Iter: 772 loss: 5.22125629e-06
Iter: 773 loss: 5.21938364e-06
Iter: 774 loss: 5.2186715e-06
Iter: 775 loss: 5.21864149e-06
Iter: 776 loss: 5.21793754e-06
Iter: 777 loss: 5.21741185e-06
Iter: 778 loss: 5.21722768e-06
Iter: 779 loss: 5.21634638e-06
Iter: 780 loss: 5.2161522e-06
Iter: 781 loss: 5.21556922e-06
Iter: 782 loss: 5.21501261e-06
Iter: 783 loss: 5.21488118e-06
Iter: 784 loss: 5.21431957e-06
Iter: 785 loss: 5.21305901e-06
Iter: 786 loss: 5.22525079e-06
Iter: 787 loss: 5.21285256e-06
Iter: 788 loss: 5.21217498e-06
Iter: 789 loss: 5.21202765e-06
Iter: 790 loss: 5.21138372e-06
Iter: 791 loss: 5.21121228e-06
Iter: 792 loss: 5.21076936e-06
Iter: 793 loss: 5.20985895e-06
Iter: 794 loss: 5.21082211e-06
Iter: 795 loss: 5.20938283e-06
Iter: 796 loss: 5.20844742e-06
Iter: 797 loss: 5.20845515e-06
Iter: 798 loss: 5.20803042e-06
Iter: 799 loss: 5.20712729e-06
Iter: 800 loss: 5.22224491e-06
Iter: 801 loss: 5.20709409e-06
Iter: 802 loss: 5.2059786e-06
Iter: 803 loss: 5.21019274e-06
Iter: 804 loss: 5.20568165e-06
Iter: 805 loss: 5.20483081e-06
Iter: 806 loss: 5.20830872e-06
Iter: 807 loss: 5.20467529e-06
Iter: 808 loss: 5.20355889e-06
Iter: 809 loss: 5.20836738e-06
Iter: 810 loss: 5.20341564e-06
Iter: 811 loss: 5.20269441e-06
Iter: 812 loss: 5.20176354e-06
Iter: 813 loss: 5.20178583e-06
Iter: 814 loss: 5.20092362e-06
Iter: 815 loss: 5.20093363e-06
Iter: 816 loss: 5.20006461e-06
Iter: 817 loss: 5.20063031e-06
Iter: 818 loss: 5.19951345e-06
Iter: 819 loss: 5.1987663e-06
Iter: 820 loss: 5.1989914e-06
Iter: 821 loss: 5.19821924e-06
Iter: 822 loss: 5.19734e-06
Iter: 823 loss: 5.21060383e-06
Iter: 824 loss: 5.19730793e-06
Iter: 825 loss: 5.19678633e-06
Iter: 826 loss: 5.19560035e-06
Iter: 827 loss: 5.21167567e-06
Iter: 828 loss: 5.19557307e-06
Iter: 829 loss: 5.19505193e-06
Iter: 830 loss: 5.19481364e-06
Iter: 831 loss: 5.19410742e-06
Iter: 832 loss: 5.19374589e-06
Iter: 833 loss: 5.19342302e-06
Iter: 834 loss: 5.19270952e-06
Iter: 835 loss: 5.19241121e-06
Iter: 836 loss: 5.19201058e-06
Iter: 837 loss: 5.19088417e-06
Iter: 838 loss: 5.19619789e-06
Iter: 839 loss: 5.19064906e-06
Iter: 840 loss: 5.18990601e-06
Iter: 841 loss: 5.18991146e-06
Iter: 842 loss: 5.18927209e-06
Iter: 843 loss: 5.18852721e-06
Iter: 844 loss: 5.18847628e-06
Iter: 845 loss: 5.18744127e-06
Iter: 846 loss: 5.1883153e-06
Iter: 847 loss: 5.18686375e-06
Iter: 848 loss: 5.18609477e-06
Iter: 849 loss: 5.18603338e-06
Iter: 850 loss: 5.18553134e-06
Iter: 851 loss: 5.18439265e-06
Iter: 852 loss: 5.19990226e-06
Iter: 853 loss: 5.18431807e-06
Iter: 854 loss: 5.18364504e-06
Iter: 855 loss: 5.18355046e-06
Iter: 856 loss: 5.18286924e-06
Iter: 857 loss: 5.18216166e-06
Iter: 858 loss: 5.18209663e-06
Iter: 859 loss: 5.18114575e-06
Iter: 860 loss: 5.18458455e-06
Iter: 861 loss: 5.18093748e-06
Iter: 862 loss: 5.17992521e-06
Iter: 863 loss: 5.18747902e-06
Iter: 864 loss: 5.17985882e-06
Iter: 865 loss: 5.17931358e-06
Iter: 866 loss: 5.17817307e-06
Iter: 867 loss: 5.1978418e-06
Iter: 868 loss: 5.1781426e-06
Iter: 869 loss: 5.17702119e-06
Iter: 870 loss: 5.18538536e-06
Iter: 871 loss: 5.1769448e-06
Iter: 872 loss: 5.17611397e-06
Iter: 873 loss: 5.18296201e-06
Iter: 874 loss: 5.17610079e-06
Iter: 875 loss: 5.17534818e-06
Iter: 876 loss: 5.17750777e-06
Iter: 877 loss: 5.17516628e-06
Iter: 878 loss: 5.17453054e-06
Iter: 879 loss: 5.17344779e-06
Iter: 880 loss: 5.17342141e-06
Iter: 881 loss: 5.17279477e-06
Iter: 882 loss: 5.17269837e-06
Iter: 883 loss: 5.17202898e-06
Iter: 884 loss: 5.17188437e-06
Iter: 885 loss: 5.17151875e-06
Iter: 886 loss: 5.17060698e-06
Iter: 887 loss: 5.17048284e-06
Iter: 888 loss: 5.16996442e-06
Iter: 889 loss: 5.16876662e-06
Iter: 890 loss: 5.1831712e-06
Iter: 891 loss: 5.16871296e-06
Iter: 892 loss: 5.16810451e-06
Iter: 893 loss: 5.16702266e-06
Iter: 894 loss: 5.1670504e-06
Iter: 895 loss: 5.16694718e-06
Iter: 896 loss: 5.16656382e-06
Iter: 897 loss: 5.16610953e-06
Iter: 898 loss: 5.16507953e-06
Iter: 899 loss: 5.17530952e-06
Iter: 900 loss: 5.1648658e-06
Iter: 901 loss: 5.16367572e-06
Iter: 902 loss: 5.16700493e-06
Iter: 903 loss: 5.16329101e-06
Iter: 904 loss: 5.16227919e-06
Iter: 905 loss: 5.16931686e-06
Iter: 906 loss: 5.16220416e-06
Iter: 907 loss: 5.16121918e-06
Iter: 908 loss: 5.1674624e-06
Iter: 909 loss: 5.16108412e-06
Iter: 910 loss: 5.16046839e-06
Iter: 911 loss: 5.1603356e-06
Iter: 912 loss: 5.15987085e-06
Iter: 913 loss: 5.15898728e-06
Iter: 914 loss: 5.15938973e-06
Iter: 915 loss: 5.15835745e-06
Iter: 916 loss: 5.15760803e-06
Iter: 917 loss: 5.15751572e-06
Iter: 918 loss: 5.15710099e-06
Iter: 919 loss: 5.15615329e-06
Iter: 920 loss: 5.16688488e-06
Iter: 921 loss: 5.15604688e-06
Iter: 922 loss: 5.15502779e-06
Iter: 923 loss: 5.17052558e-06
Iter: 924 loss: 5.15504871e-06
Iter: 925 loss: 5.15412557e-06
Iter: 926 loss: 5.15448937e-06
Iter: 927 loss: 5.15357169e-06
Iter: 928 loss: 5.15277588e-06
Iter: 929 loss: 5.15459942e-06
Iter: 930 loss: 5.1525094e-06
Iter: 931 loss: 5.1514171e-06
Iter: 932 loss: 5.1554e-06
Iter: 933 loss: 5.15116972e-06
Iter: 934 loss: 5.15051306e-06
Iter: 935 loss: 5.149564e-06
Iter: 936 loss: 5.14955e-06
Iter: 937 loss: 5.14849489e-06
Iter: 938 loss: 5.15343436e-06
Iter: 939 loss: 5.14826297e-06
Iter: 940 loss: 5.14742624e-06
Iter: 941 loss: 5.15754027e-06
Iter: 942 loss: 5.14739259e-06
Iter: 943 loss: 5.14670819e-06
Iter: 944 loss: 5.14715475e-06
Iter: 945 loss: 5.14627754e-06
Iter: 946 loss: 5.145459e-06
Iter: 947 loss: 5.14507974e-06
Iter: 948 loss: 5.1447355e-06
Iter: 949 loss: 5.14384192e-06
Iter: 950 loss: 5.14385101e-06
Iter: 951 loss: 5.14310659e-06
Iter: 952 loss: 5.14314e-06
Iter: 953 loss: 5.14250314e-06
Iter: 954 loss: 5.14176099e-06
Iter: 955 loss: 5.14312251e-06
Iter: 956 loss: 5.14136354e-06
Iter: 957 loss: 5.14049498e-06
Iter: 958 loss: 5.14903559e-06
Iter: 959 loss: 5.14043813e-06
Iter: 960 loss: 5.13987925e-06
Iter: 961 loss: 5.13884333e-06
Iter: 962 loss: 5.16289674e-06
Iter: 963 loss: 5.13880286e-06
Iter: 964 loss: 5.13853593e-06
Iter: 965 loss: 5.13824125e-06
Iter: 966 loss: 5.13777513e-06
Iter: 967 loss: 5.13655459e-06
Iter: 968 loss: 5.14654585e-06
Iter: 969 loss: 5.13629493e-06
Iter: 970 loss: 5.13502937e-06
Iter: 971 loss: 5.13849045e-06
Iter: 972 loss: 5.13455279e-06
Iter: 973 loss: 5.13356918e-06
Iter: 974 loss: 5.14600561e-06
Iter: 975 loss: 5.133591e-06
Iter: 976 loss: 5.13267651e-06
Iter: 977 loss: 5.13505256e-06
Iter: 978 loss: 5.1323691e-06
Iter: 979 loss: 5.13168607e-06
Iter: 980 loss: 5.13226223e-06
Iter: 981 loss: 5.13124451e-06
Iter: 982 loss: 5.13039686e-06
Iter: 983 loss: 5.13207942e-06
Iter: 984 loss: 5.13010127e-06
Iter: 985 loss: 5.12926181e-06
Iter: 986 loss: 5.13900113e-06
Iter: 987 loss: 5.12921906e-06
Iter: 988 loss: 5.12865e-06
Iter: 989 loss: 5.12755e-06
Iter: 990 loss: 5.15118563e-06
Iter: 991 loss: 5.12751285e-06
Iter: 992 loss: 5.12674433e-06
Iter: 993 loss: 5.12673e-06
Iter: 994 loss: 5.12601946e-06
Iter: 995 loss: 5.12539464e-06
Iter: 996 loss: 5.12516635e-06
Iter: 997 loss: 5.12433598e-06
Iter: 998 loss: 5.1285283e-06
Iter: 999 loss: 5.12421229e-06
Iter: 1000 loss: 5.12317e-06
Iter: 1001 loss: 5.12677889e-06
Iter: 1002 loss: 5.12291263e-06
Iter: 1003 loss: 5.12228053e-06
Iter: 1004 loss: 5.12088809e-06
Iter: 1005 loss: 5.14753174e-06
Iter: 1006 loss: 5.1208649e-06
Iter: 1007 loss: 5.11978033e-06
Iter: 1008 loss: 5.12997349e-06
Iter: 1009 loss: 5.1197967e-06
Iter: 1010 loss: 5.11890175e-06
Iter: 1011 loss: 5.1259849e-06
Iter: 1012 loss: 5.11886446e-06
Iter: 1013 loss: 5.11807048e-06
Iter: 1014 loss: 5.11771941e-06
Iter: 1015 loss: 5.11738881e-06
Iter: 1016 loss: 5.11649341e-06
Iter: 1017 loss: 5.11954795e-06
Iter: 1018 loss: 5.11624057e-06
Iter: 1019 loss: 5.11546841e-06
Iter: 1020 loss: 5.12168799e-06
Iter: 1021 loss: 5.11543658e-06
Iter: 1022 loss: 5.11465078e-06
Iter: 1023 loss: 5.11519829e-06
Iter: 1024 loss: 5.11424059e-06
Iter: 1025 loss: 5.11348662e-06
Iter: 1026 loss: 5.11379676e-06
Iter: 1027 loss: 5.11301641e-06
Iter: 1028 loss: 5.11215876e-06
Iter: 1029 loss: 5.12351698e-06
Iter: 1030 loss: 5.1121192e-06
Iter: 1031 loss: 5.11157805e-06
Iter: 1032 loss: 5.11057897e-06
Iter: 1033 loss: 5.13245186e-06
Iter: 1034 loss: 5.11061626e-06
Iter: 1035 loss: 5.10988775e-06
Iter: 1036 loss: 5.10982682e-06
Iter: 1037 loss: 5.10927157e-06
Iter: 1038 loss: 5.10821701e-06
Iter: 1039 loss: 5.12735e-06
Iter: 1040 loss: 5.10822974e-06
Iter: 1041 loss: 5.10711197e-06
Iter: 1042 loss: 5.1092793e-06
Iter: 1043 loss: 5.10662449e-06
Iter: 1044 loss: 5.1059933e-06
Iter: 1045 loss: 5.10590098e-06
Iter: 1046 loss: 5.10528162e-06
Iter: 1047 loss: 5.10476457e-06
Iter: 1048 loss: 5.10459e-06
Iter: 1049 loss: 5.10362588e-06
Iter: 1050 loss: 5.10489735e-06
Iter: 1051 loss: 5.10318478e-06
Iter: 1052 loss: 5.10224208e-06
Iter: 1053 loss: 5.11141479e-06
Iter: 1054 loss: 5.10226801e-06
Iter: 1055 loss: 5.10143036e-06
Iter: 1056 loss: 5.10292784e-06
Iter: 1057 loss: 5.10104474e-06
Iter: 1058 loss: 5.10024756e-06
Iter: 1059 loss: 5.10023801e-06
Iter: 1060 loss: 5.09956044e-06
Iter: 1061 loss: 5.0990534e-06
Iter: 1062 loss: 5.09899564e-06
Iter: 1063 loss: 5.09851225e-06
Iter: 1064 loss: 5.09770871e-06
Iter: 1065 loss: 5.0977319e-06
Iter: 1066 loss: 5.09706388e-06
Iter: 1067 loss: 5.09709071e-06
Iter: 1068 loss: 5.09643087e-06
Iter: 1069 loss: 5.09586289e-06
Iter: 1070 loss: 5.09566507e-06
Iter: 1071 loss: 5.09485335e-06
Iter: 1072 loss: 5.09399797e-06
Iter: 1073 loss: 5.09382699e-06
Iter: 1074 loss: 5.09281608e-06
Iter: 1075 loss: 5.09281472e-06
Iter: 1076 loss: 5.09181837e-06
Iter: 1077 loss: 5.09555639e-06
Iter: 1078 loss: 5.0915578e-06
Iter: 1079 loss: 5.09085203e-06
Iter: 1080 loss: 5.08997073e-06
Iter: 1081 loss: 5.08991434e-06
Iter: 1082 loss: 5.08882e-06
Iter: 1083 loss: 5.09962501e-06
Iter: 1084 loss: 5.08876656e-06
Iter: 1085 loss: 5.08798257e-06
Iter: 1086 loss: 5.09423444e-06
Iter: 1087 loss: 5.08794255e-06
Iter: 1088 loss: 5.08734138e-06
Iter: 1089 loss: 5.08704488e-06
Iter: 1090 loss: 5.0868548e-06
Iter: 1091 loss: 5.08600897e-06
Iter: 1092 loss: 5.08934227e-06
Iter: 1093 loss: 5.08589392e-06
Iter: 1094 loss: 5.0849203e-06
Iter: 1095 loss: 5.08641642e-06
Iter: 1096 loss: 5.08440917e-06
Iter: 1097 loss: 5.0837175e-06
Iter: 1098 loss: 5.08549374e-06
Iter: 1099 loss: 5.08342646e-06
Iter: 1100 loss: 5.08236053e-06
Iter: 1101 loss: 5.08678113e-06
Iter: 1102 loss: 5.08221683e-06
Iter: 1103 loss: 5.08152561e-06
Iter: 1104 loss: 5.08040739e-06
Iter: 1105 loss: 5.08036737e-06
Iter: 1106 loss: 5.07929281e-06
Iter: 1107 loss: 5.08157655e-06
Iter: 1108 loss: 5.07891855e-06
Iter: 1109 loss: 5.07793266e-06
Iter: 1110 loss: 5.07796176e-06
Iter: 1111 loss: 5.07733512e-06
Iter: 1112 loss: 5.07648e-06
Iter: 1113 loss: 5.07644518e-06
Iter: 1114 loss: 5.07545246e-06
Iter: 1115 loss: 5.07703953e-06
Iter: 1116 loss: 5.07499954e-06
Iter: 1117 loss: 5.07393861e-06
Iter: 1118 loss: 5.08831272e-06
Iter: 1119 loss: 5.07397453e-06
Iter: 1120 loss: 5.07315917e-06
Iter: 1121 loss: 5.07379809e-06
Iter: 1122 loss: 5.07272898e-06
Iter: 1123 loss: 5.07192271e-06
Iter: 1124 loss: 5.07265395e-06
Iter: 1125 loss: 5.07144023e-06
Iter: 1126 loss: 5.07050754e-06
Iter: 1127 loss: 5.07995719e-06
Iter: 1128 loss: 5.07050299e-06
Iter: 1129 loss: 5.06980359e-06
Iter: 1130 loss: 5.06890046e-06
Iter: 1131 loss: 5.06886227e-06
Iter: 1132 loss: 5.06841161e-06
Iter: 1133 loss: 5.06821e-06
Iter: 1134 loss: 5.06781043e-06
Iter: 1135 loss: 5.06687866e-06
Iter: 1136 loss: 5.0789e-06
Iter: 1137 loss: 5.06682545e-06
Iter: 1138 loss: 5.06562674e-06
Iter: 1139 loss: 5.06655124e-06
Iter: 1140 loss: 5.06489187e-06
Iter: 1141 loss: 5.06431616e-06
Iter: 1142 loss: 5.06419474e-06
Iter: 1143 loss: 5.06345668e-06
Iter: 1144 loss: 5.06259221e-06
Iter: 1145 loss: 5.06247943e-06
Iter: 1146 loss: 5.06145534e-06
Iter: 1147 loss: 5.06286733e-06
Iter: 1148 loss: 5.06097604e-06
Iter: 1149 loss: 5.06007291e-06
Iter: 1150 loss: 5.07291043e-06
Iter: 1151 loss: 5.06008155e-06
Iter: 1152 loss: 5.05931257e-06
Iter: 1153 loss: 5.06053675e-06
Iter: 1154 loss: 5.05895969e-06
Iter: 1155 loss: 5.05818343e-06
Iter: 1156 loss: 5.05821708e-06
Iter: 1157 loss: 5.05760681e-06
Iter: 1158 loss: 5.05671369e-06
Iter: 1159 loss: 5.06666311e-06
Iter: 1160 loss: 5.05666776e-06
Iter: 1161 loss: 5.05597063e-06
Iter: 1162 loss: 5.05582148e-06
Iter: 1163 loss: 5.05534263e-06
Iter: 1164 loss: 5.05471689e-06
Iter: 1165 loss: 5.06287142e-06
Iter: 1166 loss: 5.05469507e-06
Iter: 1167 loss: 5.05403568e-06
Iter: 1168 loss: 5.05379285e-06
Iter: 1169 loss: 5.05341632e-06
Iter: 1170 loss: 5.0527251e-06
Iter: 1171 loss: 5.05182288e-06
Iter: 1172 loss: 5.05173921e-06
Iter: 1173 loss: 5.05063508e-06
Iter: 1174 loss: 5.06329161e-06
Iter: 1175 loss: 5.05061462e-06
Iter: 1176 loss: 5.04973832e-06
Iter: 1177 loss: 5.05784828e-06
Iter: 1178 loss: 5.04972104e-06
Iter: 1179 loss: 5.04912168e-06
Iter: 1180 loss: 5.04773107e-06
Iter: 1181 loss: 5.06092056e-06
Iter: 1182 loss: 5.04753962e-06
Iter: 1183 loss: 5.04646414e-06
Iter: 1184 loss: 5.04643322e-06
Iter: 1185 loss: 5.04560558e-06
Iter: 1186 loss: 5.05235857e-06
Iter: 1187 loss: 5.04555965e-06
Iter: 1188 loss: 5.04504e-06
Iter: 1189 loss: 5.0445442e-06
Iter: 1190 loss: 5.04436048e-06
Iter: 1191 loss: 5.04347327e-06
Iter: 1192 loss: 5.0472845e-06
Iter: 1193 loss: 5.04334957e-06
Iter: 1194 loss: 5.04241962e-06
Iter: 1195 loss: 5.04735135e-06
Iter: 1196 loss: 5.04218497e-06
Iter: 1197 loss: 5.04166292e-06
Iter: 1198 loss: 5.04182208e-06
Iter: 1199 loss: 5.04122e-06
Iter: 1200 loss: 5.04031141e-06
Iter: 1201 loss: 5.04641275e-06
Iter: 1202 loss: 5.04026775e-06
Iter: 1203 loss: 5.03970205e-06
Iter: 1204 loss: 5.03824958e-06
Iter: 1205 loss: 5.05670323e-06
Iter: 1206 loss: 5.03820684e-06
Iter: 1207 loss: 5.03686e-06
Iter: 1208 loss: 5.04441778e-06
Iter: 1209 loss: 5.03668616e-06
Iter: 1210 loss: 5.03614638e-06
Iter: 1211 loss: 5.03600768e-06
Iter: 1212 loss: 5.03559113e-06
Iter: 1213 loss: 5.0347262e-06
Iter: 1214 loss: 5.05446405e-06
Iter: 1215 loss: 5.03476e-06
Iter: 1216 loss: 5.03379215e-06
Iter: 1217 loss: 5.03425781e-06
Iter: 1218 loss: 5.03312094e-06
Iter: 1219 loss: 5.03238925e-06
Iter: 1220 loss: 5.03228875e-06
Iter: 1221 loss: 5.03163255e-06
Iter: 1222 loss: 5.0313547e-06
Iter: 1223 loss: 5.03105275e-06
Iter: 1224 loss: 5.02997136e-06
Iter: 1225 loss: 5.03103229e-06
Iter: 1226 loss: 5.02950661e-06
Iter: 1227 loss: 5.02845842e-06
Iter: 1228 loss: 5.04073523e-06
Iter: 1229 loss: 5.02845887e-06
Iter: 1230 loss: 5.02777675e-06
Iter: 1231 loss: 5.02877856e-06
Iter: 1232 loss: 5.02742932e-06
Iter: 1233 loss: 5.02696275e-06
Iter: 1234 loss: 5.03149158e-06
Iter: 1235 loss: 5.02691955e-06
Iter: 1236 loss: 5.02640296e-06
Iter: 1237 loss: 5.02513922e-06
Iter: 1238 loss: 5.04502623e-06
Iter: 1239 loss: 5.02515059e-06
Iter: 1240 loss: 5.02410967e-06
Iter: 1241 loss: 5.02723969e-06
Iter: 1242 loss: 5.023835e-06
Iter: 1243 loss: 5.02291959e-06
Iter: 1244 loss: 5.02952025e-06
Iter: 1245 loss: 5.02287139e-06
Iter: 1246 loss: 5.02186685e-06
Iter: 1247 loss: 5.02488092e-06
Iter: 1248 loss: 5.02153625e-06
Iter: 1249 loss: 5.02087141e-06
Iter: 1250 loss: 5.01967043e-06
Iter: 1251 loss: 5.01968225e-06
Iter: 1252 loss: 5.0188346e-06
Iter: 1253 loss: 5.01883505e-06
Iter: 1254 loss: 5.01801287e-06
Iter: 1255 loss: 5.02086277e-06
Iter: 1256 loss: 5.0178196e-06
Iter: 1257 loss: 5.01712293e-06
Iter: 1258 loss: 5.01694376e-06
Iter: 1259 loss: 5.01652812e-06
Iter: 1260 loss: 5.01574641e-06
Iter: 1261 loss: 5.02336161e-06
Iter: 1262 loss: 5.01574505e-06
Iter: 1263 loss: 5.01499107e-06
Iter: 1264 loss: 5.01557e-06
Iter: 1265 loss: 5.01456134e-06
Iter: 1266 loss: 5.0136855e-06
Iter: 1267 loss: 5.01693739e-06
Iter: 1268 loss: 5.01347085e-06
Iter: 1269 loss: 5.01261457e-06
Iter: 1270 loss: 5.01594332e-06
Iter: 1271 loss: 5.01238037e-06
Iter: 1272 loss: 5.01180966e-06
Iter: 1273 loss: 5.01069235e-06
Iter: 1274 loss: 5.01074101e-06
Iter: 1275 loss: 5.00967826e-06
Iter: 1276 loss: 5.01462819e-06
Iter: 1277 loss: 5.00947544e-06
Iter: 1278 loss: 5.00857504e-06
Iter: 1279 loss: 5.02079774e-06
Iter: 1280 loss: 5.00855413e-06
Iter: 1281 loss: 5.00780516e-06
Iter: 1282 loss: 5.0067415e-06
Iter: 1283 loss: 5.00672877e-06
Iter: 1284 loss: 5.00573606e-06
Iter: 1285 loss: 5.00712804e-06
Iter: 1286 loss: 5.00527858e-06
Iter: 1287 loss: 5.00434726e-06
Iter: 1288 loss: 5.00437e-06
Iter: 1289 loss: 5.0037288e-06
Iter: 1290 loss: 5.00354963e-06
Iter: 1291 loss: 5.00320675e-06
Iter: 1292 loss: 5.002531e-06
Iter: 1293 loss: 5.00495389e-06
Iter: 1294 loss: 5.00228e-06
Iter: 1295 loss: 5.0015251e-06
Iter: 1296 loss: 5.00499755e-06
Iter: 1297 loss: 5.00139231e-06
Iter: 1298 loss: 5.00066471e-06
Iter: 1299 loss: 5.00095666e-06
Iter: 1300 loss: 5.00013675e-06
Iter: 1301 loss: 4.99933685e-06
Iter: 1302 loss: 5.00781425e-06
Iter: 1303 loss: 4.99930047e-06
Iter: 1304 loss: 4.99873659e-06
Iter: 1305 loss: 4.99782436e-06
Iter: 1306 loss: 4.99784437e-06
Iter: 1307 loss: 4.99671933e-06
Iter: 1308 loss: 4.99686848e-06
Iter: 1309 loss: 4.99584257e-06
Iter: 1310 loss: 4.99544058e-06
Iter: 1311 loss: 4.99518501e-06
Iter: 1312 loss: 4.99458e-06
Iter: 1313 loss: 4.993939e-06
Iter: 1314 loss: 4.99381849e-06
Iter: 1315 loss: 4.99289854e-06
Iter: 1316 loss: 4.99288808e-06
Iter: 1317 loss: 4.99215594e-06
Iter: 1318 loss: 4.99132148e-06
Iter: 1319 loss: 4.99129146e-06
Iter: 1320 loss: 4.99042881e-06
Iter: 1321 loss: 4.99072939e-06
Iter: 1322 loss: 4.98982126e-06
Iter: 1323 loss: 4.9890541e-06
Iter: 1324 loss: 4.99053385e-06
Iter: 1325 loss: 4.98868758e-06
Iter: 1326 loss: 4.98786767e-06
Iter: 1327 loss: 4.99466114e-06
Iter: 1328 loss: 4.98771897e-06
Iter: 1329 loss: 4.98703366e-06
Iter: 1330 loss: 4.98792133e-06
Iter: 1331 loss: 4.98670033e-06
Iter: 1332 loss: 4.98598911e-06
Iter: 1333 loss: 4.99024554e-06
Iter: 1334 loss: 4.98589816e-06
Iter: 1335 loss: 4.98512827e-06
Iter: 1336 loss: 4.98463487e-06
Iter: 1337 loss: 4.98436521e-06
Iter: 1338 loss: 4.9834739e-06
Iter: 1339 loss: 4.9842356e-06
Iter: 1340 loss: 4.98298414e-06
Iter: 1341 loss: 4.9819273e-06
Iter: 1342 loss: 4.9844366e-06
Iter: 1343 loss: 4.98152622e-06
Iter: 1344 loss: 4.98062218e-06
Iter: 1345 loss: 4.98058489e-06
Iter: 1346 loss: 4.98010377e-06
Iter: 1347 loss: 4.97911788e-06
Iter: 1348 loss: 4.99534508e-06
Iter: 1349 loss: 4.97910241e-06
Iter: 1350 loss: 4.97807332e-06
Iter: 1351 loss: 4.98357031e-06
Iter: 1352 loss: 4.97797055e-06
Iter: 1353 loss: 4.97712062e-06
Iter: 1354 loss: 4.9882e-06
Iter: 1355 loss: 4.97713609e-06
Iter: 1356 loss: 4.97653855e-06
Iter: 1357 loss: 4.97542351e-06
Iter: 1358 loss: 5.00012e-06
Iter: 1359 loss: 4.97545716e-06
Iter: 1360 loss: 4.97468318e-06
Iter: 1361 loss: 4.97466726e-06
Iter: 1362 loss: 4.97397104e-06
Iter: 1363 loss: 4.97439123e-06
Iter: 1364 loss: 4.97356859e-06
Iter: 1365 loss: 4.97266137e-06
Iter: 1366 loss: 4.97569727e-06
Iter: 1367 loss: 4.97247947e-06
Iter: 1368 loss: 4.97157498e-06
Iter: 1369 loss: 4.97514338e-06
Iter: 1370 loss: 4.97144538e-06
Iter: 1371 loss: 4.97084102e-06
Iter: 1372 loss: 4.97009478e-06
Iter: 1373 loss: 4.97008e-06
Iter: 1374 loss: 4.96902157e-06
Iter: 1375 loss: 4.9704754e-06
Iter: 1376 loss: 4.96852181e-06
Iter: 1377 loss: 4.96779467e-06
Iter: 1378 loss: 4.96772736e-06
Iter: 1379 loss: 4.96715893e-06
Iter: 1380 loss: 4.96645453e-06
Iter: 1381 loss: 4.96631492e-06
Iter: 1382 loss: 4.96553366e-06
Iter: 1383 loss: 4.96565417e-06
Iter: 1384 loss: 4.96492885e-06
Iter: 1385 loss: 4.96411485e-06
Iter: 1386 loss: 4.96406892e-06
Iter: 1387 loss: 4.96329358e-06
Iter: 1388 loss: 4.96312623e-06
Iter: 1389 loss: 4.96266239e-06
Iter: 1390 loss: 4.96194707e-06
Iter: 1391 loss: 4.964e-06
Iter: 1392 loss: 4.9616965e-06
Iter: 1393 loss: 4.96089251e-06
Iter: 1394 loss: 4.9658488e-06
Iter: 1395 loss: 4.96078928e-06
Iter: 1396 loss: 4.96019311e-06
Iter: 1397 loss: 4.96125813e-06
Iter: 1398 loss: 4.95991389e-06
Iter: 1399 loss: 4.95913764e-06
Iter: 1400 loss: 4.9620262e-06
Iter: 1401 loss: 4.95899667e-06
Iter: 1402 loss: 4.95826407e-06
Iter: 1403 loss: 4.95758513e-06
Iter: 1404 loss: 4.95737095e-06
Iter: 1405 loss: 4.95631321e-06
Iter: 1406 loss: 4.95803533e-06
Iter: 1407 loss: 4.95584618e-06
Iter: 1408 loss: 4.95502036e-06
Iter: 1409 loss: 4.96450639e-06
Iter: 1410 loss: 4.95495487e-06
Iter: 1411 loss: 4.95408131e-06
Iter: 1412 loss: 4.95559743e-06
Iter: 1413 loss: 4.95362565e-06
Iter: 1414 loss: 4.95306131e-06
Iter: 1415 loss: 4.95246331e-06
Iter: 1416 loss: 4.95237873e-06
Iter: 1417 loss: 4.95128734e-06
Iter: 1418 loss: 4.95741733e-06
Iter: 1419 loss: 4.9511923e-06
Iter: 1420 loss: 4.95017e-06
Iter: 1421 loss: 4.95717e-06
Iter: 1422 loss: 4.95007134e-06
Iter: 1423 loss: 4.94950473e-06
Iter: 1424 loss: 4.94862707e-06
Iter: 1425 loss: 4.94862616e-06
Iter: 1426 loss: 4.94780306e-06
Iter: 1427 loss: 4.94775713e-06
Iter: 1428 loss: 4.94719279e-06
Iter: 1429 loss: 4.94776395e-06
Iter: 1430 loss: 4.94684355e-06
Iter: 1431 loss: 4.94625692e-06
Iter: 1432 loss: 4.95060067e-06
Iter: 1433 loss: 4.94616961e-06
Iter: 1434 loss: 4.94563483e-06
Iter: 1435 loss: 4.94527285e-06
Iter: 1436 loss: 4.9450623e-06
Iter: 1437 loss: 4.94426331e-06
Iter: 1438 loss: 4.94465621e-06
Iter: 1439 loss: 4.94365759e-06
Iter: 1440 loss: 4.94270353e-06
Iter: 1441 loss: 4.94594497e-06
Iter: 1442 loss: 4.94245933e-06
Iter: 1443 loss: 4.94157939e-06
Iter: 1444 loss: 4.94158394e-06
Iter: 1445 loss: 4.94109463e-06
Iter: 1446 loss: 4.93995731e-06
Iter: 1447 loss: 4.95730183e-06
Iter: 1448 loss: 4.93986499e-06
Iter: 1449 loss: 4.93896596e-06
Iter: 1450 loss: 4.94653705e-06
Iter: 1451 loss: 4.93895413e-06
Iter: 1452 loss: 4.93825746e-06
Iter: 1453 loss: 4.94458e-06
Iter: 1454 loss: 4.93819971e-06
Iter: 1455 loss: 4.93756579e-06
Iter: 1456 loss: 4.93651942e-06
Iter: 1457 loss: 4.93651e-06
Iter: 1458 loss: 4.93571179e-06
Iter: 1459 loss: 4.94697315e-06
Iter: 1460 loss: 4.9356986e-06
Iter: 1461 loss: 4.93490916e-06
Iter: 1462 loss: 4.93589141e-06
Iter: 1463 loss: 4.93456537e-06
Iter: 1464 loss: 4.93381094e-06
Iter: 1465 loss: 4.93813059e-06
Iter: 1466 loss: 4.93373955e-06
Iter: 1467 loss: 4.9330647e-06
Iter: 1468 loss: 4.93383141e-06
Iter: 1469 loss: 4.93262723e-06
Iter: 1470 loss: 4.93199514e-06
Iter: 1471 loss: 4.93193238e-06
Iter: 1472 loss: 4.93142215e-06
Iter: 1473 loss: 4.93043808e-06
Iter: 1474 loss: 4.93095195e-06
Iter: 1475 loss: 4.92977233e-06
Iter: 1476 loss: 4.92904837e-06
Iter: 1477 loss: 4.92899562e-06
Iter: 1478 loss: 4.9283e-06
Iter: 1479 loss: 4.9283e-06
Iter: 1480 loss: 4.92775234e-06
Iter: 1481 loss: 4.92702293e-06
Iter: 1482 loss: 4.9260334e-06
Iter: 1483 loss: 4.92600248e-06
Iter: 1484 loss: 4.92555955e-06
Iter: 1485 loss: 4.92531217e-06
Iter: 1486 loss: 4.92470417e-06
Iter: 1487 loss: 4.92436084e-06
Iter: 1488 loss: 4.9241271e-06
Iter: 1489 loss: 4.9234086e-06
Iter: 1490 loss: 4.9247983e-06
Iter: 1491 loss: 4.92314757e-06
Iter: 1492 loss: 4.92240906e-06
Iter: 1493 loss: 4.93105745e-06
Iter: 1494 loss: 4.92235631e-06
Iter: 1495 loss: 4.92187519e-06
Iter: 1496 loss: 4.92176787e-06
Iter: 1497 loss: 4.92145227e-06
Iter: 1498 loss: 4.9205496e-06
Iter: 1499 loss: 4.92397703e-06
Iter: 1500 loss: 4.92029449e-06
Iter: 1501 loss: 4.91962101e-06
Iter: 1502 loss: 4.91936225e-06
Iter: 1503 loss: 4.91895662e-06
Iter: 1504 loss: 4.91798073e-06
Iter: 1505 loss: 4.91924675e-06
Iter: 1506 loss: 4.91748415e-06
Iter: 1507 loss: 4.91663377e-06
Iter: 1508 loss: 4.92544496e-06
Iter: 1509 loss: 4.91658557e-06
Iter: 1510 loss: 4.9158175e-06
Iter: 1511 loss: 4.91892843e-06
Iter: 1512 loss: 4.91563424e-06
Iter: 1513 loss: 4.91508399e-06
Iter: 1514 loss: 4.91420269e-06
Iter: 1515 loss: 4.91418041e-06
Iter: 1516 loss: 4.9133223e-06
Iter: 1517 loss: 4.91897481e-06
Iter: 1518 loss: 4.91320179e-06
Iter: 1519 loss: 4.91215678e-06
Iter: 1520 loss: 4.91765331e-06
Iter: 1521 loss: 4.91198716e-06
Iter: 1522 loss: 4.91139326e-06
Iter: 1523 loss: 4.91070659e-06
Iter: 1524 loss: 4.91061701e-06
Iter: 1525 loss: 4.90984166e-06
Iter: 1526 loss: 4.90983894e-06
Iter: 1527 loss: 4.90918637e-06
Iter: 1528 loss: 4.90980574e-06
Iter: 1529 loss: 4.90884486e-06
Iter: 1530 loss: 4.90829188e-06
Iter: 1531 loss: 4.91318269e-06
Iter: 1532 loss: 4.90829734e-06
Iter: 1533 loss: 4.90780258e-06
Iter: 1534 loss: 4.90685443e-06
Iter: 1535 loss: 4.92355821e-06
Iter: 1536 loss: 4.9068276e-06
Iter: 1537 loss: 4.9057353e-06
Iter: 1538 loss: 4.91144601e-06
Iter: 1539 loss: 4.90558068e-06
Iter: 1540 loss: 4.90471848e-06
Iter: 1541 loss: 4.90532966e-06
Iter: 1542 loss: 4.90421644e-06
Iter: 1543 loss: 4.90316097e-06
Iter: 1544 loss: 4.9184282e-06
Iter: 1545 loss: 4.90314051e-06
Iter: 1546 loss: 4.90249158e-06
Iter: 1547 loss: 4.90174625e-06
Iter: 1548 loss: 4.90164894e-06
Iter: 1549 loss: 4.90074399e-06
Iter: 1550 loss: 4.90232287e-06
Iter: 1551 loss: 4.90037655e-06
Iter: 1552 loss: 4.89960939e-06
Iter: 1553 loss: 4.8996294e-06
Iter: 1554 loss: 4.899126e-06
Iter: 1555 loss: 4.89806644e-06
Iter: 1556 loss: 4.91780565e-06
Iter: 1557 loss: 4.89806825e-06
Iter: 1558 loss: 4.89724425e-06
Iter: 1559 loss: 4.89724e-06
Iter: 1560 loss: 4.896473e-06
Iter: 1561 loss: 4.89697868e-06
Iter: 1562 loss: 4.89602735e-06
Iter: 1563 loss: 4.89511331e-06
Iter: 1564 loss: 4.89926833e-06
Iter: 1565 loss: 4.89491731e-06
Iter: 1566 loss: 4.89409922e-06
Iter: 1567 loss: 4.89461718e-06
Iter: 1568 loss: 4.89350577e-06
Iter: 1569 loss: 4.89260765e-06
Iter: 1570 loss: 4.89315335e-06
Iter: 1571 loss: 4.89213744e-06
Iter: 1572 loss: 4.89111244e-06
Iter: 1573 loss: 4.89458944e-06
Iter: 1574 loss: 4.8908546e-06
Iter: 1575 loss: 4.89027616e-06
Iter: 1576 loss: 4.89029162e-06
Iter: 1577 loss: 4.88976775e-06
Iter: 1578 loss: 4.8889915e-06
Iter: 1579 loss: 4.88891965e-06
Iter: 1580 loss: 4.88797832e-06
Iter: 1581 loss: 4.88939349e-06
Iter: 1582 loss: 4.88753176e-06
Iter: 1583 loss: 4.88689966e-06
Iter: 1584 loss: 4.88685419e-06
Iter: 1585 loss: 4.886223e-06
Iter: 1586 loss: 4.88550404e-06
Iter: 1587 loss: 4.88542855e-06
Iter: 1588 loss: 4.88467322e-06
Iter: 1589 loss: 4.88822661e-06
Iter: 1590 loss: 4.88447495e-06
Iter: 1591 loss: 4.88357955e-06
Iter: 1592 loss: 4.88808109e-06
Iter: 1593 loss: 4.88348633e-06
Iter: 1594 loss: 4.88277328e-06
Iter: 1595 loss: 4.88432715e-06
Iter: 1596 loss: 4.88258547e-06
Iter: 1597 loss: 4.88175738e-06
Iter: 1598 loss: 4.88320802e-06
Iter: 1599 loss: 4.88148817e-06
Iter: 1600 loss: 4.88083106e-06
Iter: 1601 loss: 4.88056367e-06
Iter: 1602 loss: 4.88019396e-06
Iter: 1603 loss: 4.87925809e-06
Iter: 1604 loss: 4.881927e-06
Iter: 1605 loss: 4.87886473e-06
Iter: 1606 loss: 4.87799389e-06
Iter: 1607 loss: 4.88491833e-06
Iter: 1608 loss: 4.87794841e-06
Iter: 1609 loss: 4.87705211e-06
Iter: 1610 loss: 4.87961279e-06
Iter: 1611 loss: 4.87674379e-06
Iter: 1612 loss: 4.87614307e-06
Iter: 1613 loss: 4.87518e-06
Iter: 1614 loss: 4.87511079e-06
Iter: 1615 loss: 4.87432408e-06
Iter: 1616 loss: 4.87433545e-06
Iter: 1617 loss: 4.87357238e-06
Iter: 1618 loss: 4.87513535e-06
Iter: 1619 loss: 4.87327e-06
Iter: 1620 loss: 4.8727411e-06
Iter: 1621 loss: 4.87208263e-06
Iter: 1622 loss: 4.87201851e-06
Iter: 1623 loss: 4.87113903e-06
Iter: 1624 loss: 4.87114357e-06
Iter: 1625 loss: 4.87054649e-06
Iter: 1626 loss: 4.87077432e-06
Iter: 1627 loss: 4.87011584e-06
Iter: 1628 loss: 4.86940326e-06
Iter: 1629 loss: 4.8739148e-06
Iter: 1630 loss: 4.86931913e-06
Iter: 1631 loss: 4.8687707e-06
Iter: 1632 loss: 4.86804493e-06
Iter: 1633 loss: 4.86801628e-06
Iter: 1634 loss: 4.86697718e-06
Iter: 1635 loss: 4.86991848e-06
Iter: 1636 loss: 4.86663885e-06
Iter: 1637 loss: 4.8658294e-06
Iter: 1638 loss: 4.87319812e-06
Iter: 1639 loss: 4.86577028e-06
Iter: 1640 loss: 4.86503723e-06
Iter: 1641 loss: 4.86795534e-06
Iter: 1642 loss: 4.86487534e-06
Iter: 1643 loss: 4.86424142e-06
Iter: 1644 loss: 4.86321096e-06
Iter: 1645 loss: 4.86321096e-06
Iter: 1646 loss: 4.86241606e-06
Iter: 1647 loss: 4.86240833e-06
Iter: 1648 loss: 4.8617685e-06
Iter: 1649 loss: 4.86381441e-06
Iter: 1650 loss: 4.8616057e-06
Iter: 1651 loss: 4.8609736e-06
Iter: 1652 loss: 4.86003682e-06
Iter: 1653 loss: 4.86002682e-06
Iter: 1654 loss: 4.85947112e-06
Iter: 1655 loss: 4.85934697e-06
Iter: 1656 loss: 4.85877717e-06
Iter: 1657 loss: 4.85858163e-06
Iter: 1658 loss: 4.85826695e-06
Iter: 1659 loss: 4.8576303e-06
Iter: 1660 loss: 4.86422505e-06
Iter: 1661 loss: 4.85758801e-06
Iter: 1662 loss: 4.85708915e-06
Iter: 1663 loss: 4.85628152e-06
Iter: 1664 loss: 4.85626515e-06
Iter: 1665 loss: 4.85527153e-06
Iter: 1666 loss: 4.85694727e-06
Iter: 1667 loss: 4.85480359e-06
Iter: 1668 loss: 4.85372584e-06
Iter: 1669 loss: 4.85965347e-06
Iter: 1670 loss: 4.85360215e-06
Iter: 1671 loss: 4.85272585e-06
Iter: 1672 loss: 4.86257522e-06
Iter: 1673 loss: 4.8527e-06
Iter: 1674 loss: 4.85215969e-06
Iter: 1675 loss: 4.85120745e-06
Iter: 1676 loss: 4.85117562e-06
Iter: 1677 loss: 4.85020155e-06
Iter: 1678 loss: 4.85522924e-06
Iter: 1679 loss: 4.85007195e-06
Iter: 1680 loss: 4.84923e-06
Iter: 1681 loss: 4.8601205e-06
Iter: 1682 loss: 4.84923839e-06
Iter: 1683 loss: 4.84871271e-06
Iter: 1684 loss: 4.84772863e-06
Iter: 1685 loss: 4.86930912e-06
Iter: 1686 loss: 4.84774409e-06
Iter: 1687 loss: 4.84681686e-06
Iter: 1688 loss: 4.85724559e-06
Iter: 1689 loss: 4.84675911e-06
Iter: 1690 loss: 4.84581869e-06
Iter: 1691 loss: 4.8481711e-06
Iter: 1692 loss: 4.84541761e-06
Iter: 1693 loss: 4.84482462e-06
Iter: 1694 loss: 4.84882912e-06
Iter: 1695 loss: 4.84473458e-06
Iter: 1696 loss: 4.84409247e-06
Iter: 1697 loss: 4.84346583e-06
Iter: 1698 loss: 4.84330849e-06
Iter: 1699 loss: 4.84234033e-06
Iter: 1700 loss: 4.84346037e-06
Iter: 1701 loss: 4.8419106e-06
Iter: 1702 loss: 4.84085695e-06
Iter: 1703 loss: 4.84643715e-06
Iter: 1704 loss: 4.84069733e-06
Iter: 1705 loss: 4.84008524e-06
Iter: 1706 loss: 4.84893326e-06
Iter: 1707 loss: 4.84004795e-06
Iter: 1708 loss: 4.83947088e-06
Iter: 1709 loss: 4.83891654e-06
Iter: 1710 loss: 4.83877739e-06
Iter: 1711 loss: 4.83792201e-06
Iter: 1712 loss: 4.83813756e-06
Iter: 1713 loss: 4.83728491e-06
Iter: 1714 loss: 4.83646227e-06
Iter: 1715 loss: 4.83644271e-06
Iter: 1716 loss: 4.83574695e-06
Iter: 1717 loss: 4.83518352e-06
Iter: 1718 loss: 4.83499662e-06
Iter: 1719 loss: 4.83423264e-06
Iter: 1720 loss: 4.83613712e-06
Iter: 1721 loss: 4.83391204e-06
Iter: 1722 loss: 4.83286158e-06
Iter: 1723 loss: 4.84159955e-06
Iter: 1724 loss: 4.83285203e-06
Iter: 1725 loss: 4.83229815e-06
Iter: 1726 loss: 4.83329e-06
Iter: 1727 loss: 4.83206531e-06
Iter: 1728 loss: 4.83134318e-06
Iter: 1729 loss: 4.83241456e-06
Iter: 1730 loss: 4.83100303e-06
Iter: 1731 loss: 4.83033728e-06
Iter: 1732 loss: 4.82970881e-06
Iter: 1733 loss: 4.82956284e-06
Iter: 1734 loss: 4.82838e-06
Iter: 1735 loss: 4.83291387e-06
Iter: 1736 loss: 4.82813857e-06
Iter: 1737 loss: 4.82732412e-06
Iter: 1738 loss: 4.83751955e-06
Iter: 1739 loss: 4.82728456e-06
Iter: 1740 loss: 4.8265465e-06
Iter: 1741 loss: 4.8273414e-06
Iter: 1742 loss: 4.82609448e-06
Iter: 1743 loss: 4.82537962e-06
Iter: 1744 loss: 4.82489122e-06
Iter: 1745 loss: 4.82462565e-06
Iter: 1746 loss: 4.82407722e-06
Iter: 1747 loss: 4.82401356e-06
Iter: 1748 loss: 4.82343421e-06
Iter: 1749 loss: 4.82296309e-06
Iter: 1750 loss: 4.82274936e-06
Iter: 1751 loss: 4.82178621e-06
Iter: 1752 loss: 4.8220254e-06
Iter: 1753 loss: 4.82107316e-06
Iter: 1754 loss: 4.82045e-06
Iter: 1755 loss: 4.82035739e-06
Iter: 1756 loss: 4.81979714e-06
Iter: 1757 loss: 4.81905863e-06
Iter: 1758 loss: 4.81902316e-06
Iter: 1759 loss: 4.8179113e-06
Iter: 1760 loss: 4.82580754e-06
Iter: 1761 loss: 4.8178108e-06
Iter: 1762 loss: 4.81716779e-06
Iter: 1763 loss: 4.81663119e-06
Iter: 1764 loss: 4.81642e-06
Iter: 1765 loss: 4.81544339e-06
Iter: 1766 loss: 4.81718598e-06
Iter: 1767 loss: 4.814965e-06
Iter: 1768 loss: 4.81397274e-06
Iter: 1769 loss: 4.8243428e-06
Iter: 1770 loss: 4.81396046e-06
Iter: 1771 loss: 4.81316329e-06
Iter: 1772 loss: 4.81697e-06
Iter: 1773 loss: 4.8130305e-06
Iter: 1774 loss: 4.81244842e-06
Iter: 1775 loss: 4.81164898e-06
Iter: 1776 loss: 4.81161e-06
Iter: 1777 loss: 4.81067491e-06
Iter: 1778 loss: 4.81650477e-06
Iter: 1779 loss: 4.81052757e-06
Iter: 1780 loss: 4.80948302e-06
Iter: 1781 loss: 4.8145057e-06
Iter: 1782 loss: 4.80929975e-06
Iter: 1783 loss: 4.80875951e-06
Iter: 1784 loss: 4.80830477e-06
Iter: 1785 loss: 4.80806193e-06
Iter: 1786 loss: 4.80741437e-06
Iter: 1787 loss: 4.80741892e-06
Iter: 1788 loss: 4.80669632e-06
Iter: 1789 loss: 4.80683957e-06
Iter: 1790 loss: 4.80622384e-06
Iter: 1791 loss: 4.80564722e-06
Iter: 1792 loss: 4.81181769e-06
Iter: 1793 loss: 4.8056063e-06
Iter: 1794 loss: 4.80515473e-06
Iter: 1795 loss: 4.80415474e-06
Iter: 1796 loss: 4.82277028e-06
Iter: 1797 loss: 4.80411654e-06
Iter: 1798 loss: 4.80306608e-06
Iter: 1799 loss: 4.80666404e-06
Iter: 1800 loss: 4.80276822e-06
Iter: 1801 loss: 4.80187873e-06
Iter: 1802 loss: 4.80790095e-06
Iter: 1803 loss: 4.80176095e-06
Iter: 1804 loss: 4.80097242e-06
Iter: 1805 loss: 4.80741619e-06
Iter: 1806 loss: 4.80089557e-06
Iter: 1807 loss: 4.8003003e-06
Iter: 1808 loss: 4.79984055e-06
Iter: 1809 loss: 4.79967184e-06
Iter: 1810 loss: 4.79878418e-06
Iter: 1811 loss: 4.79934261e-06
Iter: 1812 loss: 4.79822256e-06
Iter: 1813 loss: 4.79735809e-06
Iter: 1814 loss: 4.7973308e-06
Iter: 1815 loss: 4.79681876e-06
Iter: 1816 loss: 4.79601249e-06
Iter: 1817 loss: 4.79597929e-06
Iter: 1818 loss: 4.79520804e-06
Iter: 1819 loss: 4.80071458e-06
Iter: 1820 loss: 4.79509799e-06
Iter: 1821 loss: 4.79417577e-06
Iter: 1822 loss: 4.79742266e-06
Iter: 1823 loss: 4.79394475e-06
Iter: 1824 loss: 4.79332175e-06
Iter: 1825 loss: 4.79530945e-06
Iter: 1826 loss: 4.79313803e-06
Iter: 1827 loss: 4.79236951e-06
Iter: 1828 loss: 4.79275832e-06
Iter: 1829 loss: 4.79189384e-06
Iter: 1830 loss: 4.79118353e-06
Iter: 1831 loss: 4.79054052e-06
Iter: 1832 loss: 4.79034179e-06
Iter: 1833 loss: 4.78919173e-06
Iter: 1834 loss: 4.79823166e-06
Iter: 1835 loss: 4.78912625e-06
Iter: 1836 loss: 4.78838501e-06
Iter: 1837 loss: 4.79724213e-06
Iter: 1838 loss: 4.7883741e-06
Iter: 1839 loss: 4.78780839e-06
Iter: 1840 loss: 4.78750826e-06
Iter: 1841 loss: 4.78725042e-06
Iter: 1842 loss: 4.78647416e-06
Iter: 1843 loss: 4.78697439e-06
Iter: 1844 loss: 4.78596712e-06
Iter: 1845 loss: 4.78538823e-06
Iter: 1846 loss: 4.78534912e-06
Iter: 1847 loss: 4.78479024e-06
Iter: 1848 loss: 4.7839967e-06
Iter: 1849 loss: 4.78393304e-06
Iter: 1850 loss: 4.78305e-06
Iter: 1851 loss: 4.78536094e-06
Iter: 1852 loss: 4.78274251e-06
Iter: 1853 loss: 4.78200491e-06
Iter: 1854 loss: 4.78198763e-06
Iter: 1855 loss: 4.78150469e-06
Iter: 1856 loss: 4.78090078e-06
Iter: 1857 loss: 4.78088805e-06
Iter: 1858 loss: 4.77988124e-06
Iter: 1859 loss: 4.78558559e-06
Iter: 1860 loss: 4.77974299e-06
Iter: 1861 loss: 4.77914909e-06
Iter: 1862 loss: 4.778517e-06
Iter: 1863 loss: 4.7784456e-06
Iter: 1864 loss: 4.77746516e-06
Iter: 1865 loss: 4.7797821e-06
Iter: 1866 loss: 4.77712092e-06
Iter: 1867 loss: 4.77648791e-06
Iter: 1868 loss: 4.77649701e-06
Iter: 1869 loss: 4.77585945e-06
Iter: 1870 loss: 4.77613048e-06
Iter: 1871 loss: 4.77546109e-06
Iter: 1872 loss: 4.77479171e-06
Iter: 1873 loss: 4.7745425e-06
Iter: 1874 loss: 4.77416688e-06
Iter: 1875 loss: 4.77327831e-06
Iter: 1876 loss: 4.77981212e-06
Iter: 1877 loss: 4.7732392e-06
Iter: 1878 loss: 4.77233561e-06
Iter: 1879 loss: 4.77449339e-06
Iter: 1880 loss: 4.7719891e-06
Iter: 1881 loss: 4.77132e-06
Iter: 1882 loss: 4.77080948e-06
Iter: 1883 loss: 4.77055619e-06
Iter: 1884 loss: 4.77009189e-06
Iter: 1885 loss: 4.77002141e-06
Iter: 1886 loss: 4.76955393e-06
Iter: 1887 loss: 4.76919649e-06
Iter: 1888 loss: 4.76903278e-06
Iter: 1889 loss: 4.76839114e-06
Iter: 1890 loss: 4.77353024e-06
Iter: 1891 loss: 4.76833247e-06
Iter: 1892 loss: 4.7678368e-06
Iter: 1893 loss: 4.76711102e-06
Iter: 1894 loss: 4.76705463e-06
Iter: 1895 loss: 4.76621335e-06
Iter: 1896 loss: 4.76748664e-06
Iter: 1897 loss: 4.76581408e-06
Iter: 1898 loss: 4.76484365e-06
Iter: 1899 loss: 4.77069534e-06
Iter: 1900 loss: 4.76465902e-06
Iter: 1901 loss: 4.7638e-06
Iter: 1902 loss: 4.77079402e-06
Iter: 1903 loss: 4.76376272e-06
Iter: 1904 loss: 4.76316745e-06
Iter: 1905 loss: 4.7627841e-06
Iter: 1906 loss: 4.76262449e-06
Iter: 1907 loss: 4.76169e-06
Iter: 1908 loss: 4.76267223e-06
Iter: 1909 loss: 4.76118294e-06
Iter: 1910 loss: 4.76033119e-06
Iter: 1911 loss: 4.760308e-06
Iter: 1912 loss: 4.75984552e-06
Iter: 1913 loss: 4.7589574e-06
Iter: 1914 loss: 4.75897286e-06
Iter: 1915 loss: 4.75829165e-06
Iter: 1916 loss: 4.76669311e-06
Iter: 1917 loss: 4.75826619e-06
Iter: 1918 loss: 4.75752768e-06
Iter: 1919 loss: 4.75906381e-06
Iter: 1920 loss: 4.75713614e-06
Iter: 1921 loss: 4.75657816e-06
Iter: 1922 loss: 4.7583153e-06
Iter: 1923 loss: 4.75631805e-06
Iter: 1924 loss: 4.75563911e-06
Iter: 1925 loss: 4.75681372e-06
Iter: 1926 loss: 4.75537718e-06
Iter: 1927 loss: 4.75471e-06
Iter: 1928 loss: 4.75397655e-06
Iter: 1929 loss: 4.75395063e-06
Iter: 1930 loss: 4.75293564e-06
Iter: 1931 loss: 4.76047944e-06
Iter: 1932 loss: 4.75287425e-06
Iter: 1933 loss: 4.75205161e-06
Iter: 1934 loss: 4.76207333e-06
Iter: 1935 loss: 4.75204297e-06
Iter: 1936 loss: 4.75146e-06
Iter: 1937 loss: 4.75092565e-06
Iter: 1938 loss: 4.75080742e-06
Iter: 1939 loss: 4.75006436e-06
Iter: 1940 loss: 4.75242905e-06
Iter: 1941 loss: 4.74983608e-06
Iter: 1942 loss: 4.74913759e-06
Iter: 1943 loss: 4.7560261e-06
Iter: 1944 loss: 4.74912667e-06
Iter: 1945 loss: 4.74852368e-06
Iter: 1946 loss: 4.74815351e-06
Iter: 1947 loss: 4.74795115e-06
Iter: 1948 loss: 4.74727676e-06
Iter: 1949 loss: 4.74814078e-06
Iter: 1950 loss: 4.74689477e-06
Iter: 1951 loss: 4.74606895e-06
Iter: 1952 loss: 4.75704974e-06
Iter: 1953 loss: 4.74605258e-06
Iter: 1954 loss: 4.74554145e-06
Iter: 1955 loss: 4.74526132e-06
Iter: 1956 loss: 4.74500939e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi1.6
+ date
Sat Oct 31 21:30:16 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f05431e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f05436a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f0560048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f04c4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f058de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f029e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f03bee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f0288598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f03c9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f03c90d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f03c9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f017c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f01a36a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f02dc488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f02dc6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f016e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f033c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f0412488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f0409f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f0075598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f008f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f008f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f0152730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f0165730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f0165950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69a47137b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69a46d2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69a46d26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f00e6730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f00b1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69a46d2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69a47821e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69a4789048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f004d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f0047a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f69f01c2e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.011794666
test_loss: 0.0117735425
train_loss: 0.009626053
test_loss: 0.008329999
train_loss: 0.006699752
test_loss: 0.0070160907
train_loss: 0.0058792024
test_loss: 0.006104554
train_loss: 0.005576111
test_loss: 0.0056009167
train_loss: 0.0050056605
test_loss: 0.005203922
train_loss: 0.0048137684
test_loss: 0.004918298
train_loss: 0.004636746
test_loss: 0.004630891
train_loss: 0.004673334
test_loss: 0.0045225406
train_loss: 0.004092102
test_loss: 0.0042642388
train_loss: 0.003779991
test_loss: 0.004193427
train_loss: 0.00364216
test_loss: 0.003808297
train_loss: 0.0035332153
test_loss: 0.0037537608
train_loss: 0.0035875046
test_loss: 0.0036298311
train_loss: 0.0032884416
test_loss: 0.0034699724
train_loss: 0.0031532773
test_loss: 0.0033969462
train_loss: 0.0030409014
test_loss: 0.0033782758
train_loss: 0.0029660142
test_loss: 0.0032558918
train_loss: 0.0028245
test_loss: 0.0031495306
train_loss: 0.0028907917
test_loss: 0.0030886298
train_loss: 0.002807269
test_loss: 0.0030847164
train_loss: 0.0027208398
test_loss: 0.002972163
train_loss: 0.0027845544
test_loss: 0.002950618
train_loss: 0.002543405
test_loss: 0.0028900972
train_loss: 0.002573398
test_loss: 0.0029027406
train_loss: 0.0024616565
test_loss: 0.0028234597
train_loss: 0.00251137
test_loss: 0.0028127397
train_loss: 0.0024569815
test_loss: 0.0027782682
train_loss: 0.0024128049
test_loss: 0.002778037
train_loss: 0.0023535532
test_loss: 0.002754962
train_loss: 0.0023779331
test_loss: 0.0027562114
train_loss: 0.0022511343
test_loss: 0.0027346136
train_loss: 0.0023743045
test_loss: 0.0026969363
train_loss: 0.0022755987
test_loss: 0.0026944743
train_loss: 0.0022377206
test_loss: 0.0026786982
train_loss: 0.002226547
test_loss: 0.002673203
train_loss: 0.0023704383
test_loss: 0.002662564
train_loss: 0.0021509603
test_loss: 0.0026596799
train_loss: 0.0022162616
test_loss: 0.0026556822
train_loss: 0.0021878208
test_loss: 0.0026433542
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c1dc268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd08407fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c1d0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c27f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c2840d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c25fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c0ecc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c13e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c0a8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c0a8158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c15e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c03b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c03b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07c049b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0707e4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0707fcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0707c7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07077f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd070707730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd070707ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0707610d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd070761d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0706a26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07065d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07065d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07061e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07061e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07061e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0705f6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0705f6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd070515378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0705871e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0705868c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0705bc6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd07048e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0704fc1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 9.48739944e-06
Iter: 2 loss: 9.41924918e-06
Iter: 3 loss: 9.40916289e-06
Iter: 4 loss: 9.36307242e-06
Iter: 5 loss: 9.58678811e-06
Iter: 6 loss: 9.35498429e-06
Iter: 7 loss: 9.32136936e-06
Iter: 8 loss: 9.30188e-06
Iter: 9 loss: 9.28753e-06
Iter: 10 loss: 9.22799336e-06
Iter: 11 loss: 9.36052675e-06
Iter: 12 loss: 9.20532511e-06
Iter: 13 loss: 9.17061425e-06
Iter: 14 loss: 9.25178119e-06
Iter: 15 loss: 9.15788951e-06
Iter: 16 loss: 9.11727238e-06
Iter: 17 loss: 9.32568764e-06
Iter: 18 loss: 9.110674e-06
Iter: 19 loss: 9.08381207e-06
Iter: 20 loss: 9.10354629e-06
Iter: 21 loss: 9.0673775e-06
Iter: 22 loss: 9.03016e-06
Iter: 23 loss: 9.04729859e-06
Iter: 24 loss: 9.00503e-06
Iter: 25 loss: 8.98460894e-06
Iter: 26 loss: 8.9807827e-06
Iter: 27 loss: 8.96088e-06
Iter: 28 loss: 8.9144e-06
Iter: 29 loss: 9.47578701e-06
Iter: 30 loss: 8.91056152e-06
Iter: 31 loss: 8.87466922e-06
Iter: 32 loss: 9.22065e-06
Iter: 33 loss: 8.8732977e-06
Iter: 34 loss: 8.84606379e-06
Iter: 35 loss: 8.83591747e-06
Iter: 36 loss: 8.82088079e-06
Iter: 37 loss: 8.79134677e-06
Iter: 38 loss: 8.97210157e-06
Iter: 39 loss: 8.78767787e-06
Iter: 40 loss: 8.77524508e-06
Iter: 41 loss: 8.77429193e-06
Iter: 42 loss: 8.75852766e-06
Iter: 43 loss: 8.76809099e-06
Iter: 44 loss: 8.748244e-06
Iter: 45 loss: 8.7345e-06
Iter: 46 loss: 8.77246202e-06
Iter: 47 loss: 8.73013232e-06
Iter: 48 loss: 8.71686188e-06
Iter: 49 loss: 8.75356091e-06
Iter: 50 loss: 8.71260272e-06
Iter: 51 loss: 8.70020631e-06
Iter: 52 loss: 8.69217e-06
Iter: 53 loss: 8.68746247e-06
Iter: 54 loss: 8.67415656e-06
Iter: 55 loss: 8.67399285e-06
Iter: 56 loss: 8.66491246e-06
Iter: 57 loss: 8.64793856e-06
Iter: 58 loss: 9.01671683e-06
Iter: 59 loss: 8.64789945e-06
Iter: 60 loss: 8.63196237e-06
Iter: 61 loss: 8.8010529e-06
Iter: 62 loss: 8.63161858e-06
Iter: 63 loss: 8.62096658e-06
Iter: 64 loss: 8.70833537e-06
Iter: 65 loss: 8.62031629e-06
Iter: 66 loss: 8.61106e-06
Iter: 67 loss: 8.61106491e-06
Iter: 68 loss: 8.60368891e-06
Iter: 69 loss: 8.59182182e-06
Iter: 70 loss: 8.58259591e-06
Iter: 71 loss: 8.57892701e-06
Iter: 72 loss: 8.56204133e-06
Iter: 73 loss: 8.63928744e-06
Iter: 74 loss: 8.55883263e-06
Iter: 75 loss: 8.54505743e-06
Iter: 76 loss: 8.60625732e-06
Iter: 77 loss: 8.54235805e-06
Iter: 78 loss: 8.53597339e-06
Iter: 79 loss: 8.53593065e-06
Iter: 80 loss: 8.52792164e-06
Iter: 81 loss: 8.52522498e-06
Iter: 82 loss: 8.52052744e-06
Iter: 83 loss: 8.51444292e-06
Iter: 84 loss: 8.5221036e-06
Iter: 85 loss: 8.51131517e-06
Iter: 86 loss: 8.5020165e-06
Iter: 87 loss: 8.52609e-06
Iter: 88 loss: 8.49880507e-06
Iter: 89 loss: 8.49168646e-06
Iter: 90 loss: 8.5012889e-06
Iter: 91 loss: 8.48813579e-06
Iter: 92 loss: 8.48022046e-06
Iter: 93 loss: 8.52448193e-06
Iter: 94 loss: 8.47907177e-06
Iter: 95 loss: 8.47101e-06
Iter: 96 loss: 8.47327192e-06
Iter: 97 loss: 8.4651947e-06
Iter: 98 loss: 8.45811e-06
Iter: 99 loss: 8.46149123e-06
Iter: 100 loss: 8.4534e-06
Iter: 101 loss: 8.44464648e-06
Iter: 102 loss: 8.44464194e-06
Iter: 103 loss: 8.44054557e-06
Iter: 104 loss: 8.44300757e-06
Iter: 105 loss: 8.43779e-06
Iter: 106 loss: 8.43212911e-06
Iter: 107 loss: 8.42292684e-06
Iter: 108 loss: 8.42293775e-06
Iter: 109 loss: 8.41296605e-06
Iter: 110 loss: 8.48232412e-06
Iter: 111 loss: 8.41205747e-06
Iter: 112 loss: 8.40408757e-06
Iter: 113 loss: 8.42468762e-06
Iter: 114 loss: 8.40128087e-06
Iter: 115 loss: 8.39573477e-06
Iter: 116 loss: 8.46812509e-06
Iter: 117 loss: 8.39558743e-06
Iter: 118 loss: 8.38943652e-06
Iter: 119 loss: 8.41289148e-06
Iter: 120 loss: 8.38803862e-06
Iter: 121 loss: 8.38537926e-06
Iter: 122 loss: 8.37899552e-06
Iter: 123 loss: 8.45030627e-06
Iter: 124 loss: 8.37830521e-06
Iter: 125 loss: 8.37139851e-06
Iter: 126 loss: 8.3713785e-06
Iter: 127 loss: 8.36821164e-06
Iter: 128 loss: 8.3628056e-06
Iter: 129 loss: 8.36283e-06
Iter: 130 loss: 8.35682476e-06
Iter: 131 loss: 8.44081842e-06
Iter: 132 loss: 8.35684841e-06
Iter: 133 loss: 8.35252558e-06
Iter: 134 loss: 8.35270384e-06
Iter: 135 loss: 8.34925231e-06
Iter: 136 loss: 8.3435134e-06
Iter: 137 loss: 8.34490857e-06
Iter: 138 loss: 8.33937793e-06
Iter: 139 loss: 8.33675676e-06
Iter: 140 loss: 8.33572631e-06
Iter: 141 loss: 8.33284412e-06
Iter: 142 loss: 8.32704e-06
Iter: 143 loss: 8.43475573e-06
Iter: 144 loss: 8.32687692e-06
Iter: 145 loss: 8.32180558e-06
Iter: 146 loss: 8.36457912e-06
Iter: 147 loss: 8.3215009e-06
Iter: 148 loss: 8.31733723e-06
Iter: 149 loss: 8.31127363e-06
Iter: 150 loss: 8.31103898e-06
Iter: 151 loss: 8.30344834e-06
Iter: 152 loss: 8.36099e-06
Iter: 153 loss: 8.30282e-06
Iter: 154 loss: 8.29775126e-06
Iter: 155 loss: 8.31073885e-06
Iter: 156 loss: 8.2959632e-06
Iter: 157 loss: 8.2911356e-06
Iter: 158 loss: 8.32796468e-06
Iter: 159 loss: 8.29082092e-06
Iter: 160 loss: 8.28684551e-06
Iter: 161 loss: 8.33849299e-06
Iter: 162 loss: 8.28680459e-06
Iter: 163 loss: 8.28491466e-06
Iter: 164 loss: 8.28396e-06
Iter: 165 loss: 8.28310658e-06
Iter: 166 loss: 8.27899203e-06
Iter: 167 loss: 8.27633266e-06
Iter: 168 loss: 8.27478561e-06
Iter: 169 loss: 8.27145e-06
Iter: 170 loss: 8.29291275e-06
Iter: 171 loss: 8.27116219e-06
Iter: 172 loss: 8.26727592e-06
Iter: 173 loss: 8.27124131e-06
Iter: 174 loss: 8.26515134e-06
Iter: 175 loss: 8.26152791e-06
Iter: 176 loss: 8.26838732e-06
Iter: 177 loss: 8.25991447e-06
Iter: 178 loss: 8.2565457e-06
Iter: 179 loss: 8.27538861e-06
Iter: 180 loss: 8.25617644e-06
Iter: 181 loss: 8.25219468e-06
Iter: 182 loss: 8.25803181e-06
Iter: 183 loss: 8.25026109e-06
Iter: 184 loss: 8.24779818e-06
Iter: 185 loss: 8.24480321e-06
Iter: 186 loss: 8.24455947e-06
Iter: 187 loss: 8.23945e-06
Iter: 188 loss: 8.26656651e-06
Iter: 189 loss: 8.23865412e-06
Iter: 190 loss: 8.23489245e-06
Iter: 191 loss: 8.23878e-06
Iter: 192 loss: 8.23289156e-06
Iter: 193 loss: 8.22859238e-06
Iter: 194 loss: 8.24182098e-06
Iter: 195 loss: 8.22730453e-06
Iter: 196 loss: 8.22247875e-06
Iter: 197 loss: 8.2295e-06
Iter: 198 loss: 8.21999492e-06
Iter: 199 loss: 8.22302172e-06
Iter: 200 loss: 8.21866524e-06
Iter: 201 loss: 8.21759e-06
Iter: 202 loss: 8.21488902e-06
Iter: 203 loss: 8.24273047e-06
Iter: 204 loss: 8.21458707e-06
Iter: 205 loss: 8.21145477e-06
Iter: 206 loss: 8.24061135e-06
Iter: 207 loss: 8.2113811e-06
Iter: 208 loss: 8.20915193e-06
Iter: 209 loss: 8.20526111e-06
Iter: 210 loss: 8.20531204e-06
Iter: 211 loss: 8.20291643e-06
Iter: 212 loss: 8.20266177e-06
Iter: 213 loss: 8.20072819e-06
Iter: 214 loss: 8.19747947e-06
Iter: 215 loss: 8.19745e-06
Iter: 216 loss: 8.1935359e-06
Iter: 217 loss: 8.21371e-06
Iter: 218 loss: 8.19293382e-06
Iter: 219 loss: 8.19074e-06
Iter: 220 loss: 8.19069101e-06
Iter: 221 loss: 8.18911485e-06
Iter: 222 loss: 8.18459557e-06
Iter: 223 loss: 8.20653804e-06
Iter: 224 loss: 8.18303397e-06
Iter: 225 loss: 8.17914952e-06
Iter: 226 loss: 8.22356378e-06
Iter: 227 loss: 8.17915952e-06
Iter: 228 loss: 8.17568798e-06
Iter: 229 loss: 8.18058106e-06
Iter: 230 loss: 8.17400723e-06
Iter: 231 loss: 8.16993452e-06
Iter: 232 loss: 8.18326953e-06
Iter: 233 loss: 8.16863576e-06
Iter: 234 loss: 8.16576267e-06
Iter: 235 loss: 8.17602631e-06
Iter: 236 loss: 8.16499414e-06
Iter: 237 loss: 8.16331158e-06
Iter: 238 loss: 8.16305328e-06
Iter: 239 loss: 8.16162083e-06
Iter: 240 loss: 8.15902513e-06
Iter: 241 loss: 8.15906787e-06
Iter: 242 loss: 8.1566377e-06
Iter: 243 loss: 8.15915337e-06
Iter: 244 loss: 8.15525436e-06
Iter: 245 loss: 8.15179e-06
Iter: 246 loss: 8.17576802e-06
Iter: 247 loss: 8.15141721e-06
Iter: 248 loss: 8.14957184e-06
Iter: 249 loss: 8.14738087e-06
Iter: 250 loss: 8.14716077e-06
Iter: 251 loss: 8.14507712e-06
Iter: 252 loss: 8.14505893e-06
Iter: 253 loss: 8.14371379e-06
Iter: 254 loss: 8.14061696e-06
Iter: 255 loss: 8.17389446e-06
Iter: 256 loss: 8.14026e-06
Iter: 257 loss: 8.13779116e-06
Iter: 258 loss: 8.13765655e-06
Iter: 259 loss: 8.1358221e-06
Iter: 260 loss: 8.13621318e-06
Iter: 261 loss: 8.13457e-06
Iter: 262 loss: 8.13236147e-06
Iter: 263 loss: 8.12858889e-06
Iter: 264 loss: 8.12855797e-06
Iter: 265 loss: 8.12512371e-06
Iter: 266 loss: 8.1478529e-06
Iter: 267 loss: 8.12479902e-06
Iter: 268 loss: 8.121875e-06
Iter: 269 loss: 8.13289898e-06
Iter: 270 loss: 8.12111193e-06
Iter: 271 loss: 8.11902828e-06
Iter: 272 loss: 8.11903374e-06
Iter: 273 loss: 8.11704558e-06
Iter: 274 loss: 8.12412509e-06
Iter: 275 loss: 8.11658538e-06
Iter: 276 loss: 8.11542941e-06
Iter: 277 loss: 8.11279187e-06
Iter: 278 loss: 8.14107443e-06
Iter: 279 loss: 8.11246173e-06
Iter: 280 loss: 8.1095086e-06
Iter: 281 loss: 8.14407e-06
Iter: 282 loss: 8.10951497e-06
Iter: 283 loss: 8.1074777e-06
Iter: 284 loss: 8.12046892e-06
Iter: 285 loss: 8.10723941e-06
Iter: 286 loss: 8.10556867e-06
Iter: 287 loss: 8.10210622e-06
Iter: 288 loss: 8.16741704e-06
Iter: 289 loss: 8.10195434e-06
Iter: 290 loss: 8.10075107e-06
Iter: 291 loss: 8.10026722e-06
Iter: 292 loss: 8.0987229e-06
Iter: 293 loss: 8.09745507e-06
Iter: 294 loss: 8.09697667e-06
Iter: 295 loss: 8.09495941e-06
Iter: 296 loss: 8.11111931e-06
Iter: 297 loss: 8.09481298e-06
Iter: 298 loss: 8.09304765e-06
Iter: 299 loss: 8.09745e-06
Iter: 300 loss: 8.09243193e-06
Iter: 301 loss: 8.09065295e-06
Iter: 302 loss: 8.08807454e-06
Iter: 303 loss: 8.08801087e-06
Iter: 304 loss: 8.08492769e-06
Iter: 305 loss: 8.09405901e-06
Iter: 306 loss: 8.08401728e-06
Iter: 307 loss: 8.08273671e-06
Iter: 308 loss: 8.08251116e-06
Iter: 309 loss: 8.08070126e-06
Iter: 310 loss: 8.07945526e-06
Iter: 311 loss: 8.07879587e-06
Iter: 312 loss: 8.07720608e-06
Iter: 313 loss: 8.07698507e-06
Iter: 314 loss: 8.07593278e-06
Iter: 315 loss: 8.07352353e-06
Iter: 316 loss: 8.08371442e-06
Iter: 317 loss: 8.07308061e-06
Iter: 318 loss: 8.07100332e-06
Iter: 319 loss: 8.08974801e-06
Iter: 320 loss: 8.07090237e-06
Iter: 321 loss: 8.06958269e-06
Iter: 322 loss: 8.06749267e-06
Iter: 323 loss: 8.06741264e-06
Iter: 324 loss: 8.0649279e-06
Iter: 325 loss: 8.08418463e-06
Iter: 326 loss: 8.06479147e-06
Iter: 327 loss: 8.06270873e-06
Iter: 328 loss: 8.07217657e-06
Iter: 329 loss: 8.06231765e-06
Iter: 330 loss: 8.06016214e-06
Iter: 331 loss: 8.05939e-06
Iter: 332 loss: 8.05825857e-06
Iter: 333 loss: 8.05676427e-06
Iter: 334 loss: 8.05666241e-06
Iter: 335 loss: 8.05553645e-06
Iter: 336 loss: 8.05255513e-06
Iter: 337 loss: 8.07511515e-06
Iter: 338 loss: 8.05195123e-06
Iter: 339 loss: 8.04907177e-06
Iter: 340 loss: 8.06807839e-06
Iter: 341 loss: 8.04875708e-06
Iter: 342 loss: 8.04688534e-06
Iter: 343 loss: 8.06776461e-06
Iter: 344 loss: 8.04687443e-06
Iter: 345 loss: 8.04489082e-06
Iter: 346 loss: 8.05466334e-06
Iter: 347 loss: 8.04448518e-06
Iter: 348 loss: 8.04329829e-06
Iter: 349 loss: 8.04077172e-06
Iter: 350 loss: 8.09052381e-06
Iter: 351 loss: 8.04080446e-06
Iter: 352 loss: 8.03859893e-06
Iter: 353 loss: 8.03653165e-06
Iter: 354 loss: 8.03595685e-06
Iter: 355 loss: 8.03628791e-06
Iter: 356 loss: 8.03424882e-06
Iter: 357 loss: 8.03319745e-06
Iter: 358 loss: 8.0315358e-06
Iter: 359 loss: 8.03150215e-06
Iter: 360 loss: 8.02908107e-06
Iter: 361 loss: 8.0326372e-06
Iter: 362 loss: 8.0279633e-06
Iter: 363 loss: 8.02654085e-06
Iter: 364 loss: 8.02645809e-06
Iter: 365 loss: 8.02536852e-06
Iter: 366 loss: 8.02438808e-06
Iter: 367 loss: 8.02404338e-06
Iter: 368 loss: 8.02199065e-06
Iter: 369 loss: 8.02854083e-06
Iter: 370 loss: 8.02133218e-06
Iter: 371 loss: 8.01928763e-06
Iter: 372 loss: 8.02715658e-06
Iter: 373 loss: 8.01876922e-06
Iter: 374 loss: 8.01708848e-06
Iter: 375 loss: 8.01698661e-06
Iter: 376 loss: 8.01567512e-06
Iter: 377 loss: 8.01342321e-06
Iter: 378 loss: 8.01250098e-06
Iter: 379 loss: 8.01133865e-06
Iter: 380 loss: 8.01166516e-06
Iter: 381 loss: 8.0101654e-06
Iter: 382 loss: 8.00887e-06
Iter: 383 loss: 8.00704493e-06
Iter: 384 loss: 8.00699763e-06
Iter: 385 loss: 8.0054815e-06
Iter: 386 loss: 8.00456e-06
Iter: 387 loss: 8.00397902e-06
Iter: 388 loss: 8.00141061e-06
Iter: 389 loss: 8.00953057e-06
Iter: 390 loss: 8.00066e-06
Iter: 391 loss: 7.99931149e-06
Iter: 392 loss: 7.9992451e-06
Iter: 393 loss: 7.99789632e-06
Iter: 394 loss: 7.99514328e-06
Iter: 395 loss: 8.05042782e-06
Iter: 396 loss: 7.99511872e-06
Iter: 397 loss: 7.99308236e-06
Iter: 398 loss: 8.00280304e-06
Iter: 399 loss: 7.99268673e-06
Iter: 400 loss: 7.99046e-06
Iter: 401 loss: 8.0073587e-06
Iter: 402 loss: 7.99030204e-06
Iter: 403 loss: 7.98866131e-06
Iter: 404 loss: 7.99022382e-06
Iter: 405 loss: 7.98783185e-06
Iter: 406 loss: 7.98623478e-06
Iter: 407 loss: 7.99323789e-06
Iter: 408 loss: 7.98586461e-06
Iter: 409 loss: 7.98411202e-06
Iter: 410 loss: 7.98547e-06
Iter: 411 loss: 7.98302244e-06
Iter: 412 loss: 7.98133169e-06
Iter: 413 loss: 7.98113706e-06
Iter: 414 loss: 7.97992925e-06
Iter: 415 loss: 7.97733264e-06
Iter: 416 loss: 7.99057307e-06
Iter: 417 loss: 7.97698885e-06
Iter: 418 loss: 7.97577923e-06
Iter: 419 loss: 7.97569373e-06
Iter: 420 loss: 7.97445409e-06
Iter: 421 loss: 7.9730853e-06
Iter: 422 loss: 7.97287066e-06
Iter: 423 loss: 7.97159828e-06
Iter: 424 loss: 7.96862332e-06
Iter: 425 loss: 8.00892121e-06
Iter: 426 loss: 7.96847144e-06
Iter: 427 loss: 7.965642e-06
Iter: 428 loss: 8.0029331e-06
Iter: 429 loss: 7.96569111e-06
Iter: 430 loss: 7.96350287e-06
Iter: 431 loss: 7.96655331e-06
Iter: 432 loss: 7.96243512e-06
Iter: 433 loss: 7.96129098e-06
Iter: 434 loss: 7.96106269e-06
Iter: 435 loss: 7.95996311e-06
Iter: 436 loss: 7.95849337e-06
Iter: 437 loss: 7.95831511e-06
Iter: 438 loss: 7.95655797e-06
Iter: 439 loss: 7.96000495e-06
Iter: 440 loss: 7.95579308e-06
Iter: 441 loss: 7.95364303e-06
Iter: 442 loss: 7.97665598e-06
Iter: 443 loss: 7.95357e-06
Iter: 444 loss: 7.95243795e-06
Iter: 445 loss: 7.95105916e-06
Iter: 446 loss: 7.95083724e-06
Iter: 447 loss: 7.94891366e-06
Iter: 448 loss: 7.96844e-06
Iter: 449 loss: 7.94882453e-06
Iter: 450 loss: 7.94741e-06
Iter: 451 loss: 7.9471547e-06
Iter: 452 loss: 7.9461015e-06
Iter: 453 loss: 7.94440257e-06
Iter: 454 loss: 7.95242795e-06
Iter: 455 loss: 7.94403786e-06
Iter: 456 loss: 7.94237712e-06
Iter: 457 loss: 7.96203221e-06
Iter: 458 loss: 7.94233347e-06
Iter: 459 loss: 7.94163316e-06
Iter: 460 loss: 7.93950767e-06
Iter: 461 loss: 7.94657535e-06
Iter: 462 loss: 7.93843174e-06
Iter: 463 loss: 7.93649178e-06
Iter: 464 loss: 7.9364163e-06
Iter: 465 loss: 7.93506115e-06
Iter: 466 loss: 7.93211257e-06
Iter: 467 loss: 7.98233486e-06
Iter: 468 loss: 7.93208437e-06
Iter: 469 loss: 7.92968422e-06
Iter: 470 loss: 7.92967876e-06
Iter: 471 loss: 7.92757055e-06
Iter: 472 loss: 7.92558058e-06
Iter: 473 loss: 7.92518767e-06
Iter: 474 loss: 7.92339233e-06
Iter: 475 loss: 7.9233705e-06
Iter: 476 loss: 7.92158789e-06
Iter: 477 loss: 7.92790888e-06
Iter: 478 loss: 7.92123137e-06
Iter: 479 loss: 7.91989351e-06
Iter: 480 loss: 7.91904677e-06
Iter: 481 loss: 7.91839557e-06
Iter: 482 loss: 7.91646198e-06
Iter: 483 loss: 7.92478568e-06
Iter: 484 loss: 7.91606908e-06
Iter: 485 loss: 7.91442653e-06
Iter: 486 loss: 7.93322852e-06
Iter: 487 loss: 7.91439834e-06
Iter: 488 loss: 7.91314051e-06
Iter: 489 loss: 7.91119419e-06
Iter: 490 loss: 7.91116054e-06
Iter: 491 loss: 7.91034563e-06
Iter: 492 loss: 7.90996819e-06
Iter: 493 loss: 7.90920603e-06
Iter: 494 loss: 7.9096e-06
Iter: 495 loss: 7.90863214e-06
Iter: 496 loss: 7.90705053e-06
Iter: 497 loss: 7.90703143e-06
Iter: 498 loss: 7.90583181e-06
Iter: 499 loss: 7.90438207e-06
Iter: 500 loss: 7.9029578e-06
Iter: 501 loss: 7.90268496e-06
Iter: 502 loss: 7.90027934e-06
Iter: 503 loss: 7.91196362e-06
Iter: 504 loss: 7.89992555e-06
Iter: 505 loss: 7.89802743e-06
Iter: 506 loss: 7.89736441e-06
Iter: 507 loss: 7.89618389e-06
Iter: 508 loss: 7.89378373e-06
Iter: 509 loss: 7.91997809e-06
Iter: 510 loss: 7.89368642e-06
Iter: 511 loss: 7.89186834e-06
Iter: 512 loss: 7.89856676e-06
Iter: 513 loss: 7.89134538e-06
Iter: 514 loss: 7.88981197e-06
Iter: 515 loss: 7.90466e-06
Iter: 516 loss: 7.88975e-06
Iter: 517 loss: 7.88835405e-06
Iter: 518 loss: 7.88775105e-06
Iter: 519 loss: 7.88712805e-06
Iter: 520 loss: 7.88550187e-06
Iter: 521 loss: 7.88602756e-06
Iter: 522 loss: 7.88426951e-06
Iter: 523 loss: 7.88254056e-06
Iter: 524 loss: 7.88255693e-06
Iter: 525 loss: 7.88121179e-06
Iter: 526 loss: 7.88066063e-06
Iter: 527 loss: 7.87999852e-06
Iter: 528 loss: 7.87884073e-06
Iter: 529 loss: 7.87881436e-06
Iter: 530 loss: 7.87775207e-06
Iter: 531 loss: 7.87676254e-06
Iter: 532 loss: 7.87660065e-06
Iter: 533 loss: 7.87547924e-06
Iter: 534 loss: 7.88680518e-06
Iter: 535 loss: 7.87540557e-06
Iter: 536 loss: 7.87455156e-06
Iter: 537 loss: 7.87190311e-06
Iter: 538 loss: 7.87964109e-06
Iter: 539 loss: 7.87056e-06
Iter: 540 loss: 7.8677549e-06
Iter: 541 loss: 7.88693615e-06
Iter: 542 loss: 7.86752844e-06
Iter: 543 loss: 7.86512192e-06
Iter: 544 loss: 7.88288344e-06
Iter: 545 loss: 7.86493638e-06
Iter: 546 loss: 7.8631092e-06
Iter: 547 loss: 7.86424789e-06
Iter: 548 loss: 7.86189776e-06
Iter: 549 loss: 7.86017517e-06
Iter: 550 loss: 7.86013334e-06
Iter: 551 loss: 7.85888369e-06
Iter: 552 loss: 7.85814154e-06
Iter: 553 loss: 7.85768134e-06
Iter: 554 loss: 7.85572047e-06
Iter: 555 loss: 7.86063083e-06
Iter: 556 loss: 7.85510747e-06
Iter: 557 loss: 7.85386328e-06
Iter: 558 loss: 7.86908731e-06
Iter: 559 loss: 7.85385146e-06
Iter: 560 loss: 7.85247539e-06
Iter: 561 loss: 7.85032717e-06
Iter: 562 loss: 7.85025441e-06
Iter: 563 loss: 7.84999793e-06
Iter: 564 loss: 7.84920121e-06
Iter: 565 loss: 7.84844906e-06
Iter: 566 loss: 7.84733766e-06
Iter: 567 loss: 7.84728854e-06
Iter: 568 loss: 7.84581061e-06
Iter: 569 loss: 7.8484527e-06
Iter: 570 loss: 7.845043e-06
Iter: 571 loss: 7.84308759e-06
Iter: 572 loss: 7.84839085e-06
Iter: 573 loss: 7.8424473e-06
Iter: 574 loss: 7.8411922e-06
Iter: 575 loss: 7.83890482e-06
Iter: 576 loss: 7.88817488e-06
Iter: 577 loss: 7.83881842e-06
Iter: 578 loss: 7.83587166e-06
Iter: 579 loss: 7.85249904e-06
Iter: 580 loss: 7.83538417e-06
Iter: 581 loss: 7.83299583e-06
Iter: 582 loss: 7.84616896e-06
Iter: 583 loss: 7.83262203e-06
Iter: 584 loss: 7.83116684e-06
Iter: 585 loss: 7.85183693e-06
Iter: 586 loss: 7.83118412e-06
Iter: 587 loss: 7.82978e-06
Iter: 588 loss: 7.82950156e-06
Iter: 589 loss: 7.82857933e-06
Iter: 590 loss: 7.82704137e-06
Iter: 591 loss: 7.82829e-06
Iter: 592 loss: 7.82611642e-06
Iter: 593 loss: 7.82454117e-06
Iter: 594 loss: 7.84838721e-06
Iter: 595 loss: 7.82456118e-06
Iter: 596 loss: 7.82334155e-06
Iter: 597 loss: 7.82553616e-06
Iter: 598 loss: 7.82278858e-06
Iter: 599 loss: 7.82165534e-06
Iter: 600 loss: 7.82938787e-06
Iter: 601 loss: 7.82148618e-06
Iter: 602 loss: 7.82024108e-06
Iter: 603 loss: 7.81953713e-06
Iter: 604 loss: 7.81902327e-06
Iter: 605 loss: 7.81763902e-06
Iter: 606 loss: 7.81930066e-06
Iter: 607 loss: 7.81708059e-06
Iter: 608 loss: 7.81519157e-06
Iter: 609 loss: 7.82028565e-06
Iter: 610 loss: 7.81451945e-06
Iter: 611 loss: 7.81290055e-06
Iter: 612 loss: 7.81297695e-06
Iter: 613 loss: 7.81162635e-06
Iter: 614 loss: 7.80967275e-06
Iter: 615 loss: 7.81439485e-06
Iter: 616 loss: 7.80893151e-06
Iter: 617 loss: 7.8067751e-06
Iter: 618 loss: 7.80689243e-06
Iter: 619 loss: 7.80507798e-06
Iter: 620 loss: 7.80287428e-06
Iter: 621 loss: 7.82427378e-06
Iter: 622 loss: 7.80277e-06
Iter: 623 loss: 7.80101345e-06
Iter: 624 loss: 7.82469942e-06
Iter: 625 loss: 7.80106348e-06
Iter: 626 loss: 7.8000985e-06
Iter: 627 loss: 7.79865695e-06
Iter: 628 loss: 7.7986906e-06
Iter: 629 loss: 7.79695165e-06
Iter: 630 loss: 7.80972732e-06
Iter: 631 loss: 7.79681e-06
Iter: 632 loss: 7.79551738e-06
Iter: 633 loss: 7.80932896e-06
Iter: 634 loss: 7.79546099e-06
Iter: 635 loss: 7.79469701e-06
Iter: 636 loss: 7.79637594e-06
Iter: 637 loss: 7.79439688e-06
Iter: 638 loss: 7.79311085e-06
Iter: 639 loss: 7.79106813e-06
Iter: 640 loss: 7.79107086e-06
Iter: 641 loss: 7.7896093e-06
Iter: 642 loss: 7.80253686e-06
Iter: 643 loss: 7.78952e-06
Iter: 644 loss: 7.78844333e-06
Iter: 645 loss: 7.78856884e-06
Iter: 646 loss: 7.78753e-06
Iter: 647 loss: 7.78549474e-06
Iter: 648 loss: 7.78927642e-06
Iter: 649 loss: 7.78461435e-06
Iter: 650 loss: 7.78331832e-06
Iter: 651 loss: 7.78178855e-06
Iter: 652 loss: 7.78161575e-06
Iter: 653 loss: 7.77917194e-06
Iter: 654 loss: 7.79542734e-06
Iter: 655 loss: 7.77889818e-06
Iter: 656 loss: 7.77687364e-06
Iter: 657 loss: 7.77960304e-06
Iter: 658 loss: 7.77583591e-06
Iter: 659 loss: 7.77451805e-06
Iter: 660 loss: 7.77448622e-06
Iter: 661 loss: 7.77286e-06
Iter: 662 loss: 7.77124205e-06
Iter: 663 loss: 7.77093373e-06
Iter: 664 loss: 7.76925663e-06
Iter: 665 loss: 7.77864443e-06
Iter: 666 loss: 7.76895831e-06
Iter: 667 loss: 7.767434e-06
Iter: 668 loss: 7.78306e-06
Iter: 669 loss: 7.76743855e-06
Iter: 670 loss: 7.7661407e-06
Iter: 671 loss: 7.76682646e-06
Iter: 672 loss: 7.76544948e-06
Iter: 673 loss: 7.76388151e-06
Iter: 674 loss: 7.7735e-06
Iter: 675 loss: 7.76372508e-06
Iter: 676 loss: 7.76291836e-06
Iter: 677 loss: 7.76084744e-06
Iter: 678 loss: 7.77661262e-06
Iter: 679 loss: 7.76042089e-06
Iter: 680 loss: 7.75858643e-06
Iter: 681 loss: 7.75853914e-06
Iter: 682 loss: 7.75728313e-06
Iter: 683 loss: 7.75937497e-06
Iter: 684 loss: 7.7566383e-06
Iter: 685 loss: 7.75551234e-06
Iter: 686 loss: 7.75636909e-06
Iter: 687 loss: 7.754732e-06
Iter: 688 loss: 7.75295393e-06
Iter: 689 loss: 7.75294e-06
Iter: 690 loss: 7.7514278e-06
Iter: 691 loss: 7.74949331e-06
Iter: 692 loss: 7.75574517e-06
Iter: 693 loss: 7.74898e-06
Iter: 694 loss: 7.74681484e-06
Iter: 695 loss: 7.75583e-06
Iter: 696 loss: 7.74635737e-06
Iter: 697 loss: 7.74500495e-06
Iter: 698 loss: 7.74502e-06
Iter: 699 loss: 7.74385626e-06
Iter: 700 loss: 7.74210093e-06
Iter: 701 loss: 7.74208729e-06
Iter: 702 loss: 7.74079854e-06
Iter: 703 loss: 7.74082e-06
Iter: 704 loss: 7.73935608e-06
Iter: 705 loss: 7.73859574e-06
Iter: 706 loss: 7.7379409e-06
Iter: 707 loss: 7.73696593e-06
Iter: 708 loss: 7.73690226e-06
Iter: 709 loss: 7.73628562e-06
Iter: 710 loss: 7.73472493e-06
Iter: 711 loss: 7.7491768e-06
Iter: 712 loss: 7.73449e-06
Iter: 713 loss: 7.73264e-06
Iter: 714 loss: 7.73875399e-06
Iter: 715 loss: 7.73213924e-06
Iter: 716 loss: 7.73035572e-06
Iter: 717 loss: 7.74754335e-06
Iter: 718 loss: 7.73031206e-06
Iter: 719 loss: 7.72901603e-06
Iter: 720 loss: 7.72996646e-06
Iter: 721 loss: 7.72819476e-06
Iter: 722 loss: 7.72676776e-06
Iter: 723 loss: 7.72528438e-06
Iter: 724 loss: 7.72505155e-06
Iter: 725 loss: 7.72293151e-06
Iter: 726 loss: 7.74765067e-06
Iter: 727 loss: 7.7229e-06
Iter: 728 loss: 7.72129351e-06
Iter: 729 loss: 7.72026397e-06
Iter: 730 loss: 7.71951545e-06
Iter: 731 loss: 7.71887e-06
Iter: 732 loss: 7.71846226e-06
Iter: 733 loss: 7.71740088e-06
Iter: 734 loss: 7.71538544e-06
Iter: 735 loss: 7.76387242e-06
Iter: 736 loss: 7.71540545e-06
Iter: 737 loss: 7.71427858e-06
Iter: 738 loss: 7.71426221e-06
Iter: 739 loss: 7.71307077e-06
Iter: 740 loss: 7.71533e-06
Iter: 741 loss: 7.71265059e-06
Iter: 742 loss: 7.7115692e-06
Iter: 743 loss: 7.71103e-06
Iter: 744 loss: 7.71047416e-06
Iter: 745 loss: 7.70862334e-06
Iter: 746 loss: 7.71877149e-06
Iter: 747 loss: 7.70831139e-06
Iter: 748 loss: 7.70748e-06
Iter: 749 loss: 7.70527367e-06
Iter: 750 loss: 7.72662315e-06
Iter: 751 loss: 7.70495899e-06
Iter: 752 loss: 7.70345378e-06
Iter: 753 loss: 7.70337829e-06
Iter: 754 loss: 7.70189945e-06
Iter: 755 loss: 7.70450333e-06
Iter: 756 loss: 7.70128e-06
Iter: 757 loss: 7.6998067e-06
Iter: 758 loss: 7.69912185e-06
Iter: 759 loss: 7.69843336e-06
Iter: 760 loss: 7.69633061e-06
Iter: 761 loss: 7.70276165e-06
Iter: 762 loss: 7.69578583e-06
Iter: 763 loss: 7.69391136e-06
Iter: 764 loss: 7.69687085e-06
Iter: 765 loss: 7.69313192e-06
Iter: 766 loss: 7.69106464e-06
Iter: 767 loss: 7.7171826e-06
Iter: 768 loss: 7.69102098e-06
Iter: 769 loss: 7.68975497e-06
Iter: 770 loss: 7.69399e-06
Iter: 771 loss: 7.68936479e-06
Iter: 772 loss: 7.68807331e-06
Iter: 773 loss: 7.69041162e-06
Iter: 774 loss: 7.68760674e-06
Iter: 775 loss: 7.68604605e-06
Iter: 776 loss: 7.69661619e-06
Iter: 777 loss: 7.68591e-06
Iter: 778 loss: 7.68507e-06
Iter: 779 loss: 7.68388509e-06
Iter: 780 loss: 7.68387781e-06
Iter: 781 loss: 7.68222162e-06
Iter: 782 loss: 7.6975839e-06
Iter: 783 loss: 7.6821043e-06
Iter: 784 loss: 7.6811657e-06
Iter: 785 loss: 7.67943766e-06
Iter: 786 loss: 7.7196346e-06
Iter: 787 loss: 7.67944e-06
Iter: 788 loss: 7.67698475e-06
Iter: 789 loss: 7.68098289e-06
Iter: 790 loss: 7.67583e-06
Iter: 791 loss: 7.67420443e-06
Iter: 792 loss: 7.67421079e-06
Iter: 793 loss: 7.672842e-06
Iter: 794 loss: 7.67421443e-06
Iter: 795 loss: 7.67201e-06
Iter: 796 loss: 7.67058646e-06
Iter: 797 loss: 7.66762423e-06
Iter: 798 loss: 7.71926807e-06
Iter: 799 loss: 7.66757512e-06
Iter: 800 loss: 7.66535595e-06
Iter: 801 loss: 7.66534413e-06
Iter: 802 loss: 7.66368885e-06
Iter: 803 loss: 7.66856283e-06
Iter: 804 loss: 7.6630995e-06
Iter: 805 loss: 7.661175e-06
Iter: 806 loss: 7.67614711e-06
Iter: 807 loss: 7.66097e-06
Iter: 808 loss: 7.66026e-06
Iter: 809 loss: 7.66587709e-06
Iter: 810 loss: 7.66021822e-06
Iter: 811 loss: 7.65929235e-06
Iter: 812 loss: 7.6571323e-06
Iter: 813 loss: 7.68502105e-06
Iter: 814 loss: 7.65696859e-06
Iter: 815 loss: 7.65563345e-06
Iter: 816 loss: 7.65566e-06
Iter: 817 loss: 7.65454752e-06
Iter: 818 loss: 7.6544784e-06
Iter: 819 loss: 7.65360346e-06
Iter: 820 loss: 7.65199366e-06
Iter: 821 loss: 7.65273671e-06
Iter: 822 loss: 7.65091318e-06
Iter: 823 loss: 7.64907327e-06
Iter: 824 loss: 7.65186087e-06
Iter: 825 loss: 7.6482238e-06
Iter: 826 loss: 7.64628e-06
Iter: 827 loss: 7.66113e-06
Iter: 828 loss: 7.64615288e-06
Iter: 829 loss: 7.64437755e-06
Iter: 830 loss: 7.64983088e-06
Iter: 831 loss: 7.6437891e-06
Iter: 832 loss: 7.64236e-06
Iter: 833 loss: 7.6421029e-06
Iter: 834 loss: 7.64112883e-06
Iter: 835 loss: 7.63902608e-06
Iter: 836 loss: 7.64050765e-06
Iter: 837 loss: 7.63767457e-06
Iter: 838 loss: 7.63584194e-06
Iter: 839 loss: 7.65603909e-06
Iter: 840 loss: 7.63577918e-06
Iter: 841 loss: 7.63395292e-06
Iter: 842 loss: 7.64381639e-06
Iter: 843 loss: 7.63361459e-06
Iter: 844 loss: 7.632505e-06
Iter: 845 loss: 7.6411452e-06
Iter: 846 loss: 7.63247226e-06
Iter: 847 loss: 7.63138e-06
Iter: 848 loss: 7.63102616e-06
Iter: 849 loss: 7.6304168e-06
Iter: 850 loss: 7.62905256e-06
Iter: 851 loss: 7.628611e-06
Iter: 852 loss: 7.62795571e-06
Iter: 853 loss: 7.62649461e-06
Iter: 854 loss: 7.62650507e-06
Iter: 855 loss: 7.62548916e-06
Iter: 856 loss: 7.6228248e-06
Iter: 857 loss: 7.64372908e-06
Iter: 858 loss: 7.62246827e-06
Iter: 859 loss: 7.61992e-06
Iter: 860 loss: 7.63122443e-06
Iter: 861 loss: 7.61940919e-06
Iter: 862 loss: 7.61748652e-06
Iter: 863 loss: 7.61745559e-06
Iter: 864 loss: 7.61608044e-06
Iter: 865 loss: 7.61953925e-06
Iter: 866 loss: 7.61555839e-06
Iter: 867 loss: 7.61421e-06
Iter: 868 loss: 7.61340289e-06
Iter: 869 loss: 7.61277579e-06
Iter: 870 loss: 7.61063347e-06
Iter: 871 loss: 7.61520141e-06
Iter: 872 loss: 7.60975445e-06
Iter: 873 loss: 7.6074939e-06
Iter: 874 loss: 7.62173522e-06
Iter: 875 loss: 7.60733701e-06
Iter: 876 loss: 7.60606144e-06
Iter: 877 loss: 7.62302534e-06
Iter: 878 loss: 7.60606417e-06
Iter: 879 loss: 7.6047163e-06
Iter: 880 loss: 7.60341391e-06
Iter: 881 loss: 7.60309194e-06
Iter: 882 loss: 7.60115654e-06
Iter: 883 loss: 7.62494437e-06
Iter: 884 loss: 7.60110743e-06
Iter: 885 loss: 7.60026069e-06
Iter: 886 loss: 7.59835712e-06
Iter: 887 loss: 7.62035143e-06
Iter: 888 loss: 7.59811473e-06
Iter: 889 loss: 7.59666364e-06
Iter: 890 loss: 7.5965736e-06
Iter: 891 loss: 7.59541172e-06
Iter: 892 loss: 7.59429804e-06
Iter: 893 loss: 7.59403838e-06
Iter: 894 loss: 7.59219256e-06
Iter: 895 loss: 7.59218074e-06
Iter: 896 loss: 7.59075374e-06
Iter: 897 loss: 7.5882167e-06
Iter: 898 loss: 7.59238719e-06
Iter: 899 loss: 7.58704937e-06
Iter: 900 loss: 7.5854914e-06
Iter: 901 loss: 7.58538908e-06
Iter: 902 loss: 7.58384e-06
Iter: 903 loss: 7.58373699e-06
Iter: 904 loss: 7.58267288e-06
Iter: 905 loss: 7.58085253e-06
Iter: 906 loss: 7.57970656e-06
Iter: 907 loss: 7.57907219e-06
Iter: 908 loss: 7.57660609e-06
Iter: 909 loss: 7.59818158e-06
Iter: 910 loss: 7.57647467e-06
Iter: 911 loss: 7.57481666e-06
Iter: 912 loss: 7.5795233e-06
Iter: 913 loss: 7.5742123e-06
Iter: 914 loss: 7.57223097e-06
Iter: 915 loss: 7.59282466e-06
Iter: 916 loss: 7.57212911e-06
Iter: 917 loss: 7.57128782e-06
Iter: 918 loss: 7.57182124e-06
Iter: 919 loss: 7.57079533e-06
Iter: 920 loss: 7.56954205e-06
Iter: 921 loss: 7.56992631e-06
Iter: 922 loss: 7.56863437e-06
Iter: 923 loss: 7.5669027e-06
Iter: 924 loss: 7.57022872e-06
Iter: 925 loss: 7.56618283e-06
Iter: 926 loss: 7.56465488e-06
Iter: 927 loss: 7.57137059e-06
Iter: 928 loss: 7.56439977e-06
Iter: 929 loss: 7.56256e-06
Iter: 930 loss: 7.56388317e-06
Iter: 931 loss: 7.56134432e-06
Iter: 932 loss: 7.55986366e-06
Iter: 933 loss: 7.55989231e-06
Iter: 934 loss: 7.55866904e-06
Iter: 935 loss: 7.55647125e-06
Iter: 936 loss: 7.55978635e-06
Iter: 937 loss: 7.55538076e-06
Iter: 938 loss: 7.5529324e-06
Iter: 939 loss: 7.55942619e-06
Iter: 940 loss: 7.55204e-06
Iter: 941 loss: 7.55095971e-06
Iter: 942 loss: 7.55073779e-06
Iter: 943 loss: 7.5496373e-06
Iter: 944 loss: 7.54732082e-06
Iter: 945 loss: 7.58446095e-06
Iter: 946 loss: 7.5471512e-06
Iter: 947 loss: 7.54501525e-06
Iter: 948 loss: 7.55404744e-06
Iter: 949 loss: 7.54456596e-06
Iter: 950 loss: 7.54223493e-06
Iter: 951 loss: 7.54736129e-06
Iter: 952 loss: 7.54126495e-06
Iter: 953 loss: 7.54129815e-06
Iter: 954 loss: 7.54015946e-06
Iter: 955 loss: 7.53963968e-06
Iter: 956 loss: 7.53808035e-06
Iter: 957 loss: 7.54557868e-06
Iter: 958 loss: 7.5375865e-06
Iter: 959 loss: 7.53557561e-06
Iter: 960 loss: 7.56475583e-06
Iter: 961 loss: 7.53560244e-06
Iter: 962 loss: 7.53451241e-06
Iter: 963 loss: 7.53341237e-06
Iter: 964 loss: 7.53316863e-06
Iter: 965 loss: 7.53144514e-06
Iter: 966 loss: 7.53819677e-06
Iter: 967 loss: 7.53102177e-06
Iter: 968 loss: 7.52934693e-06
Iter: 969 loss: 7.54045141e-06
Iter: 970 loss: 7.52916094e-06
Iter: 971 loss: 7.52780034e-06
Iter: 972 loss: 7.52767892e-06
Iter: 973 loss: 7.52654159e-06
Iter: 974 loss: 7.52497181e-06
Iter: 975 loss: 7.52500864e-06
Iter: 976 loss: 7.52369669e-06
Iter: 977 loss: 7.52128926e-06
Iter: 978 loss: 7.52642609e-06
Iter: 979 loss: 7.52031883e-06
Iter: 980 loss: 7.51773132e-06
Iter: 981 loss: 7.52430151e-06
Iter: 982 loss: 7.51681273e-06
Iter: 983 loss: 7.51602511e-06
Iter: 984 loss: 7.51573134e-06
Iter: 985 loss: 7.51467542e-06
Iter: 986 loss: 7.51208245e-06
Iter: 987 loss: 7.53368477e-06
Iter: 988 loss: 7.51163861e-06
Iter: 989 loss: 7.50942081e-06
Iter: 990 loss: 7.52488859e-06
Iter: 991 loss: 7.50925665e-06
Iter: 992 loss: 7.50802155e-06
Iter: 993 loss: 7.50784511e-06
Iter: 994 loss: 7.50681511e-06
Iter: 995 loss: 7.50474874e-06
Iter: 996 loss: 7.55362726e-06
Iter: 997 loss: 7.50474555e-06
Iter: 998 loss: 7.50306799e-06
Iter: 999 loss: 7.50126537e-06
Iter: 1000 loss: 7.50095933e-06
Iter: 1001 loss: 7.49847732e-06
Iter: 1002 loss: 7.50898744e-06
Iter: 1003 loss: 7.49794526e-06
Iter: 1004 loss: 7.4963059e-06
Iter: 1005 loss: 7.4961813e-06
Iter: 1006 loss: 7.49496849e-06
Iter: 1007 loss: 7.49448e-06
Iter: 1008 loss: 7.49387e-06
Iter: 1009 loss: 7.49203127e-06
Iter: 1010 loss: 7.49647461e-06
Iter: 1011 loss: 7.49140781e-06
Iter: 1012 loss: 7.48995899e-06
Iter: 1013 loss: 7.50490381e-06
Iter: 1014 loss: 7.48987395e-06
Iter: 1015 loss: 7.48846742e-06
Iter: 1016 loss: 7.48655339e-06
Iter: 1017 loss: 7.48642378e-06
Iter: 1018 loss: 7.48458115e-06
Iter: 1019 loss: 7.49146375e-06
Iter: 1020 loss: 7.48404909e-06
Iter: 1021 loss: 7.48216735e-06
Iter: 1022 loss: 7.49025821e-06
Iter: 1023 loss: 7.48179264e-06
Iter: 1024 loss: 7.4801319e-06
Iter: 1025 loss: 7.4935715e-06
Iter: 1026 loss: 7.48004504e-06
Iter: 1027 loss: 7.47902232e-06
Iter: 1028 loss: 7.47691593e-06
Iter: 1029 loss: 7.51389189e-06
Iter: 1030 loss: 7.47679451e-06
Iter: 1031 loss: 7.47746e-06
Iter: 1032 loss: 7.47579088e-06
Iter: 1033 loss: 7.47508329e-06
Iter: 1034 loss: 7.47344211e-06
Iter: 1035 loss: 7.48536422e-06
Iter: 1036 loss: 7.47301056e-06
Iter: 1037 loss: 7.47109561e-06
Iter: 1038 loss: 7.47706963e-06
Iter: 1039 loss: 7.47054901e-06
Iter: 1040 loss: 7.46863088e-06
Iter: 1041 loss: 7.46914338e-06
Iter: 1042 loss: 7.4673726e-06
Iter: 1043 loss: 7.46554906e-06
Iter: 1044 loss: 7.46537216e-06
Iter: 1045 loss: 7.46459e-06
Iter: 1046 loss: 7.46265414e-06
Iter: 1047 loss: 7.48712137e-06
Iter: 1048 loss: 7.46249589e-06
Iter: 1049 loss: 7.4603231e-06
Iter: 1050 loss: 7.48794719e-06
Iter: 1051 loss: 7.46028263e-06
Iter: 1052 loss: 7.45892612e-06
Iter: 1053 loss: 7.46759088e-06
Iter: 1054 loss: 7.45880243e-06
Iter: 1055 loss: 7.45773104e-06
Iter: 1056 loss: 7.45548368e-06
Iter: 1057 loss: 7.49279025e-06
Iter: 1058 loss: 7.45538182e-06
Iter: 1059 loss: 7.45285433e-06
Iter: 1060 loss: 7.47278955e-06
Iter: 1061 loss: 7.45270336e-06
Iter: 1062 loss: 7.45117177e-06
Iter: 1063 loss: 7.46429e-06
Iter: 1064 loss: 7.45111356e-06
Iter: 1065 loss: 7.44979707e-06
Iter: 1066 loss: 7.45129591e-06
Iter: 1067 loss: 7.44913905e-06
Iter: 1068 loss: 7.447813e-06
Iter: 1069 loss: 7.45376e-06
Iter: 1070 loss: 7.44757062e-06
Iter: 1071 loss: 7.44582348e-06
Iter: 1072 loss: 7.44922045e-06
Iter: 1073 loss: 7.44512681e-06
Iter: 1074 loss: 7.44419049e-06
Iter: 1075 loss: 7.44181216e-06
Iter: 1076 loss: 7.45862872e-06
Iter: 1077 loss: 7.44123099e-06
Iter: 1078 loss: 7.43848113e-06
Iter: 1079 loss: 7.46253363e-06
Iter: 1080 loss: 7.4383488e-06
Iter: 1081 loss: 7.43674855e-06
Iter: 1082 loss: 7.43677219e-06
Iter: 1083 loss: 7.43547116e-06
Iter: 1084 loss: 7.43535111e-06
Iter: 1085 loss: 7.43435294e-06
Iter: 1086 loss: 7.43284409e-06
Iter: 1087 loss: 7.43292094e-06
Iter: 1088 loss: 7.43154669e-06
Iter: 1089 loss: 7.43003511e-06
Iter: 1090 loss: 7.42999555e-06
Iter: 1091 loss: 7.4288464e-06
Iter: 1092 loss: 7.42697284e-06
Iter: 1093 loss: 7.42693283e-06
Iter: 1094 loss: 7.42500106e-06
Iter: 1095 loss: 7.43549208e-06
Iter: 1096 loss: 7.42475186e-06
Iter: 1097 loss: 7.42313659e-06
Iter: 1098 loss: 7.42744396e-06
Iter: 1099 loss: 7.42252769e-06
Iter: 1100 loss: 7.42102839e-06
Iter: 1101 loss: 7.43525288e-06
Iter: 1102 loss: 7.42098518e-06
Iter: 1103 loss: 7.41976783e-06
Iter: 1104 loss: 7.41988424e-06
Iter: 1105 loss: 7.41887743e-06
Iter: 1106 loss: 7.41688109e-06
Iter: 1107 loss: 7.43176361e-06
Iter: 1108 loss: 7.41675694e-06
Iter: 1109 loss: 7.4157233e-06
Iter: 1110 loss: 7.41469512e-06
Iter: 1111 loss: 7.41452232e-06
Iter: 1112 loss: 7.41293479e-06
Iter: 1113 loss: 7.41122676e-06
Iter: 1114 loss: 7.41095664e-06
Iter: 1115 loss: 7.40849282e-06
Iter: 1116 loss: 7.41411577e-06
Iter: 1117 loss: 7.4075574e-06
Iter: 1118 loss: 7.40593168e-06
Iter: 1119 loss: 7.40582891e-06
Iter: 1120 loss: 7.40467476e-06
Iter: 1121 loss: 7.41094937e-06
Iter: 1122 loss: 7.40442738e-06
Iter: 1123 loss: 7.40336827e-06
Iter: 1124 loss: 7.40092673e-06
Iter: 1125 loss: 7.43196233e-06
Iter: 1126 loss: 7.40077894e-06
Iter: 1127 loss: 7.40071573e-06
Iter: 1128 loss: 7.3996157e-06
Iter: 1129 loss: 7.39880124e-06
Iter: 1130 loss: 7.39648294e-06
Iter: 1131 loss: 7.41152326e-06
Iter: 1132 loss: 7.39587085e-06
Iter: 1133 loss: 7.39386314e-06
Iter: 1134 loss: 7.41525264e-06
Iter: 1135 loss: 7.39380448e-06
Iter: 1136 loss: 7.39211646e-06
Iter: 1137 loss: 7.39637835e-06
Iter: 1138 loss: 7.39154802e-06
Iter: 1139 loss: 7.38980907e-06
Iter: 1140 loss: 7.40681207e-06
Iter: 1141 loss: 7.38968811e-06
Iter: 1142 loss: 7.38853669e-06
Iter: 1143 loss: 7.39562438e-06
Iter: 1144 loss: 7.388433e-06
Iter: 1145 loss: 7.38728068e-06
Iter: 1146 loss: 7.38593189e-06
Iter: 1147 loss: 7.38574545e-06
Iter: 1148 loss: 7.38421113e-06
Iter: 1149 loss: 7.38509743e-06
Iter: 1150 loss: 7.38319795e-06
Iter: 1151 loss: 7.38114886e-06
Iter: 1152 loss: 7.38635117e-06
Iter: 1153 loss: 7.38032031e-06
Iter: 1154 loss: 7.37860637e-06
Iter: 1155 loss: 7.38076142e-06
Iter: 1156 loss: 7.37764094e-06
Iter: 1157 loss: 7.37639675e-06
Iter: 1158 loss: 7.37630126e-06
Iter: 1159 loss: 7.37492746e-06
Iter: 1160 loss: 7.37358732e-06
Iter: 1161 loss: 7.37317168e-06
Iter: 1162 loss: 7.37150549e-06
Iter: 1163 loss: 7.37952314e-06
Iter: 1164 loss: 7.37116807e-06
Iter: 1165 loss: 7.36970424e-06
Iter: 1166 loss: 7.38117706e-06
Iter: 1167 loss: 7.36966058e-06
Iter: 1168 loss: 7.36828224e-06
Iter: 1169 loss: 7.36633319e-06
Iter: 1170 loss: 7.3662236e-06
Iter: 1171 loss: 7.36422e-06
Iter: 1172 loss: 7.36747643e-06
Iter: 1173 loss: 7.36331413e-06
Iter: 1174 loss: 7.36272705e-06
Iter: 1175 loss: 7.36227639e-06
Iter: 1176 loss: 7.36127549e-06
Iter: 1177 loss: 7.36062702e-06
Iter: 1178 loss: 7.3601982e-06
Iter: 1179 loss: 7.35869799e-06
Iter: 1180 loss: 7.36680249e-06
Iter: 1181 loss: 7.35841149e-06
Iter: 1182 loss: 7.35768117e-06
Iter: 1183 loss: 7.35616186e-06
Iter: 1184 loss: 7.39206098e-06
Iter: 1185 loss: 7.35616868e-06
Iter: 1186 loss: 7.35411095e-06
Iter: 1187 loss: 7.36053516e-06
Iter: 1188 loss: 7.3534211e-06
Iter: 1189 loss: 7.35172307e-06
Iter: 1190 loss: 7.35356571e-06
Iter: 1191 loss: 7.35085177e-06
Iter: 1192 loss: 7.34875812e-06
Iter: 1193 loss: 7.36550282e-06
Iter: 1194 loss: 7.34861442e-06
Iter: 1195 loss: 7.34746754e-06
Iter: 1196 loss: 7.35703043e-06
Iter: 1197 loss: 7.34730838e-06
Iter: 1198 loss: 7.34596688e-06
Iter: 1199 loss: 7.34470905e-06
Iter: 1200 loss: 7.34439709e-06
Iter: 1201 loss: 7.342956e-06
Iter: 1202 loss: 7.3458209e-06
Iter: 1203 loss: 7.34240348e-06
Iter: 1204 loss: 7.34036939e-06
Iter: 1205 loss: 7.34995638e-06
Iter: 1206 loss: 7.33998058e-06
Iter: 1207 loss: 7.33858906e-06
Iter: 1208 loss: 7.33847173e-06
Iter: 1209 loss: 7.3373858e-06
Iter: 1210 loss: 7.33558682e-06
Iter: 1211 loss: 7.33915886e-06
Iter: 1212 loss: 7.33492197e-06
Iter: 1213 loss: 7.33393654e-06
Iter: 1214 loss: 7.33367233e-06
Iter: 1215 loss: 7.33303e-06
Iter: 1216 loss: 7.33131401e-06
Iter: 1217 loss: 7.34241712e-06
Iter: 1218 loss: 7.33091929e-06
Iter: 1219 loss: 7.32891385e-06
Iter: 1220 loss: 7.35652293e-06
Iter: 1221 loss: 7.32893841e-06
Iter: 1222 loss: 7.32785293e-06
Iter: 1223 loss: 7.3260735e-06
Iter: 1224 loss: 7.3260212e-06
Iter: 1225 loss: 7.32382e-06
Iter: 1226 loss: 7.33070101e-06
Iter: 1227 loss: 7.32319768e-06
Iter: 1228 loss: 7.32113313e-06
Iter: 1229 loss: 7.32894659e-06
Iter: 1230 loss: 7.32067065e-06
Iter: 1231 loss: 7.31884e-06
Iter: 1232 loss: 7.32938179e-06
Iter: 1233 loss: 7.3186211e-06
Iter: 1234 loss: 7.31679756e-06
Iter: 1235 loss: 7.32686476e-06
Iter: 1236 loss: 7.31651e-06
Iter: 1237 loss: 7.31532327e-06
Iter: 1238 loss: 7.31658974e-06
Iter: 1239 loss: 7.31469027e-06
Iter: 1240 loss: 7.31334239e-06
Iter: 1241 loss: 7.31359523e-06
Iter: 1242 loss: 7.31231694e-06
Iter: 1243 loss: 7.31025921e-06
Iter: 1244 loss: 7.32867375e-06
Iter: 1245 loss: 7.31012369e-06
Iter: 1246 loss: 7.30922056e-06
Iter: 1247 loss: 7.30792226e-06
Iter: 1248 loss: 7.30783267e-06
Iter: 1249 loss: 7.30714783e-06
Iter: 1250 loss: 7.30687043e-06
Iter: 1251 loss: 7.30596e-06
Iter: 1252 loss: 7.30424472e-06
Iter: 1253 loss: 7.33563911e-06
Iter: 1254 loss: 7.30421334e-06
Iter: 1255 loss: 7.30258853e-06
Iter: 1256 loss: 7.30715465e-06
Iter: 1257 loss: 7.30205738e-06
Iter: 1258 loss: 7.30048578e-06
Iter: 1259 loss: 7.31214232e-06
Iter: 1260 loss: 7.30037664e-06
Iter: 1261 loss: 7.29884232e-06
Iter: 1262 loss: 7.29625617e-06
Iter: 1263 loss: 7.36160973e-06
Iter: 1264 loss: 7.29623389e-06
Iter: 1265 loss: 7.29396379e-06
Iter: 1266 loss: 7.29424028e-06
Iter: 1267 loss: 7.292173e-06
Iter: 1268 loss: 7.28990653e-06
Iter: 1269 loss: 7.28990472e-06
Iter: 1270 loss: 7.2878729e-06
Iter: 1271 loss: 7.30085094e-06
Iter: 1272 loss: 7.28762e-06
Iter: 1273 loss: 7.28648592e-06
Iter: 1274 loss: 7.28766281e-06
Iter: 1275 loss: 7.28579425e-06
Iter: 1276 loss: 7.28440045e-06
Iter: 1277 loss: 7.28471696e-06
Iter: 1278 loss: 7.28338819e-06
Iter: 1279 loss: 7.28131818e-06
Iter: 1280 loss: 7.30192687e-06
Iter: 1281 loss: 7.28125406e-06
Iter: 1282 loss: 7.2803291e-06
Iter: 1283 loss: 7.28010264e-06
Iter: 1284 loss: 7.27946281e-06
Iter: 1285 loss: 7.27825045e-06
Iter: 1286 loss: 7.2957464e-06
Iter: 1287 loss: 7.27823226e-06
Iter: 1288 loss: 7.27700444e-06
Iter: 1289 loss: 7.27543284e-06
Iter: 1290 loss: 7.27531642e-06
Iter: 1291 loss: 7.27386168e-06
Iter: 1292 loss: 7.27193947e-06
Iter: 1293 loss: 7.27188763e-06
Iter: 1294 loss: 7.27091037e-06
Iter: 1295 loss: 7.27042652e-06
Iter: 1296 loss: 7.26935195e-06
Iter: 1297 loss: 7.26751887e-06
Iter: 1298 loss: 7.31386081e-06
Iter: 1299 loss: 7.26751159e-06
Iter: 1300 loss: 7.2652083e-06
Iter: 1301 loss: 7.27337101e-06
Iter: 1302 loss: 7.26463259e-06
Iter: 1303 loss: 7.26279814e-06
Iter: 1304 loss: 7.265573e-06
Iter: 1305 loss: 7.26199869e-06
Iter: 1306 loss: 7.26080179e-06
Iter: 1307 loss: 7.26061035e-06
Iter: 1308 loss: 7.25970222e-06
Iter: 1309 loss: 7.25780865e-06
Iter: 1310 loss: 7.29233761e-06
Iter: 1311 loss: 7.25780319e-06
Iter: 1312 loss: 7.25604605e-06
Iter: 1313 loss: 7.2660996e-06
Iter: 1314 loss: 7.25572045e-06
Iter: 1315 loss: 7.25383234e-06
Iter: 1316 loss: 7.26124e-06
Iter: 1317 loss: 7.25340487e-06
Iter: 1318 loss: 7.25169139e-06
Iter: 1319 loss: 7.25513883e-06
Iter: 1320 loss: 7.25101745e-06
Iter: 1321 loss: 7.24983238e-06
Iter: 1322 loss: 7.25422706e-06
Iter: 1323 loss: 7.24955771e-06
Iter: 1324 loss: 7.24785605e-06
Iter: 1325 loss: 7.25161408e-06
Iter: 1326 loss: 7.24717756e-06
Iter: 1327 loss: 7.24591064e-06
Iter: 1328 loss: 7.24514757e-06
Iter: 1329 loss: 7.24467191e-06
Iter: 1330 loss: 7.24301026e-06
Iter: 1331 loss: 7.24559504e-06
Iter: 1332 loss: 7.24226356e-06
Iter: 1333 loss: 7.24069969e-06
Iter: 1334 loss: 7.25958e-06
Iter: 1335 loss: 7.24068968e-06
Iter: 1336 loss: 7.23933726e-06
Iter: 1337 loss: 7.23795256e-06
Iter: 1338 loss: 7.23761877e-06
Iter: 1339 loss: 7.23575704e-06
Iter: 1340 loss: 7.23882886e-06
Iter: 1341 loss: 7.23492303e-06
Iter: 1342 loss: 7.23330049e-06
Iter: 1343 loss: 7.23336552e-06
Iter: 1344 loss: 7.23194898e-06
Iter: 1345 loss: 7.23422727e-06
Iter: 1346 loss: 7.23134508e-06
Iter: 1347 loss: 7.2301018e-06
Iter: 1348 loss: 7.22906407e-06
Iter: 1349 loss: 7.22872073e-06
Iter: 1350 loss: 7.22704772e-06
Iter: 1351 loss: 7.24394704e-06
Iter: 1352 loss: 7.22702316e-06
Iter: 1353 loss: 7.22507593e-06
Iter: 1354 loss: 7.22473033e-06
Iter: 1355 loss: 7.22346e-06
Iter: 1356 loss: 7.22229652e-06
Iter: 1357 loss: 7.22227651e-06
Iter: 1358 loss: 7.22121877e-06
Iter: 1359 loss: 7.22295681e-06
Iter: 1360 loss: 7.22068489e-06
Iter: 1361 loss: 7.21954348e-06
Iter: 1362 loss: 7.21695505e-06
Iter: 1363 loss: 7.25578229e-06
Iter: 1364 loss: 7.2168823e-06
Iter: 1365 loss: 7.21502965e-06
Iter: 1366 loss: 7.2354328e-06
Iter: 1367 loss: 7.21505785e-06
Iter: 1368 loss: 7.21332162e-06
Iter: 1369 loss: 7.21675042e-06
Iter: 1370 loss: 7.21271135e-06
Iter: 1371 loss: 7.21085416e-06
Iter: 1372 loss: 7.22063396e-06
Iter: 1373 loss: 7.21059496e-06
Iter: 1374 loss: 7.20940079e-06
Iter: 1375 loss: 7.2070161e-06
Iter: 1376 loss: 7.24873235e-06
Iter: 1377 loss: 7.20700655e-06
Iter: 1378 loss: 7.2062694e-06
Iter: 1379 loss: 7.20572643e-06
Iter: 1380 loss: 7.20456319e-06
Iter: 1381 loss: 7.2047369e-06
Iter: 1382 loss: 7.20354728e-06
Iter: 1383 loss: 7.20186745e-06
Iter: 1384 loss: 7.20174194e-06
Iter: 1385 loss: 7.20049547e-06
Iter: 1386 loss: 7.19853051e-06
Iter: 1387 loss: 7.20363187e-06
Iter: 1388 loss: 7.1979407e-06
Iter: 1389 loss: 7.19598665e-06
Iter: 1390 loss: 7.22334835e-06
Iter: 1391 loss: 7.1959771e-06
Iter: 1392 loss: 7.19489253e-06
Iter: 1393 loss: 7.19568561e-06
Iter: 1394 loss: 7.19429318e-06
Iter: 1395 loss: 7.19244599e-06
Iter: 1396 loss: 7.19566106e-06
Iter: 1397 loss: 7.19152e-06
Iter: 1398 loss: 7.19019954e-06
Iter: 1399 loss: 7.18899082e-06
Iter: 1400 loss: 7.18871115e-06
Iter: 1401 loss: 7.18678712e-06
Iter: 1402 loss: 7.19285799e-06
Iter: 1403 loss: 7.18628962e-06
Iter: 1404 loss: 7.1846589e-06
Iter: 1405 loss: 7.19330819e-06
Iter: 1406 loss: 7.18445426e-06
Iter: 1407 loss: 7.18276942e-06
Iter: 1408 loss: 7.18868432e-06
Iter: 1409 loss: 7.18235106e-06
Iter: 1410 loss: 7.18084902e-06
Iter: 1411 loss: 7.18126103e-06
Iter: 1412 loss: 7.17976445e-06
Iter: 1413 loss: 7.17771945e-06
Iter: 1414 loss: 7.18218598e-06
Iter: 1415 loss: 7.17698822e-06
Iter: 1416 loss: 7.17601688e-06
Iter: 1417 loss: 7.17584e-06
Iter: 1418 loss: 7.17492412e-06
Iter: 1419 loss: 7.17281591e-06
Iter: 1420 loss: 7.20195658e-06
Iter: 1421 loss: 7.17270223e-06
Iter: 1422 loss: 7.17079547e-06
Iter: 1423 loss: 7.1815e-06
Iter: 1424 loss: 7.17048079e-06
Iter: 1425 loss: 7.16906561e-06
Iter: 1426 loss: 7.18266256e-06
Iter: 1427 loss: 7.16898876e-06
Iter: 1428 loss: 7.16747445e-06
Iter: 1429 loss: 7.17081139e-06
Iter: 1430 loss: 7.16693739e-06
Iter: 1431 loss: 7.16574687e-06
Iter: 1432 loss: 7.17885177e-06
Iter: 1433 loss: 7.16575369e-06
Iter: 1434 loss: 7.16493651e-06
Iter: 1435 loss: 7.16319846e-06
Iter: 1436 loss: 7.18872161e-06
Iter: 1437 loss: 7.16313161e-06
Iter: 1438 loss: 7.16121576e-06
Iter: 1439 loss: 7.15985971e-06
Iter: 1440 loss: 7.1592558e-06
Iter: 1441 loss: 7.15769283e-06
Iter: 1442 loss: 7.15762781e-06
Iter: 1443 loss: 7.15594297e-06
Iter: 1444 loss: 7.15676333e-06
Iter: 1445 loss: 7.15478291e-06
Iter: 1446 loss: 7.15245187e-06
Iter: 1447 loss: 7.16249178e-06
Iter: 1448 loss: 7.15197029e-06
Iter: 1449 loss: 7.15062379e-06
Iter: 1450 loss: 7.15038095e-06
Iter: 1451 loss: 7.14949101e-06
Iter: 1452 loss: 7.14727503e-06
Iter: 1453 loss: 7.168087e-06
Iter: 1454 loss: 7.1472432e-06
Iter: 1455 loss: 7.14568932e-06
Iter: 1456 loss: 7.1496579e-06
Iter: 1457 loss: 7.14512498e-06
Iter: 1458 loss: 7.14402813e-06
Iter: 1459 loss: 7.14250109e-06
Iter: 1460 loss: 7.14241e-06
Iter: 1461 loss: 7.14149519e-06
Iter: 1462 loss: 7.141281e-06
Iter: 1463 loss: 7.1400882e-06
Iter: 1464 loss: 7.14054113e-06
Iter: 1465 loss: 7.1392642e-06
Iter: 1466 loss: 7.13787222e-06
Iter: 1467 loss: 7.14888938e-06
Iter: 1468 loss: 7.13774e-06
Iter: 1469 loss: 7.13685722e-06
Iter: 1470 loss: 7.13561894e-06
Iter: 1471 loss: 7.13555e-06
Iter: 1472 loss: 7.13391501e-06
Iter: 1473 loss: 7.13314557e-06
Iter: 1474 loss: 7.13227291e-06
Iter: 1475 loss: 7.13033933e-06
Iter: 1476 loss: 7.1434506e-06
Iter: 1477 loss: 7.13016561e-06
Iter: 1478 loss: 7.12828751e-06
Iter: 1479 loss: 7.14104362e-06
Iter: 1480 loss: 7.12807105e-06
Iter: 1481 loss: 7.12656401e-06
Iter: 1482 loss: 7.12698966e-06
Iter: 1483 loss: 7.12539713e-06
Iter: 1484 loss: 7.12402607e-06
Iter: 1485 loss: 7.13259396e-06
Iter: 1486 loss: 7.12383689e-06
Iter: 1487 loss: 7.12248766e-06
Iter: 1488 loss: 7.12827114e-06
Iter: 1489 loss: 7.12232759e-06
Iter: 1490 loss: 7.12068822e-06
Iter: 1491 loss: 7.1192444e-06
Iter: 1492 loss: 7.11889516e-06
Iter: 1493 loss: 7.11725761e-06
Iter: 1494 loss: 7.12752581e-06
Iter: 1495 loss: 7.1170748e-06
Iter: 1496 loss: 7.11583107e-06
Iter: 1497 loss: 7.12089059e-06
Iter: 1498 loss: 7.11551456e-06
Iter: 1499 loss: 7.11374287e-06
Iter: 1500 loss: 7.11980147e-06
Iter: 1501 loss: 7.11328539e-06
Iter: 1502 loss: 7.11237772e-06
Iter: 1503 loss: 7.11798384e-06
Iter: 1504 loss: 7.11223129e-06
Iter: 1505 loss: 7.11131042e-06
Iter: 1506 loss: 7.10906897e-06
Iter: 1507 loss: 7.13135705e-06
Iter: 1508 loss: 7.10873064e-06
Iter: 1509 loss: 7.10651148e-06
Iter: 1510 loss: 7.11462417e-06
Iter: 1511 loss: 7.10598e-06
Iter: 1512 loss: 7.10407312e-06
Iter: 1513 loss: 7.11820303e-06
Iter: 1514 loss: 7.10390259e-06
Iter: 1515 loss: 7.10271661e-06
Iter: 1516 loss: 7.11306848e-06
Iter: 1517 loss: 7.10260656e-06
Iter: 1518 loss: 7.10128e-06
Iter: 1519 loss: 7.09919277e-06
Iter: 1520 loss: 7.09919095e-06
Iter: 1521 loss: 7.09758e-06
Iter: 1522 loss: 7.11549183e-06
Iter: 1523 loss: 7.09751293e-06
Iter: 1524 loss: 7.09597e-06
Iter: 1525 loss: 7.10110771e-06
Iter: 1526 loss: 7.09548112e-06
Iter: 1527 loss: 7.09402093e-06
Iter: 1528 loss: 7.09510823e-06
Iter: 1529 loss: 7.09322285e-06
Iter: 1530 loss: 7.09164397e-06
Iter: 1531 loss: 7.09374581e-06
Iter: 1532 loss: 7.09087499e-06
Iter: 1533 loss: 7.09014103e-06
Iter: 1534 loss: 7.09003962e-06
Iter: 1535 loss: 7.08910102e-06
Iter: 1536 loss: 7.08793596e-06
Iter: 1537 loss: 7.08790503e-06
Iter: 1538 loss: 7.08618245e-06
Iter: 1539 loss: 7.09431e-06
Iter: 1540 loss: 7.08586322e-06
Iter: 1541 loss: 7.0845781e-06
Iter: 1542 loss: 7.08546759e-06
Iter: 1543 loss: 7.08373955e-06
Iter: 1544 loss: 7.08242806e-06
Iter: 1545 loss: 7.08026937e-06
Iter: 1546 loss: 7.08017524e-06
Iter: 1547 loss: 7.07811296e-06
Iter: 1548 loss: 7.10007589e-06
Iter: 1549 loss: 7.07809068e-06
Iter: 1550 loss: 7.07635945e-06
Iter: 1551 loss: 7.09100868e-06
Iter: 1552 loss: 7.07628351e-06
Iter: 1553 loss: 7.07502932e-06
Iter: 1554 loss: 7.07498702e-06
Iter: 1555 loss: 7.07393929e-06
Iter: 1556 loss: 7.07227082e-06
Iter: 1557 loss: 7.07732943e-06
Iter: 1558 loss: 7.07177696e-06
Iter: 1559 loss: 7.06992796e-06
Iter: 1560 loss: 7.08582729e-06
Iter: 1561 loss: 7.0698834e-06
Iter: 1562 loss: 7.06886203e-06
Iter: 1563 loss: 7.06751325e-06
Iter: 1564 loss: 7.06745959e-06
Iter: 1565 loss: 7.06566425e-06
Iter: 1566 loss: 7.0752285e-06
Iter: 1567 loss: 7.06533638e-06
Iter: 1568 loss: 7.0639212e-06
Iter: 1569 loss: 7.08575089e-06
Iter: 1570 loss: 7.06391802e-06
Iter: 1571 loss: 7.06290666e-06
Iter: 1572 loss: 7.06222e-06
Iter: 1573 loss: 7.06180799e-06
Iter: 1574 loss: 7.06059654e-06
Iter: 1575 loss: 7.0699125e-06
Iter: 1576 loss: 7.06046922e-06
Iter: 1577 loss: 7.05951152e-06
Iter: 1578 loss: 7.05782077e-06
Iter: 1579 loss: 7.05782804e-06
Iter: 1580 loss: 7.05597722e-06
Iter: 1581 loss: 7.06023047e-06
Iter: 1582 loss: 7.05529874e-06
Iter: 1583 loss: 7.05314051e-06
Iter: 1584 loss: 7.05856473e-06
Iter: 1585 loss: 7.05238472e-06
Iter: 1586 loss: 7.05124e-06
Iter: 1587 loss: 7.05121147e-06
Iter: 1588 loss: 7.05003276e-06
Iter: 1589 loss: 7.04802278e-06
Iter: 1590 loss: 7.04798094e-06
Iter: 1591 loss: 7.04658032e-06
Iter: 1592 loss: 7.04659851e-06
Iter: 1593 loss: 7.04513423e-06
Iter: 1594 loss: 7.04581726e-06
Iter: 1595 loss: 7.04421382e-06
Iter: 1596 loss: 7.04262766e-06
Iter: 1597 loss: 7.04609965e-06
Iter: 1598 loss: 7.04211e-06
Iter: 1599 loss: 7.04105241e-06
Iter: 1600 loss: 7.04840113e-06
Iter: 1601 loss: 7.04099648e-06
Iter: 1602 loss: 7.03969454e-06
Iter: 1603 loss: 7.04128797e-06
Iter: 1604 loss: 7.03896694e-06
Iter: 1605 loss: 7.03774913e-06
Iter: 1606 loss: 7.03966771e-06
Iter: 1607 loss: 7.03713158e-06
Iter: 1608 loss: 7.03579917e-06
Iter: 1609 loss: 7.04057402e-06
Iter: 1610 loss: 7.03545174e-06
Iter: 1611 loss: 7.03429305e-06
Iter: 1612 loss: 7.03399382e-06
Iter: 1613 loss: 7.03328e-06
Iter: 1614 loss: 7.03155638e-06
Iter: 1615 loss: 7.03070418e-06
Iter: 1616 loss: 7.02992747e-06
Iter: 1617 loss: 7.02817488e-06
Iter: 1618 loss: 7.05407e-06
Iter: 1619 loss: 7.0281435e-06
Iter: 1620 loss: 7.02665966e-06
Iter: 1621 loss: 7.03219e-06
Iter: 1622 loss: 7.02630132e-06
Iter: 1623 loss: 7.0246756e-06
Iter: 1624 loss: 7.02689204e-06
Iter: 1625 loss: 7.02379521e-06
Iter: 1626 loss: 7.0227652e-06
Iter: 1627 loss: 7.02875332e-06
Iter: 1628 loss: 7.0226597e-06
Iter: 1629 loss: 7.02126863e-06
Iter: 1630 loss: 7.0209544e-06
Iter: 1631 loss: 7.02007719e-06
Iter: 1632 loss: 7.01874887e-06
Iter: 1633 loss: 7.01989848e-06
Iter: 1634 loss: 7.0180181e-06
Iter: 1635 loss: 7.01716408e-06
Iter: 1636 loss: 7.01695581e-06
Iter: 1637 loss: 7.01615863e-06
Iter: 1638 loss: 7.01523959e-06
Iter: 1639 loss: 7.01504405e-06
Iter: 1640 loss: 7.01375393e-06
Iter: 1641 loss: 7.01699e-06
Iter: 1642 loss: 7.01334e-06
Iter: 1643 loss: 7.0116821e-06
Iter: 1644 loss: 7.01411318e-06
Iter: 1645 loss: 7.01087538e-06
Iter: 1646 loss: 7.00964938e-06
Iter: 1647 loss: 7.01151248e-06
Iter: 1648 loss: 7.00907913e-06
Iter: 1649 loss: 7.00758756e-06
Iter: 1650 loss: 7.00732471e-06
Iter: 1651 loss: 7.00624696e-06
Iter: 1652 loss: 7.00430519e-06
Iter: 1653 loss: 7.01636236e-06
Iter: 1654 loss: 7.00404962e-06
Iter: 1655 loss: 7.00279088e-06
Iter: 1656 loss: 7.00280361e-06
Iter: 1657 loss: 7.00196e-06
Iter: 1658 loss: 7.00064857e-06
Iter: 1659 loss: 7.00057353e-06
Iter: 1660 loss: 6.99939574e-06
Iter: 1661 loss: 6.99943939e-06
Iter: 1662 loss: 6.99843e-06
Iter: 1663 loss: 6.9982143e-06
Iter: 1664 loss: 6.99755492e-06
Iter: 1665 loss: 6.99606881e-06
Iter: 1666 loss: 6.99543216e-06
Iter: 1667 loss: 6.99466818e-06
Iter: 1668 loss: 6.99492739e-06
Iter: 1669 loss: 6.99373504e-06
Iter: 1670 loss: 6.99324255e-06
Iter: 1671 loss: 6.99175553e-06
Iter: 1672 loss: 6.99887141e-06
Iter: 1673 loss: 6.99130669e-06
Iter: 1674 loss: 6.98983058e-06
Iter: 1675 loss: 6.98981512e-06
Iter: 1676 loss: 6.98884e-06
Iter: 1677 loss: 6.98854183e-06
Iter: 1678 loss: 6.98797385e-06
Iter: 1679 loss: 6.98647455e-06
Iter: 1680 loss: 6.98591793e-06
Iter: 1681 loss: 6.98502845e-06
Iter: 1682 loss: 6.98332406e-06
Iter: 1683 loss: 6.99635484e-06
Iter: 1684 loss: 6.9831849e-06
Iter: 1685 loss: 6.9816715e-06
Iter: 1686 loss: 6.98388158e-06
Iter: 1687 loss: 6.98095391e-06
Iter: 1688 loss: 6.97975065e-06
Iter: 1689 loss: 6.97978794e-06
Iter: 1690 loss: 6.97890755e-06
Iter: 1691 loss: 6.97715768e-06
Iter: 1692 loss: 7.00422243e-06
Iter: 1693 loss: 6.97700398e-06
Iter: 1694 loss: 6.97611267e-06
Iter: 1695 loss: 6.97582846e-06
Iter: 1696 loss: 6.9750331e-06
Iter: 1697 loss: 6.97324322e-06
Iter: 1698 loss: 7.00436703e-06
Iter: 1699 loss: 6.97318956e-06
Iter: 1700 loss: 6.97140058e-06
Iter: 1701 loss: 6.98712211e-06
Iter: 1702 loss: 6.9713451e-06
Iter: 1703 loss: 6.97045743e-06
Iter: 1704 loss: 6.97036512e-06
Iter: 1705 loss: 6.96979487e-06
Iter: 1706 loss: 6.96805273e-06
Iter: 1707 loss: 6.9746452e-06
Iter: 1708 loss: 6.96731649e-06
Iter: 1709 loss: 6.96653524e-06
Iter: 1710 loss: 6.96618e-06
Iter: 1711 loss: 6.96536517e-06
Iter: 1712 loss: 6.96393636e-06
Iter: 1713 loss: 6.96389e-06
Iter: 1714 loss: 6.96228881e-06
Iter: 1715 loss: 6.96143343e-06
Iter: 1716 loss: 6.96071857e-06
Iter: 1717 loss: 6.95868721e-06
Iter: 1718 loss: 6.98995564e-06
Iter: 1719 loss: 6.95867038e-06
Iter: 1720 loss: 6.95720883e-06
Iter: 1721 loss: 6.95710969e-06
Iter: 1722 loss: 6.95594463e-06
Iter: 1723 loss: 6.9541029e-06
Iter: 1724 loss: 6.98090935e-06
Iter: 1725 loss: 6.95408926e-06
Iter: 1726 loss: 6.95296e-06
Iter: 1727 loss: 6.95499784e-06
Iter: 1728 loss: 6.95253266e-06
Iter: 1729 loss: 6.95149311e-06
Iter: 1730 loss: 6.95503149e-06
Iter: 1731 loss: 6.95117751e-06
Iter: 1732 loss: 6.94990194e-06
Iter: 1733 loss: 6.95065091e-06
Iter: 1734 loss: 6.9490311e-06
Iter: 1735 loss: 6.94783284e-06
Iter: 1736 loss: 6.95062454e-06
Iter: 1737 loss: 6.94727214e-06
Iter: 1738 loss: 6.94627215e-06
Iter: 1739 loss: 6.9462626e-06
Iter: 1740 loss: 6.94577602e-06
Iter: 1741 loss: 6.94422943e-06
Iter: 1742 loss: 6.94662231e-06
Iter: 1743 loss: 6.94313803e-06
Iter: 1744 loss: 6.94191067e-06
Iter: 1745 loss: 6.94164282e-06
Iter: 1746 loss: 6.94047912e-06
Iter: 1747 loss: 6.94123173e-06
Iter: 1748 loss: 6.93961965e-06
Iter: 1749 loss: 6.93828861e-06
Iter: 1750 loss: 6.93803122e-06
Iter: 1751 loss: 6.93716447e-06
Iter: 1752 loss: 6.93535958e-06
Iter: 1753 loss: 6.93661559e-06
Iter: 1754 loss: 6.93417633e-06
Iter: 1755 loss: 6.93274887e-06
Iter: 1756 loss: 6.93276934e-06
Iter: 1757 loss: 6.93140191e-06
Iter: 1758 loss: 6.93361335e-06
Iter: 1759 loss: 6.93070751e-06
Iter: 1760 loss: 6.92910817e-06
Iter: 1761 loss: 6.9360276e-06
Iter: 1762 loss: 6.92877893e-06
Iter: 1763 loss: 6.92782532e-06
Iter: 1764 loss: 6.93250331e-06
Iter: 1765 loss: 6.92765798e-06
Iter: 1766 loss: 6.92652839e-06
Iter: 1767 loss: 6.92541471e-06
Iter: 1768 loss: 6.92521189e-06
Iter: 1769 loss: 6.92377489e-06
Iter: 1770 loss: 6.94358368e-06
Iter: 1771 loss: 6.92376761e-06
Iter: 1772 loss: 6.92277354e-06
Iter: 1773 loss: 6.93008133e-06
Iter: 1774 loss: 6.92270714e-06
Iter: 1775 loss: 6.92212552e-06
Iter: 1776 loss: 6.92037065e-06
Iter: 1777 loss: 6.92582307e-06
Iter: 1778 loss: 6.91955756e-06
Iter: 1779 loss: 6.91791138e-06
Iter: 1780 loss: 6.91790137e-06
Iter: 1781 loss: 6.91633386e-06
Iter: 1782 loss: 6.92269759e-06
Iter: 1783 loss: 6.91596915e-06
Iter: 1784 loss: 6.91478726e-06
Iter: 1785 loss: 6.91369223e-06
Iter: 1786 loss: 6.91340483e-06
Iter: 1787 loss: 6.91166042e-06
Iter: 1788 loss: 6.91653349e-06
Iter: 1789 loss: 6.91110199e-06
Iter: 1790 loss: 6.90940215e-06
Iter: 1791 loss: 6.91071637e-06
Iter: 1792 loss: 6.90839261e-06
Iter: 1793 loss: 6.9074049e-06
Iter: 1794 loss: 6.90714705e-06
Iter: 1795 loss: 6.90616162e-06
Iter: 1796 loss: 6.90507e-06
Iter: 1797 loss: 6.90491606e-06
Iter: 1798 loss: 6.90370189e-06
Iter: 1799 loss: 6.92201411e-06
Iter: 1800 loss: 6.90372508e-06
Iter: 1801 loss: 6.90274646e-06
Iter: 1802 loss: 6.90188e-06
Iter: 1803 loss: 6.90160095e-06
Iter: 1804 loss: 6.9000821e-06
Iter: 1805 loss: 6.91789819e-06
Iter: 1806 loss: 6.90010575e-06
Iter: 1807 loss: 6.89878152e-06
Iter: 1808 loss: 6.90198476e-06
Iter: 1809 loss: 6.89835815e-06
Iter: 1810 loss: 6.8975769e-06
Iter: 1811 loss: 6.89589e-06
Iter: 1812 loss: 6.92391268e-06
Iter: 1813 loss: 6.89585886e-06
Iter: 1814 loss: 6.89414719e-06
Iter: 1815 loss: 6.90403931e-06
Iter: 1816 loss: 6.89388526e-06
Iter: 1817 loss: 6.89246917e-06
Iter: 1818 loss: 6.90850538e-06
Iter: 1819 loss: 6.89245462e-06
Iter: 1820 loss: 6.89133867e-06
Iter: 1821 loss: 6.89083481e-06
Iter: 1822 loss: 6.89027092e-06
Iter: 1823 loss: 6.8888412e-06
Iter: 1824 loss: 6.88821729e-06
Iter: 1825 loss: 6.88740693e-06
Iter: 1826 loss: 6.88538285e-06
Iter: 1827 loss: 6.88899127e-06
Iter: 1828 loss: 6.88446562e-06
Iter: 1829 loss: 6.88339969e-06
Iter: 1830 loss: 6.88314958e-06
Iter: 1831 loss: 6.88208547e-06
Iter: 1832 loss: 6.88318e-06
Iter: 1833 loss: 6.88156206e-06
Iter: 1834 loss: 6.88018372e-06
Iter: 1835 loss: 6.88131513e-06
Iter: 1836 loss: 6.87943e-06
Iter: 1837 loss: 6.87814872e-06
Iter: 1838 loss: 6.89541048e-06
Iter: 1839 loss: 6.87814827e-06
Iter: 1840 loss: 6.87752527e-06
Iter: 1841 loss: 6.87944748e-06
Iter: 1842 loss: 6.87724969e-06
Iter: 1843 loss: 6.87622287e-06
Iter: 1844 loss: 6.87456486e-06
Iter: 1845 loss: 6.87461124e-06
Iter: 1846 loss: 6.87332704e-06
Iter: 1847 loss: 6.8762929e-06
Iter: 1848 loss: 6.87291868e-06
Iter: 1849 loss: 6.87142438e-06
Iter: 1850 loss: 6.87131114e-06
Iter: 1851 loss: 6.87022748e-06
Iter: 1852 loss: 6.86859903e-06
Iter: 1853 loss: 6.89145782e-06
Iter: 1854 loss: 6.86858675e-06
Iter: 1855 loss: 6.86717385e-06
Iter: 1856 loss: 6.8695158e-06
Iter: 1857 loss: 6.86655312e-06
Iter: 1858 loss: 6.86516341e-06
Iter: 1859 loss: 6.86547264e-06
Iter: 1860 loss: 6.86422482e-06
Iter: 1861 loss: 6.86250314e-06
Iter: 1862 loss: 6.86260728e-06
Iter: 1863 loss: 6.86110161e-06
Iter: 1864 loss: 6.85918076e-06
Iter: 1865 loss: 6.86755538e-06
Iter: 1866 loss: 6.85869509e-06
Iter: 1867 loss: 6.85766963e-06
Iter: 1868 loss: 6.85759369e-06
Iter: 1869 loss: 6.85653868e-06
Iter: 1870 loss: 6.85576742e-06
Iter: 1871 loss: 6.85542091e-06
Iter: 1872 loss: 6.85411487e-06
Iter: 1873 loss: 6.8656218e-06
Iter: 1874 loss: 6.85405712e-06
Iter: 1875 loss: 6.85301302e-06
Iter: 1876 loss: 6.85871873e-06
Iter: 1877 loss: 6.8528625e-06
Iter: 1878 loss: 6.85196528e-06
Iter: 1879 loss: 6.85372106e-06
Iter: 1880 loss: 6.85163559e-06
Iter: 1881 loss: 6.8505351e-06
Iter: 1882 loss: 6.84873794e-06
Iter: 1883 loss: 6.8487866e-06
Iter: 1884 loss: 6.84717088e-06
Iter: 1885 loss: 6.84637143e-06
Iter: 1886 loss: 6.84559e-06
Iter: 1887 loss: 6.84411043e-06
Iter: 1888 loss: 6.84396582e-06
Iter: 1889 loss: 6.84273164e-06
Iter: 1890 loss: 6.84666747e-06
Iter: 1891 loss: 6.84238967e-06
Iter: 1892 loss: 6.84143106e-06
Iter: 1893 loss: 6.84219549e-06
Iter: 1894 loss: 6.84085035e-06
Iter: 1895 loss: 6.83946564e-06
Iter: 1896 loss: 6.83789e-06
Iter: 1897 loss: 6.83776761e-06
Iter: 1898 loss: 6.83563849e-06
Iter: 1899 loss: 6.83866256e-06
Iter: 1900 loss: 6.83462713e-06
Iter: 1901 loss: 6.83235248e-06
Iter: 1902 loss: 6.86208932e-06
Iter: 1903 loss: 6.83231792e-06
Iter: 1904 loss: 6.83106327e-06
Iter: 1905 loss: 6.84672341e-06
Iter: 1906 loss: 6.83108874e-06
Iter: 1907 loss: 6.82994414e-06
Iter: 1908 loss: 6.8287718e-06
Iter: 1909 loss: 6.82855352e-06
Iter: 1910 loss: 6.82809514e-06
Iter: 1911 loss: 6.82771179e-06
Iter: 1912 loss: 6.82712971e-06
Iter: 1913 loss: 6.82596101e-06
Iter: 1914 loss: 6.84618544e-06
Iter: 1915 loss: 6.82591326e-06
Iter: 1916 loss: 6.82422478e-06
Iter: 1917 loss: 6.8314821e-06
Iter: 1918 loss: 6.82387235e-06
Iter: 1919 loss: 6.82301561e-06
Iter: 1920 loss: 6.82128848e-06
Iter: 1921 loss: 6.85600116e-06
Iter: 1922 loss: 6.8212712e-06
Iter: 1923 loss: 6.81906931e-06
Iter: 1924 loss: 6.83437884e-06
Iter: 1925 loss: 6.81887241e-06
Iter: 1926 loss: 6.81789061e-06
Iter: 1927 loss: 6.81790425e-06
Iter: 1928 loss: 6.81682286e-06
Iter: 1929 loss: 6.81515803e-06
Iter: 1930 loss: 6.81514302e-06
Iter: 1931 loss: 6.81362508e-06
Iter: 1932 loss: 6.81487063e-06
Iter: 1933 loss: 6.8126883e-06
Iter: 1934 loss: 6.81086749e-06
Iter: 1935 loss: 6.82557675e-06
Iter: 1936 loss: 6.81069105e-06
Iter: 1937 loss: 6.80938774e-06
Iter: 1938 loss: 6.81021766e-06
Iter: 1939 loss: 6.80849462e-06
Iter: 1940 loss: 6.80693756e-06
Iter: 1941 loss: 6.8212139e-06
Iter: 1942 loss: 6.8068216e-06
Iter: 1943 loss: 6.80538551e-06
Iter: 1944 loss: 6.81125312e-06
Iter: 1945 loss: 6.80507219e-06
Iter: 1946 loss: 6.80404e-06
Iter: 1947 loss: 6.80947824e-06
Iter: 1948 loss: 6.80383255e-06
Iter: 1949 loss: 6.80275116e-06
Iter: 1950 loss: 6.80378e-06
Iter: 1951 loss: 6.80201174e-06
Iter: 1952 loss: 6.80100493e-06
Iter: 1953 loss: 6.79993673e-06
Iter: 1954 loss: 6.79974164e-06
Iter: 1955 loss: 6.7982005e-06
Iter: 1956 loss: 6.8212e-06
Iter: 1957 loss: 6.79817367e-06
Iter: 1958 loss: 6.79743198e-06
Iter: 1959 loss: 6.79548521e-06
Iter: 1960 loss: 6.81480105e-06
Iter: 1961 loss: 6.79526647e-06
Iter: 1962 loss: 6.79364348e-06
Iter: 1963 loss: 6.79360346e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi2
+ date
Sat Oct 31 22:28:57 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb01780d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb0192950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb0192d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb0192378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb00b4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6bb00b28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b601e1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b6007e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b60134268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b6014a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b587bf840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b601127b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b60112c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b60115950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b60177ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b60177c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b6003d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b60222bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b587677b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b58768620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b58768bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b5863d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b600ce8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b600b8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b600b8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b585fdea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b58527950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b585bb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b585bb378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b585bb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b587b5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b585b81e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b585b8bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b585b8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b5855cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6b5848e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 1.9948783
test_loss: 1.992409
train_loss: 1.9965655
test_loss: 1.9930001
train_loss: 1.9956636
test_loss: 1.992745
train_loss: 1.9992224
test_loss: 1.9939697
train_loss: 1.9980925
test_loss: 1.9931213
train_loss: 1.9842126
test_loss: 1.9932936
train_loss: 1.9908551
test_loss: 1.9936876
train_loss: 1.9993675
test_loss: 1.9928775
train_loss: 1.9957182
test_loss: 1.9927555
train_loss: 1.9926482
test_loss: 1.9933116
train_loss: 1.9992054
test_loss: 1.9944098
train_loss: 1.9973842
test_loss: 1.9939318
train_loss: 1.9984107
test_loss: 1.9929134
train_loss: 1.9756014
test_loss: 1.9918792
train_loss: 1.9943275
test_loss: 1.9936687
train_loss: 1.9871128
test_loss: 1.9941692
train_loss: 1.9946748
test_loss: 1.9925684
train_loss: 1.9997249
test_loss: 1.9933493
train_loss: 1.9942877
test_loss: 1.9930111
train_loss: 1.9997749
test_loss: 1.9939449
train_loss: 1.9944156
test_loss: 1.9926395
train_loss: 1.9876578
test_loss: 1.9934623
train_loss: 1.9935266
test_loss: 1.9935715
train_loss: 1.9855587
test_loss: 1.9946196
train_loss: 1.9893687
test_loss: 1.995065
train_loss: 1.9876139
test_loss: 1.9936368
train_loss: 1.986339
test_loss: 1.9945787
train_loss: 1.9820333
test_loss: 1.9926512
train_loss: 1.983973
test_loss: 1.9946127
train_loss: 1.9923712
test_loss: 1.9934645
train_loss: 1.9871043
test_loss: 1.9941255
train_loss: 1.9840031
test_loss: 1.993305
train_loss: 1.9900362
test_loss: 1.9957038
train_loss: 1.9886096
test_loss: 1.993761
train_loss: 1.9745501
test_loss: 1.9949967
train_loss: 1.9914654
test_loss: 1.9939222
train_loss: 1.9858184
test_loss: 1.9938704
train_loss: 1.9884914
test_loss: 1.9932152
train_loss: 1.9719987
test_loss: 1.9926496
train_loss: 1.9957616
test_loss: 1.9947746
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efbbc160c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efbbc19d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efbbc19dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efbbc19d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efbbc1492f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efbbc149620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efbbc029d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efbbc054730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efba00672f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efba0067d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efbbc0708c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efba003c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efba003cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb547db378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efba003c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb547f8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb547606a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb547947b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb547089d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efba00c1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efba00c17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efba00a8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb546ea840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb54686598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb546860d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb5464e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb54627510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb54632268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb546321e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb545ed950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb5453d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb5456c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb5456cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb5459e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb5449bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efb5444a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 6394.33643
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Traceback (most recent call last):
  File "biholoNN_train.py", line 172, in <module>
    results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=train_func, initial_position=init_params, max_iterations=max_epochs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 287, in minimize
    parallel_iterations=parallel_iterations)[0]
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 574, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2499, in while_loop_v2
    return_same_structure=True)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2735, in while_loop
    loop_vars = body(*loop_vars)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/lbfgs.py", line 260, in _body
    max_line_search_iterations)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 156, in line_search_step
    max_iterations=max_iterations)  # No search needed for these.
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 259, in hager_zhang
    threshold_use_approximate_wolfe_condition)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/linesearch/hager_zhang.py", line 609, in _prepare_args
    val_initial = value_and_gradients_function(initial_step_size)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow_probability/python/optimizer/bfgs_utils.py", line 251, in _restricted_func
    objective_value, gradient = value_and_gradients_function(pt)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py", line 814, in _call
    results = self._stateful_fn(*args, **kwds)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
	 [[Sum_1/_32]]
  (1) Invalid argument:  Input is not invertible.
	 [[{{node PartitionedCall/gradients/MatrixDeterminant_grad/MatrixInverse}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_f_14711]

Function call stack:
f -> f

++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi2.4
+ date
Sat Oct 31 23:10:55 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd244b6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd244ced90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd243b5d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd244deb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd24453b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd2443f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd242cde18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd2443fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd242ff400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd243109d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd24102158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd24109d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd241098c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd24375f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd24375c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd24375bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd24174510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd24165488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd0c7f7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd2409e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd240aa9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd240aa620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd240649d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd240aa840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd240aa6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd2426a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd0c6f0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd24040ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd240406a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd24031400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd0c622730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd24355730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd24355510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd0c5d6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd0c5f7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcd0c69bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.7723812
test_loss: 0.7879659
train_loss: 0.7700574
test_loss: 0.78679013
train_loss: 0.7802539
test_loss: 0.7855658
train_loss: 0.7879377
test_loss: 0.78404313
train_loss: 0.7945242
test_loss: 0.78237176
train_loss: 0.7636732
test_loss: 0.7804801
train_loss: 0.7764235
test_loss: 0.7786379
train_loss: 0.78289175
test_loss: 0.77692604
train_loss: 0.74820256
test_loss: 0.77463865
train_loss: 0.7936728
test_loss: 0.7725945
train_loss: 0.74947053
test_loss: 0.7705765
train_loss: 0.7937385
test_loss: 0.7684303
train_loss: 0.73042965
test_loss: 0.76618356
train_loss: 0.7996895
test_loss: 0.76403594
train_loss: 0.74126613
test_loss: 0.7617089
train_loss: 0.76771474
test_loss: 0.7595483
train_loss: 0.71124595
test_loss: 0.75699663
train_loss: 0.7317429
test_loss: 0.7547135
train_loss: 0.7269518
test_loss: 0.7523685
train_loss: 0.74889374
test_loss: 0.7502752
train_loss: 0.751901
test_loss: 0.7477854
train_loss: 0.7606473
test_loss: 0.7450876
train_loss: 0.7592766
test_loss: 0.7426616
train_loss: 0.7317094
test_loss: 0.7403063
train_loss: 0.6736187
test_loss: 0.7375665
train_loss: 0.7250768
test_loss: 0.73510075
train_loss: 0.740116
test_loss: 0.73245865
train_loss: 0.71658736
test_loss: 0.7298571
train_loss: 0.7396847
test_loss: 0.7273694
train_loss: 0.7469763
test_loss: 0.7245089
train_loss: 0.748217
test_loss: 0.7216644
train_loss: 0.6949634
test_loss: 0.719133
train_loss: 0.71775055
test_loss: 0.7161292
train_loss: 0.66442925
test_loss: 0.7132923
train_loss: 0.7067937
test_loss: 0.7105476
train_loss: 0.6881548
test_loss: 0.7073765
train_loss: 0.69430804
test_loss: 0.7042786
train_loss: 0.72351444
test_loss: 0.701457
train_loss: 0.7471651
test_loss: 0.6980478
train_loss: 0.6727071
test_loss: 0.6947036
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4442510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4445b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4445950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e4445d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e43ccc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57e43ccbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc2a1e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc30eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc28b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc27e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc30ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc1baea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc1c37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc1878c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc1e9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc218c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc208620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc0f4950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc150620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc1508c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc0bb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc0bb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc08b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc0487b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57bc0486a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57a021a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57a0174840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57a019e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57a019e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57a01d2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57a010a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57a00d6048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57a00bf0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57a00aa730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57a014b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f57a012aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.972020507
Iter: 2 loss: 2.13213611
Iter: 3 loss: 0.7831949
Iter: 4 loss: 0.499464244
Iter: 5 loss: 7257.01123
Iter: 6 loss: 0.499448508
Iter: 7 loss: 0.284115344
Iter: 8 loss: 0.285036445
Iter: 9 loss: 0.203633249
Iter: 10 loss: 0.207522988
Iter: 11 loss: 4462.57
Iter: 12 loss: 0.207521409
Iter: 13 loss: 1256.75171
Iter: 14 loss: 1694.11597
Iter: 15 loss: 0.207517743
Iter: 16 loss: 409.116058
Iter: 17 loss: 0.207517728
Iter: 18 loss: 0.18671
Iter: 19 loss: 0.18671
Iter: 20 loss: 0.101979993
Iter: 21 loss: 0.100862101
Iter: 22 loss: 0.0707868338
Iter: 23 loss: 1615.2074
Iter: 24 loss: 2407.16064
Iter: 25 loss: 0.0714689419
Iter: 26 loss: 0.0839114934
Iter: 27 loss: 1026.80164
Iter: 28 loss: 0.0838688612
Iter: 29 loss: 3597.3103
Iter: 30 loss: 1837.91504
Iter: 31 loss: 604.537354
Iter: 32 loss: 1604.74414
Iter: 33 loss: 0.0838688686
Iter: 34 loss: 0.0718810856
Iter: 35 loss: 0.0718810856
Iter: 36 loss: 0.0595858172
Iter: 37 loss: 0.0632068664
Iter: 38 loss: 0.0539208278
Iter: 39 loss: 0.0524330847
Iter: 40 loss: 0.0511639118
Iter: 41 loss: 0.0449975654
Iter: 42 loss: 0.0563755743
Iter: 43 loss: 0.0417099446
Iter: 44 loss: 0.0330400616
Iter: 45 loss: 0.0330299176
Iter: 46 loss: 0.0301963426
Iter: 47 loss: 0.0384671465
Iter: 48 loss: 0.0299174599
Iter: 49 loss: 0.0283869393
Iter: 50 loss: 0.0312999226
Iter: 51 loss: 0.0275817513
Iter: 52 loss: 0.0260679647
Iter: 53 loss: 0.027364023
Iter: 54 loss: 0.0251729675
Iter: 55 loss: 0.0220424663
Iter: 56 loss: 0.027548885
Iter: 57 loss: 0.0208611265
Iter: 58 loss: 0.0184392761
Iter: 59 loss: 0.0245359261
Iter: 60 loss: 0.0175804067
Iter: 61 loss: 0.0159316957
Iter: 62 loss: 0.100692138
Iter: 63 loss: 0.0159246419
Iter: 64 loss: 0.0149369314
Iter: 65 loss: 0.0252881162
Iter: 66 loss: 0.0148947835
Iter: 67 loss: 0.0135276783
Iter: 68 loss: 0.0182360858
Iter: 69 loss: 0.0132166874
Iter: 70 loss: 0.0120269516
Iter: 71 loss: 0.0292594694
Iter: 72 loss: 0.0120208785
Iter: 73 loss: 0.0117588704
Iter: 74 loss: 0.0118187889
Iter: 75 loss: 0.0115796439
Iter: 76 loss: 0.0111839678
Iter: 77 loss: 0.0113667091
Iter: 78 loss: 0.0109159704
Iter: 79 loss: 0.0106201516
Iter: 80 loss: 0.0119144376
Iter: 81 loss: 0.0105574252
Iter: 82 loss: 0.0103581063
Iter: 83 loss: 0.0103570195
Iter: 84 loss: 0.0101940576
Iter: 85 loss: 0.0109428903
Iter: 86 loss: 0.0101613179
Iter: 87 loss: 0.0100094294
Iter: 88 loss: 0.0101255281
Iter: 89 loss: 0.00991156418
Iter: 90 loss: 0.0097339768
Iter: 91 loss: 0.0101173297
Iter: 92 loss: 0.00965368561
Iter: 93 loss: 0.00942747854
Iter: 94 loss: 0.0101887845
Iter: 95 loss: 0.00935961772
Iter: 96 loss: 0.00916973
Iter: 97 loss: 0.0121003147
Iter: 98 loss: 0.00916970801
Iter: 99 loss: 0.0089388825
Iter: 100 loss: 0.0117902849
Iter: 101 loss: 0.00893328246
Iter: 102 loss: 0.00875223614
Iter: 103 loss: 0.00900894124
Iter: 104 loss: 0.00866174698
Iter: 105 loss: 0.00848155376
Iter: 106 loss: 0.0111746686
Iter: 107 loss: 0.00847373158
Iter: 108 loss: 0.00828788429
Iter: 109 loss: 0.0102887619
Iter: 110 loss: 0.00828747265
Iter: 111 loss: 0.00811813865
Iter: 112 loss: 0.00874031894
Iter: 113 loss: 0.00806239806
Iter: 114 loss: 0.00789135322
Iter: 115 loss: 0.00849165767
Iter: 116 loss: 0.00784830377
Iter: 117 loss: 0.00765830232
Iter: 118 loss: 0.00833383389
Iter: 119 loss: 0.00759102963
Iter: 120 loss: 0.00742002111
Iter: 121 loss: 0.00956583116
Iter: 122 loss: 0.00741968676
Iter: 123 loss: 0.00728813931
Iter: 124 loss: 0.00912517309
Iter: 125 loss: 0.00728102494
Iter: 126 loss: 0.00715716975
Iter: 127 loss: 0.00739157759
Iter: 128 loss: 0.00710304361
Iter: 129 loss: 0.00695449486
Iter: 130 loss: 0.00788806751
Iter: 131 loss: 0.00692599313
Iter: 132 loss: 0.00691985246
Iter: 133 loss: 0.00679299515
Iter: 134 loss: 0.00669556949
Iter: 135 loss: 0.00669522304
Iter: 136 loss: 0.00662780553
Iter: 137 loss: 0.00696390774
Iter: 138 loss: 0.006614713
Iter: 139 loss: 0.00654337928
Iter: 140 loss: 0.00668136217
Iter: 141 loss: 0.0065136319
Iter: 142 loss: 0.00644204952
Iter: 143 loss: 0.00716693792
Iter: 144 loss: 0.00643661711
Iter: 145 loss: 0.00638318341
Iter: 146 loss: 0.00641459133
Iter: 147 loss: 0.00634802
Iter: 148 loss: 0.00624800753
Iter: 149 loss: 0.00665149465
Iter: 150 loss: 0.00622612517
Iter: 151 loss: 0.00613792706
Iter: 152 loss: 0.00698839873
Iter: 153 loss: 0.00613498222
Iter: 154 loss: 0.00607339619
Iter: 155 loss: 0.00677250931
Iter: 156 loss: 0.00607184134
Iter: 157 loss: 0.00599039719
Iter: 158 loss: 0.00626137527
Iter: 159 loss: 0.00597071648
Iter: 160 loss: 0.00592221273
Iter: 161 loss: 0.00614345819
Iter: 162 loss: 0.00591090508
Iter: 163 loss: 0.00586342439
Iter: 164 loss: 0.00612112554
Iter: 165 loss: 0.00585740665
Iter: 166 loss: 0.00580661884
Iter: 167 loss: 0.00604087859
Iter: 168 loss: 0.0057957368
Iter: 169 loss: 0.00575269572
Iter: 170 loss: 0.00579658244
Iter: 171 loss: 0.00572899822
Iter: 172 loss: 0.00569799915
Iter: 173 loss: 0.00568906032
Iter: 174 loss: 0.00566236302
Iter: 175 loss: 0.00571614876
Iter: 176 loss: 0.00565119553
Iter: 177 loss: 0.00562375924
Iter: 178 loss: 0.00559851527
Iter: 179 loss: 0.00559202814
Iter: 180 loss: 0.00552875455
Iter: 181 loss: 0.00594504364
Iter: 182 loss: 0.00551714748
Iter: 183 loss: 0.0054536825
Iter: 184 loss: 0.00560410554
Iter: 185 loss: 0.00543074356
Iter: 186 loss: 0.00536815077
Iter: 187 loss: 0.00600592513
Iter: 188 loss: 0.00536638591
Iter: 189 loss: 0.00533188693
Iter: 190 loss: 0.00537681393
Iter: 191 loss: 0.00531458296
Iter: 192 loss: 0.00527068507
Iter: 193 loss: 0.00524952
Iter: 194 loss: 0.00522680813
Iter: 195 loss: 0.00517648179
Iter: 196 loss: 0.00544227473
Iter: 197 loss: 0.00516972784
Iter: 198 loss: 0.00511795469
Iter: 199 loss: 0.00541027263
Iter: 200 loss: 0.00510887196
Iter: 201 loss: 0.00507151894
Iter: 202 loss: 0.00509793963
Iter: 203 loss: 0.00504879467
Iter: 204 loss: 0.00498510385
Iter: 205 loss: 0.00515160663
Iter: 206 loss: 0.00496309623
Iter: 207 loss: 0.00489061233
Iter: 208 loss: 0.00507675158
Iter: 209 loss: 0.00486202352
Iter: 210 loss: 0.00481867371
Iter: 211 loss: 0.00481707603
Iter: 212 loss: 0.00476968195
Iter: 213 loss: 0.00476953387
Iter: 214 loss: 0.00473081926
Iter: 215 loss: 0.00471557537
Iter: 216 loss: 0.00469441339
Iter: 217 loss: 0.00465289829
Iter: 218 loss: 0.00499050785
Iter: 219 loss: 0.00465110177
Iter: 220 loss: 0.00461638533
Iter: 221 loss: 0.00481237937
Iter: 222 loss: 0.00460950285
Iter: 223 loss: 0.00456757378
Iter: 224 loss: 0.00484659523
Iter: 225 loss: 0.00456263218
Iter: 226 loss: 0.00451422762
Iter: 227 loss: 0.00483618816
Iter: 228 loss: 0.00450933073
Iter: 229 loss: 0.00447832653
Iter: 230 loss: 0.00451198779
Iter: 231 loss: 0.00446131919
Iter: 232 loss: 0.00442571798
Iter: 233 loss: 0.00447475165
Iter: 234 loss: 0.00440623518
Iter: 235 loss: 0.00436756574
Iter: 236 loss: 0.00447727833
Iter: 237 loss: 0.0043562809
Iter: 238 loss: 0.00432280404
Iter: 239 loss: 0.00439703651
Iter: 240 loss: 0.00430751825
Iter: 241 loss: 0.00426408555
Iter: 242 loss: 0.00435178634
Iter: 243 loss: 0.00424653292
Iter: 244 loss: 0.00419827504
Iter: 245 loss: 0.00451076031
Iter: 246 loss: 0.00419238117
Iter: 247 loss: 0.00417159032
Iter: 248 loss: 0.00417086063
Iter: 249 loss: 0.00415497366
Iter: 250 loss: 0.00416514091
Iter: 251 loss: 0.004144873
Iter: 252 loss: 0.00410129968
Iter: 253 loss: 0.00422781426
Iter: 254 loss: 0.00408701692
Iter: 255 loss: 0.00404513488
Iter: 256 loss: 0.00452776393
Iter: 257 loss: 0.00404370204
Iter: 258 loss: 0.00401700055
Iter: 259 loss: 0.00415289402
Iter: 260 loss: 0.0040130089
Iter: 261 loss: 0.00398957543
Iter: 262 loss: 0.00400871783
Iter: 263 loss: 0.00397489639
Iter: 264 loss: 0.00395038351
Iter: 265 loss: 0.0039662905
Iter: 266 loss: 0.00393525138
Iter: 267 loss: 0.00390537805
Iter: 268 loss: 0.00416175462
Iter: 269 loss: 0.00390296
Iter: 270 loss: 0.00387672801
Iter: 271 loss: 0.00392687134
Iter: 272 loss: 0.00386582967
Iter: 273 loss: 0.00383554283
Iter: 274 loss: 0.00399827631
Iter: 275 loss: 0.00383008062
Iter: 276 loss: 0.00379673
Iter: 277 loss: 0.00384297315
Iter: 278 loss: 0.00378042017
Iter: 279 loss: 0.00375346141
Iter: 280 loss: 0.00395231741
Iter: 281 loss: 0.00375108747
Iter: 282 loss: 0.00372765469
Iter: 283 loss: 0.00409619743
Iter: 284 loss: 0.0037276228
Iter: 285 loss: 0.00371168274
Iter: 286 loss: 0.00368842762
Iter: 287 loss: 0.00368780363
Iter: 288 loss: 0.0036459188
Iter: 289 loss: 0.00374085805
Iter: 290 loss: 0.00362807419
Iter: 291 loss: 0.00359928934
Iter: 292 loss: 0.00389960338
Iter: 293 loss: 0.00359866302
Iter: 294 loss: 0.00357329077
Iter: 295 loss: 0.00370615954
Iter: 296 loss: 0.00356860552
Iter: 297 loss: 0.00355189946
Iter: 298 loss: 0.00355139119
Iter: 299 loss: 0.00353860832
Iter: 300 loss: 0.00350800762
Iter: 301 loss: 0.00350788445
Iter: 302 loss: 0.00347798807
Iter: 303 loss: 0.00351802725
Iter: 304 loss: 0.00346270134
Iter: 305 loss: 0.00342860306
Iter: 306 loss: 0.0034580538
Iter: 307 loss: 0.00340861594
Iter: 308 loss: 0.00337791443
Iter: 309 loss: 0.00352432346
Iter: 310 loss: 0.00337110972
Iter: 311 loss: 0.00334125734
Iter: 312 loss: 0.00341427932
Iter: 313 loss: 0.00333077065
Iter: 314 loss: 0.00330995955
Iter: 315 loss: 0.00330976816
Iter: 316 loss: 0.00329380203
Iter: 317 loss: 0.00332661392
Iter: 318 loss: 0.00328696286
Iter: 319 loss: 0.00325325155
Iter: 320 loss: 0.00330569572
Iter: 321 loss: 0.00323786377
Iter: 322 loss: 0.00320223067
Iter: 323 loss: 0.00327997049
Iter: 324 loss: 0.00318609807
Iter: 325 loss: 0.00317916
Iter: 326 loss: 0.00316335098
Iter: 327 loss: 0.0031354602
Iter: 328 loss: 0.00325602968
Iter: 329 loss: 0.00312967156
Iter: 330 loss: 0.00310903299
Iter: 331 loss: 0.00310857012
Iter: 332 loss: 0.00309150154
Iter: 333 loss: 0.00305299484
Iter: 334 loss: 0.00318908039
Iter: 335 loss: 0.0030439971
Iter: 336 loss: 0.00301228417
Iter: 337 loss: 0.00322289392
Iter: 338 loss: 0.0030087207
Iter: 339 loss: 0.00298371958
Iter: 340 loss: 0.00302323629
Iter: 341 loss: 0.00297169061
Iter: 342 loss: 0.00293264119
Iter: 343 loss: 0.00298475241
Iter: 344 loss: 0.00291310088
Iter: 345 loss: 0.00289503718
Iter: 346 loss: 0.00288788858
Iter: 347 loss: 0.00286080036
Iter: 348 loss: 0.0030206074
Iter: 349 loss: 0.00285674655
Iter: 350 loss: 0.00283430936
Iter: 351 loss: 0.00287896651
Iter: 352 loss: 0.00282613025
Iter: 353 loss: 0.00278364937
Iter: 354 loss: 0.00297785131
Iter: 355 loss: 0.00277323322
Iter: 356 loss: 0.00273804134
Iter: 357 loss: 0.00277581625
Iter: 358 loss: 0.00271875784
Iter: 359 loss: 0.00268801395
Iter: 360 loss: 0.00268795039
Iter: 361 loss: 0.00266045169
Iter: 362 loss: 0.00295110955
Iter: 363 loss: 0.00265968381
Iter: 364 loss: 0.00264217
Iter: 365 loss: 0.00261152186
Iter: 366 loss: 0.00261151209
Iter: 367 loss: 0.00257100142
Iter: 368 loss: 0.00272792671
Iter: 369 loss: 0.00255936151
Iter: 370 loss: 0.00251892325
Iter: 371 loss: 0.00264584273
Iter: 372 loss: 0.00250789756
Iter: 373 loss: 0.00246917875
Iter: 374 loss: 0.00271575502
Iter: 375 loss: 0.00246310374
Iter: 376 loss: 0.00243745511
Iter: 377 loss: 0.00261531956
Iter: 378 loss: 0.00243493845
Iter: 379 loss: 0.00241353945
Iter: 380 loss: 0.00248415628
Iter: 381 loss: 0.00240747957
Iter: 382 loss: 0.00238984916
Iter: 383 loss: 0.00241089682
Iter: 384 loss: 0.00238042884
Iter: 385 loss: 0.00235502329
Iter: 386 loss: 0.00242007361
Iter: 387 loss: 0.00234639738
Iter: 388 loss: 0.00231890311
Iter: 389 loss: 0.00243305694
Iter: 390 loss: 0.00231240271
Iter: 391 loss: 0.00229044491
Iter: 392 loss: 0.00228071422
Iter: 393 loss: 0.0022697947
Iter: 394 loss: 0.00225561066
Iter: 395 loss: 0.0022476241
Iter: 396 loss: 0.00222282158
Iter: 397 loss: 0.00227191718
Iter: 398 loss: 0.00221191114
Iter: 399 loss: 0.00218847021
Iter: 400 loss: 0.00222690729
Iter: 401 loss: 0.0021779961
Iter: 402 loss: 0.00215167622
Iter: 403 loss: 0.0022058012
Iter: 404 loss: 0.00214052713
Iter: 405 loss: 0.00212017912
Iter: 406 loss: 0.00212181825
Iter: 407 loss: 0.00210451754
Iter: 408 loss: 0.00207831804
Iter: 409 loss: 0.0021056314
Iter: 410 loss: 0.00206334749
Iter: 411 loss: 0.0020372672
Iter: 412 loss: 0.00218435889
Iter: 413 loss: 0.00203370675
Iter: 414 loss: 0.00200554891
Iter: 415 loss: 0.00212794589
Iter: 416 loss: 0.00199990207
Iter: 417 loss: 0.00197891938
Iter: 418 loss: 0.00201330427
Iter: 419 loss: 0.0019687477
Iter: 420 loss: 0.00194541307
Iter: 421 loss: 0.00202455488
Iter: 422 loss: 0.00193903525
Iter: 423 loss: 0.00191609142
Iter: 424 loss: 0.00192911073
Iter: 425 loss: 0.00190105755
Iter: 426 loss: 0.00187335955
Iter: 427 loss: 0.00195986917
Iter: 428 loss: 0.00186445285
Iter: 429 loss: 0.00184059446
Iter: 430 loss: 0.00192139437
Iter: 431 loss: 0.00183442095
Iter: 432 loss: 0.00181474339
Iter: 433 loss: 0.00191599282
Iter: 434 loss: 0.00181154069
Iter: 435 loss: 0.00179767981
Iter: 436 loss: 0.00185097172
Iter: 437 loss: 0.00179426745
Iter: 438 loss: 0.00177940493
Iter: 439 loss: 0.00178575981
Iter: 440 loss: 0.00176930346
Iter: 441 loss: 0.00174560351
Iter: 442 loss: 0.00180626591
Iter: 443 loss: 0.00173692498
Iter: 444 loss: 0.00171176658
Iter: 445 loss: 0.00171081675
Iter: 446 loss: 0.00169313815
Iter: 447 loss: 0.00196270086
Iter: 448 loss: 0.00169313443
Iter: 449 loss: 0.00168174307
Iter: 450 loss: 0.00167576026
Iter: 451 loss: 0.00167056965
Iter: 452 loss: 0.00165584055
Iter: 453 loss: 0.00165677641
Iter: 454 loss: 0.00164424279
Iter: 455 loss: 0.00162287662
Iter: 456 loss: 0.00175946206
Iter: 457 loss: 0.00162056787
Iter: 458 loss: 0.00160224293
Iter: 459 loss: 0.00159399281
Iter: 460 loss: 0.00158465654
Iter: 461 loss: 0.00156362075
Iter: 462 loss: 0.00176814478
Iter: 463 loss: 0.00156286033
Iter: 464 loss: 0.00154911424
Iter: 465 loss: 0.00174400152
Iter: 466 loss: 0.00154908944
Iter: 467 loss: 0.00153569807
Iter: 468 loss: 0.00155827985
Iter: 469 loss: 0.00152957242
Iter: 470 loss: 0.00151816348
Iter: 471 loss: 0.00151998969
Iter: 472 loss: 0.00150958088
Iter: 473 loss: 0.00149491616
Iter: 474 loss: 0.00149550987
Iter: 475 loss: 0.00148345833
Iter: 476 loss: 0.00146318879
Iter: 477 loss: 0.00153139816
Iter: 478 loss: 0.00145755033
Iter: 479 loss: 0.00144099013
Iter: 480 loss: 0.00155362231
Iter: 481 loss: 0.00143942051
Iter: 482 loss: 0.00142511935
Iter: 483 loss: 0.00156094111
Iter: 484 loss: 0.00142456871
Iter: 485 loss: 0.00141455501
Iter: 486 loss: 0.00141698425
Iter: 487 loss: 0.00140722375
Iter: 488 loss: 0.00139573542
Iter: 489 loss: 0.00142327207
Iter: 490 loss: 0.00139161141
Iter: 491 loss: 0.00137843494
Iter: 492 loss: 0.00139654428
Iter: 493 loss: 0.00137182558
Iter: 494 loss: 0.00135815679
Iter: 495 loss: 0.00137801946
Iter: 496 loss: 0.00135166966
Iter: 497 loss: 0.00133716408
Iter: 498 loss: 0.00138892501
Iter: 499 loss: 0.00133332249
Iter: 500 loss: 0.00132672116
Iter: 501 loss: 0.00132572406
Iter: 502 loss: 0.00131868361
Iter: 503 loss: 0.00130936527
Iter: 504 loss: 0.00130881672
Iter: 505 loss: 0.00129774492
Iter: 506 loss: 0.00130662019
Iter: 507 loss: 0.00129105372
Iter: 508 loss: 0.00127965363
Iter: 509 loss: 0.00131393573
Iter: 510 loss: 0.0012762174
Iter: 511 loss: 0.00126590906
Iter: 512 loss: 0.00130795129
Iter: 513 loss: 0.00126360403
Iter: 514 loss: 0.00125522714
Iter: 515 loss: 0.00136623939
Iter: 516 loss: 0.00125517463
Iter: 517 loss: 0.00124745234
Iter: 518 loss: 0.0012475726
Iter: 519 loss: 0.00124132191
Iter: 520 loss: 0.00123171415
Iter: 521 loss: 0.00125087565
Iter: 522 loss: 0.00122770737
Iter: 523 loss: 0.00121704792
Iter: 524 loss: 0.00121608539
Iter: 525 loss: 0.00120821432
Iter: 526 loss: 0.00119422481
Iter: 527 loss: 0.00126716495
Iter: 528 loss: 0.00119195611
Iter: 529 loss: 0.00118056522
Iter: 530 loss: 0.00118920766
Iter: 531 loss: 0.00117361103
Iter: 532 loss: 0.00116812089
Iter: 533 loss: 0.00116705522
Iter: 534 loss: 0.00116050337
Iter: 535 loss: 0.00116610969
Iter: 536 loss: 0.00115661824
Iter: 537 loss: 0.001150414
Iter: 538 loss: 0.00115372753
Iter: 539 loss: 0.00114634144
Iter: 540 loss: 0.00113883533
Iter: 541 loss: 0.00114888791
Iter: 542 loss: 0.00113506685
Iter: 543 loss: 0.0011257201
Iter: 544 loss: 0.00114530697
Iter: 545 loss: 0.00112204952
Iter: 546 loss: 0.00111300382
Iter: 547 loss: 0.00119925837
Iter: 548 loss: 0.00111264549
Iter: 549 loss: 0.00110617897
Iter: 550 loss: 0.00115613523
Iter: 551 loss: 0.00110569433
Iter: 552 loss: 0.00109990803
Iter: 553 loss: 0.00109526375
Iter: 554 loss: 0.00109354011
Iter: 555 loss: 0.00108525204
Iter: 556 loss: 0.00112776924
Iter: 557 loss: 0.00108387275
Iter: 558 loss: 0.00107689213
Iter: 559 loss: 0.00106459332
Iter: 560 loss: 0.00106458855
Iter: 561 loss: 0.00105173816
Iter: 562 loss: 0.00110653369
Iter: 563 loss: 0.0010490662
Iter: 564 loss: 0.00103939511
Iter: 565 loss: 0.00106960128
Iter: 566 loss: 0.00103652873
Iter: 567 loss: 0.00103120029
Iter: 568 loss: 0.00103073521
Iter: 569 loss: 0.00102610141
Iter: 570 loss: 0.00102053443
Iter: 571 loss: 0.00101996714
Iter: 572 loss: 0.00101232296
Iter: 573 loss: 0.00101178605
Iter: 574 loss: 0.00100604608
Iter: 575 loss: 0.000996945426
Iter: 576 loss: 0.000996919
Iter: 577 loss: 0.000991249923
Iter: 578 loss: 0.000997302704
Iter: 579 loss: 0.000988127897
Iter: 580 loss: 0.0009823828
Iter: 581 loss: 0.000994092086
Iter: 582 loss: 0.000980053097
Iter: 583 loss: 0.000972390349
Iter: 584 loss: 0.000982755562
Iter: 585 loss: 0.000968523091
Iter: 586 loss: 0.000962071703
Iter: 587 loss: 0.000968078442
Iter: 588 loss: 0.000958326505
Iter: 589 loss: 0.000949879
Iter: 590 loss: 0.000965241808
Iter: 591 loss: 0.000946257729
Iter: 592 loss: 0.000939500052
Iter: 593 loss: 0.000939497491
Iter: 594 loss: 0.000933347736
Iter: 595 loss: 0.000928795314
Iter: 596 loss: 0.000926750305
Iter: 597 loss: 0.000919028767
Iter: 598 loss: 0.000983889448
Iter: 599 loss: 0.000918564911
Iter: 600 loss: 0.000912415562
Iter: 601 loss: 0.000971639587
Iter: 602 loss: 0.000912174874
Iter: 603 loss: 0.000908827293
Iter: 604 loss: 0.000902100815
Iter: 605 loss: 0.00102811737
Iter: 606 loss: 0.000902009546
Iter: 607 loss: 0.000895936
Iter: 608 loss: 0.000965668063
Iter: 609 loss: 0.000895835401
Iter: 610 loss: 0.000890719471
Iter: 611 loss: 0.000888740935
Iter: 612 loss: 0.000885978574
Iter: 613 loss: 0.00087869761
Iter: 614 loss: 0.000948533707
Iter: 615 loss: 0.000878408144
Iter: 616 loss: 0.000873079058
Iter: 617 loss: 0.000900997897
Iter: 618 loss: 0.000872247
Iter: 619 loss: 0.000867850089
Iter: 620 loss: 0.000862420944
Iter: 621 loss: 0.000861944049
Iter: 622 loss: 0.000855034159
Iter: 623 loss: 0.000869154232
Iter: 624 loss: 0.00085224933
Iter: 625 loss: 0.000844871
Iter: 626 loss: 0.000875798054
Iter: 627 loss: 0.000843311078
Iter: 628 loss: 0.000838032458
Iter: 629 loss: 0.000912242685
Iter: 630 loss: 0.0008380207
Iter: 631 loss: 0.000834011473
Iter: 632 loss: 0.000835753046
Iter: 633 loss: 0.000831262
Iter: 634 loss: 0.000826555886
Iter: 635 loss: 0.000843000482
Iter: 636 loss: 0.000825319672
Iter: 637 loss: 0.00081958971
Iter: 638 loss: 0.00085045374
Iter: 639 loss: 0.000818722474
Iter: 640 loss: 0.000815146195
Iter: 641 loss: 0.000807328965
Iter: 642 loss: 0.000923872343
Iter: 643 loss: 0.000806998461
Iter: 644 loss: 0.000799273257
Iter: 645 loss: 0.000832446211
Iter: 646 loss: 0.000797656248
Iter: 647 loss: 0.000791351136
Iter: 648 loss: 0.000838831416
Iter: 649 loss: 0.000790849503
Iter: 650 loss: 0.000787405879
Iter: 651 loss: 0.000787232362
Iter: 652 loss: 0.000784694683
Iter: 653 loss: 0.000781399838
Iter: 654 loss: 0.00078117958
Iter: 655 loss: 0.000776417321
Iter: 656 loss: 0.00077387481
Iter: 657 loss: 0.000771713443
Iter: 658 loss: 0.000765019911
Iter: 659 loss: 0.000793418498
Iter: 660 loss: 0.000763608783
Iter: 661 loss: 0.000757943955
Iter: 662 loss: 0.000765178702
Iter: 663 loss: 0.000755035435
Iter: 664 loss: 0.000748399296
Iter: 665 loss: 0.000760708936
Iter: 666 loss: 0.000745586236
Iter: 667 loss: 0.000742820441
Iter: 668 loss: 0.000741988
Iter: 669 loss: 0.000738805218
Iter: 670 loss: 0.000743823824
Iter: 671 loss: 0.000737325754
Iter: 672 loss: 0.000733383175
Iter: 673 loss: 0.000742270029
Iter: 674 loss: 0.000731879962
Iter: 675 loss: 0.000728236919
Iter: 676 loss: 0.000722788158
Iter: 677 loss: 0.000722667901
Iter: 678 loss: 0.00071688497
Iter: 679 loss: 0.000734961417
Iter: 680 loss: 0.000715194154
Iter: 681 loss: 0.000709194574
Iter: 682 loss: 0.000729916734
Iter: 683 loss: 0.000707620755
Iter: 684 loss: 0.000704557868
Iter: 685 loss: 0.000704275793
Iter: 686 loss: 0.000700820063
Iter: 687 loss: 0.000697215553
Iter: 688 loss: 0.000696571486
Iter: 689 loss: 0.000692013069
Iter: 690 loss: 0.000689099776
Iter: 691 loss: 0.000687313
Iter: 692 loss: 0.0006818822
Iter: 693 loss: 0.000719986332
Iter: 694 loss: 0.00068138144
Iter: 695 loss: 0.00067633111
Iter: 696 loss: 0.000688136555
Iter: 697 loss: 0.000674459967
Iter: 698 loss: 0.000669384317
Iter: 699 loss: 0.000673467061
Iter: 700 loss: 0.000666322419
Iter: 701 loss: 0.000663906569
Iter: 702 loss: 0.000663110521
Iter: 703 loss: 0.00065969408
Iter: 704 loss: 0.000661156664
Iter: 705 loss: 0.000657340686
Iter: 706 loss: 0.000653204159
Iter: 707 loss: 0.000663219835
Iter: 708 loss: 0.000651715323
Iter: 709 loss: 0.000648411922
Iter: 710 loss: 0.000646274071
Iter: 711 loss: 0.000644985
Iter: 712 loss: 0.000639995502
Iter: 713 loss: 0.000654087053
Iter: 714 loss: 0.000638420286
Iter: 715 loss: 0.00063382613
Iter: 716 loss: 0.000650729635
Iter: 717 loss: 0.000632692245
Iter: 718 loss: 0.000629136921
Iter: 719 loss: 0.000629013695
Iter: 720 loss: 0.000627288653
Iter: 721 loss: 0.000622994034
Iter: 722 loss: 0.000664957217
Iter: 723 loss: 0.000622422609
Iter: 724 loss: 0.000617075071
Iter: 725 loss: 0.000632660405
Iter: 726 loss: 0.000615396828
Iter: 727 loss: 0.000610469317
Iter: 728 loss: 0.000623649685
Iter: 729 loss: 0.000608814938
Iter: 730 loss: 0.000603364315
Iter: 731 loss: 0.000632309238
Iter: 732 loss: 0.000602516637
Iter: 733 loss: 0.000598378479
Iter: 734 loss: 0.000602453772
Iter: 735 loss: 0.00059603632
Iter: 736 loss: 0.000594519195
Iter: 737 loss: 0.000593337871
Iter: 738 loss: 0.000591366086
Iter: 739 loss: 0.000588584226
Iter: 740 loss: 0.000588478695
Iter: 741 loss: 0.000585438625
Iter: 742 loss: 0.000597597
Iter: 743 loss: 0.000584743102
Iter: 744 loss: 0.000581799191
Iter: 745 loss: 0.000585562491
Iter: 746 loss: 0.000580290915
Iter: 747 loss: 0.000576413062
Iter: 748 loss: 0.000579577289
Iter: 749 loss: 0.00057406316
Iter: 750 loss: 0.000570317032
Iter: 751 loss: 0.000590018462
Iter: 752 loss: 0.000569724478
Iter: 753 loss: 0.000564817106
Iter: 754 loss: 0.000581745757
Iter: 755 loss: 0.000563492649
Iter: 756 loss: 0.000561342924
Iter: 757 loss: 0.000558485859
Iter: 758 loss: 0.000558320608
Iter: 759 loss: 0.00055352808
Iter: 760 loss: 0.000562020228
Iter: 761 loss: 0.000551400299
Iter: 762 loss: 0.000546674943
Iter: 763 loss: 0.00058701192
Iter: 764 loss: 0.000546403811
Iter: 765 loss: 0.000542495691
Iter: 766 loss: 0.000543654722
Iter: 767 loss: 0.00053968007
Iter: 768 loss: 0.00053531985
Iter: 769 loss: 0.000563045265
Iter: 770 loss: 0.000534822582
Iter: 771 loss: 0.000532630656
Iter: 772 loss: 0.00053235254
Iter: 773 loss: 0.000530786754
Iter: 774 loss: 0.000528257806
Iter: 775 loss: 0.000528240576
Iter: 776 loss: 0.000525529496
Iter: 777 loss: 0.000539165048
Iter: 778 loss: 0.000525081181
Iter: 779 loss: 0.000522166258
Iter: 780 loss: 0.00052645
Iter: 781 loss: 0.00052075024
Iter: 782 loss: 0.000517269829
Iter: 783 loss: 0.000520135276
Iter: 784 loss: 0.000515196123
Iter: 785 loss: 0.000513632433
Iter: 786 loss: 0.0005129294
Iter: 787 loss: 0.000510639569
Iter: 788 loss: 0.00051652285
Iter: 789 loss: 0.000509861391
Iter: 790 loss: 0.00050797913
Iter: 791 loss: 0.000505154
Iter: 792 loss: 0.00050509657
Iter: 793 loss: 0.000501296134
Iter: 794 loss: 0.000507903402
Iter: 795 loss: 0.000499604386
Iter: 796 loss: 0.000495247892
Iter: 797 loss: 0.000510333281
Iter: 798 loss: 0.000494109467
Iter: 799 loss: 0.000490636157
Iter: 800 loss: 0.0005125436
Iter: 801 loss: 0.000490230566
Iter: 802 loss: 0.000487987127
Iter: 803 loss: 0.000500825234
Iter: 804 loss: 0.000487683341
Iter: 805 loss: 0.000485126628
Iter: 806 loss: 0.000494611566
Iter: 807 loss: 0.000484479591
Iter: 808 loss: 0.000482642936
Iter: 809 loss: 0.000480338058
Iter: 810 loss: 0.000480149611
Iter: 811 loss: 0.000476868328
Iter: 812 loss: 0.0004971897
Iter: 813 loss: 0.000476473157
Iter: 814 loss: 0.000473790569
Iter: 815 loss: 0.000485093275
Iter: 816 loss: 0.000473230786
Iter: 817 loss: 0.000471029
Iter: 818 loss: 0.000471101026
Iter: 819 loss: 0.000469284307
Iter: 820 loss: 0.000467460428
Iter: 821 loss: 0.000467301754
Iter: 822 loss: 0.000465153018
Iter: 823 loss: 0.000463694916
Iter: 824 loss: 0.000462898111
Iter: 825 loss: 0.000460418116
Iter: 826 loss: 0.000463518198
Iter: 827 loss: 0.000459123548
Iter: 828 loss: 0.00045629582
Iter: 829 loss: 0.000456577807
Iter: 830 loss: 0.000454111636
Iter: 831 loss: 0.000450327439
Iter: 832 loss: 0.000472002925
Iter: 833 loss: 0.000449808256
Iter: 834 loss: 0.000447288476
Iter: 835 loss: 0.000484214164
Iter: 836 loss: 0.00044728574
Iter: 837 loss: 0.000445344776
Iter: 838 loss: 0.000453084882
Iter: 839 loss: 0.000444886507
Iter: 840 loss: 0.000443100638
Iter: 841 loss: 0.000443945028
Iter: 842 loss: 0.000441907352
Iter: 843 loss: 0.000439883326
Iter: 844 loss: 0.000438890827
Iter: 845 loss: 0.000437921379
Iter: 846 loss: 0.000435549737
Iter: 847 loss: 0.000472993299
Iter: 848 loss: 0.000435549708
Iter: 849 loss: 0.0004337352
Iter: 850 loss: 0.000433655863
Iter: 851 loss: 0.000432256027
Iter: 852 loss: 0.000429845852
Iter: 853 loss: 0.000435437862
Iter: 854 loss: 0.000428947591
Iter: 855 loss: 0.000428223517
Iter: 856 loss: 0.000427717692
Iter: 857 loss: 0.000426825165
Iter: 858 loss: 0.00042472803
Iter: 859 loss: 0.000449883
Iter: 860 loss: 0.000424548052
Iter: 861 loss: 0.000422255078
Iter: 862 loss: 0.000424083031
Iter: 863 loss: 0.000420871016
Iter: 864 loss: 0.00041821145
Iter: 865 loss: 0.000420388445
Iter: 866 loss: 0.00041660876
Iter: 867 loss: 0.000414420385
Iter: 868 loss: 0.00041439553
Iter: 869 loss: 0.000413046859
Iter: 870 loss: 0.000421753677
Iter: 871 loss: 0.000412895635
Iter: 872 loss: 0.000411573943
Iter: 873 loss: 0.000411762856
Iter: 874 loss: 0.000410573703
Iter: 875 loss: 0.000408766849
Iter: 876 loss: 0.000409274333
Iter: 877 loss: 0.000407456537
Iter: 878 loss: 0.00040554328
Iter: 879 loss: 0.000408239081
Iter: 880 loss: 0.00040460215
Iter: 881 loss: 0.000402209727
Iter: 882 loss: 0.000418165087
Iter: 883 loss: 0.000401958474
Iter: 884 loss: 0.000400392892
Iter: 885 loss: 0.000401482976
Iter: 886 loss: 0.000399413286
Iter: 887 loss: 0.000397429278
Iter: 888 loss: 0.00040477814
Iter: 889 loss: 0.000396937539
Iter: 890 loss: 0.000394517701
Iter: 891 loss: 0.000413193891
Iter: 892 loss: 0.000394340925
Iter: 893 loss: 0.000393209397
Iter: 894 loss: 0.000391694542
Iter: 895 loss: 0.000391611567
Iter: 896 loss: 0.000389486842
Iter: 897 loss: 0.000389648078
Iter: 898 loss: 0.000387827604
Iter: 899 loss: 0.000384882966
Iter: 900 loss: 0.000388190587
Iter: 901 loss: 0.000383282109
Iter: 902 loss: 0.000380552374
Iter: 903 loss: 0.000400990772
Iter: 904 loss: 0.000380337
Iter: 905 loss: 0.000378974946
Iter: 906 loss: 0.00037894916
Iter: 907 loss: 0.000377464195
Iter: 908 loss: 0.000380477693
Iter: 909 loss: 0.000376864919
Iter: 910 loss: 0.000375649834
Iter: 911 loss: 0.000373201561
Iter: 912 loss: 0.000419509655
Iter: 913 loss: 0.000373167219
Iter: 914 loss: 0.000370963
Iter: 915 loss: 0.000401741068
Iter: 916 loss: 0.00037095783
Iter: 917 loss: 0.000369274174
Iter: 918 loss: 0.000374753494
Iter: 919 loss: 0.000368793204
Iter: 920 loss: 0.000367018016
Iter: 921 loss: 0.000369542
Iter: 922 loss: 0.000366145512
Iter: 923 loss: 0.000366226595
Iter: 924 loss: 0.00036503392
Iter: 925 loss: 0.000364242
Iter: 926 loss: 0.000363013
Iter: 927 loss: 0.000362997031
Iter: 928 loss: 0.000361656072
Iter: 929 loss: 0.000361618499
Iter: 930 loss: 0.000360570819
Iter: 931 loss: 0.000358568184
Iter: 932 loss: 0.000362308754
Iter: 933 loss: 0.000357710407
Iter: 934 loss: 0.000355855358
Iter: 935 loss: 0.000355630327
Iter: 936 loss: 0.000354298129
Iter: 937 loss: 0.000351524563
Iter: 938 loss: 0.000363980071
Iter: 939 loss: 0.000350978837
Iter: 940 loss: 0.000349417358
Iter: 941 loss: 0.000349353213
Iter: 942 loss: 0.000347628142
Iter: 943 loss: 0.000352195872
Iter: 944 loss: 0.000347048655
Iter: 945 loss: 0.00034581247
Iter: 946 loss: 0.000344848551
Iter: 947 loss: 0.000344460917
Iter: 948 loss: 0.000342723157
Iter: 949 loss: 0.000344409025
Iter: 950 loss: 0.000341730891
Iter: 951 loss: 0.000339948863
Iter: 952 loss: 0.000349982845
Iter: 953 loss: 0.000339700899
Iter: 954 loss: 0.000338297687
Iter: 955 loss: 0.000353765383
Iter: 956 loss: 0.000338266836
Iter: 957 loss: 0.000337266771
Iter: 958 loss: 0.000337093428
Iter: 959 loss: 0.000336411
Iter: 960 loss: 0.000334803452
Iter: 961 loss: 0.000335647375
Iter: 962 loss: 0.000333734875
Iter: 963 loss: 0.000331838237
Iter: 964 loss: 0.000335219549
Iter: 965 loss: 0.000331004674
Iter: 966 loss: 0.000329449394
Iter: 967 loss: 0.000332971744
Iter: 968 loss: 0.000328859867
Iter: 969 loss: 0.000327078684
Iter: 970 loss: 0.000334797369
Iter: 971 loss: 0.000326714216
Iter: 972 loss: 0.000325144094
Iter: 973 loss: 0.000327123242
Iter: 974 loss: 0.000324331457
Iter: 975 loss: 0.000322891632
Iter: 976 loss: 0.000338469632
Iter: 977 loss: 0.000322856533
Iter: 978 loss: 0.000321364263
Iter: 979 loss: 0.000328328839
Iter: 980 loss: 0.000321076746
Iter: 981 loss: 0.000320235587
Iter: 982 loss: 0.000319511222
Iter: 983 loss: 0.000319277926
Iter: 984 loss: 0.000317623257
Iter: 985 loss: 0.000320548424
Iter: 986 loss: 0.000316890946
Iter: 987 loss: 0.000315419747
Iter: 988 loss: 0.00031796738
Iter: 989 loss: 0.000314768404
Iter: 990 loss: 0.000313611818
Iter: 991 loss: 0.000330676412
Iter: 992 loss: 0.000313610537
Iter: 993 loss: 0.000312238146
Iter: 994 loss: 0.000311727665
Iter: 995 loss: 0.000310965464
Iter: 996 loss: 0.000309852185
Iter: 997 loss: 0.000313355587
Iter: 998 loss: 0.000309529685
Iter: 999 loss: 0.000308173825
Iter: 1000 loss: 0.000308300165
Iter: 1001 loss: 0.000307123817
Iter: 1002 loss: 0.000305342837
Iter: 1003 loss: 0.000306030968
Iter: 1004 loss: 0.000304104236
Iter: 1005 loss: 0.000301959197
Iter: 1006 loss: 0.000307330163
Iter: 1007 loss: 0.000301204098
Iter: 1008 loss: 0.00029943086
Iter: 1009 loss: 0.000320124789
Iter: 1010 loss: 0.000299402571
Iter: 1011 loss: 0.000298420026
Iter: 1012 loss: 0.000298301748
Iter: 1013 loss: 0.000297541847
Iter: 1014 loss: 0.00029627711
Iter: 1015 loss: 0.000296272046
Iter: 1016 loss: 0.000294837344
Iter: 1017 loss: 0.00030313342
Iter: 1018 loss: 0.000294641563
Iter: 1019 loss: 0.000293486635
Iter: 1020 loss: 0.000294493628
Iter: 1021 loss: 0.000292804296
Iter: 1022 loss: 0.000291592529
Iter: 1023 loss: 0.000298890169
Iter: 1024 loss: 0.000291444769
Iter: 1025 loss: 0.000290395663
Iter: 1026 loss: 0.000306707807
Iter: 1027 loss: 0.000290395692
Iter: 1028 loss: 0.000289778865
Iter: 1029 loss: 0.00028808834
Iter: 1030 loss: 0.000298622181
Iter: 1031 loss: 0.000287632633
Iter: 1032 loss: 0.000285861723
Iter: 1033 loss: 0.000306866772
Iter: 1034 loss: 0.000285837974
Iter: 1035 loss: 0.000284721202
Iter: 1036 loss: 0.00028762818
Iter: 1037 loss: 0.000284340844
Iter: 1038 loss: 0.000283301983
Iter: 1039 loss: 0.000284143549
Iter: 1040 loss: 0.000282677589
Iter: 1041 loss: 0.000281356566
Iter: 1042 loss: 0.00028185753
Iter: 1043 loss: 0.000280432927
Iter: 1044 loss: 0.000280078762
Iter: 1045 loss: 0.000279543921
Iter: 1046 loss: 0.000278794119
Iter: 1047 loss: 0.000278437597
Iter: 1048 loss: 0.000278071966
Iter: 1049 loss: 0.000277049345
Iter: 1050 loss: 0.000276630162
Iter: 1051 loss: 0.000276093051
Iter: 1052 loss: 0.000274569844
Iter: 1053 loss: 0.000278450083
Iter: 1054 loss: 0.000274043676
Iter: 1055 loss: 0.000272975129
Iter: 1056 loss: 0.000272922684
Iter: 1057 loss: 0.000272301637
Iter: 1058 loss: 0.000272849575
Iter: 1059 loss: 0.000271938537
Iter: 1060 loss: 0.000270848745
Iter: 1061 loss: 0.000271491823
Iter: 1062 loss: 0.000270143704
Iter: 1063 loss: 0.000269261101
Iter: 1064 loss: 0.000268655131
Iter: 1065 loss: 0.000268331903
Iter: 1066 loss: 0.000266858813
Iter: 1067 loss: 0.000269922399
Iter: 1068 loss: 0.000266266754
Iter: 1069 loss: 0.000265001087
Iter: 1070 loss: 0.000276244711
Iter: 1071 loss: 0.000264938455
Iter: 1072 loss: 0.000263919734
Iter: 1073 loss: 0.000265855895
Iter: 1074 loss: 0.000263486814
Iter: 1075 loss: 0.000262481772
Iter: 1076 loss: 0.000263731374
Iter: 1077 loss: 0.000261957379
Iter: 1078 loss: 0.000261019595
Iter: 1079 loss: 0.000271693
Iter: 1080 loss: 0.000261003093
Iter: 1081 loss: 0.000260166067
Iter: 1082 loss: 0.000263992319
Iter: 1083 loss: 0.000260005123
Iter: 1084 loss: 0.000259317516
Iter: 1085 loss: 0.000258006621
Iter: 1086 loss: 0.000286680093
Iter: 1087 loss: 0.000258000684
Iter: 1088 loss: 0.000256564759
Iter: 1089 loss: 0.000261663517
Iter: 1090 loss: 0.000256195199
Iter: 1091 loss: 0.000255689141
Iter: 1092 loss: 0.000255515799
Iter: 1093 loss: 0.000254964631
Iter: 1094 loss: 0.000254418614
Iter: 1095 loss: 0.000254303886
Iter: 1096 loss: 0.000253439299
Iter: 1097 loss: 0.000255447929
Iter: 1098 loss: 0.000253119244
Iter: 1099 loss: 0.000252338185
Iter: 1100 loss: 0.000251226826
Iter: 1101 loss: 0.000251188845
Iter: 1102 loss: 0.000249500619
Iter: 1103 loss: 0.000260112
Iter: 1104 loss: 0.000249304721
Iter: 1105 loss: 0.000248243683
Iter: 1106 loss: 0.000247771182
Iter: 1107 loss: 0.000247236
Iter: 1108 loss: 0.000245839125
Iter: 1109 loss: 0.000254820159
Iter: 1110 loss: 0.000245683506
Iter: 1111 loss: 0.000244548981
Iter: 1112 loss: 0.000254977378
Iter: 1113 loss: 0.000244499126
Iter: 1114 loss: 0.000243610455
Iter: 1115 loss: 0.00025208859
Iter: 1116 loss: 0.000243577393
Iter: 1117 loss: 0.000242859183
Iter: 1118 loss: 0.000243571587
Iter: 1119 loss: 0.000242454407
Iter: 1120 loss: 0.000241702975
Iter: 1121 loss: 0.000241295769
Iter: 1122 loss: 0.000240959256
Iter: 1123 loss: 0.000240224297
Iter: 1124 loss: 0.000243858129
Iter: 1125 loss: 0.000240097463
Iter: 1126 loss: 0.000239503817
Iter: 1127 loss: 0.000239502784
Iter: 1128 loss: 0.000239141867
Iter: 1129 loss: 0.000238750305
Iter: 1130 loss: 0.000238689681
Iter: 1131 loss: 0.000237977132
Iter: 1132 loss: 0.000237885542
Iter: 1133 loss: 0.000237378437
Iter: 1134 loss: 0.000236576481
Iter: 1135 loss: 0.000238795823
Iter: 1136 loss: 0.000236317122
Iter: 1137 loss: 0.000235402418
Iter: 1138 loss: 0.000236806096
Iter: 1139 loss: 0.000234969164
Iter: 1140 loss: 0.000234022358
Iter: 1141 loss: 0.000234763225
Iter: 1142 loss: 0.000233450366
Iter: 1143 loss: 0.000232203616
Iter: 1144 loss: 0.000238758817
Iter: 1145 loss: 0.000232006714
Iter: 1146 loss: 0.000231673446
Iter: 1147 loss: 0.000231584505
Iter: 1148 loss: 0.000231104728
Iter: 1149 loss: 0.000230768346
Iter: 1150 loss: 0.000230596692
Iter: 1151 loss: 0.000229905592
Iter: 1152 loss: 0.000232272665
Iter: 1153 loss: 0.000229719968
Iter: 1154 loss: 0.000229165205
Iter: 1155 loss: 0.000228531615
Iter: 1156 loss: 0.000228452613
Iter: 1157 loss: 0.000228270219
Iter: 1158 loss: 0.000228037388
Iter: 1159 loss: 0.000227639277
Iter: 1160 loss: 0.000227310287
Iter: 1161 loss: 0.000227194891
Iter: 1162 loss: 0.000226606484
Iter: 1163 loss: 0.000226987351
Iter: 1164 loss: 0.000226233766
Iter: 1165 loss: 0.000225398268
Iter: 1166 loss: 0.000227572193
Iter: 1167 loss: 0.000225112744
Iter: 1168 loss: 0.000224391202
Iter: 1169 loss: 0.000225267562
Iter: 1170 loss: 0.00022401067
Iter: 1171 loss: 0.000223138966
Iter: 1172 loss: 0.000224942953
Iter: 1173 loss: 0.000222788134
Iter: 1174 loss: 0.00022167759
Iter: 1175 loss: 0.000223972616
Iter: 1176 loss: 0.000221235925
Iter: 1177 loss: 0.000220234186
Iter: 1178 loss: 0.000222452858
Iter: 1179 loss: 0.000219849913
Iter: 1180 loss: 0.000219936279
Iter: 1181 loss: 0.000219394045
Iter: 1182 loss: 0.000219088979
Iter: 1183 loss: 0.000218480069
Iter: 1184 loss: 0.00022994724
Iter: 1185 loss: 0.000218471992
Iter: 1186 loss: 0.000217722773
Iter: 1187 loss: 0.0002202412
Iter: 1188 loss: 0.00021751778
Iter: 1189 loss: 0.000216821427
Iter: 1190 loss: 0.000219112902
Iter: 1191 loss: 0.000216631888
Iter: 1192 loss: 0.000216070606
Iter: 1193 loss: 0.000216070039
Iter: 1194 loss: 0.000215748674
Iter: 1195 loss: 0.000215194566
Iter: 1196 loss: 0.00021519493
Iter: 1197 loss: 0.000214310043
Iter: 1198 loss: 0.00021592954
Iter: 1199 loss: 0.000213929452
Iter: 1200 loss: 0.000212882544
Iter: 1201 loss: 0.000219275011
Iter: 1202 loss: 0.000212753337
Iter: 1203 loss: 0.000211969513
Iter: 1204 loss: 0.000211987033
Iter: 1205 loss: 0.000211346487
Iter: 1206 loss: 0.000210372469
Iter: 1207 loss: 0.000215049455
Iter: 1208 loss: 0.000210199447
Iter: 1209 loss: 0.000209345977
Iter: 1210 loss: 0.000211914827
Iter: 1211 loss: 0.000209087419
Iter: 1212 loss: 0.000208542639
Iter: 1213 loss: 0.000208541838
Iter: 1214 loss: 0.000207915684
Iter: 1215 loss: 0.000208254496
Iter: 1216 loss: 0.000207503181
Iter: 1217 loss: 0.000207003177
Iter: 1218 loss: 0.000206733836
Iter: 1219 loss: 0.000206510798
Iter: 1220 loss: 0.000205536795
Iter: 1221 loss: 0.000208877638
Iter: 1222 loss: 0.00020527684
Iter: 1223 loss: 0.000204899843
Iter: 1224 loss: 0.00020484101
Iter: 1225 loss: 0.000204467957
Iter: 1226 loss: 0.000204389595
Iter: 1227 loss: 0.000204144133
Iter: 1228 loss: 0.000203648829
Iter: 1229 loss: 0.000202927
Iter: 1230 loss: 0.000202907104
Iter: 1231 loss: 0.000202166499
Iter: 1232 loss: 0.000202166411
Iter: 1233 loss: 0.000201475981
Iter: 1234 loss: 0.00020277017
Iter: 1235 loss: 0.000201181057
Iter: 1236 loss: 0.000200528768
Iter: 1237 loss: 0.000199934118
Iter: 1238 loss: 0.000199772694
Iter: 1239 loss: 0.000198668262
Iter: 1240 loss: 0.000203318748
Iter: 1241 loss: 0.000198433758
Iter: 1242 loss: 0.000197702582
Iter: 1243 loss: 0.000208236015
Iter: 1244 loss: 0.000197701185
Iter: 1245 loss: 0.00019706825
Iter: 1246 loss: 0.000200501978
Iter: 1247 loss: 0.000196974026
Iter: 1248 loss: 0.000196577603
Iter: 1249 loss: 0.000195854271
Iter: 1250 loss: 0.000213133229
Iter: 1251 loss: 0.000195853674
Iter: 1252 loss: 0.00019498459
Iter: 1253 loss: 0.000197805624
Iter: 1254 loss: 0.000194740453
Iter: 1255 loss: 0.000194201784
Iter: 1256 loss: 0.000194180393
Iter: 1257 loss: 0.000193740765
Iter: 1258 loss: 0.000194770968
Iter: 1259 loss: 0.00019358017
Iter: 1260 loss: 0.00019305048
Iter: 1261 loss: 0.000192269683
Iter: 1262 loss: 0.000192249427
Iter: 1263 loss: 0.000191444924
Iter: 1264 loss: 0.000193219123
Iter: 1265 loss: 0.000191134939
Iter: 1266 loss: 0.000190412888
Iter: 1267 loss: 0.000193772939
Iter: 1268 loss: 0.000190278864
Iter: 1269 loss: 0.000189578583
Iter: 1270 loss: 0.000193115353
Iter: 1271 loss: 0.000189460028
Iter: 1272 loss: 0.000188956445
Iter: 1273 loss: 0.000188276928
Iter: 1274 loss: 0.000188241509
Iter: 1275 loss: 0.000187391735
Iter: 1276 loss: 0.000192072112
Iter: 1277 loss: 0.000187268088
Iter: 1278 loss: 0.000186692589
Iter: 1279 loss: 0.000193318381
Iter: 1280 loss: 0.000186682417
Iter: 1281 loss: 0.000186185323
Iter: 1282 loss: 0.000191314262
Iter: 1283 loss: 0.000186171266
Iter: 1284 loss: 0.000185923534
Iter: 1285 loss: 0.000185321522
Iter: 1286 loss: 0.000191786967
Iter: 1287 loss: 0.000185256329
Iter: 1288 loss: 0.000184643897
Iter: 1289 loss: 0.000188189151
Iter: 1290 loss: 0.000184561417
Iter: 1291 loss: 0.000183960801
Iter: 1292 loss: 0.000188855251
Iter: 1293 loss: 0.000183922471
Iter: 1294 loss: 0.000183442855
Iter: 1295 loss: 0.000184544726
Iter: 1296 loss: 0.00018326391
Iter: 1297 loss: 0.000182916032
Iter: 1298 loss: 0.000182745629
Iter: 1299 loss: 0.000182579504
Iter: 1300 loss: 0.000182016258
Iter: 1301 loss: 0.000182450254
Iter: 1302 loss: 0.000181675176
Iter: 1303 loss: 0.000180997551
Iter: 1304 loss: 0.000182302247
Iter: 1305 loss: 0.000180714444
Iter: 1306 loss: 0.000179997587
Iter: 1307 loss: 0.000185787008
Iter: 1308 loss: 0.000179950483
Iter: 1309 loss: 0.000179367344
Iter: 1310 loss: 0.000179455601
Iter: 1311 loss: 0.000178926173
Iter: 1312 loss: 0.000178263232
Iter: 1313 loss: 0.000178731527
Iter: 1314 loss: 0.000177852242
Iter: 1315 loss: 0.000177283626
Iter: 1316 loss: 0.000177278911
Iter: 1317 loss: 0.000176732414
Iter: 1318 loss: 0.000180359202
Iter: 1319 loss: 0.000176676142
Iter: 1320 loss: 0.00017635696
Iter: 1321 loss: 0.000175614638
Iter: 1322 loss: 0.000184994336
Iter: 1323 loss: 0.000175560213
Iter: 1324 loss: 0.000175191293
Iter: 1325 loss: 0.000175145746
Iter: 1326 loss: 0.000174731831
Iter: 1327 loss: 0.000175184861
Iter: 1328 loss: 0.000174504894
Iter: 1329 loss: 0.000174040688
Iter: 1330 loss: 0.000174574845
Iter: 1331 loss: 0.000173793058
Iter: 1332 loss: 0.000173292
Iter: 1333 loss: 0.000173463952
Iter: 1334 loss: 0.000172939
Iter: 1335 loss: 0.000172361877
Iter: 1336 loss: 0.000173820095
Iter: 1337 loss: 0.000172160653
Iter: 1338 loss: 0.000171534164
Iter: 1339 loss: 0.000173838693
Iter: 1340 loss: 0.000171379055
Iter: 1341 loss: 0.000170910265
Iter: 1342 loss: 0.000172798929
Iter: 1343 loss: 0.000170805142
Iter: 1344 loss: 0.00017022049
Iter: 1345 loss: 0.000170350424
Iter: 1346 loss: 0.000169789273
Iter: 1347 loss: 0.000169232197
Iter: 1348 loss: 0.000170449654
Iter: 1349 loss: 0.000169016625
Iter: 1350 loss: 0.00016852055
Iter: 1351 loss: 0.000174080022
Iter: 1352 loss: 0.000168510975
Iter: 1353 loss: 0.000168005936
Iter: 1354 loss: 0.000170884872
Iter: 1355 loss: 0.000167936814
Iter: 1356 loss: 0.000167697057
Iter: 1357 loss: 0.000167164137
Iter: 1358 loss: 0.000174728513
Iter: 1359 loss: 0.000167137317
Iter: 1360 loss: 0.00016695424
Iter: 1361 loss: 0.000166796672
Iter: 1362 loss: 0.00016653043
Iter: 1363 loss: 0.000166292564
Iter: 1364 loss: 0.00016622401
Iter: 1365 loss: 0.000165785808
Iter: 1366 loss: 0.000166606638
Iter: 1367 loss: 0.000165600097
Iter: 1368 loss: 0.000165156278
Iter: 1369 loss: 0.000166233396
Iter: 1370 loss: 0.000164996818
Iter: 1371 loss: 0.000164486468
Iter: 1372 loss: 0.000164472964
Iter: 1373 loss: 0.000164073383
Iter: 1374 loss: 0.000163481716
Iter: 1375 loss: 0.000171631676
Iter: 1376 loss: 0.000163478631
Iter: 1377 loss: 0.000163050921
Iter: 1378 loss: 0.000164739526
Iter: 1379 loss: 0.000162951852
Iter: 1380 loss: 0.000162573706
Iter: 1381 loss: 0.000162868426
Iter: 1382 loss: 0.000162344586
Iter: 1383 loss: 0.000161828037
Iter: 1384 loss: 0.000162399781
Iter: 1385 loss: 0.000161547403
Iter: 1386 loss: 0.000161182019
Iter: 1387 loss: 0.0001657299
Iter: 1388 loss: 0.00016117809
Iter: 1389 loss: 0.000160700351
Iter: 1390 loss: 0.000161122982
Iter: 1391 loss: 0.000160422162
Iter: 1392 loss: 0.00016013642
Iter: 1393 loss: 0.000160396972
Iter: 1394 loss: 0.000159971227
Iter: 1395 loss: 0.000159627714
Iter: 1396 loss: 0.000164113968
Iter: 1397 loss: 0.00015962524
Iter: 1398 loss: 0.000159419375
Iter: 1399 loss: 0.000159005896
Iter: 1400 loss: 0.000166799975
Iter: 1401 loss: 0.000159000469
Iter: 1402 loss: 0.000158508192
Iter: 1403 loss: 0.000160855358
Iter: 1404 loss: 0.000158418392
Iter: 1405 loss: 0.000157984294
Iter: 1406 loss: 0.000158642331
Iter: 1407 loss: 0.000157776201
Iter: 1408 loss: 0.000157329545
Iter: 1409 loss: 0.00015744142
Iter: 1410 loss: 0.000157002854
Iter: 1411 loss: 0.000156432128
Iter: 1412 loss: 0.00016126025
Iter: 1413 loss: 0.000156399648
Iter: 1414 loss: 0.000155876682
Iter: 1415 loss: 0.000156845315
Iter: 1416 loss: 0.000155652408
Iter: 1417 loss: 0.000155173504
Iter: 1418 loss: 0.00015637977
Iter: 1419 loss: 0.000155006273
Iter: 1420 loss: 0.000154511959
Iter: 1421 loss: 0.000154722948
Iter: 1422 loss: 0.000154172405
Iter: 1423 loss: 0.000154653739
Iter: 1424 loss: 0.00015398713
Iter: 1425 loss: 0.000153846369
Iter: 1426 loss: 0.000153502231
Iter: 1427 loss: 0.000156999595
Iter: 1428 loss: 0.00015346
Iter: 1429 loss: 0.000153153014
Iter: 1430 loss: 0.00015315137
Iter: 1431 loss: 0.000152873079
Iter: 1432 loss: 0.000152587789
Iter: 1433 loss: 0.00015253303
Iter: 1434 loss: 0.000152158158
Iter: 1435 loss: 0.000153040921
Iter: 1436 loss: 0.000152021181
Iter: 1437 loss: 0.000151675456
Iter: 1438 loss: 0.000153157423
Iter: 1439 loss: 0.000151603585
Iter: 1440 loss: 0.000151206128
Iter: 1441 loss: 0.000151239714
Iter: 1442 loss: 0.000150897948
Iter: 1443 loss: 0.000150476291
Iter: 1444 loss: 0.00015157694
Iter: 1445 loss: 0.000150332649
Iter: 1446 loss: 0.000149802814
Iter: 1447 loss: 0.00015125901
Iter: 1448 loss: 0.000149630185
Iter: 1449 loss: 0.000149124477
Iter: 1450 loss: 0.000152852968
Iter: 1451 loss: 0.000149083251
Iter: 1452 loss: 0.000148715742
Iter: 1453 loss: 0.000148394232
Iter: 1454 loss: 0.000148297215
Iter: 1455 loss: 0.00014774577
Iter: 1456 loss: 0.00014979628
Iter: 1457 loss: 0.000147609884
Iter: 1458 loss: 0.000147409373
Iter: 1459 loss: 0.000147354163
Iter: 1460 loss: 0.000147058396
Iter: 1461 loss: 0.000147101906
Iter: 1462 loss: 0.000146833976
Iter: 1463 loss: 0.000146534076
Iter: 1464 loss: 0.000148132647
Iter: 1465 loss: 0.000146487786
Iter: 1466 loss: 0.000146247592
Iter: 1467 loss: 0.000145991246
Iter: 1468 loss: 0.000145949132
Iter: 1469 loss: 0.000145582366
Iter: 1470 loss: 0.000145642058
Iter: 1471 loss: 0.000145305719
Iter: 1472 loss: 0.000144905236
Iter: 1473 loss: 0.000149495288
Iter: 1474 loss: 0.000144897698
Iter: 1475 loss: 0.000144551232
Iter: 1476 loss: 0.0001450735
Iter: 1477 loss: 0.000144385296
Iter: 1478 loss: 0.000144055637
Iter: 1479 loss: 0.000144779944
Iter: 1480 loss: 0.00014392857
Iter: 1481 loss: 0.000143562866
Iter: 1482 loss: 0.000143870275
Iter: 1483 loss: 0.00014334546
Iter: 1484 loss: 0.000142902
Iter: 1485 loss: 0.000144971549
Iter: 1486 loss: 0.000142819074
Iter: 1487 loss: 0.000142442877
Iter: 1488 loss: 0.000143776284
Iter: 1489 loss: 0.000142345438
Iter: 1490 loss: 0.000142019853
Iter: 1491 loss: 0.000142984252
Iter: 1492 loss: 0.000141919401
Iter: 1493 loss: 0.000141647324
Iter: 1494 loss: 0.000141641285
Iter: 1495 loss: 0.000141496668
Iter: 1496 loss: 0.000141476921
Iter: 1497 loss: 0.000141374971
Iter: 1498 loss: 0.000141167489
Iter: 1499 loss: 0.000141805023
Iter: 1500 loss: 0.000141107303
Iter: 1501 loss: 0.00014088524
Iter: 1502 loss: 0.000140479417
Iter: 1503 loss: 0.000150094042
Iter: 1504 loss: 0.000140478733
Iter: 1505 loss: 0.000139962038
Iter: 1506 loss: 0.000144262827
Iter: 1507 loss: 0.000139929965
Iter: 1508 loss: 0.000139529351
Iter: 1509 loss: 0.000142652119
Iter: 1510 loss: 0.000139500684
Iter: 1511 loss: 0.000139206
Iter: 1512 loss: 0.00013898348
Iter: 1513 loss: 0.000138888194
Iter: 1514 loss: 0.000138385411
Iter: 1515 loss: 0.000139947602
Iter: 1516 loss: 0.000138238713
Iter: 1517 loss: 0.000137836614
Iter: 1518 loss: 0.000138807751
Iter: 1519 loss: 0.000137692608
Iter: 1520 loss: 0.000137309238
Iter: 1521 loss: 0.000139055832
Iter: 1522 loss: 0.000137235853
Iter: 1523 loss: 0.000136909133
Iter: 1524 loss: 0.000137796553
Iter: 1525 loss: 0.000136800634
Iter: 1526 loss: 0.000136695831
Iter: 1527 loss: 0.000136638759
Iter: 1528 loss: 0.00013648771
Iter: 1529 loss: 0.000136256742
Iter: 1530 loss: 0.000136252434
Iter: 1531 loss: 0.000135982846
Iter: 1532 loss: 0.000137184747
Iter: 1533 loss: 0.000135930284
Iter: 1534 loss: 0.000135672424
Iter: 1535 loss: 0.000135855822
Iter: 1536 loss: 0.000135511786
Iter: 1537 loss: 0.000135212264
Iter: 1538 loss: 0.00013527328
Iter: 1539 loss: 0.000134989095
Iter: 1540 loss: 0.000134662056
Iter: 1541 loss: 0.000137865834
Iter: 1542 loss: 0.000134650312
Iter: 1543 loss: 0.000134317466
Iter: 1544 loss: 0.000134418762
Iter: 1545 loss: 0.000134079397
Iter: 1546 loss: 0.000133716851
Iter: 1547 loss: 0.000135312279
Iter: 1548 loss: 0.000133644033
Iter: 1549 loss: 0.000133283364
Iter: 1550 loss: 0.000133125679
Iter: 1551 loss: 0.000132940972
Iter: 1552 loss: 0.000132424218
Iter: 1553 loss: 0.000134959788
Iter: 1554 loss: 0.00013233372
Iter: 1555 loss: 0.000132025307
Iter: 1556 loss: 0.000136558941
Iter: 1557 loss: 0.000132024885
Iter: 1558 loss: 0.000131807319
Iter: 1559 loss: 0.00013282281
Iter: 1560 loss: 0.000131765919
Iter: 1561 loss: 0.000131521127
Iter: 1562 loss: 0.000132686939
Iter: 1563 loss: 0.000131477107
Iter: 1564 loss: 0.000131331704
Iter: 1565 loss: 0.000131154768
Iter: 1566 loss: 0.000131137291
Iter: 1567 loss: 0.000130818589
Iter: 1568 loss: 0.000131694716
Iter: 1569 loss: 0.00013071447
Iter: 1570 loss: 0.000130426153
Iter: 1571 loss: 0.000130420696
Iter: 1572 loss: 0.000130192959
Iter: 1573 loss: 0.00012983862
Iter: 1574 loss: 0.000130106127
Iter: 1575 loss: 0.000129621083
Iter: 1576 loss: 0.000129295469
Iter: 1577 loss: 0.00013380454
Iter: 1578 loss: 0.000129294116
Iter: 1579 loss: 0.000129006366
Iter: 1580 loss: 0.000128891435
Iter: 1581 loss: 0.000128738029
Iter: 1582 loss: 0.000128441461
Iter: 1583 loss: 0.000130487082
Iter: 1584 loss: 0.000128413492
Iter: 1585 loss: 0.000128148662
Iter: 1586 loss: 0.000128017215
Iter: 1587 loss: 0.000127891573
Iter: 1588 loss: 0.000127518084
Iter: 1589 loss: 0.000129278051
Iter: 1590 loss: 0.000127449661
Iter: 1591 loss: 0.00012721181
Iter: 1592 loss: 0.00012720839
Iter: 1593 loss: 0.000127026055
Iter: 1594 loss: 0.000128088664
Iter: 1595 loss: 0.000127002029
Iter: 1596 loss: 0.000126842526
Iter: 1597 loss: 0.000126632134
Iter: 1598 loss: 0.000126619256
Iter: 1599 loss: 0.000126370112
Iter: 1600 loss: 0.00012769665
Iter: 1601 loss: 0.000126331928
Iter: 1602 loss: 0.000126094747
Iter: 1603 loss: 0.000126002065
Iter: 1604 loss: 0.000125874474
Iter: 1605 loss: 0.000125580467
Iter: 1606 loss: 0.000125878883
Iter: 1607 loss: 0.00012541654
Iter: 1608 loss: 0.000125024293
Iter: 1609 loss: 0.000126019702
Iter: 1610 loss: 0.000124887971
Iter: 1611 loss: 0.000124487822
Iter: 1612 loss: 0.000127673586
Iter: 1613 loss: 0.000124461396
Iter: 1614 loss: 0.000124184124
Iter: 1615 loss: 0.000124065176
Iter: 1616 loss: 0.000123922393
Iter: 1617 loss: 0.000123564532
Iter: 1618 loss: 0.000125753111
Iter: 1619 loss: 0.000123521328
Iter: 1620 loss: 0.000123180915
Iter: 1621 loss: 0.000123043879
Iter: 1622 loss: 0.00012286182
Iter: 1623 loss: 0.000122410915
Iter: 1624 loss: 0.000124653947
Iter: 1625 loss: 0.000122332975
Iter: 1626 loss: 0.000122168974
Iter: 1627 loss: 0.000122149271
Iter: 1628 loss: 0.000121939695
Iter: 1629 loss: 0.000122467085
Iter: 1630 loss: 0.000121865058
Iter: 1631 loss: 0.000121705583
Iter: 1632 loss: 0.000121607984
Iter: 1633 loss: 0.000121542267
Iter: 1634 loss: 0.000121276418
Iter: 1635 loss: 0.000121861369
Iter: 1636 loss: 0.000121174322
Iter: 1637 loss: 0.000120881989
Iter: 1638 loss: 0.00012118528
Iter: 1639 loss: 0.000120719298
Iter: 1640 loss: 0.00012040593
Iter: 1641 loss: 0.000120855882
Iter: 1642 loss: 0.000120253033
Iter: 1643 loss: 0.000119906028
Iter: 1644 loss: 0.000120115525
Iter: 1645 loss: 0.000119681208
Iter: 1646 loss: 0.000119351425
Iter: 1647 loss: 0.000119349956
Iter: 1648 loss: 0.000119121163
Iter: 1649 loss: 0.000119024335
Iter: 1650 loss: 0.000118905067
Iter: 1651 loss: 0.000118597294
Iter: 1652 loss: 0.000119756922
Iter: 1653 loss: 0.000118522854
Iter: 1654 loss: 0.000118234486
Iter: 1655 loss: 0.000118424206
Iter: 1656 loss: 0.000118052238
Iter: 1657 loss: 0.000117716816
Iter: 1658 loss: 0.00011919369
Iter: 1659 loss: 0.000117649884
Iter: 1660 loss: 0.000117389427
Iter: 1661 loss: 0.000119749297
Iter: 1662 loss: 0.000117378426
Iter: 1663 loss: 0.00011713845
Iter: 1664 loss: 0.000120033146
Iter: 1665 loss: 0.000117135147
Iter: 1666 loss: 0.000117061943
Iter: 1667 loss: 0.000116870826
Iter: 1668 loss: 0.000118328731
Iter: 1669 loss: 0.00011683318
Iter: 1670 loss: 0.000116567942
Iter: 1671 loss: 0.000118771386
Iter: 1672 loss: 0.000116552546
Iter: 1673 loss: 0.000116379328
Iter: 1674 loss: 0.000116701136
Iter: 1675 loss: 0.000116305833
Iter: 1676 loss: 0.000116106705
Iter: 1677 loss: 0.000116031129
Iter: 1678 loss: 0.000115921401
Iter: 1679 loss: 0.000115647374
Iter: 1680 loss: 0.000116656971
Iter: 1681 loss: 0.000115578616
Iter: 1682 loss: 0.00011532694
Iter: 1683 loss: 0.000116894378
Iter: 1684 loss: 0.000115297138
Iter: 1685 loss: 0.000115035131
Iter: 1686 loss: 0.000114830247
Iter: 1687 loss: 0.000114748465
Iter: 1688 loss: 0.000114424234
Iter: 1689 loss: 0.000115598661
Iter: 1690 loss: 0.000114342351
Iter: 1691 loss: 0.000114041497
Iter: 1692 loss: 0.000114825365
Iter: 1693 loss: 0.000113938382
Iter: 1694 loss: 0.000113661459
Iter: 1695 loss: 0.00011442283
Iter: 1696 loss: 0.000113570437
Iter: 1697 loss: 0.000113511152
Iter: 1698 loss: 0.000113453381
Iter: 1699 loss: 0.000113314163
Iter: 1700 loss: 0.000113325732
Iter: 1701 loss: 0.000113206217
Iter: 1702 loss: 0.000113098635
Iter: 1703 loss: 0.000112870177
Iter: 1704 loss: 0.000116590498
Iter: 1705 loss: 0.000112863177
Iter: 1706 loss: 0.000112721682
Iter: 1707 loss: 0.000112704867
Iter: 1708 loss: 0.000112589711
Iter: 1709 loss: 0.000112362657
Iter: 1710 loss: 0.000116838055
Iter: 1711 loss: 0.000112360409
Iter: 1712 loss: 0.000112097536
Iter: 1713 loss: 0.000113616712
Iter: 1714 loss: 0.000112062204
Iter: 1715 loss: 0.000111808477
Iter: 1716 loss: 0.000112377224
Iter: 1717 loss: 0.000111712587
Iter: 1718 loss: 0.000111469242
Iter: 1719 loss: 0.000112339636
Iter: 1720 loss: 0.000111407717
Iter: 1721 loss: 0.000111165471
Iter: 1722 loss: 0.000111662477
Iter: 1723 loss: 0.000111069057
Iter: 1724 loss: 0.000110785419
Iter: 1725 loss: 0.000111865091
Iter: 1726 loss: 0.000110718189
Iter: 1727 loss: 0.000110517401
Iter: 1728 loss: 0.000110613168
Iter: 1729 loss: 0.000110382141
Iter: 1730 loss: 0.000110258668
Iter: 1731 loss: 0.000110242581
Iter: 1732 loss: 0.000110098306
Iter: 1733 loss: 0.000110839901
Iter: 1734 loss: 0.000110075169
Iter: 1735 loss: 0.000109981571
Iter: 1736 loss: 0.000109779081
Iter: 1737 loss: 0.000112863352
Iter: 1738 loss: 0.00010977099
Iter: 1739 loss: 0.000109574597
Iter: 1740 loss: 0.000109853419
Iter: 1741 loss: 0.000109478344
Iter: 1742 loss: 0.000109234534
Iter: 1743 loss: 0.000111577989
Iter: 1744 loss: 0.000109224944
Iter: 1745 loss: 0.000109066066
Iter: 1746 loss: 0.000108823508
Iter: 1747 loss: 0.000108819338
Iter: 1748 loss: 0.0001085439
Iter: 1749 loss: 0.000110035631
Iter: 1750 loss: 0.000108502209
Iter: 1751 loss: 0.000108295179
Iter: 1752 loss: 0.00010928695
Iter: 1753 loss: 0.000108258515
Iter: 1754 loss: 0.000108025837
Iter: 1755 loss: 0.000108073553
Iter: 1756 loss: 0.00010785347
Iter: 1757 loss: 0.000107624292
Iter: 1758 loss: 0.000109662447
Iter: 1759 loss: 0.000107612985
Iter: 1760 loss: 0.000107396467
Iter: 1761 loss: 0.000107489323
Iter: 1762 loss: 0.000107248896
Iter: 1763 loss: 0.000107003689
Iter: 1764 loss: 0.000107453598
Iter: 1765 loss: 0.000106898042
Iter: 1766 loss: 0.000106959495
Iter: 1767 loss: 0.000106811116
Iter: 1768 loss: 0.00010671885
Iter: 1769 loss: 0.000106476291
Iter: 1770 loss: 0.000108348169
Iter: 1771 loss: 0.000106428386
Iter: 1772 loss: 0.000106199906
Iter: 1773 loss: 0.000106601627
Iter: 1774 loss: 0.000106099425
Iter: 1775 loss: 0.000105865693
Iter: 1776 loss: 0.000107029351
Iter: 1777 loss: 0.000105825238
Iter: 1778 loss: 0.000105639992
Iter: 1779 loss: 0.000107277374
Iter: 1780 loss: 0.000105630577
Iter: 1781 loss: 0.000105511659
Iter: 1782 loss: 0.00010521787
Iter: 1783 loss: 0.000108139735
Iter: 1784 loss: 0.00010517925
Iter: 1785 loss: 0.000104914441
Iter: 1786 loss: 0.000108366898
Iter: 1787 loss: 0.000104912935
Iter: 1788 loss: 0.000104718871
Iter: 1789 loss: 0.000105433865
Iter: 1790 loss: 0.000104670733
Iter: 1791 loss: 0.000104513194
Iter: 1792 loss: 0.000104792343
Iter: 1793 loss: 0.00010444424
Iter: 1794 loss: 0.000104257444
Iter: 1795 loss: 0.00010444902
Iter: 1796 loss: 0.000104152336
Iter: 1797 loss: 0.000103959843
Iter: 1798 loss: 0.000105538042
Iter: 1799 loss: 0.000103947394
Iter: 1800 loss: 0.000103844628
Iter: 1801 loss: 0.000104463703
Iter: 1802 loss: 0.000103832164
Iter: 1803 loss: 0.000103674167
Iter: 1804 loss: 0.000103710758
Iter: 1805 loss: 0.000103558152
Iter: 1806 loss: 0.000103452054
Iter: 1807 loss: 0.000103301711
Iter: 1808 loss: 0.000103296188
Iter: 1809 loss: 0.000103120576
Iter: 1810 loss: 0.000103806073
Iter: 1811 loss: 0.000103079903
Iter: 1812 loss: 0.000102909347
Iter: 1813 loss: 0.000104171071
Iter: 1814 loss: 0.000102895814
Iter: 1815 loss: 0.000102751583
Iter: 1816 loss: 0.00010275362
Iter: 1817 loss: 0.000102637219
Iter: 1818 loss: 0.000102464117
Iter: 1819 loss: 0.000102674843
Iter: 1820 loss: 0.000102373495
Iter: 1821 loss: 0.000102184349
Iter: 1822 loss: 0.000102392769
Iter: 1823 loss: 0.000102081387
Iter: 1824 loss: 0.000101873113
Iter: 1825 loss: 0.000104026505
Iter: 1826 loss: 0.000101866055
Iter: 1827 loss: 0.000101694925
Iter: 1828 loss: 0.000102104503
Iter: 1829 loss: 0.000101632046
Iter: 1830 loss: 0.000101477475
Iter: 1831 loss: 0.000101709782
Iter: 1832 loss: 0.000101403559
Iter: 1833 loss: 0.000101203637
Iter: 1834 loss: 0.000101918486
Iter: 1835 loss: 0.00010115133
Iter: 1836 loss: 0.000101120298
Iter: 1837 loss: 0.000101075726
Iter: 1838 loss: 0.000100998834
Iter: 1839 loss: 0.000100800418
Iter: 1840 loss: 0.000102459824
Iter: 1841 loss: 0.000100765232
Iter: 1842 loss: 0.000100594392
Iter: 1843 loss: 0.000100415971
Iter: 1844 loss: 0.000100384896
Iter: 1845 loss: 0.000100151949
Iter: 1846 loss: 0.000103160892
Iter: 1847 loss: 0.000100150326
Iter: 1848 loss: 9.99774e-05
Iter: 1849 loss: 0.00010138673
Iter: 1850 loss: 9.9966288e-05
Iter: 1851 loss: 9.98647592e-05
Iter: 1852 loss: 9.98042597e-05
Iter: 1853 loss: 9.97619354e-05
Iter: 1854 loss: 9.95805385e-05
Iter: 1855 loss: 9.9784229e-05
Iter: 1856 loss: 9.94827496e-05
Iter: 1857 loss: 9.92996138e-05
Iter: 1858 loss: 9.99342446e-05
Iter: 1859 loss: 9.92511777e-05
Iter: 1860 loss: 9.90754852e-05
Iter: 1861 loss: 9.93435096e-05
Iter: 1862 loss: 9.89919063e-05
Iter: 1863 loss: 9.88604806e-05
Iter: 1864 loss: 9.88607353e-05
Iter: 1865 loss: 9.87344538e-05
Iter: 1866 loss: 9.87654857e-05
Iter: 1867 loss: 9.86421073e-05
Iter: 1868 loss: 9.85913066e-05
Iter: 1869 loss: 9.85741935e-05
Iter: 1870 loss: 9.8505494e-05
Iter: 1871 loss: 9.85585502e-05
Iter: 1872 loss: 9.84636863e-05
Iter: 1873 loss: 9.8394019e-05
Iter: 1874 loss: 9.82563797e-05
Iter: 1875 loss: 0.000100968115
Iter: 1876 loss: 9.82547863e-05
Iter: 1877 loss: 9.80900295e-05
Iter: 1878 loss: 9.83123464e-05
Iter: 1879 loss: 9.80070472e-05
Iter: 1880 loss: 9.78530297e-05
Iter: 1881 loss: 9.84816e-05
Iter: 1882 loss: 9.7819895e-05
Iter: 1883 loss: 9.76636802e-05
Iter: 1884 loss: 9.90977787e-05
Iter: 1885 loss: 9.76572628e-05
Iter: 1886 loss: 9.75567382e-05
Iter: 1887 loss: 9.73761416e-05
Iter: 1888 loss: 0.000101801925
Iter: 1889 loss: 9.73761e-05
Iter: 1890 loss: 9.71700065e-05
Iter: 1891 loss: 9.80115074e-05
Iter: 1892 loss: 9.71242625e-05
Iter: 1893 loss: 9.69368266e-05
Iter: 1894 loss: 9.72888665e-05
Iter: 1895 loss: 9.68571258e-05
Iter: 1896 loss: 9.66513908e-05
Iter: 1897 loss: 9.77783784e-05
Iter: 1898 loss: 9.662089e-05
Iter: 1899 loss: 9.64752398e-05
Iter: 1900 loss: 9.71622649e-05
Iter: 1901 loss: 9.64484861e-05
Iter: 1902 loss: 9.63339116e-05
Iter: 1903 loss: 9.63342827e-05
Iter: 1904 loss: 9.62493941e-05
Iter: 1905 loss: 9.67598608e-05
Iter: 1906 loss: 9.62386694e-05
Iter: 1907 loss: 9.61843543e-05
Iter: 1908 loss: 9.60500693e-05
Iter: 1909 loss: 9.74415598e-05
Iter: 1910 loss: 9.60344114e-05
Iter: 1911 loss: 9.58774108e-05
Iter: 1912 loss: 9.65853251e-05
Iter: 1913 loss: 9.58465098e-05
Iter: 1914 loss: 9.57089505e-05
Iter: 1915 loss: 9.57124721e-05
Iter: 1916 loss: 9.55999276e-05
Iter: 1917 loss: 9.54767893e-05
Iter: 1918 loss: 9.74607392e-05
Iter: 1919 loss: 9.54768038e-05
Iter: 1920 loss: 9.53574126e-05
Iter: 1921 loss: 9.54136121e-05
Iter: 1922 loss: 9.52776245e-05
Iter: 1923 loss: 9.5154719e-05
Iter: 1924 loss: 9.50794129e-05
Iter: 1925 loss: 9.50290851e-05
Iter: 1926 loss: 9.48298039e-05
Iter: 1927 loss: 9.51913535e-05
Iter: 1928 loss: 9.47431399e-05
Iter: 1929 loss: 9.45725915e-05
Iter: 1930 loss: 9.54400894e-05
Iter: 1931 loss: 9.45448774e-05
Iter: 1932 loss: 9.44021376e-05
Iter: 1933 loss: 9.5115458e-05
Iter: 1934 loss: 9.43778e-05
Iter: 1935 loss: 9.43000123e-05
Iter: 1936 loss: 9.42985498e-05
Iter: 1937 loss: 9.42263578e-05
Iter: 1938 loss: 9.46428845e-05
Iter: 1939 loss: 9.42175e-05
Iter: 1940 loss: 9.41528269e-05
Iter: 1941 loss: 9.40137616e-05
Iter: 1942 loss: 9.61632904e-05
Iter: 1943 loss: 9.4009134e-05
Iter: 1944 loss: 9.38929661e-05
Iter: 1945 loss: 9.42406623e-05
Iter: 1946 loss: 9.38574085e-05
Iter: 1947 loss: 9.37359291e-05
Iter: 1948 loss: 9.38884768e-05
Iter: 1949 loss: 9.36732613e-05
Iter: 1950 loss: 9.35355056e-05
Iter: 1951 loss: 9.39802485e-05
Iter: 1952 loss: 9.34966956e-05
Iter: 1953 loss: 9.3370516e-05
Iter: 1954 loss: 9.45798092e-05
Iter: 1955 loss: 9.33659758e-05
Iter: 1956 loss: 9.3264418e-05
Iter: 1957 loss: 9.31720424e-05
Iter: 1958 loss: 9.31464892e-05
Iter: 1959 loss: 9.3010749e-05
Iter: 1960 loss: 9.29619564e-05
Iter: 1961 loss: 9.28871596e-05
Iter: 1962 loss: 9.270585e-05
Iter: 1963 loss: 9.39848906e-05
Iter: 1964 loss: 9.26895518e-05
Iter: 1965 loss: 9.25535569e-05
Iter: 1966 loss: 9.3310533e-05
Iter: 1967 loss: 9.2534523e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi2.8
+ date
Sun Nov  1 00:10:52 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec460620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec487ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec4879d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec4afe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec40e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec3e6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec1d6620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec1dd268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec1ddbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec2d7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec260158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec2f1e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec2f17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec041e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec04f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e47e0d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e47f7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e4768598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec0f56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec0f5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec0e7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec0e7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e46b8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e46b1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e46b1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec0957b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e477ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec231840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec2311e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28ec247bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e45e6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e4559158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e4559378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e472bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e47069d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28e45a8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.195252
test_loss: 0.19409771
train_loss: 0.07612361
test_loss: 0.07647923
train_loss: 0.06056228
test_loss: 0.058859184
train_loss: 0.046305984
test_loss: 0.052039605
train_loss: 0.044228822
test_loss: 0.048566934
train_loss: 0.045923747
test_loss: 0.044707555
train_loss: 0.040016897
test_loss: 0.043079168
train_loss: 0.037446894
test_loss: 0.03882167
train_loss: 0.034982454
test_loss: 0.037623707
train_loss: 0.03526047
test_loss: 0.035168245
train_loss: 0.030007305
test_loss: 0.03339937
train_loss: 0.03142319
test_loss: 0.032115903
train_loss: 0.026389856
test_loss: 0.030288765
train_loss: 0.028690848
test_loss: 0.029978536
train_loss: 0.024859548
test_loss: 0.028735118
train_loss: 0.023131197
test_loss: 0.027971124
train_loss: 0.025321787
test_loss: 0.027542092
train_loss: 0.02385801
test_loss: 0.026704552
train_loss: 0.02301782
test_loss: 0.026540412
train_loss: 0.023212155
test_loss: 0.025564404
train_loss: 0.024628462
test_loss: 0.025057714
train_loss: 0.025525546
test_loss: 0.024628159
train_loss: 0.020486731
test_loss: 0.024269521
train_loss: 0.021265563
test_loss: 0.023767883
train_loss: 0.020092446
test_loss: 0.023668077
train_loss: 0.020468738
test_loss: 0.023337739
train_loss: 0.023070874
test_loss: 0.023020225
train_loss: 0.018814027
test_loss: 0.02285339
train_loss: 0.023390908
test_loss: 0.022537874
train_loss: 0.018731015
test_loss: 0.022404464
train_loss: 0.018807797
test_loss: 0.022264738
train_loss: 0.02187422
test_loss: 0.022084415
train_loss: 0.018120235
test_loss: 0.022027193
train_loss: 0.017343486
test_loss: 0.02187946
train_loss: 0.018476898
test_loss: 0.021715099
train_loss: 0.01966361
test_loss: 0.02169608
train_loss: 0.019179514
test_loss: 0.021539856
train_loss: 0.019359928
test_loss: 0.021476353
train_loss: 0.018274471
test_loss: 0.02142738
train_loss: 0.018469065
test_loss: 0.02138227
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504cabc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504cacad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504caca6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504cacaa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504c9fb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504c9fbea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049805ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50497681e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049768730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049752378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50496eb7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049699e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049699bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50496a8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049662840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049668598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049668ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049635a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504959d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504959db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50495eb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50495ebd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049549840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50494d2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50495897b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049584598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50494b2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50494b8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50494b8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50494606a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049440bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50494077b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049407378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f50493ddf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5049393510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f504933d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000869982061
Iter: 2 loss: 5408.15771
Iter: 3 loss: 0.000869982061
Iter: 4 loss: 0.00491118291
Iter: 5 loss: 0.000865656708
Iter: 6 loss: 0.000848681899
Iter: 7 loss: 0.00106600136
Iter: 8 loss: 0.000848107855
Iter: 9 loss: 0.000840119319
Iter: 10 loss: 0.00082607544
Iter: 11 loss: 0.00082606962
Iter: 12 loss: 0.00081148732
Iter: 13 loss: 0.00080763
Iter: 14 loss: 0.00079860026
Iter: 15 loss: 0.000790225866
Iter: 16 loss: 0.000792896491
Iter: 17 loss: 0.000784055097
Iter: 18 loss: 0.00077821326
Iter: 19 loss: 0.000769810867
Iter: 20 loss: 0.000769580074
Iter: 21 loss: 0.000763048534
Iter: 22 loss: 0.000765718811
Iter: 23 loss: 0.000758500188
Iter: 24 loss: 0.000752306427
Iter: 25 loss: 0.00073902274
Iter: 26 loss: 0.000977990101
Iter: 27 loss: 0.000738623319
Iter: 28 loss: 0.000727638
Iter: 29 loss: 0.000763955701
Iter: 30 loss: 0.00072458724
Iter: 31 loss: 0.000718484691
Iter: 32 loss: 0.000711360539
Iter: 33 loss: 0.000710547902
Iter: 34 loss: 0.000701033277
Iter: 35 loss: 0.00076612283
Iter: 36 loss: 0.000700009754
Iter: 37 loss: 0.000694987946
Iter: 38 loss: 0.000692330301
Iter: 39 loss: 0.000690027839
Iter: 40 loss: 0.000682223414
Iter: 41 loss: 0.000733095163
Iter: 42 loss: 0.000681377365
Iter: 43 loss: 0.000676174648
Iter: 44 loss: 0.00069339585
Iter: 45 loss: 0.000674694602
Iter: 46 loss: 0.000671124435
Iter: 47 loss: 0.000679206685
Iter: 48 loss: 0.000669787929
Iter: 49 loss: 0.000666571315
Iter: 50 loss: 0.000716404116
Iter: 51 loss: 0.000666563516
Iter: 52 loss: 0.00066549459
Iter: 53 loss: 0.000664385851
Iter: 54 loss: 0.000664188876
Iter: 55 loss: 0.000661164348
Iter: 56 loss: 0.000660127669
Iter: 57 loss: 0.000658398261
Iter: 58 loss: 0.000654785894
Iter: 59 loss: 0.000669964356
Iter: 60 loss: 0.000654030067
Iter: 61 loss: 0.000651291804
Iter: 62 loss: 0.000650130445
Iter: 63 loss: 0.000648713438
Iter: 64 loss: 0.000644047686
Iter: 65 loss: 0.00067235186
Iter: 66 loss: 0.000643414853
Iter: 67 loss: 0.000639427104
Iter: 68 loss: 0.000652624934
Iter: 69 loss: 0.000638325757
Iter: 70 loss: 0.000635358621
Iter: 71 loss: 0.000648488058
Iter: 72 loss: 0.00063475885
Iter: 73 loss: 0.000631342642
Iter: 74 loss: 0.000634257449
Iter: 75 loss: 0.000629333954
Iter: 76 loss: 0.000626017281
Iter: 77 loss: 0.000647235196
Iter: 78 loss: 0.00062564807
Iter: 79 loss: 0.000624477514
Iter: 80 loss: 0.000624411216
Iter: 81 loss: 0.00062350923
Iter: 82 loss: 0.000633342774
Iter: 83 loss: 0.0006234842
Iter: 84 loss: 0.000622867956
Iter: 85 loss: 0.000622664345
Iter: 86 loss: 0.000622312829
Iter: 87 loss: 0.000621035811
Iter: 88 loss: 0.000621339423
Iter: 89 loss: 0.000620098319
Iter: 90 loss: 0.000618628808
Iter: 91 loss: 0.000623469357
Iter: 92 loss: 0.000618222752
Iter: 93 loss: 0.000616805046
Iter: 94 loss: 0.000614566845
Iter: 95 loss: 0.000614543445
Iter: 96 loss: 0.000611543888
Iter: 97 loss: 0.000623076397
Iter: 98 loss: 0.000610819669
Iter: 99 loss: 0.000608161
Iter: 100 loss: 0.000615505502
Iter: 101 loss: 0.000607308757
Iter: 102 loss: 0.00060473656
Iter: 103 loss: 0.000610966759
Iter: 104 loss: 0.00060379412
Iter: 105 loss: 0.000601011212
Iter: 106 loss: 0.00060582842
Iter: 107 loss: 0.000599778839
Iter: 108 loss: 0.000597594131
Iter: 109 loss: 0.00060632691
Iter: 110 loss: 0.000597098202
Iter: 111 loss: 0.000596146565
Iter: 112 loss: 0.000596103258
Iter: 113 loss: 0.000595496269
Iter: 114 loss: 0.000595495454
Iter: 115 loss: 0.000595041784
Iter: 116 loss: 0.000594648533
Iter: 117 loss: 0.000594528916
Iter: 118 loss: 0.00059356651
Iter: 119 loss: 0.000594572281
Iter: 120 loss: 0.000593029661
Iter: 121 loss: 0.000592122204
Iter: 122 loss: 0.000594336772
Iter: 123 loss: 0.000591797638
Iter: 124 loss: 0.000590587966
Iter: 125 loss: 0.000588663504
Iter: 126 loss: 0.00058864546
Iter: 127 loss: 0.000586186885
Iter: 128 loss: 0.000597265549
Iter: 129 loss: 0.000585702597
Iter: 130 loss: 0.000583853398
Iter: 131 loss: 0.000587438466
Iter: 132 loss: 0.000583087676
Iter: 133 loss: 0.000580997905
Iter: 134 loss: 0.000585265458
Iter: 135 loss: 0.000580142194
Iter: 136 loss: 0.000578534207
Iter: 137 loss: 0.000595216232
Iter: 138 loss: 0.00057849416
Iter: 139 loss: 0.000577515
Iter: 140 loss: 0.000577116443
Iter: 141 loss: 0.000576598
Iter: 142 loss: 0.000575712766
Iter: 143 loss: 0.000575677957
Iter: 144 loss: 0.00057514105
Iter: 145 loss: 0.000575118116
Iter: 146 loss: 0.000574697915
Iter: 147 loss: 0.000574065838
Iter: 148 loss: 0.000574054
Iter: 149 loss: 0.000573012454
Iter: 150 loss: 0.000575256185
Iter: 151 loss: 0.000572602381
Iter: 152 loss: 0.000571703655
Iter: 153 loss: 0.000573104946
Iter: 154 loss: 0.000571281416
Iter: 155 loss: 0.000569958356
Iter: 156 loss: 0.000569830707
Iter: 157 loss: 0.000568857533
Iter: 158 loss: 0.000567415613
Iter: 159 loss: 0.000573996687
Iter: 160 loss: 0.000567137904
Iter: 161 loss: 0.000566144
Iter: 162 loss: 0.000566247094
Iter: 163 loss: 0.000565380033
Iter: 164 loss: 0.000563994225
Iter: 165 loss: 0.000566968403
Iter: 166 loss: 0.000563447
Iter: 167 loss: 0.000562252593
Iter: 168 loss: 0.000571337412
Iter: 169 loss: 0.000562166097
Iter: 170 loss: 0.000561215216
Iter: 171 loss: 0.000560070737
Iter: 172 loss: 0.000559957116
Iter: 173 loss: 0.000558205065
Iter: 174 loss: 0.000566342263
Iter: 175 loss: 0.000557882478
Iter: 176 loss: 0.000558181899
Iter: 177 loss: 0.00055705366
Iter: 178 loss: 0.000556581072
Iter: 179 loss: 0.000555603241
Iter: 180 loss: 0.000572192599
Iter: 181 loss: 0.00055558281
Iter: 182 loss: 0.000554176222
Iter: 183 loss: 0.000568057178
Iter: 184 loss: 0.000554124126
Iter: 185 loss: 0.000553261372
Iter: 186 loss: 0.000554232625
Iter: 187 loss: 0.000552795827
Iter: 188 loss: 0.000551955192
Iter: 189 loss: 0.000551598729
Iter: 190 loss: 0.000551159726
Iter: 191 loss: 0.000550490338
Iter: 192 loss: 0.000550480734
Iter: 193 loss: 0.000549956167
Iter: 194 loss: 0.000549352262
Iter: 195 loss: 0.000548256212
Iter: 196 loss: 0.000574497855
Iter: 197 loss: 0.000548255804
Iter: 198 loss: 0.000547183678
Iter: 199 loss: 0.000547514064
Iter: 200 loss: 0.000546412659
Iter: 201 loss: 0.00054503954
Iter: 202 loss: 0.000553267077
Iter: 203 loss: 0.000544873066
Iter: 204 loss: 0.00054382591
Iter: 205 loss: 0.000544140872
Iter: 206 loss: 0.00054307864
Iter: 207 loss: 0.000541710411
Iter: 208 loss: 0.000539629604
Iter: 209 loss: 0.00053959142
Iter: 210 loss: 0.000539746252
Iter: 211 loss: 0.000538850261
Iter: 212 loss: 0.000538386172
Iter: 213 loss: 0.00053781172
Iter: 214 loss: 0.000537763
Iter: 215 loss: 0.000537194544
Iter: 216 loss: 0.00053707161
Iter: 217 loss: 0.000536576263
Iter: 218 loss: 0.000537801709
Iter: 219 loss: 0.000536404084
Iter: 220 loss: 0.000535898143
Iter: 221 loss: 0.000536489068
Iter: 222 loss: 0.000535629282
Iter: 223 loss: 0.000535187777
Iter: 224 loss: 0.000534907
Iter: 225 loss: 0.000534734107
Iter: 226 loss: 0.000534035615
Iter: 227 loss: 0.000542677823
Iter: 228 loss: 0.000534028048
Iter: 229 loss: 0.000533240498
Iter: 230 loss: 0.00053224666
Iter: 231 loss: 0.000532169
Iter: 232 loss: 0.000530858641
Iter: 233 loss: 0.00053675985
Iter: 234 loss: 0.000530609628
Iter: 235 loss: 0.000529392506
Iter: 236 loss: 0.000533243874
Iter: 237 loss: 0.000529038662
Iter: 238 loss: 0.000528257573
Iter: 239 loss: 0.000527557801
Iter: 240 loss: 0.000527360709
Iter: 241 loss: 0.000526795746
Iter: 242 loss: 0.000526735
Iter: 243 loss: 0.000526087068
Iter: 244 loss: 0.000525214884
Iter: 245 loss: 0.000525169657
Iter: 246 loss: 0.000524494
Iter: 247 loss: 0.000524071103
Iter: 248 loss: 0.000523801136
Iter: 249 loss: 0.000523205963
Iter: 250 loss: 0.000523127965
Iter: 251 loss: 0.000522476912
Iter: 252 loss: 0.000525473326
Iter: 253 loss: 0.000522352522
Iter: 254 loss: 0.00052174466
Iter: 255 loss: 0.000520477421
Iter: 256 loss: 0.000542153139
Iter: 257 loss: 0.000520447036
Iter: 258 loss: 0.000519110239
Iter: 259 loss: 0.000521550304
Iter: 260 loss: 0.000518532936
Iter: 261 loss: 0.000517851
Iter: 262 loss: 0.000517703302
Iter: 263 loss: 0.000517108827
Iter: 264 loss: 0.000516785774
Iter: 265 loss: 0.000516519067
Iter: 266 loss: 0.00051581359
Iter: 267 loss: 0.000514308107
Iter: 268 loss: 0.000538527791
Iter: 269 loss: 0.000514259213
Iter: 270 loss: 0.00051277515
Iter: 271 loss: 0.000530896301
Iter: 272 loss: 0.000512756058
Iter: 273 loss: 0.000512486557
Iter: 274 loss: 0.00051213085
Iter: 275 loss: 0.000511723
Iter: 276 loss: 0.000511538237
Iter: 277 loss: 0.000511332182
Iter: 278 loss: 0.000510925369
Iter: 279 loss: 0.000510566519
Iter: 280 loss: 0.000510461279
Iter: 281 loss: 0.000510052079
Iter: 282 loss: 0.00051274919
Iter: 283 loss: 0.000510009471
Iter: 284 loss: 0.000509610749
Iter: 285 loss: 0.000510312268
Iter: 286 loss: 0.000509436592
Iter: 287 loss: 0.000508951256
Iter: 288 loss: 0.000507951365
Iter: 289 loss: 0.000525670475
Iter: 290 loss: 0.000507931
Iter: 291 loss: 0.000506249489
Iter: 292 loss: 0.000508846482
Iter: 293 loss: 0.000505446107
Iter: 294 loss: 0.000504363
Iter: 295 loss: 0.000504335389
Iter: 296 loss: 0.000503570307
Iter: 297 loss: 0.000505989
Iter: 298 loss: 0.000503348419
Iter: 299 loss: 0.000502639101
Iter: 300 loss: 0.000513829407
Iter: 301 loss: 0.000502639101
Iter: 302 loss: 0.000502210052
Iter: 303 loss: 0.000501097937
Iter: 304 loss: 0.000510053535
Iter: 305 loss: 0.00050089322
Iter: 306 loss: 0.000501609058
Iter: 307 loss: 0.000500294147
Iter: 308 loss: 0.000499720278
Iter: 309 loss: 0.0005005782
Iter: 310 loss: 0.000499442569
Iter: 311 loss: 0.000498935697
Iter: 312 loss: 0.000498640235
Iter: 313 loss: 0.000498426787
Iter: 314 loss: 0.000497704721
Iter: 315 loss: 0.000498569047
Iter: 316 loss: 0.0004973209
Iter: 317 loss: 0.000496671
Iter: 318 loss: 0.000496640336
Iter: 319 loss: 0.000496308785
Iter: 320 loss: 0.000495578803
Iter: 321 loss: 0.00050619035
Iter: 322 loss: 0.000495546
Iter: 323 loss: 0.000494147
Iter: 324 loss: 0.000493829779
Iter: 325 loss: 0.000492927968
Iter: 326 loss: 0.000491266255
Iter: 327 loss: 0.000498603564
Iter: 328 loss: 0.00049092964
Iter: 329 loss: 0.000489747152
Iter: 330 loss: 0.000497955712
Iter: 331 loss: 0.000489635509
Iter: 332 loss: 0.000488926948
Iter: 333 loss: 0.000492512947
Iter: 334 loss: 0.00048880541
Iter: 335 loss: 0.000488184451
Iter: 336 loss: 0.000496223453
Iter: 337 loss: 0.000488181307
Iter: 338 loss: 0.000487735117
Iter: 339 loss: 0.000487800513
Iter: 340 loss: 0.00048739434
Iter: 341 loss: 0.00048697606
Iter: 342 loss: 0.000486964505
Iter: 343 loss: 0.000486745354
Iter: 344 loss: 0.000486106204
Iter: 345 loss: 0.000488628051
Iter: 346 loss: 0.000485840166
Iter: 347 loss: 0.000485199271
Iter: 348 loss: 0.000485196419
Iter: 349 loss: 0.00048484432
Iter: 350 loss: 0.000490461884
Iter: 351 loss: 0.00048484531
Iter: 352 loss: 0.000484590419
Iter: 353 loss: 0.000483898824
Iter: 354 loss: 0.00048826696
Iter: 355 loss: 0.00048372
Iter: 356 loss: 0.00048282044
Iter: 357 loss: 0.000483542593
Iter: 358 loss: 0.000482279574
Iter: 359 loss: 0.00048121548
Iter: 360 loss: 0.000482761097
Iter: 361 loss: 0.000480698131
Iter: 362 loss: 0.000479467795
Iter: 363 loss: 0.000480436
Iter: 364 loss: 0.000478726055
Iter: 365 loss: 0.000477337
Iter: 366 loss: 0.000486749661
Iter: 367 loss: 0.000477198279
Iter: 368 loss: 0.000475976936
Iter: 369 loss: 0.000482254254
Iter: 370 loss: 0.000475771
Iter: 371 loss: 0.000475020788
Iter: 372 loss: 0.00047497265
Iter: 373 loss: 0.000474642409
Iter: 374 loss: 0.000476181885
Iter: 375 loss: 0.000474578759
Iter: 376 loss: 0.000474202039
Iter: 377 loss: 0.000473533932
Iter: 378 loss: 0.000473533728
Iter: 379 loss: 0.000472932268
Iter: 380 loss: 0.000476635119
Iter: 381 loss: 0.000472857442
Iter: 382 loss: 0.000472502841
Iter: 383 loss: 0.000472474785
Iter: 384 loss: 0.000472191197
Iter: 385 loss: 0.000471563282
Iter: 386 loss: 0.000480545859
Iter: 387 loss: 0.000471532636
Iter: 388 loss: 0.000470870174
Iter: 389 loss: 0.000470596133
Iter: 390 loss: 0.000470244791
Iter: 391 loss: 0.000469054678
Iter: 392 loss: 0.000470886938
Iter: 393 loss: 0.000468493119
Iter: 394 loss: 0.000467454898
Iter: 395 loss: 0.00048107261
Iter: 396 loss: 0.000467447389
Iter: 397 loss: 0.000466600352
Iter: 398 loss: 0.00046714372
Iter: 399 loss: 0.000466060068
Iter: 400 loss: 0.000465031044
Iter: 401 loss: 0.000465079444
Iter: 402 loss: 0.000464223034
Iter: 403 loss: 0.000464976
Iter: 404 loss: 0.000463864824
Iter: 405 loss: 0.000463544886
Iter: 406 loss: 0.000464316225
Iter: 407 loss: 0.000463429664
Iter: 408 loss: 0.000463000411
Iter: 409 loss: 0.000463521807
Iter: 410 loss: 0.00046277279
Iter: 411 loss: 0.000462348078
Iter: 412 loss: 0.000461772026
Iter: 413 loss: 0.000461743854
Iter: 414 loss: 0.000461294403
Iter: 415 loss: 0.000461243093
Iter: 416 loss: 0.000460990093
Iter: 417 loss: 0.000460678042
Iter: 418 loss: 0.000460649957
Iter: 419 loss: 0.000460034382
Iter: 420 loss: 0.000459099
Iter: 421 loss: 0.000459082
Iter: 422 loss: 0.000458041206
Iter: 423 loss: 0.000459141738
Iter: 424 loss: 0.000457468035
Iter: 425 loss: 0.000456558453
Iter: 426 loss: 0.000459197152
Iter: 427 loss: 0.000456274865
Iter: 428 loss: 0.000455198635
Iter: 429 loss: 0.000455049769
Iter: 430 loss: 0.000454294321
Iter: 431 loss: 0.000452669861
Iter: 432 loss: 0.000461491465
Iter: 433 loss: 0.000452426379
Iter: 434 loss: 0.000451161643
Iter: 435 loss: 0.000455395901
Iter: 436 loss: 0.000450816413
Iter: 437 loss: 0.000450528431
Iter: 438 loss: 0.000450276537
Iter: 439 loss: 0.000449651503
Iter: 440 loss: 0.000449307379
Iter: 441 loss: 0.000449030369
Iter: 442 loss: 0.000448486157
Iter: 443 loss: 0.000450336433
Iter: 444 loss: 0.000448342238
Iter: 445 loss: 0.000448074657
Iter: 446 loss: 0.000448060309
Iter: 447 loss: 0.000447731058
Iter: 448 loss: 0.000447196653
Iter: 449 loss: 0.000447193044
Iter: 450 loss: 0.000446826714
Iter: 451 loss: 0.000446256774
Iter: 452 loss: 0.000446249614
Iter: 453 loss: 0.000445404148
Iter: 454 loss: 0.000446746941
Iter: 455 loss: 0.000445010839
Iter: 456 loss: 0.000444179954
Iter: 457 loss: 0.000452447741
Iter: 458 loss: 0.000444152858
Iter: 459 loss: 0.000443461933
Iter: 460 loss: 0.000444257
Iter: 461 loss: 0.000443090976
Iter: 462 loss: 0.000442486489
Iter: 463 loss: 0.000445754617
Iter: 464 loss: 0.00044239618
Iter: 465 loss: 0.000441882
Iter: 466 loss: 0.000442078424
Iter: 467 loss: 0.000441523182
Iter: 468 loss: 0.000441108888
Iter: 469 loss: 0.00044739843
Iter: 470 loss: 0.000441107841
Iter: 471 loss: 0.000440805801
Iter: 472 loss: 0.000440803822
Iter: 473 loss: 0.000440657721
Iter: 474 loss: 0.000440233096
Iter: 475 loss: 0.000441753946
Iter: 476 loss: 0.000440043921
Iter: 477 loss: 0.000439798
Iter: 478 loss: 0.000439732161
Iter: 479 loss: 0.000439362309
Iter: 480 loss: 0.000439512776
Iter: 481 loss: 0.000439107942
Iter: 482 loss: 0.000438710675
Iter: 483 loss: 0.000438166782
Iter: 484 loss: 0.000438141404
Iter: 485 loss: 0.000437478826
Iter: 486 loss: 0.000437234208
Iter: 487 loss: 0.000436868635
Iter: 488 loss: 0.000435955561
Iter: 489 loss: 0.000438493153
Iter: 490 loss: 0.000435661321
Iter: 491 loss: 0.000434741058
Iter: 492 loss: 0.000439684431
Iter: 493 loss: 0.000434603135
Iter: 494 loss: 0.00043386675
Iter: 495 loss: 0.000437393435
Iter: 496 loss: 0.000433735957
Iter: 497 loss: 0.000433058362
Iter: 498 loss: 0.000434075366
Iter: 499 loss: 0.0004327356
Iter: 500 loss: 0.00043199939
Iter: 501 loss: 0.000433793524
Iter: 502 loss: 0.000431737921
Iter: 503 loss: 0.000431925931
Iter: 504 loss: 0.000431428751
Iter: 505 loss: 0.000431155175
Iter: 506 loss: 0.000430437329
Iter: 507 loss: 0.00043586074
Iter: 508 loss: 0.000430292857
Iter: 509 loss: 0.000429685111
Iter: 510 loss: 0.000435459282
Iter: 511 loss: 0.000429659674
Iter: 512 loss: 0.000429205771
Iter: 513 loss: 0.000429186912
Iter: 514 loss: 0.000428996835
Iter: 515 loss: 0.00042853452
Iter: 516 loss: 0.000433293113
Iter: 517 loss: 0.00042847998
Iter: 518 loss: 0.000427888968
Iter: 519 loss: 0.000427630555
Iter: 520 loss: 0.000427329884
Iter: 521 loss: 0.000426599698
Iter: 522 loss: 0.000426743238
Iter: 523 loss: 0.000426055281
Iter: 524 loss: 0.000425193342
Iter: 525 loss: 0.000433579669
Iter: 526 loss: 0.000425160746
Iter: 527 loss: 0.000424417056
Iter: 528 loss: 0.000425272505
Iter: 529 loss: 0.000424016383
Iter: 530 loss: 0.000423239137
Iter: 531 loss: 0.000428850675
Iter: 532 loss: 0.000423171121
Iter: 533 loss: 0.00042255118
Iter: 534 loss: 0.000423049962
Iter: 535 loss: 0.000422177429
Iter: 536 loss: 0.000421890232
Iter: 537 loss: 0.000421800651
Iter: 538 loss: 0.000421349134
Iter: 539 loss: 0.000420861237
Iter: 540 loss: 0.000420785829
Iter: 541 loss: 0.000420234806
Iter: 542 loss: 0.000419154501
Iter: 543 loss: 0.000440656731
Iter: 544 loss: 0.000419145887
Iter: 545 loss: 0.000419412914
Iter: 546 loss: 0.000418848183
Iter: 547 loss: 0.000418458541
Iter: 548 loss: 0.000417917618
Iter: 549 loss: 0.000417894858
Iter: 550 loss: 0.000417377887
Iter: 551 loss: 0.000417825562
Iter: 552 loss: 0.000417074538
Iter: 553 loss: 0.000416408817
Iter: 554 loss: 0.00041829393
Iter: 555 loss: 0.000416196883
Iter: 556 loss: 0.000415506453
Iter: 557 loss: 0.000417249015
Iter: 558 loss: 0.000415266084
Iter: 559 loss: 0.00041475019
Iter: 560 loss: 0.000414870388
Iter: 561 loss: 0.000414370676
Iter: 562 loss: 0.000413720263
Iter: 563 loss: 0.000414427894
Iter: 564 loss: 0.000413365749
Iter: 565 loss: 0.000412724126
Iter: 566 loss: 0.000413798116
Iter: 567 loss: 0.000412431429
Iter: 568 loss: 0.000411961868
Iter: 569 loss: 0.000414995884
Iter: 570 loss: 0.00041191373
Iter: 571 loss: 0.000411442976
Iter: 572 loss: 0.000412249647
Iter: 573 loss: 0.000411232875
Iter: 574 loss: 0.000410762092
Iter: 575 loss: 0.000409751694
Iter: 576 loss: 0.000425510807
Iter: 577 loss: 0.000409716857
Iter: 578 loss: 0.000409949978
Iter: 579 loss: 0.000409351051
Iter: 580 loss: 0.000408943568
Iter: 581 loss: 0.000408584659
Iter: 582 loss: 0.000408477237
Iter: 583 loss: 0.000407989137
Iter: 584 loss: 0.000407140818
Iter: 585 loss: 0.000407140964
Iter: 586 loss: 0.000405991246
Iter: 587 loss: 0.000407964253
Iter: 588 loss: 0.000405476429
Iter: 589 loss: 0.000404462655
Iter: 590 loss: 0.000416634546
Iter: 591 loss: 0.000404449354
Iter: 592 loss: 0.000403909595
Iter: 593 loss: 0.000406628445
Iter: 594 loss: 0.000403820683
Iter: 595 loss: 0.000403313374
Iter: 596 loss: 0.000403167214
Iter: 597 loss: 0.000402859645
Iter: 598 loss: 0.000401872298
Iter: 599 loss: 0.000403079262
Iter: 600 loss: 0.000401350611
Iter: 601 loss: 0.000401588273
Iter: 602 loss: 0.000401066529
Iter: 603 loss: 0.000400709
Iter: 604 loss: 0.000400399673
Iter: 605 loss: 0.000400302524
Iter: 606 loss: 0.000399867917
Iter: 607 loss: 0.000400010846
Iter: 608 loss: 0.00039956189
Iter: 609 loss: 0.000398969627
Iter: 610 loss: 0.000398801814
Iter: 611 loss: 0.000398440112
Iter: 612 loss: 0.000398759119
Iter: 613 loss: 0.000398189179
Iter: 614 loss: 0.000397858501
Iter: 615 loss: 0.000397324475
Iter: 616 loss: 0.000397320895
Iter: 617 loss: 0.000396846561
Iter: 618 loss: 0.000397064141
Iter: 619 loss: 0.000396522955
Iter: 620 loss: 0.000395823386
Iter: 621 loss: 0.000401436409
Iter: 622 loss: 0.000395777752
Iter: 623 loss: 0.000395139912
Iter: 624 loss: 0.00039781
Iter: 625 loss: 0.000395003357
Iter: 626 loss: 0.000394422212
Iter: 627 loss: 0.000395216048
Iter: 628 loss: 0.000394130766
Iter: 629 loss: 0.000393274706
Iter: 630 loss: 0.00039488895
Iter: 631 loss: 0.000392911257
Iter: 632 loss: 0.000392085203
Iter: 633 loss: 0.000396152522
Iter: 634 loss: 0.000391939306
Iter: 635 loss: 0.00039142184
Iter: 636 loss: 0.000391363865
Iter: 637 loss: 0.000391102862
Iter: 638 loss: 0.000390394533
Iter: 639 loss: 0.000394969364
Iter: 640 loss: 0.000390215806
Iter: 641 loss: 0.000389296154
Iter: 642 loss: 0.000391126028
Iter: 643 loss: 0.000388920045
Iter: 644 loss: 0.000388233573
Iter: 645 loss: 0.000398206874
Iter: 646 loss: 0.000388231769
Iter: 647 loss: 0.000387617212
Iter: 648 loss: 0.000394712639
Iter: 649 loss: 0.000387606735
Iter: 650 loss: 0.000387344218
Iter: 651 loss: 0.000386858039
Iter: 652 loss: 0.000398064527
Iter: 653 loss: 0.00038685766
Iter: 654 loss: 0.000386236381
Iter: 655 loss: 0.000386083469
Iter: 656 loss: 0.000385685707
Iter: 657 loss: 0.000384926156
Iter: 658 loss: 0.000386786327
Iter: 659 loss: 0.000384654646
Iter: 660 loss: 0.000384042447
Iter: 661 loss: 0.000390284287
Iter: 662 loss: 0.000384023588
Iter: 663 loss: 0.000383447768
Iter: 664 loss: 0.000385447114
Iter: 665 loss: 0.000383295846
Iter: 666 loss: 0.000382869039
Iter: 667 loss: 0.000383779232
Iter: 668 loss: 0.000382702186
Iter: 669 loss: 0.000382537313
Iter: 670 loss: 0.000382450584
Iter: 671 loss: 0.000382179773
Iter: 672 loss: 0.000381882448
Iter: 673 loss: 0.000381837919
Iter: 674 loss: 0.000381508231
Iter: 675 loss: 0.000381186372
Iter: 676 loss: 0.000381114602
Iter: 677 loss: 0.000380327576
Iter: 678 loss: 0.000387803913
Iter: 679 loss: 0.000380294368
Iter: 680 loss: 0.00038100715
Iter: 681 loss: 0.000379981444
Iter: 682 loss: 0.000379759731
Iter: 683 loss: 0.000379838224
Iter: 684 loss: 0.000379604171
Iter: 685 loss: 0.000379105448
Iter: 686 loss: 0.00038032289
Iter: 687 loss: 0.000378928235
Iter: 688 loss: 0.000378253404
Iter: 689 loss: 0.000380084501
Iter: 690 loss: 0.000378030585
Iter: 691 loss: 0.000377366203
Iter: 692 loss: 0.000378266443
Iter: 693 loss: 0.000377031916
Iter: 694 loss: 0.000376275188
Iter: 695 loss: 0.000377950753
Iter: 696 loss: 0.00037598511
Iter: 697 loss: 0.000375239295
Iter: 698 loss: 0.000377849705
Iter: 699 loss: 0.000375046628
Iter: 700 loss: 0.00037474191
Iter: 701 loss: 0.000374683324
Iter: 702 loss: 0.000374334
Iter: 703 loss: 0.000375957665
Iter: 704 loss: 0.000374269439
Iter: 705 loss: 0.00037397226
Iter: 706 loss: 0.000373462244
Iter: 707 loss: 0.000373462331
Iter: 708 loss: 0.000372850656
Iter: 709 loss: 0.000374562806
Iter: 710 loss: 0.000372654322
Iter: 711 loss: 0.000372882088
Iter: 712 loss: 0.000372411101
Iter: 713 loss: 0.000372128532
Iter: 714 loss: 0.000371786096
Iter: 715 loss: 0.000371753355
Iter: 716 loss: 0.000371437491
Iter: 717 loss: 0.000370575115
Iter: 718 loss: 0.000375864503
Iter: 719 loss: 0.000370342867
Iter: 720 loss: 0.000369263173
Iter: 721 loss: 0.00037244003
Iter: 722 loss: 0.000368930574
Iter: 723 loss: 0.000368129316
Iter: 724 loss: 0.000378982
Iter: 725 loss: 0.000368125562
Iter: 726 loss: 0.000367573
Iter: 727 loss: 0.000367294298
Iter: 728 loss: 0.000367033674
Iter: 729 loss: 0.000366179331
Iter: 730 loss: 0.000370089139
Iter: 731 loss: 0.000366016291
Iter: 732 loss: 0.000365349289
Iter: 733 loss: 0.000366448046
Iter: 734 loss: 0.000365044572
Iter: 735 loss: 0.000365090789
Iter: 736 loss: 0.000364765583
Iter: 737 loss: 0.000364443142
Iter: 738 loss: 0.000364145613
Iter: 739 loss: 0.000364066713
Iter: 740 loss: 0.000363736559
Iter: 741 loss: 0.000363822677
Iter: 742 loss: 0.000363496481
Iter: 743 loss: 0.000362990948
Iter: 744 loss: 0.000363429514
Iter: 745 loss: 0.000362695457
Iter: 746 loss: 0.000362891675
Iter: 747 loss: 0.000362451829
Iter: 748 loss: 0.000362240127
Iter: 749 loss: 0.000361705752
Iter: 750 loss: 0.000366576947
Iter: 751 loss: 0.000361625396
Iter: 752 loss: 0.000361092272
Iter: 753 loss: 0.000361440529
Iter: 754 loss: 0.000360754377
Iter: 755 loss: 0.0003602652
Iter: 756 loss: 0.000361283659
Iter: 757 loss: 0.000360071455
Iter: 758 loss: 0.00035959974
Iter: 759 loss: 0.000361146842
Iter: 760 loss: 0.000359468453
Iter: 761 loss: 0.000358989666
Iter: 762 loss: 0.000359283062
Iter: 763 loss: 0.000358682766
Iter: 764 loss: 0.000358109828
Iter: 765 loss: 0.000360902515
Iter: 766 loss: 0.00035800878
Iter: 767 loss: 0.000357542682
Iter: 768 loss: 0.00035956534
Iter: 769 loss: 0.000357447949
Iter: 770 loss: 0.000357314566
Iter: 771 loss: 0.000357231387
Iter: 772 loss: 0.000357040932
Iter: 773 loss: 0.000356610137
Iter: 774 loss: 0.000362338062
Iter: 775 loss: 0.000356583419
Iter: 776 loss: 0.000356023374
Iter: 777 loss: 0.000357264245
Iter: 778 loss: 0.000355809
Iter: 779 loss: 0.000355414697
Iter: 780 loss: 0.000355397555
Iter: 781 loss: 0.000354946067
Iter: 782 loss: 0.000356375647
Iter: 783 loss: 0.000354814925
Iter: 784 loss: 0.000354575051
Iter: 785 loss: 0.000353954558
Iter: 786 loss: 0.000358953374
Iter: 787 loss: 0.000353840616
Iter: 788 loss: 0.000353162439
Iter: 789 loss: 0.000356172211
Iter: 790 loss: 0.000353029231
Iter: 791 loss: 0.000352521602
Iter: 792 loss: 0.000354084244
Iter: 793 loss: 0.000352372415
Iter: 794 loss: 0.000351853902
Iter: 795 loss: 0.000351941271
Iter: 796 loss: 0.000351464725
Iter: 797 loss: 0.000350810704
Iter: 798 loss: 0.000353602984
Iter: 799 loss: 0.000350674614
Iter: 800 loss: 0.000349936425
Iter: 801 loss: 0.0003525301
Iter: 802 loss: 0.000349745329
Iter: 803 loss: 0.000349454815
Iter: 804 loss: 0.000349416659
Iter: 805 loss: 0.000349094626
Iter: 806 loss: 0.000349979935
Iter: 807 loss: 0.000348989794
Iter: 808 loss: 0.00034870638
Iter: 809 loss: 0.000348113797
Iter: 810 loss: 0.000358115911
Iter: 811 loss: 0.000348098984
Iter: 812 loss: 0.000347652356
Iter: 813 loss: 0.000351508643
Iter: 814 loss: 0.000347629073
Iter: 815 loss: 0.000347481458
Iter: 816 loss: 0.000347375288
Iter: 817 loss: 0.000347231398
Iter: 818 loss: 0.000346855493
Iter: 819 loss: 0.000349751062
Iter: 820 loss: 0.00034678122
Iter: 821 loss: 0.000346383109
Iter: 822 loss: 0.000347267312
Iter: 823 loss: 0.00034623011
Iter: 824 loss: 0.000345838518
Iter: 825 loss: 0.000346562534
Iter: 826 loss: 0.000345669774
Iter: 827 loss: 0.000345198205
Iter: 828 loss: 0.000345694425
Iter: 829 loss: 0.000344936823
Iter: 830 loss: 0.000344402855
Iter: 831 loss: 0.000346966728
Iter: 832 loss: 0.000344308064
Iter: 833 loss: 0.000343849824
Iter: 834 loss: 0.000343808701
Iter: 835 loss: 0.000343471562
Iter: 836 loss: 0.000342935906
Iter: 837 loss: 0.000348871952
Iter: 838 loss: 0.000342925428
Iter: 839 loss: 0.0003426279
Iter: 840 loss: 0.000342610059
Iter: 841 loss: 0.000342389976
Iter: 842 loss: 0.000342159881
Iter: 843 loss: 0.00034211934
Iter: 844 loss: 0.000341813138
Iter: 845 loss: 0.000341517734
Iter: 846 loss: 0.000341449369
Iter: 847 loss: 0.000341454346
Iter: 848 loss: 0.000341255072
Iter: 849 loss: 0.000341077044
Iter: 850 loss: 0.000340770173
Iter: 851 loss: 0.000340769533
Iter: 852 loss: 0.000340303523
Iter: 853 loss: 0.00034020876
Iter: 854 loss: 0.000339898164
Iter: 855 loss: 0.00033930529
Iter: 856 loss: 0.000344235101
Iter: 857 loss: 0.000339269754
Iter: 858 loss: 0.000338711252
Iter: 859 loss: 0.000340557046
Iter: 860 loss: 0.000338557147
Iter: 861 loss: 0.000338145386
Iter: 862 loss: 0.000339119579
Iter: 863 loss: 0.000337993726
Iter: 864 loss: 0.000337531732
Iter: 865 loss: 0.000337820617
Iter: 866 loss: 0.000337236794
Iter: 867 loss: 0.000336689816
Iter: 868 loss: 0.000338479353
Iter: 869 loss: 0.000336537516
Iter: 870 loss: 0.000336144032
Iter: 871 loss: 0.00033614377
Iter: 872 loss: 0.000335788325
Iter: 873 loss: 0.000338527258
Iter: 874 loss: 0.000335762888
Iter: 875 loss: 0.000335563149
Iter: 876 loss: 0.00033535
Iter: 877 loss: 0.000335315184
Iter: 878 loss: 0.000334981683
Iter: 879 loss: 0.000337082834
Iter: 880 loss: 0.000334944169
Iter: 881 loss: 0.000334576354
Iter: 882 loss: 0.000336521363
Iter: 883 loss: 0.000334518263
Iter: 884 loss: 0.000334275072
Iter: 885 loss: 0.000333768665
Iter: 886 loss: 0.000342538464
Iter: 887 loss: 0.00033375682
Iter: 888 loss: 0.000333283417
Iter: 889 loss: 0.000334472861
Iter: 890 loss: 0.000333118427
Iter: 891 loss: 0.000332634547
Iter: 892 loss: 0.0003335457
Iter: 893 loss: 0.000332430296
Iter: 894 loss: 0.000331927091
Iter: 895 loss: 0.000335694029
Iter: 896 loss: 0.000331887335
Iter: 897 loss: 0.00033144
Iter: 898 loss: 0.000332008407
Iter: 899 loss: 0.000331208226
Iter: 900 loss: 0.000330735085
Iter: 901 loss: 0.000331834715
Iter: 902 loss: 0.000330560346
Iter: 903 loss: 0.000330034818
Iter: 904 loss: 0.000330064388
Iter: 905 loss: 0.000329624047
Iter: 906 loss: 0.000329001748
Iter: 907 loss: 0.000334183627
Iter: 908 loss: 0.000328964059
Iter: 909 loss: 0.000328925671
Iter: 910 loss: 0.000328720082
Iter: 911 loss: 0.000328595925
Iter: 912 loss: 0.000328378868
Iter: 913 loss: 0.00032837881
Iter: 914 loss: 0.000328082941
Iter: 915 loss: 0.000330861396
Iter: 916 loss: 0.000328072027
Iter: 917 loss: 0.000327869464
Iter: 918 loss: 0.000327764516
Iter: 919 loss: 0.000327670481
Iter: 920 loss: 0.000327335205
Iter: 921 loss: 0.00032672903
Iter: 922 loss: 0.000341335137
Iter: 923 loss: 0.000326728565
Iter: 924 loss: 0.000326087931
Iter: 925 loss: 0.000330191746
Iter: 926 loss: 0.000326015754
Iter: 927 loss: 0.000325511966
Iter: 928 loss: 0.000326866517
Iter: 929 loss: 0.000325344328
Iter: 930 loss: 0.000324783585
Iter: 931 loss: 0.000326429639
Iter: 932 loss: 0.000324610737
Iter: 933 loss: 0.000324094493
Iter: 934 loss: 0.000324874272
Iter: 935 loss: 0.000323848566
Iter: 936 loss: 0.000323213695
Iter: 937 loss: 0.000326843874
Iter: 938 loss: 0.000323127315
Iter: 939 loss: 0.000322646636
Iter: 940 loss: 0.000322918233
Iter: 941 loss: 0.000322333071
Iter: 942 loss: 0.000322336855
Iter: 943 loss: 0.000322131906
Iter: 944 loss: 0.000321920583
Iter: 945 loss: 0.000321713858
Iter: 946 loss: 0.000321668631
Iter: 947 loss: 0.000321435975
Iter: 948 loss: 0.000321435858
Iter: 949 loss: 0.00032125751
Iter: 950 loss: 0.000321075233
Iter: 951 loss: 0.000321040628
Iter: 952 loss: 0.000320763676
Iter: 953 loss: 0.000320669351
Iter: 954 loss: 0.000320511288
Iter: 955 loss: 0.000320106396
Iter: 956 loss: 0.000320413092
Iter: 957 loss: 0.000319859595
Iter: 958 loss: 0.000319418439
Iter: 959 loss: 0.000319589541
Iter: 960 loss: 0.000319112034
Iter: 961 loss: 0.000318515784
Iter: 962 loss: 0.000321613887
Iter: 963 loss: 0.000318421866
Iter: 964 loss: 0.000317912782
Iter: 965 loss: 0.000319874
Iter: 966 loss: 0.000317793165
Iter: 967 loss: 0.000317289465
Iter: 968 loss: 0.000318006321
Iter: 969 loss: 0.000317044265
Iter: 970 loss: 0.000316526101
Iter: 971 loss: 0.000317770871
Iter: 972 loss: 0.000316340243
Iter: 973 loss: 0.000315865909
Iter: 974 loss: 0.000318841951
Iter: 975 loss: 0.000315812358
Iter: 976 loss: 0.000315616315
Iter: 977 loss: 0.000315564947
Iter: 978 loss: 0.000315413636
Iter: 979 loss: 0.000315242389
Iter: 980 loss: 0.000315220881
Iter: 981 loss: 0.000314885227
Iter: 982 loss: 0.000316267135
Iter: 983 loss: 0.000314811041
Iter: 984 loss: 0.000314636214
Iter: 985 loss: 0.000314439414
Iter: 986 loss: 0.000314412842
Iter: 987 loss: 0.000314050587
Iter: 988 loss: 0.000314637728
Iter: 989 loss: 0.000313883065
Iter: 990 loss: 0.000313537574
Iter: 991 loss: 0.000313389406
Iter: 992 loss: 0.000313211087
Iter: 993 loss: 0.000312729855
Iter: 994 loss: 0.000314059202
Iter: 995 loss: 0.000312572811
Iter: 996 loss: 0.000312036078
Iter: 997 loss: 0.000313220546
Iter: 998 loss: 0.000311831274
Iter: 999 loss: 0.000311340962
Iter: 1000 loss: 0.000315711601
Iter: 1001 loss: 0.000311318116
Iter: 1002 loss: 0.000310935313
Iter: 1003 loss: 0.000311027456
Iter: 1004 loss: 0.000310657138
Iter: 1005 loss: 0.00031016409
Iter: 1006 loss: 0.000313442521
Iter: 1007 loss: 0.000310113828
Iter: 1008 loss: 0.000310212781
Iter: 1009 loss: 0.000309953175
Iter: 1010 loss: 0.000309839321
Iter: 1011 loss: 0.000309671974
Iter: 1012 loss: 0.000309667405
Iter: 1013 loss: 0.000309443916
Iter: 1014 loss: 0.000310803356
Iter: 1015 loss: 0.000309417926
Iter: 1016 loss: 0.000309280324
Iter: 1017 loss: 0.000308983144
Iter: 1018 loss: 0.000313515018
Iter: 1019 loss: 0.000308971299
Iter: 1020 loss: 0.000308540504
Iter: 1021 loss: 0.00031038592
Iter: 1022 loss: 0.00030845165
Iter: 1023 loss: 0.000308092218
Iter: 1024 loss: 0.00030983286
Iter: 1025 loss: 0.00030803046
Iter: 1026 loss: 0.000307751703
Iter: 1027 loss: 0.000307339651
Iter: 1028 loss: 0.000307329174
Iter: 1029 loss: 0.000306798262
Iter: 1030 loss: 0.000308205083
Iter: 1031 loss: 0.000306620204
Iter: 1032 loss: 0.000306049042
Iter: 1033 loss: 0.00030809053
Iter: 1034 loss: 0.000305903523
Iter: 1035 loss: 0.000305378839
Iter: 1036 loss: 0.000309085677
Iter: 1037 loss: 0.000305331952
Iter: 1038 loss: 0.00030491763
Iter: 1039 loss: 0.000306432
Iter: 1040 loss: 0.000304814894
Iter: 1041 loss: 0.000304663205
Iter: 1042 loss: 0.00030465459
Iter: 1043 loss: 0.000304470654
Iter: 1044 loss: 0.000304317888
Iter: 1045 loss: 0.000304265588
Iter: 1046 loss: 0.000304010639
Iter: 1047 loss: 0.000306527771
Iter: 1048 loss: 0.00030400086
Iter: 1049 loss: 0.000303836481
Iter: 1050 loss: 0.00030353191
Iter: 1051 loss: 0.00031048333
Iter: 1052 loss: 0.000303531473
Iter: 1053 loss: 0.000303186738
Iter: 1054 loss: 0.000303756853
Iter: 1055 loss: 0.000303029519
Iter: 1056 loss: 0.000302687811
Iter: 1057 loss: 0.000305096502
Iter: 1058 loss: 0.000302657136
Iter: 1059 loss: 0.000302379485
Iter: 1060 loss: 0.000302245491
Iter: 1061 loss: 0.000302111905
Iter: 1062 loss: 0.000301754917
Iter: 1063 loss: 0.00030281971
Iter: 1064 loss: 0.00030164796
Iter: 1065 loss: 0.000301215
Iter: 1066 loss: 0.000301539
Iter: 1067 loss: 0.000300950021
Iter: 1068 loss: 0.000300469721
Iter: 1069 loss: 0.000302422384
Iter: 1070 loss: 0.000300363812
Iter: 1071 loss: 0.00030001963
Iter: 1072 loss: 0.000304777117
Iter: 1073 loss: 0.000300018815
Iter: 1074 loss: 0.000299800042
Iter: 1075 loss: 0.000300940417
Iter: 1076 loss: 0.000299766107
Iter: 1077 loss: 0.000299501349
Iter: 1078 loss: 0.000300480577
Iter: 1079 loss: 0.000299435458
Iter: 1080 loss: 0.000299281732
Iter: 1081 loss: 0.00029993779
Iter: 1082 loss: 0.000299249659
Iter: 1083 loss: 0.000299084524
Iter: 1084 loss: 0.000298779109
Iter: 1085 loss: 0.000305784313
Iter: 1086 loss: 0.000298778672
Iter: 1087 loss: 0.000298476691
Iter: 1088 loss: 0.000299243489
Iter: 1089 loss: 0.000298372906
Iter: 1090 loss: 0.000298069179
Iter: 1091 loss: 0.000299335574
Iter: 1092 loss: 0.000298003899
Iter: 1093 loss: 0.000297705934
Iter: 1094 loss: 0.000297849299
Iter: 1095 loss: 0.000297507097
Iter: 1096 loss: 0.000297140214
Iter: 1097 loss: 0.000297608407
Iter: 1098 loss: 0.00029695296
Iter: 1099 loss: 0.000296550686
Iter: 1100 loss: 0.000297430379
Iter: 1101 loss: 0.000296395621
Iter: 1102 loss: 0.000295989186
Iter: 1103 loss: 0.000297815859
Iter: 1104 loss: 0.000295909907
Iter: 1105 loss: 0.000295537931
Iter: 1106 loss: 0.000297246617
Iter: 1107 loss: 0.000295468431
Iter: 1108 loss: 0.000295241
Iter: 1109 loss: 0.000295239326
Iter: 1110 loss: 0.000295066508
Iter: 1111 loss: 0.000296043872
Iter: 1112 loss: 0.000295044098
Iter: 1113 loss: 0.000294920057
Iter: 1114 loss: 0.000295025384
Iter: 1115 loss: 0.000294847
Iter: 1116 loss: 0.00029465821
Iter: 1117 loss: 0.000294461963
Iter: 1118 loss: 0.000294426427
Iter: 1119 loss: 0.000294195983
Iter: 1120 loss: 0.000294318859
Iter: 1121 loss: 0.000294044381
Iter: 1122 loss: 0.000293742196
Iter: 1123 loss: 0.00029540126
Iter: 1124 loss: 0.000293698395
Iter: 1125 loss: 0.000293414516
Iter: 1126 loss: 0.000293602236
Iter: 1127 loss: 0.000293235469
Iter: 1128 loss: 0.000292897341
Iter: 1129 loss: 0.000293494464
Iter: 1130 loss: 0.00029275025
Iter: 1131 loss: 0.000292398239
Iter: 1132 loss: 0.000293061952
Iter: 1133 loss: 0.00029224914
Iter: 1134 loss: 0.00029184384
Iter: 1135 loss: 0.000292734418
Iter: 1136 loss: 0.000291687727
Iter: 1137 loss: 0.000291306758
Iter: 1138 loss: 0.000294099562
Iter: 1139 loss: 0.000291273638
Iter: 1140 loss: 0.0002910404
Iter: 1141 loss: 0.000291040371
Iter: 1142 loss: 0.000290862517
Iter: 1143 loss: 0.000291940494
Iter: 1144 loss: 0.000290840922
Iter: 1145 loss: 0.000290702446
Iter: 1146 loss: 0.000290696276
Iter: 1147 loss: 0.000290591153
Iter: 1148 loss: 0.00029034607
Iter: 1149 loss: 0.000290597149
Iter: 1150 loss: 0.000290209253
Iter: 1151 loss: 0.00029002357
Iter: 1152 loss: 0.000289889693
Iter: 1153 loss: 0.000289824733
Iter: 1154 loss: 0.000289525429
Iter: 1155 loss: 0.000290526135
Iter: 1156 loss: 0.000289442716
Iter: 1157 loss: 0.00028912141
Iter: 1158 loss: 0.000290360098
Iter: 1159 loss: 0.000289045187
Iter: 1160 loss: 0.000288792886
Iter: 1161 loss: 0.000288724
Iter: 1162 loss: 0.000288569805
Iter: 1163 loss: 0.000288212759
Iter: 1164 loss: 0.000289492804
Iter: 1165 loss: 0.000288122799
Iter: 1166 loss: 0.000287785428
Iter: 1167 loss: 0.000288473588
Iter: 1168 loss: 0.000287649716
Iter: 1169 loss: 0.000287304312
Iter: 1170 loss: 0.000288664
Iter: 1171 loss: 0.000287226168
Iter: 1172 loss: 0.000287077593
Iter: 1173 loss: 0.000287050352
Iter: 1174 loss: 0.000286911731
Iter: 1175 loss: 0.000287475763
Iter: 1176 loss: 0.000286881463
Iter: 1177 loss: 0.00028674613
Iter: 1178 loss: 0.00028684744
Iter: 1179 loss: 0.000286663
Iter: 1180 loss: 0.00028648664
Iter: 1181 loss: 0.000286793656
Iter: 1182 loss: 0.000286407827
Iter: 1183 loss: 0.000286249357
Iter: 1184 loss: 0.000286015391
Iter: 1185 loss: 0.000286010443
Iter: 1186 loss: 0.000285701244
Iter: 1187 loss: 0.000287034403
Iter: 1188 loss: 0.00028563774
Iter: 1189 loss: 0.00028532758
Iter: 1190 loss: 0.000286530762
Iter: 1191 loss: 0.00028525418
Iter: 1192 loss: 0.000284974056
Iter: 1193 loss: 0.000285012618
Iter: 1194 loss: 0.000284762529
Iter: 1195 loss: 0.000284412497
Iter: 1196 loss: 0.000285285874
Iter: 1197 loss: 0.000284289097
Iter: 1198 loss: 0.000283917703
Iter: 1199 loss: 0.000284509
Iter: 1200 loss: 0.000283746485
Iter: 1201 loss: 0.00028335434
Iter: 1202 loss: 0.000285443733
Iter: 1203 loss: 0.000283292233
Iter: 1204 loss: 0.000283085275
Iter: 1205 loss: 0.000283083529
Iter: 1206 loss: 0.000282898633
Iter: 1207 loss: 0.000284024019
Iter: 1208 loss: 0.000282874971
Iter: 1209 loss: 0.000282720488
Iter: 1210 loss: 0.000282829831
Iter: 1211 loss: 0.000282623805
Iter: 1212 loss: 0.000282432185
Iter: 1213 loss: 0.000282969675
Iter: 1214 loss: 0.000282370806
Iter: 1215 loss: 0.000282213528
Iter: 1216 loss: 0.00028198777
Iter: 1217 loss: 0.000281981309
Iter: 1218 loss: 0.000281710178
Iter: 1219 loss: 0.000282479479
Iter: 1220 loss: 0.000281623623
Iter: 1221 loss: 0.000281365938
Iter: 1222 loss: 0.000283588422
Iter: 1223 loss: 0.000281351589
Iter: 1224 loss: 0.000281163258
Iter: 1225 loss: 0.000281041954
Iter: 1226 loss: 0.000280969194
Iter: 1227 loss: 0.000280672597
Iter: 1228 loss: 0.000281533808
Iter: 1229 loss: 0.000280579901
Iter: 1230 loss: 0.000280274311
Iter: 1231 loss: 0.00028064521
Iter: 1232 loss: 0.000280112232
Iter: 1233 loss: 0.000279783388
Iter: 1234 loss: 0.000281179557
Iter: 1235 loss: 0.000279714586
Iter: 1236 loss: 0.000279452215
Iter: 1237 loss: 0.000282207679
Iter: 1238 loss: 0.000279445725
Iter: 1239 loss: 0.00027924747
Iter: 1240 loss: 0.000281971
Iter: 1241 loss: 0.000279245956
Iter: 1242 loss: 0.000279115338
Iter: 1243 loss: 0.00027920198
Iter: 1244 loss: 0.000279033673
Iter: 1245 loss: 0.00027887008
Iter: 1246 loss: 0.00027926921
Iter: 1247 loss: 0.000278812309
Iter: 1248 loss: 0.000278636377
Iter: 1249 loss: 0.000278441294
Iter: 1250 loss: 0.000278412946
Iter: 1251 loss: 0.000278154359
Iter: 1252 loss: 0.000278481923
Iter: 1253 loss: 0.000278021267
Iter: 1254 loss: 0.000277742074
Iter: 1255 loss: 0.000280355162
Iter: 1256 loss: 0.000277729734
Iter: 1257 loss: 0.000277518237
Iter: 1258 loss: 0.000277587
Iter: 1259 loss: 0.000277367304
Iter: 1260 loss: 0.000277113169
Iter: 1261 loss: 0.000277512474
Iter: 1262 loss: 0.00027699396
Iter: 1263 loss: 0.000276710052
Iter: 1264 loss: 0.000277235347
Iter: 1265 loss: 0.000276587612
Iter: 1266 loss: 0.000276306586
Iter: 1267 loss: 0.000277817191
Iter: 1268 loss: 0.000276262057
Iter: 1269 loss: 0.000276034727
Iter: 1270 loss: 0.000277013
Iter: 1271 loss: 0.00027598755
Iter: 1272 loss: 0.000275862258
Iter: 1273 loss: 0.000275840051
Iter: 1274 loss: 0.000275738188
Iter: 1275 loss: 0.000275646948
Iter: 1276 loss: 0.00027562154
Iter: 1277 loss: 0.000275418628
Iter: 1278 loss: 0.00027589564
Iter: 1279 loss: 0.000275343948
Iter: 1280 loss: 0.000275131024
Iter: 1281 loss: 0.000275015918
Iter: 1282 loss: 0.000274920894
Iter: 1283 loss: 0.000274639
Iter: 1284 loss: 0.000274965423
Iter: 1285 loss: 0.000274487888
Iter: 1286 loss: 0.000274203398
Iter: 1287 loss: 0.000276389474
Iter: 1288 loss: 0.000274182588
Iter: 1289 loss: 0.000273915648
Iter: 1290 loss: 0.000274194375
Iter: 1291 loss: 0.000273767189
Iter: 1292 loss: 0.000273490674
Iter: 1293 loss: 0.000274123158
Iter: 1294 loss: 0.000273387413
Iter: 1295 loss: 0.000273118028
Iter: 1296 loss: 0.000273543876
Iter: 1297 loss: 0.000272992824
Iter: 1298 loss: 0.000272690086
Iter: 1299 loss: 0.000273336598
Iter: 1300 loss: 0.000272571546
Iter: 1301 loss: 0.000272257195
Iter: 1302 loss: 0.000273937476
Iter: 1303 loss: 0.000272208883
Iter: 1304 loss: 0.000272205682
Iter: 1305 loss: 0.000272091711
Iter: 1306 loss: 0.000272005855
Iter: 1307 loss: 0.000271810801
Iter: 1308 loss: 0.000274329563
Iter: 1309 loss: 0.000271797471
Iter: 1310 loss: 0.000271513651
Iter: 1311 loss: 0.000274055055
Iter: 1312 loss: 0.000271500088
Iter: 1313 loss: 0.000271262601
Iter: 1314 loss: 0.000271317782
Iter: 1315 loss: 0.000271089084
Iter: 1316 loss: 0.000270777266
Iter: 1317 loss: 0.000270511868
Iter: 1318 loss: 0.000270425342
Iter: 1319 loss: 0.000269977609
Iter: 1320 loss: 0.000274251215
Iter: 1321 loss: 0.000269958953
Iter: 1322 loss: 0.000269606331
Iter: 1323 loss: 0.000271240773
Iter: 1324 loss: 0.000269540935
Iter: 1325 loss: 0.000269269804
Iter: 1326 loss: 0.000269306242
Iter: 1327 loss: 0.000269062875
Iter: 1328 loss: 0.000268665142
Iter: 1329 loss: 0.000268550619
Iter: 1330 loss: 0.000268309494
Iter: 1331 loss: 0.0002676312
Iter: 1332 loss: 0.000269762822
Iter: 1333 loss: 0.000267434953
Iter: 1334 loss: 0.000266784977
Iter: 1335 loss: 0.0002714273
Iter: 1336 loss: 0.000266729563
Iter: 1337 loss: 0.000266587245
Iter: 1338 loss: 0.000266489806
Iter: 1339 loss: 0.000266273681
Iter: 1340 loss: 0.00026605028
Iter: 1341 loss: 0.00026600936
Iter: 1342 loss: 0.000265704934
Iter: 1343 loss: 0.00026798356
Iter: 1344 loss: 0.000265681796
Iter: 1345 loss: 0.000265416806
Iter: 1346 loss: 0.000266425632
Iter: 1347 loss: 0.00026535365
Iter: 1348 loss: 0.000265149924
Iter: 1349 loss: 0.000264803355
Iter: 1350 loss: 0.00026480321
Iter: 1351 loss: 0.000264506642
Iter: 1352 loss: 0.000266478921
Iter: 1353 loss: 0.000264476606
Iter: 1354 loss: 0.000264271512
Iter: 1355 loss: 0.000265667913
Iter: 1356 loss: 0.000264252099
Iter: 1357 loss: 0.000264070695
Iter: 1358 loss: 0.000264053408
Iter: 1359 loss: 0.000263919705
Iter: 1360 loss: 0.000263641
Iter: 1361 loss: 0.000264514412
Iter: 1362 loss: 0.000263560272
Iter: 1363 loss: 0.000263353752
Iter: 1364 loss: 0.000263703871
Iter: 1365 loss: 0.00026326158
Iter: 1366 loss: 0.000263058784
Iter: 1367 loss: 0.000264153176
Iter: 1368 loss: 0.00026302773
Iter: 1369 loss: 0.000262929418
Iter: 1370 loss: 0.000262926
Iter: 1371 loss: 0.000262799294
Iter: 1372 loss: 0.000262655289
Iter: 1373 loss: 0.000262637972
Iter: 1374 loss: 0.00026245616
Iter: 1375 loss: 0.000263204973
Iter: 1376 loss: 0.000262416841
Iter: 1377 loss: 0.00026224961
Iter: 1378 loss: 0.000262712798
Iter: 1379 loss: 0.000262194575
Iter: 1380 loss: 0.000262021495
Iter: 1381 loss: 0.000261932204
Iter: 1382 loss: 0.000261851674
Iter: 1383 loss: 0.000261656765
Iter: 1384 loss: 0.000261790294
Iter: 1385 loss: 0.000261534296
Iter: 1386 loss: 0.000261237612
Iter: 1387 loss: 0.000262179528
Iter: 1388 loss: 0.000261152774
Iter: 1389 loss: 0.000260817294
Iter: 1390 loss: 0.000261346606
Iter: 1391 loss: 0.000260659726
Iter: 1392 loss: 0.00026036214
Iter: 1393 loss: 0.000261703797
Iter: 1394 loss: 0.000260304048
Iter: 1395 loss: 0.000260044471
Iter: 1396 loss: 0.000260044821
Iter: 1397 loss: 0.000259836728
Iter: 1398 loss: 0.000259551278
Iter: 1399 loss: 0.000262197806
Iter: 1400 loss: 0.000259538647
Iter: 1401 loss: 0.000259377732
Iter: 1402 loss: 0.000261865091
Iter: 1403 loss: 0.000259377033
Iter: 1404 loss: 0.000259207445
Iter: 1405 loss: 0.000259590452
Iter: 1406 loss: 0.000259142893
Iter: 1407 loss: 0.000259035412
Iter: 1408 loss: 0.000259129913
Iter: 1409 loss: 0.000258972606
Iter: 1410 loss: 0.000258827553
Iter: 1411 loss: 0.000259320543
Iter: 1412 loss: 0.000258789805
Iter: 1413 loss: 0.000258644402
Iter: 1414 loss: 0.000258613756
Iter: 1415 loss: 0.000258518034
Iter: 1416 loss: 0.000258332322
Iter: 1417 loss: 0.000258295215
Iter: 1418 loss: 0.000258171902
Iter: 1419 loss: 0.00025794073
Iter: 1420 loss: 0.000259989116
Iter: 1421 loss: 0.000257928565
Iter: 1422 loss: 0.000257746375
Iter: 1423 loss: 0.000258133834
Iter: 1424 loss: 0.000257675
Iter: 1425 loss: 0.000257487089
Iter: 1426 loss: 0.00025755461
Iter: 1427 loss: 0.000257356
Iter: 1428 loss: 0.000257085077
Iter: 1429 loss: 0.000257712847
Iter: 1430 loss: 0.000256985775
Iter: 1431 loss: 0.00025675763
Iter: 1432 loss: 0.00025759748
Iter: 1433 loss: 0.0002567021
Iter: 1434 loss: 0.00025652
Iter: 1435 loss: 0.000258922082
Iter: 1436 loss: 0.000256519357
Iter: 1437 loss: 0.000256352942
Iter: 1438 loss: 0.000257518375
Iter: 1439 loss: 0.000256337138
Iter: 1440 loss: 0.000256251951
Iter: 1441 loss: 0.000256079482
Iter: 1442 loss: 0.000259209774
Iter: 1443 loss: 0.000256075815
Iter: 1444 loss: 0.000255846651
Iter: 1445 loss: 0.000257844396
Iter: 1446 loss: 0.00025583402
Iter: 1447 loss: 0.0002556658
Iter: 1448 loss: 0.00025576094
Iter: 1449 loss: 0.00025555637
Iter: 1450 loss: 0.000255378167
Iter: 1451 loss: 0.000255284016
Iter: 1452 loss: 0.000255202031
Iter: 1453 loss: 0.000254978193
Iter: 1454 loss: 0.000256240979
Iter: 1455 loss: 0.000254946557
Iter: 1456 loss: 0.000254737213
Iter: 1457 loss: 0.000255231542
Iter: 1458 loss: 0.000254659768
Iter: 1459 loss: 0.000254431798
Iter: 1460 loss: 0.000254608312
Iter: 1461 loss: 0.00025429207
Iter: 1462 loss: 0.000254022365
Iter: 1463 loss: 0.000255101593
Iter: 1464 loss: 0.000253960141
Iter: 1465 loss: 0.000253752165
Iter: 1466 loss: 0.000254081271
Iter: 1467 loss: 0.000253654551
Iter: 1468 loss: 0.000253504666
Iter: 1469 loss: 0.000253502629
Iter: 1470 loss: 0.000253394712
Iter: 1471 loss: 0.000254638202
Iter: 1472 loss: 0.00025339282
Iter: 1473 loss: 0.000253324688
Iter: 1474 loss: 0.000253165723
Iter: 1475 loss: 0.00025501498
Iter: 1476 loss: 0.000253151287
Iter: 1477 loss: 0.000253007311
Iter: 1478 loss: 0.000253006845
Iter: 1479 loss: 0.000252902595
Iter: 1480 loss: 0.000252869853
Iter: 1481 loss: 0.000252807222
Iter: 1482 loss: 0.000252649945
Iter: 1483 loss: 0.000252614642
Iter: 1484 loss: 0.000252512982
Iter: 1485 loss: 0.000252326688
Iter: 1486 loss: 0.000253284699
Iter: 1487 loss: 0.000252298545
Iter: 1488 loss: 0.000252124824
Iter: 1489 loss: 0.000252691127
Iter: 1490 loss: 0.000252076774
Iter: 1491 loss: 0.000251910969
Iter: 1492 loss: 0.000251980207
Iter: 1493 loss: 0.000251797523
Iter: 1494 loss: 0.000251597579
Iter: 1495 loss: 0.000252363971
Iter: 1496 loss: 0.000251550926
Iter: 1497 loss: 0.000251371
Iter: 1498 loss: 0.000251576945
Iter: 1499 loss: 0.000251273799
Iter: 1500 loss: 0.000251130172
Iter: 1501 loss: 0.000251129968
Iter: 1502 loss: 0.000251023041
Iter: 1503 loss: 0.000252313
Iter: 1504 loss: 0.000251021585
Iter: 1505 loss: 0.000250943587
Iter: 1506 loss: 0.000250756391
Iter: 1507 loss: 0.000252838887
Iter: 1508 loss: 0.000250738958
Iter: 1509 loss: 0.000250598736
Iter: 1510 loss: 0.00025059664
Iter: 1511 loss: 0.000250477809
Iter: 1512 loss: 0.000250446203
Iter: 1513 loss: 0.000250374258
Iter: 1514 loss: 0.000250206271
Iter: 1515 loss: 0.000250146084
Iter: 1516 loss: 0.000250051846
Iter: 1517 loss: 0.000249838311
Iter: 1518 loss: 0.000250493642
Iter: 1519 loss: 0.000249774865
Iter: 1520 loss: 0.000249562785
Iter: 1521 loss: 0.000250686076
Iter: 1522 loss: 0.000249530189
Iter: 1523 loss: 0.000249344215
Iter: 1524 loss: 0.000249527
Iter: 1525 loss: 0.000249238423
Iter: 1526 loss: 0.000249042234
Iter: 1527 loss: 0.000249469856
Iter: 1528 loss: 0.000248967321
Iter: 1529 loss: 0.0002487569
Iter: 1530 loss: 0.000249188
Iter: 1531 loss: 0.000248672382
Iter: 1532 loss: 0.000248521363
Iter: 1533 loss: 0.000250761688
Iter: 1534 loss: 0.000248520693
Iter: 1535 loss: 0.000248431112
Iter: 1536 loss: 0.00024842986
Iter: 1537 loss: 0.000248366734
Iter: 1538 loss: 0.000248232769
Iter: 1539 loss: 0.000250445912
Iter: 1540 loss: 0.000248229277
Iter: 1541 loss: 0.000248129421
Iter: 1542 loss: 0.000249413948
Iter: 1543 loss: 0.000248127733
Iter: 1544 loss: 0.000248035707
Iter: 1545 loss: 0.000248044438
Iter: 1546 loss: 0.000247964141
Iter: 1547 loss: 0.00024784141
Iter: 1548 loss: 0.000247854041
Iter: 1549 loss: 0.000247747754
Iter: 1550 loss: 0.000247596356
Iter: 1551 loss: 0.000247967255
Iter: 1552 loss: 0.000247542746
Iter: 1553 loss: 0.00024739193
Iter: 1554 loss: 0.000248028024
Iter: 1555 loss: 0.000247361313
Iter: 1556 loss: 0.000247215707
Iter: 1557 loss: 0.000247515272
Iter: 1558 loss: 0.000247157179
Iter: 1559 loss: 0.000247016374
Iter: 1560 loss: 0.000247084245
Iter: 1561 loss: 0.000246922718
Iter: 1562 loss: 0.000246739393
Iter: 1563 loss: 0.000247473246
Iter: 1564 loss: 0.000246698153
Iter: 1565 loss: 0.000246558164
Iter: 1566 loss: 0.000247492397
Iter: 1567 loss: 0.000246543728
Iter: 1568 loss: 0.000246479205
Iter: 1569 loss: 0.000246468699
Iter: 1570 loss: 0.000246414158
Iter: 1571 loss: 0.000246298267
Iter: 1572 loss: 0.000248160213
Iter: 1573 loss: 0.000246295094
Iter: 1574 loss: 0.000246191834
Iter: 1575 loss: 0.000246929645
Iter: 1576 loss: 0.00024618255
Iter: 1577 loss: 0.000246075564
Iter: 1578 loss: 0.000246305921
Iter: 1579 loss: 0.00024603412
Iter: 1580 loss: 0.000245939736
Iter: 1581 loss: 0.00024585161
Iter: 1582 loss: 0.000245829
Iter: 1583 loss: 0.000245677482
Iter: 1584 loss: 0.000246129406
Iter: 1585 loss: 0.00024563144
Iter: 1586 loss: 0.000245487056
Iter: 1587 loss: 0.000245932257
Iter: 1588 loss: 0.000245444768
Iter: 1589 loss: 0.000245296804
Iter: 1590 loss: 0.000245948555
Iter: 1591 loss: 0.000245267758
Iter: 1592 loss: 0.00024515012
Iter: 1593 loss: 0.000245071104
Iter: 1594 loss: 0.000245027477
Iter: 1595 loss: 0.000244850758
Iter: 1596 loss: 0.00024566651
Iter: 1597 loss: 0.000244816212
Iter: 1598 loss: 0.000244671362
Iter: 1599 loss: 0.00024533266
Iter: 1600 loss: 0.000244643976
Iter: 1601 loss: 0.000244602037
Iter: 1602 loss: 0.000244574476
Iter: 1603 loss: 0.000244519557
Iter: 1604 loss: 0.0002444
Iter: 1605 loss: 0.000246229611
Iter: 1606 loss: 0.000244396157
Iter: 1607 loss: 0.000244283292
Iter: 1608 loss: 0.000244729978
Iter: 1609 loss: 0.000244257855
Iter: 1610 loss: 0.000244133524
Iter: 1611 loss: 0.00024459252
Iter: 1612 loss: 0.000244103372
Iter: 1613 loss: 0.000244006515
Iter: 1614 loss: 0.00024391085
Iter: 1615 loss: 0.000243890274
Iter: 1616 loss: 0.000243736358
Iter: 1617 loss: 0.000244343275
Iter: 1618 loss: 0.000243700721
Iter: 1619 loss: 0.000243564049
Iter: 1620 loss: 0.000243871516
Iter: 1621 loss: 0.00024351172
Iter: 1622 loss: 0.000243374321
Iter: 1623 loss: 0.000244118593
Iter: 1624 loss: 0.00024335389
Iter: 1625 loss: 0.000243242859
Iter: 1626 loss: 0.000243158327
Iter: 1627 loss: 0.000243122908
Iter: 1628 loss: 0.000242964918
Iter: 1629 loss: 0.00024415669
Iter: 1630 loss: 0.000242953305
Iter: 1631 loss: 0.000242832437
Iter: 1632 loss: 0.000243501912
Iter: 1633 loss: 0.000242816081
Iter: 1634 loss: 0.000242774884
Iter: 1635 loss: 0.0002427625
Iter: 1636 loss: 0.000242718306
Iter: 1637 loss: 0.00024263197
Iter: 1638 loss: 0.000244375376
Iter: 1639 loss: 0.000242631286
Iter: 1640 loss: 0.000242547991
Iter: 1641 loss: 0.000242866445
Iter: 1642 loss: 0.000242528025
Iter: 1643 loss: 0.000242435
Iter: 1644 loss: 0.00024307851
Iter: 1645 loss: 0.000242426424
Iter: 1646 loss: 0.000242362526
Iter: 1647 loss: 0.00024226305
Iter: 1648 loss: 0.0002422619
Iter: 1649 loss: 0.000242145878
Iter: 1650 loss: 0.000242548937
Iter: 1651 loss: 0.000242116046
Iter: 1652 loss: 0.000242004578
Iter: 1653 loss: 0.000242234237
Iter: 1654 loss: 0.000241961228
Iter: 1655 loss: 0.000241842892
Iter: 1656 loss: 0.000242523121
Iter: 1657 loss: 0.000241826507
Iter: 1658 loss: 0.000241724076
Iter: 1659 loss: 0.000241655624
Iter: 1660 loss: 0.00024161741
Iter: 1661 loss: 0.000241481612
Iter: 1662 loss: 0.000241926289
Iter: 1663 loss: 0.000241443
Iter: 1664 loss: 0.000241317801
Iter: 1665 loss: 0.000242044334
Iter: 1666 loss: 0.000241301343
Iter: 1667 loss: 0.000241263129
Iter: 1668 loss: 0.000241248519
Iter: 1669 loss: 0.000241198752
Iter: 1670 loss: 0.000241108966
Iter: 1671 loss: 0.000243270624
Iter: 1672 loss: 0.00024110917
Iter: 1673 loss: 0.000241021888
Iter: 1674 loss: 0.000241097674
Iter: 1675 loss: 0.000240971684
Iter: 1676 loss: 0.000240859852
Iter: 1677 loss: 0.000241733418
Iter: 1678 loss: 0.000240852562
Iter: 1679 loss: 0.000240777852
Iter: 1680 loss: 0.000240678593
Iter: 1681 loss: 0.00024067222
Iter: 1682 loss: 0.000240546884
Iter: 1683 loss: 0.000240959474
Iter: 1684 loss: 0.000240512833
Iter: 1685 loss: 0.000240380847
Iter: 1686 loss: 0.00024060125
Iter: 1687 loss: 0.000240321882
Iter: 1688 loss: 0.000240185589
Iter: 1689 loss: 0.000240985348
Iter: 1690 loss: 0.000240168185
Iter: 1691 loss: 0.000240050475
Iter: 1692 loss: 0.000240020978
Iter: 1693 loss: 0.000239946152
Iter: 1694 loss: 0.000239800851
Iter: 1695 loss: 0.000240446316
Iter: 1696 loss: 0.000239772227
Iter: 1697 loss: 0.000239649991
Iter: 1698 loss: 0.000240565947
Iter: 1699 loss: 0.000239640562
Iter: 1700 loss: 0.000239598885
Iter: 1701 loss: 0.000239588786
Iter: 1702 loss: 0.00023954366
Iter: 1703 loss: 0.000239472691
Iter: 1704 loss: 0.000239471789
Iter: 1705 loss: 0.00023939427
Iter: 1706 loss: 0.000239508503
Iter: 1707 loss: 0.000239357032
Iter: 1708 loss: 0.000239273853
Iter: 1709 loss: 0.000240346708
Iter: 1710 loss: 0.000239273257
Iter: 1711 loss: 0.000239220943
Iter: 1712 loss: 0.000239144982
Iter: 1713 loss: 0.000239143221
Iter: 1714 loss: 0.000239051238
Iter: 1715 loss: 0.000239192799
Iter: 1716 loss: 0.000239007699
Iter: 1717 loss: 0.000238893277
Iter: 1718 loss: 0.000239152141
Iter: 1719 loss: 0.000238849723
Iter: 1720 loss: 0.000238729626
Iter: 1721 loss: 0.00023939184
Iter: 1722 loss: 0.000238712106
Iter: 1723 loss: 0.000238606692
Iter: 1724 loss: 0.000238671259
Iter: 1725 loss: 0.000238538603
Iter: 1726 loss: 0.00023842098
Iter: 1727 loss: 0.000238512876
Iter: 1728 loss: 0.00023835001
Iter: 1729 loss: 0.000238215987
Iter: 1730 loss: 0.000239146233
Iter: 1731 loss: 0.000238203254
Iter: 1732 loss: 0.000238168403
Iter: 1733 loss: 0.000238151842
Iter: 1734 loss: 0.000238104723
Iter: 1735 loss: 0.000238060253
Iter: 1736 loss: 0.000238049193
Iter: 1737 loss: 0.000237986853
Iter: 1738 loss: 0.000237918634
Iter: 1739 loss: 0.000237909029
Iter: 1740 loss: 0.000237811575
Iter: 1741 loss: 0.000239322486
Iter: 1742 loss: 0.000237811415
Iter: 1743 loss: 0.000237750253
Iter: 1744 loss: 0.000237659726
Iter: 1745 loss: 0.000237657398
Iter: 1746 loss: 0.0002375357
Iter: 1747 loss: 0.000237680724
Iter: 1748 loss: 0.000237471249
Iter: 1749 loss: 0.000237318178
Iter: 1750 loss: 0.000237711851
Iter: 1751 loss: 0.000237265413
Iter: 1752 loss: 0.000237116124
Iter: 1753 loss: 0.000237997214
Iter: 1754 loss: 0.000237096552
Iter: 1755 loss: 0.000236979104
Iter: 1756 loss: 0.000237089582
Iter: 1757 loss: 0.000236910884
Iter: 1758 loss: 0.000236768392
Iter: 1759 loss: 0.000236984721
Iter: 1760 loss: 0.000236701089
Iter: 1761 loss: 0.000236562162
Iter: 1762 loss: 0.000237292901
Iter: 1763 loss: 0.000236541149
Iter: 1764 loss: 0.000236518303
Iter: 1765 loss: 0.000236484571
Iter: 1766 loss: 0.00023643754
Iter: 1767 loss: 0.0002363932
Iter: 1768 loss: 0.000236381791
Iter: 1769 loss: 0.000236308
Iter: 1770 loss: 0.000236271284
Iter: 1771 loss: 0.000236236156
Iter: 1772 loss: 0.000236165419
Iter: 1773 loss: 0.000236160791
Iter: 1774 loss: 0.000236111344
Iter: 1775 loss: 0.00023602
Iter: 1776 loss: 0.000238123655
Iter: 1777 loss: 0.000236020103
Iter: 1778 loss: 0.000235900196
Iter: 1779 loss: 0.000235973333
Iter: 1780 loss: 0.000235823871
Iter: 1781 loss: 0.000235680331
Iter: 1782 loss: 0.000236439
Iter: 1783 loss: 0.000235657426
Iter: 1784 loss: 0.00023553535
Iter: 1785 loss: 0.00023602249
Iter: 1786 loss: 0.000235506886
Iter: 1787 loss: 0.000235388201
Iter: 1788 loss: 0.000235537256
Iter: 1789 loss: 0.000235326675
Iter: 1790 loss: 0.000235189582
Iter: 1791 loss: 0.000235217347
Iter: 1792 loss: 0.000235086656
Iter: 1793 loss: 0.000234913692
Iter: 1794 loss: 0.000235581989
Iter: 1795 loss: 0.000234871579
Iter: 1796 loss: 0.000234832056
Iter: 1797 loss: 0.000234792562
Iter: 1798 loss: 0.000234721
Iter: 1799 loss: 0.000234701423
Iter: 1800 loss: 0.000234655803
Iter: 1801 loss: 0.000234574807
Iter: 1802 loss: 0.00023444563
Iter: 1803 loss: 0.000234444087
Iter: 1804 loss: 0.000234338513
Iter: 1805 loss: 0.000234332721
Iter: 1806 loss: 0.000234245963
Iter: 1807 loss: 0.000234138817
Iter: 1808 loss: 0.000234129868
Iter: 1809 loss: 0.000233989704
Iter: 1810 loss: 0.000234132021
Iter: 1811 loss: 0.000233911269
Iter: 1812 loss: 0.000233754632
Iter: 1813 loss: 0.000234296633
Iter: 1814 loss: 0.000233713668
Iter: 1815 loss: 0.000233557104
Iter: 1816 loss: 0.000234249979
Iter: 1817 loss: 0.000233526924
Iter: 1818 loss: 0.000233388098
Iter: 1819 loss: 0.000233607512
Iter: 1820 loss: 0.000233323502
Iter: 1821 loss: 0.000233159313
Iter: 1822 loss: 0.000233252387
Iter: 1823 loss: 0.000233052764
Iter: 1824 loss: 0.000232856386
Iter: 1825 loss: 0.000233378509
Iter: 1826 loss: 0.000232790684
Iter: 1827 loss: 0.000232790568
Iter: 1828 loss: 0.000232703533
Iter: 1829 loss: 0.000232627179
Iter: 1830 loss: 0.00023259467
Iter: 1831 loss: 0.000232555205
Iter: 1832 loss: 0.000232446706
Iter: 1833 loss: 0.000232345512
Iter: 1834 loss: 0.000232319624
Iter: 1835 loss: 0.00023223419
Iter: 1836 loss: 0.000232223887
Iter: 1837 loss: 0.000232143051
Iter: 1838 loss: 0.000232018429
Iter: 1839 loss: 0.000232016435
Iter: 1840 loss: 0.000231851824
Iter: 1841 loss: 0.000231923928
Iter: 1842 loss: 0.000231740196
Iter: 1843 loss: 0.000231547252
Iter: 1844 loss: 0.000232856371
Iter: 1845 loss: 0.000231527971
Iter: 1846 loss: 0.000231371494
Iter: 1847 loss: 0.00023182467
Iter: 1848 loss: 0.000231322992
Iter: 1849 loss: 0.000231150814
Iter: 1850 loss: 0.000231536382
Iter: 1851 loss: 0.000231085069
Iter: 1852 loss: 0.000230914084
Iter: 1853 loss: 0.000230960082
Iter: 1854 loss: 0.000230790334
Iter: 1855 loss: 0.00023059
Iter: 1856 loss: 0.000231350859
Iter: 1857 loss: 0.000230541831
Iter: 1858 loss: 0.000230497506
Iter: 1859 loss: 0.000230462261
Iter: 1860 loss: 0.00023038192
Iter: 1861 loss: 0.00023044135
Iter: 1862 loss: 0.000230332662
Iter: 1863 loss: 0.000230253354
Iter: 1864 loss: 0.000230157253
Iter: 1865 loss: 0.000230147678
Iter: 1866 loss: 0.000230052887
Iter: 1867 loss: 0.000230052508
Iter: 1868 loss: 0.000229960206
Iter: 1869 loss: 0.000230002246
Iter: 1870 loss: 0.000229898054
Iter: 1871 loss: 0.000229801182
Iter: 1872 loss: 0.00022977
Iter: 1873 loss: 0.000229713332
Iter: 1874 loss: 0.000229562793
Iter: 1875 loss: 0.000230034901
Iter: 1876 loss: 0.000229518788
Iter: 1877 loss: 0.000229364174
Iter: 1878 loss: 0.000230285412
Iter: 1879 loss: 0.000229343976
Iter: 1880 loss: 0.000229219091
Iter: 1881 loss: 0.000229654906
Iter: 1882 loss: 0.000229186073
Iter: 1883 loss: 0.000229064244
Iter: 1884 loss: 0.000229103985
Iter: 1885 loss: 0.000228978315
Iter: 1886 loss: 0.000228834338
Iter: 1887 loss: 0.000229296886
Iter: 1888 loss: 0.000228793244
Iter: 1889 loss: 0.000228660138
Iter: 1890 loss: 0.000229619269
Iter: 1891 loss: 0.000228648249
Iter: 1892 loss: 0.00022849688
Iter: 1893 loss: 0.000229552941
Iter: 1894 loss: 0.000228481862
Iter: 1895 loss: 0.000228419303
Iter: 1896 loss: 0.000228254212
Iter: 1897 loss: 0.00022958545
Iter: 1898 loss: 0.000228224133
Iter: 1899 loss: 0.000227962155
Iter: 1900 loss: 0.000228555291
Iter: 1901 loss: 0.000227861412
Iter: 1902 loss: 0.000227732235
Iter: 1903 loss: 0.00022769245
Iter: 1904 loss: 0.000227631535
Iter: 1905 loss: 0.000227458528
Iter: 1906 loss: 0.000228333229
Iter: 1907 loss: 0.000227401615
Iter: 1908 loss: 0.000227160373
Iter: 1909 loss: 0.000228501027
Iter: 1910 loss: 0.000227125
Iter: 1911 loss: 0.000226889853
Iter: 1912 loss: 0.000227733937
Iter: 1913 loss: 0.000226829608
Iter: 1914 loss: 0.00022664851
Iter: 1915 loss: 0.000227209137
Iter: 1916 loss: 0.000226594901
Iter: 1917 loss: 0.00022643563
Iter: 1918 loss: 0.000226465985
Iter: 1919 loss: 0.000226316479
Iter: 1920 loss: 0.00022608618
Iter: 1921 loss: 0.000226274788
Iter: 1922 loss: 0.000225948068
Iter: 1923 loss: 0.000225704629
Iter: 1924 loss: 0.000226674892
Iter: 1925 loss: 0.000225649419
Iter: 1926 loss: 0.000225604075
Iter: 1927 loss: 0.000225489697
Iter: 1928 loss: 0.000225409123
Iter: 1929 loss: 0.000225204043
Iter: 1930 loss: 0.000227063312
Iter: 1931 loss: 0.000225173
Iter: 1932 loss: 0.000224844203
Iter: 1933 loss: 0.000225880707
Iter: 1934 loss: 0.000224748626
Iter: 1935 loss: 0.000224613788
Iter: 1936 loss: 0.00022459749
Iter: 1937 loss: 0.000224443909
Iter: 1938 loss: 0.0002241607
Iter: 1939 loss: 0.000230722042
Iter: 1940 loss: 0.000224160569
Iter: 1941 loss: 0.000223949464
Iter: 1942 loss: 0.00022452946
Iter: 1943 loss: 0.000223880488
Iter: 1944 loss: 0.000223686133
Iter: 1945 loss: 0.00022446482
Iter: 1946 loss: 0.000223642171
Iter: 1947 loss: 0.000223493204
Iter: 1948 loss: 0.000225166557
Iter: 1949 loss: 0.000223490817
Iter: 1950 loss: 0.000223384355
Iter: 1951 loss: 0.00022342919
Iter: 1952 loss: 0.000223310781
Iter: 1953 loss: 0.000223173498
Iter: 1954 loss: 0.000223247553
Iter: 1955 loss: 0.000223084484
Iter: 1956 loss: 0.000222961709
Iter: 1957 loss: 0.000223186609
Iter: 1958 loss: 0.000222909614
Iter: 1959 loss: 0.000222818431
Iter: 1960 loss: 0.000224076008
Iter: 1961 loss: 0.000222818257
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi3 /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi3
+ date
Sun Nov  1 01:11:57 EDT 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --load_model experiments.yidi/biholo/f0_psi0.5/300_300_300_1 --function f1 --psi 2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 50000 --batch_size 5000 --max_epochs 200 --learning_rate 0.001 --decay_rate 0.98 --loss_func weighted_MAPE
Processing model: 300_300_300_1
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 119, in worker
    result = (True, func(*args, **kwds))
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 47, in starmapstar
    return list(itertools.starmap(args[0], args[1]))
  File "/home/mrdouglas/Manifold/hypersurface_tf.py", line 204, in solve_poly
    c_solved = polyroots(coeff)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/mpmath/calculus/polynomials.py", line 196, in polyroots
    % maxsteps)
mpmath.libmp.libhyper.NoConvergence: Didn't converge in maxsteps=50 steps.
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "biholoNN_train.py", line 65, in <module>
    HS_test = Hypersurface(Z, f, n_pairs)
  File "/home/mrdouglas/Manifold/hypersurface_tf.py", line 31, in __init__
    self.points = self.__solve_points(n_pairs)
  File "/home/mrdouglas/Manifold/hypersurface_tf.py", line 227, in __solve_points
    for zpair in zpairs])):
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 274, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 644, in get
    raise self._value
mpmath.libmp.libhyper.NoConvergence: Didn't converge in maxsteps=50 steps.
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi3/300_300_300_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 16000 --load_model /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi3/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output79/f1_psi2_phi3/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6f42e2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6f42d48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6f42d4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6f4205e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6f41f0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6f41e67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6f40f3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6f4133620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6f4133598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6f4131b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6c73e42f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6c739c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6c739c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6c73939d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6c739c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6c7328c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6c731d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6c731dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a01ff9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6c731d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6c735e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6c735e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a0174950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a01ab950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a01ab6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a0109840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a00e0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a00c9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a00c9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a01df510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a0090950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a0045400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a0045488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6a00686a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd6800ab510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd68001a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 90, in <module>
    model = tf.keras.models.load_model(load_path, compile=False)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py", line 186, in load_model
    loader_impl.parse_saved_model(filepath)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py", line 113, in parse_saved_model
    constants.SAVED_MODEL_FILENAME_PB))
OSError: SavedModel file does not exist at: /home/mrdouglas/Manifold/experiments.final/output78/f1_psi2_phi3/300_300_300_1/{saved_model.pbtxt|saved_model.pb}
++ basename experiments.final/script78
+ '[' -r STOP.script78 ']'
