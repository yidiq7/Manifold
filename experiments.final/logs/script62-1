+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ fn=f1
+ psi=2
+ phi=0.4
+ layers=300_300_300_1
++ pwd
+ IN=/home/mrdouglas/Manifold/experiments.final/output61
+ true
+ for SUM in $IN/${fn}_psi${psi}_phi${phi}/summary.txt
+ OUT=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/summary.txt
+ '[' '!' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/summary.txt ']'
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output61/f1_psi2_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4// --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218689158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218689620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f423de638c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f423ddcdf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f423ddcd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f421863b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f421860c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42185d98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42185ad1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218572d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218572bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42185d9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f421853e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42184e5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42184e5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42184e5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42184e5048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42184617b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f421843d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f421849fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42183e29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42183e2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218396620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218358d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218373378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218373b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42182d08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42182d0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218306598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218306ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218265268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218279a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4218279730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f421824e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42181f22f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f42181e3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.67242944e-06
Iter: 2 loss: 3.60761646e-06
Iter: 3 loss: 3.60748777e-06
Iter: 4 loss: 3.11593726e-06
Iter: 5 loss: 5.6868821e-06
Iter: 6 loss: 3.03847628e-06
Iter: 7 loss: 2.83668032e-06
Iter: 8 loss: 3.54791405e-06
Iter: 9 loss: 2.78464358e-06
Iter: 10 loss: 2.60738761e-06
Iter: 11 loss: 3.36045105e-06
Iter: 12 loss: 2.57031661e-06
Iter: 13 loss: 2.41295947e-06
Iter: 14 loss: 2.27579562e-06
Iter: 15 loss: 2.23392271e-06
Iter: 16 loss: 2.15575301e-06
Iter: 17 loss: 2.14746115e-06
Iter: 18 loss: 2.05751076e-06
Iter: 19 loss: 1.86165983e-06
Iter: 20 loss: 4.8077568e-06
Iter: 21 loss: 1.85354043e-06
Iter: 22 loss: 1.68400948e-06
Iter: 23 loss: 2.42327087e-06
Iter: 24 loss: 1.6496889e-06
Iter: 25 loss: 1.584713e-06
Iter: 26 loss: 1.56286944e-06
Iter: 27 loss: 1.52976395e-06
Iter: 28 loss: 1.4476152e-06
Iter: 29 loss: 2.23679535e-06
Iter: 30 loss: 1.43650061e-06
Iter: 31 loss: 1.35641892e-06
Iter: 32 loss: 1.46974412e-06
Iter: 33 loss: 1.31698744e-06
Iter: 34 loss: 1.2161272e-06
Iter: 35 loss: 1.35505763e-06
Iter: 36 loss: 1.16600017e-06
Iter: 37 loss: 1.0379589e-06
Iter: 38 loss: 1.34140146e-06
Iter: 39 loss: 9.91269189e-07
Iter: 40 loss: 9.71625923e-07
Iter: 41 loss: 9.41533074e-07
Iter: 42 loss: 8.96698964e-07
Iter: 43 loss: 1.04883645e-06
Iter: 44 loss: 8.84658789e-07
Iter: 45 loss: 8.67502422e-07
Iter: 46 loss: 9.53632821e-07
Iter: 47 loss: 8.64647632e-07
Iter: 48 loss: 8.43073053e-07
Iter: 49 loss: 8.24509925e-07
Iter: 50 loss: 8.18672333e-07
Iter: 51 loss: 7.94198286e-07
Iter: 52 loss: 8.55892381e-07
Iter: 53 loss: 7.85690304e-07
Iter: 54 loss: 7.55528106e-07
Iter: 55 loss: 9.05561706e-07
Iter: 56 loss: 7.50446e-07
Iter: 57 loss: 7.21972469e-07
Iter: 58 loss: 7.32135504e-07
Iter: 59 loss: 7.01951e-07
Iter: 60 loss: 6.69656856e-07
Iter: 61 loss: 6.93391371e-07
Iter: 62 loss: 6.49791e-07
Iter: 63 loss: 6.36693699e-07
Iter: 64 loss: 6.30854458e-07
Iter: 65 loss: 6.189e-07
Iter: 66 loss: 5.94718131e-07
Iter: 67 loss: 1.03980108e-06
Iter: 68 loss: 5.94351945e-07
Iter: 69 loss: 5.79530479e-07
Iter: 70 loss: 5.7837e-07
Iter: 71 loss: 5.67344841e-07
Iter: 72 loss: 5.4012213e-07
Iter: 73 loss: 5.79997049e-07
Iter: 74 loss: 5.2696663e-07
Iter: 75 loss: 5.04382456e-07
Iter: 76 loss: 6.62467301e-07
Iter: 77 loss: 5.02310513e-07
Iter: 78 loss: 4.94813094e-07
Iter: 79 loss: 4.90692173e-07
Iter: 80 loss: 4.84829229e-07
Iter: 81 loss: 4.76205457e-07
Iter: 82 loss: 4.75970182e-07
Iter: 83 loss: 4.69411e-07
Iter: 84 loss: 4.69045347e-07
Iter: 85 loss: 4.6531315e-07
Iter: 86 loss: 4.55074371e-07
Iter: 87 loss: 5.12864517e-07
Iter: 88 loss: 4.52036602e-07
Iter: 89 loss: 4.43925387e-07
Iter: 90 loss: 4.43152288e-07
Iter: 91 loss: 4.35134893e-07
Iter: 92 loss: 4.28677595e-07
Iter: 93 loss: 4.26260499e-07
Iter: 94 loss: 4.14634201e-07
Iter: 95 loss: 4.51013193e-07
Iter: 96 loss: 4.11230957e-07
Iter: 97 loss: 4.03063893e-07
Iter: 98 loss: 4.90347588e-07
Iter: 99 loss: 4.02876083e-07
Iter: 100 loss: 3.95263214e-07
Iter: 101 loss: 3.99054016e-07
Iter: 102 loss: 3.90190053e-07
Iter: 103 loss: 3.84078504e-07
Iter: 104 loss: 3.78803122e-07
Iter: 105 loss: 3.77158983e-07
Iter: 106 loss: 3.68829262e-07
Iter: 107 loss: 3.83593175e-07
Iter: 108 loss: 3.65197764e-07
Iter: 109 loss: 3.53967152e-07
Iter: 110 loss: 3.61465254e-07
Iter: 111 loss: 3.46926697e-07
Iter: 112 loss: 3.59446e-07
Iter: 113 loss: 3.43698503e-07
Iter: 114 loss: 3.40853092e-07
Iter: 115 loss: 3.35106165e-07
Iter: 116 loss: 4.4055497e-07
Iter: 117 loss: 3.35016836e-07
Iter: 118 loss: 3.31946922e-07
Iter: 119 loss: 3.31734213e-07
Iter: 120 loss: 3.28940189e-07
Iter: 121 loss: 3.25446024e-07
Iter: 122 loss: 3.25178178e-07
Iter: 123 loss: 3.22366589e-07
Iter: 124 loss: 3.29729772e-07
Iter: 125 loss: 3.21417076e-07
Iter: 126 loss: 3.16472892e-07
Iter: 127 loss: 3.19191315e-07
Iter: 128 loss: 3.13262916e-07
Iter: 129 loss: 3.08390838e-07
Iter: 130 loss: 3.12395088e-07
Iter: 131 loss: 3.05503733e-07
Iter: 132 loss: 3.00534396e-07
Iter: 133 loss: 3.79867743e-07
Iter: 134 loss: 3.00532463e-07
Iter: 135 loss: 2.97845e-07
Iter: 136 loss: 2.97717321e-07
Iter: 137 loss: 2.95661835e-07
Iter: 138 loss: 2.91959253e-07
Iter: 139 loss: 2.92137344e-07
Iter: 140 loss: 2.89052878e-07
Iter: 141 loss: 2.84951568e-07
Iter: 142 loss: 2.86920368e-07
Iter: 143 loss: 2.82213477e-07
Iter: 144 loss: 2.76979165e-07
Iter: 145 loss: 2.86490263e-07
Iter: 146 loss: 2.74690308e-07
Iter: 147 loss: 2.71773644e-07
Iter: 148 loss: 2.71328275e-07
Iter: 149 loss: 2.67270195e-07
Iter: 150 loss: 2.6560167e-07
Iter: 151 loss: 2.63465722e-07
Iter: 152 loss: 2.61069232e-07
Iter: 153 loss: 2.83817144e-07
Iter: 154 loss: 2.60979391e-07
Iter: 155 loss: 2.5866e-07
Iter: 156 loss: 2.62637826e-07
Iter: 157 loss: 2.5762904e-07
Iter: 158 loss: 2.55970519e-07
Iter: 159 loss: 2.53061671e-07
Iter: 160 loss: 2.53052576e-07
Iter: 161 loss: 2.51609833e-07
Iter: 162 loss: 2.50930498e-07
Iter: 163 loss: 2.49686593e-07
Iter: 164 loss: 2.46257173e-07
Iter: 165 loss: 2.68141548e-07
Iter: 166 loss: 2.45391277e-07
Iter: 167 loss: 2.42779578e-07
Iter: 168 loss: 2.42653243e-07
Iter: 169 loss: 2.39693634e-07
Iter: 170 loss: 2.39164876e-07
Iter: 171 loss: 2.37167683e-07
Iter: 172 loss: 2.35105176e-07
Iter: 173 loss: 2.35968869e-07
Iter: 174 loss: 2.3371868e-07
Iter: 175 loss: 2.31452816e-07
Iter: 176 loss: 2.33597461e-07
Iter: 177 loss: 2.30173143e-07
Iter: 178 loss: 2.27615899e-07
Iter: 179 loss: 2.59086391e-07
Iter: 180 loss: 2.27579804e-07
Iter: 181 loss: 2.25823328e-07
Iter: 182 loss: 2.26906e-07
Iter: 183 loss: 2.24688563e-07
Iter: 184 loss: 2.22011778e-07
Iter: 185 loss: 2.48800802e-07
Iter: 186 loss: 2.21922079e-07
Iter: 187 loss: 2.20637943e-07
Iter: 188 loss: 2.18241354e-07
Iter: 189 loss: 2.7112759e-07
Iter: 190 loss: 2.18225253e-07
Iter: 191 loss: 2.16815579e-07
Iter: 192 loss: 2.16523318e-07
Iter: 193 loss: 2.15697469e-07
Iter: 194 loss: 2.13818026e-07
Iter: 195 loss: 2.38095907e-07
Iter: 196 loss: 2.13692758e-07
Iter: 197 loss: 2.12998941e-07
Iter: 198 loss: 2.12838728e-07
Iter: 199 loss: 2.1181026e-07
Iter: 200 loss: 2.09713221e-07
Iter: 201 loss: 2.46360855e-07
Iter: 202 loss: 2.09670901e-07
Iter: 203 loss: 2.07712702e-07
Iter: 204 loss: 2.15169251e-07
Iter: 205 loss: 2.07242749e-07
Iter: 206 loss: 2.05470144e-07
Iter: 207 loss: 2.31220412e-07
Iter: 208 loss: 2.05472759e-07
Iter: 209 loss: 2.04708101e-07
Iter: 210 loss: 2.02583067e-07
Iter: 211 loss: 2.14710212e-07
Iter: 212 loss: 2.01935308e-07
Iter: 213 loss: 1.99849254e-07
Iter: 214 loss: 2.21423363e-07
Iter: 215 loss: 1.99794613e-07
Iter: 216 loss: 1.98003292e-07
Iter: 217 loss: 2.14050189e-07
Iter: 218 loss: 1.97916108e-07
Iter: 219 loss: 1.96964891e-07
Iter: 220 loss: 2.11284956e-07
Iter: 221 loss: 1.9697606e-07
Iter: 222 loss: 1.96047182e-07
Iter: 223 loss: 1.94973239e-07
Iter: 224 loss: 1.94831131e-07
Iter: 225 loss: 1.93911887e-07
Iter: 226 loss: 1.99334096e-07
Iter: 227 loss: 1.93798044e-07
Iter: 228 loss: 1.92725423e-07
Iter: 229 loss: 1.95012632e-07
Iter: 230 loss: 1.92299552e-07
Iter: 231 loss: 1.91482599e-07
Iter: 232 loss: 1.89746487e-07
Iter: 233 loss: 2.17788354e-07
Iter: 234 loss: 1.89692884e-07
Iter: 235 loss: 1.88605298e-07
Iter: 236 loss: 1.88322787e-07
Iter: 237 loss: 1.87550413e-07
Iter: 238 loss: 1.85880538e-07
Iter: 239 loss: 2.10720685e-07
Iter: 240 loss: 1.85810535e-07
Iter: 241 loss: 1.84396839e-07
Iter: 242 loss: 1.99682e-07
Iter: 243 loss: 1.84367309e-07
Iter: 244 loss: 1.82783467e-07
Iter: 245 loss: 1.86367885e-07
Iter: 246 loss: 1.82179548e-07
Iter: 247 loss: 1.81592327e-07
Iter: 248 loss: 1.81006811e-07
Iter: 249 loss: 1.80886502e-07
Iter: 250 loss: 1.79801845e-07
Iter: 251 loss: 1.80879596e-07
Iter: 252 loss: 1.79198395e-07
Iter: 253 loss: 1.78332982e-07
Iter: 254 loss: 1.78260422e-07
Iter: 255 loss: 1.77532584e-07
Iter: 256 loss: 1.80737032e-07
Iter: 257 loss: 1.77379405e-07
Iter: 258 loss: 1.76716924e-07
Iter: 259 loss: 1.75506898e-07
Iter: 260 loss: 2.02734668e-07
Iter: 261 loss: 1.75503729e-07
Iter: 262 loss: 1.74933732e-07
Iter: 263 loss: 1.74807184e-07
Iter: 264 loss: 1.74203805e-07
Iter: 265 loss: 1.73236629e-07
Iter: 266 loss: 1.73229765e-07
Iter: 267 loss: 1.72539956e-07
Iter: 268 loss: 1.77387676e-07
Iter: 269 loss: 1.72469868e-07
Iter: 270 loss: 1.71669441e-07
Iter: 271 loss: 1.74291017e-07
Iter: 272 loss: 1.71450012e-07
Iter: 273 loss: 1.7093646e-07
Iter: 274 loss: 1.69859078e-07
Iter: 275 loss: 1.88500593e-07
Iter: 276 loss: 1.69829548e-07
Iter: 277 loss: 1.69355872e-07
Iter: 278 loss: 1.69081986e-07
Iter: 279 loss: 1.68578055e-07
Iter: 280 loss: 1.67520696e-07
Iter: 281 loss: 1.86833404e-07
Iter: 282 loss: 1.6750046e-07
Iter: 283 loss: 1.66615877e-07
Iter: 284 loss: 1.66009031e-07
Iter: 285 loss: 1.65691688e-07
Iter: 286 loss: 1.64508677e-07
Iter: 287 loss: 1.6717631e-07
Iter: 288 loss: 1.64054228e-07
Iter: 289 loss: 1.65049528e-07
Iter: 290 loss: 1.63582314e-07
Iter: 291 loss: 1.6315613e-07
Iter: 292 loss: 1.62177116e-07
Iter: 293 loss: 1.7369868e-07
Iter: 294 loss: 1.62092888e-07
Iter: 295 loss: 1.61336601e-07
Iter: 296 loss: 1.69870873e-07
Iter: 297 loss: 1.61332721e-07
Iter: 298 loss: 1.60720461e-07
Iter: 299 loss: 1.62919491e-07
Iter: 300 loss: 1.60565364e-07
Iter: 301 loss: 1.60038127e-07
Iter: 302 loss: 1.60387088e-07
Iter: 303 loss: 1.59707184e-07
Iter: 304 loss: 1.59202443e-07
Iter: 305 loss: 1.60419035e-07
Iter: 306 loss: 1.59024694e-07
Iter: 307 loss: 1.58248326e-07
Iter: 308 loss: 1.59516929e-07
Iter: 309 loss: 1.5787775e-07
Iter: 310 loss: 1.57287559e-07
Iter: 311 loss: 1.56330572e-07
Iter: 312 loss: 1.56330486e-07
Iter: 313 loss: 1.56298768e-07
Iter: 314 loss: 1.55830151e-07
Iter: 315 loss: 1.55427529e-07
Iter: 316 loss: 1.54451726e-07
Iter: 317 loss: 1.66247901e-07
Iter: 318 loss: 1.54370269e-07
Iter: 319 loss: 1.53742178e-07
Iter: 320 loss: 1.5515559e-07
Iter: 321 loss: 1.53489182e-07
Iter: 322 loss: 1.52755291e-07
Iter: 323 loss: 1.52846e-07
Iter: 324 loss: 1.52183361e-07
Iter: 325 loss: 1.52311017e-07
Iter: 326 loss: 1.51748864e-07
Iter: 327 loss: 1.51334433e-07
Iter: 328 loss: 1.50521885e-07
Iter: 329 loss: 1.66879246e-07
Iter: 330 loss: 1.50507489e-07
Iter: 331 loss: 1.49895456e-07
Iter: 332 loss: 1.56494991e-07
Iter: 333 loss: 1.49875774e-07
Iter: 334 loss: 1.49275223e-07
Iter: 335 loss: 1.49697144e-07
Iter: 336 loss: 1.48895523e-07
Iter: 337 loss: 1.48472225e-07
Iter: 338 loss: 1.5024078e-07
Iter: 339 loss: 1.48374696e-07
Iter: 340 loss: 1.48032882e-07
Iter: 341 loss: 1.48563984e-07
Iter: 342 loss: 1.47887135e-07
Iter: 343 loss: 1.47487839e-07
Iter: 344 loss: 1.51089921e-07
Iter: 345 loss: 1.47462856e-07
Iter: 346 loss: 1.47201177e-07
Iter: 347 loss: 1.46538696e-07
Iter: 348 loss: 1.52485541e-07
Iter: 349 loss: 1.4642842e-07
Iter: 350 loss: 1.45846542e-07
Iter: 351 loss: 1.53890227e-07
Iter: 352 loss: 1.4584262e-07
Iter: 353 loss: 1.45228682e-07
Iter: 354 loss: 1.47724734e-07
Iter: 355 loss: 1.45102575e-07
Iter: 356 loss: 1.44764385e-07
Iter: 357 loss: 1.44080019e-07
Iter: 358 loss: 1.56696785e-07
Iter: 359 loss: 1.44066803e-07
Iter: 360 loss: 1.43501552e-07
Iter: 361 loss: 1.48279781e-07
Iter: 362 loss: 1.43464348e-07
Iter: 363 loss: 1.4333358e-07
Iter: 364 loss: 1.43221598e-07
Iter: 365 loss: 1.43082758e-07
Iter: 366 loss: 1.42616727e-07
Iter: 367 loss: 1.43644257e-07
Iter: 368 loss: 1.42329355e-07
Iter: 369 loss: 1.42029648e-07
Iter: 370 loss: 1.41922e-07
Iter: 371 loss: 1.41574617e-07
Iter: 372 loss: 1.41014525e-07
Iter: 373 loss: 1.41012222e-07
Iter: 374 loss: 1.40528229e-07
Iter: 375 loss: 1.4229127e-07
Iter: 376 loss: 1.40408503e-07
Iter: 377 loss: 1.39955105e-07
Iter: 378 loss: 1.40720175e-07
Iter: 379 loss: 1.39757091e-07
Iter: 380 loss: 1.39443799e-07
Iter: 381 loss: 1.39427939e-07
Iter: 382 loss: 1.39202911e-07
Iter: 383 loss: 1.38946589e-07
Iter: 384 loss: 1.3890542e-07
Iter: 385 loss: 1.3857354e-07
Iter: 386 loss: 1.3801565e-07
Iter: 387 loss: 1.38024674e-07
Iter: 388 loss: 1.37634132e-07
Iter: 389 loss: 1.4242687e-07
Iter: 390 loss: 1.37629257e-07
Iter: 391 loss: 1.37205703e-07
Iter: 392 loss: 1.37481834e-07
Iter: 393 loss: 1.3693537e-07
Iter: 394 loss: 1.36602608e-07
Iter: 395 loss: 1.39035052e-07
Iter: 396 loss: 1.36582059e-07
Iter: 397 loss: 1.36314597e-07
Iter: 398 loss: 1.35900962e-07
Iter: 399 loss: 1.35902354e-07
Iter: 400 loss: 1.35467985e-07
Iter: 401 loss: 1.36210161e-07
Iter: 402 loss: 1.35264102e-07
Iter: 403 loss: 1.34954988e-07
Iter: 404 loss: 1.34916888e-07
Iter: 405 loss: 1.34746102e-07
Iter: 406 loss: 1.34290786e-07
Iter: 407 loss: 1.37375679e-07
Iter: 408 loss: 1.34184774e-07
Iter: 409 loss: 1.33680871e-07
Iter: 410 loss: 1.3617236e-07
Iter: 411 loss: 1.33575966e-07
Iter: 412 loss: 1.33159944e-07
Iter: 413 loss: 1.36486904e-07
Iter: 414 loss: 1.33117624e-07
Iter: 415 loss: 1.3277571e-07
Iter: 416 loss: 1.35383942e-07
Iter: 417 loss: 1.32759752e-07
Iter: 418 loss: 1.32546916e-07
Iter: 419 loss: 1.32187864e-07
Iter: 420 loss: 1.32183274e-07
Iter: 421 loss: 1.31735703e-07
Iter: 422 loss: 1.34545616e-07
Iter: 423 loss: 1.31697846e-07
Iter: 424 loss: 1.31312163e-07
Iter: 425 loss: 1.34576624e-07
Iter: 426 loss: 1.31298506e-07
Iter: 427 loss: 1.310529e-07
Iter: 428 loss: 1.31996089e-07
Iter: 429 loss: 1.30985669e-07
Iter: 430 loss: 1.30777337e-07
Iter: 431 loss: 1.3152291e-07
Iter: 432 loss: 1.30706312e-07
Iter: 433 loss: 1.30529358e-07
Iter: 434 loss: 1.30131369e-07
Iter: 435 loss: 1.35691167e-07
Iter: 436 loss: 1.30116902e-07
Iter: 437 loss: 1.29756501e-07
Iter: 438 loss: 1.33392263e-07
Iter: 439 loss: 1.29747434e-07
Iter: 440 loss: 1.29386592e-07
Iter: 441 loss: 1.31507676e-07
Iter: 442 loss: 1.29335945e-07
Iter: 443 loss: 1.2912183e-07
Iter: 444 loss: 1.28559321e-07
Iter: 445 loss: 1.3343886e-07
Iter: 446 loss: 1.28478646e-07
Iter: 447 loss: 1.27961911e-07
Iter: 448 loss: 1.35790515e-07
Iter: 449 loss: 1.27956653e-07
Iter: 450 loss: 1.27728697e-07
Iter: 451 loss: 1.30432454e-07
Iter: 452 loss: 1.27720938e-07
Iter: 453 loss: 1.27522469e-07
Iter: 454 loss: 1.28270329e-07
Iter: 455 loss: 1.27477335e-07
Iter: 456 loss: 1.27282263e-07
Iter: 457 loss: 1.27009159e-07
Iter: 458 loss: 1.2699374e-07
Iter: 459 loss: 1.26661035e-07
Iter: 460 loss: 1.27633541e-07
Iter: 461 loss: 1.26558149e-07
Iter: 462 loss: 1.26308251e-07
Iter: 463 loss: 1.26300989e-07
Iter: 464 loss: 1.26093909e-07
Iter: 465 loss: 1.25702428e-07
Iter: 466 loss: 1.34557496e-07
Iter: 467 loss: 1.25702513e-07
Iter: 468 loss: 1.25595491e-07
Iter: 469 loss: 1.25502254e-07
Iter: 470 loss: 1.25355299e-07
Iter: 471 loss: 1.2502943e-07
Iter: 472 loss: 1.29697298e-07
Iter: 473 loss: 1.25019454e-07
Iter: 474 loss: 1.24773038e-07
Iter: 475 loss: 1.26517222e-07
Iter: 476 loss: 1.24743607e-07
Iter: 477 loss: 1.24452399e-07
Iter: 478 loss: 1.24975458e-07
Iter: 479 loss: 1.24328437e-07
Iter: 480 loss: 1.24052974e-07
Iter: 481 loss: 1.2351002e-07
Iter: 482 loss: 1.3430882e-07
Iter: 483 loss: 1.23499959e-07
Iter: 484 loss: 1.22972935e-07
Iter: 485 loss: 1.24981682e-07
Iter: 486 loss: 1.22858253e-07
Iter: 487 loss: 1.22770302e-07
Iter: 488 loss: 1.22632599e-07
Iter: 489 loss: 1.22413269e-07
Iter: 490 loss: 1.22241318e-07
Iter: 491 loss: 1.22178989e-07
Iter: 492 loss: 1.21978317e-07
Iter: 493 loss: 1.21999008e-07
Iter: 494 loss: 1.21830922e-07
Iter: 495 loss: 1.2153734e-07
Iter: 496 loss: 1.23688864e-07
Iter: 497 loss: 1.215097e-07
Iter: 498 loss: 1.21239e-07
Iter: 499 loss: 1.22373933e-07
Iter: 500 loss: 1.21171922e-07
Iter: 501 loss: 1.20883158e-07
Iter: 502 loss: 1.20755189e-07
Iter: 503 loss: 1.20606387e-07
Iter: 504 loss: 1.2040735e-07
Iter: 505 loss: 1.20374267e-07
Iter: 506 loss: 1.20234404e-07
Iter: 507 loss: 1.19879431e-07
Iter: 508 loss: 1.22966e-07
Iter: 509 loss: 1.19835164e-07
Iter: 510 loss: 1.19742623e-07
Iter: 511 loss: 1.19673587e-07
Iter: 512 loss: 1.19522412e-07
Iter: 513 loss: 1.19241108e-07
Iter: 514 loss: 1.25644718e-07
Iter: 515 loss: 1.19244788e-07
Iter: 516 loss: 1.18977731e-07
Iter: 517 loss: 1.18914016e-07
Iter: 518 loss: 1.18746243e-07
Iter: 519 loss: 1.18329076e-07
Iter: 520 loss: 1.19145021e-07
Iter: 521 loss: 1.18155313e-07
Iter: 522 loss: 1.18139738e-07
Iter: 523 loss: 1.17940829e-07
Iter: 524 loss: 1.17795153e-07
Iter: 525 loss: 1.1752482e-07
Iter: 526 loss: 1.23047244e-07
Iter: 527 loss: 1.17522731e-07
Iter: 528 loss: 1.17278915e-07
Iter: 529 loss: 1.17497393e-07
Iter: 530 loss: 1.17132103e-07
Iter: 531 loss: 1.16932029e-07
Iter: 532 loss: 1.16928831e-07
Iter: 533 loss: 1.16784108e-07
Iter: 534 loss: 1.16924937e-07
Iter: 535 loss: 1.1669934e-07
Iter: 536 loss: 1.16539397e-07
Iter: 537 loss: 1.16911721e-07
Iter: 538 loss: 1.16474737e-07
Iter: 539 loss: 1.16302346e-07
Iter: 540 loss: 1.17786598e-07
Iter: 541 loss: 1.16294203e-07
Iter: 542 loss: 1.16170597e-07
Iter: 543 loss: 1.15852806e-07
Iter: 544 loss: 1.18272382e-07
Iter: 545 loss: 1.15801782e-07
Iter: 546 loss: 1.15752528e-07
Iter: 547 loss: 1.15640056e-07
Iter: 548 loss: 1.15511412e-07
Iter: 549 loss: 1.15237953e-07
Iter: 550 loss: 1.19780708e-07
Iter: 551 loss: 1.15226442e-07
Iter: 552 loss: 1.14977752e-07
Iter: 553 loss: 1.14879413e-07
Iter: 554 loss: 1.14726106e-07
Iter: 555 loss: 1.1442318e-07
Iter: 556 loss: 1.17138349e-07
Iter: 557 loss: 1.14410724e-07
Iter: 558 loss: 1.14221308e-07
Iter: 559 loss: 1.1421848e-07
Iter: 560 loss: 1.14133172e-07
Iter: 561 loss: 1.13928692e-07
Iter: 562 loss: 1.15975872e-07
Iter: 563 loss: 1.1389772e-07
Iter: 564 loss: 1.13643011e-07
Iter: 565 loss: 1.13691499e-07
Iter: 566 loss: 1.13439327e-07
Iter: 567 loss: 1.13422942e-07
Iter: 568 loss: 1.13313142e-07
Iter: 569 loss: 1.13219059e-07
Iter: 570 loss: 1.12980224e-07
Iter: 571 loss: 1.15711558e-07
Iter: 572 loss: 1.12975705e-07
Iter: 573 loss: 1.12736835e-07
Iter: 574 loss: 1.13689659e-07
Iter: 575 loss: 1.12684958e-07
Iter: 576 loss: 1.1247333e-07
Iter: 577 loss: 1.14737382e-07
Iter: 578 loss: 1.12470957e-07
Iter: 579 loss: 1.12340857e-07
Iter: 580 loss: 1.12444887e-07
Iter: 581 loss: 1.12258967e-07
Iter: 582 loss: 1.12144363e-07
Iter: 583 loss: 1.11973016e-07
Iter: 584 loss: 1.11973129e-07
Iter: 585 loss: 1.11840698e-07
Iter: 586 loss: 1.11815581e-07
Iter: 587 loss: 1.11706832e-07
Iter: 588 loss: 1.11437878e-07
Iter: 589 loss: 1.14509319e-07
Iter: 590 loss: 1.11416739e-07
Iter: 591 loss: 1.11141659e-07
Iter: 592 loss: 1.11257208e-07
Iter: 593 loss: 1.10961103e-07
Iter: 594 loss: 1.10736792e-07
Iter: 595 loss: 1.10729026e-07
Iter: 596 loss: 1.10548861e-07
Iter: 597 loss: 1.11881732e-07
Iter: 598 loss: 1.10527296e-07
Iter: 599 loss: 1.10446507e-07
Iter: 600 loss: 1.10273746e-07
Iter: 601 loss: 1.1309082e-07
Iter: 602 loss: 1.10264438e-07
Iter: 603 loss: 1.10165331e-07
Iter: 604 loss: 1.10152925e-07
Iter: 605 loss: 1.10021077e-07
Iter: 606 loss: 1.09730152e-07
Iter: 607 loss: 1.13543329e-07
Iter: 608 loss: 1.09708353e-07
Iter: 609 loss: 1.09523057e-07
Iter: 610 loss: 1.09513799e-07
Iter: 611 loss: 1.09326606e-07
Iter: 612 loss: 1.09346445e-07
Iter: 613 loss: 1.0918199e-07
Iter: 614 loss: 1.08990207e-07
Iter: 615 loss: 1.09371214e-07
Iter: 616 loss: 1.08909987e-07
Iter: 617 loss: 1.08735321e-07
Iter: 618 loss: 1.08741425e-07
Iter: 619 loss: 1.08595771e-07
Iter: 620 loss: 1.08548164e-07
Iter: 621 loss: 1.08509397e-07
Iter: 622 loss: 1.08434477e-07
Iter: 623 loss: 1.08257481e-07
Iter: 624 loss: 1.10399888e-07
Iter: 625 loss: 1.08239334e-07
Iter: 626 loss: 1.08028324e-07
Iter: 627 loss: 1.10982327e-07
Iter: 628 loss: 1.08026185e-07
Iter: 629 loss: 1.07854994e-07
Iter: 630 loss: 1.07850923e-07
Iter: 631 loss: 1.07735616e-07
Iter: 632 loss: 1.07488241e-07
Iter: 633 loss: 1.11655936e-07
Iter: 634 loss: 1.07484915e-07
Iter: 635 loss: 1.07225468e-07
Iter: 636 loss: 1.07898749e-07
Iter: 637 loss: 1.07141105e-07
Iter: 638 loss: 1.07035802e-07
Iter: 639 loss: 1.07000062e-07
Iter: 640 loss: 1.06911372e-07
Iter: 641 loss: 1.06786274e-07
Iter: 642 loss: 1.06784029e-07
Iter: 643 loss: 1.06626402e-07
Iter: 644 loss: 1.0860122e-07
Iter: 645 loss: 1.06629308e-07
Iter: 646 loss: 1.06551283e-07
Iter: 647 loss: 1.06430129e-07
Iter: 648 loss: 1.06424721e-07
Iter: 649 loss: 1.06242538e-07
Iter: 650 loss: 1.06648478e-07
Iter: 651 loss: 1.06168855e-07
Iter: 652 loss: 1.05979737e-07
Iter: 653 loss: 1.06925711e-07
Iter: 654 loss: 1.05955301e-07
Iter: 655 loss: 1.05718698e-07
Iter: 656 loss: 1.06231013e-07
Iter: 657 loss: 1.05647509e-07
Iter: 658 loss: 1.05517508e-07
Iter: 659 loss: 1.0548866e-07
Iter: 660 loss: 1.05406734e-07
Iter: 661 loss: 1.05322343e-07
Iter: 662 loss: 1.05324119e-07
Iter: 663 loss: 1.05229532e-07
Iter: 664 loss: 1.05167373e-07
Iter: 665 loss: 1.05136692e-07
Iter: 666 loss: 1.05030601e-07
Iter: 667 loss: 1.04995095e-07
Iter: 668 loss: 1.04933875e-07
Iter: 669 loss: 1.04809374e-07
Iter: 670 loss: 1.04808109e-07
Iter: 671 loss: 1.04674548e-07
Iter: 672 loss: 1.04578945e-07
Iter: 673 loss: 1.04524389e-07
Iter: 674 loss: 1.04414518e-07
Iter: 675 loss: 1.04412223e-07
Iter: 676 loss: 1.04317273e-07
Iter: 677 loss: 1.04206528e-07
Iter: 678 loss: 1.04191081e-07
Iter: 679 loss: 1.04076136e-07
Iter: 680 loss: 1.04733431e-07
Iter: 681 loss: 1.04070921e-07
Iter: 682 loss: 1.03992221e-07
Iter: 683 loss: 1.04037525e-07
Iter: 684 loss: 1.03933004e-07
Iter: 685 loss: 1.03802257e-07
Iter: 686 loss: 1.04505844e-07
Iter: 687 loss: 1.03782682e-07
Iter: 688 loss: 1.03696827e-07
Iter: 689 loss: 1.03493051e-07
Iter: 690 loss: 1.06197483e-07
Iter: 691 loss: 1.03479941e-07
Iter: 692 loss: 1.03263716e-07
Iter: 693 loss: 1.04428722e-07
Iter: 694 loss: 1.03222845e-07
Iter: 695 loss: 1.03050567e-07
Iter: 696 loss: 1.03050056e-07
Iter: 697 loss: 1.02973111e-07
Iter: 698 loss: 1.02835237e-07
Iter: 699 loss: 1.02837078e-07
Iter: 700 loss: 1.02731157e-07
Iter: 701 loss: 1.0363496e-07
Iter: 702 loss: 1.02724663e-07
Iter: 703 loss: 1.02605327e-07
Iter: 704 loss: 1.02948164e-07
Iter: 705 loss: 1.02570937e-07
Iter: 706 loss: 1.02471063e-07
Iter: 707 loss: 1.02677824e-07
Iter: 708 loss: 1.02431478e-07
Iter: 709 loss: 1.02289164e-07
Iter: 710 loss: 1.0259491e-07
Iter: 711 loss: 1.02225854e-07
Iter: 712 loss: 1.02109318e-07
Iter: 713 loss: 1.0203469e-07
Iter: 714 loss: 1.01990977e-07
Iter: 715 loss: 1.01809803e-07
Iter: 716 loss: 1.02804577e-07
Iter: 717 loss: 1.01784181e-07
Iter: 718 loss: 1.01686396e-07
Iter: 719 loss: 1.01687519e-07
Iter: 720 loss: 1.01614162e-07
Iter: 721 loss: 1.01475948e-07
Iter: 722 loss: 1.04269006e-07
Iter: 723 loss: 1.01474726e-07
Iter: 724 loss: 1.01340433e-07
Iter: 725 loss: 1.01423979e-07
Iter: 726 loss: 1.01247011e-07
Iter: 727 loss: 1.01158783e-07
Iter: 728 loss: 1.01147243e-07
Iter: 729 loss: 1.01035084e-07
Iter: 730 loss: 1.00815548e-07
Iter: 731 loss: 1.0476893e-07
Iter: 732 loss: 1.00819236e-07
Iter: 733 loss: 1.00613626e-07
Iter: 734 loss: 1.01424362e-07
Iter: 735 loss: 1.00560477e-07
Iter: 736 loss: 1.00465371e-07
Iter: 737 loss: 1.00447124e-07
Iter: 738 loss: 1.00376894e-07
Iter: 739 loss: 1.00256095e-07
Iter: 740 loss: 1.00255136e-07
Iter: 741 loss: 1.00141577e-07
Iter: 742 loss: 1.00138124e-07
Iter: 743 loss: 1.00072256e-07
Iter: 744 loss: 9.99338283e-08
Iter: 745 loss: 1.02653551e-07
Iter: 746 loss: 9.99335796e-08
Iter: 747 loss: 9.97832643e-08
Iter: 748 loss: 1.00529803e-07
Iter: 749 loss: 9.97542884e-08
Iter: 750 loss: 9.96430458e-08
Iter: 751 loss: 1.00331107e-07
Iter: 752 loss: 9.96282523e-08
Iter: 753 loss: 9.95075e-08
Iter: 754 loss: 9.96063108e-08
Iter: 755 loss: 9.94303875e-08
Iter: 756 loss: 9.93176954e-08
Iter: 757 loss: 9.91833105e-08
Iter: 758 loss: 9.91709896e-08
Iter: 759 loss: 9.90139313e-08
Iter: 760 loss: 1.001155e-07
Iter: 761 loss: 9.89996352e-08
Iter: 762 loss: 9.88564537e-08
Iter: 763 loss: 1.00295317e-07
Iter: 764 loss: 9.88470248e-08
Iter: 765 loss: 9.87733e-08
Iter: 766 loss: 9.85645272e-08
Iter: 767 loss: 1.00491278e-07
Iter: 768 loss: 9.85381661e-08
Iter: 769 loss: 9.84772441e-08
Iter: 770 loss: 9.84253461e-08
Iter: 771 loss: 9.83272201e-08
Iter: 772 loss: 9.82983792e-08
Iter: 773 loss: 9.82466e-08
Iter: 774 loss: 9.81678312e-08
Iter: 775 loss: 9.81680159e-08
Iter: 776 loss: 9.80875043e-08
Iter: 777 loss: 9.79581145e-08
Iter: 778 loss: 9.79573045e-08
Iter: 779 loss: 9.78269e-08
Iter: 780 loss: 9.82414861e-08
Iter: 781 loss: 9.7787435e-08
Iter: 782 loss: 9.76547909e-08
Iter: 783 loss: 9.81352741e-08
Iter: 784 loss: 9.76261845e-08
Iter: 785 loss: 9.74829746e-08
Iter: 786 loss: 9.81670141e-08
Iter: 787 loss: 9.74647065e-08
Iter: 788 loss: 9.73581393e-08
Iter: 789 loss: 9.71767378e-08
Iter: 790 loss: 9.71746204e-08
Iter: 791 loss: 9.69905258e-08
Iter: 792 loss: 9.73259091e-08
Iter: 793 loss: 9.69087708e-08
Iter: 794 loss: 9.68996e-08
Iter: 795 loss: 9.68467049e-08
Iter: 796 loss: 9.6759706e-08
Iter: 797 loss: 9.66216049e-08
Iter: 798 loss: 9.66210507e-08
Iter: 799 loss: 9.64730091e-08
Iter: 800 loss: 9.64002496e-08
Iter: 801 loss: 9.63375228e-08
Iter: 802 loss: 9.61770255e-08
Iter: 803 loss: 9.61673123e-08
Iter: 804 loss: 9.60905453e-08
Iter: 805 loss: 9.59868061e-08
Iter: 806 loss: 9.59835873e-08
Iter: 807 loss: 9.58589226e-08
Iter: 808 loss: 9.58582e-08
Iter: 809 loss: 9.57883373e-08
Iter: 810 loss: 9.56849817e-08
Iter: 811 loss: 9.56829211e-08
Iter: 812 loss: 9.55501065e-08
Iter: 813 loss: 9.57990096e-08
Iter: 814 loss: 9.54912664e-08
Iter: 815 loss: 9.53702823e-08
Iter: 816 loss: 9.53670138e-08
Iter: 817 loss: 9.52573131e-08
Iter: 818 loss: 9.5060642e-08
Iter: 819 loss: 9.50623331e-08
Iter: 820 loss: 9.47974712e-08
Iter: 821 loss: 9.48439691e-08
Iter: 822 loss: 9.45944265e-08
Iter: 823 loss: 9.4409927e-08
Iter: 824 loss: 9.67060103e-08
Iter: 825 loss: 9.44091099e-08
Iter: 826 loss: 9.43178691e-08
Iter: 827 loss: 9.43140392e-08
Iter: 828 loss: 9.42428571e-08
Iter: 829 loss: 9.4075908e-08
Iter: 830 loss: 9.59422195e-08
Iter: 831 loss: 9.40652356e-08
Iter: 832 loss: 9.40200664e-08
Iter: 833 loss: 9.39856051e-08
Iter: 834 loss: 9.3906884e-08
Iter: 835 loss: 9.3756384e-08
Iter: 836 loss: 9.65562776e-08
Iter: 837 loss: 9.37564835e-08
Iter: 838 loss: 9.36330409e-08
Iter: 839 loss: 9.51659587e-08
Iter: 840 loss: 9.36370128e-08
Iter: 841 loss: 9.34715416e-08
Iter: 842 loss: 9.35552293e-08
Iter: 843 loss: 9.33724635e-08
Iter: 844 loss: 9.32553874e-08
Iter: 845 loss: 9.32052302e-08
Iter: 846 loss: 9.31499073e-08
Iter: 847 loss: 9.30662765e-08
Iter: 848 loss: 9.30584392e-08
Iter: 849 loss: 9.29932185e-08
Iter: 850 loss: 9.30758901e-08
Iter: 851 loss: 9.29572224e-08
Iter: 852 loss: 9.28911277e-08
Iter: 853 loss: 9.28833686e-08
Iter: 854 loss: 9.28309589e-08
Iter: 855 loss: 9.27246333e-08
Iter: 856 loss: 9.26169363e-08
Iter: 857 loss: 9.25926287e-08
Iter: 858 loss: 9.24767e-08
Iter: 859 loss: 9.24681842e-08
Iter: 860 loss: 9.23456156e-08
Iter: 861 loss: 9.22877064e-08
Iter: 862 loss: 9.2218535e-08
Iter: 863 loss: 9.20906658e-08
Iter: 864 loss: 9.22526269e-08
Iter: 865 loss: 9.20241661e-08
Iter: 866 loss: 9.19545684e-08
Iter: 867 loss: 9.19371814e-08
Iter: 868 loss: 9.18963892e-08
Iter: 869 loss: 9.17845e-08
Iter: 870 loss: 9.23003114e-08
Iter: 871 loss: 9.17376894e-08
Iter: 872 loss: 9.16984249e-08
Iter: 873 loss: 9.16487579e-08
Iter: 874 loss: 9.15778884e-08
Iter: 875 loss: 9.14565561e-08
Iter: 876 loss: 9.14568261e-08
Iter: 877 loss: 9.13036331e-08
Iter: 878 loss: 9.27289e-08
Iter: 879 loss: 9.12982543e-08
Iter: 880 loss: 9.12044484e-08
Iter: 881 loss: 9.12665499e-08
Iter: 882 loss: 9.11477e-08
Iter: 883 loss: 9.09844431e-08
Iter: 884 loss: 9.11447e-08
Iter: 885 loss: 9.08985811e-08
Iter: 886 loss: 9.07972364e-08
Iter: 887 loss: 9.09242885e-08
Iter: 888 loss: 9.07498929e-08
Iter: 889 loss: 9.06484274e-08
Iter: 890 loss: 9.11495448e-08
Iter: 891 loss: 9.06317297e-08
Iter: 892 loss: 9.05193929e-08
Iter: 893 loss: 9.15790039e-08
Iter: 894 loss: 9.05090474e-08
Iter: 895 loss: 9.0430369e-08
Iter: 896 loss: 9.0272593e-08
Iter: 897 loss: 9.3320665e-08
Iter: 898 loss: 9.02735593e-08
Iter: 899 loss: 9.01482764e-08
Iter: 900 loss: 9.01510617e-08
Iter: 901 loss: 9.00633523e-08
Iter: 902 loss: 8.98803592e-08
Iter: 903 loss: 9.24185e-08
Iter: 904 loss: 8.98704045e-08
Iter: 905 loss: 8.96988581e-08
Iter: 906 loss: 9.0714579e-08
Iter: 907 loss: 8.96770587e-08
Iter: 908 loss: 8.95626471e-08
Iter: 909 loss: 8.95594638e-08
Iter: 910 loss: 8.95112393e-08
Iter: 911 loss: 8.93991512e-08
Iter: 912 loss: 9.07049298e-08
Iter: 913 loss: 8.93880525e-08
Iter: 914 loss: 8.92558631e-08
Iter: 915 loss: 9.09783893e-08
Iter: 916 loss: 8.92562042e-08
Iter: 917 loss: 8.91392204e-08
Iter: 918 loss: 8.91610341e-08
Iter: 919 loss: 8.90548861e-08
Iter: 920 loss: 8.89035761e-08
Iter: 921 loss: 8.89002365e-08
Iter: 922 loss: 8.87761331e-08
Iter: 923 loss: 8.86108324e-08
Iter: 924 loss: 8.86709586e-08
Iter: 925 loss: 8.8513076e-08
Iter: 926 loss: 8.82899e-08
Iter: 927 loss: 8.90519729e-08
Iter: 928 loss: 8.82379e-08
Iter: 929 loss: 8.81951578e-08
Iter: 930 loss: 8.81561206e-08
Iter: 931 loss: 8.8066372e-08
Iter: 932 loss: 8.79445423e-08
Iter: 933 loss: 8.79455939e-08
Iter: 934 loss: 8.78165949e-08
Iter: 935 loss: 8.77465851e-08
Iter: 936 loss: 8.76870132e-08
Iter: 937 loss: 8.76901609e-08
Iter: 938 loss: 8.76076456e-08
Iter: 939 loss: 8.75462334e-08
Iter: 940 loss: 8.73981065e-08
Iter: 941 loss: 8.93459315e-08
Iter: 942 loss: 8.73873e-08
Iter: 943 loss: 8.72701804e-08
Iter: 944 loss: 8.78511699e-08
Iter: 945 loss: 8.72524e-08
Iter: 946 loss: 8.71212649e-08
Iter: 947 loss: 8.83403857e-08
Iter: 948 loss: 8.71183659e-08
Iter: 949 loss: 8.70534649e-08
Iter: 950 loss: 8.69037748e-08
Iter: 951 loss: 8.92209684e-08
Iter: 952 loss: 8.69027446e-08
Iter: 953 loss: 8.67699086e-08
Iter: 954 loss: 8.81212188e-08
Iter: 955 loss: 8.67663488e-08
Iter: 956 loss: 8.66809415e-08
Iter: 957 loss: 8.73131185e-08
Iter: 958 loss: 8.66741914e-08
Iter: 959 loss: 8.65923369e-08
Iter: 960 loss: 8.66592e-08
Iter: 961 loss: 8.65321752e-08
Iter: 962 loss: 8.64587122e-08
Iter: 963 loss: 8.64091e-08
Iter: 964 loss: 8.63706404e-08
Iter: 965 loss: 8.62833147e-08
Iter: 966 loss: 8.74831088e-08
Iter: 967 loss: 8.62832223e-08
Iter: 968 loss: 8.61890541e-08
Iter: 969 loss: 8.61386695e-08
Iter: 970 loss: 8.60893863e-08
Iter: 971 loss: 8.59907914e-08
Iter: 972 loss: 8.61296172e-08
Iter: 973 loss: 8.59381473e-08
Iter: 974 loss: 8.58230464e-08
Iter: 975 loss: 8.73647252e-08
Iter: 976 loss: 8.58212701e-08
Iter: 977 loss: 8.57652793e-08
Iter: 978 loss: 8.56417302e-08
Iter: 979 loss: 8.77745947e-08
Iter: 980 loss: 8.56451905e-08
Iter: 981 loss: 8.55603162e-08
Iter: 982 loss: 8.64377441e-08
Iter: 983 loss: 8.55593782e-08
Iter: 984 loss: 8.5453955e-08
Iter: 985 loss: 8.58473612e-08
Iter: 986 loss: 8.54373923e-08
Iter: 987 loss: 8.53694502e-08
Iter: 988 loss: 8.52582644e-08
Iter: 989 loss: 8.52589608e-08
Iter: 990 loss: 8.51247e-08
Iter: 991 loss: 8.599838e-08
Iter: 992 loss: 8.51085602e-08
Iter: 993 loss: 8.50056381e-08
Iter: 994 loss: 8.58813323e-08
Iter: 995 loss: 8.499903e-08
Iter: 996 loss: 8.49053734e-08
Iter: 997 loss: 8.48481179e-08
Iter: 998 loss: 8.48026929e-08
Iter: 999 loss: 8.46901e-08
Iter: 1000 loss: 8.49548556e-08
Iter: 1001 loss: 8.46489101e-08
Iter: 1002 loss: 8.45961665e-08
Iter: 1003 loss: 8.45873558e-08
Iter: 1004 loss: 8.45347756e-08
Iter: 1005 loss: 8.44041494e-08
Iter: 1006 loss: 8.57563e-08
Iter: 1007 loss: 8.43992325e-08
Iter: 1008 loss: 8.42938235e-08
Iter: 1009 loss: 8.42918126e-08
Iter: 1010 loss: 8.41798311e-08
Iter: 1011 loss: 8.40581222e-08
Iter: 1012 loss: 8.40417727e-08
Iter: 1013 loss: 8.38960545e-08
Iter: 1014 loss: 8.39624477e-08
Iter: 1015 loss: 8.38051335e-08
Iter: 1016 loss: 8.38143777e-08
Iter: 1017 loss: 8.37423215e-08
Iter: 1018 loss: 8.36951131e-08
Iter: 1019 loss: 8.35767793e-08
Iter: 1020 loss: 8.5195e-08
Iter: 1021 loss: 8.3575884e-08
Iter: 1022 loss: 8.34807707e-08
Iter: 1023 loss: 8.3522437e-08
Iter: 1024 loss: 8.34143208e-08
Iter: 1025 loss: 8.33118889e-08
Iter: 1026 loss: 8.35597191e-08
Iter: 1027 loss: 8.32822e-08
Iter: 1028 loss: 8.31860874e-08
Iter: 1029 loss: 8.43782573e-08
Iter: 1030 loss: 8.31842613e-08
Iter: 1031 loss: 8.30963245e-08
Iter: 1032 loss: 8.31121696e-08
Iter: 1033 loss: 8.30261087e-08
Iter: 1034 loss: 8.2925439e-08
Iter: 1035 loss: 8.3128576e-08
Iter: 1036 loss: 8.28781452e-08
Iter: 1037 loss: 8.28171594e-08
Iter: 1038 loss: 8.30572162e-08
Iter: 1039 loss: 8.28099687e-08
Iter: 1040 loss: 8.27307574e-08
Iter: 1041 loss: 8.29593816e-08
Iter: 1042 loss: 8.2704446e-08
Iter: 1043 loss: 8.26594473e-08
Iter: 1044 loss: 8.25747577e-08
Iter: 1045 loss: 8.25727255e-08
Iter: 1046 loss: 8.25082793e-08
Iter: 1047 loss: 8.25044e-08
Iter: 1048 loss: 8.24354345e-08
Iter: 1049 loss: 8.23161557e-08
Iter: 1050 loss: 8.44128039e-08
Iter: 1051 loss: 8.23119066e-08
Iter: 1052 loss: 8.22425221e-08
Iter: 1053 loss: 8.22313808e-08
Iter: 1054 loss: 8.21533632e-08
Iter: 1055 loss: 8.2046995e-08
Iter: 1056 loss: 8.20426607e-08
Iter: 1057 loss: 8.19577366e-08
Iter: 1058 loss: 8.20143597e-08
Iter: 1059 loss: 8.19136e-08
Iter: 1060 loss: 8.18209429e-08
Iter: 1061 loss: 8.18898513e-08
Iter: 1062 loss: 8.17657693e-08
Iter: 1063 loss: 8.16706347e-08
Iter: 1064 loss: 8.16710823e-08
Iter: 1065 loss: 8.15979746e-08
Iter: 1066 loss: 8.1506542e-08
Iter: 1067 loss: 8.14967223e-08
Iter: 1068 loss: 8.13897e-08
Iter: 1069 loss: 8.13290555e-08
Iter: 1070 loss: 8.12800067e-08
Iter: 1071 loss: 8.12265313e-08
Iter: 1072 loss: 8.119612e-08
Iter: 1073 loss: 8.11271548e-08
Iter: 1074 loss: 8.11545036e-08
Iter: 1075 loss: 8.10925727e-08
Iter: 1076 loss: 8.10365393e-08
Iter: 1077 loss: 8.11272e-08
Iter: 1078 loss: 8.09993e-08
Iter: 1079 loss: 8.09299578e-08
Iter: 1080 loss: 8.12092154e-08
Iter: 1081 loss: 8.09104534e-08
Iter: 1082 loss: 8.08303611e-08
Iter: 1083 loss: 8.08599694e-08
Iter: 1084 loss: 8.07676201e-08
Iter: 1085 loss: 8.07186566e-08
Iter: 1086 loss: 8.07190332e-08
Iter: 1087 loss: 8.06684497e-08
Iter: 1088 loss: 8.05648241e-08
Iter: 1089 loss: 8.24839219e-08
Iter: 1090 loss: 8.05638578e-08
Iter: 1091 loss: 8.04744644e-08
Iter: 1092 loss: 8.03822502e-08
Iter: 1093 loss: 8.03606284e-08
Iter: 1094 loss: 8.02597384e-08
Iter: 1095 loss: 8.02570099e-08
Iter: 1096 loss: 8.01766262e-08
Iter: 1097 loss: 8.0940822e-08
Iter: 1098 loss: 8.01735851e-08
Iter: 1099 loss: 8.01307678e-08
Iter: 1100 loss: 8.00285136e-08
Iter: 1101 loss: 8.10072649e-08
Iter: 1102 loss: 8.00143667e-08
Iter: 1103 loss: 7.99428648e-08
Iter: 1104 loss: 7.99401647e-08
Iter: 1105 loss: 7.98753774e-08
Iter: 1106 loss: 8.02026108e-08
Iter: 1107 loss: 7.98586655e-08
Iter: 1108 loss: 7.98130202e-08
Iter: 1109 loss: 7.98702615e-08
Iter: 1110 loss: 7.97920734e-08
Iter: 1111 loss: 7.9739138e-08
Iter: 1112 loss: 7.99159139e-08
Iter: 1113 loss: 7.97253676e-08
Iter: 1114 loss: 7.96676645e-08
Iter: 1115 loss: 7.97130824e-08
Iter: 1116 loss: 7.96312349e-08
Iter: 1117 loss: 7.95604649e-08
Iter: 1118 loss: 7.97467905e-08
Iter: 1119 loss: 7.95317447e-08
Iter: 1120 loss: 7.94418895e-08
Iter: 1121 loss: 7.97767257e-08
Iter: 1122 loss: 7.94237e-08
Iter: 1123 loss: 7.93685544e-08
Iter: 1124 loss: 7.9254491e-08
Iter: 1125 loss: 8.13158678e-08
Iter: 1126 loss: 7.92526151e-08
Iter: 1127 loss: 7.916789e-08
Iter: 1128 loss: 7.9740623e-08
Iter: 1129 loss: 7.91643089e-08
Iter: 1130 loss: 7.91318939e-08
Iter: 1131 loss: 7.91182231e-08
Iter: 1132 loss: 7.90878e-08
Iter: 1133 loss: 7.89983261e-08
Iter: 1134 loss: 7.95509933e-08
Iter: 1135 loss: 7.89777e-08
Iter: 1136 loss: 7.88710679e-08
Iter: 1137 loss: 7.93401682e-08
Iter: 1138 loss: 7.88466394e-08
Iter: 1139 loss: 7.87924108e-08
Iter: 1140 loss: 7.87814e-08
Iter: 1141 loss: 7.8750908e-08
Iter: 1142 loss: 7.87134e-08
Iter: 1143 loss: 7.87054404e-08
Iter: 1144 loss: 7.86463e-08
Iter: 1145 loss: 7.89681422e-08
Iter: 1146 loss: 7.86400349e-08
Iter: 1147 loss: 7.85925636e-08
Iter: 1148 loss: 7.86764218e-08
Iter: 1149 loss: 7.85677301e-08
Iter: 1150 loss: 7.85068e-08
Iter: 1151 loss: 7.86543168e-08
Iter: 1152 loss: 7.84875596e-08
Iter: 1153 loss: 7.84307161e-08
Iter: 1154 loss: 7.88388093e-08
Iter: 1155 loss: 7.84303964e-08
Iter: 1156 loss: 7.83869893e-08
Iter: 1157 loss: 7.8286277e-08
Iter: 1158 loss: 7.95227137e-08
Iter: 1159 loss: 7.82801237e-08
Iter: 1160 loss: 7.81682559e-08
Iter: 1161 loss: 7.84285845e-08
Iter: 1162 loss: 7.81277e-08
Iter: 1163 loss: 7.81203937e-08
Iter: 1164 loss: 7.80882914e-08
Iter: 1165 loss: 7.80507605e-08
Iter: 1166 loss: 7.80100748e-08
Iter: 1167 loss: 7.80008946e-08
Iter: 1168 loss: 7.79499842e-08
Iter: 1169 loss: 7.78980649e-08
Iter: 1170 loss: 7.78888705e-08
Iter: 1171 loss: 7.78473677e-08
Iter: 1172 loss: 7.78252129e-08
Iter: 1173 loss: 7.77856073e-08
Iter: 1174 loss: 7.77244793e-08
Iter: 1175 loss: 7.7720884e-08
Iter: 1176 loss: 7.76525297e-08
Iter: 1177 loss: 7.849615e-08
Iter: 1178 loss: 7.76559546e-08
Iter: 1179 loss: 7.7602806e-08
Iter: 1180 loss: 7.7646007e-08
Iter: 1181 loss: 7.75694815e-08
Iter: 1182 loss: 7.75128086e-08
Iter: 1183 loss: 7.76800775e-08
Iter: 1184 loss: 7.74917908e-08
Iter: 1185 loss: 7.74411433e-08
Iter: 1186 loss: 7.79182727e-08
Iter: 1187 loss: 7.7434926e-08
Iter: 1188 loss: 7.73948e-08
Iter: 1189 loss: 7.73411273e-08
Iter: 1190 loss: 7.73414683e-08
Iter: 1191 loss: 7.72852715e-08
Iter: 1192 loss: 7.72504762e-08
Iter: 1193 loss: 7.7229295e-08
Iter: 1194 loss: 7.71517819e-08
Iter: 1195 loss: 7.7705586e-08
Iter: 1196 loss: 7.71450743e-08
Iter: 1197 loss: 7.70803652e-08
Iter: 1198 loss: 7.78757538e-08
Iter: 1199 loss: 7.70798536e-08
Iter: 1200 loss: 7.70527606e-08
Iter: 1201 loss: 7.69652502e-08
Iter: 1202 loss: 7.7433981e-08
Iter: 1203 loss: 7.69425e-08
Iter: 1204 loss: 7.68601254e-08
Iter: 1205 loss: 7.68613e-08
Iter: 1206 loss: 7.67756489e-08
Iter: 1207 loss: 7.70082096e-08
Iter: 1208 loss: 7.67528192e-08
Iter: 1209 loss: 7.67093553e-08
Iter: 1210 loss: 7.68415447e-08
Iter: 1211 loss: 7.66927855e-08
Iter: 1212 loss: 7.66288366e-08
Iter: 1213 loss: 7.66875132e-08
Iter: 1214 loss: 7.65984041e-08
Iter: 1215 loss: 7.65332118e-08
Iter: 1216 loss: 7.68076944e-08
Iter: 1217 loss: 7.65277761e-08
Iter: 1218 loss: 7.64792532e-08
Iter: 1219 loss: 7.68717783e-08
Iter: 1220 loss: 7.64800774e-08
Iter: 1221 loss: 7.64349153e-08
Iter: 1222 loss: 7.64290178e-08
Iter: 1223 loss: 7.64025074e-08
Iter: 1224 loss: 7.63519168e-08
Iter: 1225 loss: 7.62498757e-08
Iter: 1226 loss: 7.81943683e-08
Iter: 1227 loss: 7.62507426e-08
Iter: 1228 loss: 7.61345476e-08
Iter: 1229 loss: 7.68250459e-08
Iter: 1230 loss: 7.61209691e-08
Iter: 1231 loss: 7.60847172e-08
Iter: 1232 loss: 7.60664136e-08
Iter: 1233 loss: 7.60335581e-08
Iter: 1234 loss: 7.59394752e-08
Iter: 1235 loss: 7.69514941e-08
Iter: 1236 loss: 7.59435324e-08
Iter: 1237 loss: 7.58587362e-08
Iter: 1238 loss: 7.61869785e-08
Iter: 1239 loss: 7.58441558e-08
Iter: 1240 loss: 7.57730945e-08
Iter: 1241 loss: 7.68568924e-08
Iter: 1242 loss: 7.57740253e-08
Iter: 1243 loss: 7.57403e-08
Iter: 1244 loss: 7.56973151e-08
Iter: 1245 loss: 7.56914e-08
Iter: 1246 loss: 7.56132081e-08
Iter: 1247 loss: 7.62202248e-08
Iter: 1248 loss: 7.56102665e-08
Iter: 1249 loss: 7.55568905e-08
Iter: 1250 loss: 7.55510854e-08
Iter: 1251 loss: 7.55239853e-08
Iter: 1252 loss: 7.54649605e-08
Iter: 1253 loss: 7.54644844e-08
Iter: 1254 loss: 7.54221503e-08
Iter: 1255 loss: 7.54982068e-08
Iter: 1256 loss: 7.5403328e-08
Iter: 1257 loss: 7.53560059e-08
Iter: 1258 loss: 7.52739311e-08
Iter: 1259 loss: 7.677383e-08
Iter: 1260 loss: 7.52769651e-08
Iter: 1261 loss: 7.51911386e-08
Iter: 1262 loss: 7.54142491e-08
Iter: 1263 loss: 7.51606422e-08
Iter: 1264 loss: 7.51201767e-08
Iter: 1265 loss: 7.51087867e-08
Iter: 1266 loss: 7.50491509e-08
Iter: 1267 loss: 7.49651434e-08
Iter: 1268 loss: 7.49650724e-08
Iter: 1269 loss: 7.48660227e-08
Iter: 1270 loss: 7.48378284e-08
Iter: 1271 loss: 7.47826334e-08
Iter: 1272 loss: 7.48192264e-08
Iter: 1273 loss: 7.47464668e-08
Iter: 1274 loss: 7.47157287e-08
Iter: 1275 loss: 7.46527533e-08
Iter: 1276 loss: 7.56337855e-08
Iter: 1277 loss: 7.46546789e-08
Iter: 1278 loss: 7.45934e-08
Iter: 1279 loss: 7.51249587e-08
Iter: 1280 loss: 7.45861612e-08
Iter: 1281 loss: 7.45226458e-08
Iter: 1282 loss: 7.45078808e-08
Iter: 1283 loss: 7.44686588e-08
Iter: 1284 loss: 7.44008872e-08
Iter: 1285 loss: 7.46689608e-08
Iter: 1286 loss: 7.43902291e-08
Iter: 1287 loss: 7.43417701e-08
Iter: 1288 loss: 7.49696909e-08
Iter: 1289 loss: 7.43391837e-08
Iter: 1290 loss: 7.4298427e-08
Iter: 1291 loss: 7.42175885e-08
Iter: 1292 loss: 7.60151551e-08
Iter: 1293 loss: 7.42147108e-08
Iter: 1294 loss: 7.41422923e-08
Iter: 1295 loss: 7.42619619e-08
Iter: 1296 loss: 7.41096e-08
Iter: 1297 loss: 7.40523802e-08
Iter: 1298 loss: 7.44617168e-08
Iter: 1299 loss: 7.40512e-08
Iter: 1300 loss: 7.40006456e-08
Iter: 1301 loss: 7.44542277e-08
Iter: 1302 loss: 7.39994874e-08
Iter: 1303 loss: 7.39655377e-08
Iter: 1304 loss: 7.38844221e-08
Iter: 1305 loss: 7.50555387e-08
Iter: 1306 loss: 7.38844079e-08
Iter: 1307 loss: 7.38029371e-08
Iter: 1308 loss: 7.38194075e-08
Iter: 1309 loss: 7.37406722e-08
Iter: 1310 loss: 7.37177e-08
Iter: 1311 loss: 7.37022532e-08
Iter: 1312 loss: 7.3643335e-08
Iter: 1313 loss: 7.35824699e-08
Iter: 1314 loss: 7.35712149e-08
Iter: 1315 loss: 7.35287529e-08
Iter: 1316 loss: 7.35340109e-08
Iter: 1317 loss: 7.34906251e-08
Iter: 1318 loss: 7.34581249e-08
Iter: 1319 loss: 7.34586081e-08
Iter: 1320 loss: 7.34153289e-08
Iter: 1321 loss: 7.33441e-08
Iter: 1322 loss: 7.46940216e-08
Iter: 1323 loss: 7.33412e-08
Iter: 1324 loss: 7.33153769e-08
Iter: 1325 loss: 7.3304335e-08
Iter: 1326 loss: 7.32835517e-08
Iter: 1327 loss: 7.32324e-08
Iter: 1328 loss: 7.43104209e-08
Iter: 1329 loss: 7.32306233e-08
Iter: 1330 loss: 7.31762384e-08
Iter: 1331 loss: 7.31891561e-08
Iter: 1332 loss: 7.31351e-08
Iter: 1333 loss: 7.30845358e-08
Iter: 1334 loss: 7.3097624e-08
Iter: 1335 loss: 7.30547143e-08
Iter: 1336 loss: 7.2980626e-08
Iter: 1337 loss: 7.35256052e-08
Iter: 1338 loss: 7.29814076e-08
Iter: 1339 loss: 7.29393363e-08
Iter: 1340 loss: 7.28844185e-08
Iter: 1341 loss: 7.28856691e-08
Iter: 1342 loss: 7.28305452e-08
Iter: 1343 loss: 7.3097624e-08
Iter: 1344 loss: 7.28173575e-08
Iter: 1345 loss: 7.27751939e-08
Iter: 1346 loss: 7.28635783e-08
Iter: 1347 loss: 7.27557108e-08
Iter: 1348 loss: 7.27037843e-08
Iter: 1349 loss: 7.2729712e-08
Iter: 1350 loss: 7.2679569e-08
Iter: 1351 loss: 7.26191303e-08
Iter: 1352 loss: 7.26543945e-08
Iter: 1353 loss: 7.2587568e-08
Iter: 1354 loss: 7.25295592e-08
Iter: 1355 loss: 7.31659213e-08
Iter: 1356 loss: 7.25292253e-08
Iter: 1357 loss: 7.24791391e-08
Iter: 1358 loss: 7.24926e-08
Iter: 1359 loss: 7.24457934e-08
Iter: 1360 loss: 7.23960483e-08
Iter: 1361 loss: 7.23962685e-08
Iter: 1362 loss: 7.23764657e-08
Iter: 1363 loss: 7.23259745e-08
Iter: 1364 loss: 7.28633438e-08
Iter: 1365 loss: 7.23244824e-08
Iter: 1366 loss: 7.22823899e-08
Iter: 1367 loss: 7.27824698e-08
Iter: 1368 loss: 7.22801872e-08
Iter: 1369 loss: 7.22391889e-08
Iter: 1370 loss: 7.2309831e-08
Iter: 1371 loss: 7.22225906e-08
Iter: 1372 loss: 7.2188044e-08
Iter: 1373 loss: 7.22289499e-08
Iter: 1374 loss: 7.21684472e-08
Iter: 1375 loss: 7.2114716e-08
Iter: 1376 loss: 7.21671398e-08
Iter: 1377 loss: 7.20887599e-08
Iter: 1378 loss: 7.20152542e-08
Iter: 1379 loss: 7.21682056e-08
Iter: 1380 loss: 7.19884667e-08
Iter: 1381 loss: 7.19288948e-08
Iter: 1382 loss: 7.24822726e-08
Iter: 1383 loss: 7.19323694e-08
Iter: 1384 loss: 7.1897432e-08
Iter: 1385 loss: 7.18469622e-08
Iter: 1386 loss: 7.1841221e-08
Iter: 1387 loss: 7.17951423e-08
Iter: 1388 loss: 7.25757587e-08
Iter: 1389 loss: 7.17975e-08
Iter: 1390 loss: 7.17456956e-08
Iter: 1391 loss: 7.18282109e-08
Iter: 1392 loss: 7.17252746e-08
Iter: 1393 loss: 7.1694032e-08
Iter: 1394 loss: 7.21316e-08
Iter: 1395 loss: 7.16915096e-08
Iter: 1396 loss: 7.16679338e-08
Iter: 1397 loss: 7.16080706e-08
Iter: 1398 loss: 7.29054932e-08
Iter: 1399 loss: 7.16090369e-08
Iter: 1400 loss: 7.15440365e-08
Iter: 1401 loss: 7.1513611e-08
Iter: 1402 loss: 7.14801303e-08
Iter: 1403 loss: 7.14730248e-08
Iter: 1404 loss: 7.14349113e-08
Iter: 1405 loss: 7.14038677e-08
Iter: 1406 loss: 7.13421713e-08
Iter: 1407 loss: 7.19850703e-08
Iter: 1408 loss: 7.13377517e-08
Iter: 1409 loss: 7.1280617e-08
Iter: 1410 loss: 7.12784569e-08
Iter: 1411 loss: 7.12224946e-08
Iter: 1412 loss: 7.12136909e-08
Iter: 1413 loss: 7.11856742e-08
Iter: 1414 loss: 7.11153945e-08
Iter: 1415 loss: 7.14920674e-08
Iter: 1416 loss: 7.11034787e-08
Iter: 1417 loss: 7.10521562e-08
Iter: 1418 loss: 7.10592047e-08
Iter: 1419 loss: 7.10109e-08
Iter: 1420 loss: 7.0952126e-08
Iter: 1421 loss: 7.12153749e-08
Iter: 1422 loss: 7.09461077e-08
Iter: 1423 loss: 7.0895176e-08
Iter: 1424 loss: 7.10581318e-08
Iter: 1425 loss: 7.08797927e-08
Iter: 1426 loss: 7.08323427e-08
Iter: 1427 loss: 7.10731882e-08
Iter: 1428 loss: 7.08269425e-08
Iter: 1429 loss: 7.07877561e-08
Iter: 1430 loss: 7.08151191e-08
Iter: 1431 loss: 7.07585741e-08
Iter: 1432 loss: 7.07150036e-08
Iter: 1433 loss: 7.06430328e-08
Iter: 1434 loss: 7.06410646e-08
Iter: 1435 loss: 7.05479124e-08
Iter: 1436 loss: 7.08004606e-08
Iter: 1437 loss: 7.05113692e-08
Iter: 1438 loss: 7.04966254e-08
Iter: 1439 loss: 7.04620504e-08
Iter: 1440 loss: 7.04394e-08
Iter: 1441 loss: 7.03765437e-08
Iter: 1442 loss: 7.09066299e-08
Iter: 1443 loss: 7.03797625e-08
Iter: 1444 loss: 7.03247451e-08
Iter: 1445 loss: 7.07215833e-08
Iter: 1446 loss: 7.03192e-08
Iter: 1447 loss: 7.02666796e-08
Iter: 1448 loss: 7.06073635e-08
Iter: 1449 loss: 7.02590128e-08
Iter: 1450 loss: 7.02256528e-08
Iter: 1451 loss: 7.0248305e-08
Iter: 1452 loss: 7.0209154e-08
Iter: 1453 loss: 7.0145461e-08
Iter: 1454 loss: 7.01446439e-08
Iter: 1455 loss: 7.01011871e-08
Iter: 1456 loss: 7.00440523e-08
Iter: 1457 loss: 7.04000698e-08
Iter: 1458 loss: 7.00374585e-08
Iter: 1459 loss: 6.99804517e-08
Iter: 1460 loss: 7.02005067e-08
Iter: 1461 loss: 6.99725149e-08
Iter: 1462 loss: 6.9941521e-08
Iter: 1463 loss: 7.04313621e-08
Iter: 1464 loss: 6.99399223e-08
Iter: 1465 loss: 6.99222085e-08
Iter: 1466 loss: 6.98848055e-08
Iter: 1467 loss: 7.05091594e-08
Iter: 1468 loss: 6.98862266e-08
Iter: 1469 loss: 6.98406e-08
Iter: 1470 loss: 6.98782685e-08
Iter: 1471 loss: 6.9815e-08
Iter: 1472 loss: 6.97620237e-08
Iter: 1473 loss: 6.97642051e-08
Iter: 1474 loss: 6.97093441e-08
Iter: 1475 loss: 6.96276743e-08
Iter: 1476 loss: 6.97102678e-08
Iter: 1477 loss: 6.95841891e-08
Iter: 1478 loss: 6.95019153e-08
Iter: 1479 loss: 7.00828195e-08
Iter: 1480 loss: 6.94873279e-08
Iter: 1481 loss: 6.94895519e-08
Iter: 1482 loss: 6.94630913e-08
Iter: 1483 loss: 6.94404534e-08
Iter: 1484 loss: 6.93823665e-08
Iter: 1485 loss: 6.97811302e-08
Iter: 1486 loss: 6.93707491e-08
Iter: 1487 loss: 6.93140763e-08
Iter: 1488 loss: 7.01394356e-08
Iter: 1489 loss: 6.93119162e-08
Iter: 1490 loss: 6.92712661e-08
Iter: 1491 loss: 6.92465676e-08
Iter: 1492 loss: 6.92327689e-08
Iter: 1493 loss: 6.91678324e-08
Iter: 1494 loss: 6.96056901e-08
Iter: 1495 loss: 6.91593343e-08
Iter: 1496 loss: 6.91084523e-08
Iter: 1497 loss: 6.9045889e-08
Iter: 1498 loss: 6.90400697e-08
Iter: 1499 loss: 6.89984603e-08
Iter: 1500 loss: 6.89834607e-08
Iter: 1501 loss: 6.89639847e-08
Iter: 1502 loss: 6.89194195e-08
Iter: 1503 loss: 6.89178137e-08
Iter: 1504 loss: 6.88676209e-08
Iter: 1505 loss: 6.89333604e-08
Iter: 1506 loss: 6.88466457e-08
Iter: 1507 loss: 6.87919766e-08
Iter: 1508 loss: 6.90628355e-08
Iter: 1509 loss: 6.87867612e-08
Iter: 1510 loss: 6.87322839e-08
Iter: 1511 loss: 6.90491078e-08
Iter: 1512 loss: 6.87197428e-08
Iter: 1513 loss: 6.8689836e-08
Iter: 1514 loss: 6.86630699e-08
Iter: 1515 loss: 6.86587853e-08
Iter: 1516 loss: 6.86087276e-08
Iter: 1517 loss: 6.86075907e-08
Iter: 1518 loss: 6.85795527e-08
Iter: 1519 loss: 6.85038373e-08
Iter: 1520 loss: 6.96004179e-08
Iter: 1521 loss: 6.85009383e-08
Iter: 1522 loss: 6.84517332e-08
Iter: 1523 loss: 6.89972595e-08
Iter: 1524 loss: 6.84475054e-08
Iter: 1525 loss: 6.84197872e-08
Iter: 1526 loss: 6.87276867e-08
Iter: 1527 loss: 6.84202419e-08
Iter: 1528 loss: 6.83967301e-08
Iter: 1529 loss: 6.83394958e-08
Iter: 1530 loss: 6.92345736e-08
Iter: 1531 loss: 6.83341e-08
Iter: 1532 loss: 6.82773376e-08
Iter: 1533 loss: 6.85109711e-08
Iter: 1534 loss: 6.82672123e-08
Iter: 1535 loss: 6.82033487e-08
Iter: 1536 loss: 6.86355861e-08
Iter: 1537 loss: 6.81977781e-08
Iter: 1538 loss: 6.81668126e-08
Iter: 1539 loss: 6.81137635e-08
Iter: 1540 loss: 6.92504472e-08
Iter: 1541 loss: 6.8114133e-08
Iter: 1542 loss: 6.80468588e-08
Iter: 1543 loss: 6.86492427e-08
Iter: 1544 loss: 6.80451322e-08
Iter: 1545 loss: 6.80084042e-08
Iter: 1546 loss: 6.80022367e-08
Iter: 1547 loss: 6.79795065e-08
Iter: 1548 loss: 6.79432688e-08
Iter: 1549 loss: 6.79421248e-08
Iter: 1550 loss: 6.78980427e-08
Iter: 1551 loss: 6.81404089e-08
Iter: 1552 loss: 6.78889052e-08
Iter: 1553 loss: 6.78446384e-08
Iter: 1554 loss: 6.78964724e-08
Iter: 1555 loss: 6.78121737e-08
Iter: 1556 loss: 6.77693137e-08
Iter: 1557 loss: 6.7804e-08
Iter: 1558 loss: 6.77385259e-08
Iter: 1559 loss: 6.76819099e-08
Iter: 1560 loss: 6.79290224e-08
Iter: 1561 loss: 6.76674e-08
Iter: 1562 loss: 6.76264165e-08
Iter: 1563 loss: 6.78733727e-08
Iter: 1564 loss: 6.76155736e-08
Iter: 1565 loss: 6.75663685e-08
Iter: 1566 loss: 6.76118219e-08
Iter: 1567 loss: 6.75354883e-08
Iter: 1568 loss: 6.74872211e-08
Iter: 1569 loss: 6.76043541e-08
Iter: 1570 loss: 6.74747e-08
Iter: 1571 loss: 6.7415634e-08
Iter: 1572 loss: 6.75383731e-08
Iter: 1573 loss: 6.73945806e-08
Iter: 1574 loss: 6.73623219e-08
Iter: 1575 loss: 6.73083775e-08
Iter: 1576 loss: 6.73071625e-08
Iter: 1577 loss: 6.72481661e-08
Iter: 1578 loss: 6.77419223e-08
Iter: 1579 loss: 6.72473703e-08
Iter: 1580 loss: 6.71910101e-08
Iter: 1581 loss: 6.78159466e-08
Iter: 1582 loss: 6.71865692e-08
Iter: 1583 loss: 6.71546658e-08
Iter: 1584 loss: 6.71308271e-08
Iter: 1585 loss: 6.71180871e-08
Iter: 1586 loss: 6.70678659e-08
Iter: 1587 loss: 6.71285392e-08
Iter: 1588 loss: 6.70453204e-08
Iter: 1589 loss: 6.69932874e-08
Iter: 1590 loss: 6.6994815e-08
Iter: 1591 loss: 6.69688589e-08
Iter: 1592 loss: 6.69233629e-08
Iter: 1593 loss: 6.69223326e-08
Iter: 1594 loss: 6.68707614e-08
Iter: 1595 loss: 6.72791742e-08
Iter: 1596 loss: 6.68655318e-08
Iter: 1597 loss: 6.68350069e-08
Iter: 1598 loss: 6.68165683e-08
Iter: 1599 loss: 6.67965381e-08
Iter: 1600 loss: 6.6751852e-08
Iter: 1601 loss: 6.72614746e-08
Iter: 1602 loss: 6.67506086e-08
Iter: 1603 loss: 6.67007924e-08
Iter: 1604 loss: 6.6712829e-08
Iter: 1605 loss: 6.66621816e-08
Iter: 1606 loss: 6.66237554e-08
Iter: 1607 loss: 6.69981688e-08
Iter: 1608 loss: 6.66210127e-08
Iter: 1609 loss: 6.65779609e-08
Iter: 1610 loss: 6.65389877e-08
Iter: 1611 loss: 6.65228583e-08
Iter: 1612 loss: 6.6477476e-08
Iter: 1613 loss: 6.67547226e-08
Iter: 1614 loss: 6.64717135e-08
Iter: 1615 loss: 6.64268782e-08
Iter: 1616 loss: 6.67763302e-08
Iter: 1617 loss: 6.64211797e-08
Iter: 1618 loss: 6.63992523e-08
Iter: 1619 loss: 6.63722588e-08
Iter: 1620 loss: 6.63668516e-08
Iter: 1621 loss: 6.63252422e-08
Iter: 1622 loss: 6.63193163e-08
Iter: 1623 loss: 6.62873489e-08
Iter: 1624 loss: 6.6230939e-08
Iter: 1625 loss: 6.63838904e-08
Iter: 1626 loss: 6.62118111e-08
Iter: 1627 loss: 6.61799646e-08
Iter: 1628 loss: 6.61779964e-08
Iter: 1629 loss: 6.61477486e-08
Iter: 1630 loss: 6.60963266e-08
Iter: 1631 loss: 6.70896e-08
Iter: 1632 loss: 6.60948345e-08
Iter: 1633 loss: 6.60399e-08
Iter: 1634 loss: 6.66082158e-08
Iter: 1635 loss: 6.60413448e-08
Iter: 1636 loss: 6.60219683e-08
Iter: 1637 loss: 6.59987904e-08
Iter: 1638 loss: 6.59944135e-08
Iter: 1639 loss: 6.59617925e-08
Iter: 1640 loss: 6.61894788e-08
Iter: 1641 loss: 6.59573303e-08
Iter: 1642 loss: 6.59089139e-08
Iter: 1643 loss: 6.59438797e-08
Iter: 1644 loss: 6.58798953e-08
Iter: 1645 loss: 6.58368e-08
Iter: 1646 loss: 6.58361046e-08
Iter: 1647 loss: 6.58013306e-08
Iter: 1648 loss: 6.57726744e-08
Iter: 1649 loss: 6.57681909e-08
Iter: 1650 loss: 6.57442882e-08
Iter: 1651 loss: 6.56983872e-08
Iter: 1652 loss: 6.56998367e-08
Iter: 1653 loss: 6.56680896e-08
Iter: 1654 loss: 6.56703278e-08
Iter: 1655 loss: 6.56443149e-08
Iter: 1656 loss: 6.56486492e-08
Iter: 1657 loss: 6.5628015e-08
Iter: 1658 loss: 6.56030323e-08
Iter: 1659 loss: 6.56970229e-08
Iter: 1660 loss: 6.55956e-08
Iter: 1661 loss: 6.55688481e-08
Iter: 1662 loss: 6.56325483e-08
Iter: 1663 loss: 6.55535288e-08
Iter: 1664 loss: 6.55209504e-08
Iter: 1665 loss: 6.55044445e-08
Iter: 1666 loss: 6.54874199e-08
Iter: 1667 loss: 6.5439707e-08
Iter: 1668 loss: 6.5621947e-08
Iter: 1669 loss: 6.54340653e-08
Iter: 1670 loss: 6.53970744e-08
Iter: 1671 loss: 6.54279e-08
Iter: 1672 loss: 6.53790622e-08
Iter: 1673 loss: 6.53380354e-08
Iter: 1674 loss: 6.5775879e-08
Iter: 1675 loss: 6.5335e-08
Iter: 1676 loss: 6.53071552e-08
Iter: 1677 loss: 6.54309e-08
Iter: 1678 loss: 6.53009664e-08
Iter: 1679 loss: 6.52799272e-08
Iter: 1680 loss: 6.52444783e-08
Iter: 1681 loss: 6.52472352e-08
Iter: 1682 loss: 6.52236665e-08
Iter: 1683 loss: 6.52136691e-08
Iter: 1684 loss: 6.52038494e-08
Iter: 1685 loss: 6.51480789e-08
Iter: 1686 loss: 6.53139125e-08
Iter: 1687 loss: 6.51219239e-08
Iter: 1688 loss: 6.50983623e-08
Iter: 1689 loss: 6.50831922e-08
Iter: 1690 loss: 6.50526e-08
Iter: 1691 loss: 6.50175878e-08
Iter: 1692 loss: 6.50134666e-08
Iter: 1693 loss: 6.49791403e-08
Iter: 1694 loss: 6.54453487e-08
Iter: 1695 loss: 6.49795595e-08
Iter: 1696 loss: 6.49441319e-08
Iter: 1697 loss: 6.50547634e-08
Iter: 1698 loss: 6.49346248e-08
Iter: 1699 loss: 6.49134932e-08
Iter: 1700 loss: 6.49158167e-08
Iter: 1701 loss: 6.4897165e-08
Iter: 1702 loss: 6.48660219e-08
Iter: 1703 loss: 6.49315837e-08
Iter: 1704 loss: 6.4852415e-08
Iter: 1705 loss: 6.4815417e-08
Iter: 1706 loss: 6.49154401e-08
Iter: 1707 loss: 6.48022933e-08
Iter: 1708 loss: 6.47701413e-08
Iter: 1709 loss: 6.5092e-08
Iter: 1710 loss: 6.47682299e-08
Iter: 1711 loss: 6.47335057e-08
Iter: 1712 loss: 6.4710548e-08
Iter: 1713 loss: 6.46991651e-08
Iter: 1714 loss: 6.46654712e-08
Iter: 1715 loss: 6.49709e-08
Iter: 1716 loss: 6.46668212e-08
Iter: 1717 loss: 6.46268319e-08
Iter: 1718 loss: 6.46210623e-08
Iter: 1719 loss: 6.45961222e-08
Iter: 1720 loss: 6.45637925e-08
Iter: 1721 loss: 6.46528e-08
Iter: 1722 loss: 6.45450342e-08
Iter: 1723 loss: 6.45084697e-08
Iter: 1724 loss: 6.48506457e-08
Iter: 1725 loss: 6.45123e-08
Iter: 1726 loss: 6.44884821e-08
Iter: 1727 loss: 6.44496723e-08
Iter: 1728 loss: 6.53959944e-08
Iter: 1729 loss: 6.44522e-08
Iter: 1730 loss: 6.44284128e-08
Iter: 1731 loss: 6.44202771e-08
Iter: 1732 loss: 6.43960547e-08
Iter: 1733 loss: 6.43726921e-08
Iter: 1734 loss: 6.43680309e-08
Iter: 1735 loss: 6.43150742e-08
Iter: 1736 loss: 6.44205613e-08
Iter: 1737 loss: 6.43039897e-08
Iter: 1738 loss: 6.42630624e-08
Iter: 1739 loss: 6.43426432e-08
Iter: 1740 loss: 6.42480273e-08
Iter: 1741 loss: 6.42241247e-08
Iter: 1742 loss: 6.45090594e-08
Iter: 1743 loss: 6.42234639e-08
Iter: 1744 loss: 6.41919513e-08
Iter: 1745 loss: 6.42086277e-08
Iter: 1746 loss: 6.41819824e-08
Iter: 1747 loss: 6.41481819e-08
Iter: 1748 loss: 6.41278319e-08
Iter: 1749 loss: 6.4115369e-08
Iter: 1750 loss: 6.40812488e-08
Iter: 1751 loss: 6.42863114e-08
Iter: 1752 loss: 6.40740083e-08
Iter: 1753 loss: 6.40360369e-08
Iter: 1754 loss: 6.42999467e-08
Iter: 1755 loss: 6.40349214e-08
Iter: 1756 loss: 6.40174349e-08
Iter: 1757 loss: 6.39916706e-08
Iter: 1758 loss: 6.39892477e-08
Iter: 1759 loss: 6.39430624e-08
Iter: 1760 loss: 6.42817071e-08
Iter: 1761 loss: 6.39416911e-08
Iter: 1762 loss: 6.3911564e-08
Iter: 1763 loss: 6.38683701e-08
Iter: 1764 loss: 6.38696775e-08
Iter: 1765 loss: 6.38472e-08
Iter: 1766 loss: 6.38378e-08
Iter: 1767 loss: 6.38133315e-08
Iter: 1768 loss: 6.37922497e-08
Iter: 1769 loss: 6.37867e-08
Iter: 1770 loss: 6.37483097e-08
Iter: 1771 loss: 6.38953352e-08
Iter: 1772 loss: 6.37365645e-08
Iter: 1773 loss: 6.36995523e-08
Iter: 1774 loss: 6.3911429e-08
Iter: 1775 loss: 6.36954525e-08
Iter: 1776 loss: 6.36695745e-08
Iter: 1777 loss: 6.37309583e-08
Iter: 1778 loss: 6.3652223e-08
Iter: 1779 loss: 6.3631461e-08
Iter: 1780 loss: 6.38770246e-08
Iter: 1781 loss: 6.36296704e-08
Iter: 1782 loss: 6.36071746e-08
Iter: 1783 loss: 6.35914148e-08
Iter: 1784 loss: 6.3586441e-08
Iter: 1785 loss: 6.35532729e-08
Iter: 1786 loss: 6.36657731e-08
Iter: 1787 loss: 6.35413926e-08
Iter: 1788 loss: 6.35002948e-08
Iter: 1789 loss: 6.36604796e-08
Iter: 1790 loss: 6.3492692e-08
Iter: 1791 loss: 6.34639363e-08
Iter: 1792 loss: 6.34350954e-08
Iter: 1793 loss: 6.34360902e-08
Iter: 1794 loss: 6.34038813e-08
Iter: 1795 loss: 6.34037534e-08
Iter: 1796 loss: 6.33841069e-08
Iter: 1797 loss: 6.33454817e-08
Iter: 1798 loss: 6.40938538e-08
Iter: 1799 loss: 6.3347386e-08
Iter: 1800 loss: 6.33101536e-08
Iter: 1801 loss: 6.33087751e-08
Iter: 1802 loss: 6.3283224e-08
Iter: 1803 loss: 6.32446486e-08
Iter: 1804 loss: 6.32417851e-08
Iter: 1805 loss: 6.32127097e-08
Iter: 1806 loss: 6.33089101e-08
Iter: 1807 loss: 6.32025e-08
Iter: 1808 loss: 6.31715835e-08
Iter: 1809 loss: 6.34271942e-08
Iter: 1810 loss: 6.31716546e-08
Iter: 1811 loss: 6.31430481e-08
Iter: 1812 loss: 6.32782502e-08
Iter: 1813 loss: 6.3138188e-08
Iter: 1814 loss: 6.31147188e-08
Iter: 1815 loss: 6.31856238e-08
Iter: 1816 loss: 6.31120258e-08
Iter: 1817 loss: 6.30943e-08
Iter: 1818 loss: 6.30905674e-08
Iter: 1819 loss: 6.30718802e-08
Iter: 1820 loss: 6.3047608e-08
Iter: 1821 loss: 6.32581134e-08
Iter: 1822 loss: 6.30482333e-08
Iter: 1823 loss: 6.3032104e-08
Iter: 1824 loss: 6.29892369e-08
Iter: 1825 loss: 6.33467039e-08
Iter: 1826 loss: 6.29845687e-08
Iter: 1827 loss: 6.29413464e-08
Iter: 1828 loss: 6.34133244e-08
Iter: 1829 loss: 6.29410266e-08
Iter: 1830 loss: 6.29154115e-08
Iter: 1831 loss: 6.32731698e-08
Iter: 1832 loss: 6.2917664e-08
Iter: 1833 loss: 6.28973e-08
Iter: 1834 loss: 6.28816395e-08
Iter: 1835 loss: 6.28743564e-08
Iter: 1836 loss: 6.28500132e-08
Iter: 1837 loss: 6.2850205e-08
Iter: 1838 loss: 6.28356673e-08
Iter: 1839 loss: 6.28120418e-08
Iter: 1840 loss: 6.28098462e-08
Iter: 1841 loss: 6.27882173e-08
Iter: 1842 loss: 6.27694234e-08
Iter: 1843 loss: 6.27716332e-08
Iter: 1844 loss: 6.27421741e-08
Iter: 1845 loss: 6.2737e-08
Iter: 1846 loss: 6.27189962e-08
Iter: 1847 loss: 6.27167935e-08
Iter: 1848 loss: 6.27058654e-08
Iter: 1849 loss: 6.26821191e-08
Iter: 1850 loss: 6.26965715e-08
Iter: 1851 loss: 6.26677661e-08
Iter: 1852 loss: 6.26389536e-08
Iter: 1853 loss: 6.28891073e-08
Iter: 1854 loss: 6.26364454e-08
Iter: 1855 loss: 6.25991419e-08
Iter: 1856 loss: 6.26154559e-08
Iter: 1857 loss: 6.25838e-08
Iter: 1858 loss: 6.25551237e-08
Iter: 1859 loss: 6.25913827e-08
Iter: 1860 loss: 6.25399963e-08
Iter: 1861 loss: 6.25154257e-08
Iter: 1862 loss: 6.27990886e-08
Iter: 1863 loss: 6.25163636e-08
Iter: 1864 loss: 6.24974916e-08
Iter: 1865 loss: 6.24851637e-08
Iter: 1866 loss: 6.24797565e-08
Iter: 1867 loss: 6.24504182e-08
Iter: 1868 loss: 6.24481515e-08
Iter: 1869 loss: 6.24278e-08
Iter: 1870 loss: 6.23934255e-08
Iter: 1871 loss: 6.23880183e-08
Iter: 1872 loss: 6.23712779e-08
Iter: 1873 loss: 6.23369516e-08
Iter: 1874 loss: 6.30820409e-08
Iter: 1875 loss: 6.23347773e-08
Iter: 1876 loss: 6.23052543e-08
Iter: 1877 loss: 6.24734398e-08
Iter: 1878 loss: 6.22994207e-08
Iter: 1879 loss: 6.22656415e-08
Iter: 1880 loss: 6.26528e-08
Iter: 1881 loss: 6.22666647e-08
Iter: 1882 loss: 6.2254351e-08
Iter: 1883 loss: 6.22424707e-08
Iter: 1884 loss: 6.22372909e-08
Iter: 1885 loss: 6.22159e-08
Iter: 1886 loss: 6.22163725e-08
Iter: 1887 loss: 6.22021119e-08
Iter: 1888 loss: 6.21725533e-08
Iter: 1889 loss: 6.22883718e-08
Iter: 1890 loss: 6.21652063e-08
Iter: 1891 loss: 6.21350864e-08
Iter: 1892 loss: 6.23530951e-08
Iter: 1893 loss: 6.21322e-08
Iter: 1894 loss: 6.21098e-08
Iter: 1895 loss: 6.20858e-08
Iter: 1896 loss: 6.2078243e-08
Iter: 1897 loss: 6.20578e-08
Iter: 1898 loss: 6.20558183e-08
Iter: 1899 loss: 6.20368823e-08
Iter: 1900 loss: 6.20141236e-08
Iter: 1901 loss: 6.20145926e-08
Iter: 1902 loss: 6.19860501e-08
Iter: 1903 loss: 6.20663059e-08
Iter: 1904 loss: 6.19786533e-08
Iter: 1905 loss: 6.19515106e-08
Iter: 1906 loss: 6.22606109e-08
Iter: 1907 loss: 6.19513258e-08
Iter: 1908 loss: 6.1936575e-08
Iter: 1909 loss: 6.18979854e-08
Iter: 1910 loss: 6.22474303e-08
Iter: 1911 loss: 6.18949656e-08
Iter: 1912 loss: 6.18644833e-08
Iter: 1913 loss: 6.18679508e-08
Iter: 1914 loss: 6.18377e-08
Iter: 1915 loss: 6.18391169e-08
Iter: 1916 loss: 6.18142479e-08
Iter: 1917 loss: 6.17884695e-08
Iter: 1918 loss: 6.18810461e-08
Iter: 1919 loss: 6.17842772e-08
Iter: 1920 loss: 6.17672242e-08
Iter: 1921 loss: 6.1750896e-08
Iter: 1922 loss: 6.17468e-08
Iter: 1923 loss: 6.17196108e-08
Iter: 1924 loss: 6.18701179e-08
Iter: 1925 loss: 6.17120719e-08
Iter: 1926 loss: 6.16959142e-08
Iter: 1927 loss: 6.16954736e-08
Iter: 1928 loss: 6.16811917e-08
Iter: 1929 loss: 6.16502e-08
Iter: 1930 loss: 6.20299403e-08
Iter: 1931 loss: 6.16479454e-08
Iter: 1932 loss: 6.16076505e-08
Iter: 1933 loss: 6.17210247e-08
Iter: 1934 loss: 6.16011206e-08
Iter: 1935 loss: 6.15752853e-08
Iter: 1936 loss: 6.15733668e-08
Iter: 1937 loss: 6.15523774e-08
Iter: 1938 loss: 6.15682296e-08
Iter: 1939 loss: 6.15481213e-08
Iter: 1940 loss: 6.15253e-08
Iter: 1941 loss: 6.15092688e-08
Iter: 1942 loss: 6.15004723e-08
Iter: 1943 loss: 6.14735569e-08
Iter: 1944 loss: 6.17735196e-08
Iter: 1945 loss: 6.14760438e-08
Iter: 1946 loss: 6.14454621e-08
Iter: 1947 loss: 6.14801792e-08
Iter: 1948 loss: 6.14322104e-08
Iter: 1949 loss: 6.14070785e-08
Iter: 1950 loss: 6.14518214e-08
Iter: 1951 loss: 6.13999802e-08
Iter: 1952 loss: 6.13703293e-08
Iter: 1953 loss: 6.14704305e-08
Iter: 1954 loss: 6.13632523e-08
Iter: 1955 loss: 6.13288336e-08
Iter: 1956 loss: 6.1417289e-08
Iter: 1957 loss: 6.13233837e-08
Iter: 1958 loss: 6.13009092e-08
Iter: 1959 loss: 6.13045756e-08
Iter: 1960 loss: 6.12848083e-08
Iter: 1961 loss: 6.12688069e-08
Iter: 1962 loss: 6.13700593e-08
Iter: 1963 loss: 6.12636413e-08
Iter: 1964 loss: 6.12426589e-08
Iter: 1965 loss: 6.13693203e-08
Iter: 1966 loss: 6.12407405e-08
Iter: 1967 loss: 6.12281212e-08
Iter: 1968 loss: 6.1196566e-08
Iter: 1969 loss: 6.18899492e-08
Iter: 1970 loss: 6.11976603e-08
Iter: 1971 loss: 6.11646414e-08
Iter: 1972 loss: 6.12843039e-08
Iter: 1973 loss: 6.11614297e-08
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
+ sleep 300
+ true
+ for SUM in $IN/${fn}_psi${psi}_phi${phi}/summary.txt
+ OUT=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/summary.txt
+ '[' '!' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/summary.txt ']'
+ sleep 300
+ true
+ for SUM in $IN/${fn}_psi${psi}_phi${phi}/summary.txt
+ OUT=/home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/summary.txt
+ '[' '!' -r /home/mrdouglas/Manifold/experiments.final/output62/f1_psi2_phi0.4/summary.txt ']'
+ sleep 300
