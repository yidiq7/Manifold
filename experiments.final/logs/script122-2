+ RUN=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ LAYERS=300_100_100_100_1
+ case $RUN in
+ PSI='0 1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 800 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output120
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output122
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.final/output120/f1_psi0_phi0
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0
+ date
Sun Nov  8 13:21:32 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83c037b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83cf0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83d28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83bb1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83bc11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83bb8bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83ce46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83c39158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83c34b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83aee620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83b1c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83b7eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83b6f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83a78048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83aac9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83adb510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83abbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83b6c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83a3b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83a48e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd839708c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd8397a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd839aad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83933840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd604802f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd6047ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd839f57b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd839ef268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd603dec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd603e3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd603ab1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd603ccbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd839d16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd839b4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd3815ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd38120620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.37136078
test_loss: 0.3718866
train_loss: 0.3183519
test_loss: 0.31862602
train_loss: 0.2615493
test_loss: 0.2610588
train_loss: 0.2011903
test_loss: 0.20217901
train_loss: 0.14412837
test_loss: 0.14466074
train_loss: 0.09411223
test_loss: 0.09430672
train_loss: 0.06570731
test_loss: 0.06665362
train_loss: 0.05795943
test_loss: 0.058024142
train_loss: 0.05496139
test_loss: 0.05541862
train_loss: 0.054364573
test_loss: 0.054272514
train_loss: 0.053404428
test_loss: 0.05361733
train_loss: 0.05293052
test_loss: 0.053186037
train_loss: 0.052511998
test_loss: 0.052888703
train_loss: 0.05224951
test_loss: 0.05257774
train_loss: 0.052528635
test_loss: 0.05238816
train_loss: 0.05229443
test_loss: 0.052114327
train_loss: 0.05160591
test_loss: 0.051826257
train_loss: 0.051051736
test_loss: 0.051529273
train_loss: 0.051824603
test_loss: 0.051115356
train_loss: 0.050583914
test_loss: 0.05068703
train_loss: 0.049683332
test_loss: 0.05011492
train_loss: 0.04863812
test_loss: 0.04945878
train_loss: 0.047926366
test_loss: 0.048572212
train_loss: 0.046441436
test_loss: 0.04759389
train_loss: 0.046164036
test_loss: 0.046417117
train_loss: 0.044083055
test_loss: 0.044987395
train_loss: 0.04382411
test_loss: 0.043389987
train_loss: 0.04159741
test_loss: 0.041317347
train_loss: 0.038512077
test_loss: 0.039081443
train_loss: 0.036201403
test_loss: 0.03621192
train_loss: 0.03272224
test_loss: 0.033124227
train_loss: 0.029275846
test_loss: 0.0294035
train_loss: 0.024675872
test_loss: 0.025208099
train_loss: 0.02051524
test_loss: 0.020705508
train_loss: 0.01648038
test_loss: 0.016506083
train_loss: 0.013015712
test_loss: 0.01326541
train_loss: 0.01058764
test_loss: 0.010975327
train_loss: 0.009785691
test_loss: 0.009896638
train_loss: 0.008961037
test_loss: 0.008945455
train_loss: 0.008278104
test_loss: 0.00849061
train_loss: 0.00808288
test_loss: 0.008014735
train_loss: 0.007838277
test_loss: 0.0077758366
train_loss: 0.0074584996
test_loss: 0.0075601353
train_loss: 0.007377573
test_loss: 0.007410541
train_loss: 0.0071113915
test_loss: 0.0071971724
train_loss: 0.0068611572
test_loss: 0.0070315152
train_loss: 0.0071130903
test_loss: 0.0070380564
train_loss: 0.0067420406
test_loss: 0.0069844206
train_loss: 0.0063384036
test_loss: 0.006809312
train_loss: 0.006313932
test_loss: 0.006425849
train_loss: 0.006618985
test_loss: 0.0066622104
train_loss: 0.00612072
test_loss: 0.0062958687
train_loss: 0.0062349774
test_loss: 0.0063705076
train_loss: 0.00584119
test_loss: 0.0059566405
train_loss: 0.005701905
test_loss: 0.005836885
train_loss: 0.0055509536
test_loss: 0.0055919075
train_loss: 0.005559606
test_loss: 0.005556443
train_loss: 0.005171459
test_loss: 0.005199746
train_loss: 0.004905947
test_loss: 0.0048912195
train_loss: 0.0050214264
test_loss: 0.0051383483
train_loss: 0.0047934465
test_loss: 0.0048864232
train_loss: 0.004466814
test_loss: 0.0045619216
train_loss: 0.00446761
test_loss: 0.00452095
train_loss: 0.004081498
test_loss: 0.0040055783
train_loss: 0.0042787865
test_loss: 0.003942505
train_loss: 0.0039861873
test_loss: 0.0040093106
train_loss: 0.004030025
test_loss: 0.0037679705
train_loss: 0.003936293
test_loss: 0.0035125585
train_loss: 0.0036084338
test_loss: 0.0036107814
train_loss: 0.0036029052
test_loss: 0.0035249523
train_loss: 0.0034696448
test_loss: 0.0034770207
train_loss: 0.0031216056
test_loss: 0.0030057002
train_loss: 0.003466016
test_loss: 0.0032251026
train_loss: 0.0029256353
test_loss: 0.0030687905
train_loss: 0.002910193
test_loss: 0.003268385
train_loss: 0.0027890939
test_loss: 0.0033528788
train_loss: 0.0029205293
test_loss: 0.003071163
train_loss: 0.0032748925
test_loss: 0.003647243
train_loss: 0.0034294866
test_loss: 0.002945699
train_loss: 0.0030001577
test_loss: 0.003275898
train_loss: 0.0030476279
test_loss: 0.0028956607
train_loss: 0.0029420883
test_loss: 0.0033514185
train_loss: 0.003050046
test_loss: 0.0028077064
train_loss: 0.0033014102
test_loss: 0.0034968737
train_loss: 0.0031350902
test_loss: 0.00288011
train_loss: 0.0029054834
test_loss: 0.002596342
train_loss: 0.0030989877
test_loss: 0.0032472536
train_loss: 0.0027845178
test_loss: 0.0028302993
train_loss: 0.003372624
test_loss: 0.0036820483
train_loss: 0.0027603942
test_loss: 0.0025934977
train_loss: 0.0025168285
test_loss: 0.00284368
train_loss: 0.0028114822
test_loss: 0.003450772
train_loss: 0.0032268765
test_loss: 0.0033553012
train_loss: 0.0027834894
test_loss: 0.0040714727
train_loss: 0.003206253
test_loss: 0.0031219376
train_loss: 0.0031759313
test_loss: 0.002990071
train_loss: 0.002527939
test_loss: 0.0025857438
train_loss: 0.0031560352
test_loss: 0.0036965683
train_loss: 0.0027234931
test_loss: 0.0026948382
train_loss: 0.0027136034
test_loss: 0.0026041027
train_loss: 0.0025378682
test_loss: 0.0023651905
train_loss: 0.0026585325
test_loss: 0.002553786
train_loss: 0.0027375722
test_loss: 0.002506441
train_loss: 0.0029459812
test_loss: 0.003498726
train_loss: 0.0025848104
test_loss: 0.0027647715
train_loss: 0.0022403686
test_loss: 0.002620046
train_loss: 0.0024563149
test_loss: 0.0029078026
train_loss: 0.0031015654
test_loss: 0.003415361
train_loss: 0.0028705166
test_loss: 0.0027582282
train_loss: 0.0029846942
test_loss: 0.0028633485
train_loss: 0.002414343
test_loss: 0.0032625245
train_loss: 0.0028289356
test_loss: 0.002485928
train_loss: 0.0029634226
test_loss: 0.0027332965
train_loss: 0.0028865421
test_loss: 0.002744283
train_loss: 0.0028881817
test_loss: 0.0025302118
train_loss: 0.002420722
test_loss: 0.0028595251
train_loss: 0.0026836249
test_loss: 0.0024962306
train_loss: 0.0025071085
test_loss: 0.0026248815
train_loss: 0.0027099275
test_loss: 0.0034836163
train_loss: 0.0031118854
test_loss: 0.0026164623
train_loss: 0.0024950167
test_loss: 0.0026171964
train_loss: 0.0028706142
test_loss: 0.0030135273
train_loss: 0.0024396202
test_loss: 0.003223285
train_loss: 0.0027919253
test_loss: 0.0027226494
train_loss: 0.0029783787
test_loss: 0.002755008
train_loss: 0.003713786
test_loss: 0.00282046
train_loss: 0.0024774286
test_loss: 0.002877373
train_loss: 0.003148843
test_loss: 0.0029400978
train_loss: 0.0026573527
test_loss: 0.0024876161
train_loss: 0.00271089
test_loss: 0.002641378
train_loss: 0.0025917094
test_loss: 0.0030605549
train_loss: 0.0027261428
test_loss: 0.0026373987
train_loss: 0.002783986
test_loss: 0.0023207094
train_loss: 0.0024791914
test_loss: 0.0025358526
train_loss: 0.0023814999
test_loss: 0.0032267033
train_loss: 0.0023652236
test_loss: 0.0023442751
train_loss: 0.0026625732
test_loss: 0.0023002452
train_loss: 0.0027067217
test_loss: 0.002664291
train_loss: 0.0030576345
test_loss: 0.0026925139
train_loss: 0.0027506691
test_loss: 0.002515877
train_loss: 0.0021591596
test_loss: 0.0023835446
train_loss: 0.0033655958
test_loss: 0.0028339783
train_loss: 0.002814142
test_loss: 0.002445723
train_loss: 0.0027645286
test_loss: 0.0030468642
train_loss: 0.0034558834
test_loss: 0.0027663165
train_loss: 0.002246969
test_loss: 0.0025362442
train_loss: 0.0022845115
test_loss: 0.0023775958
train_loss: 0.0027037775
test_loss: 0.0027080292
train_loss: 0.002851784
test_loss: 0.0028654411
train_loss: 0.0027988805
test_loss: 0.0025348077
train_loss: 0.002664582
test_loss: 0.0032458357
train_loss: 0.0028945068
test_loss: 0.0025730182
train_loss: 0.002465132
test_loss: 0.0030949092
train_loss: 0.0023900012
test_loss: 0.0026132548
train_loss: 0.0025599825
test_loss: 0.0029235452
train_loss: 0.0027196333
test_loss: 0.0027032571
train_loss: 0.0025434287
test_loss: 0.002367605
train_loss: 0.002791319
test_loss: 0.003144055
train_loss: 0.0025762736
test_loss: 0.0024333545
train_loss: 0.0026617679
test_loss: 0.0027866159
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 0 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e6a1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e75a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e754d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e7a37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e797268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e7abc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e649730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e65b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e66fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e61e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e5b4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e5d2b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e592620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e5910d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e5a1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e56d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e51c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e5219d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e4cd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e4dfea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e4a2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e4b1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e43ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e4698c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77fad378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77fc0d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77f76840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77f162f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77f2cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77ee17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e6831e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77e98c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77ebc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf505f91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf50614bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf505c66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.18935623e-05
Iter: 2 loss: 7.92691499e-05
Iter: 3 loss: 6.06270669e-06
Iter: 4 loss: 5.09192841e-06
Iter: 5 loss: 3.56350984e-06
Iter: 6 loss: 3.54664667e-06
Iter: 7 loss: 2.90539515e-06
Iter: 8 loss: 2.74069771e-06
Iter: 9 loss: 2.53868097e-06
Iter: 10 loss: 2.33608057e-06
Iter: 11 loss: 2.29516172e-06
Iter: 12 loss: 2.03402e-06
Iter: 13 loss: 4.72890861e-06
Iter: 14 loss: 2.02651518e-06
Iter: 15 loss: 1.93436927e-06
Iter: 16 loss: 3.04034211e-06
Iter: 17 loss: 1.93324468e-06
Iter: 18 loss: 1.86420061e-06
Iter: 19 loss: 1.72044668e-06
Iter: 20 loss: 4.17640649e-06
Iter: 21 loss: 1.71707097e-06
Iter: 22 loss: 1.64152652e-06
Iter: 23 loss: 2.68381154e-06
Iter: 24 loss: 1.64126698e-06
Iter: 25 loss: 1.59210458e-06
Iter: 26 loss: 2.30633486e-06
Iter: 27 loss: 1.59204569e-06
Iter: 28 loss: 1.57785962e-06
Iter: 29 loss: 1.54249403e-06
Iter: 30 loss: 1.88545084e-06
Iter: 31 loss: 1.5378738e-06
Iter: 32 loss: 1.51390168e-06
Iter: 33 loss: 1.52520602e-06
Iter: 34 loss: 1.49779135e-06
Iter: 35 loss: 1.47940409e-06
Iter: 36 loss: 1.53207907e-06
Iter: 37 loss: 1.47367928e-06
Iter: 38 loss: 1.45783679e-06
Iter: 39 loss: 1.42914337e-06
Iter: 40 loss: 2.12282976e-06
Iter: 41 loss: 1.42917565e-06
Iter: 42 loss: 1.41391274e-06
Iter: 43 loss: 1.61533126e-06
Iter: 44 loss: 1.41383646e-06
Iter: 45 loss: 1.3971129e-06
Iter: 46 loss: 1.47162359e-06
Iter: 47 loss: 1.39382087e-06
Iter: 48 loss: 1.38471978e-06
Iter: 49 loss: 1.3651636e-06
Iter: 50 loss: 1.67277858e-06
Iter: 51 loss: 1.36449603e-06
Iter: 52 loss: 1.35706955e-06
Iter: 53 loss: 1.35316282e-06
Iter: 54 loss: 1.34480115e-06
Iter: 55 loss: 1.35103392e-06
Iter: 56 loss: 1.33971662e-06
Iter: 57 loss: 1.32861476e-06
Iter: 58 loss: 1.35913479e-06
Iter: 59 loss: 1.32502873e-06
Iter: 60 loss: 1.31953084e-06
Iter: 61 loss: 1.31883087e-06
Iter: 62 loss: 1.31498189e-06
Iter: 63 loss: 1.3063493e-06
Iter: 64 loss: 1.40747443e-06
Iter: 65 loss: 1.30624198e-06
Iter: 66 loss: 1.30200169e-06
Iter: 67 loss: 1.29089767e-06
Iter: 68 loss: 1.37789539e-06
Iter: 69 loss: 1.28882016e-06
Iter: 70 loss: 1.29102159e-06
Iter: 71 loss: 1.28564818e-06
Iter: 72 loss: 1.28192789e-06
Iter: 73 loss: 1.27923386e-06
Iter: 74 loss: 1.27796557e-06
Iter: 75 loss: 1.27482963e-06
Iter: 76 loss: 1.27104863e-06
Iter: 77 loss: 1.27065709e-06
Iter: 78 loss: 1.26542295e-06
Iter: 79 loss: 1.27744227e-06
Iter: 80 loss: 1.26340387e-06
Iter: 81 loss: 1.26013822e-06
Iter: 82 loss: 1.25925442e-06
Iter: 83 loss: 1.25776774e-06
Iter: 84 loss: 1.25398788e-06
Iter: 85 loss: 1.28435056e-06
Iter: 86 loss: 1.25323947e-06
Iter: 87 loss: 1.2504172e-06
Iter: 88 loss: 1.25036809e-06
Iter: 89 loss: 1.24708936e-06
Iter: 90 loss: 1.24718372e-06
Iter: 91 loss: 1.24450708e-06
Iter: 92 loss: 1.24119822e-06
Iter: 93 loss: 1.26311875e-06
Iter: 94 loss: 1.24087069e-06
Iter: 95 loss: 1.23800044e-06
Iter: 96 loss: 1.23212442e-06
Iter: 97 loss: 1.33472406e-06
Iter: 98 loss: 1.23199834e-06
Iter: 99 loss: 1.23503821e-06
Iter: 100 loss: 1.22994743e-06
Iter: 101 loss: 1.22872927e-06
Iter: 102 loss: 1.22590473e-06
Iter: 103 loss: 1.25676638e-06
Iter: 104 loss: 1.2256171e-06
Iter: 105 loss: 1.22335155e-06
Iter: 106 loss: 1.23571272e-06
Iter: 107 loss: 1.22298911e-06
Iter: 108 loss: 1.22002416e-06
Iter: 109 loss: 1.23597101e-06
Iter: 110 loss: 1.21960738e-06
Iter: 111 loss: 1.21851872e-06
Iter: 112 loss: 1.21596054e-06
Iter: 113 loss: 1.24720555e-06
Iter: 114 loss: 1.21569701e-06
Iter: 115 loss: 1.2152791e-06
Iter: 116 loss: 1.21448011e-06
Iter: 117 loss: 1.21319931e-06
Iter: 118 loss: 1.21202561e-06
Iter: 119 loss: 1.21165624e-06
Iter: 120 loss: 1.21040898e-06
Iter: 121 loss: 1.20848335e-06
Iter: 122 loss: 1.2084065e-06
Iter: 123 loss: 1.20671871e-06
Iter: 124 loss: 1.22699714e-06
Iter: 125 loss: 1.20669347e-06
Iter: 126 loss: 1.2047559e-06
Iter: 127 loss: 1.20591562e-06
Iter: 128 loss: 1.20350978e-06
Iter: 129 loss: 1.20232744e-06
Iter: 130 loss: 1.20266395e-06
Iter: 131 loss: 1.20149548e-06
Iter: 132 loss: 1.19993706e-06
Iter: 133 loss: 1.20945538e-06
Iter: 134 loss: 1.19975459e-06
Iter: 135 loss: 1.1987662e-06
Iter: 136 loss: 1.19725109e-06
Iter: 137 loss: 1.19724018e-06
Iter: 138 loss: 1.1955683e-06
Iter: 139 loss: 1.21302071e-06
Iter: 140 loss: 1.19552465e-06
Iter: 141 loss: 1.19404035e-06
Iter: 142 loss: 1.21708865e-06
Iter: 143 loss: 1.19407127e-06
Iter: 144 loss: 1.19356753e-06
Iter: 145 loss: 1.19475089e-06
Iter: 146 loss: 1.19340962e-06
Iter: 147 loss: 1.19268771e-06
Iter: 148 loss: 1.19162905e-06
Iter: 149 loss: 1.19159927e-06
Iter: 150 loss: 1.19079061e-06
Iter: 151 loss: 1.19409628e-06
Iter: 152 loss: 1.1906078e-06
Iter: 153 loss: 1.18932019e-06
Iter: 154 loss: 1.1911136e-06
Iter: 155 loss: 1.1886807e-06
Iter: 156 loss: 1.18779167e-06
Iter: 157 loss: 1.18647574e-06
Iter: 158 loss: 1.1864222e-06
Iter: 159 loss: 1.1866257e-06
Iter: 160 loss: 1.18585763e-06
Iter: 161 loss: 1.18545381e-06
Iter: 162 loss: 1.18485423e-06
Iter: 163 loss: 1.18480045e-06
Iter: 164 loss: 1.1842576e-06
Iter: 165 loss: 1.18614832e-06
Iter: 166 loss: 1.18409844e-06
Iter: 167 loss: 1.18336925e-06
Iter: 168 loss: 1.18326807e-06
Iter: 169 loss: 1.18276182e-06
Iter: 170 loss: 1.18193168e-06
Iter: 171 loss: 1.18174967e-06
Iter: 172 loss: 1.18117441e-06
Iter: 173 loss: 1.18021387e-06
Iter: 174 loss: 1.18522212e-06
Iter: 175 loss: 1.18008029e-06
Iter: 176 loss: 1.17956108e-06
Iter: 177 loss: 1.18771641e-06
Iter: 178 loss: 1.17954642e-06
Iter: 179 loss: 1.17885907e-06
Iter: 180 loss: 1.17847139e-06
Iter: 181 loss: 1.17811305e-06
Iter: 182 loss: 1.17753098e-06
Iter: 183 loss: 1.1777247e-06
Iter: 184 loss: 1.1771341e-06
Iter: 185 loss: 1.17662307e-06
Iter: 186 loss: 1.17661091e-06
Iter: 187 loss: 1.17637296e-06
Iter: 188 loss: 1.17578463e-06
Iter: 189 loss: 1.18005323e-06
Iter: 190 loss: 1.17564809e-06
Iter: 191 loss: 1.17537479e-06
Iter: 192 loss: 1.175237e-06
Iter: 193 loss: 1.17503203e-06
Iter: 194 loss: 1.17441232e-06
Iter: 195 loss: 1.17678076e-06
Iter: 196 loss: 1.17407535e-06
Iter: 197 loss: 1.17373372e-06
Iter: 198 loss: 1.1736081e-06
Iter: 199 loss: 1.17311811e-06
Iter: 200 loss: 1.17469733e-06
Iter: 201 loss: 1.1729262e-06
Iter: 202 loss: 1.17250852e-06
Iter: 203 loss: 1.17218622e-06
Iter: 204 loss: 1.17206378e-06
Iter: 205 loss: 1.17153422e-06
Iter: 206 loss: 1.17271429e-06
Iter: 207 loss: 1.17138131e-06
Iter: 208 loss: 1.17092065e-06
Iter: 209 loss: 1.17253114e-06
Iter: 210 loss: 1.17081322e-06
Iter: 211 loss: 1.17044897e-06
Iter: 212 loss: 1.1728921e-06
Iter: 213 loss: 1.1703994e-06
Iter: 214 loss: 1.17003219e-06
Iter: 215 loss: 1.16986166e-06
Iter: 216 loss: 1.16966362e-06
Iter: 217 loss: 1.16938645e-06
Iter: 218 loss: 1.1735392e-06
Iter: 219 loss: 1.16937781e-06
Iter: 220 loss: 1.16912679e-06
Iter: 221 loss: 1.16907404e-06
Iter: 222 loss: 1.16883507e-06
Iter: 223 loss: 1.16857177e-06
Iter: 224 loss: 1.16985279e-06
Iter: 225 loss: 1.16855358e-06
Iter: 226 loss: 1.16819956e-06
Iter: 227 loss: 1.16759759e-06
Iter: 228 loss: 1.18233106e-06
Iter: 229 loss: 1.16759634e-06
Iter: 230 loss: 1.16707713e-06
Iter: 231 loss: 1.16671538e-06
Iter: 232 loss: 1.16659896e-06
Iter: 233 loss: 1.16666615e-06
Iter: 234 loss: 1.16627689e-06
Iter: 235 loss: 1.16599335e-06
Iter: 236 loss: 1.16582146e-06
Iter: 237 loss: 1.16565104e-06
Iter: 238 loss: 1.16540082e-06
Iter: 239 loss: 1.16540855e-06
Iter: 240 loss: 1.16513775e-06
Iter: 241 loss: 1.16467459e-06
Iter: 242 loss: 1.17045943e-06
Iter: 243 loss: 1.16470699e-06
Iter: 244 loss: 1.16426895e-06
Iter: 245 loss: 1.16468732e-06
Iter: 246 loss: 1.16408864e-06
Iter: 247 loss: 1.16381193e-06
Iter: 248 loss: 1.16742012e-06
Iter: 249 loss: 1.16381455e-06
Iter: 250 loss: 1.16356568e-06
Iter: 251 loss: 1.16358228e-06
Iter: 252 loss: 1.16334036e-06
Iter: 253 loss: 1.16308331e-06
Iter: 254 loss: 1.16294223e-06
Iter: 255 loss: 1.16278738e-06
Iter: 256 loss: 1.16245906e-06
Iter: 257 loss: 1.16510705e-06
Iter: 258 loss: 1.16244018e-06
Iter: 259 loss: 1.16196861e-06
Iter: 260 loss: 1.16248327e-06
Iter: 261 loss: 1.16175067e-06
Iter: 262 loss: 1.16144406e-06
Iter: 263 loss: 1.16106287e-06
Iter: 264 loss: 1.16102547e-06
Iter: 265 loss: 1.16094725e-06
Iter: 266 loss: 1.16074136e-06
Iter: 267 loss: 1.16060028e-06
Iter: 268 loss: 1.16013837e-06
Iter: 269 loss: 1.16128297e-06
Iter: 270 loss: 1.15989224e-06
Iter: 271 loss: 1.15942385e-06
Iter: 272 loss: 1.16374531e-06
Iter: 273 loss: 1.15938951e-06
Iter: 274 loss: 1.15911985e-06
Iter: 275 loss: 1.15911212e-06
Iter: 276 loss: 1.15891953e-06
Iter: 277 loss: 1.15839794e-06
Iter: 278 loss: 1.16514184e-06
Iter: 279 loss: 1.15838111e-06
Iter: 280 loss: 1.15825742e-06
Iter: 281 loss: 1.15810712e-06
Iter: 282 loss: 1.15791295e-06
Iter: 283 loss: 1.15784769e-06
Iter: 284 loss: 1.15776379e-06
Iter: 285 loss: 1.1574698e-06
Iter: 286 loss: 1.15795081e-06
Iter: 287 loss: 1.15730927e-06
Iter: 288 loss: 1.15709281e-06
Iter: 289 loss: 1.15676266e-06
Iter: 290 loss: 1.15677199e-06
Iter: 291 loss: 1.15665512e-06
Iter: 292 loss: 1.15655553e-06
Iter: 293 loss: 1.15632088e-06
Iter: 294 loss: 1.1559016e-06
Iter: 295 loss: 1.16417789e-06
Iter: 296 loss: 1.15594094e-06
Iter: 297 loss: 1.15555486e-06
Iter: 298 loss: 1.15545413e-06
Iter: 299 loss: 1.15523142e-06
Iter: 300 loss: 1.155316e-06
Iter: 301 loss: 1.15507646e-06
Iter: 302 loss: 1.15494174e-06
Iter: 303 loss: 1.15449666e-06
Iter: 304 loss: 1.15866465e-06
Iter: 305 loss: 1.15450143e-06
Iter: 306 loss: 1.15418459e-06
Iter: 307 loss: 1.15679973e-06
Iter: 308 loss: 1.15416651e-06
Iter: 309 loss: 1.15386308e-06
Iter: 310 loss: 1.15671583e-06
Iter: 311 loss: 1.15386013e-06
Iter: 312 loss: 1.15373496e-06
Iter: 313 loss: 1.15339071e-06
Iter: 314 loss: 1.15553951e-06
Iter: 315 loss: 1.15332455e-06
Iter: 316 loss: 1.15307273e-06
Iter: 317 loss: 1.15302396e-06
Iter: 318 loss: 1.15288401e-06
Iter: 319 loss: 1.1527718e-06
Iter: 320 loss: 1.15277419e-06
Iter: 321 loss: 1.1525467e-06
Iter: 322 loss: 1.15252647e-06
Iter: 323 loss: 1.15235434e-06
Iter: 324 loss: 1.15207217e-06
Iter: 325 loss: 1.15225907e-06
Iter: 326 loss: 1.15187231e-06
Iter: 327 loss: 1.15158423e-06
Iter: 328 loss: 1.1531082e-06
Iter: 329 loss: 1.15158286e-06
Iter: 330 loss: 1.15130865e-06
Iter: 331 loss: 1.1523e-06
Iter: 332 loss: 1.15122771e-06
Iter: 333 loss: 1.15104115e-06
Iter: 334 loss: 1.15072419e-06
Iter: 335 loss: 1.15076602e-06
Iter: 336 loss: 1.15064859e-06
Iter: 337 loss: 1.15056127e-06
Iter: 338 loss: 1.15041473e-06
Iter: 339 loss: 1.15019407e-06
Iter: 340 loss: 1.15015234e-06
Iter: 341 loss: 1.14995032e-06
Iter: 342 loss: 1.14996806e-06
Iter: 343 loss: 1.14975626e-06
Iter: 344 loss: 1.14958652e-06
Iter: 345 loss: 1.14953559e-06
Iter: 346 loss: 1.14938632e-06
Iter: 347 loss: 1.14934892e-06
Iter: 348 loss: 1.1491893e-06
Iter: 349 loss: 1.14908403e-06
Iter: 350 loss: 1.14903355e-06
Iter: 351 loss: 1.14882948e-06
Iter: 352 loss: 1.14923819e-06
Iter: 353 loss: 1.14868703e-06
Iter: 354 loss: 1.14846353e-06
Iter: 355 loss: 1.14848854e-06
Iter: 356 loss: 1.14832301e-06
Iter: 357 loss: 1.14805e-06
Iter: 358 loss: 1.14892805e-06
Iter: 359 loss: 1.14796683e-06
Iter: 360 loss: 1.14763304e-06
Iter: 361 loss: 1.14840577e-06
Iter: 362 loss: 1.1475214e-06
Iter: 363 loss: 1.14726834e-06
Iter: 364 loss: 1.14703266e-06
Iter: 365 loss: 1.14699606e-06
Iter: 366 loss: 1.14662339e-06
Iter: 367 loss: 1.1470654e-06
Iter: 368 loss: 1.1464565e-06
Iter: 369 loss: 1.14642398e-06
Iter: 370 loss: 1.14627437e-06
Iter: 371 loss: 1.14618069e-06
Iter: 372 loss: 1.14589898e-06
Iter: 373 loss: 1.15053524e-06
Iter: 374 loss: 1.14589238e-06
Iter: 375 loss: 1.14569184e-06
Iter: 376 loss: 1.14567479e-06
Iter: 377 loss: 1.1455744e-06
Iter: 378 loss: 1.14536294e-06
Iter: 379 loss: 1.14537806e-06
Iter: 380 loss: 1.14519094e-06
Iter: 381 loss: 1.14518218e-06
Iter: 382 loss: 1.14506088e-06
Iter: 383 loss: 1.1448999e-06
Iter: 384 loss: 1.14874433e-06
Iter: 385 loss: 1.14486204e-06
Iter: 386 loss: 1.14451541e-06
Iter: 387 loss: 1.14488057e-06
Iter: 388 loss: 1.14432339e-06
Iter: 389 loss: 1.14401394e-06
Iter: 390 loss: 1.14401121e-06
Iter: 391 loss: 1.1439065e-06
Iter: 392 loss: 1.14389877e-06
Iter: 393 loss: 1.14379895e-06
Iter: 394 loss: 1.14358113e-06
Iter: 395 loss: 1.14404008e-06
Iter: 396 loss: 1.14349234e-06
Iter: 397 loss: 1.14335307e-06
Iter: 398 loss: 1.14297893e-06
Iter: 399 loss: 1.15044213e-06
Iter: 400 loss: 1.14298837e-06
Iter: 401 loss: 1.14287673e-06
Iter: 402 loss: 1.14282125e-06
Iter: 403 loss: 1.1425725e-06
Iter: 404 loss: 1.14238514e-06
Iter: 405 loss: 1.14226725e-06
Iter: 406 loss: 1.14208581e-06
Iter: 407 loss: 1.14304237e-06
Iter: 408 loss: 1.14205818e-06
Iter: 409 loss: 1.14179863e-06
Iter: 410 loss: 1.14234797e-06
Iter: 411 loss: 1.14166801e-06
Iter: 412 loss: 1.14152476e-06
Iter: 413 loss: 1.14196905e-06
Iter: 414 loss: 1.1414636e-06
Iter: 415 loss: 1.1411596e-06
Iter: 416 loss: 1.14170666e-06
Iter: 417 loss: 1.14104114e-06
Iter: 418 loss: 1.14090903e-06
Iter: 419 loss: 1.14075e-06
Iter: 420 loss: 1.14069894e-06
Iter: 421 loss: 1.14050135e-06
Iter: 422 loss: 1.14052182e-06
Iter: 423 loss: 1.14039631e-06
Iter: 424 loss: 1.14018553e-06
Iter: 425 loss: 1.1402135e-06
Iter: 426 loss: 1.14002546e-06
Iter: 427 loss: 1.14001864e-06
Iter: 428 loss: 1.13992166e-06
Iter: 429 loss: 1.13964677e-06
Iter: 430 loss: 1.14229692e-06
Iter: 431 loss: 1.1396296e-06
Iter: 432 loss: 1.13939052e-06
Iter: 433 loss: 1.14043019e-06
Iter: 434 loss: 1.13932663e-06
Iter: 435 loss: 1.13908618e-06
Iter: 436 loss: 1.14186435e-06
Iter: 437 loss: 1.139083e-06
Iter: 438 loss: 1.13893066e-06
Iter: 439 loss: 1.13881401e-06
Iter: 440 loss: 1.1387757e-06
Iter: 441 loss: 1.13857732e-06
Iter: 442 loss: 1.14157433e-06
Iter: 443 loss: 1.13855288e-06
Iter: 444 loss: 1.13840929e-06
Iter: 445 loss: 1.13881015e-06
Iter: 446 loss: 1.13835767e-06
Iter: 447 loss: 1.13820545e-06
Iter: 448 loss: 1.139201e-06
Iter: 449 loss: 1.13821886e-06
Iter: 450 loss: 1.13812837e-06
Iter: 451 loss: 1.13795011e-06
Iter: 452 loss: 1.13795772e-06
Iter: 453 loss: 1.1377831e-06
Iter: 454 loss: 1.13980479e-06
Iter: 455 loss: 1.13780141e-06
Iter: 456 loss: 1.13765941e-06
Iter: 457 loss: 1.13829287e-06
Iter: 458 loss: 1.13755084e-06
Iter: 459 loss: 1.1374425e-06
Iter: 460 loss: 1.13772012e-06
Iter: 461 loss: 1.1374126e-06
Iter: 462 loss: 1.1373329e-06
Iter: 463 loss: 1.13696274e-06
Iter: 464 loss: 1.138568e-06
Iter: 465 loss: 1.13691e-06
Iter: 466 loss: 1.1365687e-06
Iter: 467 loss: 1.13659053e-06
Iter: 468 loss: 1.13641772e-06
Iter: 469 loss: 1.13645285e-06
Iter: 470 loss: 1.13632711e-06
Iter: 471 loss: 1.13610167e-06
Iter: 472 loss: 1.13856186e-06
Iter: 473 loss: 1.13609121e-06
Iter: 474 loss: 1.13589851e-06
Iter: 475 loss: 1.13587498e-06
Iter: 476 loss: 1.1357622e-06
Iter: 477 loss: 1.13580495e-06
Iter: 478 loss: 1.13566898e-06
Iter: 479 loss: 1.13546855e-06
Iter: 480 loss: 1.13633837e-06
Iter: 481 loss: 1.13538658e-06
Iter: 482 loss: 1.13523038e-06
Iter: 483 loss: 1.13495435e-06
Iter: 484 loss: 1.13494616e-06
Iter: 485 loss: 1.13471128e-06
Iter: 486 loss: 1.13568717e-06
Iter: 487 loss: 1.13467604e-06
Iter: 488 loss: 1.1344988e-06
Iter: 489 loss: 1.13451767e-06
Iter: 490 loss: 1.13438853e-06
Iter: 491 loss: 1.13431702e-06
Iter: 492 loss: 1.13427268e-06
Iter: 493 loss: 1.13406463e-06
Iter: 494 loss: 1.13372971e-06
Iter: 495 loss: 1.13373324e-06
Iter: 496 loss: 1.1334381e-06
Iter: 497 loss: 1.13495241e-06
Iter: 498 loss: 1.13337774e-06
Iter: 499 loss: 1.13335705e-06
Iter: 500 loss: 1.13327781e-06
Iter: 501 loss: 1.13321198e-06
Iter: 502 loss: 1.13316014e-06
Iter: 503 loss: 1.13308079e-06
Iter: 504 loss: 1.13299598e-06
Iter: 505 loss: 1.13334454e-06
Iter: 506 loss: 1.13297153e-06
Iter: 507 loss: 1.13288729e-06
Iter: 508 loss: 1.13292936e-06
Iter: 509 loss: 1.13276906e-06
Iter: 510 loss: 1.132634e-06
Iter: 511 loss: 1.13406441e-06
Iter: 512 loss: 1.13265412e-06
Iter: 513 loss: 1.13253293e-06
Iter: 514 loss: 1.13234682e-06
Iter: 515 loss: 1.13694637e-06
Iter: 516 loss: 1.13233818e-06
Iter: 517 loss: 1.13215333e-06
Iter: 518 loss: 1.13243709e-06
Iter: 519 loss: 1.13201804e-06
Iter: 520 loss: 1.13189526e-06
Iter: 521 loss: 1.13191663e-06
Iter: 522 loss: 1.13175133e-06
Iter: 523 loss: 1.13225542e-06
Iter: 524 loss: 1.13173155e-06
Iter: 525 loss: 1.13164231e-06
Iter: 526 loss: 1.13153953e-06
Iter: 527 loss: 1.1315301e-06
Iter: 528 loss: 1.131315e-06
Iter: 529 loss: 1.13101271e-06
Iter: 530 loss: 1.13101305e-06
Iter: 531 loss: 1.13074168e-06
Iter: 532 loss: 1.13074066e-06
Iter: 533 loss: 1.13060014e-06
Iter: 534 loss: 1.13059775e-06
Iter: 535 loss: 1.13053272e-06
Iter: 536 loss: 1.13033047e-06
Iter: 537 loss: 1.13306385e-06
Iter: 538 loss: 1.13033593e-06
Iter: 539 loss: 1.13007377e-06
Iter: 540 loss: 1.13248916e-06
Iter: 541 loss: 1.1301006e-06
Iter: 542 loss: 1.12997066e-06
Iter: 543 loss: 1.13032172e-06
Iter: 544 loss: 1.12993439e-06
Iter: 545 loss: 1.12978262e-06
Iter: 546 loss: 1.12999521e-06
Iter: 547 loss: 1.12968621e-06
Iter: 548 loss: 1.1295482e-06
Iter: 549 loss: 1.12944235e-06
Iter: 550 loss: 1.12942905e-06
Iter: 551 loss: 1.1292334e-06
Iter: 552 loss: 1.12956207e-06
Iter: 553 loss: 1.12916791e-06
Iter: 554 loss: 1.12907878e-06
Iter: 555 loss: 1.12904877e-06
Iter: 556 loss: 1.12897806e-06
Iter: 557 loss: 1.12888654e-06
Iter: 558 loss: 1.12891917e-06
Iter: 559 loss: 1.12870305e-06
Iter: 560 loss: 1.12869429e-06
Iter: 561 loss: 1.12856833e-06
Iter: 562 loss: 1.12834334e-06
Iter: 563 loss: 1.12848261e-06
Iter: 564 loss: 1.12821249e-06
Iter: 565 loss: 1.12805344e-06
Iter: 566 loss: 1.12855025e-06
Iter: 567 loss: 1.12796806e-06
Iter: 568 loss: 1.12792668e-06
Iter: 569 loss: 1.12789053e-06
Iter: 570 loss: 1.1277873e-06
Iter: 571 loss: 1.12760051e-06
Iter: 572 loss: 1.13135798e-06
Iter: 573 loss: 1.12755674e-06
Iter: 574 loss: 1.12740759e-06
Iter: 575 loss: 1.1291196e-06
Iter: 576 loss: 1.12740929e-06
Iter: 577 loss: 1.12731027e-06
Iter: 578 loss: 1.12837438e-06
Iter: 579 loss: 1.12733596e-06
Iter: 580 loss: 1.1273e-06
Iter: 581 loss: 1.12715782e-06
Iter: 582 loss: 1.12898056e-06
Iter: 583 loss: 1.12717044e-06
Iter: 584 loss: 1.12703253e-06
Iter: 585 loss: 1.12739303e-06
Iter: 586 loss: 1.1270123e-06
Iter: 587 loss: 1.12686394e-06
Iter: 588 loss: 1.12686394e-06
Iter: 589 loss: 1.12679174e-06
Iter: 590 loss: 1.12664475e-06
Iter: 591 loss: 1.12665202e-06
Iter: 592 loss: 1.12649082e-06
Iter: 593 loss: 1.12720647e-06
Iter: 594 loss: 1.12643329e-06
Iter: 595 loss: 1.12637849e-06
Iter: 596 loss: 1.12634802e-06
Iter: 597 loss: 1.12628436e-06
Iter: 598 loss: 1.12617715e-06
Iter: 599 loss: 1.12619159e-06
Iter: 600 loss: 1.12603914e-06
Iter: 601 loss: 1.12581e-06
Iter: 602 loss: 1.13080978e-06
Iter: 603 loss: 1.12582347e-06
Iter: 604 loss: 1.12579937e-06
Iter: 605 loss: 1.12569296e-06
Iter: 606 loss: 1.12557916e-06
Iter: 607 loss: 1.12554699e-06
Iter: 608 loss: 1.12542943e-06
Iter: 609 loss: 1.12538783e-06
Iter: 610 loss: 1.12533735e-06
Iter: 611 loss: 1.12529938e-06
Iter: 612 loss: 1.12516909e-06
Iter: 613 loss: 1.1251559e-06
Iter: 614 loss: 1.12514726e-06
Iter: 615 loss: 1.12500038e-06
Iter: 616 loss: 1.12565692e-06
Iter: 617 loss: 1.12497673e-06
Iter: 618 loss: 1.12481894e-06
Iter: 619 loss: 1.12627754e-06
Iter: 620 loss: 1.12482599e-06
Iter: 621 loss: 1.1247223e-06
Iter: 622 loss: 1.12466626e-06
Iter: 623 loss: 1.12463044e-06
Iter: 624 loss: 1.12453631e-06
Iter: 625 loss: 1.12559678e-06
Iter: 626 loss: 1.12451312e-06
Iter: 627 loss: 1.12445139e-06
Iter: 628 loss: 1.12534849e-06
Iter: 629 loss: 1.12447913e-06
Iter: 630 loss: 1.12437215e-06
Iter: 631 loss: 1.12434793e-06
Iter: 632 loss: 1.12432167e-06
Iter: 633 loss: 1.12423174e-06
Iter: 634 loss: 1.12413488e-06
Iter: 635 loss: 1.12415387e-06
Iter: 636 loss: 1.12402461e-06
Iter: 637 loss: 1.12402904e-06
Iter: 638 loss: 1.12393013e-06
Iter: 639 loss: 1.12432576e-06
Iter: 640 loss: 1.12387863e-06
Iter: 641 loss: 1.12384782e-06
Iter: 642 loss: 1.12368025e-06
Iter: 643 loss: 1.12524845e-06
Iter: 644 loss: 1.12365615e-06
Iter: 645 loss: 1.12361954e-06
Iter: 646 loss: 1.12350813e-06
Iter: 647 loss: 1.12348994e-06
Iter: 648 loss: 1.12331543e-06
Iter: 649 loss: 1.12424584e-06
Iter: 650 loss: 1.12325438e-06
Iter: 651 loss: 1.12311238e-06
Iter: 652 loss: 1.12307725e-06
Iter: 653 loss: 1.12293981e-06
Iter: 654 loss: 1.1229447e-06
Iter: 655 loss: 1.12282487e-06
Iter: 656 loss: 1.12269254e-06
Iter: 657 loss: 1.1225851e-06
Iter: 658 loss: 1.12251723e-06
Iter: 659 loss: 1.12230259e-06
Iter: 660 loss: 1.1224638e-06
Iter: 661 loss: 1.12220471e-06
Iter: 662 loss: 1.12202497e-06
Iter: 663 loss: 1.12203247e-06
Iter: 664 loss: 1.12188332e-06
Iter: 665 loss: 1.1216132e-06
Iter: 666 loss: 1.12629141e-06
Iter: 667 loss: 1.12161615e-06
Iter: 668 loss: 1.12129726e-06
Iter: 669 loss: 1.12179305e-06
Iter: 670 loss: 1.12116618e-06
Iter: 671 loss: 1.12085627e-06
Iter: 672 loss: 1.12292037e-06
Iter: 673 loss: 1.12074986e-06
Iter: 674 loss: 1.12065788e-06
Iter: 675 loss: 1.12063424e-06
Iter: 676 loss: 1.12051225e-06
Iter: 677 loss: 1.12030307e-06
Iter: 678 loss: 1.12030648e-06
Iter: 679 loss: 1.12018392e-06
Iter: 680 loss: 1.12019495e-06
Iter: 681 loss: 1.12008183e-06
Iter: 682 loss: 1.11975055e-06
Iter: 683 loss: 1.12358202e-06
Iter: 684 loss: 1.11970007e-06
Iter: 685 loss: 1.11949987e-06
Iter: 686 loss: 1.1220784e-06
Iter: 687 loss: 1.11950976e-06
Iter: 688 loss: 1.11933582e-06
Iter: 689 loss: 1.11959571e-06
Iter: 690 loss: 1.11925419e-06
Iter: 691 loss: 1.11905206e-06
Iter: 692 loss: 1.12051384e-06
Iter: 693 loss: 1.11907343e-06
Iter: 694 loss: 1.11887834e-06
Iter: 695 loss: 1.11899055e-06
Iter: 696 loss: 1.1187791e-06
Iter: 697 loss: 1.11856491e-06
Iter: 698 loss: 1.1190948e-06
Iter: 699 loss: 1.11850431e-06
Iter: 700 loss: 1.11831491e-06
Iter: 701 loss: 1.11821157e-06
Iter: 702 loss: 1.11818395e-06
Iter: 703 loss: 1.11803433e-06
Iter: 704 loss: 1.11869713e-06
Iter: 705 loss: 1.11800512e-06
Iter: 706 loss: 1.11785937e-06
Iter: 707 loss: 1.11851568e-06
Iter: 708 loss: 1.11785698e-06
Iter: 709 loss: 1.11769282e-06
Iter: 710 loss: 1.11903637e-06
Iter: 711 loss: 1.11769464e-06
Iter: 712 loss: 1.11762097e-06
Iter: 713 loss: 1.11753252e-06
Iter: 714 loss: 1.1175182e-06
Iter: 715 loss: 1.11749034e-06
Iter: 716 loss: 1.1174476e-06
Iter: 717 loss: 1.11740496e-06
Iter: 718 loss: 1.1172757e-06
Iter: 719 loss: 1.11764189e-06
Iter: 720 loss: 1.11719373e-06
Iter: 721 loss: 1.11704298e-06
Iter: 722 loss: 1.11777399e-06
Iter: 723 loss: 1.11698671e-06
Iter: 724 loss: 1.11682357e-06
Iter: 725 loss: 1.11896213e-06
Iter: 726 loss: 1.11680106e-06
Iter: 727 loss: 1.11670738e-06
Iter: 728 loss: 1.11671443e-06
Iter: 729 loss: 1.11662871e-06
Iter: 730 loss: 1.11644817e-06
Iter: 731 loss: 1.11709505e-06
Iter: 732 loss: 1.11644476e-06
Iter: 733 loss: 1.11624308e-06
Iter: 734 loss: 1.11674876e-06
Iter: 735 loss: 1.11619534e-06
Iter: 736 loss: 1.11610939e-06
Iter: 737 loss: 1.11589759e-06
Iter: 738 loss: 1.11934992e-06
Iter: 739 loss: 1.1158628e-06
Iter: 740 loss: 1.11563816e-06
Iter: 741 loss: 1.11711893e-06
Iter: 742 loss: 1.11564123e-06
Iter: 743 loss: 1.11539578e-06
Iter: 744 loss: 1.1154342e-06
Iter: 745 loss: 1.11530767e-06
Iter: 746 loss: 1.11568261e-06
Iter: 747 loss: 1.11527538e-06
Iter: 748 loss: 1.11515533e-06
Iter: 749 loss: 1.11538225e-06
Iter: 750 loss: 1.11512088e-06
Iter: 751 loss: 1.11497354e-06
Iter: 752 loss: 1.11554709e-06
Iter: 753 loss: 1.11497093e-06
Iter: 754 loss: 1.1149059e-06
Iter: 755 loss: 1.1147763e-06
Iter: 756 loss: 1.11640975e-06
Iter: 757 loss: 1.11473946e-06
Iter: 758 loss: 1.11462316e-06
Iter: 759 loss: 1.11508734e-06
Iter: 760 loss: 1.11456575e-06
Iter: 761 loss: 1.11431825e-06
Iter: 762 loss: 1.11628378e-06
Iter: 763 loss: 1.11436327e-06
Iter: 764 loss: 1.11422855e-06
Iter: 765 loss: 1.11429677e-06
Iter: 766 loss: 1.11408519e-06
Iter: 767 loss: 1.11396707e-06
Iter: 768 loss: 1.11492477e-06
Iter: 769 loss: 1.11392876e-06
Iter: 770 loss: 1.11381428e-06
Iter: 771 loss: 1.11380541e-06
Iter: 772 loss: 1.11369263e-06
Iter: 773 loss: 1.11353643e-06
Iter: 774 loss: 1.11342501e-06
Iter: 775 loss: 1.11339182e-06
Iter: 776 loss: 1.11320901e-06
Iter: 777 loss: 1.11399936e-06
Iter: 778 loss: 1.1131234e-06
Iter: 779 loss: 1.11287488e-06
Iter: 780 loss: 1.11521649e-06
Iter: 781 loss: 1.11291342e-06
Iter: 782 loss: 1.11284271e-06
Iter: 783 loss: 1.11284396e-06
Iter: 784 loss: 1.11273266e-06
Iter: 785 loss: 1.11261352e-06
Iter: 786 loss: 1.11402755e-06
Iter: 787 loss: 1.11261556e-06
Iter: 788 loss: 1.11250392e-06
Iter: 789 loss: 1.11243173e-06
Iter: 790 loss: 1.11240649e-06
Iter: 791 loss: 1.11227268e-06
Iter: 792 loss: 1.11223915e-06
Iter: 793 loss: 1.11214831e-06
Iter: 794 loss: 1.11198312e-06
Iter: 795 loss: 1.1131915e-06
Iter: 796 loss: 1.11199131e-06
Iter: 797 loss: 1.1117819e-06
Iter: 798 loss: 1.1126657e-06
Iter: 799 loss: 1.11176325e-06
Iter: 800 loss: 1.11165605e-06
Iter: 801 loss: 1.11175916e-06
Iter: 802 loss: 1.11157908e-06
Iter: 803 loss: 1.1114671e-06
Iter: 804 loss: 1.11271368e-06
Iter: 805 loss: 1.11143913e-06
Iter: 806 loss: 1.11139821e-06
Iter: 807 loss: 1.11122358e-06
Iter: 808 loss: 1.11356496e-06
Iter: 809 loss: 1.11117845e-06
Iter: 810 loss: 1.1110476e-06
Iter: 811 loss: 1.11165036e-06
Iter: 812 loss: 1.11097586e-06
Iter: 813 loss: 1.11090685e-06
Iter: 814 loss: 1.11091151e-06
Iter: 815 loss: 1.11082636e-06
Iter: 816 loss: 1.11068357e-06
Iter: 817 loss: 1.11372128e-06
Iter: 818 loss: 1.11067789e-06
Iter: 819 loss: 1.11062764e-06
Iter: 820 loss: 1.11056829e-06
Iter: 821 loss: 1.11047359e-06
Iter: 822 loss: 1.11047746e-06
Iter: 823 loss: 1.11039913e-06
Iter: 824 loss: 1.11028908e-06
Iter: 825 loss: 1.11023178e-06
Iter: 826 loss: 1.11016664e-06
Iter: 827 loss: 1.11001907e-06
Iter: 828 loss: 1.11046813e-06
Iter: 829 loss: 1.10998042e-06
Iter: 830 loss: 1.10982592e-06
Iter: 831 loss: 1.11121904e-06
Iter: 832 loss: 1.10979192e-06
Iter: 833 loss: 1.10967972e-06
Iter: 834 loss: 1.1099155e-06
Iter: 835 loss: 1.10964072e-06
Iter: 836 loss: 1.10953522e-06
Iter: 837 loss: 1.11005329e-06
Iter: 838 loss: 1.10950009e-06
Iter: 839 loss: 1.10938959e-06
Iter: 840 loss: 1.10923884e-06
Iter: 841 loss: 1.11211602e-06
Iter: 842 loss: 1.1092161e-06
Iter: 843 loss: 1.10901919e-06
Iter: 844 loss: 1.10984706e-06
Iter: 845 loss: 1.1089603e-06
Iter: 846 loss: 1.10884616e-06
Iter: 847 loss: 1.10885594e-06
Iter: 848 loss: 1.10874828e-06
Iter: 849 loss: 1.10860037e-06
Iter: 850 loss: 1.10859878e-06
Iter: 851 loss: 1.10848771e-06
Iter: 852 loss: 1.10848714e-06
Iter: 853 loss: 1.108324e-06
Iter: 854 loss: 1.10858412e-06
Iter: 855 loss: 1.1083373e-06
Iter: 856 loss: 1.108226e-06
Iter: 857 loss: 1.10813176e-06
Iter: 858 loss: 1.10817689e-06
Iter: 859 loss: 1.10793769e-06
Iter: 860 loss: 1.10814415e-06
Iter: 861 loss: 1.10786175e-06
Iter: 862 loss: 1.10770236e-06
Iter: 863 loss: 1.10768713e-06
Iter: 864 loss: 1.1075964e-06
Iter: 865 loss: 1.107662e-06
Iter: 866 loss: 1.10754468e-06
Iter: 867 loss: 1.10743304e-06
Iter: 868 loss: 1.1080889e-06
Iter: 869 loss: 1.10744679e-06
Iter: 870 loss: 1.10732742e-06
Iter: 871 loss: 1.10719714e-06
Iter: 872 loss: 1.10719122e-06
Iter: 873 loss: 1.10698898e-06
Iter: 874 loss: 1.10756991e-06
Iter: 875 loss: 1.10695032e-06
Iter: 876 loss: 1.10681344e-06
Iter: 877 loss: 1.10678798e-06
Iter: 878 loss: 1.10674421e-06
Iter: 879 loss: 1.10665417e-06
Iter: 880 loss: 1.10658596e-06
Iter: 881 loss: 1.10648421e-06
Iter: 882 loss: 1.10723545e-06
Iter: 883 loss: 1.10646954e-06
Iter: 884 loss: 1.1062981e-06
Iter: 885 loss: 1.10686676e-06
Iter: 886 loss: 1.10626706e-06
Iter: 887 loss: 1.10613246e-06
Iter: 888 loss: 1.10609426e-06
Iter: 889 loss: 1.10603469e-06
Iter: 890 loss: 1.10587052e-06
Iter: 891 loss: 1.10594738e-06
Iter: 892 loss: 1.10575343e-06
Iter: 893 loss: 1.10565543e-06
Iter: 894 loss: 1.1056211e-06
Iter: 895 loss: 1.10552935e-06
Iter: 896 loss: 1.10551446e-06
Iter: 897 loss: 1.10544363e-06
Iter: 898 loss: 1.10532255e-06
Iter: 899 loss: 1.10610233e-06
Iter: 900 loss: 1.10529356e-06
Iter: 901 loss: 1.10517851e-06
Iter: 902 loss: 1.10498308e-06
Iter: 903 loss: 1.10495955e-06
Iter: 904 loss: 1.10477538e-06
Iter: 905 loss: 1.10531141e-06
Iter: 906 loss: 1.10470637e-06
Iter: 907 loss: 1.10458916e-06
Iter: 908 loss: 1.10460235e-06
Iter: 909 loss: 1.10444989e-06
Iter: 910 loss: 1.10444637e-06
Iter: 911 loss: 1.10435303e-06
Iter: 912 loss: 1.10424708e-06
Iter: 913 loss: 1.10446229e-06
Iter: 914 loss: 1.10417136e-06
Iter: 915 loss: 1.10402561e-06
Iter: 916 loss: 1.10578458e-06
Iter: 917 loss: 1.10401129e-06
Iter: 918 loss: 1.10393557e-06
Iter: 919 loss: 1.10389578e-06
Iter: 920 loss: 1.10386645e-06
Iter: 921 loss: 1.1036667e-06
Iter: 922 loss: 1.10375686e-06
Iter: 923 loss: 1.10356336e-06
Iter: 924 loss: 1.10347924e-06
Iter: 925 loss: 1.1034457e-06
Iter: 926 loss: 1.10336055e-06
Iter: 927 loss: 1.1033278e-06
Iter: 928 loss: 1.10326414e-06
Iter: 929 loss: 1.10312749e-06
Iter: 930 loss: 1.10391375e-06
Iter: 931 loss: 1.1030935e-06
Iter: 932 loss: 1.10300425e-06
Iter: 933 loss: 1.10293161e-06
Iter: 934 loss: 1.10286317e-06
Iter: 935 loss: 1.10270184e-06
Iter: 936 loss: 1.10297708e-06
Iter: 937 loss: 1.10265478e-06
Iter: 938 loss: 1.10250858e-06
Iter: 939 loss: 1.10427845e-06
Iter: 940 loss: 1.1025146e-06
Iter: 941 loss: 1.10239046e-06
Iter: 942 loss: 1.10253973e-06
Iter: 943 loss: 1.10233577e-06
Iter: 944 loss: 1.10225346e-06
Iter: 945 loss: 1.1023069e-06
Iter: 946 loss: 1.10217866e-06
Iter: 947 loss: 1.10209737e-06
Iter: 948 loss: 1.10210397e-06
Iter: 949 loss: 1.10200358e-06
Iter: 950 loss: 1.10193525e-06
Iter: 951 loss: 1.10194424e-06
Iter: 952 loss: 1.10179667e-06
Iter: 953 loss: 1.10186124e-06
Iter: 954 loss: 1.10167559e-06
Iter: 955 loss: 1.10159363e-06
Iter: 956 loss: 1.10297162e-06
Iter: 957 loss: 1.10159112e-06
Iter: 958 loss: 1.10151041e-06
Iter: 959 loss: 1.10148255e-06
Iter: 960 loss: 1.10139194e-06
Iter: 961 loss: 1.10124984e-06
Iter: 962 loss: 1.10199585e-06
Iter: 963 loss: 1.10122483e-06
Iter: 964 loss: 1.10110159e-06
Iter: 965 loss: 1.10102621e-06
Iter: 966 loss: 1.10097301e-06
Iter: 967 loss: 1.10077008e-06
Iter: 968 loss: 1.10084875e-06
Iter: 969 loss: 1.10071073e-06
Iter: 970 loss: 1.10052e-06
Iter: 971 loss: 1.10213171e-06
Iter: 972 loss: 1.10053986e-06
Iter: 973 loss: 1.10034989e-06
Iter: 974 loss: 1.10080236e-06
Iter: 975 loss: 1.10031544e-06
Iter: 976 loss: 1.10013264e-06
Iter: 977 loss: 1.1000626e-06
Iter: 978 loss: 1.10001531e-06
Iter: 979 loss: 1.09990799e-06
Iter: 980 loss: 1.09987684e-06
Iter: 981 loss: 1.09978203e-06
Iter: 982 loss: 1.09966163e-06
Iter: 983 loss: 1.10324049e-06
Iter: 984 loss: 1.09962275e-06
Iter: 985 loss: 1.09940959e-06
Iter: 986 loss: 1.09984853e-06
Iter: 987 loss: 1.09934672e-06
Iter: 988 loss: 1.09917357e-06
Iter: 989 loss: 1.10018698e-06
Iter: 990 loss: 1.09916436e-06
Iter: 991 loss: 1.0989545e-06
Iter: 992 loss: 1.09927225e-06
Iter: 993 loss: 1.09882444e-06
Iter: 994 loss: 1.09868301e-06
Iter: 995 loss: 1.09939401e-06
Iter: 996 loss: 1.09863072e-06
Iter: 997 loss: 1.09843654e-06
Iter: 998 loss: 1.09863936e-06
Iter: 999 loss: 1.09824987e-06
Iter: 1000 loss: 1.09813595e-06
Iter: 1001 loss: 1.09813732e-06
Iter: 1002 loss: 1.09794564e-06
Iter: 1003 loss: 1.09777864e-06
Iter: 1004 loss: 1.09925463e-06
Iter: 1005 loss: 1.09771167e-06
Iter: 1006 loss: 1.0975167e-06
Iter: 1007 loss: 1.09863913e-06
Iter: 1008 loss: 1.0975059e-06
Iter: 1009 loss: 1.09738221e-06
Iter: 1010 loss: 1.09730377e-06
Iter: 1011 loss: 1.09726864e-06
Iter: 1012 loss: 1.09710504e-06
Iter: 1013 loss: 1.09709435e-06
Iter: 1014 loss: 1.09701841e-06
Iter: 1015 loss: 1.09683526e-06
Iter: 1016 loss: 1.10095391e-06
Iter: 1017 loss: 1.0968181e-06
Iter: 1018 loss: 1.09665689e-06
Iter: 1019 loss: 1.09726523e-06
Iter: 1020 loss: 1.0965764e-06
Iter: 1021 loss: 1.09645202e-06
Iter: 1022 loss: 1.0970914e-06
Iter: 1023 loss: 1.09642565e-06
Iter: 1024 loss: 1.09625262e-06
Iter: 1025 loss: 1.09690359e-06
Iter: 1026 loss: 1.09623011e-06
Iter: 1027 loss: 1.0960855e-06
Iter: 1028 loss: 1.09632174e-06
Iter: 1029 loss: 1.09605139e-06
Iter: 1030 loss: 1.09589348e-06
Iter: 1031 loss: 1.09606822e-06
Iter: 1032 loss: 1.0957699e-06
Iter: 1033 loss: 1.09564849e-06
Iter: 1034 loss: 1.09567395e-06
Iter: 1035 loss: 1.09553457e-06
Iter: 1036 loss: 1.09531447e-06
Iter: 1037 loss: 1.0960772e-06
Iter: 1038 loss: 1.09529253e-06
Iter: 1039 loss: 1.09512268e-06
Iter: 1040 loss: 1.09655457e-06
Iter: 1041 loss: 1.09512871e-06
Iter: 1042 loss: 1.09500252e-06
Iter: 1043 loss: 1.09492953e-06
Iter: 1044 loss: 1.09486575e-06
Iter: 1045 loss: 1.09476662e-06
Iter: 1046 loss: 1.09476855e-06
Iter: 1047 loss: 1.0946552e-06
Iter: 1048 loss: 1.0944799e-06
Iter: 1049 loss: 1.09449331e-06
Iter: 1050 loss: 1.0943329e-06
Iter: 1051 loss: 1.09496432e-06
Iter: 1052 loss: 1.09424343e-06
Iter: 1053 loss: 1.09413929e-06
Iter: 1054 loss: 1.09439134e-06
Iter: 1055 loss: 1.09406142e-06
Iter: 1056 loss: 1.09389021e-06
Iter: 1057 loss: 1.09496614e-06
Iter: 1058 loss: 1.09384121e-06
Iter: 1059 loss: 1.09372183e-06
Iter: 1060 loss: 1.09402436e-06
Iter: 1061 loss: 1.09366601e-06
Iter: 1062 loss: 1.09353209e-06
Iter: 1063 loss: 1.09374946e-06
Iter: 1064 loss: 1.09346433e-06
Iter: 1065 loss: 1.09332325e-06
Iter: 1066 loss: 1.09323685e-06
Iter: 1067 loss: 1.0932049e-06
Iter: 1068 loss: 1.09295502e-06
Iter: 1069 loss: 1.09344e-06
Iter: 1070 loss: 1.09289795e-06
Iter: 1071 loss: 1.09270491e-06
Iter: 1072 loss: 1.09520488e-06
Iter: 1073 loss: 1.09270968e-06
Iter: 1074 loss: 1.09258008e-06
Iter: 1075 loss: 1.09249652e-06
Iter: 1076 loss: 1.09242546e-06
Iter: 1077 loss: 1.09234e-06
Iter: 1078 loss: 1.09229597e-06
Iter: 1079 loss: 1.0921691e-06
Iter: 1080 loss: 1.09202244e-06
Iter: 1081 loss: 1.09202119e-06
Iter: 1082 loss: 1.0917895e-06
Iter: 1083 loss: 1.09226607e-06
Iter: 1084 loss: 1.09177518e-06
Iter: 1085 loss: 1.0915702e-06
Iter: 1086 loss: 1.09239318e-06
Iter: 1087 loss: 1.09153871e-06
Iter: 1088 loss: 1.09134248e-06
Iter: 1089 loss: 1.09219536e-06
Iter: 1090 loss: 1.09131861e-06
Iter: 1091 loss: 1.09113512e-06
Iter: 1092 loss: 1.09146617e-06
Iter: 1093 loss: 1.09110329e-06
Iter: 1094 loss: 1.09091e-06
Iter: 1095 loss: 1.09133543e-06
Iter: 1096 loss: 1.09084738e-06
Iter: 1097 loss: 1.09068355e-06
Iter: 1098 loss: 1.09058965e-06
Iter: 1099 loss: 1.090512e-06
Iter: 1100 loss: 1.09030952e-06
Iter: 1101 loss: 1.09123823e-06
Iter: 1102 loss: 1.09021039e-06
Iter: 1103 loss: 1.09001735e-06
Iter: 1104 loss: 1.09005464e-06
Iter: 1105 loss: 1.08996028e-06
Iter: 1106 loss: 1.0898159e-06
Iter: 1107 loss: 1.08976337e-06
Iter: 1108 loss: 1.08966788e-06
Iter: 1109 loss: 1.09156224e-06
Iter: 1110 loss: 1.08961262e-06
Iter: 1111 loss: 1.08943391e-06
Iter: 1112 loss: 1.08937695e-06
Iter: 1113 loss: 1.08929203e-06
Iter: 1114 loss: 1.08912718e-06
Iter: 1115 loss: 1.08942254e-06
Iter: 1116 loss: 1.08902793e-06
Iter: 1117 loss: 1.08884558e-06
Iter: 1118 loss: 1.08920756e-06
Iter: 1119 loss: 1.08876384e-06
Iter: 1120 loss: 1.08847064e-06
Iter: 1121 loss: 1.09006157e-06
Iter: 1122 loss: 1.0884703e-06
Iter: 1123 loss: 1.08827476e-06
Iter: 1124 loss: 1.08853317e-06
Iter: 1125 loss: 1.08821905e-06
Iter: 1126 loss: 1.08797178e-06
Iter: 1127 loss: 1.08864083e-06
Iter: 1128 loss: 1.08794143e-06
Iter: 1129 loss: 1.08773384e-06
Iter: 1130 loss: 1.08758286e-06
Iter: 1131 loss: 1.08755489e-06
Iter: 1132 loss: 1.08727613e-06
Iter: 1133 loss: 1.08816903e-06
Iter: 1134 loss: 1.08723486e-06
Iter: 1135 loss: 1.08700851e-06
Iter: 1136 loss: 1.08964264e-06
Iter: 1137 loss: 1.08697168e-06
Iter: 1138 loss: 1.08680888e-06
Iter: 1139 loss: 1.08670474e-06
Iter: 1140 loss: 1.08664699e-06
Iter: 1141 loss: 1.08647271e-06
Iter: 1142 loss: 1.08924144e-06
Iter: 1143 loss: 1.08646907e-06
Iter: 1144 loss: 1.08626421e-06
Iter: 1145 loss: 1.08632048e-06
Iter: 1146 loss: 1.0861429e-06
Iter: 1147 loss: 1.08593053e-06
Iter: 1148 loss: 1.08607901e-06
Iter: 1149 loss: 1.08579707e-06
Iter: 1150 loss: 1.08557833e-06
Iter: 1151 loss: 1.08595248e-06
Iter: 1152 loss: 1.08546942e-06
Iter: 1153 loss: 1.08522863e-06
Iter: 1154 loss: 1.08837628e-06
Iter: 1155 loss: 1.08526035e-06
Iter: 1156 loss: 1.0850996e-06
Iter: 1157 loss: 1.08511426e-06
Iter: 1158 loss: 1.08500569e-06
Iter: 1159 loss: 1.08477525e-06
Iter: 1160 loss: 1.08612608e-06
Iter: 1161 loss: 1.084779e-06
Iter: 1162 loss: 1.08465451e-06
Iter: 1163 loss: 1.08446852e-06
Iter: 1164 loss: 1.08937e-06
Iter: 1165 loss: 1.08447489e-06
Iter: 1166 loss: 1.08423592e-06
Iter: 1167 loss: 1.08592133e-06
Iter: 1168 loss: 1.08421898e-06
Iter: 1169 loss: 1.08407085e-06
Iter: 1170 loss: 1.08542838e-06
Iter: 1171 loss: 1.08403674e-06
Iter: 1172 loss: 1.08390986e-06
Iter: 1173 loss: 1.08381892e-06
Iter: 1174 loss: 1.08378481e-06
Iter: 1175 loss: 1.08358176e-06
Iter: 1176 loss: 1.0851079e-06
Iter: 1177 loss: 1.08357722e-06
Iter: 1178 loss: 1.08341123e-06
Iter: 1179 loss: 1.08366373e-06
Iter: 1180 loss: 1.08325639e-06
Iter: 1181 loss: 1.08313861e-06
Iter: 1182 loss: 1.08317931e-06
Iter: 1183 loss: 1.08302834e-06
Iter: 1184 loss: 1.08278539e-06
Iter: 1185 loss: 1.08286849e-06
Iter: 1186 loss: 1.08265601e-06
Iter: 1187 loss: 1.08240363e-06
Iter: 1188 loss: 1.08243034e-06
Iter: 1189 loss: 1.08225095e-06
Iter: 1190 loss: 1.08223935e-06
Iter: 1191 loss: 1.08213749e-06
Iter: 1192 loss: 1.0819324e-06
Iter: 1193 loss: 1.08350036e-06
Iter: 1194 loss: 1.08191409e-06
Iter: 1195 loss: 1.08174856e-06
Iter: 1196 loss: 1.08156576e-06
Iter: 1197 loss: 1.08156019e-06
Iter: 1198 loss: 1.08129348e-06
Iter: 1199 loss: 1.08333074e-06
Iter: 1200 loss: 1.08125596e-06
Iter: 1201 loss: 1.08112067e-06
Iter: 1202 loss: 1.08315908e-06
Iter: 1203 loss: 1.08114045e-06
Iter: 1204 loss: 1.08100835e-06
Iter: 1205 loss: 1.08078461e-06
Iter: 1206 loss: 1.08081827e-06
Iter: 1207 loss: 1.08062318e-06
Iter: 1208 loss: 1.08315737e-06
Iter: 1209 loss: 1.08060976e-06
Iter: 1210 loss: 1.08042696e-06
Iter: 1211 loss: 1.08094764e-06
Iter: 1212 loss: 1.08035374e-06
Iter: 1213 loss: 1.08023096e-06
Iter: 1214 loss: 1.08022323e-06
Iter: 1215 loss: 1.08014933e-06
Iter: 1216 loss: 1.07992264e-06
Iter: 1217 loss: 1.07999529e-06
Iter: 1218 loss: 1.07983078e-06
Iter: 1219 loss: 1.07964422e-06
Iter: 1220 loss: 1.07962387e-06
Iter: 1221 loss: 1.07951382e-06
Iter: 1222 loss: 1.07946448e-06
Iter: 1223 loss: 1.07940218e-06
Iter: 1224 loss: 1.07916253e-06
Iter: 1225 loss: 1.08118024e-06
Iter: 1226 loss: 1.0791764e-06
Iter: 1227 loss: 1.07906249e-06
Iter: 1228 loss: 1.07881533e-06
Iter: 1229 loss: 1.08415975e-06
Iter: 1230 loss: 1.07880192e-06
Iter: 1231 loss: 1.07848246e-06
Iter: 1232 loss: 1.08010454e-06
Iter: 1233 loss: 1.07846245e-06
Iter: 1234 loss: 1.07832318e-06
Iter: 1235 loss: 1.0782644e-06
Iter: 1236 loss: 1.0781456e-06
Iter: 1237 loss: 1.07799929e-06
Iter: 1238 loss: 1.07794585e-06
Iter: 1239 loss: 1.07774304e-06
Iter: 1240 loss: 1.07949586e-06
Iter: 1241 loss: 1.07775577e-06
Iter: 1242 loss: 1.07752692e-06
Iter: 1243 loss: 1.07826054e-06
Iter: 1244 loss: 1.07745427e-06
Iter: 1245 loss: 1.07734513e-06
Iter: 1246 loss: 1.07726464e-06
Iter: 1247 loss: 1.07720609e-06
Iter: 1248 loss: 1.07699736e-06
Iter: 1249 loss: 1.07708581e-06
Iter: 1250 loss: 1.07685719e-06
Iter: 1251 loss: 1.07669257e-06
Iter: 1252 loss: 1.07666256e-06
Iter: 1253 loss: 1.07650567e-06
Iter: 1254 loss: 1.07639869e-06
Iter: 1255 loss: 1.07635549e-06
Iter: 1256 loss: 1.07617734e-06
Iter: 1257 loss: 1.07856363e-06
Iter: 1258 loss: 1.0761969e-06
Iter: 1259 loss: 1.0760441e-06
Iter: 1260 loss: 1.0757874e-06
Iter: 1261 loss: 1.08027e-06
Iter: 1262 loss: 1.07578512e-06
Iter: 1263 loss: 1.07550568e-06
Iter: 1264 loss: 1.07690971e-06
Iter: 1265 loss: 1.07546657e-06
Iter: 1266 loss: 1.07531309e-06
Iter: 1267 loss: 1.07530911e-06
Iter: 1268 loss: 1.07517826e-06
Iter: 1269 loss: 1.07503399e-06
Iter: 1270 loss: 1.0750357e-06
Iter: 1271 loss: 1.07484391e-06
Iter: 1272 loss: 1.07601886e-06
Iter: 1273 loss: 1.07484038e-06
Iter: 1274 loss: 1.0745955e-06
Iter: 1275 loss: 1.0753605e-06
Iter: 1276 loss: 1.0745706e-06
Iter: 1277 loss: 1.07444464e-06
Iter: 1278 loss: 1.07443043e-06
Iter: 1279 loss: 1.07431742e-06
Iter: 1280 loss: 1.07409301e-06
Iter: 1281 loss: 1.0740323e-06
Iter: 1282 loss: 1.07396568e-06
Iter: 1283 loss: 1.07371443e-06
Iter: 1284 loss: 1.07368407e-06
Iter: 1285 loss: 1.07353958e-06
Iter: 1286 loss: 1.07357801e-06
Iter: 1287 loss: 1.07340645e-06
Iter: 1288 loss: 1.07320147e-06
Iter: 1289 loss: 1.07549226e-06
Iter: 1290 loss: 1.0732017e-06
Iter: 1291 loss: 1.07308324e-06
Iter: 1292 loss: 1.07275287e-06
Iter: 1293 loss: 1.07713163e-06
Iter: 1294 loss: 1.07274354e-06
Iter: 1295 loss: 1.07242079e-06
Iter: 1296 loss: 1.07383016e-06
Iter: 1297 loss: 1.07233359e-06
Iter: 1298 loss: 1.0721576e-06
Iter: 1299 loss: 1.07515802e-06
Iter: 1300 loss: 1.07216761e-06
Iter: 1301 loss: 1.07201072e-06
Iter: 1302 loss: 1.07181518e-06
Iter: 1303 loss: 1.07182541e-06
Iter: 1304 loss: 1.07152414e-06
Iter: 1305 loss: 1.07273809e-06
Iter: 1306 loss: 1.07154483e-06
Iter: 1307 loss: 1.07127971e-06
Iter: 1308 loss: 1.07275036e-06
Iter: 1309 loss: 1.07122764e-06
Iter: 1310 loss: 1.0710927e-06
Iter: 1311 loss: 1.07100846e-06
Iter: 1312 loss: 1.07093797e-06
Iter: 1313 loss: 1.07073038e-06
Iter: 1314 loss: 1.07085941e-06
Iter: 1315 loss: 1.0705719e-06
Iter: 1316 loss: 1.07038227e-06
Iter: 1317 loss: 1.07035794e-06
Iter: 1318 loss: 1.07022697e-06
Iter: 1319 loss: 1.07023448e-06
Iter: 1320 loss: 1.07005565e-06
Iter: 1321 loss: 1.06992036e-06
Iter: 1322 loss: 1.07160758e-06
Iter: 1323 loss: 1.06991365e-06
Iter: 1324 loss: 1.06978837e-06
Iter: 1325 loss: 1.06954042e-06
Iter: 1326 loss: 1.07509993e-06
Iter: 1327 loss: 1.06950711e-06
Iter: 1328 loss: 1.06927814e-06
Iter: 1329 loss: 1.07042638e-06
Iter: 1330 loss: 1.06925734e-06
Iter: 1331 loss: 1.06909499e-06
Iter: 1332 loss: 1.07127266e-06
Iter: 1333 loss: 1.06911443e-06
Iter: 1334 loss: 1.06900382e-06
Iter: 1335 loss: 1.06895948e-06
Iter: 1336 loss: 1.06884499e-06
Iter: 1337 loss: 1.06870539e-06
Iter: 1338 loss: 1.06922982e-06
Iter: 1339 loss: 1.06864593e-06
Iter: 1340 loss: 1.06847733e-06
Iter: 1341 loss: 1.06995253e-06
Iter: 1342 loss: 1.06850155e-06
Iter: 1343 loss: 1.06836433e-06
Iter: 1344 loss: 1.06826087e-06
Iter: 1345 loss: 1.06821631e-06
Iter: 1346 loss: 1.06806601e-06
Iter: 1347 loss: 1.06809011e-06
Iter: 1348 loss: 1.06793141e-06
Iter: 1349 loss: 1.06776542e-06
Iter: 1350 loss: 1.07031769e-06
Iter: 1351 loss: 1.06777088e-06
Iter: 1352 loss: 1.06764537e-06
Iter: 1353 loss: 1.06783114e-06
Iter: 1354 loss: 1.06752498e-06
Iter: 1355 loss: 1.06742459e-06
Iter: 1356 loss: 1.06824643e-06
Iter: 1357 loss: 1.06739401e-06
Iter: 1358 loss: 1.06733489e-06
Iter: 1359 loss: 1.06711161e-06
Iter: 1360 loss: 1.06713981e-06
Iter: 1361 loss: 1.06691618e-06
Iter: 1362 loss: 1.06784046e-06
Iter: 1363 loss: 1.06689436e-06
Iter: 1364 loss: 1.06676191e-06
Iter: 1365 loss: 1.0683932e-06
Iter: 1366 loss: 1.06677271e-06
Iter: 1367 loss: 1.06667926e-06
Iter: 1368 loss: 1.06659695e-06
Iter: 1369 loss: 1.06656535e-06
Iter: 1370 loss: 1.06642369e-06
Iter: 1371 loss: 1.06687503e-06
Iter: 1372 loss: 1.06639334e-06
Iter: 1373 loss: 1.06625509e-06
Iter: 1374 loss: 1.06794607e-06
Iter: 1375 loss: 1.06623634e-06
Iter: 1376 loss: 1.06614152e-06
Iter: 1377 loss: 1.06605955e-06
Iter: 1378 loss: 1.06607445e-06
Iter: 1379 loss: 1.06589528e-06
Iter: 1380 loss: 1.0660109e-06
Iter: 1381 loss: 1.06579262e-06
Iter: 1382 loss: 1.0656795e-06
Iter: 1383 loss: 1.06750156e-06
Iter: 1384 loss: 1.06563812e-06
Iter: 1385 loss: 1.06554637e-06
Iter: 1386 loss: 1.0658398e-06
Iter: 1387 loss: 1.06551875e-06
Iter: 1388 loss: 1.06541938e-06
Iter: 1389 loss: 1.0659744e-06
Iter: 1390 loss: 1.06541495e-06
Iter: 1391 loss: 1.06529865e-06
Iter: 1392 loss: 1.06522134e-06
Iter: 1393 loss: 1.06519269e-06
Iter: 1394 loss: 1.06502716e-06
Iter: 1395 loss: 1.06561811e-06
Iter: 1396 loss: 1.06500204e-06
Iter: 1397 loss: 1.06486095e-06
Iter: 1398 loss: 1.06484924e-06
Iter: 1399 loss: 1.06480104e-06
Iter: 1400 loss: 1.06474863e-06
Iter: 1401 loss: 1.06472976e-06
Iter: 1402 loss: 1.06459765e-06
Iter: 1403 loss: 1.06480366e-06
Iter: 1404 loss: 1.06455809e-06
Iter: 1405 loss: 1.06442963e-06
Iter: 1406 loss: 1.06582729e-06
Iter: 1407 loss: 1.06440575e-06
Iter: 1408 loss: 1.06431082e-06
Iter: 1409 loss: 1.06423192e-06
Iter: 1410 loss: 1.06420669e-06
Iter: 1411 loss: 1.06410243e-06
Iter: 1412 loss: 1.06411062e-06
Iter: 1413 loss: 1.06402683e-06
Iter: 1414 loss: 1.06385244e-06
Iter: 1415 loss: 1.06561333e-06
Iter: 1416 loss: 1.06383641e-06
Iter: 1417 loss: 1.06369362e-06
Iter: 1418 loss: 1.06420748e-06
Iter: 1419 loss: 1.06368395e-06
Iter: 1420 loss: 1.06359232e-06
Iter: 1421 loss: 1.06396101e-06
Iter: 1422 loss: 1.06353662e-06
Iter: 1423 loss: 1.06343077e-06
Iter: 1424 loss: 1.06324774e-06
Iter: 1425 loss: 1.06323057e-06
Iter: 1426 loss: 1.06306447e-06
Iter: 1427 loss: 1.06352559e-06
Iter: 1428 loss: 1.06301343e-06
Iter: 1429 loss: 1.06286643e-06
Iter: 1430 loss: 1.06286154e-06
Iter: 1431 loss: 1.06274263e-06
Iter: 1432 loss: 1.06265759e-06
Iter: 1433 loss: 1.06264088e-06
Iter: 1434 loss: 1.06246057e-06
Iter: 1435 loss: 1.06291179e-06
Iter: 1436 loss: 1.06240736e-06
Iter: 1437 loss: 1.06225161e-06
Iter: 1438 loss: 1.06413142e-06
Iter: 1439 loss: 1.06223251e-06
Iter: 1440 loss: 1.0621302e-06
Iter: 1441 loss: 1.06206789e-06
Iter: 1442 loss: 1.06202731e-06
Iter: 1443 loss: 1.06185576e-06
Iter: 1444 loss: 1.06190623e-06
Iter: 1445 loss: 1.06174093e-06
Iter: 1446 loss: 1.0615679e-06
Iter: 1447 loss: 1.06364564e-06
Iter: 1448 loss: 1.06153152e-06
Iter: 1449 loss: 1.06141351e-06
Iter: 1450 loss: 1.06205357e-06
Iter: 1451 loss: 1.06135826e-06
Iter: 1452 loss: 1.06129596e-06
Iter: 1453 loss: 1.06157518e-06
Iter: 1454 loss: 1.06121593e-06
Iter: 1455 loss: 1.06111463e-06
Iter: 1456 loss: 1.06092011e-06
Iter: 1457 loss: 1.06089328e-06
Iter: 1458 loss: 1.06071639e-06
Iter: 1459 loss: 1.06100424e-06
Iter: 1460 loss: 1.06058712e-06
Iter: 1461 loss: 1.06044547e-06
Iter: 1462 loss: 1.06284961e-06
Iter: 1463 loss: 1.06044274e-06
Iter: 1464 loss: 1.06024709e-06
Iter: 1465 loss: 1.06030552e-06
Iter: 1466 loss: 1.06011612e-06
Iter: 1467 loss: 1.06001312e-06
Iter: 1468 loss: 1.06048e-06
Iter: 1469 loss: 1.05994286e-06
Iter: 1470 loss: 1.05983531e-06
Iter: 1471 loss: 1.06121797e-06
Iter: 1472 loss: 1.05981337e-06
Iter: 1473 loss: 1.05968081e-06
Iter: 1474 loss: 1.05960362e-06
Iter: 1475 loss: 1.05958657e-06
Iter: 1476 loss: 1.05942036e-06
Iter: 1477 loss: 1.05952063e-06
Iter: 1478 loss: 1.05930667e-06
Iter: 1479 loss: 1.05917889e-06
Iter: 1480 loss: 1.06033303e-06
Iter: 1481 loss: 1.05917331e-06
Iter: 1482 loss: 1.05900472e-06
Iter: 1483 loss: 1.0598842e-06
Iter: 1484 loss: 1.05896856e-06
Iter: 1485 loss: 1.05887159e-06
Iter: 1486 loss: 1.05913682e-06
Iter: 1487 loss: 1.05879963e-06
Iter: 1488 loss: 1.05864933e-06
Iter: 1489 loss: 1.05864513e-06
Iter: 1490 loss: 1.05848744e-06
Iter: 1491 loss: 1.05837842e-06
Iter: 1492 loss: 1.05842958e-06
Iter: 1493 loss: 1.05823256e-06
Iter: 1494 loss: 1.05809465e-06
Iter: 1495 loss: 1.05809852e-06
Iter: 1496 loss: 1.05799916e-06
Iter: 1497 loss: 1.05812387e-06
Iter: 1498 loss: 1.05793856e-06
Iter: 1499 loss: 1.05781066e-06
Iter: 1500 loss: 1.05807544e-06
Iter: 1501 loss: 1.0577894e-06
Iter: 1502 loss: 1.0576008e-06
Iter: 1503 loss: 1.05867457e-06
Iter: 1504 loss: 1.05765048e-06
Iter: 1505 loss: 1.05757556e-06
Iter: 1506 loss: 1.05750723e-06
Iter: 1507 loss: 1.05747256e-06
Iter: 1508 loss: 1.05733e-06
Iter: 1509 loss: 1.0573392e-06
Iter: 1510 loss: 1.05721756e-06
Iter: 1511 loss: 1.05704498e-06
Iter: 1512 loss: 1.05842344e-06
Iter: 1513 loss: 1.05705487e-06
Iter: 1514 loss: 1.05691811e-06
Iter: 1515 loss: 1.05779304e-06
Iter: 1516 loss: 1.0568607e-06
Iter: 1517 loss: 1.05681056e-06
Iter: 1518 loss: 1.0569604e-06
Iter: 1519 loss: 1.05676429e-06
Iter: 1520 loss: 1.0565974e-06
Iter: 1521 loss: 1.0566697e-06
Iter: 1522 loss: 1.05647428e-06
Iter: 1523 loss: 1.05632216e-06
Iter: 1524 loss: 1.05632023e-06
Iter: 1525 loss: 1.05626623e-06
Iter: 1526 loss: 1.05614754e-06
Iter: 1527 loss: 1.05610343e-06
Iter: 1528 loss: 1.05598542e-06
Iter: 1529 loss: 1.05609433e-06
Iter: 1530 loss: 1.05588799e-06
Iter: 1531 loss: 1.05578397e-06
Iter: 1532 loss: 1.05615027e-06
Iter: 1533 loss: 1.05577249e-06
Iter: 1534 loss: 1.05566983e-06
Iter: 1535 loss: 1.05673575e-06
Iter: 1536 loss: 1.05564982e-06
Iter: 1537 loss: 1.05556569e-06
Iter: 1538 loss: 1.05548349e-06
Iter: 1539 loss: 1.0554736e-06
Iter: 1540 loss: 1.05531763e-06
Iter: 1541 loss: 1.05542495e-06
Iter: 1542 loss: 1.0552028e-06
Iter: 1543 loss: 1.05507559e-06
Iter: 1544 loss: 1.05578738e-06
Iter: 1545 loss: 1.05507297e-06
Iter: 1546 loss: 1.05491108e-06
Iter: 1547 loss: 1.05596575e-06
Iter: 1548 loss: 1.05489096e-06
Iter: 1549 loss: 1.05480535e-06
Iter: 1550 loss: 1.0549636e-06
Iter: 1551 loss: 1.05477773e-06
Iter: 1552 loss: 1.05457457e-06
Iter: 1553 loss: 1.0548024e-06
Iter: 1554 loss: 1.054515e-06
Iter: 1555 loss: 1.05438801e-06
Iter: 1556 loss: 1.05427466e-06
Iter: 1557 loss: 1.05418701e-06
Iter: 1558 loss: 1.05406707e-06
Iter: 1559 loss: 1.05597871e-06
Iter: 1560 loss: 1.05409822e-06
Iter: 1561 loss: 1.0538788e-06
Iter: 1562 loss: 1.05418462e-06
Iter: 1563 loss: 1.05379775e-06
Iter: 1564 loss: 1.05363688e-06
Iter: 1565 loss: 1.05394417e-06
Iter: 1566 loss: 1.05360118e-06
Iter: 1567 loss: 1.05345794e-06
Iter: 1568 loss: 1.05478205e-06
Iter: 1569 loss: 1.05346317e-06
Iter: 1570 loss: 1.05333947e-06
Iter: 1571 loss: 1.05326717e-06
Iter: 1572 loss: 1.05320294e-06
Iter: 1573 loss: 1.05303639e-06
Iter: 1574 loss: 1.05308663e-06
Iter: 1575 loss: 1.0528945e-06
Iter: 1576 loss: 1.05269532e-06
Iter: 1577 loss: 1.05360198e-06
Iter: 1578 loss: 1.05263973e-06
Iter: 1579 loss: 1.05246863e-06
Iter: 1580 loss: 1.0543281e-06
Iter: 1581 loss: 1.05243475e-06
Iter: 1582 loss: 1.05229992e-06
Iter: 1583 loss: 1.0523371e-06
Iter: 1584 loss: 1.05221818e-06
Iter: 1585 loss: 1.05200741e-06
Iter: 1586 loss: 1.05276035e-06
Iter: 1587 loss: 1.05193101e-06
Iter: 1588 loss: 1.05181141e-06
Iter: 1589 loss: 1.05164679e-06
Iter: 1590 loss: 1.05164202e-06
Iter: 1591 loss: 1.05142124e-06
Iter: 1592 loss: 1.05380627e-06
Iter: 1593 loss: 1.05142067e-06
Iter: 1594 loss: 1.05121933e-06
Iter: 1595 loss: 1.05185006e-06
Iter: 1596 loss: 1.05119193e-06
Iter: 1597 loss: 1.05106267e-06
Iter: 1598 loss: 1.05133506e-06
Iter: 1599 loss: 1.05097524e-06
Iter: 1600 loss: 1.05087952e-06
Iter: 1601 loss: 1.05194647e-06
Iter: 1602 loss: 1.05083268e-06
Iter: 1603 loss: 1.05076458e-06
Iter: 1604 loss: 1.05062566e-06
Iter: 1605 loss: 1.05057529e-06
Iter: 1606 loss: 1.05043114e-06
Iter: 1607 loss: 1.05056847e-06
Iter: 1608 loss: 1.05032814e-06
Iter: 1609 loss: 1.05008621e-06
Iter: 1610 loss: 1.05146478e-06
Iter: 1611 loss: 1.05009e-06
Iter: 1612 loss: 1.04992728e-06
Iter: 1613 loss: 1.05193453e-06
Iter: 1614 loss: 1.0499208e-06
Iter: 1615 loss: 1.04979551e-06
Iter: 1616 loss: 1.04972867e-06
Iter: 1617 loss: 1.0496924e-06
Iter: 1618 loss: 1.04948776e-06
Iter: 1619 loss: 1.0503195e-06
Iter: 1620 loss: 1.04946207e-06
Iter: 1621 loss: 1.04933861e-06
Iter: 1622 loss: 1.04923402e-06
Iter: 1623 loss: 1.04918206e-06
Iter: 1624 loss: 1.04899277e-06
Iter: 1625 loss: 1.05019694e-06
Iter: 1626 loss: 1.04898311e-06
Iter: 1627 loss: 1.04875187e-06
Iter: 1628 loss: 1.04952528e-06
Iter: 1629 loss: 1.04869673e-06
Iter: 1630 loss: 1.04857622e-06
Iter: 1631 loss: 1.0489515e-06
Iter: 1632 loss: 1.04848777e-06
Iter: 1633 loss: 1.0484049e-06
Iter: 1634 loss: 1.04992785e-06
Iter: 1635 loss: 1.04837261e-06
Iter: 1636 loss: 1.04824119e-06
Iter: 1637 loss: 1.04804849e-06
Iter: 1638 loss: 1.05356241e-06
Iter: 1639 loss: 1.04802143e-06
Iter: 1640 loss: 1.04772357e-06
Iter: 1641 loss: 1.04811215e-06
Iter: 1642 loss: 1.04761807e-06
Iter: 1643 loss: 1.04729554e-06
Iter: 1644 loss: 1.04819674e-06
Iter: 1645 loss: 1.04727292e-06
Iter: 1646 loss: 1.04701621e-06
Iter: 1647 loss: 1.05039987e-06
Iter: 1648 loss: 1.04704327e-06
Iter: 1649 loss: 1.04684591e-06
Iter: 1650 loss: 1.04678486e-06
Iter: 1651 loss: 1.04668175e-06
Iter: 1652 loss: 1.04646131e-06
Iter: 1653 loss: 1.04820288e-06
Iter: 1654 loss: 1.04642561e-06
Iter: 1655 loss: 1.04627895e-06
Iter: 1656 loss: 1.04603532e-06
Iter: 1657 loss: 1.05074412e-06
Iter: 1658 loss: 1.04600485e-06
Iter: 1659 loss: 1.04576293e-06
Iter: 1660 loss: 1.04749915e-06
Iter: 1661 loss: 1.04573962e-06
Iter: 1662 loss: 1.04542391e-06
Iter: 1663 loss: 1.04765741e-06
Iter: 1664 loss: 1.04542562e-06
Iter: 1665 loss: 1.04527976e-06
Iter: 1666 loss: 1.04535525e-06
Iter: 1667 loss: 1.04516221e-06
Iter: 1668 loss: 1.04501294e-06
Iter: 1669 loss: 1.04673302e-06
Iter: 1670 loss: 1.04503658e-06
Iter: 1671 loss: 1.04488208e-06
Iter: 1672 loss: 1.04482183e-06
Iter: 1673 loss: 1.04476203e-06
Iter: 1674 loss: 1.04460207e-06
Iter: 1675 loss: 1.04473702e-06
Iter: 1676 loss: 1.0444852e-06
Iter: 1677 loss: 1.04433036e-06
Iter: 1678 loss: 1.04536161e-06
Iter: 1679 loss: 1.04426863e-06
Iter: 1680 loss: 1.04420064e-06
Iter: 1681 loss: 1.04418586e-06
Iter: 1682 loss: 1.04407809e-06
Iter: 1683 loss: 1.04396327e-06
Iter: 1684 loss: 1.04394144e-06
Iter: 1685 loss: 1.04376386e-06
Iter: 1686 loss: 1.04577964e-06
Iter: 1687 loss: 1.04377671e-06
Iter: 1688 loss: 1.04363994e-06
Iter: 1689 loss: 1.04344372e-06
Iter: 1690 loss: 1.04826336e-06
Iter: 1691 loss: 1.04345008e-06
Iter: 1692 loss: 1.0432459e-06
Iter: 1693 loss: 1.0439046e-06
Iter: 1694 loss: 1.04317428e-06
Iter: 1695 loss: 1.04304513e-06
Iter: 1696 loss: 1.04301307e-06
Iter: 1697 loss: 1.04291428e-06
Iter: 1698 loss: 1.04283072e-06
Iter: 1699 loss: 1.04281344e-06
Iter: 1700 loss: 1.04266e-06
Iter: 1701 loss: 1.04445837e-06
Iter: 1702 loss: 1.04267531e-06
Iter: 1703 loss: 1.04256492e-06
Iter: 1704 loss: 1.04239246e-06
Iter: 1705 loss: 1.04236119e-06
Iter: 1706 loss: 1.04215565e-06
Iter: 1707 loss: 1.04241212e-06
Iter: 1708 loss: 1.0420888e-06
Iter: 1709 loss: 1.04181e-06
Iter: 1710 loss: 1.04291053e-06
Iter: 1711 loss: 1.04177184e-06
Iter: 1712 loss: 1.04157084e-06
Iter: 1713 loss: 1.04337823e-06
Iter: 1714 loss: 1.04159699e-06
Iter: 1715 loss: 1.04144021e-06
Iter: 1716 loss: 1.04122466e-06
Iter: 1717 loss: 1.04123205e-06
Iter: 1718 loss: 1.04107801e-06
Iter: 1719 loss: 1.04104652e-06
Iter: 1720 loss: 1.04089838e-06
Iter: 1721 loss: 1.04069431e-06
Iter: 1722 loss: 1.04641697e-06
Iter: 1723 loss: 1.04070989e-06
Iter: 1724 loss: 1.04045409e-06
Iter: 1725 loss: 1.04074741e-06
Iter: 1726 loss: 1.04028447e-06
Iter: 1727 loss: 1.04018136e-06
Iter: 1728 loss: 1.04016965e-06
Iter: 1729 loss: 1.04002959e-06
Iter: 1730 loss: 1.03987259e-06
Iter: 1731 loss: 1.04386379e-06
Iter: 1732 loss: 1.03984144e-06
Iter: 1733 loss: 1.03961884e-06
Iter: 1734 loss: 1.04177525e-06
Iter: 1735 loss: 1.03964271e-06
Iter: 1736 loss: 1.03946343e-06
Iter: 1737 loss: 1.03948582e-06
Iter: 1738 loss: 1.03939283e-06
Iter: 1739 loss: 1.03914795e-06
Iter: 1740 loss: 1.03911214e-06
Iter: 1741 loss: 1.03898583e-06
Iter: 1742 loss: 1.03879347e-06
Iter: 1743 loss: 1.03990487e-06
Iter: 1744 loss: 1.03868069e-06
Iter: 1745 loss: 1.03860702e-06
Iter: 1746 loss: 1.03855916e-06
Iter: 1747 loss: 1.03848129e-06
Iter: 1748 loss: 1.03840523e-06
Iter: 1749 loss: 1.03836453e-06
Iter: 1750 loss: 1.03831871e-06
Iter: 1751 loss: 1.03977163e-06
Iter: 1752 loss: 1.03828449e-06
Iter: 1753 loss: 1.03815375e-06
Iter: 1754 loss: 1.03815182e-06
Iter: 1755 loss: 1.03801335e-06
Iter: 1756 loss: 1.03793548e-06
Iter: 1757 loss: 1.03792649e-06
Iter: 1758 loss: 1.03785032e-06
Iter: 1759 loss: 1.03784078e-06
Iter: 1760 loss: 1.03775506e-06
Iter: 1761 loss: 1.03769491e-06
Iter: 1762 loss: 1.03760215e-06
Iter: 1763 loss: 1.03757429e-06
Iter: 1764 loss: 1.03746584e-06
Iter: 1765 loss: 1.0386608e-06
Iter: 1766 loss: 1.03749562e-06
Iter: 1767 loss: 1.03734737e-06
Iter: 1768 loss: 1.03754905e-06
Iter: 1769 loss: 1.03728598e-06
Iter: 1770 loss: 1.03717343e-06
Iter: 1771 loss: 1.03722732e-06
Iter: 1772 loss: 1.03711523e-06
Iter: 1773 loss: 1.03701279e-06
Iter: 1774 loss: 1.03681043e-06
Iter: 1775 loss: 1.03678235e-06
Iter: 1776 loss: 1.03652519e-06
Iter: 1777 loss: 1.03855882e-06
Iter: 1778 loss: 1.03644061e-06
Iter: 1779 loss: 1.03618549e-06
Iter: 1780 loss: 1.03719321e-06
Iter: 1781 loss: 1.03612729e-06
Iter: 1782 loss: 1.03600166e-06
Iter: 1783 loss: 1.03585444e-06
Iter: 1784 loss: 1.03580135e-06
Iter: 1785 loss: 1.03553452e-06
Iter: 1786 loss: 1.03878938e-06
Iter: 1787 loss: 1.03551235e-06
Iter: 1788 loss: 1.03539662e-06
Iter: 1789 loss: 1.03515663e-06
Iter: 1790 loss: 1.03869547e-06
Iter: 1791 loss: 1.03512252e-06
Iter: 1792 loss: 1.03493846e-06
Iter: 1793 loss: 1.03802381e-06
Iter: 1794 loss: 1.03490311e-06
Iter: 1795 loss: 1.03474565e-06
Iter: 1796 loss: 1.03637672e-06
Iter: 1797 loss: 1.03470302e-06
Iter: 1798 loss: 1.03458854e-06
Iter: 1799 loss: 1.0345567e-06
Iter: 1800 loss: 1.03449361e-06
Iter: 1801 loss: 1.03434434e-06
Iter: 1802 loss: 1.03551076e-06
Iter: 1803 loss: 1.03426248e-06
Iter: 1804 loss: 1.03414027e-06
Iter: 1805 loss: 1.03417824e-06
Iter: 1806 loss: 1.03403829e-06
Iter: 1807 loss: 1.03381876e-06
Iter: 1808 loss: 1.03404e-06
Iter: 1809 loss: 1.03372759e-06
Iter: 1810 loss: 1.03356115e-06
Iter: 1811 loss: 1.03413777e-06
Iter: 1812 loss: 1.03348384e-06
Iter: 1813 loss: 1.03331706e-06
Iter: 1814 loss: 1.03332775e-06
Iter: 1815 loss: 1.03324783e-06
Iter: 1816 loss: 1.03301704e-06
Iter: 1817 loss: 1.03722152e-06
Iter: 1818 loss: 1.03302546e-06
Iter: 1819 loss: 1.03279888e-06
Iter: 1820 loss: 1.0327708e-06
Iter: 1821 loss: 1.03268326e-06
Iter: 1822 loss: 1.03239972e-06
Iter: 1823 loss: 1.03556476e-06
Iter: 1824 loss: 1.03234731e-06
Iter: 1825 loss: 1.03195975e-06
Iter: 1826 loss: 1.03243542e-06
Iter: 1827 loss: 1.03183174e-06
Iter: 1828 loss: 1.03197453e-06
Iter: 1829 loss: 1.03172101e-06
Iter: 1830 loss: 1.03160221e-06
Iter: 1831 loss: 1.03167918e-06
Iter: 1832 loss: 1.03155935e-06
Iter: 1833 loss: 1.03145817e-06
Iter: 1834 loss: 1.03206287e-06
Iter: 1835 loss: 1.03145931e-06
Iter: 1836 loss: 1.0313338e-06
Iter: 1837 loss: 1.03118487e-06
Iter: 1838 loss: 1.03120044e-06
Iter: 1839 loss: 1.03101706e-06
Iter: 1840 loss: 1.03145e-06
Iter: 1841 loss: 1.03092975e-06
Iter: 1842 loss: 1.03078014e-06
Iter: 1843 loss: 1.03078878e-06
Iter: 1844 loss: 1.0306145e-06
Iter: 1845 loss: 1.03054765e-06
Iter: 1846 loss: 1.03050752e-06
Iter: 1847 loss: 1.03036973e-06
Iter: 1848 loss: 1.0302831e-06
Iter: 1849 loss: 1.03025729e-06
Iter: 1850 loss: 1.0301037e-06
Iter: 1851 loss: 1.03002606e-06
Iter: 1852 loss: 1.02992726e-06
Iter: 1853 loss: 1.02965964e-06
Iter: 1854 loss: 1.03340403e-06
Iter: 1855 loss: 1.02967624e-06
Iter: 1856 loss: 1.02953277e-06
Iter: 1857 loss: 1.02927584e-06
Iter: 1858 loss: 1.03388186e-06
Iter: 1859 loss: 1.02929755e-06
Iter: 1860 loss: 1.02920694e-06
Iter: 1861 loss: 1.02913157e-06
Iter: 1862 loss: 1.02908268e-06
Iter: 1863 loss: 1.02910803e-06
Iter: 1864 loss: 1.02899082e-06
Iter: 1865 loss: 1.02880495e-06
Iter: 1866 loss: 1.02984177e-06
Iter: 1867 loss: 1.02878903e-06
Iter: 1868 loss: 1.02871468e-06
Iter: 1869 loss: 1.02875731e-06
Iter: 1870 loss: 1.02869581e-06
Iter: 1871 loss: 1.0285662e-06
Iter: 1872 loss: 1.02850788e-06
Iter: 1873 loss: 1.02845138e-06
Iter: 1874 loss: 1.02830018e-06
Iter: 1875 loss: 1.0291684e-06
Iter: 1876 loss: 1.02824856e-06
Iter: 1877 loss: 1.02811418e-06
Iter: 1878 loss: 1.02832917e-06
Iter: 1879 loss: 1.02802483e-06
Iter: 1880 loss: 1.0279465e-06
Iter: 1881 loss: 1.02962963e-06
Iter: 1882 loss: 1.02793797e-06
Iter: 1883 loss: 1.02781632e-06
Iter: 1884 loss: 1.02762726e-06
Iter: 1885 loss: 1.03031607e-06
Iter: 1886 loss: 1.02761248e-06
Iter: 1887 loss: 1.02743809e-06
Iter: 1888 loss: 1.02830609e-06
Iter: 1889 loss: 1.02733134e-06
Iter: 1890 loss: 1.02714728e-06
Iter: 1891 loss: 1.02997365e-06
Iter: 1892 loss: 1.02716649e-06
Iter: 1893 loss: 1.02712613e-06
Iter: 1894 loss: 1.02687943e-06
Iter: 1895 loss: 1.02881813e-06
Iter: 1896 loss: 1.02687432e-06
Iter: 1897 loss: 1.02677893e-06
Iter: 1898 loss: 1.02671447e-06
Iter: 1899 loss: 1.02660681e-06
Iter: 1900 loss: 1.02640092e-06
Iter: 1901 loss: 1.03093248e-06
Iter: 1902 loss: 1.02639e-06
Iter: 1903 loss: 1.02619128e-06
Iter: 1904 loss: 1.02637034e-06
Iter: 1905 loss: 1.02616832e-06
Iter: 1906 loss: 1.02602746e-06
Iter: 1907 loss: 1.02765875e-06
Iter: 1908 loss: 1.02604838e-06
Iter: 1909 loss: 1.02588501e-06
Iter: 1910 loss: 1.02595322e-06
Iter: 1911 loss: 1.02582931e-06
Iter: 1912 loss: 1.02574461e-06
Iter: 1913 loss: 1.02566639e-06
Iter: 1914 loss: 1.0256424e-06
Iter: 1915 loss: 1.02553759e-06
Iter: 1916 loss: 1.02562467e-06
Iter: 1917 loss: 1.0254729e-06
Iter: 1918 loss: 1.02529248e-06
Iter: 1919 loss: 1.02597096e-06
Iter: 1920 loss: 1.02526928e-06
Iter: 1921 loss: 1.02510569e-06
Iter: 1922 loss: 1.02479214e-06
Iter: 1923 loss: 1.02479066e-06
Iter: 1924 loss: 1.02461968e-06
Iter: 1925 loss: 1.02465515e-06
Iter: 1926 loss: 1.02436536e-06
Iter: 1927 loss: 1.02457716e-06
Iter: 1928 loss: 1.02424394e-06
Iter: 1929 loss: 1.0241364e-06
Iter: 1930 loss: 1.02422575e-06
Iter: 1931 loss: 1.02404783e-06
Iter: 1932 loss: 1.02397155e-06
Iter: 1933 loss: 1.02393938e-06
Iter: 1934 loss: 1.02389583e-06
Iter: 1935 loss: 1.02376907e-06
Iter: 1936 loss: 1.02529361e-06
Iter: 1937 loss: 1.02374293e-06
Iter: 1938 loss: 1.02367369e-06
Iter: 1939 loss: 1.02368051e-06
Iter: 1940 loss: 1.02363458e-06
Iter: 1941 loss: 1.02348974e-06
Iter: 1942 loss: 1.02471404e-06
Iter: 1943 loss: 1.0234786e-06
Iter: 1944 loss: 1.02328988e-06
Iter: 1945 loss: 1.02380613e-06
Iter: 1946 loss: 1.02328124e-06
Iter: 1947 loss: 1.02311719e-06
Iter: 1948 loss: 1.02522222e-06
Iter: 1949 loss: 1.02315676e-06
Iter: 1950 loss: 1.02308559e-06
Iter: 1951 loss: 1.02292177e-06
Iter: 1952 loss: 1.02563115e-06
Iter: 1953 loss: 1.02293063e-06
Iter: 1954 loss: 1.02283457e-06
Iter: 1955 loss: 1.02363583e-06
Iter: 1956 loss: 1.02285026e-06
Iter: 1957 loss: 1.02272304e-06
Iter: 1958 loss: 1.02277022e-06
Iter: 1959 loss: 1.02267381e-06
Iter: 1960 loss: 1.02258196e-06
Iter: 1961 loss: 1.0236314e-06
Iter: 1962 loss: 1.02258707e-06
Iter: 1963 loss: 1.02250397e-06
Iter: 1964 loss: 1.02242757e-06
Iter: 1965 loss: 1.02239994e-06
Iter: 1966 loss: 1.02234458e-06
Iter: 1967 loss: 1.02342e-06
Iter: 1968 loss: 1.02232866e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.4
+ date
Sun Nov  8 14:18:34 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed72eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed7d59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed7628c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed6ca620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed6ca1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed72c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed638ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5faae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5fa7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5fa598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5b21e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ad0fa378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5bc268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ad0a92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5bcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5b2e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ad099c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ad0ab048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3388083b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33880571e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33880ac048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f338802d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f338803e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3388026f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3370240378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ad052f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f337023f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ad052378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33701e71e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33702030d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33701ce510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f337017e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f337016b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33701269d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3370186268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33701bbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.40854874
test_loss: 0.39860576
train_loss: 0.40081748
test_loss: 0.39860862
train_loss: 0.40047324
test_loss: 0.39854836
train_loss: 0.39614093
test_loss: 0.39856803
train_loss: 0.39220178
test_loss: 0.39851508
train_loss: 0.39945495
test_loss: 0.39853853
train_loss: 0.3999036
test_loss: 0.3985193
train_loss: 0.40120715
test_loss: 0.3984876
train_loss: 0.39663404
test_loss: 0.39849183
train_loss: 0.39214972
test_loss: 0.39841354
train_loss: 0.396082
test_loss: 0.39837837
train_loss: 0.40598163
test_loss: 0.39835018
train_loss: 0.39498824
test_loss: 0.3983785
train_loss: 0.39673936
test_loss: 0.3983079
train_loss: 0.40391517
test_loss: 0.39823598
train_loss: 0.39502838
test_loss: 0.39821622
train_loss: 0.39574876
test_loss: 0.3981832
train_loss: 0.39443737
test_loss: 0.3981335
train_loss: 0.39344242
test_loss: 0.39806172
train_loss: 0.3991065
test_loss: 0.39802805
train_loss: 0.389171
test_loss: 0.39794135
train_loss: 0.40119505
test_loss: 0.3979001
train_loss: 0.39439315
test_loss: 0.39782965
train_loss: 0.40088993
test_loss: 0.3977724
train_loss: 0.38977614
test_loss: 0.39764956
train_loss: 0.39837107
test_loss: 0.39762452
train_loss: 0.39234304
test_loss: 0.39753306
train_loss: 0.3989296
test_loss: 0.39742458
train_loss: 0.3990696
test_loss: 0.39735413
train_loss: 0.39054394
test_loss: 0.3972469
train_loss: 0.39923048
test_loss: 0.39711705
train_loss: 0.40165445
test_loss: 0.39702973
train_loss: 0.40123722
test_loss: 0.39689106
train_loss: 0.39975327
test_loss: 0.3967522
train_loss: 0.39113072
test_loss: 0.39661548
train_loss: 0.40281504
test_loss: 0.39653417
train_loss: 0.3961606
test_loss: 0.39634284
train_loss: 0.3976063
test_loss: 0.39621904
train_loss: 0.39613375
test_loss: 0.39606282
train_loss: 0.39391655
test_loss: 0.39589402
train_loss: 0.39437917
test_loss: 0.39568958
train_loss: 0.39087218
test_loss: 0.3955473
train_loss: 0.38587117
test_loss: 0.39532623
train_loss: 0.39250427
test_loss: 0.39511752
train_loss: 0.39634636
test_loss: 0.39495358
train_loss: 0.3975371
test_loss: 0.3946913
train_loss: 0.39779273
test_loss: 0.3944389
train_loss: 0.38869828
test_loss: 0.3941933
train_loss: 0.39360452
test_loss: 0.39389297
train_loss: 0.3966214
test_loss: 0.39365023
train_loss: 0.3838704
test_loss: 0.39340982
train_loss: 0.3977863
test_loss: 0.3930421
train_loss: 0.39183477
test_loss: 0.39274865
train_loss: 0.39834607
test_loss: 0.39237192
train_loss: 0.39218676
test_loss: 0.39203876
train_loss: 0.3962687
test_loss: 0.39165282
train_loss: 0.39300302
test_loss: 0.39126885
train_loss: 0.3919863
test_loss: 0.3908466
train_loss: 0.39541686
test_loss: 0.3903718
train_loss: 0.39508882
test_loss: 0.38994965
train_loss: 0.38566524
test_loss: 0.3894356
train_loss: 0.39128852
test_loss: 0.38891453
train_loss: 0.38506633
test_loss: 0.3884109
train_loss: 0.38737842
test_loss: 0.38783407
train_loss: 0.3815711
test_loss: 0.38722554
train_loss: 0.38738126
test_loss: 0.3865987
train_loss: 0.3902673
test_loss: 0.38593048
train_loss: 0.3904535
test_loss: 0.38516787
train_loss: 0.38091165
test_loss: 0.38449147
train_loss: 0.37966168
test_loss: 0.38370392
train_loss: 0.3892745
test_loss: 0.3829061
train_loss: 0.37499437
test_loss: 0.38201904
train_loss: 0.39045703
test_loss: 0.38113293
train_loss: 0.38461894
test_loss: 0.38016853
train_loss: 0.37611425
test_loss: 0.37911335
train_loss: 0.3735879
test_loss: 0.37804526
train_loss: 0.38046694
test_loss: 0.37698781
train_loss: 0.3738048
test_loss: 0.37574214
train_loss: 0.37493402
test_loss: 0.37450173
train_loss: 0.3762365
test_loss: 0.3731612
train_loss: 0.37490422
test_loss: 0.37171784
train_loss: 0.37393105
test_loss: 0.37026024
train_loss: 0.3701453
test_loss: 0.3686799
train_loss: 0.37206805
test_loss: 0.36699432
train_loss: 0.36059394
test_loss: 0.36518005
train_loss: 0.37143224
test_loss: 0.36330795
train_loss: 0.35892177
test_loss: 0.36124784
train_loss: 0.35933745
test_loss: 0.35910502
train_loss: 0.35948205
test_loss: 0.3568509
train_loss: 0.3533877
test_loss: 0.35440406
train_loss: 0.35392332
test_loss: 0.35184306
train_loss: 0.35162187
test_loss: 0.34913066
train_loss: 0.34859753
test_loss: 0.34618092
train_loss: 0.3398931
test_loss: 0.343044
train_loss: 0.33949012
test_loss: 0.33963445
train_loss: 0.33877087
test_loss: 0.33591014
train_loss: 0.3295389
test_loss: 0.33198753
train_loss: 0.32567185
test_loss: 0.32762858
train_loss: 0.322905
test_loss: 0.32290852
train_loss: 0.31560308
test_loss: 0.3177639
train_loss: 0.30486435
test_loss: 0.31221995
train_loss: 0.3026098
test_loss: 0.30617395
train_loss: 0.29965743
test_loss: 0.29960144
train_loss: 0.29352537
test_loss: 0.29244027
train_loss: 0.28230423
test_loss: 0.28451666
train_loss: 0.27449638
test_loss: 0.2760102
train_loss: 0.25897947
test_loss: 0.26680318
train_loss: 0.25273305
test_loss: 0.25695768
train_loss: 0.24602598
test_loss: 0.24643455
train_loss: 0.23088652
test_loss: 0.2353546
train_loss: 0.22102037
test_loss: 0.22367261
train_loss: 0.2132948
test_loss: 0.21163878
train_loss: 0.19403267
test_loss: 0.1992839
train_loss: 0.18716662
test_loss: 0.18665914
train_loss: 0.17090748
test_loss: 0.17375328
train_loss: 0.15641922
test_loss: 0.16061923
train_loss: 0.14850922
test_loss: 0.14712308
train_loss: 0.13432197
test_loss: 0.13348995
train_loss: 0.119363144
test_loss: 0.12008149
train_loss: 0.10749045
test_loss: 0.10736214
train_loss: 0.09635484
test_loss: 0.09571247
train_loss: 0.084711686
test_loss: 0.08559424
train_loss: 0.07712586
test_loss: 0.07732214
train_loss: 0.06937189
test_loss: 0.071038805
train_loss: 0.06639985
test_loss: 0.06657582
train_loss: 0.06355646
test_loss: 0.063631624
train_loss: 0.060069554
test_loss: 0.061774068
train_loss: 0.060810387
test_loss: 0.06060076
train_loss: 0.06017011
test_loss: 0.05986519
train_loss: 0.058184285
test_loss: 0.05937687
train_loss: 0.058099978
test_loss: 0.059060514
train_loss: 0.05821094
test_loss: 0.05883518
train_loss: 0.059250884
test_loss: 0.058646314
train_loss: 0.058129247
test_loss: 0.05847865
train_loss: 0.058163248
test_loss: 0.058367588
train_loss: 0.057358567
test_loss: 0.05826366
train_loss: 0.058047973
test_loss: 0.058147073
train_loss: 0.057487845
test_loss: 0.05805587
train_loss: 0.057764567
test_loss: 0.057973962
train_loss: 0.058114257
test_loss: 0.057915233
train_loss: 0.05759386
test_loss: 0.05781757
train_loss: 0.057503525
test_loss: 0.057753284
train_loss: 0.05716829
test_loss: 0.057660777
train_loss: 0.056508042
test_loss: 0.05754996
train_loss: 0.057276163
test_loss: 0.057477046
train_loss: 0.05590658
test_loss: 0.057414904
train_loss: 0.055992924
test_loss: 0.05732012
train_loss: 0.057326302
test_loss: 0.057173472
train_loss: 0.05670646
test_loss: 0.05712146
train_loss: 0.056185402
test_loss: 0.056975104
train_loss: 0.05685746
test_loss: 0.05685014
train_loss: 0.055548936
test_loss: 0.056700815
train_loss: 0.056749307
test_loss: 0.056555968
train_loss: 0.055990435
test_loss: 0.0564018
train_loss: 0.056249328
test_loss: 0.056178093
train_loss: 0.05440398
test_loss: 0.05595403
train_loss: 0.054785468
test_loss: 0.055728275
train_loss: 0.055785827
test_loss: 0.0554911
train_loss: 0.05438985
test_loss: 0.055139143
train_loss: 0.05473665
test_loss: 0.05480728
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781460dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78146218c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78600907b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78145a3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78145a30d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78600902f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78145936a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78144ef730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78144ef268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78144b1158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78144b19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78e1d780d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814420378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781442b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781443c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781443c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78144b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814459d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781435bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781437c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814342048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78142e52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78142dd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814314f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78142a9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814291f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814261620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814291378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78141da1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78142050d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781420f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78141cc1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78141b17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814181598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814133268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814147f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.00473442115
Iter: 2 loss: 0.00524752215
Iter: 3 loss: 0.00468631554
Iter: 4 loss: 0.00466631213
Iter: 5 loss: 0.00494899228
Iter: 6 loss: 0.00466627674
Iter: 7 loss: 0.0046580988
Iter: 8 loss: 0.00466060825
Iter: 9 loss: 0.00465225428
Iter: 10 loss: 0.00464465376
Iter: 11 loss: 0.00471658912
Iter: 12 loss: 0.00464434503
Iter: 13 loss: 0.00464007072
Iter: 14 loss: 0.00463260058
Iter: 15 loss: 0.00463259825
Iter: 16 loss: 0.00462438539
Iter: 17 loss: 0.00468764594
Iter: 18 loss: 0.00462378096
Iter: 19 loss: 0.00461413106
Iter: 20 loss: 0.00461413
Iter: 21 loss: 0.00460644346
Iter: 22 loss: 0.00458990131
Iter: 23 loss: 0.00455071451
Iter: 24 loss: 0.00499776192
Iter: 25 loss: 0.00454698224
Iter: 26 loss: 0.00448899157
Iter: 27 loss: 0.00477120467
Iter: 28 loss: 0.00447821431
Iter: 29 loss: 0.00440012803
Iter: 30 loss: 0.0048222784
Iter: 31 loss: 0.00438154815
Iter: 32 loss: 0.00430702465
Iter: 33 loss: 0.00440787151
Iter: 34 loss: 0.00426973868
Iter: 35 loss: 0.00420086598
Iter: 36 loss: 0.00463738292
Iter: 37 loss: 0.00418943213
Iter: 38 loss: 0.00417829864
Iter: 39 loss: 0.0041665188
Iter: 40 loss: 0.00415016431
Iter: 41 loss: 0.0043389718
Iter: 42 loss: 0.00414983463
Iter: 43 loss: 0.00414052373
Iter: 44 loss: 0.00429337472
Iter: 45 loss: 0.00414051767
Iter: 46 loss: 0.00413331
Iter: 47 loss: 0.00414061872
Iter: 48 loss: 0.00412929058
Iter: 49 loss: 0.00411845837
Iter: 50 loss: 0.00422786828
Iter: 51 loss: 0.00411815
Iter: 52 loss: 0.00410535373
Iter: 53 loss: 0.00415144442
Iter: 54 loss: 0.00410181424
Iter: 55 loss: 0.00409026165
Iter: 56 loss: 0.00407754909
Iter: 57 loss: 0.00407563569
Iter: 58 loss: 0.00404751906
Iter: 59 loss: 0.00417410489
Iter: 60 loss: 0.00404148083
Iter: 61 loss: 0.0040115118
Iter: 62 loss: 0.00408725208
Iter: 63 loss: 0.00399968028
Iter: 64 loss: 0.00396705698
Iter: 65 loss: 0.00401354674
Iter: 66 loss: 0.00395098049
Iter: 67 loss: 0.00390617037
Iter: 68 loss: 0.00451946538
Iter: 69 loss: 0.00390496082
Iter: 70 loss: 0.00387692358
Iter: 71 loss: 0.00412718626
Iter: 72 loss: 0.0038753564
Iter: 73 loss: 0.00385725033
Iter: 74 loss: 0.00409048283
Iter: 75 loss: 0.00385720585
Iter: 76 loss: 0.00383059774
Iter: 77 loss: 0.00397294667
Iter: 78 loss: 0.0038247928
Iter: 79 loss: 0.0038046313
Iter: 80 loss: 0.00380276376
Iter: 81 loss: 0.00378296105
Iter: 82 loss: 0.00383964623
Iter: 83 loss: 0.00377703574
Iter: 84 loss: 0.00375101366
Iter: 85 loss: 0.00374800921
Iter: 86 loss: 0.00372794783
Iter: 87 loss: 0.00369256642
Iter: 88 loss: 0.004178592
Iter: 89 loss: 0.00369255291
Iter: 90 loss: 0.00365864043
Iter: 91 loss: 0.00377038028
Iter: 92 loss: 0.00365072116
Iter: 93 loss: 0.00362280337
Iter: 94 loss: 0.00362242479
Iter: 95 loss: 0.0035905519
Iter: 96 loss: 0.00371421268
Iter: 97 loss: 0.00358377676
Iter: 98 loss: 0.0035570804
Iter: 99 loss: 0.00358456513
Iter: 100 loss: 0.00354307052
Iter: 101 loss: 0.00351746
Iter: 102 loss: 0.0041522705
Iter: 103 loss: 0.00351745938
Iter: 104 loss: 0.00349332811
Iter: 105 loss: 0.00365827139
Iter: 106 loss: 0.00349107105
Iter: 107 loss: 0.00347511587
Iter: 108 loss: 0.00348986662
Iter: 109 loss: 0.0034662378
Iter: 110 loss: 0.00345324283
Iter: 111 loss: 0.00345774088
Iter: 112 loss: 0.00344415801
Iter: 113 loss: 0.00342951715
Iter: 114 loss: 0.00365284784
Iter: 115 loss: 0.00342951878
Iter: 116 loss: 0.00341002643
Iter: 117 loss: 0.00341994478
Iter: 118 loss: 0.00339591736
Iter: 119 loss: 0.003362583
Iter: 120 loss: 0.00340298773
Iter: 121 loss: 0.00334457913
Iter: 122 loss: 0.0033212339
Iter: 123 loss: 0.00331864716
Iter: 124 loss: 0.00330128
Iter: 125 loss: 0.00329119153
Iter: 126 loss: 0.00328379311
Iter: 127 loss: 0.00324681518
Iter: 128 loss: 0.00367875188
Iter: 129 loss: 0.0032460154
Iter: 130 loss: 0.00321555207
Iter: 131 loss: 0.00321551855
Iter: 132 loss: 0.00320255128
Iter: 133 loss: 0.00326395221
Iter: 134 loss: 0.00320005976
Iter: 135 loss: 0.00318800588
Iter: 136 loss: 0.00336086843
Iter: 137 loss: 0.00318798772
Iter: 138 loss: 0.00317692943
Iter: 139 loss: 0.00328607112
Iter: 140 loss: 0.00317650987
Iter: 141 loss: 0.00317005301
Iter: 142 loss: 0.00319877267
Iter: 143 loss: 0.00316880387
Iter: 144 loss: 0.00316550652
Iter: 145 loss: 0.00316251488
Iter: 146 loss: 0.00316170836
Iter: 147 loss: 0.00315606268
Iter: 148 loss: 0.00317091076
Iter: 149 loss: 0.00315417629
Iter: 150 loss: 0.00314897671
Iter: 151 loss: 0.00315012061
Iter: 152 loss: 0.0031451832
Iter: 153 loss: 0.00313971704
Iter: 154 loss: 0.00319192652
Iter: 155 loss: 0.0031395168
Iter: 156 loss: 0.00313487463
Iter: 157 loss: 0.00313891983
Iter: 158 loss: 0.00313216308
Iter: 159 loss: 0.00312654255
Iter: 160 loss: 0.00311778788
Iter: 161 loss: 0.00311766891
Iter: 162 loss: 0.00310530141
Iter: 163 loss: 0.00319710607
Iter: 164 loss: 0.00310425833
Iter: 165 loss: 0.00309372274
Iter: 166 loss: 0.0031815269
Iter: 167 loss: 0.00309311785
Iter: 168 loss: 0.00308219832
Iter: 169 loss: 0.00310689094
Iter: 170 loss: 0.00307805324
Iter: 171 loss: 0.00306996983
Iter: 172 loss: 0.00306746
Iter: 173 loss: 0.00306266267
Iter: 174 loss: 0.00305473059
Iter: 175 loss: 0.00317035429
Iter: 176 loss: 0.00305471662
Iter: 177 loss: 0.00304838223
Iter: 178 loss: 0.00304316473
Iter: 179 loss: 0.00304133072
Iter: 180 loss: 0.00303136278
Iter: 181 loss: 0.0031168377
Iter: 182 loss: 0.00303079188
Iter: 183 loss: 0.00302363397
Iter: 184 loss: 0.00301212305
Iter: 185 loss: 0.00301202922
Iter: 186 loss: 0.00299980771
Iter: 187 loss: 0.00306453928
Iter: 188 loss: 0.00299783191
Iter: 189 loss: 0.0029884642
Iter: 190 loss: 0.00310123106
Iter: 191 loss: 0.00298835244
Iter: 192 loss: 0.00298219407
Iter: 193 loss: 0.00300327269
Iter: 194 loss: 0.00298035797
Iter: 195 loss: 0.00296979817
Iter: 196 loss: 0.00300702918
Iter: 197 loss: 0.00296705449
Iter: 198 loss: 0.00295636337
Iter: 199 loss: 0.00298832264
Iter: 200 loss: 0.00295309885
Iter: 201 loss: 0.00294484315
Iter: 202 loss: 0.00299617695
Iter: 203 loss: 0.00294387457
Iter: 204 loss: 0.00293272524
Iter: 205 loss: 0.00294771045
Iter: 206 loss: 0.00292719761
Iter: 207 loss: 0.00291545177
Iter: 208 loss: 0.00296717323
Iter: 209 loss: 0.00291303988
Iter: 210 loss: 0.00290128426
Iter: 211 loss: 0.00287980493
Iter: 212 loss: 0.00340822805
Iter: 213 loss: 0.0028797905
Iter: 214 loss: 0.00284948666
Iter: 215 loss: 0.0029416054
Iter: 216 loss: 0.00283900276
Iter: 217 loss: 0.00280141085
Iter: 218 loss: 0.00299354573
Iter: 219 loss: 0.00279625738
Iter: 220 loss: 0.00276657613
Iter: 221 loss: 0.00276596961
Iter: 222 loss: 0.00274122041
Iter: 223 loss: 0.00270338985
Iter: 224 loss: 0.00308167608
Iter: 225 loss: 0.00270256866
Iter: 226 loss: 0.00266588805
Iter: 227 loss: 0.00409085304
Iter: 228 loss: 0.00266588526
Iter: 229 loss: 0.00264716195
Iter: 230 loss: 0.00264714193
Iter: 231 loss: 0.00263315253
Iter: 232 loss: 0.00263433857
Iter: 233 loss: 0.00262243766
Iter: 234 loss: 0.00259905262
Iter: 235 loss: 0.00265864423
Iter: 236 loss: 0.00258946838
Iter: 237 loss: 0.00257367035
Iter: 238 loss: 0.00257024565
Iter: 239 loss: 0.00255223131
Iter: 240 loss: 0.00255164108
Iter: 241 loss: 0.00253883726
Iter: 242 loss: 0.00261486461
Iter: 243 loss: 0.0025366277
Iter: 244 loss: 0.00252705417
Iter: 245 loss: 0.00254658214
Iter: 246 loss: 0.00252327
Iter: 247 loss: 0.00250456179
Iter: 248 loss: 0.00266026822
Iter: 249 loss: 0.00250328286
Iter: 250 loss: 0.00248509459
Iter: 251 loss: 0.00255239662
Iter: 252 loss: 0.0024805842
Iter: 253 loss: 0.00246542157
Iter: 254 loss: 0.00251918752
Iter: 255 loss: 0.0024614674
Iter: 256 loss: 0.00244097691
Iter: 257 loss: 0.00244789431
Iter: 258 loss: 0.00242637866
Iter: 259 loss: 0.00240705395
Iter: 260 loss: 0.0025125579
Iter: 261 loss: 0.00240277126
Iter: 262 loss: 0.00236420659
Iter: 263 loss: 0.00289811124
Iter: 264 loss: 0.00236410368
Iter: 265 loss: 0.00234224088
Iter: 266 loss: 0.00250206864
Iter: 267 loss: 0.00233775913
Iter: 268 loss: 0.00230180938
Iter: 269 loss: 0.00249854522
Iter: 270 loss: 0.00229694368
Iter: 271 loss: 0.00227631908
Iter: 272 loss: 0.00227596355
Iter: 273 loss: 0.00225162925
Iter: 274 loss: 0.00251061469
Iter: 275 loss: 0.00225107279
Iter: 276 loss: 0.00222814782
Iter: 277 loss: 0.00227832678
Iter: 278 loss: 0.00221902831
Iter: 279 loss: 0.00219757808
Iter: 280 loss: 0.00235507591
Iter: 281 loss: 0.00219480623
Iter: 282 loss: 0.00217224937
Iter: 283 loss: 0.00217224914
Iter: 284 loss: 0.00215473562
Iter: 285 loss: 0.00237254053
Iter: 286 loss: 0.00215410092
Iter: 287 loss: 0.00213713711
Iter: 288 loss: 0.00217366219
Iter: 289 loss: 0.00213046651
Iter: 290 loss: 0.00210649031
Iter: 291 loss: 0.00249123527
Iter: 292 loss: 0.00210647681
Iter: 293 loss: 0.00208324427
Iter: 294 loss: 0.00217198394
Iter: 295 loss: 0.00207749475
Iter: 296 loss: 0.002060981
Iter: 297 loss: 0.00210721092
Iter: 298 loss: 0.00205563288
Iter: 299 loss: 0.00203293376
Iter: 300 loss: 0.00208744267
Iter: 301 loss: 0.00202276814
Iter: 302 loss: 0.0020088749
Iter: 303 loss: 0.00203482225
Iter: 304 loss: 0.00200296473
Iter: 305 loss: 0.00198100274
Iter: 306 loss: 0.00204027072
Iter: 307 loss: 0.00197147019
Iter: 308 loss: 0.00193539157
Iter: 309 loss: 0.00214482564
Iter: 310 loss: 0.00193056418
Iter: 311 loss: 0.00193209376
Iter: 312 loss: 0.00191528164
Iter: 313 loss: 0.00189483771
Iter: 314 loss: 0.00189873017
Iter: 315 loss: 0.00187939615
Iter: 316 loss: 0.00186084968
Iter: 317 loss: 0.00199135835
Iter: 318 loss: 0.00185913348
Iter: 319 loss: 0.00184692512
Iter: 320 loss: 0.00184740778
Iter: 321 loss: 0.00183643063
Iter: 322 loss: 0.00180078205
Iter: 323 loss: 0.00180028868
Iter: 324 loss: 0.0017790494
Iter: 325 loss: 0.00177727418
Iter: 326 loss: 0.00175722409
Iter: 327 loss: 0.00187699287
Iter: 328 loss: 0.00175482151
Iter: 329 loss: 0.00173454254
Iter: 330 loss: 0.00170344
Iter: 331 loss: 0.00170287653
Iter: 332 loss: 0.0016661624
Iter: 333 loss: 0.00172197446
Iter: 334 loss: 0.00164734875
Iter: 335 loss: 0.00162438978
Iter: 336 loss: 0.00182525907
Iter: 337 loss: 0.00162333483
Iter: 338 loss: 0.00159444974
Iter: 339 loss: 0.0020856834
Iter: 340 loss: 0.00159332983
Iter: 341 loss: 0.00156924117
Iter: 342 loss: 0.00171241909
Iter: 343 loss: 0.00156617
Iter: 344 loss: 0.00155662734
Iter: 345 loss: 0.00155399507
Iter: 346 loss: 0.00154400524
Iter: 347 loss: 0.00155647821
Iter: 348 loss: 0.0015388215
Iter: 349 loss: 0.00152415829
Iter: 350 loss: 0.00168907596
Iter: 351 loss: 0.00152356806
Iter: 352 loss: 0.00150960649
Iter: 353 loss: 0.00151123945
Iter: 354 loss: 0.00149897113
Iter: 355 loss: 0.00148300501
Iter: 356 loss: 0.00151594798
Iter: 357 loss: 0.00147679669
Iter: 358 loss: 0.00146080565
Iter: 359 loss: 0.00151732517
Iter: 360 loss: 0.00145518908
Iter: 361 loss: 0.00143048051
Iter: 362 loss: 0.00151483505
Iter: 363 loss: 0.00142391783
Iter: 364 loss: 0.0014037271
Iter: 365 loss: 0.00171870214
Iter: 366 loss: 0.0014036881
Iter: 367 loss: 0.00138439238
Iter: 368 loss: 0.00141683687
Iter: 369 loss: 0.00137578056
Iter: 370 loss: 0.00136016251
Iter: 371 loss: 0.00136877166
Iter: 372 loss: 0.00134941831
Iter: 373 loss: 0.00133398292
Iter: 374 loss: 0.00134188682
Iter: 375 loss: 0.00132396258
Iter: 376 loss: 0.00130396476
Iter: 377 loss: 0.00134389848
Iter: 378 loss: 0.00129577483
Iter: 379 loss: 0.00128137635
Iter: 380 loss: 0.00127691252
Iter: 381 loss: 0.00126475317
Iter: 382 loss: 0.00128408766
Iter: 383 loss: 0.00125878234
Iter: 384 loss: 0.00123300147
Iter: 385 loss: 0.00132020377
Iter: 386 loss: 0.00122627779
Iter: 387 loss: 0.00120801141
Iter: 388 loss: 0.00120602013
Iter: 389 loss: 0.00119379128
Iter: 390 loss: 0.00119391363
Iter: 391 loss: 0.00118396105
Iter: 392 loss: 0.00116120151
Iter: 393 loss: 0.00119040243
Iter: 394 loss: 0.00114939525
Iter: 395 loss: 0.00112347165
Iter: 396 loss: 0.00123187713
Iter: 397 loss: 0.00111727731
Iter: 398 loss: 0.00109563605
Iter: 399 loss: 0.00111728651
Iter: 400 loss: 0.00108306296
Iter: 401 loss: 0.00108146295
Iter: 402 loss: 0.00107107928
Iter: 403 loss: 0.00106319156
Iter: 404 loss: 0.00113540678
Iter: 405 loss: 0.0010627287
Iter: 406 loss: 0.00105205341
Iter: 407 loss: 0.00104941311
Iter: 408 loss: 0.0010426729
Iter: 409 loss: 0.00101656443
Iter: 410 loss: 0.00120673236
Iter: 411 loss: 0.00101372728
Iter: 412 loss: 0.00100428914
Iter: 413 loss: 0.0010006337
Iter: 414 loss: 0.000993353897
Iter: 415 loss: 0.00099859538
Iter: 416 loss: 0.000988858868
Iter: 417 loss: 0.000979915494
Iter: 418 loss: 0.000984533224
Iter: 419 loss: 0.000973655726
Iter: 420 loss: 0.000962858088
Iter: 421 loss: 0.000963866827
Iter: 422 loss: 0.000954661169
Iter: 423 loss: 0.00093500549
Iter: 424 loss: 0.00114940782
Iter: 425 loss: 0.000934072246
Iter: 426 loss: 0.000917888
Iter: 427 loss: 0.000917875033
Iter: 428 loss: 0.000900619256
Iter: 429 loss: 0.00106191891
Iter: 430 loss: 0.000899943756
Iter: 431 loss: 0.000889559451
Iter: 432 loss: 0.000876853126
Iter: 433 loss: 0.000875698635
Iter: 434 loss: 0.000860283093
Iter: 435 loss: 0.000852655387
Iter: 436 loss: 0.000845301605
Iter: 437 loss: 0.000841942558
Iter: 438 loss: 0.000838277338
Iter: 439 loss: 0.000830129255
Iter: 440 loss: 0.00083117
Iter: 441 loss: 0.0008239171
Iter: 442 loss: 0.000812703569
Iter: 443 loss: 0.000830957899
Iter: 444 loss: 0.000807528035
Iter: 445 loss: 0.000800437178
Iter: 446 loss: 0.000838529901
Iter: 447 loss: 0.000799559639
Iter: 448 loss: 0.000793025596
Iter: 449 loss: 0.000817634165
Iter: 450 loss: 0.000791377795
Iter: 451 loss: 0.000784860225
Iter: 452 loss: 0.000771945808
Iter: 453 loss: 0.00101852242
Iter: 454 loss: 0.000771807041
Iter: 455 loss: 0.000757106056
Iter: 456 loss: 0.000775085529
Iter: 457 loss: 0.000749213388
Iter: 458 loss: 0.000732245855
Iter: 459 loss: 0.000820042565
Iter: 460 loss: 0.000729700434
Iter: 461 loss: 0.000711826375
Iter: 462 loss: 0.000856023806
Iter: 463 loss: 0.000710366818
Iter: 464 loss: 0.000699578319
Iter: 465 loss: 0.000697949843
Iter: 466 loss: 0.00069015281
Iter: 467 loss: 0.000684386352
Iter: 468 loss: 0.0006818
Iter: 469 loss: 0.000672802154
Iter: 470 loss: 0.000683833438
Iter: 471 loss: 0.000667476445
Iter: 472 loss: 0.000654231233
Iter: 473 loss: 0.000766061479
Iter: 474 loss: 0.000653497933
Iter: 475 loss: 0.000656122
Iter: 476 loss: 0.000648325891
Iter: 477 loss: 0.000644801185
Iter: 478 loss: 0.000641384861
Iter: 479 loss: 0.000640612794
Iter: 480 loss: 0.000629961491
Iter: 481 loss: 0.000622695487
Iter: 482 loss: 0.000618774793
Iter: 483 loss: 0.00061868655
Iter: 484 loss: 0.000613666663
Iter: 485 loss: 0.000608512084
Iter: 486 loss: 0.000598691
Iter: 487 loss: 0.000799322035
Iter: 488 loss: 0.000598648
Iter: 489 loss: 0.00058881822
Iter: 490 loss: 0.000647421693
Iter: 491 loss: 0.000587549468
Iter: 492 loss: 0.000579036248
Iter: 493 loss: 0.000570153468
Iter: 494 loss: 0.000568497169
Iter: 495 loss: 0.000555008708
Iter: 496 loss: 0.000576153
Iter: 497 loss: 0.000549130375
Iter: 498 loss: 0.000539883564
Iter: 499 loss: 0.000547015923
Iter: 500 loss: 0.00053409941
Iter: 501 loss: 0.000527714
Iter: 502 loss: 0.000611738127
Iter: 503 loss: 0.000527678349
Iter: 504 loss: 0.000523086404
Iter: 505 loss: 0.000515175634
Iter: 506 loss: 0.000515170686
Iter: 507 loss: 0.000504438765
Iter: 508 loss: 0.000522825227
Iter: 509 loss: 0.000499438203
Iter: 510 loss: 0.000496587949
Iter: 511 loss: 0.000493626751
Iter: 512 loss: 0.000490970095
Iter: 513 loss: 0.000492085936
Iter: 514 loss: 0.000489165599
Iter: 515 loss: 0.000482692965
Iter: 516 loss: 0.000509063364
Iter: 517 loss: 0.00048118757
Iter: 518 loss: 0.000474007189
Iter: 519 loss: 0.000549031713
Iter: 520 loss: 0.000473789551
Iter: 521 loss: 0.000468433078
Iter: 522 loss: 0.000475058099
Iter: 523 loss: 0.00046565698
Iter: 524 loss: 0.000459425908
Iter: 525 loss: 0.000453045708
Iter: 526 loss: 0.000451793894
Iter: 527 loss: 0.000443077966
Iter: 528 loss: 0.000445884391
Iter: 529 loss: 0.000436893955
Iter: 530 loss: 0.000426199666
Iter: 531 loss: 0.000434878515
Iter: 532 loss: 0.000419811491
Iter: 533 loss: 0.0004124837
Iter: 534 loss: 0.000412276073
Iter: 535 loss: 0.000405366591
Iter: 536 loss: 0.000435088092
Iter: 537 loss: 0.000403912854
Iter: 538 loss: 0.000399339071
Iter: 539 loss: 0.000429559383
Iter: 540 loss: 0.000398871372
Iter: 541 loss: 0.000396281714
Iter: 542 loss: 0.000396267686
Iter: 543 loss: 0.00039372241
Iter: 544 loss: 0.000390754198
Iter: 545 loss: 0.000390393368
Iter: 546 loss: 0.00038514749
Iter: 547 loss: 0.000381805323
Iter: 548 loss: 0.000379757956
Iter: 549 loss: 0.000374128431
Iter: 550 loss: 0.000390263274
Iter: 551 loss: 0.00037220103
Iter: 552 loss: 0.000369500223
Iter: 553 loss: 0.000378762023
Iter: 554 loss: 0.000368777663
Iter: 555 loss: 0.000364030449
Iter: 556 loss: 0.000360198552
Iter: 557 loss: 0.000358772
Iter: 558 loss: 0.00035381355
Iter: 559 loss: 0.00041189816
Iter: 560 loss: 0.000353723299
Iter: 561 loss: 0.000349467737
Iter: 562 loss: 0.000348902395
Iter: 563 loss: 0.000345896638
Iter: 564 loss: 0.000342049112
Iter: 565 loss: 0.000352512754
Iter: 566 loss: 0.000340701023
Iter: 567 loss: 0.000337076315
Iter: 568 loss: 0.000331922696
Iter: 569 loss: 0.000331739779
Iter: 570 loss: 0.000328840018
Iter: 571 loss: 0.000328160386
Iter: 572 loss: 0.000325879897
Iter: 573 loss: 0.000354316726
Iter: 574 loss: 0.000325856672
Iter: 575 loss: 0.000324136723
Iter: 576 loss: 0.000333082338
Iter: 577 loss: 0.00032385136
Iter: 578 loss: 0.000322154956
Iter: 579 loss: 0.000322026026
Iter: 580 loss: 0.000320761348
Iter: 581 loss: 0.00031753935
Iter: 582 loss: 0.000313634489
Iter: 583 loss: 0.000313241937
Iter: 584 loss: 0.000310104631
Iter: 585 loss: 0.000322085252
Iter: 586 loss: 0.000309384079
Iter: 587 loss: 0.000307367067
Iter: 588 loss: 0.000306274625
Iter: 589 loss: 0.000305374182
Iter: 590 loss: 0.000301887456
Iter: 591 loss: 0.000339048798
Iter: 592 loss: 0.000301794615
Iter: 593 loss: 0.000300090236
Iter: 594 loss: 0.000300628919
Iter: 595 loss: 0.000298879982
Iter: 596 loss: 0.000297015591
Iter: 597 loss: 0.000294394093
Iter: 598 loss: 0.000294295285
Iter: 599 loss: 0.000290487311
Iter: 600 loss: 0.000290970551
Iter: 601 loss: 0.000287580129
Iter: 602 loss: 0.00028581536
Iter: 603 loss: 0.000285775808
Iter: 604 loss: 0.000284479058
Iter: 605 loss: 0.000292280485
Iter: 606 loss: 0.000284317182
Iter: 607 loss: 0.000282598718
Iter: 608 loss: 0.000281218207
Iter: 609 loss: 0.000280698237
Iter: 610 loss: 0.000277030602
Iter: 611 loss: 0.000283824978
Iter: 612 loss: 0.000275468279
Iter: 613 loss: 0.000273428683
Iter: 614 loss: 0.000273419166
Iter: 615 loss: 0.000271509867
Iter: 616 loss: 0.000278074207
Iter: 617 loss: 0.000271024386
Iter: 618 loss: 0.000269381329
Iter: 619 loss: 0.000266170769
Iter: 620 loss: 0.000331778196
Iter: 621 loss: 0.000266147079
Iter: 622 loss: 0.000264645321
Iter: 623 loss: 0.000263984635
Iter: 624 loss: 0.000261408481
Iter: 625 loss: 0.000259864086
Iter: 626 loss: 0.000258800865
Iter: 627 loss: 0.000255432504
Iter: 628 loss: 0.00030674733
Iter: 629 loss: 0.000255431107
Iter: 630 loss: 0.000252468482
Iter: 631 loss: 0.000275499595
Iter: 632 loss: 0.000252239261
Iter: 633 loss: 0.000250431767
Iter: 634 loss: 0.000248325
Iter: 635 loss: 0.000248085795
Iter: 636 loss: 0.000245486095
Iter: 637 loss: 0.00026413158
Iter: 638 loss: 0.000245258154
Iter: 639 loss: 0.000243976392
Iter: 640 loss: 0.000244399795
Iter: 641 loss: 0.000243059461
Iter: 642 loss: 0.000240479858
Iter: 643 loss: 0.000251511374
Iter: 644 loss: 0.000239957211
Iter: 645 loss: 0.000237197703
Iter: 646 loss: 0.000246926327
Iter: 647 loss: 0.000236464373
Iter: 648 loss: 0.000231889731
Iter: 649 loss: 0.000236784588
Iter: 650 loss: 0.000229374127
Iter: 651 loss: 0.0002291985
Iter: 652 loss: 0.00022823672
Iter: 653 loss: 0.000227530531
Iter: 654 loss: 0.000226479795
Iter: 655 loss: 0.000226455159
Iter: 656 loss: 0.00022291897
Iter: 657 loss: 0.000216970453
Iter: 658 loss: 0.000216951696
Iter: 659 loss: 0.000213974505
Iter: 660 loss: 0.000249615434
Iter: 661 loss: 0.000213933206
Iter: 662 loss: 0.000211514693
Iter: 663 loss: 0.000212621642
Iter: 664 loss: 0.000209886406
Iter: 665 loss: 0.000207585254
Iter: 666 loss: 0.000221552225
Iter: 667 loss: 0.000207306643
Iter: 668 loss: 0.00020605512
Iter: 669 loss: 0.000206008262
Iter: 670 loss: 0.000205423945
Iter: 671 loss: 0.000204354059
Iter: 672 loss: 0.00022998848
Iter: 673 loss: 0.000204354903
Iter: 674 loss: 0.000202016876
Iter: 675 loss: 0.000219873662
Iter: 676 loss: 0.00020182642
Iter: 677 loss: 0.000199952803
Iter: 678 loss: 0.000199641814
Iter: 679 loss: 0.000198358553
Iter: 680 loss: 0.000195735294
Iter: 681 loss: 0.000201398332
Iter: 682 loss: 0.000194705557
Iter: 683 loss: 0.000192704116
Iter: 684 loss: 0.000192570777
Iter: 685 loss: 0.000191478175
Iter: 686 loss: 0.000197136047
Iter: 687 loss: 0.000191308791
Iter: 688 loss: 0.000190573497
Iter: 689 loss: 0.000191111394
Iter: 690 loss: 0.000190119172
Iter: 691 loss: 0.000189052022
Iter: 692 loss: 0.000187051934
Iter: 693 loss: 0.000231493032
Iter: 694 loss: 0.000187047233
Iter: 695 loss: 0.000185288372
Iter: 696 loss: 0.000185241719
Iter: 697 loss: 0.00018385562
Iter: 698 loss: 0.000183392665
Iter: 699 loss: 0.000182594231
Iter: 700 loss: 0.000183727883
Iter: 701 loss: 0.000182040734
Iter: 702 loss: 0.000181542026
Iter: 703 loss: 0.0001805137
Iter: 704 loss: 0.000201793548
Iter: 705 loss: 0.000180495816
Iter: 706 loss: 0.000178307295
Iter: 707 loss: 0.00018368219
Iter: 708 loss: 0.000177538313
Iter: 709 loss: 0.000175122987
Iter: 710 loss: 0.000173218155
Iter: 711 loss: 0.000172486063
Iter: 712 loss: 0.000170835163
Iter: 713 loss: 0.000177232592
Iter: 714 loss: 0.00017045124
Iter: 715 loss: 0.000169341845
Iter: 716 loss: 0.000169337261
Iter: 717 loss: 0.000168378188
Iter: 718 loss: 0.000174758621
Iter: 719 loss: 0.000168280734
Iter: 720 loss: 0.000167661608
Iter: 721 loss: 0.000170382322
Iter: 722 loss: 0.000167536316
Iter: 723 loss: 0.000166945363
Iter: 724 loss: 0.000165624922
Iter: 725 loss: 0.000184142511
Iter: 726 loss: 0.000165555335
Iter: 727 loss: 0.000164227677
Iter: 728 loss: 0.000171455511
Iter: 729 loss: 0.000164029203
Iter: 730 loss: 0.000162705212
Iter: 731 loss: 0.000166932135
Iter: 732 loss: 0.000162327095
Iter: 733 loss: 0.000162155164
Iter: 734 loss: 0.000161829681
Iter: 735 loss: 0.000161378557
Iter: 736 loss: 0.000160092837
Iter: 737 loss: 0.000166260143
Iter: 738 loss: 0.000159640942
Iter: 739 loss: 0.000158230134
Iter: 740 loss: 0.000165255129
Iter: 741 loss: 0.000157998744
Iter: 742 loss: 0.000157055896
Iter: 743 loss: 0.000158415205
Iter: 744 loss: 0.000156599053
Iter: 745 loss: 0.000155553644
Iter: 746 loss: 0.000155211834
Iter: 747 loss: 0.000154605135
Iter: 748 loss: 0.000153287692
Iter: 749 loss: 0.000158035982
Iter: 750 loss: 0.000152948749
Iter: 751 loss: 0.000151898654
Iter: 752 loss: 0.000157170754
Iter: 753 loss: 0.000151724293
Iter: 754 loss: 0.000150880194
Iter: 755 loss: 0.000163811637
Iter: 756 loss: 0.000150880383
Iter: 757 loss: 0.000150218024
Iter: 758 loss: 0.000149184139
Iter: 759 loss: 0.000149170693
Iter: 760 loss: 0.0001481129
Iter: 761 loss: 0.000164000259
Iter: 762 loss: 0.000148112216
Iter: 763 loss: 0.00014746307
Iter: 764 loss: 0.000149395317
Iter: 765 loss: 0.000147266197
Iter: 766 loss: 0.000146534148
Iter: 767 loss: 0.000149060827
Iter: 768 loss: 0.000146343024
Iter: 769 loss: 0.000146109014
Iter: 770 loss: 0.000146047227
Iter: 771 loss: 0.000145786093
Iter: 772 loss: 0.000145011116
Iter: 773 loss: 0.000147594
Iter: 774 loss: 0.000144650519
Iter: 775 loss: 0.000143698722
Iter: 776 loss: 0.000142493373
Iter: 777 loss: 0.000142400764
Iter: 778 loss: 0.000140647287
Iter: 779 loss: 0.000140644988
Iter: 780 loss: 0.000139806754
Iter: 781 loss: 0.00013964533
Iter: 782 loss: 0.000139084092
Iter: 783 loss: 0.000137645344
Iter: 784 loss: 0.000136601739
Iter: 785 loss: 0.00013611489
Iter: 786 loss: 0.000136372284
Iter: 787 loss: 0.000135425958
Iter: 788 loss: 0.000135054637
Iter: 789 loss: 0.00013730365
Iter: 790 loss: 0.000135008595
Iter: 791 loss: 0.000134657326
Iter: 792 loss: 0.000133585476
Iter: 793 loss: 0.000136168921
Iter: 794 loss: 0.000132975518
Iter: 795 loss: 0.000133517853
Iter: 796 loss: 0.000132662055
Iter: 797 loss: 0.000132353918
Iter: 798 loss: 0.000132350062
Iter: 799 loss: 0.000132106565
Iter: 800 loss: 0.00013158872
Iter: 801 loss: 0.000134325557
Iter: 802 loss: 0.00013151
Iter: 803 loss: 0.000130952845
Iter: 804 loss: 0.000131428154
Iter: 805 loss: 0.000130623346
Iter: 806 loss: 0.000130318382
Iter: 807 loss: 0.00012948236
Iter: 808 loss: 0.000134540271
Iter: 809 loss: 0.000129256528
Iter: 810 loss: 0.000128209183
Iter: 811 loss: 0.000136877614
Iter: 812 loss: 0.000128147964
Iter: 813 loss: 0.000127104169
Iter: 814 loss: 0.00013033711
Iter: 815 loss: 0.000126804516
Iter: 816 loss: 0.000126038591
Iter: 817 loss: 0.000125471764
Iter: 818 loss: 0.000125217106
Iter: 819 loss: 0.00012437944
Iter: 820 loss: 0.000130901055
Iter: 821 loss: 0.000124318452
Iter: 822 loss: 0.000123827471
Iter: 823 loss: 0.000122713813
Iter: 824 loss: 0.000137338691
Iter: 825 loss: 0.000122643833
Iter: 826 loss: 0.000121605706
Iter: 827 loss: 0.000129407257
Iter: 828 loss: 0.000121527228
Iter: 829 loss: 0.000120804601
Iter: 830 loss: 0.000121489218
Iter: 831 loss: 0.000120392942
Iter: 832 loss: 0.000120958539
Iter: 833 loss: 0.000120018347
Iter: 834 loss: 0.000119588803
Iter: 835 loss: 0.000119507109
Iter: 836 loss: 0.000119363525
Iter: 837 loss: 0.000119783443
Iter: 838 loss: 0.000119319826
Iter: 839 loss: 0.000119149619
Iter: 840 loss: 0.000118640979
Iter: 841 loss: 0.000120203142
Iter: 842 loss: 0.000118392272
Iter: 843 loss: 0.000117252406
Iter: 844 loss: 0.000121241101
Iter: 845 loss: 0.000116956158
Iter: 846 loss: 0.000115924078
Iter: 847 loss: 0.000131795707
Iter: 848 loss: 0.000115924158
Iter: 849 loss: 0.000115680443
Iter: 850 loss: 0.000115314084
Iter: 851 loss: 0.00011530659
Iter: 852 loss: 0.000114849136
Iter: 853 loss: 0.000114849361
Iter: 854 loss: 0.000114496768
Iter: 855 loss: 0.000113752016
Iter: 856 loss: 0.000125680293
Iter: 857 loss: 0.000113730101
Iter: 858 loss: 0.000112634807
Iter: 859 loss: 0.000114801769
Iter: 860 loss: 0.000112194699
Iter: 861 loss: 0.000113112532
Iter: 862 loss: 0.000111641828
Iter: 863 loss: 0.000111399735
Iter: 864 loss: 0.000110773515
Iter: 865 loss: 0.000115925213
Iter: 866 loss: 0.000110660214
Iter: 867 loss: 0.000110308552
Iter: 868 loss: 0.000110270521
Iter: 869 loss: 0.000110032721
Iter: 870 loss: 0.000110803223
Iter: 871 loss: 0.000109964945
Iter: 872 loss: 0.00010944932
Iter: 873 loss: 0.000112215472
Iter: 874 loss: 0.000109374014
Iter: 875 loss: 0.000108539658
Iter: 876 loss: 0.000111690504
Iter: 877 loss: 0.000108343389
Iter: 878 loss: 0.000107921136
Iter: 879 loss: 0.00010804773
Iter: 880 loss: 0.00010761978
Iter: 881 loss: 0.000106548963
Iter: 882 loss: 0.000106648731
Iter: 883 loss: 0.00010572475
Iter: 884 loss: 0.000104987063
Iter: 885 loss: 0.000104987048
Iter: 886 loss: 0.000104501392
Iter: 887 loss: 0.000104371051
Iter: 888 loss: 0.000104071674
Iter: 889 loss: 0.000103461833
Iter: 890 loss: 0.000102526537
Iter: 891 loss: 0.000102511629
Iter: 892 loss: 0.000101825426
Iter: 893 loss: 0.000107414569
Iter: 894 loss: 0.000101781174
Iter: 895 loss: 0.000101273115
Iter: 896 loss: 0.000101599464
Iter: 897 loss: 0.000100950827
Iter: 898 loss: 0.000100838864
Iter: 899 loss: 0.000100609177
Iter: 900 loss: 0.000100107514
Iter: 901 loss: 0.000100535355
Iter: 902 loss: 9.98146643e-05
Iter: 903 loss: 9.9200166e-05
Iter: 904 loss: 9.82142374e-05
Iter: 905 loss: 9.82060155e-05
Iter: 906 loss: 9.71181216e-05
Iter: 907 loss: 0.000101248675
Iter: 908 loss: 9.68619424e-05
Iter: 909 loss: 9.6644304e-05
Iter: 910 loss: 9.65674917e-05
Iter: 911 loss: 9.62426711e-05
Iter: 912 loss: 9.58013115e-05
Iter: 913 loss: 9.57800221e-05
Iter: 914 loss: 9.5429481e-05
Iter: 915 loss: 9.5411604e-05
Iter: 916 loss: 9.51758775e-05
Iter: 917 loss: 9.47978697e-05
Iter: 918 loss: 9.47949e-05
Iter: 919 loss: 9.42828192e-05
Iter: 920 loss: 9.4789124e-05
Iter: 921 loss: 9.3994764e-05
Iter: 922 loss: 9.36927099e-05
Iter: 923 loss: 9.36751894e-05
Iter: 924 loss: 9.33400224e-05
Iter: 925 loss: 9.24559863e-05
Iter: 926 loss: 9.89338878e-05
Iter: 927 loss: 9.22709442e-05
Iter: 928 loss: 9.23843e-05
Iter: 929 loss: 9.18055084e-05
Iter: 930 loss: 9.14917036e-05
Iter: 931 loss: 9.14732591e-05
Iter: 932 loss: 9.13825934e-05
Iter: 933 loss: 9.10819726e-05
Iter: 934 loss: 9.10789095e-05
Iter: 935 loss: 9.07694193e-05
Iter: 936 loss: 9.00550367e-05
Iter: 937 loss: 9.03937e-05
Iter: 938 loss: 8.95748672e-05
Iter: 939 loss: 8.92012e-05
Iter: 940 loss: 9.43235791e-05
Iter: 941 loss: 8.91997915e-05
Iter: 942 loss: 8.87352217e-05
Iter: 943 loss: 8.8951e-05
Iter: 944 loss: 8.84207911e-05
Iter: 945 loss: 8.8288507e-05
Iter: 946 loss: 8.82443419e-05
Iter: 947 loss: 8.81078449e-05
Iter: 948 loss: 8.79078725e-05
Iter: 949 loss: 8.79029394e-05
Iter: 950 loss: 8.75122641e-05
Iter: 951 loss: 8.68812349e-05
Iter: 952 loss: 8.68768e-05
Iter: 953 loss: 8.65361217e-05
Iter: 954 loss: 9.12200558e-05
Iter: 955 loss: 8.65342809e-05
Iter: 956 loss: 8.61208828e-05
Iter: 957 loss: 8.53559759e-05
Iter: 958 loss: 0.000102531885
Iter: 959 loss: 8.5353764e-05
Iter: 960 loss: 8.46775e-05
Iter: 961 loss: 8.82945751e-05
Iter: 962 loss: 8.45766408e-05
Iter: 963 loss: 8.53097154e-05
Iter: 964 loss: 8.4312851e-05
Iter: 965 loss: 8.41908914e-05
Iter: 966 loss: 8.38900451e-05
Iter: 967 loss: 8.68478746e-05
Iter: 968 loss: 8.38508859e-05
Iter: 969 loss: 8.35023311e-05
Iter: 970 loss: 8.31895886e-05
Iter: 971 loss: 8.31025827e-05
Iter: 972 loss: 8.25239258e-05
Iter: 973 loss: 8.23653172e-05
Iter: 974 loss: 8.20140413e-05
Iter: 975 loss: 8.15845342e-05
Iter: 976 loss: 8.15372332e-05
Iter: 977 loss: 8.12567887e-05
Iter: 978 loss: 8.19620182e-05
Iter: 979 loss: 8.11592909e-05
Iter: 980 loss: 8.08552577e-05
Iter: 981 loss: 8.01686401e-05
Iter: 982 loss: 8.93139513e-05
Iter: 983 loss: 8.01262213e-05
Iter: 984 loss: 7.98051406e-05
Iter: 985 loss: 7.98046094e-05
Iter: 986 loss: 7.94377e-05
Iter: 987 loss: 8.18821e-05
Iter: 988 loss: 7.94019943e-05
Iter: 989 loss: 7.92414576e-05
Iter: 990 loss: 7.87681201e-05
Iter: 991 loss: 8.04587471e-05
Iter: 992 loss: 7.8557583e-05
Iter: 993 loss: 7.81150302e-05
Iter: 994 loss: 7.85846787e-05
Iter: 995 loss: 7.78708418e-05
Iter: 996 loss: 7.81339841e-05
Iter: 997 loss: 7.77556706e-05
Iter: 998 loss: 7.76021479e-05
Iter: 999 loss: 7.74102373e-05
Iter: 1000 loss: 7.73949e-05
Iter: 1001 loss: 7.70406332e-05
Iter: 1002 loss: 7.67396559e-05
Iter: 1003 loss: 7.66415178e-05
Iter: 1004 loss: 7.59276591e-05
Iter: 1005 loss: 8.2350678e-05
Iter: 1006 loss: 7.5894568e-05
Iter: 1007 loss: 7.54725042e-05
Iter: 1008 loss: 7.88377802e-05
Iter: 1009 loss: 7.54466382e-05
Iter: 1010 loss: 7.52059714e-05
Iter: 1011 loss: 7.45549696e-05
Iter: 1012 loss: 7.86737437e-05
Iter: 1013 loss: 7.43923883e-05
Iter: 1014 loss: 7.41162221e-05
Iter: 1015 loss: 7.41103067e-05
Iter: 1016 loss: 7.39711177e-05
Iter: 1017 loss: 7.39887182e-05
Iter: 1018 loss: 7.38654198e-05
Iter: 1019 loss: 7.36635047e-05
Iter: 1020 loss: 7.459e-05
Iter: 1021 loss: 7.36250513e-05
Iter: 1022 loss: 7.34218702e-05
Iter: 1023 loss: 7.37824448e-05
Iter: 1024 loss: 7.33320048e-05
Iter: 1025 loss: 7.29033636e-05
Iter: 1026 loss: 7.43291457e-05
Iter: 1027 loss: 7.27861334e-05
Iter: 1028 loss: 7.28904124e-05
Iter: 1029 loss: 7.26508733e-05
Iter: 1030 loss: 7.25449572e-05
Iter: 1031 loss: 7.24693382e-05
Iter: 1032 loss: 7.24332276e-05
Iter: 1033 loss: 7.22495679e-05
Iter: 1034 loss: 7.1838047e-05
Iter: 1035 loss: 7.74552e-05
Iter: 1036 loss: 7.18139199e-05
Iter: 1037 loss: 7.14844355e-05
Iter: 1038 loss: 7.14200214e-05
Iter: 1039 loss: 7.12024048e-05
Iter: 1040 loss: 7.09497399e-05
Iter: 1041 loss: 7.01814206e-05
Iter: 1042 loss: 7.21663309e-05
Iter: 1043 loss: 6.97598662e-05
Iter: 1044 loss: 6.90421584e-05
Iter: 1045 loss: 7.30523316e-05
Iter: 1046 loss: 6.89425e-05
Iter: 1047 loss: 6.88837608e-05
Iter: 1048 loss: 6.8803718e-05
Iter: 1049 loss: 6.86730709e-05
Iter: 1050 loss: 6.83552498e-05
Iter: 1051 loss: 7.17054209e-05
Iter: 1052 loss: 6.83208782e-05
Iter: 1053 loss: 6.81029705e-05
Iter: 1054 loss: 6.80817757e-05
Iter: 1055 loss: 6.7912988e-05
Iter: 1056 loss: 6.81810561e-05
Iter: 1057 loss: 6.78359502e-05
Iter: 1058 loss: 6.75874471e-05
Iter: 1059 loss: 6.74045441e-05
Iter: 1060 loss: 6.73207833e-05
Iter: 1061 loss: 6.72089809e-05
Iter: 1062 loss: 6.71913149e-05
Iter: 1063 loss: 6.70131267e-05
Iter: 1064 loss: 6.70496156e-05
Iter: 1065 loss: 6.68820721e-05
Iter: 1066 loss: 6.67527929e-05
Iter: 1067 loss: 6.6492e-05
Iter: 1068 loss: 7.12767869e-05
Iter: 1069 loss: 6.64880936e-05
Iter: 1070 loss: 6.62499151e-05
Iter: 1071 loss: 6.91769601e-05
Iter: 1072 loss: 6.6246379e-05
Iter: 1073 loss: 6.61300146e-05
Iter: 1074 loss: 6.59815123e-05
Iter: 1075 loss: 6.59696379e-05
Iter: 1076 loss: 6.57449345e-05
Iter: 1077 loss: 6.55136901e-05
Iter: 1078 loss: 6.54686592e-05
Iter: 1079 loss: 6.53893949e-05
Iter: 1080 loss: 6.5353961e-05
Iter: 1081 loss: 6.5210028e-05
Iter: 1082 loss: 6.53994211e-05
Iter: 1083 loss: 6.51370137e-05
Iter: 1084 loss: 6.49648427e-05
Iter: 1085 loss: 6.71673e-05
Iter: 1086 loss: 6.49625872e-05
Iter: 1087 loss: 6.47669513e-05
Iter: 1088 loss: 6.47851411e-05
Iter: 1089 loss: 6.46163389e-05
Iter: 1090 loss: 6.43721e-05
Iter: 1091 loss: 6.42034211e-05
Iter: 1092 loss: 6.41155711e-05
Iter: 1093 loss: 6.39436257e-05
Iter: 1094 loss: 6.39360587e-05
Iter: 1095 loss: 6.37409394e-05
Iter: 1096 loss: 6.45411128e-05
Iter: 1097 loss: 6.36984259e-05
Iter: 1098 loss: 6.36188488e-05
Iter: 1099 loss: 6.34389289e-05
Iter: 1100 loss: 6.58943e-05
Iter: 1101 loss: 6.34294847e-05
Iter: 1102 loss: 6.32879528e-05
Iter: 1103 loss: 6.46496701e-05
Iter: 1104 loss: 6.32825249e-05
Iter: 1105 loss: 6.30967043e-05
Iter: 1106 loss: 6.30268478e-05
Iter: 1107 loss: 6.29249625e-05
Iter: 1108 loss: 6.27574846e-05
Iter: 1109 loss: 6.27310437e-05
Iter: 1110 loss: 6.26155233e-05
Iter: 1111 loss: 6.23730157e-05
Iter: 1112 loss: 6.28800772e-05
Iter: 1113 loss: 6.22783118e-05
Iter: 1114 loss: 6.19092098e-05
Iter: 1115 loss: 6.3889951e-05
Iter: 1116 loss: 6.18536578e-05
Iter: 1117 loss: 6.17463957e-05
Iter: 1118 loss: 6.14969467e-05
Iter: 1119 loss: 6.45594409e-05
Iter: 1120 loss: 6.14772289e-05
Iter: 1121 loss: 6.1382e-05
Iter: 1122 loss: 6.13391312e-05
Iter: 1123 loss: 6.12198e-05
Iter: 1124 loss: 6.12490257e-05
Iter: 1125 loss: 6.1133047e-05
Iter: 1126 loss: 6.09808703e-05
Iter: 1127 loss: 6.14635064e-05
Iter: 1128 loss: 6.09375e-05
Iter: 1129 loss: 6.07674883e-05
Iter: 1130 loss: 6.07499569e-05
Iter: 1131 loss: 6.06765389e-05
Iter: 1132 loss: 6.04466659e-05
Iter: 1133 loss: 6.08286064e-05
Iter: 1134 loss: 6.02899745e-05
Iter: 1135 loss: 6.00871208e-05
Iter: 1136 loss: 6.18775593e-05
Iter: 1137 loss: 6.00765e-05
Iter: 1138 loss: 5.99006235e-05
Iter: 1139 loss: 6.14492383e-05
Iter: 1140 loss: 5.98911647e-05
Iter: 1141 loss: 5.97773251e-05
Iter: 1142 loss: 5.96034624e-05
Iter: 1143 loss: 5.95997299e-05
Iter: 1144 loss: 5.95051133e-05
Iter: 1145 loss: 5.95139427e-05
Iter: 1146 loss: 5.94319827e-05
Iter: 1147 loss: 5.92708093e-05
Iter: 1148 loss: 6.1035229e-05
Iter: 1149 loss: 5.92672113e-05
Iter: 1150 loss: 5.91525641e-05
Iter: 1151 loss: 5.89533374e-05
Iter: 1152 loss: 5.89530027e-05
Iter: 1153 loss: 5.88334733e-05
Iter: 1154 loss: 5.88287621e-05
Iter: 1155 loss: 5.87195536e-05
Iter: 1156 loss: 5.91626558e-05
Iter: 1157 loss: 5.86958e-05
Iter: 1158 loss: 5.86262104e-05
Iter: 1159 loss: 5.84196059e-05
Iter: 1160 loss: 5.90561685e-05
Iter: 1161 loss: 5.83163637e-05
Iter: 1162 loss: 5.86249371e-05
Iter: 1163 loss: 5.82156572e-05
Iter: 1164 loss: 5.81834611e-05
Iter: 1165 loss: 5.81457716e-05
Iter: 1166 loss: 5.81413442e-05
Iter: 1167 loss: 5.77258652e-05
Iter: 1168 loss: 6.10775896e-05
Iter: 1169 loss: 5.76991e-05
Iter: 1170 loss: 5.77948667e-05
Iter: 1171 loss: 5.7559264e-05
Iter: 1172 loss: 5.74994192e-05
Iter: 1173 loss: 5.7681129e-05
Iter: 1174 loss: 5.74808291e-05
Iter: 1175 loss: 5.73748111e-05
Iter: 1176 loss: 5.71256096e-05
Iter: 1177 loss: 6.008325e-05
Iter: 1178 loss: 5.71045821e-05
Iter: 1179 loss: 5.70054581e-05
Iter: 1180 loss: 5.76988969e-05
Iter: 1181 loss: 5.69968979e-05
Iter: 1182 loss: 5.68859032e-05
Iter: 1183 loss: 5.67066818e-05
Iter: 1184 loss: 5.67050556e-05
Iter: 1185 loss: 5.64863694e-05
Iter: 1186 loss: 5.69444346e-05
Iter: 1187 loss: 5.63987342e-05
Iter: 1188 loss: 5.62755e-05
Iter: 1189 loss: 5.62640635e-05
Iter: 1190 loss: 5.61563356e-05
Iter: 1191 loss: 5.67899297e-05
Iter: 1192 loss: 5.61431043e-05
Iter: 1193 loss: 5.60895314e-05
Iter: 1194 loss: 5.63585854e-05
Iter: 1195 loss: 5.60811495e-05
Iter: 1196 loss: 5.60157787e-05
Iter: 1197 loss: 5.60793887e-05
Iter: 1198 loss: 5.59798136e-05
Iter: 1199 loss: 5.58919419e-05
Iter: 1200 loss: 5.56772e-05
Iter: 1201 loss: 5.781871e-05
Iter: 1202 loss: 5.56516e-05
Iter: 1203 loss: 5.55420338e-05
Iter: 1204 loss: 5.55414517e-05
Iter: 1205 loss: 5.543511e-05
Iter: 1206 loss: 5.53409118e-05
Iter: 1207 loss: 5.53133541e-05
Iter: 1208 loss: 5.55604056e-05
Iter: 1209 loss: 5.52804122e-05
Iter: 1210 loss: 5.52641686e-05
Iter: 1211 loss: 5.52203164e-05
Iter: 1212 loss: 5.56304949e-05
Iter: 1213 loss: 5.5214703e-05
Iter: 1214 loss: 5.51025878e-05
Iter: 1215 loss: 5.49164033e-05
Iter: 1216 loss: 5.4915603e-05
Iter: 1217 loss: 5.48048847e-05
Iter: 1218 loss: 5.58584434e-05
Iter: 1219 loss: 5.48011194e-05
Iter: 1220 loss: 5.47179698e-05
Iter: 1221 loss: 5.45834191e-05
Iter: 1222 loss: 5.45837247e-05
Iter: 1223 loss: 5.45739022e-05
Iter: 1224 loss: 5.45157491e-05
Iter: 1225 loss: 5.44814175e-05
Iter: 1226 loss: 5.44143877e-05
Iter: 1227 loss: 5.58816537e-05
Iter: 1228 loss: 5.4414304e-05
Iter: 1229 loss: 5.42716734e-05
Iter: 1230 loss: 5.5687211e-05
Iter: 1231 loss: 5.42673224e-05
Iter: 1232 loss: 5.41071422e-05
Iter: 1233 loss: 5.4510343e-05
Iter: 1234 loss: 5.40512119e-05
Iter: 1235 loss: 5.39394023e-05
Iter: 1236 loss: 5.39088214e-05
Iter: 1237 loss: 5.38396926e-05
Iter: 1238 loss: 5.37454362e-05
Iter: 1239 loss: 5.40153851e-05
Iter: 1240 loss: 5.37157757e-05
Iter: 1241 loss: 5.36509688e-05
Iter: 1242 loss: 5.36507905e-05
Iter: 1243 loss: 5.35514373e-05
Iter: 1244 loss: 5.3570584e-05
Iter: 1245 loss: 5.34781175e-05
Iter: 1246 loss: 5.336514e-05
Iter: 1247 loss: 5.35184809e-05
Iter: 1248 loss: 5.33084203e-05
Iter: 1249 loss: 5.32071863e-05
Iter: 1250 loss: 5.29996978e-05
Iter: 1251 loss: 5.66629096e-05
Iter: 1252 loss: 5.2995445e-05
Iter: 1253 loss: 5.2836167e-05
Iter: 1254 loss: 5.28330056e-05
Iter: 1255 loss: 5.27188e-05
Iter: 1256 loss: 5.26854419e-05
Iter: 1257 loss: 5.26170261e-05
Iter: 1258 loss: 5.25463292e-05
Iter: 1259 loss: 5.2508658e-05
Iter: 1260 loss: 5.24822899e-05
Iter: 1261 loss: 5.24227798e-05
Iter: 1262 loss: 5.3139167e-05
Iter: 1263 loss: 5.24172865e-05
Iter: 1264 loss: 5.26647054e-05
Iter: 1265 loss: 5.23643903e-05
Iter: 1266 loss: 5.23133567e-05
Iter: 1267 loss: 5.22748815e-05
Iter: 1268 loss: 5.22586888e-05
Iter: 1269 loss: 5.21628572e-05
Iter: 1270 loss: 5.19700516e-05
Iter: 1271 loss: 5.55719e-05
Iter: 1272 loss: 5.19674613e-05
Iter: 1273 loss: 5.18327979e-05
Iter: 1274 loss: 5.23123636e-05
Iter: 1275 loss: 5.179859e-05
Iter: 1276 loss: 5.17022963e-05
Iter: 1277 loss: 5.27037191e-05
Iter: 1278 loss: 5.17003209e-05
Iter: 1279 loss: 5.16166947e-05
Iter: 1280 loss: 5.17378503e-05
Iter: 1281 loss: 5.15762295e-05
Iter: 1282 loss: 5.14657731e-05
Iter: 1283 loss: 5.12547485e-05
Iter: 1284 loss: 5.5697521e-05
Iter: 1285 loss: 5.12542319e-05
Iter: 1286 loss: 5.10989557e-05
Iter: 1287 loss: 5.10277168e-05
Iter: 1288 loss: 5.09512101e-05
Iter: 1289 loss: 5.06693e-05
Iter: 1290 loss: 5.20968024e-05
Iter: 1291 loss: 5.06227116e-05
Iter: 1292 loss: 5.04609488e-05
Iter: 1293 loss: 5.04586787e-05
Iter: 1294 loss: 5.02712137e-05
Iter: 1295 loss: 5.1394105e-05
Iter: 1296 loss: 5.02497642e-05
Iter: 1297 loss: 5.02157345e-05
Iter: 1298 loss: 5.0130071e-05
Iter: 1299 loss: 5.08886e-05
Iter: 1300 loss: 5.01165705e-05
Iter: 1301 loss: 4.99800226e-05
Iter: 1302 loss: 5.06523793e-05
Iter: 1303 loss: 4.99567832e-05
Iter: 1304 loss: 4.98711051e-05
Iter: 1305 loss: 4.9977607e-05
Iter: 1306 loss: 4.98266963e-05
Iter: 1307 loss: 4.978168e-05
Iter: 1308 loss: 4.97321707e-05
Iter: 1309 loss: 4.97249421e-05
Iter: 1310 loss: 4.95979184e-05
Iter: 1311 loss: 5.0925446e-05
Iter: 1312 loss: 4.9594757e-05
Iter: 1313 loss: 4.95051427e-05
Iter: 1314 loss: 4.94990672e-05
Iter: 1315 loss: 4.94547239e-05
Iter: 1316 loss: 4.9399e-05
Iter: 1317 loss: 4.93949301e-05
Iter: 1318 loss: 4.93196276e-05
Iter: 1319 loss: 4.94848282e-05
Iter: 1320 loss: 4.92912877e-05
Iter: 1321 loss: 4.92415638e-05
Iter: 1322 loss: 4.91221654e-05
Iter: 1323 loss: 5.04037816e-05
Iter: 1324 loss: 4.91099781e-05
Iter: 1325 loss: 4.90429375e-05
Iter: 1326 loss: 4.94107408e-05
Iter: 1327 loss: 4.90334533e-05
Iter: 1328 loss: 4.89693157e-05
Iter: 1329 loss: 4.94899214e-05
Iter: 1330 loss: 4.89657359e-05
Iter: 1331 loss: 4.89300546e-05
Iter: 1332 loss: 4.93744155e-05
Iter: 1333 loss: 4.89296981e-05
Iter: 1334 loss: 4.89103477e-05
Iter: 1335 loss: 4.8873997e-05
Iter: 1336 loss: 4.96625798e-05
Iter: 1337 loss: 4.88737569e-05
Iter: 1338 loss: 4.87821e-05
Iter: 1339 loss: 4.87763318e-05
Iter: 1340 loss: 4.86813406e-05
Iter: 1341 loss: 4.88535297e-05
Iter: 1342 loss: 4.86399622e-05
Iter: 1343 loss: 4.85470882e-05
Iter: 1344 loss: 4.83964432e-05
Iter: 1345 loss: 4.83950062e-05
Iter: 1346 loss: 4.82144023e-05
Iter: 1347 loss: 4.95234199e-05
Iter: 1348 loss: 4.81991447e-05
Iter: 1349 loss: 4.79504415e-05
Iter: 1350 loss: 4.91058e-05
Iter: 1351 loss: 4.79040937e-05
Iter: 1352 loss: 4.7842379e-05
Iter: 1353 loss: 4.79788396e-05
Iter: 1354 loss: 4.78184156e-05
Iter: 1355 loss: 4.77039066e-05
Iter: 1356 loss: 4.75265297e-05
Iter: 1357 loss: 4.75243578e-05
Iter: 1358 loss: 4.7376554e-05
Iter: 1359 loss: 4.80036688e-05
Iter: 1360 loss: 4.73462205e-05
Iter: 1361 loss: 4.71906e-05
Iter: 1362 loss: 4.761542e-05
Iter: 1363 loss: 4.71407548e-05
Iter: 1364 loss: 4.73839827e-05
Iter: 1365 loss: 4.71200037e-05
Iter: 1366 loss: 4.71089e-05
Iter: 1367 loss: 4.70725136e-05
Iter: 1368 loss: 4.70625528e-05
Iter: 1369 loss: 4.70313571e-05
Iter: 1370 loss: 4.69774277e-05
Iter: 1371 loss: 4.69484839e-05
Iter: 1372 loss: 4.68368817e-05
Iter: 1373 loss: 4.70527229e-05
Iter: 1374 loss: 4.67910213e-05
Iter: 1375 loss: 4.6710833e-05
Iter: 1376 loss: 4.6684494e-05
Iter: 1377 loss: 4.66371712e-05
Iter: 1378 loss: 4.65661324e-05
Iter: 1379 loss: 4.70953419e-05
Iter: 1380 loss: 4.65604317e-05
Iter: 1381 loss: 4.65023841e-05
Iter: 1382 loss: 4.65397752e-05
Iter: 1383 loss: 4.64652621e-05
Iter: 1384 loss: 4.64028308e-05
Iter: 1385 loss: 4.63209544e-05
Iter: 1386 loss: 4.63156211e-05
Iter: 1387 loss: 4.62168682e-05
Iter: 1388 loss: 4.63660872e-05
Iter: 1389 loss: 4.61695e-05
Iter: 1390 loss: 4.60934825e-05
Iter: 1391 loss: 4.59884286e-05
Iter: 1392 loss: 4.59841904e-05
Iter: 1393 loss: 4.58557188e-05
Iter: 1394 loss: 4.58433642e-05
Iter: 1395 loss: 4.57491697e-05
Iter: 1396 loss: 4.59318217e-05
Iter: 1397 loss: 4.57071073e-05
Iter: 1398 loss: 4.56524504e-05
Iter: 1399 loss: 4.56381094e-05
Iter: 1400 loss: 4.56049602e-05
Iter: 1401 loss: 4.5504843e-05
Iter: 1402 loss: 4.57135829e-05
Iter: 1403 loss: 4.54660403e-05
Iter: 1404 loss: 4.53855318e-05
Iter: 1405 loss: 4.54590772e-05
Iter: 1406 loss: 4.53395769e-05
Iter: 1407 loss: 4.52454588e-05
Iter: 1408 loss: 4.52447566e-05
Iter: 1409 loss: 4.51966189e-05
Iter: 1410 loss: 4.51950582e-05
Iter: 1411 loss: 4.51635678e-05
Iter: 1412 loss: 4.51121232e-05
Iter: 1413 loss: 4.51112282e-05
Iter: 1414 loss: 4.50428e-05
Iter: 1415 loss: 4.50755106e-05
Iter: 1416 loss: 4.49969084e-05
Iter: 1417 loss: 4.49112049e-05
Iter: 1418 loss: 4.60067567e-05
Iter: 1419 loss: 4.49106665e-05
Iter: 1420 loss: 4.48680657e-05
Iter: 1421 loss: 4.4797609e-05
Iter: 1422 loss: 4.47971179e-05
Iter: 1423 loss: 4.47165658e-05
Iter: 1424 loss: 4.53652101e-05
Iter: 1425 loss: 4.4711509e-05
Iter: 1426 loss: 4.46940467e-05
Iter: 1427 loss: 4.46890917e-05
Iter: 1428 loss: 4.46573977e-05
Iter: 1429 loss: 4.45995247e-05
Iter: 1430 loss: 4.59349976e-05
Iter: 1431 loss: 4.45993719e-05
Iter: 1432 loss: 4.45752885e-05
Iter: 1433 loss: 4.45175938e-05
Iter: 1434 loss: 4.51273299e-05
Iter: 1435 loss: 4.45111e-05
Iter: 1436 loss: 4.44552716e-05
Iter: 1437 loss: 4.44119287e-05
Iter: 1438 loss: 4.43936769e-05
Iter: 1439 loss: 4.43370554e-05
Iter: 1440 loss: 4.42800811e-05
Iter: 1441 loss: 4.42683522e-05
Iter: 1442 loss: 4.41295524e-05
Iter: 1443 loss: 4.4099339e-05
Iter: 1444 loss: 4.40097647e-05
Iter: 1445 loss: 4.38104289e-05
Iter: 1446 loss: 4.48495412e-05
Iter: 1447 loss: 4.37792914e-05
Iter: 1448 loss: 4.36702721e-05
Iter: 1449 loss: 4.37277558e-05
Iter: 1450 loss: 4.35991242e-05
Iter: 1451 loss: 4.35180082e-05
Iter: 1452 loss: 4.34764661e-05
Iter: 1453 loss: 4.33601272e-05
Iter: 1454 loss: 4.31753615e-05
Iter: 1455 loss: 4.3173859e-05
Iter: 1456 loss: 4.3107837e-05
Iter: 1457 loss: 4.31005101e-05
Iter: 1458 loss: 4.31868284e-05
Iter: 1459 loss: 4.30751534e-05
Iter: 1460 loss: 4.30513428e-05
Iter: 1461 loss: 4.30864675e-05
Iter: 1462 loss: 4.3040287e-05
Iter: 1463 loss: 4.30184955e-05
Iter: 1464 loss: 4.29588035e-05
Iter: 1465 loss: 4.32617962e-05
Iter: 1466 loss: 4.2939686e-05
Iter: 1467 loss: 4.36798e-05
Iter: 1468 loss: 4.28965977e-05
Iter: 1469 loss: 4.28393978e-05
Iter: 1470 loss: 4.28541753e-05
Iter: 1471 loss: 4.27975356e-05
Iter: 1472 loss: 4.27193809e-05
Iter: 1473 loss: 4.28698477e-05
Iter: 1474 loss: 4.268665e-05
Iter: 1475 loss: 4.25795224e-05
Iter: 1476 loss: 4.27347914e-05
Iter: 1477 loss: 4.2527281e-05
Iter: 1478 loss: 4.23485108e-05
Iter: 1479 loss: 4.22062512e-05
Iter: 1480 loss: 4.21513541e-05
Iter: 1481 loss: 4.20552569e-05
Iter: 1482 loss: 4.20339311e-05
Iter: 1483 loss: 4.1895506e-05
Iter: 1484 loss: 4.23904e-05
Iter: 1485 loss: 4.18598138e-05
Iter: 1486 loss: 4.18053096e-05
Iter: 1487 loss: 4.16633629e-05
Iter: 1488 loss: 4.27153e-05
Iter: 1489 loss: 4.16324838e-05
Iter: 1490 loss: 4.15301329e-05
Iter: 1491 loss: 4.3089356e-05
Iter: 1492 loss: 4.15300783e-05
Iter: 1493 loss: 4.15398172e-05
Iter: 1494 loss: 4.14777132e-05
Iter: 1495 loss: 4.14654642e-05
Iter: 1496 loss: 4.14317183e-05
Iter: 1497 loss: 4.16063776e-05
Iter: 1498 loss: 4.14215756e-05
Iter: 1499 loss: 4.14517417e-05
Iter: 1500 loss: 4.13797425e-05
Iter: 1501 loss: 4.13338385e-05
Iter: 1502 loss: 4.12892769e-05
Iter: 1503 loss: 4.12789595e-05
Iter: 1504 loss: 4.12019799e-05
Iter: 1505 loss: 4.11660076e-05
Iter: 1506 loss: 4.11287037e-05
Iter: 1507 loss: 4.10775028e-05
Iter: 1508 loss: 4.11997753e-05
Iter: 1509 loss: 4.10590583e-05
Iter: 1510 loss: 4.1000003e-05
Iter: 1511 loss: 4.09515633e-05
Iter: 1512 loss: 4.09340828e-05
Iter: 1513 loss: 4.08311e-05
Iter: 1514 loss: 4.07656407e-05
Iter: 1515 loss: 4.07255357e-05
Iter: 1516 loss: 4.05904e-05
Iter: 1517 loss: 4.03683553e-05
Iter: 1518 loss: 4.03673839e-05
Iter: 1519 loss: 4.01825819e-05
Iter: 1520 loss: 4.09811473e-05
Iter: 1521 loss: 4.01438301e-05
Iter: 1522 loss: 4.00419231e-05
Iter: 1523 loss: 4.00414137e-05
Iter: 1524 loss: 4.00322133e-05
Iter: 1525 loss: 4.0017585e-05
Iter: 1526 loss: 3.9987186e-05
Iter: 1527 loss: 3.99453347e-05
Iter: 1528 loss: 3.99427445e-05
Iter: 1529 loss: 3.98713528e-05
Iter: 1530 loss: 3.98558113e-05
Iter: 1531 loss: 3.98077173e-05
Iter: 1532 loss: 3.97002514e-05
Iter: 1533 loss: 4.03797021e-05
Iter: 1534 loss: 3.96878168e-05
Iter: 1535 loss: 3.96426694e-05
Iter: 1536 loss: 3.97939e-05
Iter: 1537 loss: 3.96299183e-05
Iter: 1538 loss: 3.95863135e-05
Iter: 1539 loss: 3.95355491e-05
Iter: 1540 loss: 3.95291936e-05
Iter: 1541 loss: 3.94324306e-05
Iter: 1542 loss: 4.00576027e-05
Iter: 1543 loss: 3.94219096e-05
Iter: 1544 loss: 3.93551e-05
Iter: 1545 loss: 3.93341143e-05
Iter: 1546 loss: 3.9295046e-05
Iter: 1547 loss: 3.91716603e-05
Iter: 1548 loss: 3.90749192e-05
Iter: 1549 loss: 3.90361747e-05
Iter: 1550 loss: 3.88748158e-05
Iter: 1551 loss: 3.88553744e-05
Iter: 1552 loss: 3.87404179e-05
Iter: 1553 loss: 3.86733809e-05
Iter: 1554 loss: 3.8646e-05
Iter: 1555 loss: 3.86273641e-05
Iter: 1556 loss: 3.86179636e-05
Iter: 1557 loss: 3.85859166e-05
Iter: 1558 loss: 3.90287605e-05
Iter: 1559 loss: 3.85858184e-05
Iter: 1560 loss: 3.85762032e-05
Iter: 1561 loss: 3.85463863e-05
Iter: 1562 loss: 3.86325337e-05
Iter: 1563 loss: 3.85307321e-05
Iter: 1564 loss: 3.847669e-05
Iter: 1565 loss: 3.88351036e-05
Iter: 1566 loss: 3.84716186e-05
Iter: 1567 loss: 3.84470914e-05
Iter: 1568 loss: 3.84740706e-05
Iter: 1569 loss: 3.84340747e-05
Iter: 1570 loss: 3.8399332e-05
Iter: 1571 loss: 3.83345614e-05
Iter: 1572 loss: 3.97716758e-05
Iter: 1573 loss: 3.83343795e-05
Iter: 1574 loss: 3.82519029e-05
Iter: 1575 loss: 3.89845809e-05
Iter: 1576 loss: 3.82472172e-05
Iter: 1577 loss: 3.82088292e-05
Iter: 1578 loss: 3.81813952e-05
Iter: 1579 loss: 3.81679856e-05
Iter: 1580 loss: 3.80968049e-05
Iter: 1581 loss: 3.80026177e-05
Iter: 1582 loss: 3.79966914e-05
Iter: 1583 loss: 3.78798977e-05
Iter: 1584 loss: 3.86556821e-05
Iter: 1585 loss: 3.78678596e-05
Iter: 1586 loss: 3.77679389e-05
Iter: 1587 loss: 3.78460682e-05
Iter: 1588 loss: 3.77078031e-05
Iter: 1589 loss: 3.75708e-05
Iter: 1590 loss: 3.77931065e-05
Iter: 1591 loss: 3.75081945e-05
Iter: 1592 loss: 3.83598417e-05
Iter: 1593 loss: 3.7486665e-05
Iter: 1594 loss: 3.74698284e-05
Iter: 1595 loss: 3.74156443e-05
Iter: 1596 loss: 3.74785668e-05
Iter: 1597 loss: 3.73740477e-05
Iter: 1598 loss: 3.73286712e-05
Iter: 1599 loss: 3.73197108e-05
Iter: 1600 loss: 3.72789582e-05
Iter: 1601 loss: 3.72897921e-05
Iter: 1602 loss: 3.72497052e-05
Iter: 1603 loss: 3.71680544e-05
Iter: 1604 loss: 3.70055e-05
Iter: 1605 loss: 4.01395155e-05
Iter: 1606 loss: 3.70042835e-05
Iter: 1607 loss: 3.69097324e-05
Iter: 1608 loss: 3.69076879e-05
Iter: 1609 loss: 3.68112742e-05
Iter: 1610 loss: 3.70589478e-05
Iter: 1611 loss: 3.67774664e-05
Iter: 1612 loss: 3.672807e-05
Iter: 1613 loss: 3.67271e-05
Iter: 1614 loss: 3.66883905e-05
Iter: 1615 loss: 3.66268287e-05
Iter: 1616 loss: 3.64797124e-05
Iter: 1617 loss: 3.8140206e-05
Iter: 1618 loss: 3.64653824e-05
Iter: 1619 loss: 3.64092484e-05
Iter: 1620 loss: 3.67115135e-05
Iter: 1621 loss: 3.64011612e-05
Iter: 1622 loss: 3.63845102e-05
Iter: 1623 loss: 3.63327e-05
Iter: 1624 loss: 3.63876534e-05
Iter: 1625 loss: 3.62914798e-05
Iter: 1626 loss: 3.63025756e-05
Iter: 1627 loss: 3.62651044e-05
Iter: 1628 loss: 3.62587743e-05
Iter: 1629 loss: 3.62421088e-05
Iter: 1630 loss: 3.63847066e-05
Iter: 1631 loss: 3.62389947e-05
Iter: 1632 loss: 3.61908969e-05
Iter: 1633 loss: 3.65687811e-05
Iter: 1634 loss: 3.61874336e-05
Iter: 1635 loss: 3.6117839e-05
Iter: 1636 loss: 3.62996798e-05
Iter: 1637 loss: 3.60944177e-05
Iter: 1638 loss: 3.60587292e-05
Iter: 1639 loss: 3.60263221e-05
Iter: 1640 loss: 3.60167687e-05
Iter: 1641 loss: 3.59626501e-05
Iter: 1642 loss: 3.68187066e-05
Iter: 1643 loss: 3.59627484e-05
Iter: 1644 loss: 3.59316728e-05
Iter: 1645 loss: 3.58567959e-05
Iter: 1646 loss: 3.66091772e-05
Iter: 1647 loss: 3.58472316e-05
Iter: 1648 loss: 3.57381832e-05
Iter: 1649 loss: 3.57223034e-05
Iter: 1650 loss: 3.56461787e-05
Iter: 1651 loss: 3.59842379e-05
Iter: 1652 loss: 3.55544798e-05
Iter: 1653 loss: 3.55208831e-05
Iter: 1654 loss: 3.54606927e-05
Iter: 1655 loss: 3.69327172e-05
Iter: 1656 loss: 3.54610529e-05
Iter: 1657 loss: 3.54238036e-05
Iter: 1658 loss: 3.5420926e-05
Iter: 1659 loss: 3.54027725e-05
Iter: 1660 loss: 3.55660341e-05
Iter: 1661 loss: 3.54019649e-05
Iter: 1662 loss: 3.53823489e-05
Iter: 1663 loss: 3.53193391e-05
Iter: 1664 loss: 3.54336298e-05
Iter: 1665 loss: 3.52781863e-05
Iter: 1666 loss: 3.52026764e-05
Iter: 1667 loss: 3.51988492e-05
Iter: 1668 loss: 3.51469862e-05
Iter: 1669 loss: 3.51844028e-05
Iter: 1670 loss: 3.51158887e-05
Iter: 1671 loss: 3.50477312e-05
Iter: 1672 loss: 3.49873662e-05
Iter: 1673 loss: 3.49697184e-05
Iter: 1674 loss: 3.49767688e-05
Iter: 1675 loss: 3.49317488e-05
Iter: 1676 loss: 3.48979956e-05
Iter: 1677 loss: 3.48347094e-05
Iter: 1678 loss: 3.61923558e-05
Iter: 1679 loss: 3.48341673e-05
Iter: 1680 loss: 3.47668065e-05
Iter: 1681 loss: 3.47952882e-05
Iter: 1682 loss: 3.47206733e-05
Iter: 1683 loss: 3.46868619e-05
Iter: 1684 loss: 3.4820263e-05
Iter: 1685 loss: 3.46784946e-05
Iter: 1686 loss: 3.46387242e-05
Iter: 1687 loss: 3.47437926e-05
Iter: 1688 loss: 3.46254856e-05
Iter: 1689 loss: 3.46086599e-05
Iter: 1690 loss: 3.45951266e-05
Iter: 1691 loss: 3.45730477e-05
Iter: 1692 loss: 3.45727276e-05
Iter: 1693 loss: 3.4558383e-05
Iter: 1694 loss: 3.45093795e-05
Iter: 1695 loss: 3.44274231e-05
Iter: 1696 loss: 3.44213586e-05
Iter: 1697 loss: 3.4337143e-05
Iter: 1698 loss: 3.43365318e-05
Iter: 1699 loss: 3.43181346e-05
Iter: 1700 loss: 3.42614803e-05
Iter: 1701 loss: 3.43850443e-05
Iter: 1702 loss: 3.42268468e-05
Iter: 1703 loss: 3.41367886e-05
Iter: 1704 loss: 3.4494944e-05
Iter: 1705 loss: 3.41164341e-05
Iter: 1706 loss: 3.40615225e-05
Iter: 1707 loss: 3.40114384e-05
Iter: 1708 loss: 3.39979888e-05
Iter: 1709 loss: 3.39586149e-05
Iter: 1710 loss: 3.39490216e-05
Iter: 1711 loss: 3.39006619e-05
Iter: 1712 loss: 3.40746192e-05
Iter: 1713 loss: 3.38880527e-05
Iter: 1714 loss: 3.38640166e-05
Iter: 1715 loss: 3.38003229e-05
Iter: 1716 loss: 3.42943204e-05
Iter: 1717 loss: 3.37882448e-05
Iter: 1718 loss: 3.37449092e-05
Iter: 1719 loss: 3.37415804e-05
Iter: 1720 loss: 3.37238744e-05
Iter: 1721 loss: 3.37635356e-05
Iter: 1722 loss: 3.37174752e-05
Iter: 1723 loss: 3.36910489e-05
Iter: 1724 loss: 3.39412472e-05
Iter: 1725 loss: 3.36903504e-05
Iter: 1726 loss: 3.36573685e-05
Iter: 1727 loss: 3.36182056e-05
Iter: 1728 loss: 3.36143203e-05
Iter: 1729 loss: 3.35569e-05
Iter: 1730 loss: 3.3549506e-05
Iter: 1731 loss: 3.35093282e-05
Iter: 1732 loss: 3.34658871e-05
Iter: 1733 loss: 3.33859716e-05
Iter: 1734 loss: 3.51930576e-05
Iter: 1735 loss: 3.33857934e-05
Iter: 1736 loss: 3.32595519e-05
Iter: 1737 loss: 3.38042737e-05
Iter: 1738 loss: 3.32345699e-05
Iter: 1739 loss: 3.31628544e-05
Iter: 1740 loss: 3.34666111e-05
Iter: 1741 loss: 3.31473493e-05
Iter: 1742 loss: 3.30975236e-05
Iter: 1743 loss: 3.30024704e-05
Iter: 1744 loss: 3.49885449e-05
Iter: 1745 loss: 3.30023395e-05
Iter: 1746 loss: 3.2947959e-05
Iter: 1747 loss: 3.29441e-05
Iter: 1748 loss: 3.28853312e-05
Iter: 1749 loss: 3.30732437e-05
Iter: 1750 loss: 3.2868782e-05
Iter: 1751 loss: 3.28426249e-05
Iter: 1752 loss: 3.29926734e-05
Iter: 1753 loss: 3.28389287e-05
Iter: 1754 loss: 3.28069036e-05
Iter: 1755 loss: 3.2818265e-05
Iter: 1756 loss: 3.27844755e-05
Iter: 1757 loss: 3.27492853e-05
Iter: 1758 loss: 3.28406823e-05
Iter: 1759 loss: 3.27379457e-05
Iter: 1760 loss: 3.26935551e-05
Iter: 1761 loss: 3.30245493e-05
Iter: 1762 loss: 3.26895752e-05
Iter: 1763 loss: 3.26726658e-05
Iter: 1764 loss: 3.26265435e-05
Iter: 1765 loss: 3.28855931e-05
Iter: 1766 loss: 3.26126828e-05
Iter: 1767 loss: 3.25300061e-05
Iter: 1768 loss: 3.25780202e-05
Iter: 1769 loss: 3.24756547e-05
Iter: 1770 loss: 3.23802815e-05
Iter: 1771 loss: 3.22195556e-05
Iter: 1772 loss: 3.22188716e-05
Iter: 1773 loss: 3.20933068e-05
Iter: 1774 loss: 3.20839463e-05
Iter: 1775 loss: 3.1967953e-05
Iter: 1776 loss: 3.30413459e-05
Iter: 1777 loss: 3.19640712e-05
Iter: 1778 loss: 3.19160863e-05
Iter: 1779 loss: 3.20235e-05
Iter: 1780 loss: 3.18979874e-05
Iter: 1781 loss: 3.18294551e-05
Iter: 1782 loss: 3.16662699e-05
Iter: 1783 loss: 3.34474462e-05
Iter: 1784 loss: 3.16503501e-05
Iter: 1785 loss: 3.23851491e-05
Iter: 1786 loss: 3.16183432e-05
Iter: 1787 loss: 3.1593554e-05
Iter: 1788 loss: 3.16556361e-05
Iter: 1789 loss: 3.15851503e-05
Iter: 1790 loss: 3.1554755e-05
Iter: 1791 loss: 3.14697027e-05
Iter: 1792 loss: 3.19873325e-05
Iter: 1793 loss: 3.14454774e-05
Iter: 1794 loss: 3.13732198e-05
Iter: 1795 loss: 3.20124273e-05
Iter: 1796 loss: 3.13700439e-05
Iter: 1797 loss: 3.13058881e-05
Iter: 1798 loss: 3.15852922e-05
Iter: 1799 loss: 3.12930206e-05
Iter: 1800 loss: 3.12237389e-05
Iter: 1801 loss: 3.15700527e-05
Iter: 1802 loss: 3.12113189e-05
Iter: 1803 loss: 3.11783297e-05
Iter: 1804 loss: 3.11215554e-05
Iter: 1805 loss: 3.11215335e-05
Iter: 1806 loss: 3.10276e-05
Iter: 1807 loss: 3.16729784e-05
Iter: 1808 loss: 3.10180876e-05
Iter: 1809 loss: 3.09537536e-05
Iter: 1810 loss: 3.10246687e-05
Iter: 1811 loss: 3.09185198e-05
Iter: 1812 loss: 3.08530252e-05
Iter: 1813 loss: 3.10250543e-05
Iter: 1814 loss: 3.08300587e-05
Iter: 1815 loss: 3.07617374e-05
Iter: 1816 loss: 3.13946839e-05
Iter: 1817 loss: 3.07589944e-05
Iter: 1818 loss: 3.06941e-05
Iter: 1819 loss: 3.07578302e-05
Iter: 1820 loss: 3.06573784e-05
Iter: 1821 loss: 3.07141199e-05
Iter: 1822 loss: 3.06186048e-05
Iter: 1823 loss: 3.05963586e-05
Iter: 1824 loss: 3.06100883e-05
Iter: 1825 loss: 3.0582225e-05
Iter: 1826 loss: 3.05569629e-05
Iter: 1827 loss: 3.0515992e-05
Iter: 1828 loss: 3.05153335e-05
Iter: 1829 loss: 3.04378827e-05
Iter: 1830 loss: 3.08616363e-05
Iter: 1831 loss: 3.04263667e-05
Iter: 1832 loss: 3.03956094e-05
Iter: 1833 loss: 3.03806155e-05
Iter: 1834 loss: 3.03459419e-05
Iter: 1835 loss: 3.02625358e-05
Iter: 1836 loss: 3.1157615e-05
Iter: 1837 loss: 3.02544086e-05
Iter: 1838 loss: 3.01585642e-05
Iter: 1839 loss: 3.03607594e-05
Iter: 1840 loss: 3.01203472e-05
Iter: 1841 loss: 3.00372521e-05
Iter: 1842 loss: 2.986447e-05
Iter: 1843 loss: 3.29674658e-05
Iter: 1844 loss: 2.98607683e-05
Iter: 1845 loss: 2.97713359e-05
Iter: 1846 loss: 3.10614705e-05
Iter: 1847 loss: 2.9771345e-05
Iter: 1848 loss: 2.96820581e-05
Iter: 1849 loss: 2.97618899e-05
Iter: 1850 loss: 2.96299659e-05
Iter: 1851 loss: 2.95735135e-05
Iter: 1852 loss: 2.9790077e-05
Iter: 1853 loss: 2.95605932e-05
Iter: 1854 loss: 2.95482769e-05
Iter: 1855 loss: 2.95331829e-05
Iter: 1856 loss: 2.94964157e-05
Iter: 1857 loss: 2.95192385e-05
Iter: 1858 loss: 2.94746078e-05
Iter: 1859 loss: 2.94351412e-05
Iter: 1860 loss: 2.93724333e-05
Iter: 1861 loss: 2.93721314e-05
Iter: 1862 loss: 2.92829463e-05
Iter: 1863 loss: 3.02692497e-05
Iter: 1864 loss: 2.92821223e-05
Iter: 1865 loss: 2.92181558e-05
Iter: 1866 loss: 2.96458238e-05
Iter: 1867 loss: 2.92123423e-05
Iter: 1868 loss: 2.91721608e-05
Iter: 1869 loss: 2.95045029e-05
Iter: 1870 loss: 2.91691522e-05
Iter: 1871 loss: 2.91463657e-05
Iter: 1872 loss: 2.91020624e-05
Iter: 1873 loss: 2.99972744e-05
Iter: 1874 loss: 2.91015422e-05
Iter: 1875 loss: 2.90129574e-05
Iter: 1876 loss: 2.89154559e-05
Iter: 1877 loss: 2.8901286e-05
Iter: 1878 loss: 2.87731873e-05
Iter: 1879 loss: 2.94350029e-05
Iter: 1880 loss: 2.87516632e-05
Iter: 1881 loss: 2.86805916e-05
Iter: 1882 loss: 2.97564602e-05
Iter: 1883 loss: 2.86807644e-05
Iter: 1884 loss: 2.86311461e-05
Iter: 1885 loss: 2.8576249e-05
Iter: 1886 loss: 2.85685492e-05
Iter: 1887 loss: 2.84881207e-05
Iter: 1888 loss: 2.89160689e-05
Iter: 1889 loss: 2.84739581e-05
Iter: 1890 loss: 2.84854214e-05
Iter: 1891 loss: 2.84625021e-05
Iter: 1892 loss: 2.84502603e-05
Iter: 1893 loss: 2.84299786e-05
Iter: 1894 loss: 2.84293619e-05
Iter: 1895 loss: 2.83858517e-05
Iter: 1896 loss: 2.8347753e-05
Iter: 1897 loss: 2.83356731e-05
Iter: 1898 loss: 2.83205773e-05
Iter: 1899 loss: 2.83043009e-05
Iter: 1900 loss: 2.82665842e-05
Iter: 1901 loss: 2.83402787e-05
Iter: 1902 loss: 2.82512374e-05
Iter: 1903 loss: 2.82173369e-05
Iter: 1904 loss: 2.83334721e-05
Iter: 1905 loss: 2.82080928e-05
Iter: 1906 loss: 2.8162267e-05
Iter: 1907 loss: 2.81239809e-05
Iter: 1908 loss: 2.81111697e-05
Iter: 1909 loss: 2.80150343e-05
Iter: 1910 loss: 2.80980476e-05
Iter: 1911 loss: 2.79591695e-05
Iter: 1912 loss: 2.79147134e-05
Iter: 1913 loss: 2.79226697e-05
Iter: 1914 loss: 2.78810567e-05
Iter: 1915 loss: 2.77976087e-05
Iter: 1916 loss: 2.79186606e-05
Iter: 1917 loss: 2.77581021e-05
Iter: 1918 loss: 2.78264342e-05
Iter: 1919 loss: 2.77159779e-05
Iter: 1920 loss: 2.76906085e-05
Iter: 1921 loss: 2.76648534e-05
Iter: 1922 loss: 2.76597293e-05
Iter: 1923 loss: 2.76175815e-05
Iter: 1924 loss: 2.76153878e-05
Iter: 1925 loss: 2.76037972e-05
Iter: 1926 loss: 2.75731709e-05
Iter: 1927 loss: 2.78201969e-05
Iter: 1928 loss: 2.75671518e-05
Iter: 1929 loss: 2.75077546e-05
Iter: 1930 loss: 2.78303232e-05
Iter: 1931 loss: 2.74988161e-05
Iter: 1932 loss: 2.74489139e-05
Iter: 1933 loss: 2.77304607e-05
Iter: 1934 loss: 2.74417289e-05
Iter: 1935 loss: 2.74210652e-05
Iter: 1936 loss: 2.74612976e-05
Iter: 1937 loss: 2.74132326e-05
Iter: 1938 loss: 2.73793958e-05
Iter: 1939 loss: 2.73514488e-05
Iter: 1940 loss: 2.73419701e-05
Iter: 1941 loss: 2.72929901e-05
Iter: 1942 loss: 2.73295827e-05
Iter: 1943 loss: 2.72627331e-05
Iter: 1944 loss: 2.72208163e-05
Iter: 1945 loss: 2.77504605e-05
Iter: 1946 loss: 2.72204325e-05
Iter: 1947 loss: 2.71792269e-05
Iter: 1948 loss: 2.70656983e-05
Iter: 1949 loss: 2.77542167e-05
Iter: 1950 loss: 2.70350702e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.8
+ date
Sun Nov  8 15:13:49 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322c45c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322c889d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322b9d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322d02730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322d02158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322ba3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322ade840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322b60598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322b60048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322ab9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322ab92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33229e7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322a038c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322a471e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322a42620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322a669d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322a77d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5d05f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322abd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5d05048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f332299b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5cef268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f332299bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c85c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c85950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c50ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32b0487d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c502f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c0a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c370d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c12400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32b04db158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32b04db510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32b04d4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32b04321e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32b0457c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.48700154
test_loss: 0.49055308
train_loss: 0.46593893
test_loss: 0.47122043
train_loss: 0.45372126
test_loss: 0.44546172
train_loss: 0.41408134
test_loss: 0.4154609
train_loss: 0.37890992
test_loss: 0.38370243
train_loss: 0.34845036
test_loss: 0.35039428
train_loss: 0.32031336
test_loss: 0.31547594
train_loss: 0.28060904
test_loss: 0.2792708
train_loss: 0.24340805
test_loss: 0.24191205
train_loss: 0.20383784
test_loss: 0.2049451
train_loss: 0.17433244
test_loss: 0.17094672
train_loss: 0.14078113
test_loss: 0.14164941
train_loss: 0.117814064
test_loss: 0.11757349
train_loss: 0.09768741
test_loss: 0.09838612
train_loss: 0.08473359
test_loss: 0.08388011
train_loss: 0.073400885
test_loss: 0.07401935
train_loss: 0.06907702
test_loss: 0.068387024
train_loss: 0.06412223
test_loss: 0.06561275
train_loss: 0.06319759
test_loss: 0.06436497
train_loss: 0.061797943
test_loss: 0.06373756
train_loss: 0.063238606
test_loss: 0.06337673
train_loss: 0.062373415
test_loss: 0.06313545
train_loss: 0.062313378
test_loss: 0.06296047
train_loss: 0.063229285
test_loss: 0.06283252
train_loss: 0.061690606
test_loss: 0.06270845
train_loss: 0.06133627
test_loss: 0.06260966
train_loss: 0.06401385
test_loss: 0.06251477
train_loss: 0.061234362
test_loss: 0.062401872
train_loss: 0.061885215
test_loss: 0.06237953
train_loss: 0.060625672
test_loss: 0.06228263
train_loss: 0.06166519
test_loss: 0.062166102
train_loss: 0.062761635
test_loss: 0.062118955
train_loss: 0.061633147
test_loss: 0.06196964
train_loss: 0.061406583
test_loss: 0.061848667
train_loss: 0.0609881
test_loss: 0.061727222
train_loss: 0.05978163
test_loss: 0.061569344
train_loss: 0.06078554
test_loss: 0.061368898
train_loss: 0.061763607
test_loss: 0.061185453
train_loss: 0.0602391
test_loss: 0.06096214
train_loss: 0.059894003
test_loss: 0.06076329
train_loss: 0.0603165
test_loss: 0.06047938
train_loss: 0.059510242
test_loss: 0.060210742
train_loss: 0.059585176
test_loss: 0.059971772
train_loss: 0.05827367
test_loss: 0.05974709
train_loss: 0.058508508
test_loss: 0.059596784
train_loss: 0.05890901
test_loss: 0.059209086
train_loss: 0.05888218
test_loss: 0.05903517
train_loss: 0.059137058
test_loss: 0.058711573
train_loss: 0.057280034
test_loss: 0.058460075
train_loss: 0.05726093
test_loss: 0.05821323
train_loss: 0.05720301
test_loss: 0.05794056
train_loss: 0.056390993
test_loss: 0.057677176
train_loss: 0.055792052
test_loss: 0.057357263
train_loss: 0.055929117
test_loss: 0.056880735
train_loss: 0.055755265
test_loss: 0.05640816
train_loss: 0.055945
test_loss: 0.05580481
train_loss: 0.054710906
test_loss: 0.05513195
train_loss: 0.055085957
test_loss: 0.054155853
train_loss: 0.052616913
test_loss: 0.05300797
train_loss: 0.050750665
test_loss: 0.051659092
train_loss: 0.049333982
test_loss: 0.05013545
train_loss: 0.04847934
test_loss: 0.048538487
train_loss: 0.045003727
test_loss: 0.046736628
train_loss: 0.04403948
test_loss: 0.044645414
train_loss: 0.040943757
test_loss: 0.042352766
train_loss: 0.038799558
test_loss: 0.040002376
train_loss: 0.03654048
test_loss: 0.037369397
train_loss: 0.03324253
test_loss: 0.034536988
train_loss: 0.030760368
test_loss: 0.031505186
train_loss: 0.027180826
test_loss: 0.027818833
train_loss: 0.024026113
test_loss: 0.024110215
train_loss: 0.020581508
test_loss: 0.020638198
train_loss: 0.016916005
test_loss: 0.017626416
train_loss: 0.014167286
test_loss: 0.01488016
train_loss: 0.012286585
test_loss: 0.012740644
train_loss: 0.010830435
test_loss: 0.011348542
train_loss: 0.010408332
test_loss: 0.010404031
train_loss: 0.009167238
test_loss: 0.009952922
train_loss: 0.008464707
test_loss: 0.008823276
train_loss: 0.008206389
test_loss: 0.00836352
train_loss: 0.0074788234
test_loss: 0.0081259655
train_loss: 0.0076480247
test_loss: 0.0074417195
train_loss: 0.006783188
test_loss: 0.007281548
train_loss: 0.006686287
test_loss: 0.006776295
train_loss: 0.0062102415
test_loss: 0.006652292
train_loss: 0.0061057913
test_loss: 0.006919803
train_loss: 0.005667397
test_loss: 0.005738978
train_loss: 0.0058682337
test_loss: 0.008888647
train_loss: 0.0055620503
test_loss: 0.00539806
train_loss: 0.0053197467
test_loss: 0.0050974702
train_loss: 0.004852638
test_loss: 0.005012508
train_loss: 0.0049937614
test_loss: 0.005018332
train_loss: 0.0045273434
test_loss: 0.0047184457
train_loss: 0.0050580488
test_loss: 0.005175895
train_loss: 0.004486068
test_loss: 0.0044619753
train_loss: 0.0046114447
test_loss: 0.004771621
train_loss: 0.005036513
test_loss: 0.004658835
train_loss: 0.003910587
test_loss: 0.004994685
train_loss: 0.004007464
test_loss: 0.0046958607
train_loss: 0.0040905373
test_loss: 0.0038870124
train_loss: 0.0052944412
test_loss: 0.0042480035
train_loss: 0.0046311156
test_loss: 0.0050274907
train_loss: 0.0036046864
test_loss: 0.0038097447
train_loss: 0.003908188
test_loss: 0.0038455522
train_loss: 0.0041842186
test_loss: 0.0042547663
train_loss: 0.0043501286
test_loss: 0.0038560752
train_loss: 0.0039767893
test_loss: 0.0042661536
train_loss: 0.003826624
test_loss: 0.004215628
train_loss: 0.003465327
test_loss: 0.004624725
train_loss: 0.0038219683
test_loss: 0.0034987049
train_loss: 0.0053857774
test_loss: 0.004026689
train_loss: 0.0038281952
test_loss: 0.0039653345
train_loss: 0.0033102836
test_loss: 0.0032615236
train_loss: 0.0029956256
test_loss: 0.0031763392
train_loss: 0.003590158
test_loss: 0.0036499684
train_loss: 0.0038162065
test_loss: 0.0040912116
train_loss: 0.0032672146
test_loss: 0.0038594855
train_loss: 0.003323847
test_loss: 0.0034757413
train_loss: 0.0029866288
test_loss: 0.0034355775
train_loss: 0.0035709587
test_loss: 0.0035322758
train_loss: 0.0038000164
test_loss: 0.0035233467
train_loss: 0.0036928132
test_loss: 0.0036026067
train_loss: 0.0031949936
test_loss: 0.0036052282
train_loss: 0.0036500741
test_loss: 0.0031205853
train_loss: 0.0030318233
test_loss: 0.0030286526
train_loss: 0.0033137002
test_loss: 0.0032224553
train_loss: 0.0030444781
test_loss: 0.0033301893
train_loss: 0.0033955644
test_loss: 0.0030449145
train_loss: 0.0032897547
test_loss: 0.0034647603
train_loss: 0.0033704618
test_loss: 0.0030433035
train_loss: 0.0028136584
test_loss: 0.0033419922
train_loss: 0.0040235957
test_loss: 0.0031555176
train_loss: 0.0030614631
test_loss: 0.003387405
train_loss: 0.0032269084
test_loss: 0.0033848847
train_loss: 0.002965702
test_loss: 0.0028464391
train_loss: 0.0028990281
test_loss: 0.0028269156
train_loss: 0.002921071
test_loss: 0.003083124
train_loss: 0.0030659232
test_loss: 0.0031089785
train_loss: 0.0032058014
test_loss: 0.0031333163
train_loss: 0.0028606462
test_loss: 0.0034927907
train_loss: 0.0033998708
test_loss: 0.0031778142
train_loss: 0.0032468615
test_loss: 0.00318104
train_loss: 0.0033862435
test_loss: 0.0034372122
train_loss: 0.0026705363
test_loss: 0.0031310993
train_loss: 0.0026409233
test_loss: 0.0031941775
train_loss: 0.0028027869
test_loss: 0.0030709498
train_loss: 0.0031692935
test_loss: 0.0029560712
train_loss: 0.0036115162
test_loss: 0.0030456954
train_loss: 0.0031118798
test_loss: 0.0032579429
train_loss: 0.0027854443
test_loss: 0.0026760935
train_loss: 0.002881075
test_loss: 0.0030882293
train_loss: 0.0030552018
test_loss: 0.0029760327
train_loss: 0.0031863407
test_loss: 0.0035426384
train_loss: 0.0033876218
test_loss: 0.0027618017
train_loss: 0.0031223397
test_loss: 0.003000641
train_loss: 0.0030620391
test_loss: 0.0027911116
train_loss: 0.0029491354
test_loss: 0.0034141787
train_loss: 0.0027412665
test_loss: 0.0028492175
train_loss: 0.0032791668
test_loss: 0.0032314148
train_loss: 0.0034319086
test_loss: 0.002742403
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.8/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc551c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc4ec950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc5897b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc49a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc49a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc5892f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc45c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc3c0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc3c0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc375158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc3759d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc324e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc352d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc3091e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc31bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc2d9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc2d9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc2d9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc24d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc1eba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc20a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3d3f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3d426a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3d72f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3d0b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3cf4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3cc0620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3cf4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3c441e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3c680d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3c72510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f298c3d61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f298c3c3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f298c3859d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f298c332268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f298c2dcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.16335759e-05
Iter: 2 loss: 4.751503e-05
Iter: 3 loss: 9.12757241e-06
Iter: 4 loss: 7.81941435e-06
Iter: 5 loss: 8.61165245e-06
Iter: 6 loss: 6.97721e-06
Iter: 7 loss: 5.22997698e-06
Iter: 8 loss: 1.19700599e-05
Iter: 9 loss: 4.8192851e-06
Iter: 10 loss: 4.31621629e-06
Iter: 11 loss: 7.30855845e-06
Iter: 12 loss: 4.25394774e-06
Iter: 13 loss: 3.88240551e-06
Iter: 14 loss: 3.64620746e-06
Iter: 15 loss: 3.49985726e-06
Iter: 16 loss: 3.28820829e-06
Iter: 17 loss: 6.36246796e-06
Iter: 18 loss: 3.28798455e-06
Iter: 19 loss: 3.14566e-06
Iter: 20 loss: 3.38220161e-06
Iter: 21 loss: 3.08107019e-06
Iter: 22 loss: 2.98669329e-06
Iter: 23 loss: 3.78687901e-06
Iter: 24 loss: 2.9812511e-06
Iter: 25 loss: 2.91431888e-06
Iter: 26 loss: 2.94844062e-06
Iter: 27 loss: 2.86971044e-06
Iter: 28 loss: 2.79647429e-06
Iter: 29 loss: 3.08357676e-06
Iter: 30 loss: 2.77971344e-06
Iter: 31 loss: 2.73266187e-06
Iter: 32 loss: 2.71302179e-06
Iter: 33 loss: 2.68834219e-06
Iter: 34 loss: 2.64269738e-06
Iter: 35 loss: 3.2434225e-06
Iter: 36 loss: 2.64236132e-06
Iter: 37 loss: 2.61169521e-06
Iter: 38 loss: 2.61114337e-06
Iter: 39 loss: 2.59840931e-06
Iter: 40 loss: 2.67185396e-06
Iter: 41 loss: 2.59664034e-06
Iter: 42 loss: 2.58428759e-06
Iter: 43 loss: 2.55598388e-06
Iter: 44 loss: 2.92391883e-06
Iter: 45 loss: 2.55414443e-06
Iter: 46 loss: 2.52763539e-06
Iter: 47 loss: 2.92706454e-06
Iter: 48 loss: 2.52754398e-06
Iter: 49 loss: 2.5146046e-06
Iter: 50 loss: 2.48943616e-06
Iter: 51 loss: 3.00989677e-06
Iter: 52 loss: 2.48923197e-06
Iter: 53 loss: 2.46737955e-06
Iter: 54 loss: 2.46739091e-06
Iter: 55 loss: 2.45428282e-06
Iter: 56 loss: 2.45367255e-06
Iter: 57 loss: 2.44361877e-06
Iter: 58 loss: 2.42525448e-06
Iter: 59 loss: 2.51729625e-06
Iter: 60 loss: 2.42226452e-06
Iter: 61 loss: 2.40679765e-06
Iter: 62 loss: 2.41012708e-06
Iter: 63 loss: 2.39563678e-06
Iter: 64 loss: 2.37593417e-06
Iter: 65 loss: 2.44454532e-06
Iter: 66 loss: 2.37068252e-06
Iter: 67 loss: 2.35718858e-06
Iter: 68 loss: 2.35575453e-06
Iter: 69 loss: 2.34589925e-06
Iter: 70 loss: 2.34649451e-06
Iter: 71 loss: 2.33970627e-06
Iter: 72 loss: 2.33282685e-06
Iter: 73 loss: 2.33798119e-06
Iter: 74 loss: 2.32853654e-06
Iter: 75 loss: 2.32287971e-06
Iter: 76 loss: 2.39197902e-06
Iter: 77 loss: 2.32280763e-06
Iter: 78 loss: 2.32002e-06
Iter: 79 loss: 2.31475337e-06
Iter: 80 loss: 2.44245712e-06
Iter: 81 loss: 2.31480976e-06
Iter: 82 loss: 2.30736146e-06
Iter: 83 loss: 2.34810705e-06
Iter: 84 loss: 2.3064481e-06
Iter: 85 loss: 2.30268915e-06
Iter: 86 loss: 2.2965296e-06
Iter: 87 loss: 2.29650823e-06
Iter: 88 loss: 2.28875433e-06
Iter: 89 loss: 2.38057919e-06
Iter: 90 loss: 2.28865383e-06
Iter: 91 loss: 2.28407453e-06
Iter: 92 loss: 2.28609656e-06
Iter: 93 loss: 2.28111458e-06
Iter: 94 loss: 2.27445184e-06
Iter: 95 loss: 2.29411535e-06
Iter: 96 loss: 2.27238934e-06
Iter: 97 loss: 2.26686461e-06
Iter: 98 loss: 2.26966063e-06
Iter: 99 loss: 2.26319094e-06
Iter: 100 loss: 2.2559866e-06
Iter: 101 loss: 2.27851569e-06
Iter: 102 loss: 2.25385156e-06
Iter: 103 loss: 2.24851237e-06
Iter: 104 loss: 2.24817677e-06
Iter: 105 loss: 2.24420933e-06
Iter: 106 loss: 2.24977975e-06
Iter: 107 loss: 2.24214227e-06
Iter: 108 loss: 2.24008932e-06
Iter: 109 loss: 2.23680422e-06
Iter: 110 loss: 2.23677716e-06
Iter: 111 loss: 2.2328461e-06
Iter: 112 loss: 2.26800307e-06
Iter: 113 loss: 2.2324964e-06
Iter: 114 loss: 2.23068719e-06
Iter: 115 loss: 2.22996732e-06
Iter: 116 loss: 2.2289621e-06
Iter: 117 loss: 2.22515382e-06
Iter: 118 loss: 2.22627091e-06
Iter: 119 loss: 2.222391e-06
Iter: 120 loss: 2.21912887e-06
Iter: 121 loss: 2.21932441e-06
Iter: 122 loss: 2.2165641e-06
Iter: 123 loss: 2.21308323e-06
Iter: 124 loss: 2.21299479e-06
Iter: 125 loss: 2.21110872e-06
Iter: 126 loss: 2.20847e-06
Iter: 127 loss: 2.2083791e-06
Iter: 128 loss: 2.20403854e-06
Iter: 129 loss: 2.23141433e-06
Iter: 130 loss: 2.20360607e-06
Iter: 131 loss: 2.20111747e-06
Iter: 132 loss: 2.20175343e-06
Iter: 133 loss: 2.19940625e-06
Iter: 134 loss: 2.19617e-06
Iter: 135 loss: 2.20694983e-06
Iter: 136 loss: 2.19533308e-06
Iter: 137 loss: 2.19250501e-06
Iter: 138 loss: 2.19231561e-06
Iter: 139 loss: 2.19023536e-06
Iter: 140 loss: 2.18953664e-06
Iter: 141 loss: 2.18884543e-06
Iter: 142 loss: 2.18701143e-06
Iter: 143 loss: 2.18579498e-06
Iter: 144 loss: 2.18505875e-06
Iter: 145 loss: 2.18374953e-06
Iter: 146 loss: 2.18766536e-06
Iter: 147 loss: 2.18349055e-06
Iter: 148 loss: 2.1815174e-06
Iter: 149 loss: 2.18032392e-06
Iter: 150 loss: 2.17951856e-06
Iter: 151 loss: 2.17764182e-06
Iter: 152 loss: 2.17813977e-06
Iter: 153 loss: 2.17630577e-06
Iter: 154 loss: 2.17394131e-06
Iter: 155 loss: 2.20729453e-06
Iter: 156 loss: 2.17388629e-06
Iter: 157 loss: 2.17258525e-06
Iter: 158 loss: 2.17037336e-06
Iter: 159 loss: 2.17038132e-06
Iter: 160 loss: 2.16811668e-06
Iter: 161 loss: 2.16811713e-06
Iter: 162 loss: 2.16692e-06
Iter: 163 loss: 2.16713465e-06
Iter: 164 loss: 2.16601984e-06
Iter: 165 loss: 2.16428452e-06
Iter: 166 loss: 2.18114678e-06
Iter: 167 loss: 2.16430226e-06
Iter: 168 loss: 2.16333092e-06
Iter: 169 loss: 2.16146236e-06
Iter: 170 loss: 2.20526181e-06
Iter: 171 loss: 2.16146282e-06
Iter: 172 loss: 2.15966611e-06
Iter: 173 loss: 2.1597034e-06
Iter: 174 loss: 2.15879368e-06
Iter: 175 loss: 2.15875798e-06
Iter: 176 loss: 2.15829164e-06
Iter: 177 loss: 2.15690716e-06
Iter: 178 loss: 2.16189369e-06
Iter: 179 loss: 2.15634054e-06
Iter: 180 loss: 2.15587534e-06
Iter: 181 loss: 2.15539558e-06
Iter: 182 loss: 2.15466844e-06
Iter: 183 loss: 2.15264072e-06
Iter: 184 loss: 2.15967339e-06
Iter: 185 loss: 2.151751e-06
Iter: 186 loss: 2.14993361e-06
Iter: 187 loss: 2.16852231e-06
Iter: 188 loss: 2.14984061e-06
Iter: 189 loss: 2.14825286e-06
Iter: 190 loss: 2.16361809e-06
Iter: 191 loss: 2.14813281e-06
Iter: 192 loss: 2.14720512e-06
Iter: 193 loss: 2.14580609e-06
Iter: 194 loss: 2.14583497e-06
Iter: 195 loss: 2.14433021e-06
Iter: 196 loss: 2.16705735e-06
Iter: 197 loss: 2.14434681e-06
Iter: 198 loss: 2.14337751e-06
Iter: 199 loss: 2.14295096e-06
Iter: 200 loss: 2.14240185e-06
Iter: 201 loss: 2.14102329e-06
Iter: 202 loss: 2.15109935e-06
Iter: 203 loss: 2.14091096e-06
Iter: 204 loss: 2.13990506e-06
Iter: 205 loss: 2.14027727e-06
Iter: 206 loss: 2.1391977e-06
Iter: 207 loss: 2.1382873e-06
Iter: 208 loss: 2.13823705e-06
Iter: 209 loss: 2.13766452e-06
Iter: 210 loss: 2.13667431e-06
Iter: 211 loss: 2.15699242e-06
Iter: 212 loss: 2.13661838e-06
Iter: 213 loss: 2.13573412e-06
Iter: 214 loss: 2.13874637e-06
Iter: 215 loss: 2.13542808e-06
Iter: 216 loss: 2.13430076e-06
Iter: 217 loss: 2.14122247e-06
Iter: 218 loss: 2.13413796e-06
Iter: 219 loss: 2.13343606e-06
Iter: 220 loss: 2.13178532e-06
Iter: 221 loss: 2.1495905e-06
Iter: 222 loss: 2.13170711e-06
Iter: 223 loss: 2.13002181e-06
Iter: 224 loss: 2.13572275e-06
Iter: 225 loss: 2.12971781e-06
Iter: 226 loss: 2.12837222e-06
Iter: 227 loss: 2.12831173e-06
Iter: 228 loss: 2.12767418e-06
Iter: 229 loss: 2.12599161e-06
Iter: 230 loss: 2.14727697e-06
Iter: 231 loss: 2.12587588e-06
Iter: 232 loss: 2.12498367e-06
Iter: 233 loss: 2.1247838e-06
Iter: 234 loss: 2.1239025e-06
Iter: 235 loss: 2.12363625e-06
Iter: 236 loss: 2.12312375e-06
Iter: 237 loss: 2.12203349e-06
Iter: 238 loss: 2.13094427e-06
Iter: 239 loss: 2.122046e-06
Iter: 240 loss: 2.12121768e-06
Iter: 241 loss: 2.12370219e-06
Iter: 242 loss: 2.12098416e-06
Iter: 243 loss: 2.11977249e-06
Iter: 244 loss: 2.12454825e-06
Iter: 245 loss: 2.11959627e-06
Iter: 246 loss: 2.11908468e-06
Iter: 247 loss: 2.11789597e-06
Iter: 248 loss: 2.12703799e-06
Iter: 249 loss: 2.11771703e-06
Iter: 250 loss: 2.11684801e-06
Iter: 251 loss: 2.11666793e-06
Iter: 252 loss: 2.11625229e-06
Iter: 253 loss: 2.11520887e-06
Iter: 254 loss: 2.12312852e-06
Iter: 255 loss: 2.11505653e-06
Iter: 256 loss: 2.1137339e-06
Iter: 257 loss: 2.1145604e-06
Iter: 258 loss: 2.1129722e-06
Iter: 259 loss: 2.11176848e-06
Iter: 260 loss: 2.12570353e-06
Iter: 261 loss: 2.11175848e-06
Iter: 262 loss: 2.11084171e-06
Iter: 263 loss: 2.11834868e-06
Iter: 264 loss: 2.11079123e-06
Iter: 265 loss: 2.11005408e-06
Iter: 266 loss: 2.11264705e-06
Iter: 267 loss: 2.10999406e-06
Iter: 268 loss: 2.10933581e-06
Iter: 269 loss: 2.10785402e-06
Iter: 270 loss: 2.1239016e-06
Iter: 271 loss: 2.1076421e-06
Iter: 272 loss: 2.10713347e-06
Iter: 273 loss: 2.1069452e-06
Iter: 274 loss: 2.10602957e-06
Iter: 275 loss: 2.10567532e-06
Iter: 276 loss: 2.10538747e-06
Iter: 277 loss: 2.10506369e-06
Iter: 278 loss: 2.10479698e-06
Iter: 279 loss: 2.10438566e-06
Iter: 280 loss: 2.10412645e-06
Iter: 281 loss: 2.10403527e-06
Iter: 282 loss: 2.10315989e-06
Iter: 283 loss: 2.10245207e-06
Iter: 284 loss: 2.10231883e-06
Iter: 285 loss: 2.10154076e-06
Iter: 286 loss: 2.10494431e-06
Iter: 287 loss: 2.10134e-06
Iter: 288 loss: 2.10041026e-06
Iter: 289 loss: 2.10393773e-06
Iter: 290 loss: 2.10007693e-06
Iter: 291 loss: 2.09951122e-06
Iter: 292 loss: 2.0982643e-06
Iter: 293 loss: 2.11213501e-06
Iter: 294 loss: 2.09806512e-06
Iter: 295 loss: 2.09690461e-06
Iter: 296 loss: 2.10188159e-06
Iter: 297 loss: 2.09661061e-06
Iter: 298 loss: 2.09560903e-06
Iter: 299 loss: 2.09848031e-06
Iter: 300 loss: 2.09509199e-06
Iter: 301 loss: 2.09458358e-06
Iter: 302 loss: 2.09445466e-06
Iter: 303 loss: 2.09393829e-06
Iter: 304 loss: 2.09306677e-06
Iter: 305 loss: 2.10916642e-06
Iter: 306 loss: 2.09302198e-06
Iter: 307 loss: 2.09225641e-06
Iter: 308 loss: 2.1026417e-06
Iter: 309 loss: 2.09229165e-06
Iter: 310 loss: 2.09157133e-06
Iter: 311 loss: 2.09096879e-06
Iter: 312 loss: 2.09075233e-06
Iter: 313 loss: 2.09044379e-06
Iter: 314 loss: 2.09036352e-06
Iter: 315 loss: 2.08982169e-06
Iter: 316 loss: 2.08961592e-06
Iter: 317 loss: 2.08946631e-06
Iter: 318 loss: 2.08883694e-06
Iter: 319 loss: 2.08822166e-06
Iter: 320 loss: 2.08813367e-06
Iter: 321 loss: 2.08729034e-06
Iter: 322 loss: 2.09445352e-06
Iter: 323 loss: 2.08733e-06
Iter: 324 loss: 2.08669144e-06
Iter: 325 loss: 2.08760093e-06
Iter: 326 loss: 2.08644701e-06
Iter: 327 loss: 2.08573738e-06
Iter: 328 loss: 2.08570327e-06
Iter: 329 loss: 2.08523829e-06
Iter: 330 loss: 2.0844202e-06
Iter: 331 loss: 2.08337019e-06
Iter: 332 loss: 2.08331357e-06
Iter: 333 loss: 2.08212441e-06
Iter: 334 loss: 2.08906954e-06
Iter: 335 loss: 2.08190841e-06
Iter: 336 loss: 2.08169627e-06
Iter: 337 loss: 2.08129177e-06
Iter: 338 loss: 2.08100664e-06
Iter: 339 loss: 2.0802313e-06
Iter: 340 loss: 2.08438655e-06
Iter: 341 loss: 2.07992753e-06
Iter: 342 loss: 2.07978292e-06
Iter: 343 loss: 2.07948051e-06
Iter: 344 loss: 2.07904327e-06
Iter: 345 loss: 2.07860512e-06
Iter: 346 loss: 2.07851735e-06
Iter: 347 loss: 2.07810149e-06
Iter: 348 loss: 2.07803669e-06
Iter: 349 loss: 2.07780772e-06
Iter: 350 loss: 2.07747235e-06
Iter: 351 loss: 2.07739276e-06
Iter: 352 loss: 2.07681137e-06
Iter: 353 loss: 2.0771065e-06
Iter: 354 loss: 2.0765774e-06
Iter: 355 loss: 2.07598168e-06
Iter: 356 loss: 2.07605626e-06
Iter: 357 loss: 2.07557741e-06
Iter: 358 loss: 2.075064e-06
Iter: 359 loss: 2.07499852e-06
Iter: 360 loss: 2.0741727e-06
Iter: 361 loss: 2.0740074e-06
Iter: 362 loss: 2.07357402e-06
Iter: 363 loss: 2.07257244e-06
Iter: 364 loss: 2.07239145e-06
Iter: 365 loss: 2.07172752e-06
Iter: 366 loss: 2.07038579e-06
Iter: 367 loss: 2.07462836e-06
Iter: 368 loss: 2.06998402e-06
Iter: 369 loss: 2.06991308e-06
Iter: 370 loss: 2.06939421e-06
Iter: 371 loss: 2.06891696e-06
Iter: 372 loss: 2.06808886e-06
Iter: 373 loss: 2.08982851e-06
Iter: 374 loss: 2.06805908e-06
Iter: 375 loss: 2.06726781e-06
Iter: 376 loss: 2.06796017e-06
Iter: 377 loss: 2.06669938e-06
Iter: 378 loss: 2.06687309e-06
Iter: 379 loss: 2.06634149e-06
Iter: 380 loss: 2.06614686e-06
Iter: 381 loss: 2.06648019e-06
Iter: 382 loss: 2.06592676e-06
Iter: 383 loss: 2.06539085e-06
Iter: 384 loss: 2.06464551e-06
Iter: 385 loss: 2.06464847e-06
Iter: 386 loss: 2.06424693e-06
Iter: 387 loss: 2.06438403e-06
Iter: 388 loss: 2.06389518e-06
Iter: 389 loss: 2.06323671e-06
Iter: 390 loss: 2.06856794e-06
Iter: 391 loss: 2.06311438e-06
Iter: 392 loss: 2.06270829e-06
Iter: 393 loss: 2.06654067e-06
Iter: 394 loss: 2.06271375e-06
Iter: 395 loss: 2.06228447e-06
Iter: 396 loss: 2.06147388e-06
Iter: 397 loss: 2.07658809e-06
Iter: 398 loss: 2.06143727e-06
Iter: 399 loss: 2.060518e-06
Iter: 400 loss: 2.06477148e-06
Iter: 401 loss: 2.06041614e-06
Iter: 402 loss: 2.059689e-06
Iter: 403 loss: 2.05889137e-06
Iter: 404 loss: 2.05884885e-06
Iter: 405 loss: 2.05770402e-06
Iter: 406 loss: 2.06482309e-06
Iter: 407 loss: 2.05768174e-06
Iter: 408 loss: 2.05666e-06
Iter: 409 loss: 2.0686723e-06
Iter: 410 loss: 2.05665606e-06
Iter: 411 loss: 2.05615788e-06
Iter: 412 loss: 2.0549578e-06
Iter: 413 loss: 2.06354639e-06
Iter: 414 loss: 2.05472793e-06
Iter: 415 loss: 2.05594051e-06
Iter: 416 loss: 2.0543298e-06
Iter: 417 loss: 2.05396668e-06
Iter: 418 loss: 2.05324432e-06
Iter: 419 loss: 2.06949107e-06
Iter: 420 loss: 2.05322885e-06
Iter: 421 loss: 2.05251627e-06
Iter: 422 loss: 2.05321362e-06
Iter: 423 loss: 2.05221932e-06
Iter: 424 loss: 2.0515e-06
Iter: 425 loss: 2.0565808e-06
Iter: 426 loss: 2.05139986e-06
Iter: 427 loss: 2.05083256e-06
Iter: 428 loss: 2.05371475e-06
Iter: 429 loss: 2.05064316e-06
Iter: 430 loss: 2.05021479e-06
Iter: 431 loss: 2.05222113e-06
Iter: 432 loss: 2.0501277e-06
Iter: 433 loss: 2.04956314e-06
Iter: 434 loss: 2.04916728e-06
Iter: 435 loss: 2.04898038e-06
Iter: 436 loss: 2.04830167e-06
Iter: 437 loss: 2.04955904e-06
Iter: 438 loss: 2.04812886e-06
Iter: 439 loss: 2.04721823e-06
Iter: 440 loss: 2.05061e-06
Iter: 441 loss: 2.04698813e-06
Iter: 442 loss: 2.0464829e-06
Iter: 443 loss: 2.04609682e-06
Iter: 444 loss: 2.04594016e-06
Iter: 445 loss: 2.04541334e-06
Iter: 446 loss: 2.04532807e-06
Iter: 447 loss: 2.04490789e-06
Iter: 448 loss: 2.04408025e-06
Iter: 449 loss: 2.05935021e-06
Iter: 450 loss: 2.04404137e-06
Iter: 451 loss: 2.04450726e-06
Iter: 452 loss: 2.0438365e-06
Iter: 453 loss: 2.04353501e-06
Iter: 454 loss: 2.04296157e-06
Iter: 455 loss: 2.04819844e-06
Iter: 456 loss: 2.04288244e-06
Iter: 457 loss: 2.04213393e-06
Iter: 458 loss: 2.04325693e-06
Iter: 459 loss: 2.04185267e-06
Iter: 460 loss: 2.04153821e-06
Iter: 461 loss: 2.04089747e-06
Iter: 462 loss: 2.04083472e-06
Iter: 463 loss: 2.04007983e-06
Iter: 464 loss: 2.04812295e-06
Iter: 465 loss: 2.04005937e-06
Iter: 466 loss: 2.03960894e-06
Iter: 467 loss: 2.03881928e-06
Iter: 468 loss: 2.03898526e-06
Iter: 469 loss: 2.0380794e-06
Iter: 470 loss: 2.04385333e-06
Iter: 471 loss: 2.03792956e-06
Iter: 472 loss: 2.03728041e-06
Iter: 473 loss: 2.03832883e-06
Iter: 474 loss: 2.03699847e-06
Iter: 475 loss: 2.03623085e-06
Iter: 476 loss: 2.03790614e-06
Iter: 477 loss: 2.03599802e-06
Iter: 478 loss: 2.03526201e-06
Iter: 479 loss: 2.04094226e-06
Iter: 480 loss: 2.03520108e-06
Iter: 481 loss: 2.03463946e-06
Iter: 482 loss: 2.03501349e-06
Iter: 483 loss: 2.03416357e-06
Iter: 484 loss: 2.03381614e-06
Iter: 485 loss: 2.03376203e-06
Iter: 486 loss: 2.03331024e-06
Iter: 487 loss: 2.03390027e-06
Iter: 488 loss: 2.03314266e-06
Iter: 489 loss: 2.03269951e-06
Iter: 490 loss: 2.03302557e-06
Iter: 491 loss: 2.03226818e-06
Iter: 492 loss: 2.03186187e-06
Iter: 493 loss: 2.0315133e-06
Iter: 494 loss: 2.03144737e-06
Iter: 495 loss: 2.03123273e-06
Iter: 496 loss: 2.03114791e-06
Iter: 497 loss: 2.03084801e-06
Iter: 498 loss: 2.03057925e-06
Iter: 499 loss: 2.03051923e-06
Iter: 500 loss: 2.03003356e-06
Iter: 501 loss: 2.0325615e-06
Iter: 502 loss: 2.02998854e-06
Iter: 503 loss: 2.02960587e-06
Iter: 504 loss: 2.02926367e-06
Iter: 505 loss: 2.02913e-06
Iter: 506 loss: 2.02836327e-06
Iter: 507 loss: 2.03086893e-06
Iter: 508 loss: 2.02829438e-06
Iter: 509 loss: 2.02766614e-06
Iter: 510 loss: 2.02760293e-06
Iter: 511 loss: 2.02719025e-06
Iter: 512 loss: 2.02651881e-06
Iter: 513 loss: 2.02948831e-06
Iter: 514 loss: 2.02628939e-06
Iter: 515 loss: 2.02570186e-06
Iter: 516 loss: 2.03210743e-06
Iter: 517 loss: 2.0256457e-06
Iter: 518 loss: 2.0253849e-06
Iter: 519 loss: 2.02765295e-06
Iter: 520 loss: 2.02528486e-06
Iter: 521 loss: 2.02495949e-06
Iter: 522 loss: 2.025085e-06
Iter: 523 loss: 2.02460842e-06
Iter: 524 loss: 2.02434899e-06
Iter: 525 loss: 2.02388242e-06
Iter: 526 loss: 2.02383217e-06
Iter: 527 loss: 2.02308206e-06
Iter: 528 loss: 2.02840488e-06
Iter: 529 loss: 2.02314482e-06
Iter: 530 loss: 2.02271985e-06
Iter: 531 loss: 2.02221327e-06
Iter: 532 loss: 2.02218234e-06
Iter: 533 loss: 2.02162e-06
Iter: 534 loss: 2.02163801e-06
Iter: 535 loss: 2.02127535e-06
Iter: 536 loss: 2.02124966e-06
Iter: 537 loss: 2.02095021e-06
Iter: 538 loss: 2.02049887e-06
Iter: 539 loss: 2.02344154e-06
Iter: 540 loss: 2.02041929e-06
Iter: 541 loss: 2.01999501e-06
Iter: 542 loss: 2.01996522e-06
Iter: 543 loss: 2.01974717e-06
Iter: 544 loss: 2.0191228e-06
Iter: 545 loss: 2.01889088e-06
Iter: 546 loss: 2.01852481e-06
Iter: 547 loss: 2.01774537e-06
Iter: 548 loss: 2.02202114e-06
Iter: 549 loss: 2.01753483e-06
Iter: 550 loss: 2.01696389e-06
Iter: 551 loss: 2.02323872e-06
Iter: 552 loss: 2.01692137e-06
Iter: 553 loss: 2.01666035e-06
Iter: 554 loss: 2.01661442e-06
Iter: 555 loss: 2.01620833e-06
Iter: 556 loss: 2.01574403e-06
Iter: 557 loss: 2.01582498e-06
Iter: 558 loss: 2.0154198e-06
Iter: 559 loss: 2.01532339e-06
Iter: 560 loss: 2.01488297e-06
Iter: 561 loss: 2.01410148e-06
Iter: 562 loss: 2.01773719e-06
Iter: 563 loss: 2.01385456e-06
Iter: 564 loss: 2.01338753e-06
Iter: 565 loss: 2.01373268e-06
Iter: 566 loss: 2.01304101e-06
Iter: 567 loss: 2.01244461e-06
Iter: 568 loss: 2.02039382e-06
Iter: 569 loss: 2.012398e-06
Iter: 570 loss: 2.01203648e-06
Iter: 571 loss: 2.01173702e-06
Iter: 572 loss: 2.0114926e-06
Iter: 573 loss: 2.01073522e-06
Iter: 574 loss: 2.01244166e-06
Iter: 575 loss: 2.01037301e-06
Iter: 576 loss: 2.00978866e-06
Iter: 577 loss: 2.01060379e-06
Iter: 578 loss: 2.0094883e-06
Iter: 579 loss: 2.00875138e-06
Iter: 580 loss: 2.00968816e-06
Iter: 581 loss: 2.00834893e-06
Iter: 582 loss: 2.00772729e-06
Iter: 583 loss: 2.01040825e-06
Iter: 584 loss: 2.00750173e-06
Iter: 585 loss: 2.00721115e-06
Iter: 586 loss: 2.00706882e-06
Iter: 587 loss: 2.00674276e-06
Iter: 588 loss: 2.00834256e-06
Iter: 589 loss: 2.0065911e-06
Iter: 590 loss: 2.00640261e-06
Iter: 591 loss: 2.00599948e-06
Iter: 592 loss: 2.01560806e-06
Iter: 593 loss: 2.00596969e-06
Iter: 594 loss: 2.00557542e-06
Iter: 595 loss: 2.00970862e-06
Iter: 596 loss: 2.00552745e-06
Iter: 597 loss: 2.00511931e-06
Iter: 598 loss: 2.00474915e-06
Iter: 599 loss: 2.00458e-06
Iter: 600 loss: 2.00420186e-06
Iter: 601 loss: 2.00415593e-06
Iter: 602 loss: 2.00369868e-06
Iter: 603 loss: 2.00358681e-06
Iter: 604 loss: 2.00327668e-06
Iter: 605 loss: 2.00288878e-06
Iter: 606 loss: 2.00670593e-06
Iter: 607 loss: 2.00282648e-06
Iter: 608 loss: 2.00250952e-06
Iter: 609 loss: 2.0020284e-06
Iter: 610 loss: 2.0019761e-06
Iter: 611 loss: 2.0013722e-06
Iter: 612 loss: 2.00590534e-06
Iter: 613 loss: 2.00131626e-06
Iter: 614 loss: 2.00092154e-06
Iter: 615 loss: 2.00132354e-06
Iter: 616 loss: 2.00053773e-06
Iter: 617 loss: 2.00011755e-06
Iter: 618 loss: 2.00011254e-06
Iter: 619 loss: 1.99978808e-06
Iter: 620 loss: 2.00257728e-06
Iter: 621 loss: 1.99977626e-06
Iter: 622 loss: 1.99948727e-06
Iter: 623 loss: 1.9990714e-06
Iter: 624 loss: 2.00738941e-06
Iter: 625 loss: 1.99906071e-06
Iter: 626 loss: 1.9987192e-06
Iter: 627 loss: 2.0018806e-06
Iter: 628 loss: 1.99851866e-06
Iter: 629 loss: 1.99816031e-06
Iter: 630 loss: 1.99815145e-06
Iter: 631 loss: 1.99783176e-06
Iter: 632 loss: 1.99736019e-06
Iter: 633 loss: 2.00098839e-06
Iter: 634 loss: 1.99725241e-06
Iter: 635 loss: 1.99683313e-06
Iter: 636 loss: 1.99750048e-06
Iter: 637 loss: 1.99655392e-06
Iter: 638 loss: 1.9961517e-06
Iter: 639 loss: 1.99719261e-06
Iter: 640 loss: 1.99609781e-06
Iter: 641 loss: 1.99559599e-06
Iter: 642 loss: 1.99541046e-06
Iter: 643 loss: 1.99501324e-06
Iter: 644 loss: 1.99442843e-06
Iter: 645 loss: 1.99529359e-06
Iter: 646 loss: 1.99419e-06
Iter: 647 loss: 1.99332271e-06
Iter: 648 loss: 1.99414535e-06
Iter: 649 loss: 1.99281931e-06
Iter: 650 loss: 1.99225747e-06
Iter: 651 loss: 1.99222586e-06
Iter: 652 loss: 1.99172268e-06
Iter: 653 loss: 1.99421e-06
Iter: 654 loss: 1.99163128e-06
Iter: 655 loss: 1.99125225e-06
Iter: 656 loss: 1.99037731e-06
Iter: 657 loss: 1.99034662e-06
Iter: 658 loss: 1.98970565e-06
Iter: 659 loss: 1.9930626e-06
Iter: 660 loss: 1.9895981e-06
Iter: 661 loss: 1.98888051e-06
Iter: 662 loss: 1.98987209e-06
Iter: 663 loss: 1.98853581e-06
Iter: 664 loss: 1.98775115e-06
Iter: 665 loss: 1.99101146e-06
Iter: 666 loss: 1.98759676e-06
Iter: 667 loss: 1.98689941e-06
Iter: 668 loss: 1.98981252e-06
Iter: 669 loss: 1.98680141e-06
Iter: 670 loss: 1.98638895e-06
Iter: 671 loss: 1.98701287e-06
Iter: 672 loss: 1.98607813e-06
Iter: 673 loss: 1.9854308e-06
Iter: 674 loss: 1.98624366e-06
Iter: 675 loss: 1.98522298e-06
Iter: 676 loss: 1.9844797e-06
Iter: 677 loss: 1.98426915e-06
Iter: 678 loss: 1.98387693e-06
Iter: 679 loss: 1.9829572e-06
Iter: 680 loss: 1.98744237e-06
Iter: 681 loss: 1.98269254e-06
Iter: 682 loss: 1.98212319e-06
Iter: 683 loss: 1.98840144e-06
Iter: 684 loss: 1.98215889e-06
Iter: 685 loss: 1.98168209e-06
Iter: 686 loss: 1.98604494e-06
Iter: 687 loss: 1.98164889e-06
Iter: 688 loss: 1.98107296e-06
Iter: 689 loss: 1.98088946e-06
Iter: 690 loss: 1.98077578e-06
Iter: 691 loss: 1.98022849e-06
Iter: 692 loss: 1.98059774e-06
Iter: 693 loss: 1.98008411e-06
Iter: 694 loss: 1.97929921e-06
Iter: 695 loss: 1.9827346e-06
Iter: 696 loss: 1.97921077e-06
Iter: 697 loss: 1.97867325e-06
Iter: 698 loss: 1.97933241e-06
Iter: 699 loss: 1.97839427e-06
Iter: 700 loss: 1.97775535e-06
Iter: 701 loss: 1.98252496e-06
Iter: 702 loss: 1.97767395e-06
Iter: 703 loss: 1.97726195e-06
Iter: 704 loss: 1.97709664e-06
Iter: 705 loss: 1.97684631e-06
Iter: 706 loss: 1.97620079e-06
Iter: 707 loss: 1.97904774e-06
Iter: 708 loss: 1.97609756e-06
Iter: 709 loss: 1.97543068e-06
Iter: 710 loss: 1.97491158e-06
Iter: 711 loss: 1.97473901e-06
Iter: 712 loss: 1.97369491e-06
Iter: 713 loss: 1.97796589e-06
Iter: 714 loss: 1.97348186e-06
Iter: 715 loss: 1.97257532e-06
Iter: 716 loss: 1.97454733e-06
Iter: 717 loss: 1.97220743e-06
Iter: 718 loss: 1.97186455e-06
Iter: 719 loss: 1.97162899e-06
Iter: 720 loss: 1.9712329e-06
Iter: 721 loss: 1.97091663e-06
Iter: 722 loss: 1.97092277e-06
Iter: 723 loss: 1.97026657e-06
Iter: 724 loss: 1.96943188e-06
Iter: 725 loss: 1.96929864e-06
Iter: 726 loss: 1.96844121e-06
Iter: 727 loss: 1.96843439e-06
Iter: 728 loss: 1.96786209e-06
Iter: 729 loss: 1.96786e-06
Iter: 730 loss: 1.96731571e-06
Iter: 731 loss: 1.96658698e-06
Iter: 732 loss: 1.97524014e-06
Iter: 733 loss: 1.96664541e-06
Iter: 734 loss: 1.96619703e-06
Iter: 735 loss: 1.96564474e-06
Iter: 736 loss: 1.96556721e-06
Iter: 737 loss: 1.96484962e-06
Iter: 738 loss: 1.97129134e-06
Iter: 739 loss: 1.96485325e-06
Iter: 740 loss: 1.96421979e-06
Iter: 741 loss: 1.96394535e-06
Iter: 742 loss: 1.96362089e-06
Iter: 743 loss: 1.96282508e-06
Iter: 744 loss: 1.96553174e-06
Iter: 745 loss: 1.96274596e-06
Iter: 746 loss: 1.96207566e-06
Iter: 747 loss: 1.96329665e-06
Iter: 748 loss: 1.96174506e-06
Iter: 749 loss: 1.9617155e-06
Iter: 750 loss: 1.96144651e-06
Iter: 751 loss: 1.96121664e-06
Iter: 752 loss: 1.96088149e-06
Iter: 753 loss: 1.96079964e-06
Iter: 754 loss: 1.96034489e-06
Iter: 755 loss: 1.95996222e-06
Iter: 756 loss: 1.95989128e-06
Iter: 757 loss: 1.9593715e-06
Iter: 758 loss: 1.96588758e-06
Iter: 759 loss: 1.95934626e-06
Iter: 760 loss: 1.95888856e-06
Iter: 761 loss: 1.95844063e-06
Iter: 762 loss: 1.95842222e-06
Iter: 763 loss: 1.95780444e-06
Iter: 764 loss: 1.95783628e-06
Iter: 765 loss: 1.95738357e-06
Iter: 766 loss: 1.95682856e-06
Iter: 767 loss: 1.95682742e-06
Iter: 768 loss: 1.9561171e-06
Iter: 769 loss: 1.96057158e-06
Iter: 770 loss: 1.95601615e-06
Iter: 771 loss: 1.95532e-06
Iter: 772 loss: 1.95549478e-06
Iter: 773 loss: 1.95497614e-06
Iter: 774 loss: 1.95423854e-06
Iter: 775 loss: 1.95515827e-06
Iter: 776 loss: 1.95396979e-06
Iter: 777 loss: 1.95304483e-06
Iter: 778 loss: 1.95431221e-06
Iter: 779 loss: 1.95279426e-06
Iter: 780 loss: 1.95259759e-06
Iter: 781 loss: 1.95237931e-06
Iter: 782 loss: 1.95198504e-06
Iter: 783 loss: 1.95167559e-06
Iter: 784 loss: 1.95157963e-06
Iter: 785 loss: 1.95092684e-06
Iter: 786 loss: 1.95036273e-06
Iter: 787 loss: 1.95022722e-06
Iter: 788 loss: 1.94945619e-06
Iter: 789 loss: 1.95238795e-06
Iter: 790 loss: 1.94936774e-06
Iter: 791 loss: 1.94841459e-06
Iter: 792 loss: 1.95046255e-06
Iter: 793 loss: 1.94804943e-06
Iter: 794 loss: 1.94755648e-06
Iter: 795 loss: 1.95320854e-06
Iter: 796 loss: 1.94753102e-06
Iter: 797 loss: 1.94693962e-06
Iter: 798 loss: 1.94683616e-06
Iter: 799 loss: 1.94634936e-06
Iter: 800 loss: 1.9458239e-06
Iter: 801 loss: 1.9473473e-06
Iter: 802 loss: 1.94562335e-06
Iter: 803 loss: 1.9447582e-06
Iter: 804 loss: 1.94671156e-06
Iter: 805 loss: 1.94450649e-06
Iter: 806 loss: 1.94382937e-06
Iter: 807 loss: 1.94390282e-06
Iter: 808 loss: 1.94326572e-06
Iter: 809 loss: 1.94227414e-06
Iter: 810 loss: 1.94432414e-06
Iter: 811 loss: 1.94172844e-06
Iter: 812 loss: 1.94068843e-06
Iter: 813 loss: 1.94712129e-06
Iter: 814 loss: 1.94065296e-06
Iter: 815 loss: 1.93984988e-06
Iter: 816 loss: 1.95349662e-06
Iter: 817 loss: 1.93977849e-06
Iter: 818 loss: 1.93947039e-06
Iter: 819 loss: 1.93889855e-06
Iter: 820 loss: 1.9388076e-06
Iter: 821 loss: 1.93814026e-06
Iter: 822 loss: 1.93782739e-06
Iter: 823 loss: 1.93739334e-06
Iter: 824 loss: 1.93681649e-06
Iter: 825 loss: 1.93680489e-06
Iter: 826 loss: 1.93625e-06
Iter: 827 loss: 1.93566507e-06
Iter: 828 loss: 1.93549067e-06
Iter: 829 loss: 1.93487813e-06
Iter: 830 loss: 1.93486608e-06
Iter: 831 loss: 1.93446067e-06
Iter: 832 loss: 1.93367032e-06
Iter: 833 loss: 1.94853806e-06
Iter: 834 loss: 1.93370943e-06
Iter: 835 loss: 1.93295091e-06
Iter: 836 loss: 1.94233508e-06
Iter: 837 loss: 1.93292863e-06
Iter: 838 loss: 1.93247274e-06
Iter: 839 loss: 1.93210758e-06
Iter: 840 loss: 1.93183314e-06
Iter: 841 loss: 1.93090386e-06
Iter: 842 loss: 1.93237111e-06
Iter: 843 loss: 1.93055644e-06
Iter: 844 loss: 1.92967809e-06
Iter: 845 loss: 1.93034612e-06
Iter: 846 loss: 1.92908465e-06
Iter: 847 loss: 1.92867105e-06
Iter: 848 loss: 1.92859852e-06
Iter: 849 loss: 1.9277993e-06
Iter: 850 loss: 1.92881453e-06
Iter: 851 loss: 1.92758762e-06
Iter: 852 loss: 1.92715402e-06
Iter: 853 loss: 1.9267477e-06
Iter: 854 loss: 1.92657239e-06
Iter: 855 loss: 1.92567177e-06
Iter: 856 loss: 1.92700463e-06
Iter: 857 loss: 1.92527887e-06
Iter: 858 loss: 1.9244676e-06
Iter: 859 loss: 1.93215556e-06
Iter: 860 loss: 1.92444145e-06
Iter: 861 loss: 1.9238289e-06
Iter: 862 loss: 1.92360449e-06
Iter: 863 loss: 1.92327775e-06
Iter: 864 loss: 1.92263224e-06
Iter: 865 loss: 1.92257221e-06
Iter: 866 loss: 1.92210223e-06
Iter: 867 loss: 1.92137077e-06
Iter: 868 loss: 1.93257074e-06
Iter: 869 loss: 1.92128891e-06
Iter: 870 loss: 1.92055813e-06
Iter: 871 loss: 1.92053903e-06
Iter: 872 loss: 1.9199756e-06
Iter: 873 loss: 1.91911545e-06
Iter: 874 loss: 1.91912227e-06
Iter: 875 loss: 1.91824302e-06
Iter: 876 loss: 1.92395441e-06
Iter: 877 loss: 1.91817207e-06
Iter: 878 loss: 1.91746585e-06
Iter: 879 loss: 1.91738354e-06
Iter: 880 loss: 1.91682739e-06
Iter: 881 loss: 1.9169695e-06
Iter: 882 loss: 1.91657955e-06
Iter: 883 loss: 1.91618278e-06
Iter: 884 loss: 1.91579238e-06
Iter: 885 loss: 1.91571894e-06
Iter: 886 loss: 1.91532604e-06
Iter: 887 loss: 1.91521485e-06
Iter: 888 loss: 1.91492154e-06
Iter: 889 loss: 1.91444883e-06
Iter: 890 loss: 1.91641584e-06
Iter: 891 loss: 1.91435629e-06
Iter: 892 loss: 1.9138256e-06
Iter: 893 loss: 1.91445088e-06
Iter: 894 loss: 1.91343452e-06
Iter: 895 loss: 1.91285199e-06
Iter: 896 loss: 1.9171539e-06
Iter: 897 loss: 1.91284744e-06
Iter: 898 loss: 1.91245977e-06
Iter: 899 loss: 1.91256436e-06
Iter: 900 loss: 1.91225126e-06
Iter: 901 loss: 1.91175332e-06
Iter: 902 loss: 1.91304139e-06
Iter: 903 loss: 1.91147569e-06
Iter: 904 loss: 1.91101162e-06
Iter: 905 loss: 1.91067079e-06
Iter: 906 loss: 1.91055028e-06
Iter: 907 loss: 1.90995138e-06
Iter: 908 loss: 1.91491381e-06
Iter: 909 loss: 1.90994183e-06
Iter: 910 loss: 1.90936453e-06
Iter: 911 loss: 1.91028789e-06
Iter: 912 loss: 1.90918649e-06
Iter: 913 loss: 1.90842e-06
Iter: 914 loss: 1.91183972e-06
Iter: 915 loss: 1.90829405e-06
Iter: 916 loss: 1.90797664e-06
Iter: 917 loss: 1.91287972e-06
Iter: 918 loss: 1.90793298e-06
Iter: 919 loss: 1.90751166e-06
Iter: 920 loss: 1.90668356e-06
Iter: 921 loss: 1.92181506e-06
Iter: 922 loss: 1.90666333e-06
Iter: 923 loss: 1.906026e-06
Iter: 924 loss: 1.90616038e-06
Iter: 925 loss: 1.90553055e-06
Iter: 926 loss: 1.90496166e-06
Iter: 927 loss: 1.91547269e-06
Iter: 928 loss: 1.90493972e-06
Iter: 929 loss: 1.90435946e-06
Iter: 930 loss: 1.90526725e-06
Iter: 931 loss: 1.90422452e-06
Iter: 932 loss: 1.90369747e-06
Iter: 933 loss: 1.90815695e-06
Iter: 934 loss: 1.90366313e-06
Iter: 935 loss: 1.90326136e-06
Iter: 936 loss: 1.9025606e-06
Iter: 937 loss: 1.90251194e-06
Iter: 938 loss: 1.90174035e-06
Iter: 939 loss: 1.9022691e-06
Iter: 940 loss: 1.90116725e-06
Iter: 941 loss: 1.90081391e-06
Iter: 942 loss: 1.90065725e-06
Iter: 943 loss: 1.90017772e-06
Iter: 944 loss: 1.90062565e-06
Iter: 945 loss: 1.89988725e-06
Iter: 946 loss: 1.89921366e-06
Iter: 947 loss: 1.89990271e-06
Iter: 948 loss: 1.89883531e-06
Iter: 949 loss: 1.89842558e-06
Iter: 950 loss: 1.89846116e-06
Iter: 951 loss: 1.89785646e-06
Iter: 952 loss: 1.89741081e-06
Iter: 953 loss: 1.8971798e-06
Iter: 954 loss: 1.89662865e-06
Iter: 955 loss: 1.9000297e-06
Iter: 956 loss: 1.89665298e-06
Iter: 957 loss: 1.89618095e-06
Iter: 958 loss: 1.89518505e-06
Iter: 959 loss: 1.91256049e-06
Iter: 960 loss: 1.89517959e-06
Iter: 961 loss: 1.89433786e-06
Iter: 962 loss: 1.89692878e-06
Iter: 963 loss: 1.89402363e-06
Iter: 964 loss: 1.89380967e-06
Iter: 965 loss: 1.89351556e-06
Iter: 966 loss: 1.89318678e-06
Iter: 967 loss: 1.8925715e-06
Iter: 968 loss: 1.90638968e-06
Iter: 969 loss: 1.89250682e-06
Iter: 970 loss: 1.89199181e-06
Iter: 971 loss: 1.89489867e-06
Iter: 972 loss: 1.89183811e-06
Iter: 973 loss: 1.89127513e-06
Iter: 974 loss: 1.89280331e-06
Iter: 975 loss: 1.89114076e-06
Iter: 976 loss: 1.89062541e-06
Iter: 977 loss: 1.89130628e-06
Iter: 978 loss: 1.890372e-06
Iter: 979 loss: 1.88984131e-06
Iter: 980 loss: 1.89445609e-06
Iter: 981 loss: 1.88972263e-06
Iter: 982 loss: 1.88916476e-06
Iter: 983 loss: 1.88951196e-06
Iter: 984 loss: 1.88879483e-06
Iter: 985 loss: 1.88846e-06
Iter: 986 loss: 1.88845684e-06
Iter: 987 loss: 1.88810588e-06
Iter: 988 loss: 1.88747276e-06
Iter: 989 loss: 1.88752915e-06
Iter: 990 loss: 1.88710919e-06
Iter: 991 loss: 1.89082834e-06
Iter: 992 loss: 1.88702711e-06
Iter: 993 loss: 1.88664512e-06
Iter: 994 loss: 1.88588422e-06
Iter: 995 loss: 1.89896241e-06
Iter: 996 loss: 1.88582385e-06
Iter: 997 loss: 1.88525769e-06
Iter: 998 loss: 1.89002162e-06
Iter: 999 loss: 1.88515537e-06
Iter: 1000 loss: 1.8847727e-06
Iter: 1001 loss: 1.88469e-06
Iter: 1002 loss: 1.88443437e-06
Iter: 1003 loss: 1.88380159e-06
Iter: 1004 loss: 1.89267371e-06
Iter: 1005 loss: 1.88371462e-06
Iter: 1006 loss: 1.88331819e-06
Iter: 1007 loss: 1.88805677e-06
Iter: 1008 loss: 1.88322269e-06
Iter: 1009 loss: 1.88270155e-06
Iter: 1010 loss: 1.8835234e-06
Iter: 1011 loss: 1.88253318e-06
Iter: 1012 loss: 1.88210959e-06
Iter: 1013 loss: 1.88458432e-06
Iter: 1014 loss: 1.8819261e-06
Iter: 1015 loss: 1.88161914e-06
Iter: 1016 loss: 1.88375486e-06
Iter: 1017 loss: 1.88161903e-06
Iter: 1018 loss: 1.88128774e-06
Iter: 1019 loss: 1.88179592e-06
Iter: 1020 loss: 1.88110016e-06
Iter: 1021 loss: 1.88073398e-06
Iter: 1022 loss: 1.88073841e-06
Iter: 1023 loss: 1.88059732e-06
Iter: 1024 loss: 1.88025956e-06
Iter: 1025 loss: 1.88577133e-06
Iter: 1026 loss: 1.88022273e-06
Iter: 1027 loss: 1.87973455e-06
Iter: 1028 loss: 1.88285048e-06
Iter: 1029 loss: 1.87975752e-06
Iter: 1030 loss: 1.87948933e-06
Iter: 1031 loss: 1.87890032e-06
Iter: 1032 loss: 1.88951492e-06
Iter: 1033 loss: 1.87903311e-06
Iter: 1034 loss: 1.87879778e-06
Iter: 1035 loss: 1.87857052e-06
Iter: 1036 loss: 1.87833393e-06
Iter: 1037 loss: 1.87811042e-06
Iter: 1038 loss: 1.87801083e-06
Iter: 1039 loss: 1.87759758e-06
Iter: 1040 loss: 1.87744672e-06
Iter: 1041 loss: 1.87714795e-06
Iter: 1042 loss: 1.87684054e-06
Iter: 1043 loss: 1.88315937e-06
Iter: 1044 loss: 1.87682792e-06
Iter: 1045 loss: 1.87647925e-06
Iter: 1046 loss: 1.87601336e-06
Iter: 1047 loss: 1.88940635e-06
Iter: 1048 loss: 1.87604712e-06
Iter: 1049 loss: 1.87547653e-06
Iter: 1050 loss: 1.8754389e-06
Iter: 1051 loss: 1.87518629e-06
Iter: 1052 loss: 1.87498654e-06
Iter: 1053 loss: 1.87488126e-06
Iter: 1054 loss: 1.87459159e-06
Iter: 1055 loss: 1.87454259e-06
Iter: 1056 loss: 1.87427952e-06
Iter: 1057 loss: 1.87452179e-06
Iter: 1058 loss: 1.87414889e-06
Iter: 1059 loss: 1.87388741e-06
Iter: 1060 loss: 1.87369051e-06
Iter: 1061 loss: 1.87359933e-06
Iter: 1062 loss: 1.87299281e-06
Iter: 1063 loss: 1.87631895e-06
Iter: 1064 loss: 1.87289265e-06
Iter: 1065 loss: 1.87241653e-06
Iter: 1066 loss: 1.87204728e-06
Iter: 1067 loss: 1.87188982e-06
Iter: 1068 loss: 1.87166893e-06
Iter: 1069 loss: 1.87159458e-06
Iter: 1070 loss: 1.87120668e-06
Iter: 1071 loss: 1.87053479e-06
Iter: 1072 loss: 1.88438821e-06
Iter: 1073 loss: 1.87049227e-06
Iter: 1074 loss: 1.87007686e-06
Iter: 1075 loss: 1.87124988e-06
Iter: 1076 loss: 1.86980049e-06
Iter: 1077 loss: 1.86900115e-06
Iter: 1078 loss: 1.87148555e-06
Iter: 1079 loss: 1.86891691e-06
Iter: 1080 loss: 1.86813361e-06
Iter: 1081 loss: 1.87115552e-06
Iter: 1082 loss: 1.86805573e-06
Iter: 1083 loss: 1.86738976e-06
Iter: 1084 loss: 1.8680729e-06
Iter: 1085 loss: 1.86694979e-06
Iter: 1086 loss: 1.86650891e-06
Iter: 1087 loss: 1.86671321e-06
Iter: 1088 loss: 1.86616353e-06
Iter: 1089 loss: 1.86593365e-06
Iter: 1090 loss: 1.86571151e-06
Iter: 1091 loss: 1.8655245e-06
Iter: 1092 loss: 1.86496061e-06
Iter: 1093 loss: 1.86687657e-06
Iter: 1094 loss: 1.86472562e-06
Iter: 1095 loss: 1.86422835e-06
Iter: 1096 loss: 1.86412296e-06
Iter: 1097 loss: 1.86371642e-06
Iter: 1098 loss: 1.86300326e-06
Iter: 1099 loss: 1.86291982e-06
Iter: 1100 loss: 1.86258796e-06
Iter: 1101 loss: 1.86854925e-06
Iter: 1102 loss: 1.86260172e-06
Iter: 1103 loss: 1.86203943e-06
Iter: 1104 loss: 1.86301963e-06
Iter: 1105 loss: 1.86186458e-06
Iter: 1106 loss: 1.86138686e-06
Iter: 1107 loss: 1.86083957e-06
Iter: 1108 loss: 1.86079956e-06
Iter: 1109 loss: 1.86021066e-06
Iter: 1110 loss: 1.86029e-06
Iter: 1111 loss: 1.85975694e-06
Iter: 1112 loss: 1.85900649e-06
Iter: 1113 loss: 1.86269835e-06
Iter: 1114 loss: 1.85882607e-06
Iter: 1115 loss: 1.85822682e-06
Iter: 1116 loss: 1.86287116e-06
Iter: 1117 loss: 1.8583371e-06
Iter: 1118 loss: 1.85769545e-06
Iter: 1119 loss: 1.85885574e-06
Iter: 1120 loss: 1.85745239e-06
Iter: 1121 loss: 1.85712611e-06
Iter: 1122 loss: 1.86039551e-06
Iter: 1123 loss: 1.85714316e-06
Iter: 1124 loss: 1.85663839e-06
Iter: 1125 loss: 1.85691579e-06
Iter: 1126 loss: 1.85639851e-06
Iter: 1127 loss: 1.85611725e-06
Iter: 1128 loss: 1.85552653e-06
Iter: 1129 loss: 1.85554836e-06
Iter: 1130 loss: 1.85508804e-06
Iter: 1131 loss: 1.8622718e-06
Iter: 1132 loss: 1.85500653e-06
Iter: 1133 loss: 1.85468491e-06
Iter: 1134 loss: 1.85388353e-06
Iter: 1135 loss: 1.87307194e-06
Iter: 1136 loss: 1.85393321e-06
Iter: 1137 loss: 1.85329361e-06
Iter: 1138 loss: 1.85326189e-06
Iter: 1139 loss: 1.8527511e-06
Iter: 1140 loss: 1.85251031e-06
Iter: 1141 loss: 1.85215868e-06
Iter: 1142 loss: 1.85159263e-06
Iter: 1143 loss: 1.8545104e-06
Iter: 1144 loss: 1.85138197e-06
Iter: 1145 loss: 1.85083297e-06
Iter: 1146 loss: 1.850285e-06
Iter: 1147 loss: 1.8502667e-06
Iter: 1148 loss: 1.84960049e-06
Iter: 1149 loss: 1.84959879e-06
Iter: 1150 loss: 1.84920168e-06
Iter: 1151 loss: 1.85053409e-06
Iter: 1152 loss: 1.84913938e-06
Iter: 1153 loss: 1.84877558e-06
Iter: 1154 loss: 1.85014744e-06
Iter: 1155 loss: 1.84868281e-06
Iter: 1156 loss: 1.8484069e-06
Iter: 1157 loss: 1.85115232e-06
Iter: 1158 loss: 1.84836642e-06
Iter: 1159 loss: 1.84814621e-06
Iter: 1160 loss: 1.84772841e-06
Iter: 1161 loss: 1.85204283e-06
Iter: 1162 loss: 1.84769897e-06
Iter: 1163 loss: 1.84729663e-06
Iter: 1164 loss: 1.85253896e-06
Iter: 1165 loss: 1.84727344e-06
Iter: 1166 loss: 1.84696921e-06
Iter: 1167 loss: 1.84663406e-06
Iter: 1168 loss: 1.8464948e-06
Iter: 1169 loss: 1.84596502e-06
Iter: 1170 loss: 1.84603232e-06
Iter: 1171 loss: 1.84558598e-06
Iter: 1172 loss: 1.84464193e-06
Iter: 1173 loss: 1.85410727e-06
Iter: 1174 loss: 1.84469127e-06
Iter: 1175 loss: 1.84418718e-06
Iter: 1176 loss: 1.84427427e-06
Iter: 1177 loss: 1.843723e-06
Iter: 1178 loss: 1.84317378e-06
Iter: 1179 loss: 1.84322141e-06
Iter: 1180 loss: 1.84251667e-06
Iter: 1181 loss: 1.84186399e-06
Iter: 1182 loss: 1.84380644e-06
Iter: 1183 loss: 1.84176065e-06
Iter: 1184 loss: 1.84108728e-06
Iter: 1185 loss: 1.8484659e-06
Iter: 1186 loss: 1.84106318e-06
Iter: 1187 loss: 1.84070245e-06
Iter: 1188 loss: 1.8417727e-06
Iter: 1189 loss: 1.84047428e-06
Iter: 1190 loss: 1.84003397e-06
Iter: 1191 loss: 1.84326109e-06
Iter: 1192 loss: 1.83997042e-06
Iter: 1193 loss: 1.83956695e-06
Iter: 1194 loss: 1.83895577e-06
Iter: 1195 loss: 1.85365855e-06
Iter: 1196 loss: 1.8389527e-06
Iter: 1197 loss: 1.83837517e-06
Iter: 1198 loss: 1.84027465e-06
Iter: 1199 loss: 1.83810312e-06
Iter: 1200 loss: 1.83752513e-06
Iter: 1201 loss: 1.8403e-06
Iter: 1202 loss: 1.83738246e-06
Iter: 1203 loss: 1.83680163e-06
Iter: 1204 loss: 1.836092e-06
Iter: 1205 loss: 1.83607528e-06
Iter: 1206 loss: 1.83564657e-06
Iter: 1207 loss: 1.8356875e-06
Iter: 1208 loss: 1.83514442e-06
Iter: 1209 loss: 1.83474515e-06
Iter: 1210 loss: 1.83457701e-06
Iter: 1211 loss: 1.83395571e-06
Iter: 1212 loss: 1.83664463e-06
Iter: 1213 loss: 1.83378415e-06
Iter: 1214 loss: 1.8333119e-06
Iter: 1215 loss: 1.83295629e-06
Iter: 1216 loss: 1.8328966e-06
Iter: 1217 loss: 1.83224063e-06
Iter: 1218 loss: 1.83884936e-06
Iter: 1219 loss: 1.83224199e-06
Iter: 1220 loss: 1.83181965e-06
Iter: 1221 loss: 1.83462544e-06
Iter: 1222 loss: 1.83181351e-06
Iter: 1223 loss: 1.83144755e-06
Iter: 1224 loss: 1.835425e-06
Iter: 1225 loss: 1.83148927e-06
Iter: 1226 loss: 1.8312262e-06
Iter: 1227 loss: 1.83095358e-06
Iter: 1228 loss: 1.83100678e-06
Iter: 1229 loss: 1.83050406e-06
Iter: 1230 loss: 1.83006387e-06
Iter: 1231 loss: 1.82999588e-06
Iter: 1232 loss: 1.82974418e-06
Iter: 1233 loss: 1.82969302e-06
Iter: 1234 loss: 1.82931194e-06
Iter: 1235 loss: 1.82922133e-06
Iter: 1236 loss: 1.82901317e-06
Iter: 1237 loss: 1.82854046e-06
Iter: 1238 loss: 1.82864471e-06
Iter: 1239 loss: 1.82829729e-06
Iter: 1240 loss: 1.82790518e-06
Iter: 1241 loss: 1.83101281e-06
Iter: 1242 loss: 1.82788824e-06
Iter: 1243 loss: 1.8276362e-06
Iter: 1244 loss: 1.82755878e-06
Iter: 1245 loss: 1.82735766e-06
Iter: 1246 loss: 1.82713484e-06
Iter: 1247 loss: 1.82721385e-06
Iter: 1248 loss: 1.82687802e-06
Iter: 1249 loss: 1.82787858e-06
Iter: 1250 loss: 1.82672954e-06
Iter: 1251 loss: 1.82644271e-06
Iter: 1252 loss: 1.82634972e-06
Iter: 1253 loss: 1.82620329e-06
Iter: 1254 loss: 1.8259251e-06
Iter: 1255 loss: 1.82593431e-06
Iter: 1256 loss: 1.82581778e-06
Iter: 1257 loss: 1.8264775e-06
Iter: 1258 loss: 1.8257806e-06
Iter: 1259 loss: 1.82556369e-06
Iter: 1260 loss: 1.82660892e-06
Iter: 1261 loss: 1.8255605e-06
Iter: 1262 loss: 1.82546569e-06
Iter: 1263 loss: 1.82520193e-06
Iter: 1264 loss: 1.82992392e-06
Iter: 1265 loss: 1.82518579e-06
Iter: 1266 loss: 1.82498388e-06
Iter: 1267 loss: 1.82572501e-06
Iter: 1268 loss: 1.82499275e-06
Iter: 1269 loss: 1.8247249e-06
Iter: 1270 loss: 1.82574513e-06
Iter: 1271 loss: 1.82468114e-06
Iter: 1272 loss: 1.82454505e-06
Iter: 1273 loss: 1.82424583e-06
Iter: 1274 loss: 1.82424901e-06
Iter: 1275 loss: 1.82413851e-06
Iter: 1276 loss: 1.82407416e-06
Iter: 1277 loss: 1.82387043e-06
Iter: 1278 loss: 1.82366648e-06
Iter: 1279 loss: 1.82360782e-06
Iter: 1280 loss: 1.82346866e-06
Iter: 1281 loss: 1.82347299e-06
Iter: 1282 loss: 1.82329632e-06
Iter: 1283 loss: 1.82304575e-06
Iter: 1284 loss: 1.82635438e-06
Iter: 1285 loss: 1.82297072e-06
Iter: 1286 loss: 1.82282542e-06
Iter: 1287 loss: 1.82274061e-06
Iter: 1288 loss: 1.82264625e-06
Iter: 1289 loss: 1.82255098e-06
Iter: 1290 loss: 1.82253484e-06
Iter: 1291 loss: 1.82239364e-06
Iter: 1292 loss: 1.82271094e-06
Iter: 1293 loss: 1.82238477e-06
Iter: 1294 loss: 1.82225278e-06
Iter: 1295 loss: 1.82212352e-06
Iter: 1296 loss: 1.82211966e-06
Iter: 1297 loss: 1.82187455e-06
Iter: 1298 loss: 1.82244935e-06
Iter: 1299 loss: 1.82192e-06
Iter: 1300 loss: 1.82171027e-06
Iter: 1301 loss: 1.82200233e-06
Iter: 1302 loss: 1.82165434e-06
Iter: 1303 loss: 1.82155895e-06
Iter: 1304 loss: 1.82142674e-06
Iter: 1305 loss: 1.82132953e-06
Iter: 1306 loss: 1.82115173e-06
Iter: 1307 loss: 1.82234976e-06
Iter: 1308 loss: 1.82116821e-06
Iter: 1309 loss: 1.82102872e-06
Iter: 1310 loss: 1.82100757e-06
Iter: 1311 loss: 1.82093299e-06
Iter: 1312 loss: 1.82063252e-06
Iter: 1313 loss: 1.82275699e-06
Iter: 1314 loss: 1.82069437e-06
Iter: 1315 loss: 1.8205086e-06
Iter: 1316 loss: 1.82097187e-06
Iter: 1317 loss: 1.82046961e-06
Iter: 1318 loss: 1.82018084e-06
Iter: 1319 loss: 1.82202723e-06
Iter: 1320 loss: 1.8201597e-06
Iter: 1321 loss: 1.82000531e-06
Iter: 1322 loss: 1.81965549e-06
Iter: 1323 loss: 1.82426561e-06
Iter: 1324 loss: 1.81964833e-06
Iter: 1325 loss: 1.81997655e-06
Iter: 1326 loss: 1.81949486e-06
Iter: 1327 loss: 1.81939959e-06
Iter: 1328 loss: 1.8191688e-06
Iter: 1329 loss: 1.82423469e-06
Iter: 1330 loss: 1.81916437e-06
Iter: 1331 loss: 1.81897576e-06
Iter: 1332 loss: 1.81946154e-06
Iter: 1333 loss: 1.8187734e-06
Iter: 1334 loss: 1.8185051e-06
Iter: 1335 loss: 1.82021552e-06
Iter: 1336 loss: 1.81840301e-06
Iter: 1337 loss: 1.81824851e-06
Iter: 1338 loss: 1.81845678e-06
Iter: 1339 loss: 1.81812e-06
Iter: 1340 loss: 1.81794894e-06
Iter: 1341 loss: 1.81793257e-06
Iter: 1342 loss: 1.81780126e-06
Iter: 1343 loss: 1.81774533e-06
Iter: 1344 loss: 1.81763812e-06
Iter: 1345 loss: 1.81739574e-06
Iter: 1346 loss: 1.81831285e-06
Iter: 1347 loss: 1.81740222e-06
Iter: 1348 loss: 1.81719588e-06
Iter: 1349 loss: 1.81910514e-06
Iter: 1350 loss: 1.81716302e-06
Iter: 1351 loss: 1.81710789e-06
Iter: 1352 loss: 1.81697658e-06
Iter: 1353 loss: 1.81771293e-06
Iter: 1354 loss: 1.81681844e-06
Iter: 1355 loss: 1.81665291e-06
Iter: 1356 loss: 1.81654809e-06
Iter: 1357 loss: 1.81648591e-06
Iter: 1358 loss: 1.81696043e-06
Iter: 1359 loss: 1.81650034e-06
Iter: 1360 loss: 1.81635085e-06
Iter: 1361 loss: 1.81674739e-06
Iter: 1362 loss: 1.81629116e-06
Iter: 1363 loss: 1.81624875e-06
Iter: 1364 loss: 1.81613768e-06
Iter: 1365 loss: 1.8161428e-06
Iter: 1366 loss: 1.8158396e-06
Iter: 1367 loss: 1.81588962e-06
Iter: 1368 loss: 1.815676e-06
Iter: 1369 loss: 1.81551604e-06
Iter: 1370 loss: 1.81610801e-06
Iter: 1371 loss: 1.81533585e-06
Iter: 1372 loss: 1.81523228e-06
Iter: 1373 loss: 1.81523433e-06
Iter: 1374 loss: 1.81508835e-06
Iter: 1375 loss: 1.8149517e-06
Iter: 1376 loss: 1.81497285e-06
Iter: 1377 loss: 1.81476162e-06
Iter: 1378 loss: 1.81598944e-06
Iter: 1379 loss: 1.81466453e-06
Iter: 1380 loss: 1.81446114e-06
Iter: 1381 loss: 1.81628923e-06
Iter: 1382 loss: 1.81442147e-06
Iter: 1383 loss: 1.8143719e-06
Iter: 1384 loss: 1.81463304e-06
Iter: 1385 loss: 1.81428072e-06
Iter: 1386 loss: 1.8141601e-06
Iter: 1387 loss: 1.8142207e-06
Iter: 1388 loss: 1.81402584e-06
Iter: 1389 loss: 1.81385224e-06
Iter: 1390 loss: 1.81365294e-06
Iter: 1391 loss: 1.81360144e-06
Iter: 1392 loss: 1.81368557e-06
Iter: 1393 loss: 1.81349799e-06
Iter: 1394 loss: 1.81347275e-06
Iter: 1395 loss: 1.81335304e-06
Iter: 1396 loss: 1.81418989e-06
Iter: 1397 loss: 1.81320127e-06
Iter: 1398 loss: 1.81297742e-06
Iter: 1399 loss: 1.81390033e-06
Iter: 1400 loss: 1.8129299e-06
Iter: 1401 loss: 1.81271389e-06
Iter: 1402 loss: 1.81250061e-06
Iter: 1403 loss: 1.81244025e-06
Iter: 1404 loss: 1.81227574e-06
Iter: 1405 loss: 1.81332518e-06
Iter: 1406 loss: 1.81223913e-06
Iter: 1407 loss: 1.81208918e-06
Iter: 1408 loss: 1.81296173e-06
Iter: 1409 loss: 1.81209464e-06
Iter: 1410 loss: 1.81197083e-06
Iter: 1411 loss: 1.81183054e-06
Iter: 1412 loss: 1.81180815e-06
Iter: 1413 loss: 1.81163e-06
Iter: 1414 loss: 1.81210567e-06
Iter: 1415 loss: 1.81153951e-06
Iter: 1416 loss: 1.81143798e-06
Iter: 1417 loss: 1.81130576e-06
Iter: 1418 loss: 1.81125415e-06
Iter: 1419 loss: 1.81112728e-06
Iter: 1420 loss: 1.81288328e-06
Iter: 1421 loss: 1.8110884e-06
Iter: 1422 loss: 1.81103496e-06
Iter: 1423 loss: 1.81090093e-06
Iter: 1424 loss: 1.81085966e-06
Iter: 1425 loss: 1.8107487e-06
Iter: 1426 loss: 1.81068526e-06
Iter: 1427 loss: 1.81054429e-06
Iter: 1428 loss: 1.81070754e-06
Iter: 1429 loss: 1.81045812e-06
Iter: 1430 loss: 1.81036148e-06
Iter: 1431 loss: 1.81023404e-06
Iter: 1432 loss: 1.81023006e-06
Iter: 1433 loss: 1.81013365e-06
Iter: 1434 loss: 1.81211612e-06
Iter: 1435 loss: 1.8101573e-06
Iter: 1436 loss: 1.81004248e-06
Iter: 1437 loss: 1.80977872e-06
Iter: 1438 loss: 1.81262692e-06
Iter: 1439 loss: 1.80967379e-06
Iter: 1440 loss: 1.80950144e-06
Iter: 1441 loss: 1.80957659e-06
Iter: 1442 loss: 1.80938514e-06
Iter: 1443 loss: 1.80946608e-06
Iter: 1444 loss: 1.80923053e-06
Iter: 1445 loss: 1.80901372e-06
Iter: 1446 loss: 1.8088374e-06
Iter: 1447 loss: 1.80877521e-06
Iter: 1448 loss: 1.80863094e-06
Iter: 1449 loss: 1.80865732e-06
Iter: 1450 loss: 1.80843381e-06
Iter: 1451 loss: 1.80821348e-06
Iter: 1452 loss: 1.80825884e-06
Iter: 1453 loss: 1.8079777e-06
Iter: 1454 loss: 1.81094049e-06
Iter: 1455 loss: 1.80787538e-06
Iter: 1456 loss: 1.80770064e-06
Iter: 1457 loss: 1.8096805e-06
Iter: 1458 loss: 1.80766e-06
Iter: 1459 loss: 1.80748179e-06
Iter: 1460 loss: 1.80801533e-06
Iter: 1461 loss: 1.80729251e-06
Iter: 1462 loss: 1.80707423e-06
Iter: 1463 loss: 1.80681036e-06
Iter: 1464 loss: 1.81443863e-06
Iter: 1465 loss: 1.80681729e-06
Iter: 1466 loss: 1.80640518e-06
Iter: 1467 loss: 1.80722736e-06
Iter: 1468 loss: 1.80621873e-06
Iter: 1469 loss: 1.80567747e-06
Iter: 1470 loss: 1.80704194e-06
Iter: 1471 loss: 1.80555753e-06
Iter: 1472 loss: 1.80506754e-06
Iter: 1473 loss: 1.80757957e-06
Iter: 1474 loss: 1.80497636e-06
Iter: 1475 loss: 1.80460847e-06
Iter: 1476 loss: 1.80491611e-06
Iter: 1477 loss: 1.80440247e-06
Iter: 1478 loss: 1.80386883e-06
Iter: 1479 loss: 1.80604252e-06
Iter: 1480 loss: 1.80377958e-06
Iter: 1481 loss: 1.80339111e-06
Iter: 1482 loss: 1.8040389e-06
Iter: 1483 loss: 1.80333393e-06
Iter: 1484 loss: 1.80299855e-06
Iter: 1485 loss: 1.80383722e-06
Iter: 1486 loss: 1.80294376e-06
Iter: 1487 loss: 1.80278744e-06
Iter: 1488 loss: 1.80279596e-06
Iter: 1489 loss: 1.80246502e-06
Iter: 1490 loss: 1.80241125e-06
Iter: 1491 loss: 1.80226687e-06
Iter: 1492 loss: 1.80211646e-06
Iter: 1493 loss: 1.80210304e-06
Iter: 1494 loss: 1.80202835e-06
Iter: 1495 loss: 1.80178199e-06
Iter: 1496 loss: 1.80311611e-06
Iter: 1497 loss: 1.80161703e-06
Iter: 1498 loss: 1.80130144e-06
Iter: 1499 loss: 1.80245149e-06
Iter: 1500 loss: 1.80126244e-06
Iter: 1501 loss: 1.80088432e-06
Iter: 1502 loss: 1.80143752e-06
Iter: 1503 loss: 1.80070174e-06
Iter: 1504 loss: 1.80022278e-06
Iter: 1505 loss: 1.80488883e-06
Iter: 1506 loss: 1.8001524e-06
Iter: 1507 loss: 1.80001234e-06
Iter: 1508 loss: 1.79991309e-06
Iter: 1509 loss: 1.79985147e-06
Iter: 1510 loss: 1.7994472e-06
Iter: 1511 loss: 1.79986478e-06
Iter: 1512 loss: 1.7991415e-06
Iter: 1513 loss: 1.79879476e-06
Iter: 1514 loss: 1.79960512e-06
Iter: 1515 loss: 1.79851236e-06
Iter: 1516 loss: 1.79804522e-06
Iter: 1517 loss: 1.79916901e-06
Iter: 1518 loss: 1.79786639e-06
Iter: 1519 loss: 1.7975766e-06
Iter: 1520 loss: 1.79757455e-06
Iter: 1521 loss: 1.79727897e-06
Iter: 1522 loss: 1.79717949e-06
Iter: 1523 loss: 1.7970018e-06
Iter: 1524 loss: 1.79678159e-06
Iter: 1525 loss: 1.79676022e-06
Iter: 1526 loss: 1.79659241e-06
Iter: 1527 loss: 1.79626477e-06
Iter: 1528 loss: 1.79799144e-06
Iter: 1529 loss: 1.79598362e-06
Iter: 1530 loss: 1.79560823e-06
Iter: 1531 loss: 1.79801054e-06
Iter: 1532 loss: 1.79559083e-06
Iter: 1533 loss: 1.79503991e-06
Iter: 1534 loss: 1.79689789e-06
Iter: 1535 loss: 1.7949103e-06
Iter: 1536 loss: 1.79451729e-06
Iter: 1537 loss: 1.79480639e-06
Iter: 1538 loss: 1.79419249e-06
Iter: 1539 loss: 1.79358244e-06
Iter: 1540 loss: 1.79476058e-06
Iter: 1541 loss: 1.79343238e-06
Iter: 1542 loss: 1.79287031e-06
Iter: 1543 loss: 1.79515439e-06
Iter: 1544 loss: 1.79273866e-06
Iter: 1545 loss: 1.79214078e-06
Iter: 1546 loss: 1.79476172e-06
Iter: 1547 loss: 1.79207723e-06
Iter: 1548 loss: 1.79157814e-06
Iter: 1549 loss: 1.79146923e-06
Iter: 1550 loss: 1.79115409e-06
Iter: 1551 loss: 1.79049925e-06
Iter: 1552 loss: 1.79332255e-06
Iter: 1553 loss: 1.79037761e-06
Iter: 1554 loss: 1.79003337e-06
Iter: 1555 loss: 1.78996856e-06
Iter: 1556 loss: 1.78963592e-06
Iter: 1557 loss: 1.79030587e-06
Iter: 1558 loss: 1.78956361e-06
Iter: 1559 loss: 1.78919367e-06
Iter: 1560 loss: 1.79190545e-06
Iter: 1561 loss: 1.78921232e-06
Iter: 1562 loss: 1.7890336e-06
Iter: 1563 loss: 1.78859773e-06
Iter: 1564 loss: 1.79040978e-06
Iter: 1565 loss: 1.78826372e-06
Iter: 1566 loss: 1.78786979e-06
Iter: 1567 loss: 1.7879463e-06
Iter: 1568 loss: 1.78750929e-06
Iter: 1569 loss: 1.78744631e-06
Iter: 1570 loss: 1.78719029e-06
Iter: 1571 loss: 1.78677078e-06
Iter: 1572 loss: 1.78772871e-06
Iter: 1573 loss: 1.78658047e-06
Iter: 1574 loss: 1.78608991e-06
Iter: 1575 loss: 1.78863388e-06
Iter: 1576 loss: 1.785981e-06
Iter: 1577 loss: 1.78569667e-06
Iter: 1578 loss: 1.7876265e-06
Iter: 1579 loss: 1.78555194e-06
Iter: 1580 loss: 1.78533162e-06
Iter: 1581 loss: 1.78504263e-06
Iter: 1582 loss: 1.78484504e-06
Iter: 1583 loss: 1.78449864e-06
Iter: 1584 loss: 1.78620485e-06
Iter: 1585 loss: 1.78442588e-06
Iter: 1586 loss: 1.78395828e-06
Iter: 1587 loss: 1.78602204e-06
Iter: 1588 loss: 1.78392759e-06
Iter: 1589 loss: 1.78343862e-06
Iter: 1590 loss: 1.7853572e-06
Iter: 1591 loss: 1.78327764e-06
Iter: 1592 loss: 1.78295102e-06
Iter: 1593 loss: 1.78505104e-06
Iter: 1594 loss: 1.78291532e-06
Iter: 1595 loss: 1.78256596e-06
Iter: 1596 loss: 1.78182768e-06
Iter: 1597 loss: 1.79180824e-06
Iter: 1598 loss: 1.78173923e-06
Iter: 1599 loss: 1.78116386e-06
Iter: 1600 loss: 1.78307744e-06
Iter: 1601 loss: 1.78095456e-06
Iter: 1602 loss: 1.78031576e-06
Iter: 1603 loss: 1.78667244e-06
Iter: 1604 loss: 1.78037703e-06
Iter: 1605 loss: 1.78007781e-06
Iter: 1606 loss: 1.77930315e-06
Iter: 1607 loss: 1.79271e-06
Iter: 1608 loss: 1.77937477e-06
Iter: 1609 loss: 1.7787155e-06
Iter: 1610 loss: 1.77871527e-06
Iter: 1611 loss: 1.77829884e-06
Iter: 1612 loss: 1.77893162e-06
Iter: 1613 loss: 1.7780211e-06
Iter: 1614 loss: 1.77742049e-06
Iter: 1615 loss: 1.77830157e-06
Iter: 1616 loss: 1.77713036e-06
Iter: 1617 loss: 1.77643324e-06
Iter: 1618 loss: 1.77815446e-06
Iter: 1619 loss: 1.77621587e-06
Iter: 1620 loss: 1.77549828e-06
Iter: 1621 loss: 1.77658239e-06
Iter: 1622 loss: 1.77523975e-06
Iter: 1623 loss: 1.77462618e-06
Iter: 1624 loss: 1.77460515e-06
Iter: 1625 loss: 1.77413892e-06
Iter: 1626 loss: 1.77535685e-06
Iter: 1627 loss: 1.7741861e-06
Iter: 1628 loss: 1.77376342e-06
Iter: 1629 loss: 1.77308505e-06
Iter: 1630 loss: 1.77312222e-06
Iter: 1631 loss: 1.77251309e-06
Iter: 1632 loss: 1.77203651e-06
Iter: 1633 loss: 1.77184836e-06
Iter: 1634 loss: 1.77127185e-06
Iter: 1635 loss: 1.77122138e-06
Iter: 1636 loss: 1.77066659e-06
Iter: 1637 loss: 1.76972299e-06
Iter: 1638 loss: 1.78742698e-06
Iter: 1639 loss: 1.76974163e-06
Iter: 1640 loss: 1.76915114e-06
Iter: 1641 loss: 1.76916728e-06
Iter: 1642 loss: 1.76860544e-06
Iter: 1643 loss: 1.76959929e-06
Iter: 1644 loss: 1.76834544e-06
Iter: 1645 loss: 1.76770095e-06
Iter: 1646 loss: 1.76984713e-06
Iter: 1647 loss: 1.76754827e-06
Iter: 1648 loss: 1.76708033e-06
Iter: 1649 loss: 1.76722529e-06
Iter: 1650 loss: 1.76677065e-06
Iter: 1651 loss: 1.7659421e-06
Iter: 1652 loss: 1.76871754e-06
Iter: 1653 loss: 1.76566164e-06
Iter: 1654 loss: 1.76522906e-06
Iter: 1655 loss: 1.76525941e-06
Iter: 1656 loss: 1.76486321e-06
Iter: 1657 loss: 1.76595631e-06
Iter: 1658 loss: 1.76473043e-06
Iter: 1659 loss: 1.76437175e-06
Iter: 1660 loss: 1.76406297e-06
Iter: 1661 loss: 1.76389972e-06
Iter: 1662 loss: 1.76349045e-06
Iter: 1663 loss: 1.76297749e-06
Iter: 1664 loss: 1.76283663e-06
Iter: 1665 loss: 1.76252456e-06
Iter: 1666 loss: 1.76251e-06
Iter: 1667 loss: 1.76212689e-06
Iter: 1668 loss: 1.76152116e-06
Iter: 1669 loss: 1.7615647e-06
Iter: 1670 loss: 1.76096535e-06
Iter: 1671 loss: 1.76166884e-06
Iter: 1672 loss: 1.76069659e-06
Iter: 1673 loss: 1.75990704e-06
Iter: 1674 loss: 1.76547042e-06
Iter: 1675 loss: 1.75977038e-06
Iter: 1676 loss: 1.75934838e-06
Iter: 1677 loss: 1.76106289e-06
Iter: 1678 loss: 1.75918342e-06
Iter: 1679 loss: 1.7586367e-06
Iter: 1680 loss: 1.75894093e-06
Iter: 1681 loss: 1.75824414e-06
Iter: 1682 loss: 1.75763296e-06
Iter: 1683 loss: 1.76125263e-06
Iter: 1684 loss: 1.75751302e-06
Iter: 1685 loss: 1.75716593e-06
Iter: 1686 loss: 1.7610156e-06
Iter: 1687 loss: 1.75704531e-06
Iter: 1688 loss: 1.75678588e-06
Iter: 1689 loss: 1.75823016e-06
Iter: 1690 loss: 1.75670095e-06
Iter: 1691 loss: 1.75632567e-06
Iter: 1692 loss: 1.75672562e-06
Iter: 1693 loss: 1.75604032e-06
Iter: 1694 loss: 1.75582647e-06
Iter: 1695 loss: 1.75510309e-06
Iter: 1696 loss: 1.76598633e-06
Iter: 1697 loss: 1.7551115e-06
Iter: 1698 loss: 1.75451294e-06
Iter: 1699 loss: 1.75969603e-06
Iter: 1700 loss: 1.7544711e-06
Iter: 1701 loss: 1.75383e-06
Iter: 1702 loss: 1.75482069e-06
Iter: 1703 loss: 1.7534519e-06
Iter: 1704 loss: 1.75290643e-06
Iter: 1705 loss: 1.75197795e-06
Iter: 1706 loss: 1.75195737e-06
Iter: 1707 loss: 1.75118885e-06
Iter: 1708 loss: 1.75113541e-06
Iter: 1709 loss: 1.75048183e-06
Iter: 1710 loss: 1.75043715e-06
Iter: 1711 loss: 1.74993716e-06
Iter: 1712 loss: 1.74902709e-06
Iter: 1713 loss: 1.75186972e-06
Iter: 1714 loss: 1.7487572e-06
Iter: 1715 loss: 1.74805359e-06
Iter: 1716 loss: 1.74978481e-06
Iter: 1717 loss: 1.74772708e-06
Iter: 1718 loss: 1.74700187e-06
Iter: 1719 loss: 1.75063428e-06
Iter: 1720 loss: 1.74685556e-06
Iter: 1721 loss: 1.74608522e-06
Iter: 1722 loss: 1.75263608e-06
Iter: 1723 loss: 1.74612842e-06
Iter: 1724 loss: 1.74553509e-06
Iter: 1725 loss: 1.74627746e-06
Iter: 1726 loss: 1.74523723e-06
Iter: 1727 loss: 1.74481829e-06
Iter: 1728 loss: 1.74382603e-06
Iter: 1729 loss: 1.75828518e-06
Iter: 1730 loss: 1.74387401e-06
Iter: 1731 loss: 1.7427725e-06
Iter: 1732 loss: 1.7499035e-06
Iter: 1733 loss: 1.74266791e-06
Iter: 1734 loss: 1.74171544e-06
Iter: 1735 loss: 1.74899708e-06
Iter: 1736 loss: 1.7417193e-06
Iter: 1737 loss: 1.74115405e-06
Iter: 1738 loss: 1.74004856e-06
Iter: 1739 loss: 1.76198591e-06
Iter: 1740 loss: 1.74002525e-06
Iter: 1741 loss: 1.73936542e-06
Iter: 1742 loss: 1.73929652e-06
Iter: 1743 loss: 1.73849241e-06
Iter: 1744 loss: 1.73949059e-06
Iter: 1745 loss: 1.73815397e-06
Iter: 1746 loss: 1.73736146e-06
Iter: 1747 loss: 1.7405481e-06
Iter: 1748 loss: 1.73728893e-06
Iter: 1749 loss: 1.73664762e-06
Iter: 1750 loss: 1.73639262e-06
Iter: 1751 loss: 1.7360037e-06
Iter: 1752 loss: 1.7350842e-06
Iter: 1753 loss: 1.74251409e-06
Iter: 1754 loss: 1.73506817e-06
Iter: 1755 loss: 1.73433261e-06
Iter: 1756 loss: 1.74379034e-06
Iter: 1757 loss: 1.73434682e-06
Iter: 1758 loss: 1.73393641e-06
Iter: 1759 loss: 1.73443198e-06
Iter: 1760 loss: 1.73366436e-06
Iter: 1761 loss: 1.73314675e-06
Iter: 1762 loss: 1.73229273e-06
Iter: 1763 loss: 1.73233025e-06
Iter: 1764 loss: 1.73150306e-06
Iter: 1765 loss: 1.73333865e-06
Iter: 1766 loss: 1.73117701e-06
Iter: 1767 loss: 1.73041883e-06
Iter: 1768 loss: 1.73959938e-06
Iter: 1769 loss: 1.7304028e-06
Iter: 1770 loss: 1.72993043e-06
Iter: 1771 loss: 1.72914133e-06
Iter: 1772 loss: 1.72903037e-06
Iter: 1773 loss: 1.72853652e-06
Iter: 1774 loss: 1.73344108e-06
Iter: 1775 loss: 1.72841692e-06
Iter: 1776 loss: 1.72786235e-06
Iter: 1777 loss: 1.7309485e-06
Iter: 1778 loss: 1.72770706e-06
Iter: 1779 loss: 1.72732018e-06
Iter: 1780 loss: 1.72847967e-06
Iter: 1781 loss: 1.72723639e-06
Iter: 1782 loss: 1.72675902e-06
Iter: 1783 loss: 1.72653563e-06
Iter: 1784 loss: 1.72640182e-06
Iter: 1785 loss: 1.72575278e-06
Iter: 1786 loss: 1.73201215e-06
Iter: 1787 loss: 1.72578495e-06
Iter: 1788 loss: 1.72536522e-06
Iter: 1789 loss: 1.72780324e-06
Iter: 1790 loss: 1.72538216e-06
Iter: 1791 loss: 1.72494754e-06
Iter: 1792 loss: 1.72534942e-06
Iter: 1793 loss: 1.7246349e-06
Iter: 1794 loss: 1.72420334e-06
Iter: 1795 loss: 1.72393936e-06
Iter: 1796 loss: 1.72365685e-06
Iter: 1797 loss: 1.72315174e-06
Iter: 1798 loss: 1.72313253e-06
Iter: 1799 loss: 1.722668e-06
Iter: 1800 loss: 1.72231535e-06
Iter: 1801 loss: 1.72219234e-06
Iter: 1802 loss: 1.72182718e-06
Iter: 1803 loss: 1.7215383e-06
Iter: 1804 loss: 1.72138198e-06
Iter: 1805 loss: 1.72082741e-06
Iter: 1806 loss: 1.72119587e-06
Iter: 1807 loss: 1.72049863e-06
Iter: 1808 loss: 1.72005389e-06
Iter: 1809 loss: 1.72000068e-06
Iter: 1810 loss: 1.71968645e-06
Iter: 1811 loss: 1.71970237e-06
Iter: 1812 loss: 1.71939064e-06
Iter: 1813 loss: 1.71887564e-06
Iter: 1814 loss: 1.71908141e-06
Iter: 1815 loss: 1.71856232e-06
Iter: 1816 loss: 1.71790327e-06
Iter: 1817 loss: 1.72120451e-06
Iter: 1818 loss: 1.71789247e-06
Iter: 1819 loss: 1.717342e-06
Iter: 1820 loss: 1.721025e-06
Iter: 1821 loss: 1.7172564e-06
Iter: 1822 loss: 1.71668876e-06
Iter: 1823 loss: 1.7193172e-06
Iter: 1824 loss: 1.71666738e-06
Iter: 1825 loss: 1.71629131e-06
Iter: 1826 loss: 1.71597617e-06
Iter: 1827 loss: 1.71593831e-06
Iter: 1828 loss: 1.71535385e-06
Iter: 1829 loss: 1.71496924e-06
Iter: 1830 loss: 1.71481418e-06
Iter: 1831 loss: 1.71434328e-06
Iter: 1832 loss: 1.71433021e-06
Iter: 1833 loss: 1.71388717e-06
Iter: 1834 loss: 1.71377962e-06
Iter: 1835 loss: 1.71344641e-06
Iter: 1836 loss: 1.7128757e-06
Iter: 1837 loss: 1.71296188e-06
Iter: 1838 loss: 1.71243482e-06
Iter: 1839 loss: 1.71208194e-06
Iter: 1840 loss: 1.71200918e-06
Iter: 1841 loss: 1.71165868e-06
Iter: 1842 loss: 1.71137845e-06
Iter: 1843 loss: 1.71120053e-06
Iter: 1844 loss: 1.71067541e-06
Iter: 1845 loss: 1.71369049e-06
Iter: 1846 loss: 1.71048271e-06
Iter: 1847 loss: 1.71013551e-06
Iter: 1848 loss: 1.71019462e-06
Iter: 1849 loss: 1.70991757e-06
Iter: 1850 loss: 1.70935277e-06
Iter: 1851 loss: 1.71400222e-06
Iter: 1852 loss: 1.70926683e-06
Iter: 1853 loss: 1.70893418e-06
Iter: 1854 loss: 1.71394584e-06
Iter: 1855 loss: 1.70884528e-06
Iter: 1856 loss: 1.70868066e-06
Iter: 1857 loss: 1.70822329e-06
Iter: 1858 loss: 1.71789907e-06
Iter: 1859 loss: 1.70820363e-06
Iter: 1860 loss: 1.70761655e-06
Iter: 1861 loss: 1.70755959e-06
Iter: 1862 loss: 1.70711758e-06
Iter: 1863 loss: 1.7065106e-06
Iter: 1864 loss: 1.7084792e-06
Iter: 1865 loss: 1.70637963e-06
Iter: 1866 loss: 1.70554222e-06
Iter: 1867 loss: 1.70878502e-06
Iter: 1868 loss: 1.70525436e-06
Iter: 1869 loss: 1.7047e-06
Iter: 1870 loss: 1.70426563e-06
Iter: 1871 loss: 1.7040843e-06
Iter: 1872 loss: 1.70353451e-06
Iter: 1873 loss: 1.71084889e-06
Iter: 1874 loss: 1.70356191e-06
Iter: 1875 loss: 1.70293822e-06
Iter: 1876 loss: 1.70436476e-06
Iter: 1877 loss: 1.70275734e-06
Iter: 1878 loss: 1.70234739e-06
Iter: 1879 loss: 1.70413614e-06
Iter: 1880 loss: 1.70225871e-06
Iter: 1881 loss: 1.70183466e-06
Iter: 1882 loss: 1.70132e-06
Iter: 1883 loss: 1.70136343e-06
Iter: 1884 loss: 1.70104988e-06
Iter: 1885 loss: 1.70092301e-06
Iter: 1886 loss: 1.70064334e-06
Iter: 1887 loss: 1.70325359e-06
Iter: 1888 loss: 1.70064607e-06
Iter: 1889 loss: 1.70053795e-06
Iter: 1890 loss: 1.6999719e-06
Iter: 1891 loss: 1.70990904e-06
Iter: 1892 loss: 1.70003807e-06
Iter: 1893 loss: 1.69953648e-06
Iter: 1894 loss: 1.70024919e-06
Iter: 1895 loss: 1.69938426e-06
Iter: 1896 loss: 1.6988804e-06
Iter: 1897 loss: 1.69892451e-06
Iter: 1898 loss: 1.69845589e-06
Iter: 1899 loss: 1.69816121e-06
Iter: 1900 loss: 1.69811142e-06
Iter: 1901 loss: 1.69780844e-06
Iter: 1902 loss: 1.69722057e-06
Iter: 1903 loss: 1.71100487e-06
Iter: 1904 loss: 1.69728366e-06
Iter: 1905 loss: 1.69667567e-06
Iter: 1906 loss: 1.69908674e-06
Iter: 1907 loss: 1.69656767e-06
Iter: 1908 loss: 1.69612315e-06
Iter: 1909 loss: 1.70244084e-06
Iter: 1910 loss: 1.69602185e-06
Iter: 1911 loss: 1.69590896e-06
Iter: 1912 loss: 1.69574082e-06
Iter: 1913 loss: 1.69549935e-06
Iter: 1914 loss: 1.69492455e-06
Iter: 1915 loss: 1.69654118e-06
Iter: 1916 loss: 1.69487498e-06
Iter: 1917 loss: 1.69455245e-06
Iter: 1918 loss: 1.69544933e-06
Iter: 1919 loss: 1.69436555e-06
Iter: 1920 loss: 1.69389659e-06
Iter: 1921 loss: 1.69709301e-06
Iter: 1922 loss: 1.69380553e-06
Iter: 1923 loss: 1.69349528e-06
Iter: 1924 loss: 1.69601481e-06
Iter: 1925 loss: 1.69335192e-06
Iter: 1926 loss: 1.69323766e-06
Iter: 1927 loss: 1.69277882e-06
Iter: 1928 loss: 1.69881594e-06
Iter: 1929 loss: 1.69280565e-06
Iter: 1930 loss: 1.69225586e-06
Iter: 1931 loss: 1.69292059e-06
Iter: 1932 loss: 1.69203986e-06
Iter: 1933 loss: 1.69151576e-06
Iter: 1934 loss: 1.69423106e-06
Iter: 1935 loss: 1.69147984e-06
Iter: 1936 loss: 1.69097029e-06
Iter: 1937 loss: 1.69216219e-06
Iter: 1938 loss: 1.69079476e-06
Iter: 1939 loss: 1.69032137e-06
Iter: 1940 loss: 1.68983081e-06
Iter: 1941 loss: 1.68980887e-06
Iter: 1942 loss: 1.68935856e-06
Iter: 1943 loss: 1.68929114e-06
Iter: 1944 loss: 1.6889162e-06
Iter: 1945 loss: 1.68943279e-06
Iter: 1946 loss: 1.68873567e-06
Iter: 1947 loss: 1.68829604e-06
Iter: 1948 loss: 1.68947099e-06
Iter: 1949 loss: 1.68819884e-06
Iter: 1950 loss: 1.68778752e-06
Iter: 1951 loss: 1.68742258e-06
Iter: 1952 loss: 1.68722511e-06
Iter: 1953 loss: 1.68715565e-06
Iter: 1954 loss: 1.68697829e-06
Iter: 1955 loss: 1.6868396e-06
Iter: 1956 loss: 1.68745419e-06
Iter: 1957 loss: 1.68668657e-06
Iter: 1958 loss: 1.68636836e-06
Iter: 1959 loss: 1.68576412e-06
Iter: 1960 loss: 1.69491591e-06
Iter: 1961 loss: 1.68571273e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.2 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi1.2
+ date
Sun Nov  8 16:08:36 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.2/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.2/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca3205510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca31caa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca317d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca30f8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca30f81e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca317d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca30c30d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca3070840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca3070a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca30708c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca2fb92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca2fb1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80aa28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80b121e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80b2b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80ac79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80af2d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80a3af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca2fcc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80a3a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80a48510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80a47268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80a48e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c22ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c22a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c242ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c17dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c2422f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c291158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c2920d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c27c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c167158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c167620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c146ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c0d9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c0cec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
