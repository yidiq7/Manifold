+ RUN=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ LAYERS=300_100_100_100_1
+ case $RUN in
+ PSI='0 1'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 800 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output120
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output122
+ for fn in f1
+ case $fn in
+ OPT=--phi
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.final/output120/f1_psi0_phi0
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0
+ date
Sun Nov  8 13:21:32 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83c037b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83cf0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83d28c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83bb1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83bc11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83bb8bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83ce46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83c39158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83c34b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83aee620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83b1c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83b7eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83b6f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83a78048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83aac9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83adb510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83abbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83b6c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83a3b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83a48e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd839708c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd8397a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd839aad90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd83933840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd604802f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd6047ad08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd839f57b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd839ef268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd603dec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd603e3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd603ab1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd603ccbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd839d16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd839b4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd3815ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7efd38120620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.37136078
test_loss: 0.3718866
train_loss: 0.3183519
test_loss: 0.31862602
train_loss: 0.2615493
test_loss: 0.2610588
train_loss: 0.2011903
test_loss: 0.20217901
train_loss: 0.14412837
test_loss: 0.14466074
train_loss: 0.09411223
test_loss: 0.09430672
train_loss: 0.06570731
test_loss: 0.06665362
train_loss: 0.05795943
test_loss: 0.058024142
train_loss: 0.05496139
test_loss: 0.05541862
train_loss: 0.054364573
test_loss: 0.054272514
train_loss: 0.053404428
test_loss: 0.05361733
train_loss: 0.05293052
test_loss: 0.053186037
train_loss: 0.052511998
test_loss: 0.052888703
train_loss: 0.05224951
test_loss: 0.05257774
train_loss: 0.052528635
test_loss: 0.05238816
train_loss: 0.05229443
test_loss: 0.052114327
train_loss: 0.05160591
test_loss: 0.051826257
train_loss: 0.051051736
test_loss: 0.051529273
train_loss: 0.051824603
test_loss: 0.051115356
train_loss: 0.050583914
test_loss: 0.05068703
train_loss: 0.049683332
test_loss: 0.05011492
train_loss: 0.04863812
test_loss: 0.04945878
train_loss: 0.047926366
test_loss: 0.048572212
train_loss: 0.046441436
test_loss: 0.04759389
train_loss: 0.046164036
test_loss: 0.046417117
train_loss: 0.044083055
test_loss: 0.044987395
train_loss: 0.04382411
test_loss: 0.043389987
train_loss: 0.04159741
test_loss: 0.041317347
train_loss: 0.038512077
test_loss: 0.039081443
train_loss: 0.036201403
test_loss: 0.03621192
train_loss: 0.03272224
test_loss: 0.033124227
train_loss: 0.029275846
test_loss: 0.0294035
train_loss: 0.024675872
test_loss: 0.025208099
train_loss: 0.02051524
test_loss: 0.020705508
train_loss: 0.01648038
test_loss: 0.016506083
train_loss: 0.013015712
test_loss: 0.01326541
train_loss: 0.01058764
test_loss: 0.010975327
train_loss: 0.009785691
test_loss: 0.009896638
train_loss: 0.008961037
test_loss: 0.008945455
train_loss: 0.008278104
test_loss: 0.00849061
train_loss: 0.00808288
test_loss: 0.008014735
train_loss: 0.007838277
test_loss: 0.0077758366
train_loss: 0.0074584996
test_loss: 0.0075601353
train_loss: 0.007377573
test_loss: 0.007410541
train_loss: 0.0071113915
test_loss: 0.0071971724
train_loss: 0.0068611572
test_loss: 0.0070315152
train_loss: 0.0071130903
test_loss: 0.0070380564
train_loss: 0.0067420406
test_loss: 0.0069844206
train_loss: 0.0063384036
test_loss: 0.006809312
train_loss: 0.006313932
test_loss: 0.006425849
train_loss: 0.006618985
test_loss: 0.0066622104
train_loss: 0.00612072
test_loss: 0.0062958687
train_loss: 0.0062349774
test_loss: 0.0063705076
train_loss: 0.00584119
test_loss: 0.0059566405
train_loss: 0.005701905
test_loss: 0.005836885
train_loss: 0.0055509536
test_loss: 0.0055919075
train_loss: 0.005559606
test_loss: 0.005556443
train_loss: 0.005171459
test_loss: 0.005199746
train_loss: 0.004905947
test_loss: 0.0048912195
train_loss: 0.0050214264
test_loss: 0.0051383483
train_loss: 0.0047934465
test_loss: 0.0048864232
train_loss: 0.004466814
test_loss: 0.0045619216
train_loss: 0.00446761
test_loss: 0.00452095
train_loss: 0.004081498
test_loss: 0.0040055783
train_loss: 0.0042787865
test_loss: 0.003942505
train_loss: 0.0039861873
test_loss: 0.0040093106
train_loss: 0.004030025
test_loss: 0.0037679705
train_loss: 0.003936293
test_loss: 0.0035125585
train_loss: 0.0036084338
test_loss: 0.0036107814
train_loss: 0.0036029052
test_loss: 0.0035249523
train_loss: 0.0034696448
test_loss: 0.0034770207
train_loss: 0.0031216056
test_loss: 0.0030057002
train_loss: 0.003466016
test_loss: 0.0032251026
train_loss: 0.0029256353
test_loss: 0.0030687905
train_loss: 0.002910193
test_loss: 0.003268385
train_loss: 0.0027890939
test_loss: 0.0033528788
train_loss: 0.0029205293
test_loss: 0.003071163
train_loss: 0.0032748925
test_loss: 0.003647243
train_loss: 0.0034294866
test_loss: 0.002945699
train_loss: 0.0030001577
test_loss: 0.003275898
train_loss: 0.0030476279
test_loss: 0.0028956607
train_loss: 0.0029420883
test_loss: 0.0033514185
train_loss: 0.003050046
test_loss: 0.0028077064
train_loss: 0.0033014102
test_loss: 0.0034968737
train_loss: 0.0031350902
test_loss: 0.00288011
train_loss: 0.0029054834
test_loss: 0.002596342
train_loss: 0.0030989877
test_loss: 0.0032472536
train_loss: 0.0027845178
test_loss: 0.0028302993
train_loss: 0.003372624
test_loss: 0.0036820483
train_loss: 0.0027603942
test_loss: 0.0025934977
train_loss: 0.0025168285
test_loss: 0.00284368
train_loss: 0.0028114822
test_loss: 0.003450772
train_loss: 0.0032268765
test_loss: 0.0033553012
train_loss: 0.0027834894
test_loss: 0.0040714727
train_loss: 0.003206253
test_loss: 0.0031219376
train_loss: 0.0031759313
test_loss: 0.002990071
train_loss: 0.002527939
test_loss: 0.0025857438
train_loss: 0.0031560352
test_loss: 0.0036965683
train_loss: 0.0027234931
test_loss: 0.0026948382
train_loss: 0.0027136034
test_loss: 0.0026041027
train_loss: 0.0025378682
test_loss: 0.0023651905
train_loss: 0.0026585325
test_loss: 0.002553786
train_loss: 0.0027375722
test_loss: 0.002506441
train_loss: 0.0029459812
test_loss: 0.003498726
train_loss: 0.0025848104
test_loss: 0.0027647715
train_loss: 0.0022403686
test_loss: 0.002620046
train_loss: 0.0024563149
test_loss: 0.0029078026
train_loss: 0.0031015654
test_loss: 0.003415361
train_loss: 0.0028705166
test_loss: 0.0027582282
train_loss: 0.0029846942
test_loss: 0.0028633485
train_loss: 0.002414343
test_loss: 0.0032625245
train_loss: 0.0028289356
test_loss: 0.002485928
train_loss: 0.0029634226
test_loss: 0.0027332965
train_loss: 0.0028865421
test_loss: 0.002744283
train_loss: 0.0028881817
test_loss: 0.0025302118
train_loss: 0.002420722
test_loss: 0.0028595251
train_loss: 0.0026836249
test_loss: 0.0024962306
train_loss: 0.0025071085
test_loss: 0.0026248815
train_loss: 0.0027099275
test_loss: 0.0034836163
train_loss: 0.0031118854
test_loss: 0.0026164623
train_loss: 0.0024950167
test_loss: 0.0026171964
train_loss: 0.0028706142
test_loss: 0.0030135273
train_loss: 0.0024396202
test_loss: 0.003223285
train_loss: 0.0027919253
test_loss: 0.0027226494
train_loss: 0.0029783787
test_loss: 0.002755008
train_loss: 0.003713786
test_loss: 0.00282046
train_loss: 0.0024774286
test_loss: 0.002877373
train_loss: 0.003148843
test_loss: 0.0029400978
train_loss: 0.0026573527
test_loss: 0.0024876161
train_loss: 0.00271089
test_loss: 0.002641378
train_loss: 0.0025917094
test_loss: 0.0030605549
train_loss: 0.0027261428
test_loss: 0.0026373987
train_loss: 0.002783986
test_loss: 0.0023207094
train_loss: 0.0024791914
test_loss: 0.0025358526
train_loss: 0.0023814999
test_loss: 0.0032267033
train_loss: 0.0023652236
test_loss: 0.0023442751
train_loss: 0.0026625732
test_loss: 0.0023002452
train_loss: 0.0027067217
test_loss: 0.002664291
train_loss: 0.0030576345
test_loss: 0.0026925139
train_loss: 0.0027506691
test_loss: 0.002515877
train_loss: 0.0021591596
test_loss: 0.0023835446
train_loss: 0.0033655958
test_loss: 0.0028339783
train_loss: 0.002814142
test_loss: 0.002445723
train_loss: 0.0027645286
test_loss: 0.0030468642
train_loss: 0.0034558834
test_loss: 0.0027663165
train_loss: 0.002246969
test_loss: 0.0025362442
train_loss: 0.0022845115
test_loss: 0.0023775958
train_loss: 0.0027037775
test_loss: 0.0027080292
train_loss: 0.002851784
test_loss: 0.0028654411
train_loss: 0.0027988805
test_loss: 0.0025348077
train_loss: 0.002664582
test_loss: 0.0032458357
train_loss: 0.0028945068
test_loss: 0.0025730182
train_loss: 0.002465132
test_loss: 0.0030949092
train_loss: 0.0023900012
test_loss: 0.0026132548
train_loss: 0.0025599825
test_loss: 0.0029235452
train_loss: 0.0027196333
test_loss: 0.0027032571
train_loss: 0.0025434287
test_loss: 0.002367605
train_loss: 0.002791319
test_loss: 0.003144055
train_loss: 0.0025762736
test_loss: 0.0024333545
train_loss: 0.0026617679
test_loss: 0.0027866159
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 0 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e6a1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e75a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e754d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e7a37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e797268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e7abc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e649730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e65b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e66fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e61e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e5b4158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e5d2b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e592620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e5910d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e5a1ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e56d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e51c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e5219d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e4cd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e4dfea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e4a2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e4b1400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e43ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e4698c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77fad378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77fc0d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77f76840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77f162f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77f2cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77ee17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf8e6831e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77e98c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf77ebc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf505f91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf50614bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcf505c66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.18935623e-05
Iter: 2 loss: 7.92691499e-05
Iter: 3 loss: 6.06270669e-06
Iter: 4 loss: 5.09192841e-06
Iter: 5 loss: 3.56350984e-06
Iter: 6 loss: 3.54664667e-06
Iter: 7 loss: 2.90539515e-06
Iter: 8 loss: 2.74069771e-06
Iter: 9 loss: 2.53868097e-06
Iter: 10 loss: 2.33608057e-06
Iter: 11 loss: 2.29516172e-06
Iter: 12 loss: 2.03402e-06
Iter: 13 loss: 4.72890861e-06
Iter: 14 loss: 2.02651518e-06
Iter: 15 loss: 1.93436927e-06
Iter: 16 loss: 3.04034211e-06
Iter: 17 loss: 1.93324468e-06
Iter: 18 loss: 1.86420061e-06
Iter: 19 loss: 1.72044668e-06
Iter: 20 loss: 4.17640649e-06
Iter: 21 loss: 1.71707097e-06
Iter: 22 loss: 1.64152652e-06
Iter: 23 loss: 2.68381154e-06
Iter: 24 loss: 1.64126698e-06
Iter: 25 loss: 1.59210458e-06
Iter: 26 loss: 2.30633486e-06
Iter: 27 loss: 1.59204569e-06
Iter: 28 loss: 1.57785962e-06
Iter: 29 loss: 1.54249403e-06
Iter: 30 loss: 1.88545084e-06
Iter: 31 loss: 1.5378738e-06
Iter: 32 loss: 1.51390168e-06
Iter: 33 loss: 1.52520602e-06
Iter: 34 loss: 1.49779135e-06
Iter: 35 loss: 1.47940409e-06
Iter: 36 loss: 1.53207907e-06
Iter: 37 loss: 1.47367928e-06
Iter: 38 loss: 1.45783679e-06
Iter: 39 loss: 1.42914337e-06
Iter: 40 loss: 2.12282976e-06
Iter: 41 loss: 1.42917565e-06
Iter: 42 loss: 1.41391274e-06
Iter: 43 loss: 1.61533126e-06
Iter: 44 loss: 1.41383646e-06
Iter: 45 loss: 1.3971129e-06
Iter: 46 loss: 1.47162359e-06
Iter: 47 loss: 1.39382087e-06
Iter: 48 loss: 1.38471978e-06
Iter: 49 loss: 1.3651636e-06
Iter: 50 loss: 1.67277858e-06
Iter: 51 loss: 1.36449603e-06
Iter: 52 loss: 1.35706955e-06
Iter: 53 loss: 1.35316282e-06
Iter: 54 loss: 1.34480115e-06
Iter: 55 loss: 1.35103392e-06
Iter: 56 loss: 1.33971662e-06
Iter: 57 loss: 1.32861476e-06
Iter: 58 loss: 1.35913479e-06
Iter: 59 loss: 1.32502873e-06
Iter: 60 loss: 1.31953084e-06
Iter: 61 loss: 1.31883087e-06
Iter: 62 loss: 1.31498189e-06
Iter: 63 loss: 1.3063493e-06
Iter: 64 loss: 1.40747443e-06
Iter: 65 loss: 1.30624198e-06
Iter: 66 loss: 1.30200169e-06
Iter: 67 loss: 1.29089767e-06
Iter: 68 loss: 1.37789539e-06
Iter: 69 loss: 1.28882016e-06
Iter: 70 loss: 1.29102159e-06
Iter: 71 loss: 1.28564818e-06
Iter: 72 loss: 1.28192789e-06
Iter: 73 loss: 1.27923386e-06
Iter: 74 loss: 1.27796557e-06
Iter: 75 loss: 1.27482963e-06
Iter: 76 loss: 1.27104863e-06
Iter: 77 loss: 1.27065709e-06
Iter: 78 loss: 1.26542295e-06
Iter: 79 loss: 1.27744227e-06
Iter: 80 loss: 1.26340387e-06
Iter: 81 loss: 1.26013822e-06
Iter: 82 loss: 1.25925442e-06
Iter: 83 loss: 1.25776774e-06
Iter: 84 loss: 1.25398788e-06
Iter: 85 loss: 1.28435056e-06
Iter: 86 loss: 1.25323947e-06
Iter: 87 loss: 1.2504172e-06
Iter: 88 loss: 1.25036809e-06
Iter: 89 loss: 1.24708936e-06
Iter: 90 loss: 1.24718372e-06
Iter: 91 loss: 1.24450708e-06
Iter: 92 loss: 1.24119822e-06
Iter: 93 loss: 1.26311875e-06
Iter: 94 loss: 1.24087069e-06
Iter: 95 loss: 1.23800044e-06
Iter: 96 loss: 1.23212442e-06
Iter: 97 loss: 1.33472406e-06
Iter: 98 loss: 1.23199834e-06
Iter: 99 loss: 1.23503821e-06
Iter: 100 loss: 1.22994743e-06
Iter: 101 loss: 1.22872927e-06
Iter: 102 loss: 1.22590473e-06
Iter: 103 loss: 1.25676638e-06
Iter: 104 loss: 1.2256171e-06
Iter: 105 loss: 1.22335155e-06
Iter: 106 loss: 1.23571272e-06
Iter: 107 loss: 1.22298911e-06
Iter: 108 loss: 1.22002416e-06
Iter: 109 loss: 1.23597101e-06
Iter: 110 loss: 1.21960738e-06
Iter: 111 loss: 1.21851872e-06
Iter: 112 loss: 1.21596054e-06
Iter: 113 loss: 1.24720555e-06
Iter: 114 loss: 1.21569701e-06
Iter: 115 loss: 1.2152791e-06
Iter: 116 loss: 1.21448011e-06
Iter: 117 loss: 1.21319931e-06
Iter: 118 loss: 1.21202561e-06
Iter: 119 loss: 1.21165624e-06
Iter: 120 loss: 1.21040898e-06
Iter: 121 loss: 1.20848335e-06
Iter: 122 loss: 1.2084065e-06
Iter: 123 loss: 1.20671871e-06
Iter: 124 loss: 1.22699714e-06
Iter: 125 loss: 1.20669347e-06
Iter: 126 loss: 1.2047559e-06
Iter: 127 loss: 1.20591562e-06
Iter: 128 loss: 1.20350978e-06
Iter: 129 loss: 1.20232744e-06
Iter: 130 loss: 1.20266395e-06
Iter: 131 loss: 1.20149548e-06
Iter: 132 loss: 1.19993706e-06
Iter: 133 loss: 1.20945538e-06
Iter: 134 loss: 1.19975459e-06
Iter: 135 loss: 1.1987662e-06
Iter: 136 loss: 1.19725109e-06
Iter: 137 loss: 1.19724018e-06
Iter: 138 loss: 1.1955683e-06
Iter: 139 loss: 1.21302071e-06
Iter: 140 loss: 1.19552465e-06
Iter: 141 loss: 1.19404035e-06
Iter: 142 loss: 1.21708865e-06
Iter: 143 loss: 1.19407127e-06
Iter: 144 loss: 1.19356753e-06
Iter: 145 loss: 1.19475089e-06
Iter: 146 loss: 1.19340962e-06
Iter: 147 loss: 1.19268771e-06
Iter: 148 loss: 1.19162905e-06
Iter: 149 loss: 1.19159927e-06
Iter: 150 loss: 1.19079061e-06
Iter: 151 loss: 1.19409628e-06
Iter: 152 loss: 1.1906078e-06
Iter: 153 loss: 1.18932019e-06
Iter: 154 loss: 1.1911136e-06
Iter: 155 loss: 1.1886807e-06
Iter: 156 loss: 1.18779167e-06
Iter: 157 loss: 1.18647574e-06
Iter: 158 loss: 1.1864222e-06
Iter: 159 loss: 1.1866257e-06
Iter: 160 loss: 1.18585763e-06
Iter: 161 loss: 1.18545381e-06
Iter: 162 loss: 1.18485423e-06
Iter: 163 loss: 1.18480045e-06
Iter: 164 loss: 1.1842576e-06
Iter: 165 loss: 1.18614832e-06
Iter: 166 loss: 1.18409844e-06
Iter: 167 loss: 1.18336925e-06
Iter: 168 loss: 1.18326807e-06
Iter: 169 loss: 1.18276182e-06
Iter: 170 loss: 1.18193168e-06
Iter: 171 loss: 1.18174967e-06
Iter: 172 loss: 1.18117441e-06
Iter: 173 loss: 1.18021387e-06
Iter: 174 loss: 1.18522212e-06
Iter: 175 loss: 1.18008029e-06
Iter: 176 loss: 1.17956108e-06
Iter: 177 loss: 1.18771641e-06
Iter: 178 loss: 1.17954642e-06
Iter: 179 loss: 1.17885907e-06
Iter: 180 loss: 1.17847139e-06
Iter: 181 loss: 1.17811305e-06
Iter: 182 loss: 1.17753098e-06
Iter: 183 loss: 1.1777247e-06
Iter: 184 loss: 1.1771341e-06
Iter: 185 loss: 1.17662307e-06
Iter: 186 loss: 1.17661091e-06
Iter: 187 loss: 1.17637296e-06
Iter: 188 loss: 1.17578463e-06
Iter: 189 loss: 1.18005323e-06
Iter: 190 loss: 1.17564809e-06
Iter: 191 loss: 1.17537479e-06
Iter: 192 loss: 1.175237e-06
Iter: 193 loss: 1.17503203e-06
Iter: 194 loss: 1.17441232e-06
Iter: 195 loss: 1.17678076e-06
Iter: 196 loss: 1.17407535e-06
Iter: 197 loss: 1.17373372e-06
Iter: 198 loss: 1.1736081e-06
Iter: 199 loss: 1.17311811e-06
Iter: 200 loss: 1.17469733e-06
Iter: 201 loss: 1.1729262e-06
Iter: 202 loss: 1.17250852e-06
Iter: 203 loss: 1.17218622e-06
Iter: 204 loss: 1.17206378e-06
Iter: 205 loss: 1.17153422e-06
Iter: 206 loss: 1.17271429e-06
Iter: 207 loss: 1.17138131e-06
Iter: 208 loss: 1.17092065e-06
Iter: 209 loss: 1.17253114e-06
Iter: 210 loss: 1.17081322e-06
Iter: 211 loss: 1.17044897e-06
Iter: 212 loss: 1.1728921e-06
Iter: 213 loss: 1.1703994e-06
Iter: 214 loss: 1.17003219e-06
Iter: 215 loss: 1.16986166e-06
Iter: 216 loss: 1.16966362e-06
Iter: 217 loss: 1.16938645e-06
Iter: 218 loss: 1.1735392e-06
Iter: 219 loss: 1.16937781e-06
Iter: 220 loss: 1.16912679e-06
Iter: 221 loss: 1.16907404e-06
Iter: 222 loss: 1.16883507e-06
Iter: 223 loss: 1.16857177e-06
Iter: 224 loss: 1.16985279e-06
Iter: 225 loss: 1.16855358e-06
Iter: 226 loss: 1.16819956e-06
Iter: 227 loss: 1.16759759e-06
Iter: 228 loss: 1.18233106e-06
Iter: 229 loss: 1.16759634e-06
Iter: 230 loss: 1.16707713e-06
Iter: 231 loss: 1.16671538e-06
Iter: 232 loss: 1.16659896e-06
Iter: 233 loss: 1.16666615e-06
Iter: 234 loss: 1.16627689e-06
Iter: 235 loss: 1.16599335e-06
Iter: 236 loss: 1.16582146e-06
Iter: 237 loss: 1.16565104e-06
Iter: 238 loss: 1.16540082e-06
Iter: 239 loss: 1.16540855e-06
Iter: 240 loss: 1.16513775e-06
Iter: 241 loss: 1.16467459e-06
Iter: 242 loss: 1.17045943e-06
Iter: 243 loss: 1.16470699e-06
Iter: 244 loss: 1.16426895e-06
Iter: 245 loss: 1.16468732e-06
Iter: 246 loss: 1.16408864e-06
Iter: 247 loss: 1.16381193e-06
Iter: 248 loss: 1.16742012e-06
Iter: 249 loss: 1.16381455e-06
Iter: 250 loss: 1.16356568e-06
Iter: 251 loss: 1.16358228e-06
Iter: 252 loss: 1.16334036e-06
Iter: 253 loss: 1.16308331e-06
Iter: 254 loss: 1.16294223e-06
Iter: 255 loss: 1.16278738e-06
Iter: 256 loss: 1.16245906e-06
Iter: 257 loss: 1.16510705e-06
Iter: 258 loss: 1.16244018e-06
Iter: 259 loss: 1.16196861e-06
Iter: 260 loss: 1.16248327e-06
Iter: 261 loss: 1.16175067e-06
Iter: 262 loss: 1.16144406e-06
Iter: 263 loss: 1.16106287e-06
Iter: 264 loss: 1.16102547e-06
Iter: 265 loss: 1.16094725e-06
Iter: 266 loss: 1.16074136e-06
Iter: 267 loss: 1.16060028e-06
Iter: 268 loss: 1.16013837e-06
Iter: 269 loss: 1.16128297e-06
Iter: 270 loss: 1.15989224e-06
Iter: 271 loss: 1.15942385e-06
Iter: 272 loss: 1.16374531e-06
Iter: 273 loss: 1.15938951e-06
Iter: 274 loss: 1.15911985e-06
Iter: 275 loss: 1.15911212e-06
Iter: 276 loss: 1.15891953e-06
Iter: 277 loss: 1.15839794e-06
Iter: 278 loss: 1.16514184e-06
Iter: 279 loss: 1.15838111e-06
Iter: 280 loss: 1.15825742e-06
Iter: 281 loss: 1.15810712e-06
Iter: 282 loss: 1.15791295e-06
Iter: 283 loss: 1.15784769e-06
Iter: 284 loss: 1.15776379e-06
Iter: 285 loss: 1.1574698e-06
Iter: 286 loss: 1.15795081e-06
Iter: 287 loss: 1.15730927e-06
Iter: 288 loss: 1.15709281e-06
Iter: 289 loss: 1.15676266e-06
Iter: 290 loss: 1.15677199e-06
Iter: 291 loss: 1.15665512e-06
Iter: 292 loss: 1.15655553e-06
Iter: 293 loss: 1.15632088e-06
Iter: 294 loss: 1.1559016e-06
Iter: 295 loss: 1.16417789e-06
Iter: 296 loss: 1.15594094e-06
Iter: 297 loss: 1.15555486e-06
Iter: 298 loss: 1.15545413e-06
Iter: 299 loss: 1.15523142e-06
Iter: 300 loss: 1.155316e-06
Iter: 301 loss: 1.15507646e-06
Iter: 302 loss: 1.15494174e-06
Iter: 303 loss: 1.15449666e-06
Iter: 304 loss: 1.15866465e-06
Iter: 305 loss: 1.15450143e-06
Iter: 306 loss: 1.15418459e-06
Iter: 307 loss: 1.15679973e-06
Iter: 308 loss: 1.15416651e-06
Iter: 309 loss: 1.15386308e-06
Iter: 310 loss: 1.15671583e-06
Iter: 311 loss: 1.15386013e-06
Iter: 312 loss: 1.15373496e-06
Iter: 313 loss: 1.15339071e-06
Iter: 314 loss: 1.15553951e-06
Iter: 315 loss: 1.15332455e-06
Iter: 316 loss: 1.15307273e-06
Iter: 317 loss: 1.15302396e-06
Iter: 318 loss: 1.15288401e-06
Iter: 319 loss: 1.1527718e-06
Iter: 320 loss: 1.15277419e-06
Iter: 321 loss: 1.1525467e-06
Iter: 322 loss: 1.15252647e-06
Iter: 323 loss: 1.15235434e-06
Iter: 324 loss: 1.15207217e-06
Iter: 325 loss: 1.15225907e-06
Iter: 326 loss: 1.15187231e-06
Iter: 327 loss: 1.15158423e-06
Iter: 328 loss: 1.1531082e-06
Iter: 329 loss: 1.15158286e-06
Iter: 330 loss: 1.15130865e-06
Iter: 331 loss: 1.1523e-06
Iter: 332 loss: 1.15122771e-06
Iter: 333 loss: 1.15104115e-06
Iter: 334 loss: 1.15072419e-06
Iter: 335 loss: 1.15076602e-06
Iter: 336 loss: 1.15064859e-06
Iter: 337 loss: 1.15056127e-06
Iter: 338 loss: 1.15041473e-06
Iter: 339 loss: 1.15019407e-06
Iter: 340 loss: 1.15015234e-06
Iter: 341 loss: 1.14995032e-06
Iter: 342 loss: 1.14996806e-06
Iter: 343 loss: 1.14975626e-06
Iter: 344 loss: 1.14958652e-06
Iter: 345 loss: 1.14953559e-06
Iter: 346 loss: 1.14938632e-06
Iter: 347 loss: 1.14934892e-06
Iter: 348 loss: 1.1491893e-06
Iter: 349 loss: 1.14908403e-06
Iter: 350 loss: 1.14903355e-06
Iter: 351 loss: 1.14882948e-06
Iter: 352 loss: 1.14923819e-06
Iter: 353 loss: 1.14868703e-06
Iter: 354 loss: 1.14846353e-06
Iter: 355 loss: 1.14848854e-06
Iter: 356 loss: 1.14832301e-06
Iter: 357 loss: 1.14805e-06
Iter: 358 loss: 1.14892805e-06
Iter: 359 loss: 1.14796683e-06
Iter: 360 loss: 1.14763304e-06
Iter: 361 loss: 1.14840577e-06
Iter: 362 loss: 1.1475214e-06
Iter: 363 loss: 1.14726834e-06
Iter: 364 loss: 1.14703266e-06
Iter: 365 loss: 1.14699606e-06
Iter: 366 loss: 1.14662339e-06
Iter: 367 loss: 1.1470654e-06
Iter: 368 loss: 1.1464565e-06
Iter: 369 loss: 1.14642398e-06
Iter: 370 loss: 1.14627437e-06
Iter: 371 loss: 1.14618069e-06
Iter: 372 loss: 1.14589898e-06
Iter: 373 loss: 1.15053524e-06
Iter: 374 loss: 1.14589238e-06
Iter: 375 loss: 1.14569184e-06
Iter: 376 loss: 1.14567479e-06
Iter: 377 loss: 1.1455744e-06
Iter: 378 loss: 1.14536294e-06
Iter: 379 loss: 1.14537806e-06
Iter: 380 loss: 1.14519094e-06
Iter: 381 loss: 1.14518218e-06
Iter: 382 loss: 1.14506088e-06
Iter: 383 loss: 1.1448999e-06
Iter: 384 loss: 1.14874433e-06
Iter: 385 loss: 1.14486204e-06
Iter: 386 loss: 1.14451541e-06
Iter: 387 loss: 1.14488057e-06
Iter: 388 loss: 1.14432339e-06
Iter: 389 loss: 1.14401394e-06
Iter: 390 loss: 1.14401121e-06
Iter: 391 loss: 1.1439065e-06
Iter: 392 loss: 1.14389877e-06
Iter: 393 loss: 1.14379895e-06
Iter: 394 loss: 1.14358113e-06
Iter: 395 loss: 1.14404008e-06
Iter: 396 loss: 1.14349234e-06
Iter: 397 loss: 1.14335307e-06
Iter: 398 loss: 1.14297893e-06
Iter: 399 loss: 1.15044213e-06
Iter: 400 loss: 1.14298837e-06
Iter: 401 loss: 1.14287673e-06
Iter: 402 loss: 1.14282125e-06
Iter: 403 loss: 1.1425725e-06
Iter: 404 loss: 1.14238514e-06
Iter: 405 loss: 1.14226725e-06
Iter: 406 loss: 1.14208581e-06
Iter: 407 loss: 1.14304237e-06
Iter: 408 loss: 1.14205818e-06
Iter: 409 loss: 1.14179863e-06
Iter: 410 loss: 1.14234797e-06
Iter: 411 loss: 1.14166801e-06
Iter: 412 loss: 1.14152476e-06
Iter: 413 loss: 1.14196905e-06
Iter: 414 loss: 1.1414636e-06
Iter: 415 loss: 1.1411596e-06
Iter: 416 loss: 1.14170666e-06
Iter: 417 loss: 1.14104114e-06
Iter: 418 loss: 1.14090903e-06
Iter: 419 loss: 1.14075e-06
Iter: 420 loss: 1.14069894e-06
Iter: 421 loss: 1.14050135e-06
Iter: 422 loss: 1.14052182e-06
Iter: 423 loss: 1.14039631e-06
Iter: 424 loss: 1.14018553e-06
Iter: 425 loss: 1.1402135e-06
Iter: 426 loss: 1.14002546e-06
Iter: 427 loss: 1.14001864e-06
Iter: 428 loss: 1.13992166e-06
Iter: 429 loss: 1.13964677e-06
Iter: 430 loss: 1.14229692e-06
Iter: 431 loss: 1.1396296e-06
Iter: 432 loss: 1.13939052e-06
Iter: 433 loss: 1.14043019e-06
Iter: 434 loss: 1.13932663e-06
Iter: 435 loss: 1.13908618e-06
Iter: 436 loss: 1.14186435e-06
Iter: 437 loss: 1.139083e-06
Iter: 438 loss: 1.13893066e-06
Iter: 439 loss: 1.13881401e-06
Iter: 440 loss: 1.1387757e-06
Iter: 441 loss: 1.13857732e-06
Iter: 442 loss: 1.14157433e-06
Iter: 443 loss: 1.13855288e-06
Iter: 444 loss: 1.13840929e-06
Iter: 445 loss: 1.13881015e-06
Iter: 446 loss: 1.13835767e-06
Iter: 447 loss: 1.13820545e-06
Iter: 448 loss: 1.139201e-06
Iter: 449 loss: 1.13821886e-06
Iter: 450 loss: 1.13812837e-06
Iter: 451 loss: 1.13795011e-06
Iter: 452 loss: 1.13795772e-06
Iter: 453 loss: 1.1377831e-06
Iter: 454 loss: 1.13980479e-06
Iter: 455 loss: 1.13780141e-06
Iter: 456 loss: 1.13765941e-06
Iter: 457 loss: 1.13829287e-06
Iter: 458 loss: 1.13755084e-06
Iter: 459 loss: 1.1374425e-06
Iter: 460 loss: 1.13772012e-06
Iter: 461 loss: 1.1374126e-06
Iter: 462 loss: 1.1373329e-06
Iter: 463 loss: 1.13696274e-06
Iter: 464 loss: 1.138568e-06
Iter: 465 loss: 1.13691e-06
Iter: 466 loss: 1.1365687e-06
Iter: 467 loss: 1.13659053e-06
Iter: 468 loss: 1.13641772e-06
Iter: 469 loss: 1.13645285e-06
Iter: 470 loss: 1.13632711e-06
Iter: 471 loss: 1.13610167e-06
Iter: 472 loss: 1.13856186e-06
Iter: 473 loss: 1.13609121e-06
Iter: 474 loss: 1.13589851e-06
Iter: 475 loss: 1.13587498e-06
Iter: 476 loss: 1.1357622e-06
Iter: 477 loss: 1.13580495e-06
Iter: 478 loss: 1.13566898e-06
Iter: 479 loss: 1.13546855e-06
Iter: 480 loss: 1.13633837e-06
Iter: 481 loss: 1.13538658e-06
Iter: 482 loss: 1.13523038e-06
Iter: 483 loss: 1.13495435e-06
Iter: 484 loss: 1.13494616e-06
Iter: 485 loss: 1.13471128e-06
Iter: 486 loss: 1.13568717e-06
Iter: 487 loss: 1.13467604e-06
Iter: 488 loss: 1.1344988e-06
Iter: 489 loss: 1.13451767e-06
Iter: 490 loss: 1.13438853e-06
Iter: 491 loss: 1.13431702e-06
Iter: 492 loss: 1.13427268e-06
Iter: 493 loss: 1.13406463e-06
Iter: 494 loss: 1.13372971e-06
Iter: 495 loss: 1.13373324e-06
Iter: 496 loss: 1.1334381e-06
Iter: 497 loss: 1.13495241e-06
Iter: 498 loss: 1.13337774e-06
Iter: 499 loss: 1.13335705e-06
Iter: 500 loss: 1.13327781e-06
Iter: 501 loss: 1.13321198e-06
Iter: 502 loss: 1.13316014e-06
Iter: 503 loss: 1.13308079e-06
Iter: 504 loss: 1.13299598e-06
Iter: 505 loss: 1.13334454e-06
Iter: 506 loss: 1.13297153e-06
Iter: 507 loss: 1.13288729e-06
Iter: 508 loss: 1.13292936e-06
Iter: 509 loss: 1.13276906e-06
Iter: 510 loss: 1.132634e-06
Iter: 511 loss: 1.13406441e-06
Iter: 512 loss: 1.13265412e-06
Iter: 513 loss: 1.13253293e-06
Iter: 514 loss: 1.13234682e-06
Iter: 515 loss: 1.13694637e-06
Iter: 516 loss: 1.13233818e-06
Iter: 517 loss: 1.13215333e-06
Iter: 518 loss: 1.13243709e-06
Iter: 519 loss: 1.13201804e-06
Iter: 520 loss: 1.13189526e-06
Iter: 521 loss: 1.13191663e-06
Iter: 522 loss: 1.13175133e-06
Iter: 523 loss: 1.13225542e-06
Iter: 524 loss: 1.13173155e-06
Iter: 525 loss: 1.13164231e-06
Iter: 526 loss: 1.13153953e-06
Iter: 527 loss: 1.1315301e-06
Iter: 528 loss: 1.131315e-06
Iter: 529 loss: 1.13101271e-06
Iter: 530 loss: 1.13101305e-06
Iter: 531 loss: 1.13074168e-06
Iter: 532 loss: 1.13074066e-06
Iter: 533 loss: 1.13060014e-06
Iter: 534 loss: 1.13059775e-06
Iter: 535 loss: 1.13053272e-06
Iter: 536 loss: 1.13033047e-06
Iter: 537 loss: 1.13306385e-06
Iter: 538 loss: 1.13033593e-06
Iter: 539 loss: 1.13007377e-06
Iter: 540 loss: 1.13248916e-06
Iter: 541 loss: 1.1301006e-06
Iter: 542 loss: 1.12997066e-06
Iter: 543 loss: 1.13032172e-06
Iter: 544 loss: 1.12993439e-06
Iter: 545 loss: 1.12978262e-06
Iter: 546 loss: 1.12999521e-06
Iter: 547 loss: 1.12968621e-06
Iter: 548 loss: 1.1295482e-06
Iter: 549 loss: 1.12944235e-06
Iter: 550 loss: 1.12942905e-06
Iter: 551 loss: 1.1292334e-06
Iter: 552 loss: 1.12956207e-06
Iter: 553 loss: 1.12916791e-06
Iter: 554 loss: 1.12907878e-06
Iter: 555 loss: 1.12904877e-06
Iter: 556 loss: 1.12897806e-06
Iter: 557 loss: 1.12888654e-06
Iter: 558 loss: 1.12891917e-06
Iter: 559 loss: 1.12870305e-06
Iter: 560 loss: 1.12869429e-06
Iter: 561 loss: 1.12856833e-06
Iter: 562 loss: 1.12834334e-06
Iter: 563 loss: 1.12848261e-06
Iter: 564 loss: 1.12821249e-06
Iter: 565 loss: 1.12805344e-06
Iter: 566 loss: 1.12855025e-06
Iter: 567 loss: 1.12796806e-06
Iter: 568 loss: 1.12792668e-06
Iter: 569 loss: 1.12789053e-06
Iter: 570 loss: 1.1277873e-06
Iter: 571 loss: 1.12760051e-06
Iter: 572 loss: 1.13135798e-06
Iter: 573 loss: 1.12755674e-06
Iter: 574 loss: 1.12740759e-06
Iter: 575 loss: 1.1291196e-06
Iter: 576 loss: 1.12740929e-06
Iter: 577 loss: 1.12731027e-06
Iter: 578 loss: 1.12837438e-06
Iter: 579 loss: 1.12733596e-06
Iter: 580 loss: 1.1273e-06
Iter: 581 loss: 1.12715782e-06
Iter: 582 loss: 1.12898056e-06
Iter: 583 loss: 1.12717044e-06
Iter: 584 loss: 1.12703253e-06
Iter: 585 loss: 1.12739303e-06
Iter: 586 loss: 1.1270123e-06
Iter: 587 loss: 1.12686394e-06
Iter: 588 loss: 1.12686394e-06
Iter: 589 loss: 1.12679174e-06
Iter: 590 loss: 1.12664475e-06
Iter: 591 loss: 1.12665202e-06
Iter: 592 loss: 1.12649082e-06
Iter: 593 loss: 1.12720647e-06
Iter: 594 loss: 1.12643329e-06
Iter: 595 loss: 1.12637849e-06
Iter: 596 loss: 1.12634802e-06
Iter: 597 loss: 1.12628436e-06
Iter: 598 loss: 1.12617715e-06
Iter: 599 loss: 1.12619159e-06
Iter: 600 loss: 1.12603914e-06
Iter: 601 loss: 1.12581e-06
Iter: 602 loss: 1.13080978e-06
Iter: 603 loss: 1.12582347e-06
Iter: 604 loss: 1.12579937e-06
Iter: 605 loss: 1.12569296e-06
Iter: 606 loss: 1.12557916e-06
Iter: 607 loss: 1.12554699e-06
Iter: 608 loss: 1.12542943e-06
Iter: 609 loss: 1.12538783e-06
Iter: 610 loss: 1.12533735e-06
Iter: 611 loss: 1.12529938e-06
Iter: 612 loss: 1.12516909e-06
Iter: 613 loss: 1.1251559e-06
Iter: 614 loss: 1.12514726e-06
Iter: 615 loss: 1.12500038e-06
Iter: 616 loss: 1.12565692e-06
Iter: 617 loss: 1.12497673e-06
Iter: 618 loss: 1.12481894e-06
Iter: 619 loss: 1.12627754e-06
Iter: 620 loss: 1.12482599e-06
Iter: 621 loss: 1.1247223e-06
Iter: 622 loss: 1.12466626e-06
Iter: 623 loss: 1.12463044e-06
Iter: 624 loss: 1.12453631e-06
Iter: 625 loss: 1.12559678e-06
Iter: 626 loss: 1.12451312e-06
Iter: 627 loss: 1.12445139e-06
Iter: 628 loss: 1.12534849e-06
Iter: 629 loss: 1.12447913e-06
Iter: 630 loss: 1.12437215e-06
Iter: 631 loss: 1.12434793e-06
Iter: 632 loss: 1.12432167e-06
Iter: 633 loss: 1.12423174e-06
Iter: 634 loss: 1.12413488e-06
Iter: 635 loss: 1.12415387e-06
Iter: 636 loss: 1.12402461e-06
Iter: 637 loss: 1.12402904e-06
Iter: 638 loss: 1.12393013e-06
Iter: 639 loss: 1.12432576e-06
Iter: 640 loss: 1.12387863e-06
Iter: 641 loss: 1.12384782e-06
Iter: 642 loss: 1.12368025e-06
Iter: 643 loss: 1.12524845e-06
Iter: 644 loss: 1.12365615e-06
Iter: 645 loss: 1.12361954e-06
Iter: 646 loss: 1.12350813e-06
Iter: 647 loss: 1.12348994e-06
Iter: 648 loss: 1.12331543e-06
Iter: 649 loss: 1.12424584e-06
Iter: 650 loss: 1.12325438e-06
Iter: 651 loss: 1.12311238e-06
Iter: 652 loss: 1.12307725e-06
Iter: 653 loss: 1.12293981e-06
Iter: 654 loss: 1.1229447e-06
Iter: 655 loss: 1.12282487e-06
Iter: 656 loss: 1.12269254e-06
Iter: 657 loss: 1.1225851e-06
Iter: 658 loss: 1.12251723e-06
Iter: 659 loss: 1.12230259e-06
Iter: 660 loss: 1.1224638e-06
Iter: 661 loss: 1.12220471e-06
Iter: 662 loss: 1.12202497e-06
Iter: 663 loss: 1.12203247e-06
Iter: 664 loss: 1.12188332e-06
Iter: 665 loss: 1.1216132e-06
Iter: 666 loss: 1.12629141e-06
Iter: 667 loss: 1.12161615e-06
Iter: 668 loss: 1.12129726e-06
Iter: 669 loss: 1.12179305e-06
Iter: 670 loss: 1.12116618e-06
Iter: 671 loss: 1.12085627e-06
Iter: 672 loss: 1.12292037e-06
Iter: 673 loss: 1.12074986e-06
Iter: 674 loss: 1.12065788e-06
Iter: 675 loss: 1.12063424e-06
Iter: 676 loss: 1.12051225e-06
Iter: 677 loss: 1.12030307e-06
Iter: 678 loss: 1.12030648e-06
Iter: 679 loss: 1.12018392e-06
Iter: 680 loss: 1.12019495e-06
Iter: 681 loss: 1.12008183e-06
Iter: 682 loss: 1.11975055e-06
Iter: 683 loss: 1.12358202e-06
Iter: 684 loss: 1.11970007e-06
Iter: 685 loss: 1.11949987e-06
Iter: 686 loss: 1.1220784e-06
Iter: 687 loss: 1.11950976e-06
Iter: 688 loss: 1.11933582e-06
Iter: 689 loss: 1.11959571e-06
Iter: 690 loss: 1.11925419e-06
Iter: 691 loss: 1.11905206e-06
Iter: 692 loss: 1.12051384e-06
Iter: 693 loss: 1.11907343e-06
Iter: 694 loss: 1.11887834e-06
Iter: 695 loss: 1.11899055e-06
Iter: 696 loss: 1.1187791e-06
Iter: 697 loss: 1.11856491e-06
Iter: 698 loss: 1.1190948e-06
Iter: 699 loss: 1.11850431e-06
Iter: 700 loss: 1.11831491e-06
Iter: 701 loss: 1.11821157e-06
Iter: 702 loss: 1.11818395e-06
Iter: 703 loss: 1.11803433e-06
Iter: 704 loss: 1.11869713e-06
Iter: 705 loss: 1.11800512e-06
Iter: 706 loss: 1.11785937e-06
Iter: 707 loss: 1.11851568e-06
Iter: 708 loss: 1.11785698e-06
Iter: 709 loss: 1.11769282e-06
Iter: 710 loss: 1.11903637e-06
Iter: 711 loss: 1.11769464e-06
Iter: 712 loss: 1.11762097e-06
Iter: 713 loss: 1.11753252e-06
Iter: 714 loss: 1.1175182e-06
Iter: 715 loss: 1.11749034e-06
Iter: 716 loss: 1.1174476e-06
Iter: 717 loss: 1.11740496e-06
Iter: 718 loss: 1.1172757e-06
Iter: 719 loss: 1.11764189e-06
Iter: 720 loss: 1.11719373e-06
Iter: 721 loss: 1.11704298e-06
Iter: 722 loss: 1.11777399e-06
Iter: 723 loss: 1.11698671e-06
Iter: 724 loss: 1.11682357e-06
Iter: 725 loss: 1.11896213e-06
Iter: 726 loss: 1.11680106e-06
Iter: 727 loss: 1.11670738e-06
Iter: 728 loss: 1.11671443e-06
Iter: 729 loss: 1.11662871e-06
Iter: 730 loss: 1.11644817e-06
Iter: 731 loss: 1.11709505e-06
Iter: 732 loss: 1.11644476e-06
Iter: 733 loss: 1.11624308e-06
Iter: 734 loss: 1.11674876e-06
Iter: 735 loss: 1.11619534e-06
Iter: 736 loss: 1.11610939e-06
Iter: 737 loss: 1.11589759e-06
Iter: 738 loss: 1.11934992e-06
Iter: 739 loss: 1.1158628e-06
Iter: 740 loss: 1.11563816e-06
Iter: 741 loss: 1.11711893e-06
Iter: 742 loss: 1.11564123e-06
Iter: 743 loss: 1.11539578e-06
Iter: 744 loss: 1.1154342e-06
Iter: 745 loss: 1.11530767e-06
Iter: 746 loss: 1.11568261e-06
Iter: 747 loss: 1.11527538e-06
Iter: 748 loss: 1.11515533e-06
Iter: 749 loss: 1.11538225e-06
Iter: 750 loss: 1.11512088e-06
Iter: 751 loss: 1.11497354e-06
Iter: 752 loss: 1.11554709e-06
Iter: 753 loss: 1.11497093e-06
Iter: 754 loss: 1.1149059e-06
Iter: 755 loss: 1.1147763e-06
Iter: 756 loss: 1.11640975e-06
Iter: 757 loss: 1.11473946e-06
Iter: 758 loss: 1.11462316e-06
Iter: 759 loss: 1.11508734e-06
Iter: 760 loss: 1.11456575e-06
Iter: 761 loss: 1.11431825e-06
Iter: 762 loss: 1.11628378e-06
Iter: 763 loss: 1.11436327e-06
Iter: 764 loss: 1.11422855e-06
Iter: 765 loss: 1.11429677e-06
Iter: 766 loss: 1.11408519e-06
Iter: 767 loss: 1.11396707e-06
Iter: 768 loss: 1.11492477e-06
Iter: 769 loss: 1.11392876e-06
Iter: 770 loss: 1.11381428e-06
Iter: 771 loss: 1.11380541e-06
Iter: 772 loss: 1.11369263e-06
Iter: 773 loss: 1.11353643e-06
Iter: 774 loss: 1.11342501e-06
Iter: 775 loss: 1.11339182e-06
Iter: 776 loss: 1.11320901e-06
Iter: 777 loss: 1.11399936e-06
Iter: 778 loss: 1.1131234e-06
Iter: 779 loss: 1.11287488e-06
Iter: 780 loss: 1.11521649e-06
Iter: 781 loss: 1.11291342e-06
Iter: 782 loss: 1.11284271e-06
Iter: 783 loss: 1.11284396e-06
Iter: 784 loss: 1.11273266e-06
Iter: 785 loss: 1.11261352e-06
Iter: 786 loss: 1.11402755e-06
Iter: 787 loss: 1.11261556e-06
Iter: 788 loss: 1.11250392e-06
Iter: 789 loss: 1.11243173e-06
Iter: 790 loss: 1.11240649e-06
Iter: 791 loss: 1.11227268e-06
Iter: 792 loss: 1.11223915e-06
Iter: 793 loss: 1.11214831e-06
Iter: 794 loss: 1.11198312e-06
Iter: 795 loss: 1.1131915e-06
Iter: 796 loss: 1.11199131e-06
Iter: 797 loss: 1.1117819e-06
Iter: 798 loss: 1.1126657e-06
Iter: 799 loss: 1.11176325e-06
Iter: 800 loss: 1.11165605e-06
Iter: 801 loss: 1.11175916e-06
Iter: 802 loss: 1.11157908e-06
Iter: 803 loss: 1.1114671e-06
Iter: 804 loss: 1.11271368e-06
Iter: 805 loss: 1.11143913e-06
Iter: 806 loss: 1.11139821e-06
Iter: 807 loss: 1.11122358e-06
Iter: 808 loss: 1.11356496e-06
Iter: 809 loss: 1.11117845e-06
Iter: 810 loss: 1.1110476e-06
Iter: 811 loss: 1.11165036e-06
Iter: 812 loss: 1.11097586e-06
Iter: 813 loss: 1.11090685e-06
Iter: 814 loss: 1.11091151e-06
Iter: 815 loss: 1.11082636e-06
Iter: 816 loss: 1.11068357e-06
Iter: 817 loss: 1.11372128e-06
Iter: 818 loss: 1.11067789e-06
Iter: 819 loss: 1.11062764e-06
Iter: 820 loss: 1.11056829e-06
Iter: 821 loss: 1.11047359e-06
Iter: 822 loss: 1.11047746e-06
Iter: 823 loss: 1.11039913e-06
Iter: 824 loss: 1.11028908e-06
Iter: 825 loss: 1.11023178e-06
Iter: 826 loss: 1.11016664e-06
Iter: 827 loss: 1.11001907e-06
Iter: 828 loss: 1.11046813e-06
Iter: 829 loss: 1.10998042e-06
Iter: 830 loss: 1.10982592e-06
Iter: 831 loss: 1.11121904e-06
Iter: 832 loss: 1.10979192e-06
Iter: 833 loss: 1.10967972e-06
Iter: 834 loss: 1.1099155e-06
Iter: 835 loss: 1.10964072e-06
Iter: 836 loss: 1.10953522e-06
Iter: 837 loss: 1.11005329e-06
Iter: 838 loss: 1.10950009e-06
Iter: 839 loss: 1.10938959e-06
Iter: 840 loss: 1.10923884e-06
Iter: 841 loss: 1.11211602e-06
Iter: 842 loss: 1.1092161e-06
Iter: 843 loss: 1.10901919e-06
Iter: 844 loss: 1.10984706e-06
Iter: 845 loss: 1.1089603e-06
Iter: 846 loss: 1.10884616e-06
Iter: 847 loss: 1.10885594e-06
Iter: 848 loss: 1.10874828e-06
Iter: 849 loss: 1.10860037e-06
Iter: 850 loss: 1.10859878e-06
Iter: 851 loss: 1.10848771e-06
Iter: 852 loss: 1.10848714e-06
Iter: 853 loss: 1.108324e-06
Iter: 854 loss: 1.10858412e-06
Iter: 855 loss: 1.1083373e-06
Iter: 856 loss: 1.108226e-06
Iter: 857 loss: 1.10813176e-06
Iter: 858 loss: 1.10817689e-06
Iter: 859 loss: 1.10793769e-06
Iter: 860 loss: 1.10814415e-06
Iter: 861 loss: 1.10786175e-06
Iter: 862 loss: 1.10770236e-06
Iter: 863 loss: 1.10768713e-06
Iter: 864 loss: 1.1075964e-06
Iter: 865 loss: 1.107662e-06
Iter: 866 loss: 1.10754468e-06
Iter: 867 loss: 1.10743304e-06
Iter: 868 loss: 1.1080889e-06
Iter: 869 loss: 1.10744679e-06
Iter: 870 loss: 1.10732742e-06
Iter: 871 loss: 1.10719714e-06
Iter: 872 loss: 1.10719122e-06
Iter: 873 loss: 1.10698898e-06
Iter: 874 loss: 1.10756991e-06
Iter: 875 loss: 1.10695032e-06
Iter: 876 loss: 1.10681344e-06
Iter: 877 loss: 1.10678798e-06
Iter: 878 loss: 1.10674421e-06
Iter: 879 loss: 1.10665417e-06
Iter: 880 loss: 1.10658596e-06
Iter: 881 loss: 1.10648421e-06
Iter: 882 loss: 1.10723545e-06
Iter: 883 loss: 1.10646954e-06
Iter: 884 loss: 1.1062981e-06
Iter: 885 loss: 1.10686676e-06
Iter: 886 loss: 1.10626706e-06
Iter: 887 loss: 1.10613246e-06
Iter: 888 loss: 1.10609426e-06
Iter: 889 loss: 1.10603469e-06
Iter: 890 loss: 1.10587052e-06
Iter: 891 loss: 1.10594738e-06
Iter: 892 loss: 1.10575343e-06
Iter: 893 loss: 1.10565543e-06
Iter: 894 loss: 1.1056211e-06
Iter: 895 loss: 1.10552935e-06
Iter: 896 loss: 1.10551446e-06
Iter: 897 loss: 1.10544363e-06
Iter: 898 loss: 1.10532255e-06
Iter: 899 loss: 1.10610233e-06
Iter: 900 loss: 1.10529356e-06
Iter: 901 loss: 1.10517851e-06
Iter: 902 loss: 1.10498308e-06
Iter: 903 loss: 1.10495955e-06
Iter: 904 loss: 1.10477538e-06
Iter: 905 loss: 1.10531141e-06
Iter: 906 loss: 1.10470637e-06
Iter: 907 loss: 1.10458916e-06
Iter: 908 loss: 1.10460235e-06
Iter: 909 loss: 1.10444989e-06
Iter: 910 loss: 1.10444637e-06
Iter: 911 loss: 1.10435303e-06
Iter: 912 loss: 1.10424708e-06
Iter: 913 loss: 1.10446229e-06
Iter: 914 loss: 1.10417136e-06
Iter: 915 loss: 1.10402561e-06
Iter: 916 loss: 1.10578458e-06
Iter: 917 loss: 1.10401129e-06
Iter: 918 loss: 1.10393557e-06
Iter: 919 loss: 1.10389578e-06
Iter: 920 loss: 1.10386645e-06
Iter: 921 loss: 1.1036667e-06
Iter: 922 loss: 1.10375686e-06
Iter: 923 loss: 1.10356336e-06
Iter: 924 loss: 1.10347924e-06
Iter: 925 loss: 1.1034457e-06
Iter: 926 loss: 1.10336055e-06
Iter: 927 loss: 1.1033278e-06
Iter: 928 loss: 1.10326414e-06
Iter: 929 loss: 1.10312749e-06
Iter: 930 loss: 1.10391375e-06
Iter: 931 loss: 1.1030935e-06
Iter: 932 loss: 1.10300425e-06
Iter: 933 loss: 1.10293161e-06
Iter: 934 loss: 1.10286317e-06
Iter: 935 loss: 1.10270184e-06
Iter: 936 loss: 1.10297708e-06
Iter: 937 loss: 1.10265478e-06
Iter: 938 loss: 1.10250858e-06
Iter: 939 loss: 1.10427845e-06
Iter: 940 loss: 1.1025146e-06
Iter: 941 loss: 1.10239046e-06
Iter: 942 loss: 1.10253973e-06
Iter: 943 loss: 1.10233577e-06
Iter: 944 loss: 1.10225346e-06
Iter: 945 loss: 1.1023069e-06
Iter: 946 loss: 1.10217866e-06
Iter: 947 loss: 1.10209737e-06
Iter: 948 loss: 1.10210397e-06
Iter: 949 loss: 1.10200358e-06
Iter: 950 loss: 1.10193525e-06
Iter: 951 loss: 1.10194424e-06
Iter: 952 loss: 1.10179667e-06
Iter: 953 loss: 1.10186124e-06
Iter: 954 loss: 1.10167559e-06
Iter: 955 loss: 1.10159363e-06
Iter: 956 loss: 1.10297162e-06
Iter: 957 loss: 1.10159112e-06
Iter: 958 loss: 1.10151041e-06
Iter: 959 loss: 1.10148255e-06
Iter: 960 loss: 1.10139194e-06
Iter: 961 loss: 1.10124984e-06
Iter: 962 loss: 1.10199585e-06
Iter: 963 loss: 1.10122483e-06
Iter: 964 loss: 1.10110159e-06
Iter: 965 loss: 1.10102621e-06
Iter: 966 loss: 1.10097301e-06
Iter: 967 loss: 1.10077008e-06
Iter: 968 loss: 1.10084875e-06
Iter: 969 loss: 1.10071073e-06
Iter: 970 loss: 1.10052e-06
Iter: 971 loss: 1.10213171e-06
Iter: 972 loss: 1.10053986e-06
Iter: 973 loss: 1.10034989e-06
Iter: 974 loss: 1.10080236e-06
Iter: 975 loss: 1.10031544e-06
Iter: 976 loss: 1.10013264e-06
Iter: 977 loss: 1.1000626e-06
Iter: 978 loss: 1.10001531e-06
Iter: 979 loss: 1.09990799e-06
Iter: 980 loss: 1.09987684e-06
Iter: 981 loss: 1.09978203e-06
Iter: 982 loss: 1.09966163e-06
Iter: 983 loss: 1.10324049e-06
Iter: 984 loss: 1.09962275e-06
Iter: 985 loss: 1.09940959e-06
Iter: 986 loss: 1.09984853e-06
Iter: 987 loss: 1.09934672e-06
Iter: 988 loss: 1.09917357e-06
Iter: 989 loss: 1.10018698e-06
Iter: 990 loss: 1.09916436e-06
Iter: 991 loss: 1.0989545e-06
Iter: 992 loss: 1.09927225e-06
Iter: 993 loss: 1.09882444e-06
Iter: 994 loss: 1.09868301e-06
Iter: 995 loss: 1.09939401e-06
Iter: 996 loss: 1.09863072e-06
Iter: 997 loss: 1.09843654e-06
Iter: 998 loss: 1.09863936e-06
Iter: 999 loss: 1.09824987e-06
Iter: 1000 loss: 1.09813595e-06
Iter: 1001 loss: 1.09813732e-06
Iter: 1002 loss: 1.09794564e-06
Iter: 1003 loss: 1.09777864e-06
Iter: 1004 loss: 1.09925463e-06
Iter: 1005 loss: 1.09771167e-06
Iter: 1006 loss: 1.0975167e-06
Iter: 1007 loss: 1.09863913e-06
Iter: 1008 loss: 1.0975059e-06
Iter: 1009 loss: 1.09738221e-06
Iter: 1010 loss: 1.09730377e-06
Iter: 1011 loss: 1.09726864e-06
Iter: 1012 loss: 1.09710504e-06
Iter: 1013 loss: 1.09709435e-06
Iter: 1014 loss: 1.09701841e-06
Iter: 1015 loss: 1.09683526e-06
Iter: 1016 loss: 1.10095391e-06
Iter: 1017 loss: 1.0968181e-06
Iter: 1018 loss: 1.09665689e-06
Iter: 1019 loss: 1.09726523e-06
Iter: 1020 loss: 1.0965764e-06
Iter: 1021 loss: 1.09645202e-06
Iter: 1022 loss: 1.0970914e-06
Iter: 1023 loss: 1.09642565e-06
Iter: 1024 loss: 1.09625262e-06
Iter: 1025 loss: 1.09690359e-06
Iter: 1026 loss: 1.09623011e-06
Iter: 1027 loss: 1.0960855e-06
Iter: 1028 loss: 1.09632174e-06
Iter: 1029 loss: 1.09605139e-06
Iter: 1030 loss: 1.09589348e-06
Iter: 1031 loss: 1.09606822e-06
Iter: 1032 loss: 1.0957699e-06
Iter: 1033 loss: 1.09564849e-06
Iter: 1034 loss: 1.09567395e-06
Iter: 1035 loss: 1.09553457e-06
Iter: 1036 loss: 1.09531447e-06
Iter: 1037 loss: 1.0960772e-06
Iter: 1038 loss: 1.09529253e-06
Iter: 1039 loss: 1.09512268e-06
Iter: 1040 loss: 1.09655457e-06
Iter: 1041 loss: 1.09512871e-06
Iter: 1042 loss: 1.09500252e-06
Iter: 1043 loss: 1.09492953e-06
Iter: 1044 loss: 1.09486575e-06
Iter: 1045 loss: 1.09476662e-06
Iter: 1046 loss: 1.09476855e-06
Iter: 1047 loss: 1.0946552e-06
Iter: 1048 loss: 1.0944799e-06
Iter: 1049 loss: 1.09449331e-06
Iter: 1050 loss: 1.0943329e-06
Iter: 1051 loss: 1.09496432e-06
Iter: 1052 loss: 1.09424343e-06
Iter: 1053 loss: 1.09413929e-06
Iter: 1054 loss: 1.09439134e-06
Iter: 1055 loss: 1.09406142e-06
Iter: 1056 loss: 1.09389021e-06
Iter: 1057 loss: 1.09496614e-06
Iter: 1058 loss: 1.09384121e-06
Iter: 1059 loss: 1.09372183e-06
Iter: 1060 loss: 1.09402436e-06
Iter: 1061 loss: 1.09366601e-06
Iter: 1062 loss: 1.09353209e-06
Iter: 1063 loss: 1.09374946e-06
Iter: 1064 loss: 1.09346433e-06
Iter: 1065 loss: 1.09332325e-06
Iter: 1066 loss: 1.09323685e-06
Iter: 1067 loss: 1.0932049e-06
Iter: 1068 loss: 1.09295502e-06
Iter: 1069 loss: 1.09344e-06
Iter: 1070 loss: 1.09289795e-06
Iter: 1071 loss: 1.09270491e-06
Iter: 1072 loss: 1.09520488e-06
Iter: 1073 loss: 1.09270968e-06
Iter: 1074 loss: 1.09258008e-06
Iter: 1075 loss: 1.09249652e-06
Iter: 1076 loss: 1.09242546e-06
Iter: 1077 loss: 1.09234e-06
Iter: 1078 loss: 1.09229597e-06
Iter: 1079 loss: 1.0921691e-06
Iter: 1080 loss: 1.09202244e-06
Iter: 1081 loss: 1.09202119e-06
Iter: 1082 loss: 1.0917895e-06
Iter: 1083 loss: 1.09226607e-06
Iter: 1084 loss: 1.09177518e-06
Iter: 1085 loss: 1.0915702e-06
Iter: 1086 loss: 1.09239318e-06
Iter: 1087 loss: 1.09153871e-06
Iter: 1088 loss: 1.09134248e-06
Iter: 1089 loss: 1.09219536e-06
Iter: 1090 loss: 1.09131861e-06
Iter: 1091 loss: 1.09113512e-06
Iter: 1092 loss: 1.09146617e-06
Iter: 1093 loss: 1.09110329e-06
Iter: 1094 loss: 1.09091e-06
Iter: 1095 loss: 1.09133543e-06
Iter: 1096 loss: 1.09084738e-06
Iter: 1097 loss: 1.09068355e-06
Iter: 1098 loss: 1.09058965e-06
Iter: 1099 loss: 1.090512e-06
Iter: 1100 loss: 1.09030952e-06
Iter: 1101 loss: 1.09123823e-06
Iter: 1102 loss: 1.09021039e-06
Iter: 1103 loss: 1.09001735e-06
Iter: 1104 loss: 1.09005464e-06
Iter: 1105 loss: 1.08996028e-06
Iter: 1106 loss: 1.0898159e-06
Iter: 1107 loss: 1.08976337e-06
Iter: 1108 loss: 1.08966788e-06
Iter: 1109 loss: 1.09156224e-06
Iter: 1110 loss: 1.08961262e-06
Iter: 1111 loss: 1.08943391e-06
Iter: 1112 loss: 1.08937695e-06
Iter: 1113 loss: 1.08929203e-06
Iter: 1114 loss: 1.08912718e-06
Iter: 1115 loss: 1.08942254e-06
Iter: 1116 loss: 1.08902793e-06
Iter: 1117 loss: 1.08884558e-06
Iter: 1118 loss: 1.08920756e-06
Iter: 1119 loss: 1.08876384e-06
Iter: 1120 loss: 1.08847064e-06
Iter: 1121 loss: 1.09006157e-06
Iter: 1122 loss: 1.0884703e-06
Iter: 1123 loss: 1.08827476e-06
Iter: 1124 loss: 1.08853317e-06
Iter: 1125 loss: 1.08821905e-06
Iter: 1126 loss: 1.08797178e-06
Iter: 1127 loss: 1.08864083e-06
Iter: 1128 loss: 1.08794143e-06
Iter: 1129 loss: 1.08773384e-06
Iter: 1130 loss: 1.08758286e-06
Iter: 1131 loss: 1.08755489e-06
Iter: 1132 loss: 1.08727613e-06
Iter: 1133 loss: 1.08816903e-06
Iter: 1134 loss: 1.08723486e-06
Iter: 1135 loss: 1.08700851e-06
Iter: 1136 loss: 1.08964264e-06
Iter: 1137 loss: 1.08697168e-06
Iter: 1138 loss: 1.08680888e-06
Iter: 1139 loss: 1.08670474e-06
Iter: 1140 loss: 1.08664699e-06
Iter: 1141 loss: 1.08647271e-06
Iter: 1142 loss: 1.08924144e-06
Iter: 1143 loss: 1.08646907e-06
Iter: 1144 loss: 1.08626421e-06
Iter: 1145 loss: 1.08632048e-06
Iter: 1146 loss: 1.0861429e-06
Iter: 1147 loss: 1.08593053e-06
Iter: 1148 loss: 1.08607901e-06
Iter: 1149 loss: 1.08579707e-06
Iter: 1150 loss: 1.08557833e-06
Iter: 1151 loss: 1.08595248e-06
Iter: 1152 loss: 1.08546942e-06
Iter: 1153 loss: 1.08522863e-06
Iter: 1154 loss: 1.08837628e-06
Iter: 1155 loss: 1.08526035e-06
Iter: 1156 loss: 1.0850996e-06
Iter: 1157 loss: 1.08511426e-06
Iter: 1158 loss: 1.08500569e-06
Iter: 1159 loss: 1.08477525e-06
Iter: 1160 loss: 1.08612608e-06
Iter: 1161 loss: 1.084779e-06
Iter: 1162 loss: 1.08465451e-06
Iter: 1163 loss: 1.08446852e-06
Iter: 1164 loss: 1.08937e-06
Iter: 1165 loss: 1.08447489e-06
Iter: 1166 loss: 1.08423592e-06
Iter: 1167 loss: 1.08592133e-06
Iter: 1168 loss: 1.08421898e-06
Iter: 1169 loss: 1.08407085e-06
Iter: 1170 loss: 1.08542838e-06
Iter: 1171 loss: 1.08403674e-06
Iter: 1172 loss: 1.08390986e-06
Iter: 1173 loss: 1.08381892e-06
Iter: 1174 loss: 1.08378481e-06
Iter: 1175 loss: 1.08358176e-06
Iter: 1176 loss: 1.0851079e-06
Iter: 1177 loss: 1.08357722e-06
Iter: 1178 loss: 1.08341123e-06
Iter: 1179 loss: 1.08366373e-06
Iter: 1180 loss: 1.08325639e-06
Iter: 1181 loss: 1.08313861e-06
Iter: 1182 loss: 1.08317931e-06
Iter: 1183 loss: 1.08302834e-06
Iter: 1184 loss: 1.08278539e-06
Iter: 1185 loss: 1.08286849e-06
Iter: 1186 loss: 1.08265601e-06
Iter: 1187 loss: 1.08240363e-06
Iter: 1188 loss: 1.08243034e-06
Iter: 1189 loss: 1.08225095e-06
Iter: 1190 loss: 1.08223935e-06
Iter: 1191 loss: 1.08213749e-06
Iter: 1192 loss: 1.0819324e-06
Iter: 1193 loss: 1.08350036e-06
Iter: 1194 loss: 1.08191409e-06
Iter: 1195 loss: 1.08174856e-06
Iter: 1196 loss: 1.08156576e-06
Iter: 1197 loss: 1.08156019e-06
Iter: 1198 loss: 1.08129348e-06
Iter: 1199 loss: 1.08333074e-06
Iter: 1200 loss: 1.08125596e-06
Iter: 1201 loss: 1.08112067e-06
Iter: 1202 loss: 1.08315908e-06
Iter: 1203 loss: 1.08114045e-06
Iter: 1204 loss: 1.08100835e-06
Iter: 1205 loss: 1.08078461e-06
Iter: 1206 loss: 1.08081827e-06
Iter: 1207 loss: 1.08062318e-06
Iter: 1208 loss: 1.08315737e-06
Iter: 1209 loss: 1.08060976e-06
Iter: 1210 loss: 1.08042696e-06
Iter: 1211 loss: 1.08094764e-06
Iter: 1212 loss: 1.08035374e-06
Iter: 1213 loss: 1.08023096e-06
Iter: 1214 loss: 1.08022323e-06
Iter: 1215 loss: 1.08014933e-06
Iter: 1216 loss: 1.07992264e-06
Iter: 1217 loss: 1.07999529e-06
Iter: 1218 loss: 1.07983078e-06
Iter: 1219 loss: 1.07964422e-06
Iter: 1220 loss: 1.07962387e-06
Iter: 1221 loss: 1.07951382e-06
Iter: 1222 loss: 1.07946448e-06
Iter: 1223 loss: 1.07940218e-06
Iter: 1224 loss: 1.07916253e-06
Iter: 1225 loss: 1.08118024e-06
Iter: 1226 loss: 1.0791764e-06
Iter: 1227 loss: 1.07906249e-06
Iter: 1228 loss: 1.07881533e-06
Iter: 1229 loss: 1.08415975e-06
Iter: 1230 loss: 1.07880192e-06
Iter: 1231 loss: 1.07848246e-06
Iter: 1232 loss: 1.08010454e-06
Iter: 1233 loss: 1.07846245e-06
Iter: 1234 loss: 1.07832318e-06
Iter: 1235 loss: 1.0782644e-06
Iter: 1236 loss: 1.0781456e-06
Iter: 1237 loss: 1.07799929e-06
Iter: 1238 loss: 1.07794585e-06
Iter: 1239 loss: 1.07774304e-06
Iter: 1240 loss: 1.07949586e-06
Iter: 1241 loss: 1.07775577e-06
Iter: 1242 loss: 1.07752692e-06
Iter: 1243 loss: 1.07826054e-06
Iter: 1244 loss: 1.07745427e-06
Iter: 1245 loss: 1.07734513e-06
Iter: 1246 loss: 1.07726464e-06
Iter: 1247 loss: 1.07720609e-06
Iter: 1248 loss: 1.07699736e-06
Iter: 1249 loss: 1.07708581e-06
Iter: 1250 loss: 1.07685719e-06
Iter: 1251 loss: 1.07669257e-06
Iter: 1252 loss: 1.07666256e-06
Iter: 1253 loss: 1.07650567e-06
Iter: 1254 loss: 1.07639869e-06
Iter: 1255 loss: 1.07635549e-06
Iter: 1256 loss: 1.07617734e-06
Iter: 1257 loss: 1.07856363e-06
Iter: 1258 loss: 1.0761969e-06
Iter: 1259 loss: 1.0760441e-06
Iter: 1260 loss: 1.0757874e-06
Iter: 1261 loss: 1.08027e-06
Iter: 1262 loss: 1.07578512e-06
Iter: 1263 loss: 1.07550568e-06
Iter: 1264 loss: 1.07690971e-06
Iter: 1265 loss: 1.07546657e-06
Iter: 1266 loss: 1.07531309e-06
Iter: 1267 loss: 1.07530911e-06
Iter: 1268 loss: 1.07517826e-06
Iter: 1269 loss: 1.07503399e-06
Iter: 1270 loss: 1.0750357e-06
Iter: 1271 loss: 1.07484391e-06
Iter: 1272 loss: 1.07601886e-06
Iter: 1273 loss: 1.07484038e-06
Iter: 1274 loss: 1.0745955e-06
Iter: 1275 loss: 1.0753605e-06
Iter: 1276 loss: 1.0745706e-06
Iter: 1277 loss: 1.07444464e-06
Iter: 1278 loss: 1.07443043e-06
Iter: 1279 loss: 1.07431742e-06
Iter: 1280 loss: 1.07409301e-06
Iter: 1281 loss: 1.0740323e-06
Iter: 1282 loss: 1.07396568e-06
Iter: 1283 loss: 1.07371443e-06
Iter: 1284 loss: 1.07368407e-06
Iter: 1285 loss: 1.07353958e-06
Iter: 1286 loss: 1.07357801e-06
Iter: 1287 loss: 1.07340645e-06
Iter: 1288 loss: 1.07320147e-06
Iter: 1289 loss: 1.07549226e-06
Iter: 1290 loss: 1.0732017e-06
Iter: 1291 loss: 1.07308324e-06
Iter: 1292 loss: 1.07275287e-06
Iter: 1293 loss: 1.07713163e-06
Iter: 1294 loss: 1.07274354e-06
Iter: 1295 loss: 1.07242079e-06
Iter: 1296 loss: 1.07383016e-06
Iter: 1297 loss: 1.07233359e-06
Iter: 1298 loss: 1.0721576e-06
Iter: 1299 loss: 1.07515802e-06
Iter: 1300 loss: 1.07216761e-06
Iter: 1301 loss: 1.07201072e-06
Iter: 1302 loss: 1.07181518e-06
Iter: 1303 loss: 1.07182541e-06
Iter: 1304 loss: 1.07152414e-06
Iter: 1305 loss: 1.07273809e-06
Iter: 1306 loss: 1.07154483e-06
Iter: 1307 loss: 1.07127971e-06
Iter: 1308 loss: 1.07275036e-06
Iter: 1309 loss: 1.07122764e-06
Iter: 1310 loss: 1.0710927e-06
Iter: 1311 loss: 1.07100846e-06
Iter: 1312 loss: 1.07093797e-06
Iter: 1313 loss: 1.07073038e-06
Iter: 1314 loss: 1.07085941e-06
Iter: 1315 loss: 1.0705719e-06
Iter: 1316 loss: 1.07038227e-06
Iter: 1317 loss: 1.07035794e-06
Iter: 1318 loss: 1.07022697e-06
Iter: 1319 loss: 1.07023448e-06
Iter: 1320 loss: 1.07005565e-06
Iter: 1321 loss: 1.06992036e-06
Iter: 1322 loss: 1.07160758e-06
Iter: 1323 loss: 1.06991365e-06
Iter: 1324 loss: 1.06978837e-06
Iter: 1325 loss: 1.06954042e-06
Iter: 1326 loss: 1.07509993e-06
Iter: 1327 loss: 1.06950711e-06
Iter: 1328 loss: 1.06927814e-06
Iter: 1329 loss: 1.07042638e-06
Iter: 1330 loss: 1.06925734e-06
Iter: 1331 loss: 1.06909499e-06
Iter: 1332 loss: 1.07127266e-06
Iter: 1333 loss: 1.06911443e-06
Iter: 1334 loss: 1.06900382e-06
Iter: 1335 loss: 1.06895948e-06
Iter: 1336 loss: 1.06884499e-06
Iter: 1337 loss: 1.06870539e-06
Iter: 1338 loss: 1.06922982e-06
Iter: 1339 loss: 1.06864593e-06
Iter: 1340 loss: 1.06847733e-06
Iter: 1341 loss: 1.06995253e-06
Iter: 1342 loss: 1.06850155e-06
Iter: 1343 loss: 1.06836433e-06
Iter: 1344 loss: 1.06826087e-06
Iter: 1345 loss: 1.06821631e-06
Iter: 1346 loss: 1.06806601e-06
Iter: 1347 loss: 1.06809011e-06
Iter: 1348 loss: 1.06793141e-06
Iter: 1349 loss: 1.06776542e-06
Iter: 1350 loss: 1.07031769e-06
Iter: 1351 loss: 1.06777088e-06
Iter: 1352 loss: 1.06764537e-06
Iter: 1353 loss: 1.06783114e-06
Iter: 1354 loss: 1.06752498e-06
Iter: 1355 loss: 1.06742459e-06
Iter: 1356 loss: 1.06824643e-06
Iter: 1357 loss: 1.06739401e-06
Iter: 1358 loss: 1.06733489e-06
Iter: 1359 loss: 1.06711161e-06
Iter: 1360 loss: 1.06713981e-06
Iter: 1361 loss: 1.06691618e-06
Iter: 1362 loss: 1.06784046e-06
Iter: 1363 loss: 1.06689436e-06
Iter: 1364 loss: 1.06676191e-06
Iter: 1365 loss: 1.0683932e-06
Iter: 1366 loss: 1.06677271e-06
Iter: 1367 loss: 1.06667926e-06
Iter: 1368 loss: 1.06659695e-06
Iter: 1369 loss: 1.06656535e-06
Iter: 1370 loss: 1.06642369e-06
Iter: 1371 loss: 1.06687503e-06
Iter: 1372 loss: 1.06639334e-06
Iter: 1373 loss: 1.06625509e-06
Iter: 1374 loss: 1.06794607e-06
Iter: 1375 loss: 1.06623634e-06
Iter: 1376 loss: 1.06614152e-06
Iter: 1377 loss: 1.06605955e-06
Iter: 1378 loss: 1.06607445e-06
Iter: 1379 loss: 1.06589528e-06
Iter: 1380 loss: 1.0660109e-06
Iter: 1381 loss: 1.06579262e-06
Iter: 1382 loss: 1.0656795e-06
Iter: 1383 loss: 1.06750156e-06
Iter: 1384 loss: 1.06563812e-06
Iter: 1385 loss: 1.06554637e-06
Iter: 1386 loss: 1.0658398e-06
Iter: 1387 loss: 1.06551875e-06
Iter: 1388 loss: 1.06541938e-06
Iter: 1389 loss: 1.0659744e-06
Iter: 1390 loss: 1.06541495e-06
Iter: 1391 loss: 1.06529865e-06
Iter: 1392 loss: 1.06522134e-06
Iter: 1393 loss: 1.06519269e-06
Iter: 1394 loss: 1.06502716e-06
Iter: 1395 loss: 1.06561811e-06
Iter: 1396 loss: 1.06500204e-06
Iter: 1397 loss: 1.06486095e-06
Iter: 1398 loss: 1.06484924e-06
Iter: 1399 loss: 1.06480104e-06
Iter: 1400 loss: 1.06474863e-06
Iter: 1401 loss: 1.06472976e-06
Iter: 1402 loss: 1.06459765e-06
Iter: 1403 loss: 1.06480366e-06
Iter: 1404 loss: 1.06455809e-06
Iter: 1405 loss: 1.06442963e-06
Iter: 1406 loss: 1.06582729e-06
Iter: 1407 loss: 1.06440575e-06
Iter: 1408 loss: 1.06431082e-06
Iter: 1409 loss: 1.06423192e-06
Iter: 1410 loss: 1.06420669e-06
Iter: 1411 loss: 1.06410243e-06
Iter: 1412 loss: 1.06411062e-06
Iter: 1413 loss: 1.06402683e-06
Iter: 1414 loss: 1.06385244e-06
Iter: 1415 loss: 1.06561333e-06
Iter: 1416 loss: 1.06383641e-06
Iter: 1417 loss: 1.06369362e-06
Iter: 1418 loss: 1.06420748e-06
Iter: 1419 loss: 1.06368395e-06
Iter: 1420 loss: 1.06359232e-06
Iter: 1421 loss: 1.06396101e-06
Iter: 1422 loss: 1.06353662e-06
Iter: 1423 loss: 1.06343077e-06
Iter: 1424 loss: 1.06324774e-06
Iter: 1425 loss: 1.06323057e-06
Iter: 1426 loss: 1.06306447e-06
Iter: 1427 loss: 1.06352559e-06
Iter: 1428 loss: 1.06301343e-06
Iter: 1429 loss: 1.06286643e-06
Iter: 1430 loss: 1.06286154e-06
Iter: 1431 loss: 1.06274263e-06
Iter: 1432 loss: 1.06265759e-06
Iter: 1433 loss: 1.06264088e-06
Iter: 1434 loss: 1.06246057e-06
Iter: 1435 loss: 1.06291179e-06
Iter: 1436 loss: 1.06240736e-06
Iter: 1437 loss: 1.06225161e-06
Iter: 1438 loss: 1.06413142e-06
Iter: 1439 loss: 1.06223251e-06
Iter: 1440 loss: 1.0621302e-06
Iter: 1441 loss: 1.06206789e-06
Iter: 1442 loss: 1.06202731e-06
Iter: 1443 loss: 1.06185576e-06
Iter: 1444 loss: 1.06190623e-06
Iter: 1445 loss: 1.06174093e-06
Iter: 1446 loss: 1.0615679e-06
Iter: 1447 loss: 1.06364564e-06
Iter: 1448 loss: 1.06153152e-06
Iter: 1449 loss: 1.06141351e-06
Iter: 1450 loss: 1.06205357e-06
Iter: 1451 loss: 1.06135826e-06
Iter: 1452 loss: 1.06129596e-06
Iter: 1453 loss: 1.06157518e-06
Iter: 1454 loss: 1.06121593e-06
Iter: 1455 loss: 1.06111463e-06
Iter: 1456 loss: 1.06092011e-06
Iter: 1457 loss: 1.06089328e-06
Iter: 1458 loss: 1.06071639e-06
Iter: 1459 loss: 1.06100424e-06
Iter: 1460 loss: 1.06058712e-06
Iter: 1461 loss: 1.06044547e-06
Iter: 1462 loss: 1.06284961e-06
Iter: 1463 loss: 1.06044274e-06
Iter: 1464 loss: 1.06024709e-06
Iter: 1465 loss: 1.06030552e-06
Iter: 1466 loss: 1.06011612e-06
Iter: 1467 loss: 1.06001312e-06
Iter: 1468 loss: 1.06048e-06
Iter: 1469 loss: 1.05994286e-06
Iter: 1470 loss: 1.05983531e-06
Iter: 1471 loss: 1.06121797e-06
Iter: 1472 loss: 1.05981337e-06
Iter: 1473 loss: 1.05968081e-06
Iter: 1474 loss: 1.05960362e-06
Iter: 1475 loss: 1.05958657e-06
Iter: 1476 loss: 1.05942036e-06
Iter: 1477 loss: 1.05952063e-06
Iter: 1478 loss: 1.05930667e-06
Iter: 1479 loss: 1.05917889e-06
Iter: 1480 loss: 1.06033303e-06
Iter: 1481 loss: 1.05917331e-06
Iter: 1482 loss: 1.05900472e-06
Iter: 1483 loss: 1.0598842e-06
Iter: 1484 loss: 1.05896856e-06
Iter: 1485 loss: 1.05887159e-06
Iter: 1486 loss: 1.05913682e-06
Iter: 1487 loss: 1.05879963e-06
Iter: 1488 loss: 1.05864933e-06
Iter: 1489 loss: 1.05864513e-06
Iter: 1490 loss: 1.05848744e-06
Iter: 1491 loss: 1.05837842e-06
Iter: 1492 loss: 1.05842958e-06
Iter: 1493 loss: 1.05823256e-06
Iter: 1494 loss: 1.05809465e-06
Iter: 1495 loss: 1.05809852e-06
Iter: 1496 loss: 1.05799916e-06
Iter: 1497 loss: 1.05812387e-06
Iter: 1498 loss: 1.05793856e-06
Iter: 1499 loss: 1.05781066e-06
Iter: 1500 loss: 1.05807544e-06
Iter: 1501 loss: 1.0577894e-06
Iter: 1502 loss: 1.0576008e-06
Iter: 1503 loss: 1.05867457e-06
Iter: 1504 loss: 1.05765048e-06
Iter: 1505 loss: 1.05757556e-06
Iter: 1506 loss: 1.05750723e-06
Iter: 1507 loss: 1.05747256e-06
Iter: 1508 loss: 1.05733e-06
Iter: 1509 loss: 1.0573392e-06
Iter: 1510 loss: 1.05721756e-06
Iter: 1511 loss: 1.05704498e-06
Iter: 1512 loss: 1.05842344e-06
Iter: 1513 loss: 1.05705487e-06
Iter: 1514 loss: 1.05691811e-06
Iter: 1515 loss: 1.05779304e-06
Iter: 1516 loss: 1.0568607e-06
Iter: 1517 loss: 1.05681056e-06
Iter: 1518 loss: 1.0569604e-06
Iter: 1519 loss: 1.05676429e-06
Iter: 1520 loss: 1.0565974e-06
Iter: 1521 loss: 1.0566697e-06
Iter: 1522 loss: 1.05647428e-06
Iter: 1523 loss: 1.05632216e-06
Iter: 1524 loss: 1.05632023e-06
Iter: 1525 loss: 1.05626623e-06
Iter: 1526 loss: 1.05614754e-06
Iter: 1527 loss: 1.05610343e-06
Iter: 1528 loss: 1.05598542e-06
Iter: 1529 loss: 1.05609433e-06
Iter: 1530 loss: 1.05588799e-06
Iter: 1531 loss: 1.05578397e-06
Iter: 1532 loss: 1.05615027e-06
Iter: 1533 loss: 1.05577249e-06
Iter: 1534 loss: 1.05566983e-06
Iter: 1535 loss: 1.05673575e-06
Iter: 1536 loss: 1.05564982e-06
Iter: 1537 loss: 1.05556569e-06
Iter: 1538 loss: 1.05548349e-06
Iter: 1539 loss: 1.0554736e-06
Iter: 1540 loss: 1.05531763e-06
Iter: 1541 loss: 1.05542495e-06
Iter: 1542 loss: 1.0552028e-06
Iter: 1543 loss: 1.05507559e-06
Iter: 1544 loss: 1.05578738e-06
Iter: 1545 loss: 1.05507297e-06
Iter: 1546 loss: 1.05491108e-06
Iter: 1547 loss: 1.05596575e-06
Iter: 1548 loss: 1.05489096e-06
Iter: 1549 loss: 1.05480535e-06
Iter: 1550 loss: 1.0549636e-06
Iter: 1551 loss: 1.05477773e-06
Iter: 1552 loss: 1.05457457e-06
Iter: 1553 loss: 1.0548024e-06
Iter: 1554 loss: 1.054515e-06
Iter: 1555 loss: 1.05438801e-06
Iter: 1556 loss: 1.05427466e-06
Iter: 1557 loss: 1.05418701e-06
Iter: 1558 loss: 1.05406707e-06
Iter: 1559 loss: 1.05597871e-06
Iter: 1560 loss: 1.05409822e-06
Iter: 1561 loss: 1.0538788e-06
Iter: 1562 loss: 1.05418462e-06
Iter: 1563 loss: 1.05379775e-06
Iter: 1564 loss: 1.05363688e-06
Iter: 1565 loss: 1.05394417e-06
Iter: 1566 loss: 1.05360118e-06
Iter: 1567 loss: 1.05345794e-06
Iter: 1568 loss: 1.05478205e-06
Iter: 1569 loss: 1.05346317e-06
Iter: 1570 loss: 1.05333947e-06
Iter: 1571 loss: 1.05326717e-06
Iter: 1572 loss: 1.05320294e-06
Iter: 1573 loss: 1.05303639e-06
Iter: 1574 loss: 1.05308663e-06
Iter: 1575 loss: 1.0528945e-06
Iter: 1576 loss: 1.05269532e-06
Iter: 1577 loss: 1.05360198e-06
Iter: 1578 loss: 1.05263973e-06
Iter: 1579 loss: 1.05246863e-06
Iter: 1580 loss: 1.0543281e-06
Iter: 1581 loss: 1.05243475e-06
Iter: 1582 loss: 1.05229992e-06
Iter: 1583 loss: 1.0523371e-06
Iter: 1584 loss: 1.05221818e-06
Iter: 1585 loss: 1.05200741e-06
Iter: 1586 loss: 1.05276035e-06
Iter: 1587 loss: 1.05193101e-06
Iter: 1588 loss: 1.05181141e-06
Iter: 1589 loss: 1.05164679e-06
Iter: 1590 loss: 1.05164202e-06
Iter: 1591 loss: 1.05142124e-06
Iter: 1592 loss: 1.05380627e-06
Iter: 1593 loss: 1.05142067e-06
Iter: 1594 loss: 1.05121933e-06
Iter: 1595 loss: 1.05185006e-06
Iter: 1596 loss: 1.05119193e-06
Iter: 1597 loss: 1.05106267e-06
Iter: 1598 loss: 1.05133506e-06
Iter: 1599 loss: 1.05097524e-06
Iter: 1600 loss: 1.05087952e-06
Iter: 1601 loss: 1.05194647e-06
Iter: 1602 loss: 1.05083268e-06
Iter: 1603 loss: 1.05076458e-06
Iter: 1604 loss: 1.05062566e-06
Iter: 1605 loss: 1.05057529e-06
Iter: 1606 loss: 1.05043114e-06
Iter: 1607 loss: 1.05056847e-06
Iter: 1608 loss: 1.05032814e-06
Iter: 1609 loss: 1.05008621e-06
Iter: 1610 loss: 1.05146478e-06
Iter: 1611 loss: 1.05009e-06
Iter: 1612 loss: 1.04992728e-06
Iter: 1613 loss: 1.05193453e-06
Iter: 1614 loss: 1.0499208e-06
Iter: 1615 loss: 1.04979551e-06
Iter: 1616 loss: 1.04972867e-06
Iter: 1617 loss: 1.0496924e-06
Iter: 1618 loss: 1.04948776e-06
Iter: 1619 loss: 1.0503195e-06
Iter: 1620 loss: 1.04946207e-06
Iter: 1621 loss: 1.04933861e-06
Iter: 1622 loss: 1.04923402e-06
Iter: 1623 loss: 1.04918206e-06
Iter: 1624 loss: 1.04899277e-06
Iter: 1625 loss: 1.05019694e-06
Iter: 1626 loss: 1.04898311e-06
Iter: 1627 loss: 1.04875187e-06
Iter: 1628 loss: 1.04952528e-06
Iter: 1629 loss: 1.04869673e-06
Iter: 1630 loss: 1.04857622e-06
Iter: 1631 loss: 1.0489515e-06
Iter: 1632 loss: 1.04848777e-06
Iter: 1633 loss: 1.0484049e-06
Iter: 1634 loss: 1.04992785e-06
Iter: 1635 loss: 1.04837261e-06
Iter: 1636 loss: 1.04824119e-06
Iter: 1637 loss: 1.04804849e-06
Iter: 1638 loss: 1.05356241e-06
Iter: 1639 loss: 1.04802143e-06
Iter: 1640 loss: 1.04772357e-06
Iter: 1641 loss: 1.04811215e-06
Iter: 1642 loss: 1.04761807e-06
Iter: 1643 loss: 1.04729554e-06
Iter: 1644 loss: 1.04819674e-06
Iter: 1645 loss: 1.04727292e-06
Iter: 1646 loss: 1.04701621e-06
Iter: 1647 loss: 1.05039987e-06
Iter: 1648 loss: 1.04704327e-06
Iter: 1649 loss: 1.04684591e-06
Iter: 1650 loss: 1.04678486e-06
Iter: 1651 loss: 1.04668175e-06
Iter: 1652 loss: 1.04646131e-06
Iter: 1653 loss: 1.04820288e-06
Iter: 1654 loss: 1.04642561e-06
Iter: 1655 loss: 1.04627895e-06
Iter: 1656 loss: 1.04603532e-06
Iter: 1657 loss: 1.05074412e-06
Iter: 1658 loss: 1.04600485e-06
Iter: 1659 loss: 1.04576293e-06
Iter: 1660 loss: 1.04749915e-06
Iter: 1661 loss: 1.04573962e-06
Iter: 1662 loss: 1.04542391e-06
Iter: 1663 loss: 1.04765741e-06
Iter: 1664 loss: 1.04542562e-06
Iter: 1665 loss: 1.04527976e-06
Iter: 1666 loss: 1.04535525e-06
Iter: 1667 loss: 1.04516221e-06
Iter: 1668 loss: 1.04501294e-06
Iter: 1669 loss: 1.04673302e-06
Iter: 1670 loss: 1.04503658e-06
Iter: 1671 loss: 1.04488208e-06
Iter: 1672 loss: 1.04482183e-06
Iter: 1673 loss: 1.04476203e-06
Iter: 1674 loss: 1.04460207e-06
Iter: 1675 loss: 1.04473702e-06
Iter: 1676 loss: 1.0444852e-06
Iter: 1677 loss: 1.04433036e-06
Iter: 1678 loss: 1.04536161e-06
Iter: 1679 loss: 1.04426863e-06
Iter: 1680 loss: 1.04420064e-06
Iter: 1681 loss: 1.04418586e-06
Iter: 1682 loss: 1.04407809e-06
Iter: 1683 loss: 1.04396327e-06
Iter: 1684 loss: 1.04394144e-06
Iter: 1685 loss: 1.04376386e-06
Iter: 1686 loss: 1.04577964e-06
Iter: 1687 loss: 1.04377671e-06
Iter: 1688 loss: 1.04363994e-06
Iter: 1689 loss: 1.04344372e-06
Iter: 1690 loss: 1.04826336e-06
Iter: 1691 loss: 1.04345008e-06
Iter: 1692 loss: 1.0432459e-06
Iter: 1693 loss: 1.0439046e-06
Iter: 1694 loss: 1.04317428e-06
Iter: 1695 loss: 1.04304513e-06
Iter: 1696 loss: 1.04301307e-06
Iter: 1697 loss: 1.04291428e-06
Iter: 1698 loss: 1.04283072e-06
Iter: 1699 loss: 1.04281344e-06
Iter: 1700 loss: 1.04266e-06
Iter: 1701 loss: 1.04445837e-06
Iter: 1702 loss: 1.04267531e-06
Iter: 1703 loss: 1.04256492e-06
Iter: 1704 loss: 1.04239246e-06
Iter: 1705 loss: 1.04236119e-06
Iter: 1706 loss: 1.04215565e-06
Iter: 1707 loss: 1.04241212e-06
Iter: 1708 loss: 1.0420888e-06
Iter: 1709 loss: 1.04181e-06
Iter: 1710 loss: 1.04291053e-06
Iter: 1711 loss: 1.04177184e-06
Iter: 1712 loss: 1.04157084e-06
Iter: 1713 loss: 1.04337823e-06
Iter: 1714 loss: 1.04159699e-06
Iter: 1715 loss: 1.04144021e-06
Iter: 1716 loss: 1.04122466e-06
Iter: 1717 loss: 1.04123205e-06
Iter: 1718 loss: 1.04107801e-06
Iter: 1719 loss: 1.04104652e-06
Iter: 1720 loss: 1.04089838e-06
Iter: 1721 loss: 1.04069431e-06
Iter: 1722 loss: 1.04641697e-06
Iter: 1723 loss: 1.04070989e-06
Iter: 1724 loss: 1.04045409e-06
Iter: 1725 loss: 1.04074741e-06
Iter: 1726 loss: 1.04028447e-06
Iter: 1727 loss: 1.04018136e-06
Iter: 1728 loss: 1.04016965e-06
Iter: 1729 loss: 1.04002959e-06
Iter: 1730 loss: 1.03987259e-06
Iter: 1731 loss: 1.04386379e-06
Iter: 1732 loss: 1.03984144e-06
Iter: 1733 loss: 1.03961884e-06
Iter: 1734 loss: 1.04177525e-06
Iter: 1735 loss: 1.03964271e-06
Iter: 1736 loss: 1.03946343e-06
Iter: 1737 loss: 1.03948582e-06
Iter: 1738 loss: 1.03939283e-06
Iter: 1739 loss: 1.03914795e-06
Iter: 1740 loss: 1.03911214e-06
Iter: 1741 loss: 1.03898583e-06
Iter: 1742 loss: 1.03879347e-06
Iter: 1743 loss: 1.03990487e-06
Iter: 1744 loss: 1.03868069e-06
Iter: 1745 loss: 1.03860702e-06
Iter: 1746 loss: 1.03855916e-06
Iter: 1747 loss: 1.03848129e-06
Iter: 1748 loss: 1.03840523e-06
Iter: 1749 loss: 1.03836453e-06
Iter: 1750 loss: 1.03831871e-06
Iter: 1751 loss: 1.03977163e-06
Iter: 1752 loss: 1.03828449e-06
Iter: 1753 loss: 1.03815375e-06
Iter: 1754 loss: 1.03815182e-06
Iter: 1755 loss: 1.03801335e-06
Iter: 1756 loss: 1.03793548e-06
Iter: 1757 loss: 1.03792649e-06
Iter: 1758 loss: 1.03785032e-06
Iter: 1759 loss: 1.03784078e-06
Iter: 1760 loss: 1.03775506e-06
Iter: 1761 loss: 1.03769491e-06
Iter: 1762 loss: 1.03760215e-06
Iter: 1763 loss: 1.03757429e-06
Iter: 1764 loss: 1.03746584e-06
Iter: 1765 loss: 1.0386608e-06
Iter: 1766 loss: 1.03749562e-06
Iter: 1767 loss: 1.03734737e-06
Iter: 1768 loss: 1.03754905e-06
Iter: 1769 loss: 1.03728598e-06
Iter: 1770 loss: 1.03717343e-06
Iter: 1771 loss: 1.03722732e-06
Iter: 1772 loss: 1.03711523e-06
Iter: 1773 loss: 1.03701279e-06
Iter: 1774 loss: 1.03681043e-06
Iter: 1775 loss: 1.03678235e-06
Iter: 1776 loss: 1.03652519e-06
Iter: 1777 loss: 1.03855882e-06
Iter: 1778 loss: 1.03644061e-06
Iter: 1779 loss: 1.03618549e-06
Iter: 1780 loss: 1.03719321e-06
Iter: 1781 loss: 1.03612729e-06
Iter: 1782 loss: 1.03600166e-06
Iter: 1783 loss: 1.03585444e-06
Iter: 1784 loss: 1.03580135e-06
Iter: 1785 loss: 1.03553452e-06
Iter: 1786 loss: 1.03878938e-06
Iter: 1787 loss: 1.03551235e-06
Iter: 1788 loss: 1.03539662e-06
Iter: 1789 loss: 1.03515663e-06
Iter: 1790 loss: 1.03869547e-06
Iter: 1791 loss: 1.03512252e-06
Iter: 1792 loss: 1.03493846e-06
Iter: 1793 loss: 1.03802381e-06
Iter: 1794 loss: 1.03490311e-06
Iter: 1795 loss: 1.03474565e-06
Iter: 1796 loss: 1.03637672e-06
Iter: 1797 loss: 1.03470302e-06
Iter: 1798 loss: 1.03458854e-06
Iter: 1799 loss: 1.0345567e-06
Iter: 1800 loss: 1.03449361e-06
Iter: 1801 loss: 1.03434434e-06
Iter: 1802 loss: 1.03551076e-06
Iter: 1803 loss: 1.03426248e-06
Iter: 1804 loss: 1.03414027e-06
Iter: 1805 loss: 1.03417824e-06
Iter: 1806 loss: 1.03403829e-06
Iter: 1807 loss: 1.03381876e-06
Iter: 1808 loss: 1.03404e-06
Iter: 1809 loss: 1.03372759e-06
Iter: 1810 loss: 1.03356115e-06
Iter: 1811 loss: 1.03413777e-06
Iter: 1812 loss: 1.03348384e-06
Iter: 1813 loss: 1.03331706e-06
Iter: 1814 loss: 1.03332775e-06
Iter: 1815 loss: 1.03324783e-06
Iter: 1816 loss: 1.03301704e-06
Iter: 1817 loss: 1.03722152e-06
Iter: 1818 loss: 1.03302546e-06
Iter: 1819 loss: 1.03279888e-06
Iter: 1820 loss: 1.0327708e-06
Iter: 1821 loss: 1.03268326e-06
Iter: 1822 loss: 1.03239972e-06
Iter: 1823 loss: 1.03556476e-06
Iter: 1824 loss: 1.03234731e-06
Iter: 1825 loss: 1.03195975e-06
Iter: 1826 loss: 1.03243542e-06
Iter: 1827 loss: 1.03183174e-06
Iter: 1828 loss: 1.03197453e-06
Iter: 1829 loss: 1.03172101e-06
Iter: 1830 loss: 1.03160221e-06
Iter: 1831 loss: 1.03167918e-06
Iter: 1832 loss: 1.03155935e-06
Iter: 1833 loss: 1.03145817e-06
Iter: 1834 loss: 1.03206287e-06
Iter: 1835 loss: 1.03145931e-06
Iter: 1836 loss: 1.0313338e-06
Iter: 1837 loss: 1.03118487e-06
Iter: 1838 loss: 1.03120044e-06
Iter: 1839 loss: 1.03101706e-06
Iter: 1840 loss: 1.03145e-06
Iter: 1841 loss: 1.03092975e-06
Iter: 1842 loss: 1.03078014e-06
Iter: 1843 loss: 1.03078878e-06
Iter: 1844 loss: 1.0306145e-06
Iter: 1845 loss: 1.03054765e-06
Iter: 1846 loss: 1.03050752e-06
Iter: 1847 loss: 1.03036973e-06
Iter: 1848 loss: 1.0302831e-06
Iter: 1849 loss: 1.03025729e-06
Iter: 1850 loss: 1.0301037e-06
Iter: 1851 loss: 1.03002606e-06
Iter: 1852 loss: 1.02992726e-06
Iter: 1853 loss: 1.02965964e-06
Iter: 1854 loss: 1.03340403e-06
Iter: 1855 loss: 1.02967624e-06
Iter: 1856 loss: 1.02953277e-06
Iter: 1857 loss: 1.02927584e-06
Iter: 1858 loss: 1.03388186e-06
Iter: 1859 loss: 1.02929755e-06
Iter: 1860 loss: 1.02920694e-06
Iter: 1861 loss: 1.02913157e-06
Iter: 1862 loss: 1.02908268e-06
Iter: 1863 loss: 1.02910803e-06
Iter: 1864 loss: 1.02899082e-06
Iter: 1865 loss: 1.02880495e-06
Iter: 1866 loss: 1.02984177e-06
Iter: 1867 loss: 1.02878903e-06
Iter: 1868 loss: 1.02871468e-06
Iter: 1869 loss: 1.02875731e-06
Iter: 1870 loss: 1.02869581e-06
Iter: 1871 loss: 1.0285662e-06
Iter: 1872 loss: 1.02850788e-06
Iter: 1873 loss: 1.02845138e-06
Iter: 1874 loss: 1.02830018e-06
Iter: 1875 loss: 1.0291684e-06
Iter: 1876 loss: 1.02824856e-06
Iter: 1877 loss: 1.02811418e-06
Iter: 1878 loss: 1.02832917e-06
Iter: 1879 loss: 1.02802483e-06
Iter: 1880 loss: 1.0279465e-06
Iter: 1881 loss: 1.02962963e-06
Iter: 1882 loss: 1.02793797e-06
Iter: 1883 loss: 1.02781632e-06
Iter: 1884 loss: 1.02762726e-06
Iter: 1885 loss: 1.03031607e-06
Iter: 1886 loss: 1.02761248e-06
Iter: 1887 loss: 1.02743809e-06
Iter: 1888 loss: 1.02830609e-06
Iter: 1889 loss: 1.02733134e-06
Iter: 1890 loss: 1.02714728e-06
Iter: 1891 loss: 1.02997365e-06
Iter: 1892 loss: 1.02716649e-06
Iter: 1893 loss: 1.02712613e-06
Iter: 1894 loss: 1.02687943e-06
Iter: 1895 loss: 1.02881813e-06
Iter: 1896 loss: 1.02687432e-06
Iter: 1897 loss: 1.02677893e-06
Iter: 1898 loss: 1.02671447e-06
Iter: 1899 loss: 1.02660681e-06
Iter: 1900 loss: 1.02640092e-06
Iter: 1901 loss: 1.03093248e-06
Iter: 1902 loss: 1.02639e-06
Iter: 1903 loss: 1.02619128e-06
Iter: 1904 loss: 1.02637034e-06
Iter: 1905 loss: 1.02616832e-06
Iter: 1906 loss: 1.02602746e-06
Iter: 1907 loss: 1.02765875e-06
Iter: 1908 loss: 1.02604838e-06
Iter: 1909 loss: 1.02588501e-06
Iter: 1910 loss: 1.02595322e-06
Iter: 1911 loss: 1.02582931e-06
Iter: 1912 loss: 1.02574461e-06
Iter: 1913 loss: 1.02566639e-06
Iter: 1914 loss: 1.0256424e-06
Iter: 1915 loss: 1.02553759e-06
Iter: 1916 loss: 1.02562467e-06
Iter: 1917 loss: 1.0254729e-06
Iter: 1918 loss: 1.02529248e-06
Iter: 1919 loss: 1.02597096e-06
Iter: 1920 loss: 1.02526928e-06
Iter: 1921 loss: 1.02510569e-06
Iter: 1922 loss: 1.02479214e-06
Iter: 1923 loss: 1.02479066e-06
Iter: 1924 loss: 1.02461968e-06
Iter: 1925 loss: 1.02465515e-06
Iter: 1926 loss: 1.02436536e-06
Iter: 1927 loss: 1.02457716e-06
Iter: 1928 loss: 1.02424394e-06
Iter: 1929 loss: 1.0241364e-06
Iter: 1930 loss: 1.02422575e-06
Iter: 1931 loss: 1.02404783e-06
Iter: 1932 loss: 1.02397155e-06
Iter: 1933 loss: 1.02393938e-06
Iter: 1934 loss: 1.02389583e-06
Iter: 1935 loss: 1.02376907e-06
Iter: 1936 loss: 1.02529361e-06
Iter: 1937 loss: 1.02374293e-06
Iter: 1938 loss: 1.02367369e-06
Iter: 1939 loss: 1.02368051e-06
Iter: 1940 loss: 1.02363458e-06
Iter: 1941 loss: 1.02348974e-06
Iter: 1942 loss: 1.02471404e-06
Iter: 1943 loss: 1.0234786e-06
Iter: 1944 loss: 1.02328988e-06
Iter: 1945 loss: 1.02380613e-06
Iter: 1946 loss: 1.02328124e-06
Iter: 1947 loss: 1.02311719e-06
Iter: 1948 loss: 1.02522222e-06
Iter: 1949 loss: 1.02315676e-06
Iter: 1950 loss: 1.02308559e-06
Iter: 1951 loss: 1.02292177e-06
Iter: 1952 loss: 1.02563115e-06
Iter: 1953 loss: 1.02293063e-06
Iter: 1954 loss: 1.02283457e-06
Iter: 1955 loss: 1.02363583e-06
Iter: 1956 loss: 1.02285026e-06
Iter: 1957 loss: 1.02272304e-06
Iter: 1958 loss: 1.02277022e-06
Iter: 1959 loss: 1.02267381e-06
Iter: 1960 loss: 1.02258196e-06
Iter: 1961 loss: 1.0236314e-06
Iter: 1962 loss: 1.02258707e-06
Iter: 1963 loss: 1.02250397e-06
Iter: 1964 loss: 1.02242757e-06
Iter: 1965 loss: 1.02239994e-06
Iter: 1966 loss: 1.02234458e-06
Iter: 1967 loss: 1.02342e-06
Iter: 1968 loss: 1.02232866e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.4
+ date
Sun Nov  8 14:18:34 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed72eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed7d59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed7628c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed6ca620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed6ca1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed72c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed638ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5faae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5fa7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5fa598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5b21e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ad0fa378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5bc268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ad0a92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5bcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ed5b2e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ad099c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ad0ab048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3388083b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33880571e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33880ac048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f338802d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f338803e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3388026f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3370240378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ad052f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f337023f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33ad052378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33701e71e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33702030d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33701ce510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f337017e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f337016b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33701269d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3370186268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33701bbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.40854874
test_loss: 0.39860576
train_loss: 0.40081748
test_loss: 0.39860862
train_loss: 0.40047324
test_loss: 0.39854836
train_loss: 0.39614093
test_loss: 0.39856803
train_loss: 0.39220178
test_loss: 0.39851508
train_loss: 0.39945495
test_loss: 0.39853853
train_loss: 0.3999036
test_loss: 0.3985193
train_loss: 0.40120715
test_loss: 0.3984876
train_loss: 0.39663404
test_loss: 0.39849183
train_loss: 0.39214972
test_loss: 0.39841354
train_loss: 0.396082
test_loss: 0.39837837
train_loss: 0.40598163
test_loss: 0.39835018
train_loss: 0.39498824
test_loss: 0.3983785
train_loss: 0.39673936
test_loss: 0.3983079
train_loss: 0.40391517
test_loss: 0.39823598
train_loss: 0.39502838
test_loss: 0.39821622
train_loss: 0.39574876
test_loss: 0.3981832
train_loss: 0.39443737
test_loss: 0.3981335
train_loss: 0.39344242
test_loss: 0.39806172
train_loss: 0.3991065
test_loss: 0.39802805
train_loss: 0.389171
test_loss: 0.39794135
train_loss: 0.40119505
test_loss: 0.3979001
train_loss: 0.39439315
test_loss: 0.39782965
train_loss: 0.40088993
test_loss: 0.3977724
train_loss: 0.38977614
test_loss: 0.39764956
train_loss: 0.39837107
test_loss: 0.39762452
train_loss: 0.39234304
test_loss: 0.39753306
train_loss: 0.3989296
test_loss: 0.39742458
train_loss: 0.3990696
test_loss: 0.39735413
train_loss: 0.39054394
test_loss: 0.3972469
train_loss: 0.39923048
test_loss: 0.39711705
train_loss: 0.40165445
test_loss: 0.39702973
train_loss: 0.40123722
test_loss: 0.39689106
train_loss: 0.39975327
test_loss: 0.3967522
train_loss: 0.39113072
test_loss: 0.39661548
train_loss: 0.40281504
test_loss: 0.39653417
train_loss: 0.3961606
test_loss: 0.39634284
train_loss: 0.3976063
test_loss: 0.39621904
train_loss: 0.39613375
test_loss: 0.39606282
train_loss: 0.39391655
test_loss: 0.39589402
train_loss: 0.39437917
test_loss: 0.39568958
train_loss: 0.39087218
test_loss: 0.3955473
train_loss: 0.38587117
test_loss: 0.39532623
train_loss: 0.39250427
test_loss: 0.39511752
train_loss: 0.39634636
test_loss: 0.39495358
train_loss: 0.3975371
test_loss: 0.3946913
train_loss: 0.39779273
test_loss: 0.3944389
train_loss: 0.38869828
test_loss: 0.3941933
train_loss: 0.39360452
test_loss: 0.39389297
train_loss: 0.3966214
test_loss: 0.39365023
train_loss: 0.3838704
test_loss: 0.39340982
train_loss: 0.3977863
test_loss: 0.3930421
train_loss: 0.39183477
test_loss: 0.39274865
train_loss: 0.39834607
test_loss: 0.39237192
train_loss: 0.39218676
test_loss: 0.39203876
train_loss: 0.3962687
test_loss: 0.39165282
train_loss: 0.39300302
test_loss: 0.39126885
train_loss: 0.3919863
test_loss: 0.3908466
train_loss: 0.39541686
test_loss: 0.3903718
train_loss: 0.39508882
test_loss: 0.38994965
train_loss: 0.38566524
test_loss: 0.3894356
train_loss: 0.39128852
test_loss: 0.38891453
train_loss: 0.38506633
test_loss: 0.3884109
train_loss: 0.38737842
test_loss: 0.38783407
train_loss: 0.3815711
test_loss: 0.38722554
train_loss: 0.38738126
test_loss: 0.3865987
train_loss: 0.3902673
test_loss: 0.38593048
train_loss: 0.3904535
test_loss: 0.38516787
train_loss: 0.38091165
test_loss: 0.38449147
train_loss: 0.37966168
test_loss: 0.38370392
train_loss: 0.3892745
test_loss: 0.3829061
train_loss: 0.37499437
test_loss: 0.38201904
train_loss: 0.39045703
test_loss: 0.38113293
train_loss: 0.38461894
test_loss: 0.38016853
train_loss: 0.37611425
test_loss: 0.37911335
train_loss: 0.3735879
test_loss: 0.37804526
train_loss: 0.38046694
test_loss: 0.37698781
train_loss: 0.3738048
test_loss: 0.37574214
train_loss: 0.37493402
test_loss: 0.37450173
train_loss: 0.3762365
test_loss: 0.3731612
train_loss: 0.37490422
test_loss: 0.37171784
train_loss: 0.37393105
test_loss: 0.37026024
train_loss: 0.3701453
test_loss: 0.3686799
train_loss: 0.37206805
test_loss: 0.36699432
train_loss: 0.36059394
test_loss: 0.36518005
train_loss: 0.37143224
test_loss: 0.36330795
train_loss: 0.35892177
test_loss: 0.36124784
train_loss: 0.35933745
test_loss: 0.35910502
train_loss: 0.35948205
test_loss: 0.3568509
train_loss: 0.3533877
test_loss: 0.35440406
train_loss: 0.35392332
test_loss: 0.35184306
train_loss: 0.35162187
test_loss: 0.34913066
train_loss: 0.34859753
test_loss: 0.34618092
train_loss: 0.3398931
test_loss: 0.343044
train_loss: 0.33949012
test_loss: 0.33963445
train_loss: 0.33877087
test_loss: 0.33591014
train_loss: 0.3295389
test_loss: 0.33198753
train_loss: 0.32567185
test_loss: 0.32762858
train_loss: 0.322905
test_loss: 0.32290852
train_loss: 0.31560308
test_loss: 0.3177639
train_loss: 0.30486435
test_loss: 0.31221995
train_loss: 0.3026098
test_loss: 0.30617395
train_loss: 0.29965743
test_loss: 0.29960144
train_loss: 0.29352537
test_loss: 0.29244027
train_loss: 0.28230423
test_loss: 0.28451666
train_loss: 0.27449638
test_loss: 0.2760102
train_loss: 0.25897947
test_loss: 0.26680318
train_loss: 0.25273305
test_loss: 0.25695768
train_loss: 0.24602598
test_loss: 0.24643455
train_loss: 0.23088652
test_loss: 0.2353546
train_loss: 0.22102037
test_loss: 0.22367261
train_loss: 0.2132948
test_loss: 0.21163878
train_loss: 0.19403267
test_loss: 0.1992839
train_loss: 0.18716662
test_loss: 0.18665914
train_loss: 0.17090748
test_loss: 0.17375328
train_loss: 0.15641922
test_loss: 0.16061923
train_loss: 0.14850922
test_loss: 0.14712308
train_loss: 0.13432197
test_loss: 0.13348995
train_loss: 0.119363144
test_loss: 0.12008149
train_loss: 0.10749045
test_loss: 0.10736214
train_loss: 0.09635484
test_loss: 0.09571247
train_loss: 0.084711686
test_loss: 0.08559424
train_loss: 0.07712586
test_loss: 0.07732214
train_loss: 0.06937189
test_loss: 0.071038805
train_loss: 0.06639985
test_loss: 0.06657582
train_loss: 0.06355646
test_loss: 0.063631624
train_loss: 0.060069554
test_loss: 0.061774068
train_loss: 0.060810387
test_loss: 0.06060076
train_loss: 0.06017011
test_loss: 0.05986519
train_loss: 0.058184285
test_loss: 0.05937687
train_loss: 0.058099978
test_loss: 0.059060514
train_loss: 0.05821094
test_loss: 0.05883518
train_loss: 0.059250884
test_loss: 0.058646314
train_loss: 0.058129247
test_loss: 0.05847865
train_loss: 0.058163248
test_loss: 0.058367588
train_loss: 0.057358567
test_loss: 0.05826366
train_loss: 0.058047973
test_loss: 0.058147073
train_loss: 0.057487845
test_loss: 0.05805587
train_loss: 0.057764567
test_loss: 0.057973962
train_loss: 0.058114257
test_loss: 0.057915233
train_loss: 0.05759386
test_loss: 0.05781757
train_loss: 0.057503525
test_loss: 0.057753284
train_loss: 0.05716829
test_loss: 0.057660777
train_loss: 0.056508042
test_loss: 0.05754996
train_loss: 0.057276163
test_loss: 0.057477046
train_loss: 0.05590658
test_loss: 0.057414904
train_loss: 0.055992924
test_loss: 0.05732012
train_loss: 0.057326302
test_loss: 0.057173472
train_loss: 0.05670646
test_loss: 0.05712146
train_loss: 0.056185402
test_loss: 0.056975104
train_loss: 0.05685746
test_loss: 0.05685014
train_loss: 0.055548936
test_loss: 0.056700815
train_loss: 0.056749307
test_loss: 0.056555968
train_loss: 0.055990435
test_loss: 0.0564018
train_loss: 0.056249328
test_loss: 0.056178093
train_loss: 0.05440398
test_loss: 0.05595403
train_loss: 0.054785468
test_loss: 0.055728275
train_loss: 0.055785827
test_loss: 0.0554911
train_loss: 0.05438985
test_loss: 0.055139143
train_loss: 0.05473665
test_loss: 0.05480728
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781460dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78146218c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78600907b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78145a3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78145a30d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78600902f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78145936a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78144ef730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78144ef268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78144b1158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78144b19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78e1d780d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814420378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781442b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781443c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781443c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78144b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814459d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781435bb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781437c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814342048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78142e52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78142dd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814314f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78142a9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814291f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814261620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814291378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78141da1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78142050d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f781420f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78141cc1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f78141b17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814181598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814133268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7814147f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.00473442115
Iter: 2 loss: 0.00524752215
Iter: 3 loss: 0.00468631554
Iter: 4 loss: 0.00466631213
Iter: 5 loss: 0.00494899228
Iter: 6 loss: 0.00466627674
Iter: 7 loss: 0.0046580988
Iter: 8 loss: 0.00466060825
Iter: 9 loss: 0.00465225428
Iter: 10 loss: 0.00464465376
Iter: 11 loss: 0.00471658912
Iter: 12 loss: 0.00464434503
Iter: 13 loss: 0.00464007072
Iter: 14 loss: 0.00463260058
Iter: 15 loss: 0.00463259825
Iter: 16 loss: 0.00462438539
Iter: 17 loss: 0.00468764594
Iter: 18 loss: 0.00462378096
Iter: 19 loss: 0.00461413106
Iter: 20 loss: 0.00461413
Iter: 21 loss: 0.00460644346
Iter: 22 loss: 0.00458990131
Iter: 23 loss: 0.00455071451
Iter: 24 loss: 0.00499776192
Iter: 25 loss: 0.00454698224
Iter: 26 loss: 0.00448899157
Iter: 27 loss: 0.00477120467
Iter: 28 loss: 0.00447821431
Iter: 29 loss: 0.00440012803
Iter: 30 loss: 0.0048222784
Iter: 31 loss: 0.00438154815
Iter: 32 loss: 0.00430702465
Iter: 33 loss: 0.00440787151
Iter: 34 loss: 0.00426973868
Iter: 35 loss: 0.00420086598
Iter: 36 loss: 0.00463738292
Iter: 37 loss: 0.00418943213
Iter: 38 loss: 0.00417829864
Iter: 39 loss: 0.0041665188
Iter: 40 loss: 0.00415016431
Iter: 41 loss: 0.0043389718
Iter: 42 loss: 0.00414983463
Iter: 43 loss: 0.00414052373
Iter: 44 loss: 0.00429337472
Iter: 45 loss: 0.00414051767
Iter: 46 loss: 0.00413331
Iter: 47 loss: 0.00414061872
Iter: 48 loss: 0.00412929058
Iter: 49 loss: 0.00411845837
Iter: 50 loss: 0.00422786828
Iter: 51 loss: 0.00411815
Iter: 52 loss: 0.00410535373
Iter: 53 loss: 0.00415144442
Iter: 54 loss: 0.00410181424
Iter: 55 loss: 0.00409026165
Iter: 56 loss: 0.00407754909
Iter: 57 loss: 0.00407563569
Iter: 58 loss: 0.00404751906
Iter: 59 loss: 0.00417410489
Iter: 60 loss: 0.00404148083
Iter: 61 loss: 0.0040115118
Iter: 62 loss: 0.00408725208
Iter: 63 loss: 0.00399968028
Iter: 64 loss: 0.00396705698
Iter: 65 loss: 0.00401354674
Iter: 66 loss: 0.00395098049
Iter: 67 loss: 0.00390617037
Iter: 68 loss: 0.00451946538
Iter: 69 loss: 0.00390496082
Iter: 70 loss: 0.00387692358
Iter: 71 loss: 0.00412718626
Iter: 72 loss: 0.0038753564
Iter: 73 loss: 0.00385725033
Iter: 74 loss: 0.00409048283
Iter: 75 loss: 0.00385720585
Iter: 76 loss: 0.00383059774
Iter: 77 loss: 0.00397294667
Iter: 78 loss: 0.0038247928
Iter: 79 loss: 0.0038046313
Iter: 80 loss: 0.00380276376
Iter: 81 loss: 0.00378296105
Iter: 82 loss: 0.00383964623
Iter: 83 loss: 0.00377703574
Iter: 84 loss: 0.00375101366
Iter: 85 loss: 0.00374800921
Iter: 86 loss: 0.00372794783
Iter: 87 loss: 0.00369256642
Iter: 88 loss: 0.004178592
Iter: 89 loss: 0.00369255291
Iter: 90 loss: 0.00365864043
Iter: 91 loss: 0.00377038028
Iter: 92 loss: 0.00365072116
Iter: 93 loss: 0.00362280337
Iter: 94 loss: 0.00362242479
Iter: 95 loss: 0.0035905519
Iter: 96 loss: 0.00371421268
Iter: 97 loss: 0.00358377676
Iter: 98 loss: 0.0035570804
Iter: 99 loss: 0.00358456513
Iter: 100 loss: 0.00354307052
Iter: 101 loss: 0.00351746
Iter: 102 loss: 0.0041522705
Iter: 103 loss: 0.00351745938
Iter: 104 loss: 0.00349332811
Iter: 105 loss: 0.00365827139
Iter: 106 loss: 0.00349107105
Iter: 107 loss: 0.00347511587
Iter: 108 loss: 0.00348986662
Iter: 109 loss: 0.0034662378
Iter: 110 loss: 0.00345324283
Iter: 111 loss: 0.00345774088
Iter: 112 loss: 0.00344415801
Iter: 113 loss: 0.00342951715
Iter: 114 loss: 0.00365284784
Iter: 115 loss: 0.00342951878
Iter: 116 loss: 0.00341002643
Iter: 117 loss: 0.00341994478
Iter: 118 loss: 0.00339591736
Iter: 119 loss: 0.003362583
Iter: 120 loss: 0.00340298773
Iter: 121 loss: 0.00334457913
Iter: 122 loss: 0.0033212339
Iter: 123 loss: 0.00331864716
Iter: 124 loss: 0.00330128
Iter: 125 loss: 0.00329119153
Iter: 126 loss: 0.00328379311
Iter: 127 loss: 0.00324681518
Iter: 128 loss: 0.00367875188
Iter: 129 loss: 0.0032460154
Iter: 130 loss: 0.00321555207
Iter: 131 loss: 0.00321551855
Iter: 132 loss: 0.00320255128
Iter: 133 loss: 0.00326395221
Iter: 134 loss: 0.00320005976
Iter: 135 loss: 0.00318800588
Iter: 136 loss: 0.00336086843
Iter: 137 loss: 0.00318798772
Iter: 138 loss: 0.00317692943
Iter: 139 loss: 0.00328607112
Iter: 140 loss: 0.00317650987
Iter: 141 loss: 0.00317005301
Iter: 142 loss: 0.00319877267
Iter: 143 loss: 0.00316880387
Iter: 144 loss: 0.00316550652
Iter: 145 loss: 0.00316251488
Iter: 146 loss: 0.00316170836
Iter: 147 loss: 0.00315606268
Iter: 148 loss: 0.00317091076
Iter: 149 loss: 0.00315417629
Iter: 150 loss: 0.00314897671
Iter: 151 loss: 0.00315012061
Iter: 152 loss: 0.0031451832
Iter: 153 loss: 0.00313971704
Iter: 154 loss: 0.00319192652
Iter: 155 loss: 0.0031395168
Iter: 156 loss: 0.00313487463
Iter: 157 loss: 0.00313891983
Iter: 158 loss: 0.00313216308
Iter: 159 loss: 0.00312654255
Iter: 160 loss: 0.00311778788
Iter: 161 loss: 0.00311766891
Iter: 162 loss: 0.00310530141
Iter: 163 loss: 0.00319710607
Iter: 164 loss: 0.00310425833
Iter: 165 loss: 0.00309372274
Iter: 166 loss: 0.0031815269
Iter: 167 loss: 0.00309311785
Iter: 168 loss: 0.00308219832
Iter: 169 loss: 0.00310689094
Iter: 170 loss: 0.00307805324
Iter: 171 loss: 0.00306996983
Iter: 172 loss: 0.00306746
Iter: 173 loss: 0.00306266267
Iter: 174 loss: 0.00305473059
Iter: 175 loss: 0.00317035429
Iter: 176 loss: 0.00305471662
Iter: 177 loss: 0.00304838223
Iter: 178 loss: 0.00304316473
Iter: 179 loss: 0.00304133072
Iter: 180 loss: 0.00303136278
Iter: 181 loss: 0.0031168377
Iter: 182 loss: 0.00303079188
Iter: 183 loss: 0.00302363397
Iter: 184 loss: 0.00301212305
Iter: 185 loss: 0.00301202922
Iter: 186 loss: 0.00299980771
Iter: 187 loss: 0.00306453928
Iter: 188 loss: 0.00299783191
Iter: 189 loss: 0.0029884642
Iter: 190 loss: 0.00310123106
Iter: 191 loss: 0.00298835244
Iter: 192 loss: 0.00298219407
Iter: 193 loss: 0.00300327269
Iter: 194 loss: 0.00298035797
Iter: 195 loss: 0.00296979817
Iter: 196 loss: 0.00300702918
Iter: 197 loss: 0.00296705449
Iter: 198 loss: 0.00295636337
Iter: 199 loss: 0.00298832264
Iter: 200 loss: 0.00295309885
Iter: 201 loss: 0.00294484315
Iter: 202 loss: 0.00299617695
Iter: 203 loss: 0.00294387457
Iter: 204 loss: 0.00293272524
Iter: 205 loss: 0.00294771045
Iter: 206 loss: 0.00292719761
Iter: 207 loss: 0.00291545177
Iter: 208 loss: 0.00296717323
Iter: 209 loss: 0.00291303988
Iter: 210 loss: 0.00290128426
Iter: 211 loss: 0.00287980493
Iter: 212 loss: 0.00340822805
Iter: 213 loss: 0.0028797905
Iter: 214 loss: 0.00284948666
Iter: 215 loss: 0.0029416054
Iter: 216 loss: 0.00283900276
Iter: 217 loss: 0.00280141085
Iter: 218 loss: 0.00299354573
Iter: 219 loss: 0.00279625738
Iter: 220 loss: 0.00276657613
Iter: 221 loss: 0.00276596961
Iter: 222 loss: 0.00274122041
Iter: 223 loss: 0.00270338985
Iter: 224 loss: 0.00308167608
Iter: 225 loss: 0.00270256866
Iter: 226 loss: 0.00266588805
Iter: 227 loss: 0.00409085304
Iter: 228 loss: 0.00266588526
Iter: 229 loss: 0.00264716195
Iter: 230 loss: 0.00264714193
Iter: 231 loss: 0.00263315253
Iter: 232 loss: 0.00263433857
Iter: 233 loss: 0.00262243766
Iter: 234 loss: 0.00259905262
Iter: 235 loss: 0.00265864423
Iter: 236 loss: 0.00258946838
Iter: 237 loss: 0.00257367035
Iter: 238 loss: 0.00257024565
Iter: 239 loss: 0.00255223131
Iter: 240 loss: 0.00255164108
Iter: 241 loss: 0.00253883726
Iter: 242 loss: 0.00261486461
Iter: 243 loss: 0.0025366277
Iter: 244 loss: 0.00252705417
Iter: 245 loss: 0.00254658214
Iter: 246 loss: 0.00252327
Iter: 247 loss: 0.00250456179
Iter: 248 loss: 0.00266026822
Iter: 249 loss: 0.00250328286
Iter: 250 loss: 0.00248509459
Iter: 251 loss: 0.00255239662
Iter: 252 loss: 0.0024805842
Iter: 253 loss: 0.00246542157
Iter: 254 loss: 0.00251918752
Iter: 255 loss: 0.0024614674
Iter: 256 loss: 0.00244097691
Iter: 257 loss: 0.00244789431
Iter: 258 loss: 0.00242637866
Iter: 259 loss: 0.00240705395
Iter: 260 loss: 0.0025125579
Iter: 261 loss: 0.00240277126
Iter: 262 loss: 0.00236420659
Iter: 263 loss: 0.00289811124
Iter: 264 loss: 0.00236410368
Iter: 265 loss: 0.00234224088
Iter: 266 loss: 0.00250206864
Iter: 267 loss: 0.00233775913
Iter: 268 loss: 0.00230180938
Iter: 269 loss: 0.00249854522
Iter: 270 loss: 0.00229694368
Iter: 271 loss: 0.00227631908
Iter: 272 loss: 0.00227596355
Iter: 273 loss: 0.00225162925
Iter: 274 loss: 0.00251061469
Iter: 275 loss: 0.00225107279
Iter: 276 loss: 0.00222814782
Iter: 277 loss: 0.00227832678
Iter: 278 loss: 0.00221902831
Iter: 279 loss: 0.00219757808
Iter: 280 loss: 0.00235507591
Iter: 281 loss: 0.00219480623
Iter: 282 loss: 0.00217224937
Iter: 283 loss: 0.00217224914
Iter: 284 loss: 0.00215473562
Iter: 285 loss: 0.00237254053
Iter: 286 loss: 0.00215410092
Iter: 287 loss: 0.00213713711
Iter: 288 loss: 0.00217366219
Iter: 289 loss: 0.00213046651
Iter: 290 loss: 0.00210649031
Iter: 291 loss: 0.00249123527
Iter: 292 loss: 0.00210647681
Iter: 293 loss: 0.00208324427
Iter: 294 loss: 0.00217198394
Iter: 295 loss: 0.00207749475
Iter: 296 loss: 0.002060981
Iter: 297 loss: 0.00210721092
Iter: 298 loss: 0.00205563288
Iter: 299 loss: 0.00203293376
Iter: 300 loss: 0.00208744267
Iter: 301 loss: 0.00202276814
Iter: 302 loss: 0.0020088749
Iter: 303 loss: 0.00203482225
Iter: 304 loss: 0.00200296473
Iter: 305 loss: 0.00198100274
Iter: 306 loss: 0.00204027072
Iter: 307 loss: 0.00197147019
Iter: 308 loss: 0.00193539157
Iter: 309 loss: 0.00214482564
Iter: 310 loss: 0.00193056418
Iter: 311 loss: 0.00193209376
Iter: 312 loss: 0.00191528164
Iter: 313 loss: 0.00189483771
Iter: 314 loss: 0.00189873017
Iter: 315 loss: 0.00187939615
Iter: 316 loss: 0.00186084968
Iter: 317 loss: 0.00199135835
Iter: 318 loss: 0.00185913348
Iter: 319 loss: 0.00184692512
Iter: 320 loss: 0.00184740778
Iter: 321 loss: 0.00183643063
Iter: 322 loss: 0.00180078205
Iter: 323 loss: 0.00180028868
Iter: 324 loss: 0.0017790494
Iter: 325 loss: 0.00177727418
Iter: 326 loss: 0.00175722409
Iter: 327 loss: 0.00187699287
Iter: 328 loss: 0.00175482151
Iter: 329 loss: 0.00173454254
Iter: 330 loss: 0.00170344
Iter: 331 loss: 0.00170287653
Iter: 332 loss: 0.0016661624
Iter: 333 loss: 0.00172197446
Iter: 334 loss: 0.00164734875
Iter: 335 loss: 0.00162438978
Iter: 336 loss: 0.00182525907
Iter: 337 loss: 0.00162333483
Iter: 338 loss: 0.00159444974
Iter: 339 loss: 0.0020856834
Iter: 340 loss: 0.00159332983
Iter: 341 loss: 0.00156924117
Iter: 342 loss: 0.00171241909
Iter: 343 loss: 0.00156617
Iter: 344 loss: 0.00155662734
Iter: 345 loss: 0.00155399507
Iter: 346 loss: 0.00154400524
Iter: 347 loss: 0.00155647821
Iter: 348 loss: 0.0015388215
Iter: 349 loss: 0.00152415829
Iter: 350 loss: 0.00168907596
Iter: 351 loss: 0.00152356806
Iter: 352 loss: 0.00150960649
Iter: 353 loss: 0.00151123945
Iter: 354 loss: 0.00149897113
Iter: 355 loss: 0.00148300501
Iter: 356 loss: 0.00151594798
Iter: 357 loss: 0.00147679669
Iter: 358 loss: 0.00146080565
Iter: 359 loss: 0.00151732517
Iter: 360 loss: 0.00145518908
Iter: 361 loss: 0.00143048051
Iter: 362 loss: 0.00151483505
Iter: 363 loss: 0.00142391783
Iter: 364 loss: 0.0014037271
Iter: 365 loss: 0.00171870214
Iter: 366 loss: 0.0014036881
Iter: 367 loss: 0.00138439238
Iter: 368 loss: 0.00141683687
Iter: 369 loss: 0.00137578056
Iter: 370 loss: 0.00136016251
Iter: 371 loss: 0.00136877166
Iter: 372 loss: 0.00134941831
Iter: 373 loss: 0.00133398292
Iter: 374 loss: 0.00134188682
Iter: 375 loss: 0.00132396258
Iter: 376 loss: 0.00130396476
Iter: 377 loss: 0.00134389848
Iter: 378 loss: 0.00129577483
Iter: 379 loss: 0.00128137635
Iter: 380 loss: 0.00127691252
Iter: 381 loss: 0.00126475317
Iter: 382 loss: 0.00128408766
Iter: 383 loss: 0.00125878234
Iter: 384 loss: 0.00123300147
Iter: 385 loss: 0.00132020377
Iter: 386 loss: 0.00122627779
Iter: 387 loss: 0.00120801141
Iter: 388 loss: 0.00120602013
Iter: 389 loss: 0.00119379128
Iter: 390 loss: 0.00119391363
Iter: 391 loss: 0.00118396105
Iter: 392 loss: 0.00116120151
Iter: 393 loss: 0.00119040243
Iter: 394 loss: 0.00114939525
Iter: 395 loss: 0.00112347165
Iter: 396 loss: 0.00123187713
Iter: 397 loss: 0.00111727731
Iter: 398 loss: 0.00109563605
Iter: 399 loss: 0.00111728651
Iter: 400 loss: 0.00108306296
Iter: 401 loss: 0.00108146295
Iter: 402 loss: 0.00107107928
Iter: 403 loss: 0.00106319156
Iter: 404 loss: 0.00113540678
Iter: 405 loss: 0.0010627287
Iter: 406 loss: 0.00105205341
Iter: 407 loss: 0.00104941311
Iter: 408 loss: 0.0010426729
Iter: 409 loss: 0.00101656443
Iter: 410 loss: 0.00120673236
Iter: 411 loss: 0.00101372728
Iter: 412 loss: 0.00100428914
Iter: 413 loss: 0.0010006337
Iter: 414 loss: 0.000993353897
Iter: 415 loss: 0.00099859538
Iter: 416 loss: 0.000988858868
Iter: 417 loss: 0.000979915494
Iter: 418 loss: 0.000984533224
Iter: 419 loss: 0.000973655726
Iter: 420 loss: 0.000962858088
Iter: 421 loss: 0.000963866827
Iter: 422 loss: 0.000954661169
Iter: 423 loss: 0.00093500549
Iter: 424 loss: 0.00114940782
Iter: 425 loss: 0.000934072246
Iter: 426 loss: 0.000917888
Iter: 427 loss: 0.000917875033
Iter: 428 loss: 0.000900619256
Iter: 429 loss: 0.00106191891
Iter: 430 loss: 0.000899943756
Iter: 431 loss: 0.000889559451
Iter: 432 loss: 0.000876853126
Iter: 433 loss: 0.000875698635
Iter: 434 loss: 0.000860283093
Iter: 435 loss: 0.000852655387
Iter: 436 loss: 0.000845301605
Iter: 437 loss: 0.000841942558
Iter: 438 loss: 0.000838277338
Iter: 439 loss: 0.000830129255
Iter: 440 loss: 0.00083117
Iter: 441 loss: 0.0008239171
Iter: 442 loss: 0.000812703569
Iter: 443 loss: 0.000830957899
Iter: 444 loss: 0.000807528035
Iter: 445 loss: 0.000800437178
Iter: 446 loss: 0.000838529901
Iter: 447 loss: 0.000799559639
Iter: 448 loss: 0.000793025596
Iter: 449 loss: 0.000817634165
Iter: 450 loss: 0.000791377795
Iter: 451 loss: 0.000784860225
Iter: 452 loss: 0.000771945808
Iter: 453 loss: 0.00101852242
Iter: 454 loss: 0.000771807041
Iter: 455 loss: 0.000757106056
Iter: 456 loss: 0.000775085529
Iter: 457 loss: 0.000749213388
Iter: 458 loss: 0.000732245855
Iter: 459 loss: 0.000820042565
Iter: 460 loss: 0.000729700434
Iter: 461 loss: 0.000711826375
Iter: 462 loss: 0.000856023806
Iter: 463 loss: 0.000710366818
Iter: 464 loss: 0.000699578319
Iter: 465 loss: 0.000697949843
Iter: 466 loss: 0.00069015281
Iter: 467 loss: 0.000684386352
Iter: 468 loss: 0.0006818
Iter: 469 loss: 0.000672802154
Iter: 470 loss: 0.000683833438
Iter: 471 loss: 0.000667476445
Iter: 472 loss: 0.000654231233
Iter: 473 loss: 0.000766061479
Iter: 474 loss: 0.000653497933
Iter: 475 loss: 0.000656122
Iter: 476 loss: 0.000648325891
Iter: 477 loss: 0.000644801185
Iter: 478 loss: 0.000641384861
Iter: 479 loss: 0.000640612794
Iter: 480 loss: 0.000629961491
Iter: 481 loss: 0.000622695487
Iter: 482 loss: 0.000618774793
Iter: 483 loss: 0.00061868655
Iter: 484 loss: 0.000613666663
Iter: 485 loss: 0.000608512084
Iter: 486 loss: 0.000598691
Iter: 487 loss: 0.000799322035
Iter: 488 loss: 0.000598648
Iter: 489 loss: 0.00058881822
Iter: 490 loss: 0.000647421693
Iter: 491 loss: 0.000587549468
Iter: 492 loss: 0.000579036248
Iter: 493 loss: 0.000570153468
Iter: 494 loss: 0.000568497169
Iter: 495 loss: 0.000555008708
Iter: 496 loss: 0.000576153
Iter: 497 loss: 0.000549130375
Iter: 498 loss: 0.000539883564
Iter: 499 loss: 0.000547015923
Iter: 500 loss: 0.00053409941
Iter: 501 loss: 0.000527714
Iter: 502 loss: 0.000611738127
Iter: 503 loss: 0.000527678349
Iter: 504 loss: 0.000523086404
Iter: 505 loss: 0.000515175634
Iter: 506 loss: 0.000515170686
Iter: 507 loss: 0.000504438765
Iter: 508 loss: 0.000522825227
Iter: 509 loss: 0.000499438203
Iter: 510 loss: 0.000496587949
Iter: 511 loss: 0.000493626751
Iter: 512 loss: 0.000490970095
Iter: 513 loss: 0.000492085936
Iter: 514 loss: 0.000489165599
Iter: 515 loss: 0.000482692965
Iter: 516 loss: 0.000509063364
Iter: 517 loss: 0.00048118757
Iter: 518 loss: 0.000474007189
Iter: 519 loss: 0.000549031713
Iter: 520 loss: 0.000473789551
Iter: 521 loss: 0.000468433078
Iter: 522 loss: 0.000475058099
Iter: 523 loss: 0.00046565698
Iter: 524 loss: 0.000459425908
Iter: 525 loss: 0.000453045708
Iter: 526 loss: 0.000451793894
Iter: 527 loss: 0.000443077966
Iter: 528 loss: 0.000445884391
Iter: 529 loss: 0.000436893955
Iter: 530 loss: 0.000426199666
Iter: 531 loss: 0.000434878515
Iter: 532 loss: 0.000419811491
Iter: 533 loss: 0.0004124837
Iter: 534 loss: 0.000412276073
Iter: 535 loss: 0.000405366591
Iter: 536 loss: 0.000435088092
Iter: 537 loss: 0.000403912854
Iter: 538 loss: 0.000399339071
Iter: 539 loss: 0.000429559383
Iter: 540 loss: 0.000398871372
Iter: 541 loss: 0.000396281714
Iter: 542 loss: 0.000396267686
Iter: 543 loss: 0.00039372241
Iter: 544 loss: 0.000390754198
Iter: 545 loss: 0.000390393368
Iter: 546 loss: 0.00038514749
Iter: 547 loss: 0.000381805323
Iter: 548 loss: 0.000379757956
Iter: 549 loss: 0.000374128431
Iter: 550 loss: 0.000390263274
Iter: 551 loss: 0.00037220103
Iter: 552 loss: 0.000369500223
Iter: 553 loss: 0.000378762023
Iter: 554 loss: 0.000368777663
Iter: 555 loss: 0.000364030449
Iter: 556 loss: 0.000360198552
Iter: 557 loss: 0.000358772
Iter: 558 loss: 0.00035381355
Iter: 559 loss: 0.00041189816
Iter: 560 loss: 0.000353723299
Iter: 561 loss: 0.000349467737
Iter: 562 loss: 0.000348902395
Iter: 563 loss: 0.000345896638
Iter: 564 loss: 0.000342049112
Iter: 565 loss: 0.000352512754
Iter: 566 loss: 0.000340701023
Iter: 567 loss: 0.000337076315
Iter: 568 loss: 0.000331922696
Iter: 569 loss: 0.000331739779
Iter: 570 loss: 0.000328840018
Iter: 571 loss: 0.000328160386
Iter: 572 loss: 0.000325879897
Iter: 573 loss: 0.000354316726
Iter: 574 loss: 0.000325856672
Iter: 575 loss: 0.000324136723
Iter: 576 loss: 0.000333082338
Iter: 577 loss: 0.00032385136
Iter: 578 loss: 0.000322154956
Iter: 579 loss: 0.000322026026
Iter: 580 loss: 0.000320761348
Iter: 581 loss: 0.00031753935
Iter: 582 loss: 0.000313634489
Iter: 583 loss: 0.000313241937
Iter: 584 loss: 0.000310104631
Iter: 585 loss: 0.000322085252
Iter: 586 loss: 0.000309384079
Iter: 587 loss: 0.000307367067
Iter: 588 loss: 0.000306274625
Iter: 589 loss: 0.000305374182
Iter: 590 loss: 0.000301887456
Iter: 591 loss: 0.000339048798
Iter: 592 loss: 0.000301794615
Iter: 593 loss: 0.000300090236
Iter: 594 loss: 0.000300628919
Iter: 595 loss: 0.000298879982
Iter: 596 loss: 0.000297015591
Iter: 597 loss: 0.000294394093
Iter: 598 loss: 0.000294295285
Iter: 599 loss: 0.000290487311
Iter: 600 loss: 0.000290970551
Iter: 601 loss: 0.000287580129
Iter: 602 loss: 0.00028581536
Iter: 603 loss: 0.000285775808
Iter: 604 loss: 0.000284479058
Iter: 605 loss: 0.000292280485
Iter: 606 loss: 0.000284317182
Iter: 607 loss: 0.000282598718
Iter: 608 loss: 0.000281218207
Iter: 609 loss: 0.000280698237
Iter: 610 loss: 0.000277030602
Iter: 611 loss: 0.000283824978
Iter: 612 loss: 0.000275468279
Iter: 613 loss: 0.000273428683
Iter: 614 loss: 0.000273419166
Iter: 615 loss: 0.000271509867
Iter: 616 loss: 0.000278074207
Iter: 617 loss: 0.000271024386
Iter: 618 loss: 0.000269381329
Iter: 619 loss: 0.000266170769
Iter: 620 loss: 0.000331778196
Iter: 621 loss: 0.000266147079
Iter: 622 loss: 0.000264645321
Iter: 623 loss: 0.000263984635
Iter: 624 loss: 0.000261408481
Iter: 625 loss: 0.000259864086
Iter: 626 loss: 0.000258800865
Iter: 627 loss: 0.000255432504
Iter: 628 loss: 0.00030674733
Iter: 629 loss: 0.000255431107
Iter: 630 loss: 0.000252468482
Iter: 631 loss: 0.000275499595
Iter: 632 loss: 0.000252239261
Iter: 633 loss: 0.000250431767
Iter: 634 loss: 0.000248325
Iter: 635 loss: 0.000248085795
Iter: 636 loss: 0.000245486095
Iter: 637 loss: 0.00026413158
Iter: 638 loss: 0.000245258154
Iter: 639 loss: 0.000243976392
Iter: 640 loss: 0.000244399795
Iter: 641 loss: 0.000243059461
Iter: 642 loss: 0.000240479858
Iter: 643 loss: 0.000251511374
Iter: 644 loss: 0.000239957211
Iter: 645 loss: 0.000237197703
Iter: 646 loss: 0.000246926327
Iter: 647 loss: 0.000236464373
Iter: 648 loss: 0.000231889731
Iter: 649 loss: 0.000236784588
Iter: 650 loss: 0.000229374127
Iter: 651 loss: 0.0002291985
Iter: 652 loss: 0.00022823672
Iter: 653 loss: 0.000227530531
Iter: 654 loss: 0.000226479795
Iter: 655 loss: 0.000226455159
Iter: 656 loss: 0.00022291897
Iter: 657 loss: 0.000216970453
Iter: 658 loss: 0.000216951696
Iter: 659 loss: 0.000213974505
Iter: 660 loss: 0.000249615434
Iter: 661 loss: 0.000213933206
Iter: 662 loss: 0.000211514693
Iter: 663 loss: 0.000212621642
Iter: 664 loss: 0.000209886406
Iter: 665 loss: 0.000207585254
Iter: 666 loss: 0.000221552225
Iter: 667 loss: 0.000207306643
Iter: 668 loss: 0.00020605512
Iter: 669 loss: 0.000206008262
Iter: 670 loss: 0.000205423945
Iter: 671 loss: 0.000204354059
Iter: 672 loss: 0.00022998848
Iter: 673 loss: 0.000204354903
Iter: 674 loss: 0.000202016876
Iter: 675 loss: 0.000219873662
Iter: 676 loss: 0.00020182642
Iter: 677 loss: 0.000199952803
Iter: 678 loss: 0.000199641814
Iter: 679 loss: 0.000198358553
Iter: 680 loss: 0.000195735294
Iter: 681 loss: 0.000201398332
Iter: 682 loss: 0.000194705557
Iter: 683 loss: 0.000192704116
Iter: 684 loss: 0.000192570777
Iter: 685 loss: 0.000191478175
Iter: 686 loss: 0.000197136047
Iter: 687 loss: 0.000191308791
Iter: 688 loss: 0.000190573497
Iter: 689 loss: 0.000191111394
Iter: 690 loss: 0.000190119172
Iter: 691 loss: 0.000189052022
Iter: 692 loss: 0.000187051934
Iter: 693 loss: 0.000231493032
Iter: 694 loss: 0.000187047233
Iter: 695 loss: 0.000185288372
Iter: 696 loss: 0.000185241719
Iter: 697 loss: 0.00018385562
Iter: 698 loss: 0.000183392665
Iter: 699 loss: 0.000182594231
Iter: 700 loss: 0.000183727883
Iter: 701 loss: 0.000182040734
Iter: 702 loss: 0.000181542026
Iter: 703 loss: 0.0001805137
Iter: 704 loss: 0.000201793548
Iter: 705 loss: 0.000180495816
Iter: 706 loss: 0.000178307295
Iter: 707 loss: 0.00018368219
Iter: 708 loss: 0.000177538313
Iter: 709 loss: 0.000175122987
Iter: 710 loss: 0.000173218155
Iter: 711 loss: 0.000172486063
Iter: 712 loss: 0.000170835163
Iter: 713 loss: 0.000177232592
Iter: 714 loss: 0.00017045124
Iter: 715 loss: 0.000169341845
Iter: 716 loss: 0.000169337261
Iter: 717 loss: 0.000168378188
Iter: 718 loss: 0.000174758621
Iter: 719 loss: 0.000168280734
Iter: 720 loss: 0.000167661608
Iter: 721 loss: 0.000170382322
Iter: 722 loss: 0.000167536316
Iter: 723 loss: 0.000166945363
Iter: 724 loss: 0.000165624922
Iter: 725 loss: 0.000184142511
Iter: 726 loss: 0.000165555335
Iter: 727 loss: 0.000164227677
Iter: 728 loss: 0.000171455511
Iter: 729 loss: 0.000164029203
Iter: 730 loss: 0.000162705212
Iter: 731 loss: 0.000166932135
Iter: 732 loss: 0.000162327095
Iter: 733 loss: 0.000162155164
Iter: 734 loss: 0.000161829681
Iter: 735 loss: 0.000161378557
Iter: 736 loss: 0.000160092837
Iter: 737 loss: 0.000166260143
Iter: 738 loss: 0.000159640942
Iter: 739 loss: 0.000158230134
Iter: 740 loss: 0.000165255129
Iter: 741 loss: 0.000157998744
Iter: 742 loss: 0.000157055896
Iter: 743 loss: 0.000158415205
Iter: 744 loss: 0.000156599053
Iter: 745 loss: 0.000155553644
Iter: 746 loss: 0.000155211834
Iter: 747 loss: 0.000154605135
Iter: 748 loss: 0.000153287692
Iter: 749 loss: 0.000158035982
Iter: 750 loss: 0.000152948749
Iter: 751 loss: 0.000151898654
Iter: 752 loss: 0.000157170754
Iter: 753 loss: 0.000151724293
Iter: 754 loss: 0.000150880194
Iter: 755 loss: 0.000163811637
Iter: 756 loss: 0.000150880383
Iter: 757 loss: 0.000150218024
Iter: 758 loss: 0.000149184139
Iter: 759 loss: 0.000149170693
Iter: 760 loss: 0.0001481129
Iter: 761 loss: 0.000164000259
Iter: 762 loss: 0.000148112216
Iter: 763 loss: 0.00014746307
Iter: 764 loss: 0.000149395317
Iter: 765 loss: 0.000147266197
Iter: 766 loss: 0.000146534148
Iter: 767 loss: 0.000149060827
Iter: 768 loss: 0.000146343024
Iter: 769 loss: 0.000146109014
Iter: 770 loss: 0.000146047227
Iter: 771 loss: 0.000145786093
Iter: 772 loss: 0.000145011116
Iter: 773 loss: 0.000147594
Iter: 774 loss: 0.000144650519
Iter: 775 loss: 0.000143698722
Iter: 776 loss: 0.000142493373
Iter: 777 loss: 0.000142400764
Iter: 778 loss: 0.000140647287
Iter: 779 loss: 0.000140644988
Iter: 780 loss: 0.000139806754
Iter: 781 loss: 0.00013964533
Iter: 782 loss: 0.000139084092
Iter: 783 loss: 0.000137645344
Iter: 784 loss: 0.000136601739
Iter: 785 loss: 0.00013611489
Iter: 786 loss: 0.000136372284
Iter: 787 loss: 0.000135425958
Iter: 788 loss: 0.000135054637
Iter: 789 loss: 0.00013730365
Iter: 790 loss: 0.000135008595
Iter: 791 loss: 0.000134657326
Iter: 792 loss: 0.000133585476
Iter: 793 loss: 0.000136168921
Iter: 794 loss: 0.000132975518
Iter: 795 loss: 0.000133517853
Iter: 796 loss: 0.000132662055
Iter: 797 loss: 0.000132353918
Iter: 798 loss: 0.000132350062
Iter: 799 loss: 0.000132106565
Iter: 800 loss: 0.00013158872
Iter: 801 loss: 0.000134325557
Iter: 802 loss: 0.00013151
Iter: 803 loss: 0.000130952845
Iter: 804 loss: 0.000131428154
Iter: 805 loss: 0.000130623346
Iter: 806 loss: 0.000130318382
Iter: 807 loss: 0.00012948236
Iter: 808 loss: 0.000134540271
Iter: 809 loss: 0.000129256528
Iter: 810 loss: 0.000128209183
Iter: 811 loss: 0.000136877614
Iter: 812 loss: 0.000128147964
Iter: 813 loss: 0.000127104169
Iter: 814 loss: 0.00013033711
Iter: 815 loss: 0.000126804516
Iter: 816 loss: 0.000126038591
Iter: 817 loss: 0.000125471764
Iter: 818 loss: 0.000125217106
Iter: 819 loss: 0.00012437944
Iter: 820 loss: 0.000130901055
Iter: 821 loss: 0.000124318452
Iter: 822 loss: 0.000123827471
Iter: 823 loss: 0.000122713813
Iter: 824 loss: 0.000137338691
Iter: 825 loss: 0.000122643833
Iter: 826 loss: 0.000121605706
Iter: 827 loss: 0.000129407257
Iter: 828 loss: 0.000121527228
Iter: 829 loss: 0.000120804601
Iter: 830 loss: 0.000121489218
Iter: 831 loss: 0.000120392942
Iter: 832 loss: 0.000120958539
Iter: 833 loss: 0.000120018347
Iter: 834 loss: 0.000119588803
Iter: 835 loss: 0.000119507109
Iter: 836 loss: 0.000119363525
Iter: 837 loss: 0.000119783443
Iter: 838 loss: 0.000119319826
Iter: 839 loss: 0.000119149619
Iter: 840 loss: 0.000118640979
Iter: 841 loss: 0.000120203142
Iter: 842 loss: 0.000118392272
Iter: 843 loss: 0.000117252406
Iter: 844 loss: 0.000121241101
Iter: 845 loss: 0.000116956158
Iter: 846 loss: 0.000115924078
Iter: 847 loss: 0.000131795707
Iter: 848 loss: 0.000115924158
Iter: 849 loss: 0.000115680443
Iter: 850 loss: 0.000115314084
Iter: 851 loss: 0.00011530659
Iter: 852 loss: 0.000114849136
Iter: 853 loss: 0.000114849361
Iter: 854 loss: 0.000114496768
Iter: 855 loss: 0.000113752016
Iter: 856 loss: 0.000125680293
Iter: 857 loss: 0.000113730101
Iter: 858 loss: 0.000112634807
Iter: 859 loss: 0.000114801769
Iter: 860 loss: 0.000112194699
Iter: 861 loss: 0.000113112532
Iter: 862 loss: 0.000111641828
Iter: 863 loss: 0.000111399735
Iter: 864 loss: 0.000110773515
Iter: 865 loss: 0.000115925213
Iter: 866 loss: 0.000110660214
Iter: 867 loss: 0.000110308552
Iter: 868 loss: 0.000110270521
Iter: 869 loss: 0.000110032721
Iter: 870 loss: 0.000110803223
Iter: 871 loss: 0.000109964945
Iter: 872 loss: 0.00010944932
Iter: 873 loss: 0.000112215472
Iter: 874 loss: 0.000109374014
Iter: 875 loss: 0.000108539658
Iter: 876 loss: 0.000111690504
Iter: 877 loss: 0.000108343389
Iter: 878 loss: 0.000107921136
Iter: 879 loss: 0.00010804773
Iter: 880 loss: 0.00010761978
Iter: 881 loss: 0.000106548963
Iter: 882 loss: 0.000106648731
Iter: 883 loss: 0.00010572475
Iter: 884 loss: 0.000104987063
Iter: 885 loss: 0.000104987048
Iter: 886 loss: 0.000104501392
Iter: 887 loss: 0.000104371051
Iter: 888 loss: 0.000104071674
Iter: 889 loss: 0.000103461833
Iter: 890 loss: 0.000102526537
Iter: 891 loss: 0.000102511629
Iter: 892 loss: 0.000101825426
Iter: 893 loss: 0.000107414569
Iter: 894 loss: 0.000101781174
Iter: 895 loss: 0.000101273115
Iter: 896 loss: 0.000101599464
Iter: 897 loss: 0.000100950827
Iter: 898 loss: 0.000100838864
Iter: 899 loss: 0.000100609177
Iter: 900 loss: 0.000100107514
Iter: 901 loss: 0.000100535355
Iter: 902 loss: 9.98146643e-05
Iter: 903 loss: 9.9200166e-05
Iter: 904 loss: 9.82142374e-05
Iter: 905 loss: 9.82060155e-05
Iter: 906 loss: 9.71181216e-05
Iter: 907 loss: 0.000101248675
Iter: 908 loss: 9.68619424e-05
Iter: 909 loss: 9.6644304e-05
Iter: 910 loss: 9.65674917e-05
Iter: 911 loss: 9.62426711e-05
Iter: 912 loss: 9.58013115e-05
Iter: 913 loss: 9.57800221e-05
Iter: 914 loss: 9.5429481e-05
Iter: 915 loss: 9.5411604e-05
Iter: 916 loss: 9.51758775e-05
Iter: 917 loss: 9.47978697e-05
Iter: 918 loss: 9.47949e-05
Iter: 919 loss: 9.42828192e-05
Iter: 920 loss: 9.4789124e-05
Iter: 921 loss: 9.3994764e-05
Iter: 922 loss: 9.36927099e-05
Iter: 923 loss: 9.36751894e-05
Iter: 924 loss: 9.33400224e-05
Iter: 925 loss: 9.24559863e-05
Iter: 926 loss: 9.89338878e-05
Iter: 927 loss: 9.22709442e-05
Iter: 928 loss: 9.23843e-05
Iter: 929 loss: 9.18055084e-05
Iter: 930 loss: 9.14917036e-05
Iter: 931 loss: 9.14732591e-05
Iter: 932 loss: 9.13825934e-05
Iter: 933 loss: 9.10819726e-05
Iter: 934 loss: 9.10789095e-05
Iter: 935 loss: 9.07694193e-05
Iter: 936 loss: 9.00550367e-05
Iter: 937 loss: 9.03937e-05
Iter: 938 loss: 8.95748672e-05
Iter: 939 loss: 8.92012e-05
Iter: 940 loss: 9.43235791e-05
Iter: 941 loss: 8.91997915e-05
Iter: 942 loss: 8.87352217e-05
Iter: 943 loss: 8.8951e-05
Iter: 944 loss: 8.84207911e-05
Iter: 945 loss: 8.8288507e-05
Iter: 946 loss: 8.82443419e-05
Iter: 947 loss: 8.81078449e-05
Iter: 948 loss: 8.79078725e-05
Iter: 949 loss: 8.79029394e-05
Iter: 950 loss: 8.75122641e-05
Iter: 951 loss: 8.68812349e-05
Iter: 952 loss: 8.68768e-05
Iter: 953 loss: 8.65361217e-05
Iter: 954 loss: 9.12200558e-05
Iter: 955 loss: 8.65342809e-05
Iter: 956 loss: 8.61208828e-05
Iter: 957 loss: 8.53559759e-05
Iter: 958 loss: 0.000102531885
Iter: 959 loss: 8.5353764e-05
Iter: 960 loss: 8.46775e-05
Iter: 961 loss: 8.82945751e-05
Iter: 962 loss: 8.45766408e-05
Iter: 963 loss: 8.53097154e-05
Iter: 964 loss: 8.4312851e-05
Iter: 965 loss: 8.41908914e-05
Iter: 966 loss: 8.38900451e-05
Iter: 967 loss: 8.68478746e-05
Iter: 968 loss: 8.38508859e-05
Iter: 969 loss: 8.35023311e-05
Iter: 970 loss: 8.31895886e-05
Iter: 971 loss: 8.31025827e-05
Iter: 972 loss: 8.25239258e-05
Iter: 973 loss: 8.23653172e-05
Iter: 974 loss: 8.20140413e-05
Iter: 975 loss: 8.15845342e-05
Iter: 976 loss: 8.15372332e-05
Iter: 977 loss: 8.12567887e-05
Iter: 978 loss: 8.19620182e-05
Iter: 979 loss: 8.11592909e-05
Iter: 980 loss: 8.08552577e-05
Iter: 981 loss: 8.01686401e-05
Iter: 982 loss: 8.93139513e-05
Iter: 983 loss: 8.01262213e-05
Iter: 984 loss: 7.98051406e-05
Iter: 985 loss: 7.98046094e-05
Iter: 986 loss: 7.94377e-05
Iter: 987 loss: 8.18821e-05
Iter: 988 loss: 7.94019943e-05
Iter: 989 loss: 7.92414576e-05
Iter: 990 loss: 7.87681201e-05
Iter: 991 loss: 8.04587471e-05
Iter: 992 loss: 7.8557583e-05
Iter: 993 loss: 7.81150302e-05
Iter: 994 loss: 7.85846787e-05
Iter: 995 loss: 7.78708418e-05
Iter: 996 loss: 7.81339841e-05
Iter: 997 loss: 7.77556706e-05
Iter: 998 loss: 7.76021479e-05
Iter: 999 loss: 7.74102373e-05
Iter: 1000 loss: 7.73949e-05
Iter: 1001 loss: 7.70406332e-05
Iter: 1002 loss: 7.67396559e-05
Iter: 1003 loss: 7.66415178e-05
Iter: 1004 loss: 7.59276591e-05
Iter: 1005 loss: 8.2350678e-05
Iter: 1006 loss: 7.5894568e-05
Iter: 1007 loss: 7.54725042e-05
Iter: 1008 loss: 7.88377802e-05
Iter: 1009 loss: 7.54466382e-05
Iter: 1010 loss: 7.52059714e-05
Iter: 1011 loss: 7.45549696e-05
Iter: 1012 loss: 7.86737437e-05
Iter: 1013 loss: 7.43923883e-05
Iter: 1014 loss: 7.41162221e-05
Iter: 1015 loss: 7.41103067e-05
Iter: 1016 loss: 7.39711177e-05
Iter: 1017 loss: 7.39887182e-05
Iter: 1018 loss: 7.38654198e-05
Iter: 1019 loss: 7.36635047e-05
Iter: 1020 loss: 7.459e-05
Iter: 1021 loss: 7.36250513e-05
Iter: 1022 loss: 7.34218702e-05
Iter: 1023 loss: 7.37824448e-05
Iter: 1024 loss: 7.33320048e-05
Iter: 1025 loss: 7.29033636e-05
Iter: 1026 loss: 7.43291457e-05
Iter: 1027 loss: 7.27861334e-05
Iter: 1028 loss: 7.28904124e-05
Iter: 1029 loss: 7.26508733e-05
Iter: 1030 loss: 7.25449572e-05
Iter: 1031 loss: 7.24693382e-05
Iter: 1032 loss: 7.24332276e-05
Iter: 1033 loss: 7.22495679e-05
Iter: 1034 loss: 7.1838047e-05
Iter: 1035 loss: 7.74552e-05
Iter: 1036 loss: 7.18139199e-05
Iter: 1037 loss: 7.14844355e-05
Iter: 1038 loss: 7.14200214e-05
Iter: 1039 loss: 7.12024048e-05
Iter: 1040 loss: 7.09497399e-05
Iter: 1041 loss: 7.01814206e-05
Iter: 1042 loss: 7.21663309e-05
Iter: 1043 loss: 6.97598662e-05
Iter: 1044 loss: 6.90421584e-05
Iter: 1045 loss: 7.30523316e-05
Iter: 1046 loss: 6.89425e-05
Iter: 1047 loss: 6.88837608e-05
Iter: 1048 loss: 6.8803718e-05
Iter: 1049 loss: 6.86730709e-05
Iter: 1050 loss: 6.83552498e-05
Iter: 1051 loss: 7.17054209e-05
Iter: 1052 loss: 6.83208782e-05
Iter: 1053 loss: 6.81029705e-05
Iter: 1054 loss: 6.80817757e-05
Iter: 1055 loss: 6.7912988e-05
Iter: 1056 loss: 6.81810561e-05
Iter: 1057 loss: 6.78359502e-05
Iter: 1058 loss: 6.75874471e-05
Iter: 1059 loss: 6.74045441e-05
Iter: 1060 loss: 6.73207833e-05
Iter: 1061 loss: 6.72089809e-05
Iter: 1062 loss: 6.71913149e-05
Iter: 1063 loss: 6.70131267e-05
Iter: 1064 loss: 6.70496156e-05
Iter: 1065 loss: 6.68820721e-05
Iter: 1066 loss: 6.67527929e-05
Iter: 1067 loss: 6.6492e-05
Iter: 1068 loss: 7.12767869e-05
Iter: 1069 loss: 6.64880936e-05
Iter: 1070 loss: 6.62499151e-05
Iter: 1071 loss: 6.91769601e-05
Iter: 1072 loss: 6.6246379e-05
Iter: 1073 loss: 6.61300146e-05
Iter: 1074 loss: 6.59815123e-05
Iter: 1075 loss: 6.59696379e-05
Iter: 1076 loss: 6.57449345e-05
Iter: 1077 loss: 6.55136901e-05
Iter: 1078 loss: 6.54686592e-05
Iter: 1079 loss: 6.53893949e-05
Iter: 1080 loss: 6.5353961e-05
Iter: 1081 loss: 6.5210028e-05
Iter: 1082 loss: 6.53994211e-05
Iter: 1083 loss: 6.51370137e-05
Iter: 1084 loss: 6.49648427e-05
Iter: 1085 loss: 6.71673e-05
Iter: 1086 loss: 6.49625872e-05
Iter: 1087 loss: 6.47669513e-05
Iter: 1088 loss: 6.47851411e-05
Iter: 1089 loss: 6.46163389e-05
Iter: 1090 loss: 6.43721e-05
Iter: 1091 loss: 6.42034211e-05
Iter: 1092 loss: 6.41155711e-05
Iter: 1093 loss: 6.39436257e-05
Iter: 1094 loss: 6.39360587e-05
Iter: 1095 loss: 6.37409394e-05
Iter: 1096 loss: 6.45411128e-05
Iter: 1097 loss: 6.36984259e-05
Iter: 1098 loss: 6.36188488e-05
Iter: 1099 loss: 6.34389289e-05
Iter: 1100 loss: 6.58943e-05
Iter: 1101 loss: 6.34294847e-05
Iter: 1102 loss: 6.32879528e-05
Iter: 1103 loss: 6.46496701e-05
Iter: 1104 loss: 6.32825249e-05
Iter: 1105 loss: 6.30967043e-05
Iter: 1106 loss: 6.30268478e-05
Iter: 1107 loss: 6.29249625e-05
Iter: 1108 loss: 6.27574846e-05
Iter: 1109 loss: 6.27310437e-05
Iter: 1110 loss: 6.26155233e-05
Iter: 1111 loss: 6.23730157e-05
Iter: 1112 loss: 6.28800772e-05
Iter: 1113 loss: 6.22783118e-05
Iter: 1114 loss: 6.19092098e-05
Iter: 1115 loss: 6.3889951e-05
Iter: 1116 loss: 6.18536578e-05
Iter: 1117 loss: 6.17463957e-05
Iter: 1118 loss: 6.14969467e-05
Iter: 1119 loss: 6.45594409e-05
Iter: 1120 loss: 6.14772289e-05
Iter: 1121 loss: 6.1382e-05
Iter: 1122 loss: 6.13391312e-05
Iter: 1123 loss: 6.12198e-05
Iter: 1124 loss: 6.12490257e-05
Iter: 1125 loss: 6.1133047e-05
Iter: 1126 loss: 6.09808703e-05
Iter: 1127 loss: 6.14635064e-05
Iter: 1128 loss: 6.09375e-05
Iter: 1129 loss: 6.07674883e-05
Iter: 1130 loss: 6.07499569e-05
Iter: 1131 loss: 6.06765389e-05
Iter: 1132 loss: 6.04466659e-05
Iter: 1133 loss: 6.08286064e-05
Iter: 1134 loss: 6.02899745e-05
Iter: 1135 loss: 6.00871208e-05
Iter: 1136 loss: 6.18775593e-05
Iter: 1137 loss: 6.00765e-05
Iter: 1138 loss: 5.99006235e-05
Iter: 1139 loss: 6.14492383e-05
Iter: 1140 loss: 5.98911647e-05
Iter: 1141 loss: 5.97773251e-05
Iter: 1142 loss: 5.96034624e-05
Iter: 1143 loss: 5.95997299e-05
Iter: 1144 loss: 5.95051133e-05
Iter: 1145 loss: 5.95139427e-05
Iter: 1146 loss: 5.94319827e-05
Iter: 1147 loss: 5.92708093e-05
Iter: 1148 loss: 6.1035229e-05
Iter: 1149 loss: 5.92672113e-05
Iter: 1150 loss: 5.91525641e-05
Iter: 1151 loss: 5.89533374e-05
Iter: 1152 loss: 5.89530027e-05
Iter: 1153 loss: 5.88334733e-05
Iter: 1154 loss: 5.88287621e-05
Iter: 1155 loss: 5.87195536e-05
Iter: 1156 loss: 5.91626558e-05
Iter: 1157 loss: 5.86958e-05
Iter: 1158 loss: 5.86262104e-05
Iter: 1159 loss: 5.84196059e-05
Iter: 1160 loss: 5.90561685e-05
Iter: 1161 loss: 5.83163637e-05
Iter: 1162 loss: 5.86249371e-05
Iter: 1163 loss: 5.82156572e-05
Iter: 1164 loss: 5.81834611e-05
Iter: 1165 loss: 5.81457716e-05
Iter: 1166 loss: 5.81413442e-05
Iter: 1167 loss: 5.77258652e-05
Iter: 1168 loss: 6.10775896e-05
Iter: 1169 loss: 5.76991e-05
Iter: 1170 loss: 5.77948667e-05
Iter: 1171 loss: 5.7559264e-05
Iter: 1172 loss: 5.74994192e-05
Iter: 1173 loss: 5.7681129e-05
Iter: 1174 loss: 5.74808291e-05
Iter: 1175 loss: 5.73748111e-05
Iter: 1176 loss: 5.71256096e-05
Iter: 1177 loss: 6.008325e-05
Iter: 1178 loss: 5.71045821e-05
Iter: 1179 loss: 5.70054581e-05
Iter: 1180 loss: 5.76988969e-05
Iter: 1181 loss: 5.69968979e-05
Iter: 1182 loss: 5.68859032e-05
Iter: 1183 loss: 5.67066818e-05
Iter: 1184 loss: 5.67050556e-05
Iter: 1185 loss: 5.64863694e-05
Iter: 1186 loss: 5.69444346e-05
Iter: 1187 loss: 5.63987342e-05
Iter: 1188 loss: 5.62755e-05
Iter: 1189 loss: 5.62640635e-05
Iter: 1190 loss: 5.61563356e-05
Iter: 1191 loss: 5.67899297e-05
Iter: 1192 loss: 5.61431043e-05
Iter: 1193 loss: 5.60895314e-05
Iter: 1194 loss: 5.63585854e-05
Iter: 1195 loss: 5.60811495e-05
Iter: 1196 loss: 5.60157787e-05
Iter: 1197 loss: 5.60793887e-05
Iter: 1198 loss: 5.59798136e-05
Iter: 1199 loss: 5.58919419e-05
Iter: 1200 loss: 5.56772e-05
Iter: 1201 loss: 5.781871e-05
Iter: 1202 loss: 5.56516e-05
Iter: 1203 loss: 5.55420338e-05
Iter: 1204 loss: 5.55414517e-05
Iter: 1205 loss: 5.543511e-05
Iter: 1206 loss: 5.53409118e-05
Iter: 1207 loss: 5.53133541e-05
Iter: 1208 loss: 5.55604056e-05
Iter: 1209 loss: 5.52804122e-05
Iter: 1210 loss: 5.52641686e-05
Iter: 1211 loss: 5.52203164e-05
Iter: 1212 loss: 5.56304949e-05
Iter: 1213 loss: 5.5214703e-05
Iter: 1214 loss: 5.51025878e-05
Iter: 1215 loss: 5.49164033e-05
Iter: 1216 loss: 5.4915603e-05
Iter: 1217 loss: 5.48048847e-05
Iter: 1218 loss: 5.58584434e-05
Iter: 1219 loss: 5.48011194e-05
Iter: 1220 loss: 5.47179698e-05
Iter: 1221 loss: 5.45834191e-05
Iter: 1222 loss: 5.45837247e-05
Iter: 1223 loss: 5.45739022e-05
Iter: 1224 loss: 5.45157491e-05
Iter: 1225 loss: 5.44814175e-05
Iter: 1226 loss: 5.44143877e-05
Iter: 1227 loss: 5.58816537e-05
Iter: 1228 loss: 5.4414304e-05
Iter: 1229 loss: 5.42716734e-05
Iter: 1230 loss: 5.5687211e-05
Iter: 1231 loss: 5.42673224e-05
Iter: 1232 loss: 5.41071422e-05
Iter: 1233 loss: 5.4510343e-05
Iter: 1234 loss: 5.40512119e-05
Iter: 1235 loss: 5.39394023e-05
Iter: 1236 loss: 5.39088214e-05
Iter: 1237 loss: 5.38396926e-05
Iter: 1238 loss: 5.37454362e-05
Iter: 1239 loss: 5.40153851e-05
Iter: 1240 loss: 5.37157757e-05
Iter: 1241 loss: 5.36509688e-05
Iter: 1242 loss: 5.36507905e-05
Iter: 1243 loss: 5.35514373e-05
Iter: 1244 loss: 5.3570584e-05
Iter: 1245 loss: 5.34781175e-05
Iter: 1246 loss: 5.336514e-05
Iter: 1247 loss: 5.35184809e-05
Iter: 1248 loss: 5.33084203e-05
Iter: 1249 loss: 5.32071863e-05
Iter: 1250 loss: 5.29996978e-05
Iter: 1251 loss: 5.66629096e-05
Iter: 1252 loss: 5.2995445e-05
Iter: 1253 loss: 5.2836167e-05
Iter: 1254 loss: 5.28330056e-05
Iter: 1255 loss: 5.27188e-05
Iter: 1256 loss: 5.26854419e-05
Iter: 1257 loss: 5.26170261e-05
Iter: 1258 loss: 5.25463292e-05
Iter: 1259 loss: 5.2508658e-05
Iter: 1260 loss: 5.24822899e-05
Iter: 1261 loss: 5.24227798e-05
Iter: 1262 loss: 5.3139167e-05
Iter: 1263 loss: 5.24172865e-05
Iter: 1264 loss: 5.26647054e-05
Iter: 1265 loss: 5.23643903e-05
Iter: 1266 loss: 5.23133567e-05
Iter: 1267 loss: 5.22748815e-05
Iter: 1268 loss: 5.22586888e-05
Iter: 1269 loss: 5.21628572e-05
Iter: 1270 loss: 5.19700516e-05
Iter: 1271 loss: 5.55719e-05
Iter: 1272 loss: 5.19674613e-05
Iter: 1273 loss: 5.18327979e-05
Iter: 1274 loss: 5.23123636e-05
Iter: 1275 loss: 5.179859e-05
Iter: 1276 loss: 5.17022963e-05
Iter: 1277 loss: 5.27037191e-05
Iter: 1278 loss: 5.17003209e-05
Iter: 1279 loss: 5.16166947e-05
Iter: 1280 loss: 5.17378503e-05
Iter: 1281 loss: 5.15762295e-05
Iter: 1282 loss: 5.14657731e-05
Iter: 1283 loss: 5.12547485e-05
Iter: 1284 loss: 5.5697521e-05
Iter: 1285 loss: 5.12542319e-05
Iter: 1286 loss: 5.10989557e-05
Iter: 1287 loss: 5.10277168e-05
Iter: 1288 loss: 5.09512101e-05
Iter: 1289 loss: 5.06693e-05
Iter: 1290 loss: 5.20968024e-05
Iter: 1291 loss: 5.06227116e-05
Iter: 1292 loss: 5.04609488e-05
Iter: 1293 loss: 5.04586787e-05
Iter: 1294 loss: 5.02712137e-05
Iter: 1295 loss: 5.1394105e-05
Iter: 1296 loss: 5.02497642e-05
Iter: 1297 loss: 5.02157345e-05
Iter: 1298 loss: 5.0130071e-05
Iter: 1299 loss: 5.08886e-05
Iter: 1300 loss: 5.01165705e-05
Iter: 1301 loss: 4.99800226e-05
Iter: 1302 loss: 5.06523793e-05
Iter: 1303 loss: 4.99567832e-05
Iter: 1304 loss: 4.98711051e-05
Iter: 1305 loss: 4.9977607e-05
Iter: 1306 loss: 4.98266963e-05
Iter: 1307 loss: 4.978168e-05
Iter: 1308 loss: 4.97321707e-05
Iter: 1309 loss: 4.97249421e-05
Iter: 1310 loss: 4.95979184e-05
Iter: 1311 loss: 5.0925446e-05
Iter: 1312 loss: 4.9594757e-05
Iter: 1313 loss: 4.95051427e-05
Iter: 1314 loss: 4.94990672e-05
Iter: 1315 loss: 4.94547239e-05
Iter: 1316 loss: 4.9399e-05
Iter: 1317 loss: 4.93949301e-05
Iter: 1318 loss: 4.93196276e-05
Iter: 1319 loss: 4.94848282e-05
Iter: 1320 loss: 4.92912877e-05
Iter: 1321 loss: 4.92415638e-05
Iter: 1322 loss: 4.91221654e-05
Iter: 1323 loss: 5.04037816e-05
Iter: 1324 loss: 4.91099781e-05
Iter: 1325 loss: 4.90429375e-05
Iter: 1326 loss: 4.94107408e-05
Iter: 1327 loss: 4.90334533e-05
Iter: 1328 loss: 4.89693157e-05
Iter: 1329 loss: 4.94899214e-05
Iter: 1330 loss: 4.89657359e-05
Iter: 1331 loss: 4.89300546e-05
Iter: 1332 loss: 4.93744155e-05
Iter: 1333 loss: 4.89296981e-05
Iter: 1334 loss: 4.89103477e-05
Iter: 1335 loss: 4.8873997e-05
Iter: 1336 loss: 4.96625798e-05
Iter: 1337 loss: 4.88737569e-05
Iter: 1338 loss: 4.87821e-05
Iter: 1339 loss: 4.87763318e-05
Iter: 1340 loss: 4.86813406e-05
Iter: 1341 loss: 4.88535297e-05
Iter: 1342 loss: 4.86399622e-05
Iter: 1343 loss: 4.85470882e-05
Iter: 1344 loss: 4.83964432e-05
Iter: 1345 loss: 4.83950062e-05
Iter: 1346 loss: 4.82144023e-05
Iter: 1347 loss: 4.95234199e-05
Iter: 1348 loss: 4.81991447e-05
Iter: 1349 loss: 4.79504415e-05
Iter: 1350 loss: 4.91058e-05
Iter: 1351 loss: 4.79040937e-05
Iter: 1352 loss: 4.7842379e-05
Iter: 1353 loss: 4.79788396e-05
Iter: 1354 loss: 4.78184156e-05
Iter: 1355 loss: 4.77039066e-05
Iter: 1356 loss: 4.75265297e-05
Iter: 1357 loss: 4.75243578e-05
Iter: 1358 loss: 4.7376554e-05
Iter: 1359 loss: 4.80036688e-05
Iter: 1360 loss: 4.73462205e-05
Iter: 1361 loss: 4.71906e-05
Iter: 1362 loss: 4.761542e-05
Iter: 1363 loss: 4.71407548e-05
Iter: 1364 loss: 4.73839827e-05
Iter: 1365 loss: 4.71200037e-05
Iter: 1366 loss: 4.71089e-05
Iter: 1367 loss: 4.70725136e-05
Iter: 1368 loss: 4.70625528e-05
Iter: 1369 loss: 4.70313571e-05
Iter: 1370 loss: 4.69774277e-05
Iter: 1371 loss: 4.69484839e-05
Iter: 1372 loss: 4.68368817e-05
Iter: 1373 loss: 4.70527229e-05
Iter: 1374 loss: 4.67910213e-05
Iter: 1375 loss: 4.6710833e-05
Iter: 1376 loss: 4.6684494e-05
Iter: 1377 loss: 4.66371712e-05
Iter: 1378 loss: 4.65661324e-05
Iter: 1379 loss: 4.70953419e-05
Iter: 1380 loss: 4.65604317e-05
Iter: 1381 loss: 4.65023841e-05
Iter: 1382 loss: 4.65397752e-05
Iter: 1383 loss: 4.64652621e-05
Iter: 1384 loss: 4.64028308e-05
Iter: 1385 loss: 4.63209544e-05
Iter: 1386 loss: 4.63156211e-05
Iter: 1387 loss: 4.62168682e-05
Iter: 1388 loss: 4.63660872e-05
Iter: 1389 loss: 4.61695e-05
Iter: 1390 loss: 4.60934825e-05
Iter: 1391 loss: 4.59884286e-05
Iter: 1392 loss: 4.59841904e-05
Iter: 1393 loss: 4.58557188e-05
Iter: 1394 loss: 4.58433642e-05
Iter: 1395 loss: 4.57491697e-05
Iter: 1396 loss: 4.59318217e-05
Iter: 1397 loss: 4.57071073e-05
Iter: 1398 loss: 4.56524504e-05
Iter: 1399 loss: 4.56381094e-05
Iter: 1400 loss: 4.56049602e-05
Iter: 1401 loss: 4.5504843e-05
Iter: 1402 loss: 4.57135829e-05
Iter: 1403 loss: 4.54660403e-05
Iter: 1404 loss: 4.53855318e-05
Iter: 1405 loss: 4.54590772e-05
Iter: 1406 loss: 4.53395769e-05
Iter: 1407 loss: 4.52454588e-05
Iter: 1408 loss: 4.52447566e-05
Iter: 1409 loss: 4.51966189e-05
Iter: 1410 loss: 4.51950582e-05
Iter: 1411 loss: 4.51635678e-05
Iter: 1412 loss: 4.51121232e-05
Iter: 1413 loss: 4.51112282e-05
Iter: 1414 loss: 4.50428e-05
Iter: 1415 loss: 4.50755106e-05
Iter: 1416 loss: 4.49969084e-05
Iter: 1417 loss: 4.49112049e-05
Iter: 1418 loss: 4.60067567e-05
Iter: 1419 loss: 4.49106665e-05
Iter: 1420 loss: 4.48680657e-05
Iter: 1421 loss: 4.4797609e-05
Iter: 1422 loss: 4.47971179e-05
Iter: 1423 loss: 4.47165658e-05
Iter: 1424 loss: 4.53652101e-05
Iter: 1425 loss: 4.4711509e-05
Iter: 1426 loss: 4.46940467e-05
Iter: 1427 loss: 4.46890917e-05
Iter: 1428 loss: 4.46573977e-05
Iter: 1429 loss: 4.45995247e-05
Iter: 1430 loss: 4.59349976e-05
Iter: 1431 loss: 4.45993719e-05
Iter: 1432 loss: 4.45752885e-05
Iter: 1433 loss: 4.45175938e-05
Iter: 1434 loss: 4.51273299e-05
Iter: 1435 loss: 4.45111e-05
Iter: 1436 loss: 4.44552716e-05
Iter: 1437 loss: 4.44119287e-05
Iter: 1438 loss: 4.43936769e-05
Iter: 1439 loss: 4.43370554e-05
Iter: 1440 loss: 4.42800811e-05
Iter: 1441 loss: 4.42683522e-05
Iter: 1442 loss: 4.41295524e-05
Iter: 1443 loss: 4.4099339e-05
Iter: 1444 loss: 4.40097647e-05
Iter: 1445 loss: 4.38104289e-05
Iter: 1446 loss: 4.48495412e-05
Iter: 1447 loss: 4.37792914e-05
Iter: 1448 loss: 4.36702721e-05
Iter: 1449 loss: 4.37277558e-05
Iter: 1450 loss: 4.35991242e-05
Iter: 1451 loss: 4.35180082e-05
Iter: 1452 loss: 4.34764661e-05
Iter: 1453 loss: 4.33601272e-05
Iter: 1454 loss: 4.31753615e-05
Iter: 1455 loss: 4.3173859e-05
Iter: 1456 loss: 4.3107837e-05
Iter: 1457 loss: 4.31005101e-05
Iter: 1458 loss: 4.31868284e-05
Iter: 1459 loss: 4.30751534e-05
Iter: 1460 loss: 4.30513428e-05
Iter: 1461 loss: 4.30864675e-05
Iter: 1462 loss: 4.3040287e-05
Iter: 1463 loss: 4.30184955e-05
Iter: 1464 loss: 4.29588035e-05
Iter: 1465 loss: 4.32617962e-05
Iter: 1466 loss: 4.2939686e-05
Iter: 1467 loss: 4.36798e-05
Iter: 1468 loss: 4.28965977e-05
Iter: 1469 loss: 4.28393978e-05
Iter: 1470 loss: 4.28541753e-05
Iter: 1471 loss: 4.27975356e-05
Iter: 1472 loss: 4.27193809e-05
Iter: 1473 loss: 4.28698477e-05
Iter: 1474 loss: 4.268665e-05
Iter: 1475 loss: 4.25795224e-05
Iter: 1476 loss: 4.27347914e-05
Iter: 1477 loss: 4.2527281e-05
Iter: 1478 loss: 4.23485108e-05
Iter: 1479 loss: 4.22062512e-05
Iter: 1480 loss: 4.21513541e-05
Iter: 1481 loss: 4.20552569e-05
Iter: 1482 loss: 4.20339311e-05
Iter: 1483 loss: 4.1895506e-05
Iter: 1484 loss: 4.23904e-05
Iter: 1485 loss: 4.18598138e-05
Iter: 1486 loss: 4.18053096e-05
Iter: 1487 loss: 4.16633629e-05
Iter: 1488 loss: 4.27153e-05
Iter: 1489 loss: 4.16324838e-05
Iter: 1490 loss: 4.15301329e-05
Iter: 1491 loss: 4.3089356e-05
Iter: 1492 loss: 4.15300783e-05
Iter: 1493 loss: 4.15398172e-05
Iter: 1494 loss: 4.14777132e-05
Iter: 1495 loss: 4.14654642e-05
Iter: 1496 loss: 4.14317183e-05
Iter: 1497 loss: 4.16063776e-05
Iter: 1498 loss: 4.14215756e-05
Iter: 1499 loss: 4.14517417e-05
Iter: 1500 loss: 4.13797425e-05
Iter: 1501 loss: 4.13338385e-05
Iter: 1502 loss: 4.12892769e-05
Iter: 1503 loss: 4.12789595e-05
Iter: 1504 loss: 4.12019799e-05
Iter: 1505 loss: 4.11660076e-05
Iter: 1506 loss: 4.11287037e-05
Iter: 1507 loss: 4.10775028e-05
Iter: 1508 loss: 4.11997753e-05
Iter: 1509 loss: 4.10590583e-05
Iter: 1510 loss: 4.1000003e-05
Iter: 1511 loss: 4.09515633e-05
Iter: 1512 loss: 4.09340828e-05
Iter: 1513 loss: 4.08311e-05
Iter: 1514 loss: 4.07656407e-05
Iter: 1515 loss: 4.07255357e-05
Iter: 1516 loss: 4.05904e-05
Iter: 1517 loss: 4.03683553e-05
Iter: 1518 loss: 4.03673839e-05
Iter: 1519 loss: 4.01825819e-05
Iter: 1520 loss: 4.09811473e-05
Iter: 1521 loss: 4.01438301e-05
Iter: 1522 loss: 4.00419231e-05
Iter: 1523 loss: 4.00414137e-05
Iter: 1524 loss: 4.00322133e-05
Iter: 1525 loss: 4.0017585e-05
Iter: 1526 loss: 3.9987186e-05
Iter: 1527 loss: 3.99453347e-05
Iter: 1528 loss: 3.99427445e-05
Iter: 1529 loss: 3.98713528e-05
Iter: 1530 loss: 3.98558113e-05
Iter: 1531 loss: 3.98077173e-05
Iter: 1532 loss: 3.97002514e-05
Iter: 1533 loss: 4.03797021e-05
Iter: 1534 loss: 3.96878168e-05
Iter: 1535 loss: 3.96426694e-05
Iter: 1536 loss: 3.97939e-05
Iter: 1537 loss: 3.96299183e-05
Iter: 1538 loss: 3.95863135e-05
Iter: 1539 loss: 3.95355491e-05
Iter: 1540 loss: 3.95291936e-05
Iter: 1541 loss: 3.94324306e-05
Iter: 1542 loss: 4.00576027e-05
Iter: 1543 loss: 3.94219096e-05
Iter: 1544 loss: 3.93551e-05
Iter: 1545 loss: 3.93341143e-05
Iter: 1546 loss: 3.9295046e-05
Iter: 1547 loss: 3.91716603e-05
Iter: 1548 loss: 3.90749192e-05
Iter: 1549 loss: 3.90361747e-05
Iter: 1550 loss: 3.88748158e-05
Iter: 1551 loss: 3.88553744e-05
Iter: 1552 loss: 3.87404179e-05
Iter: 1553 loss: 3.86733809e-05
Iter: 1554 loss: 3.8646e-05
Iter: 1555 loss: 3.86273641e-05
Iter: 1556 loss: 3.86179636e-05
Iter: 1557 loss: 3.85859166e-05
Iter: 1558 loss: 3.90287605e-05
Iter: 1559 loss: 3.85858184e-05
Iter: 1560 loss: 3.85762032e-05
Iter: 1561 loss: 3.85463863e-05
Iter: 1562 loss: 3.86325337e-05
Iter: 1563 loss: 3.85307321e-05
Iter: 1564 loss: 3.847669e-05
Iter: 1565 loss: 3.88351036e-05
Iter: 1566 loss: 3.84716186e-05
Iter: 1567 loss: 3.84470914e-05
Iter: 1568 loss: 3.84740706e-05
Iter: 1569 loss: 3.84340747e-05
Iter: 1570 loss: 3.8399332e-05
Iter: 1571 loss: 3.83345614e-05
Iter: 1572 loss: 3.97716758e-05
Iter: 1573 loss: 3.83343795e-05
Iter: 1574 loss: 3.82519029e-05
Iter: 1575 loss: 3.89845809e-05
Iter: 1576 loss: 3.82472172e-05
Iter: 1577 loss: 3.82088292e-05
Iter: 1578 loss: 3.81813952e-05
Iter: 1579 loss: 3.81679856e-05
Iter: 1580 loss: 3.80968049e-05
Iter: 1581 loss: 3.80026177e-05
Iter: 1582 loss: 3.79966914e-05
Iter: 1583 loss: 3.78798977e-05
Iter: 1584 loss: 3.86556821e-05
Iter: 1585 loss: 3.78678596e-05
Iter: 1586 loss: 3.77679389e-05
Iter: 1587 loss: 3.78460682e-05
Iter: 1588 loss: 3.77078031e-05
Iter: 1589 loss: 3.75708e-05
Iter: 1590 loss: 3.77931065e-05
Iter: 1591 loss: 3.75081945e-05
Iter: 1592 loss: 3.83598417e-05
Iter: 1593 loss: 3.7486665e-05
Iter: 1594 loss: 3.74698284e-05
Iter: 1595 loss: 3.74156443e-05
Iter: 1596 loss: 3.74785668e-05
Iter: 1597 loss: 3.73740477e-05
Iter: 1598 loss: 3.73286712e-05
Iter: 1599 loss: 3.73197108e-05
Iter: 1600 loss: 3.72789582e-05
Iter: 1601 loss: 3.72897921e-05
Iter: 1602 loss: 3.72497052e-05
Iter: 1603 loss: 3.71680544e-05
Iter: 1604 loss: 3.70055e-05
Iter: 1605 loss: 4.01395155e-05
Iter: 1606 loss: 3.70042835e-05
Iter: 1607 loss: 3.69097324e-05
Iter: 1608 loss: 3.69076879e-05
Iter: 1609 loss: 3.68112742e-05
Iter: 1610 loss: 3.70589478e-05
Iter: 1611 loss: 3.67774664e-05
Iter: 1612 loss: 3.672807e-05
Iter: 1613 loss: 3.67271e-05
Iter: 1614 loss: 3.66883905e-05
Iter: 1615 loss: 3.66268287e-05
Iter: 1616 loss: 3.64797124e-05
Iter: 1617 loss: 3.8140206e-05
Iter: 1618 loss: 3.64653824e-05
Iter: 1619 loss: 3.64092484e-05
Iter: 1620 loss: 3.67115135e-05
Iter: 1621 loss: 3.64011612e-05
Iter: 1622 loss: 3.63845102e-05
Iter: 1623 loss: 3.63327e-05
Iter: 1624 loss: 3.63876534e-05
Iter: 1625 loss: 3.62914798e-05
Iter: 1626 loss: 3.63025756e-05
Iter: 1627 loss: 3.62651044e-05
Iter: 1628 loss: 3.62587743e-05
Iter: 1629 loss: 3.62421088e-05
Iter: 1630 loss: 3.63847066e-05
Iter: 1631 loss: 3.62389947e-05
Iter: 1632 loss: 3.61908969e-05
Iter: 1633 loss: 3.65687811e-05
Iter: 1634 loss: 3.61874336e-05
Iter: 1635 loss: 3.6117839e-05
Iter: 1636 loss: 3.62996798e-05
Iter: 1637 loss: 3.60944177e-05
Iter: 1638 loss: 3.60587292e-05
Iter: 1639 loss: 3.60263221e-05
Iter: 1640 loss: 3.60167687e-05
Iter: 1641 loss: 3.59626501e-05
Iter: 1642 loss: 3.68187066e-05
Iter: 1643 loss: 3.59627484e-05
Iter: 1644 loss: 3.59316728e-05
Iter: 1645 loss: 3.58567959e-05
Iter: 1646 loss: 3.66091772e-05
Iter: 1647 loss: 3.58472316e-05
Iter: 1648 loss: 3.57381832e-05
Iter: 1649 loss: 3.57223034e-05
Iter: 1650 loss: 3.56461787e-05
Iter: 1651 loss: 3.59842379e-05
Iter: 1652 loss: 3.55544798e-05
Iter: 1653 loss: 3.55208831e-05
Iter: 1654 loss: 3.54606927e-05
Iter: 1655 loss: 3.69327172e-05
Iter: 1656 loss: 3.54610529e-05
Iter: 1657 loss: 3.54238036e-05
Iter: 1658 loss: 3.5420926e-05
Iter: 1659 loss: 3.54027725e-05
Iter: 1660 loss: 3.55660341e-05
Iter: 1661 loss: 3.54019649e-05
Iter: 1662 loss: 3.53823489e-05
Iter: 1663 loss: 3.53193391e-05
Iter: 1664 loss: 3.54336298e-05
Iter: 1665 loss: 3.52781863e-05
Iter: 1666 loss: 3.52026764e-05
Iter: 1667 loss: 3.51988492e-05
Iter: 1668 loss: 3.51469862e-05
Iter: 1669 loss: 3.51844028e-05
Iter: 1670 loss: 3.51158887e-05
Iter: 1671 loss: 3.50477312e-05
Iter: 1672 loss: 3.49873662e-05
Iter: 1673 loss: 3.49697184e-05
Iter: 1674 loss: 3.49767688e-05
Iter: 1675 loss: 3.49317488e-05
Iter: 1676 loss: 3.48979956e-05
Iter: 1677 loss: 3.48347094e-05
Iter: 1678 loss: 3.61923558e-05
Iter: 1679 loss: 3.48341673e-05
Iter: 1680 loss: 3.47668065e-05
Iter: 1681 loss: 3.47952882e-05
Iter: 1682 loss: 3.47206733e-05
Iter: 1683 loss: 3.46868619e-05
Iter: 1684 loss: 3.4820263e-05
Iter: 1685 loss: 3.46784946e-05
Iter: 1686 loss: 3.46387242e-05
Iter: 1687 loss: 3.47437926e-05
Iter: 1688 loss: 3.46254856e-05
Iter: 1689 loss: 3.46086599e-05
Iter: 1690 loss: 3.45951266e-05
Iter: 1691 loss: 3.45730477e-05
Iter: 1692 loss: 3.45727276e-05
Iter: 1693 loss: 3.4558383e-05
Iter: 1694 loss: 3.45093795e-05
Iter: 1695 loss: 3.44274231e-05
Iter: 1696 loss: 3.44213586e-05
Iter: 1697 loss: 3.4337143e-05
Iter: 1698 loss: 3.43365318e-05
Iter: 1699 loss: 3.43181346e-05
Iter: 1700 loss: 3.42614803e-05
Iter: 1701 loss: 3.43850443e-05
Iter: 1702 loss: 3.42268468e-05
Iter: 1703 loss: 3.41367886e-05
Iter: 1704 loss: 3.4494944e-05
Iter: 1705 loss: 3.41164341e-05
Iter: 1706 loss: 3.40615225e-05
Iter: 1707 loss: 3.40114384e-05
Iter: 1708 loss: 3.39979888e-05
Iter: 1709 loss: 3.39586149e-05
Iter: 1710 loss: 3.39490216e-05
Iter: 1711 loss: 3.39006619e-05
Iter: 1712 loss: 3.40746192e-05
Iter: 1713 loss: 3.38880527e-05
Iter: 1714 loss: 3.38640166e-05
Iter: 1715 loss: 3.38003229e-05
Iter: 1716 loss: 3.42943204e-05
Iter: 1717 loss: 3.37882448e-05
Iter: 1718 loss: 3.37449092e-05
Iter: 1719 loss: 3.37415804e-05
Iter: 1720 loss: 3.37238744e-05
Iter: 1721 loss: 3.37635356e-05
Iter: 1722 loss: 3.37174752e-05
Iter: 1723 loss: 3.36910489e-05
Iter: 1724 loss: 3.39412472e-05
Iter: 1725 loss: 3.36903504e-05
Iter: 1726 loss: 3.36573685e-05
Iter: 1727 loss: 3.36182056e-05
Iter: 1728 loss: 3.36143203e-05
Iter: 1729 loss: 3.35569e-05
Iter: 1730 loss: 3.3549506e-05
Iter: 1731 loss: 3.35093282e-05
Iter: 1732 loss: 3.34658871e-05
Iter: 1733 loss: 3.33859716e-05
Iter: 1734 loss: 3.51930576e-05
Iter: 1735 loss: 3.33857934e-05
Iter: 1736 loss: 3.32595519e-05
Iter: 1737 loss: 3.38042737e-05
Iter: 1738 loss: 3.32345699e-05
Iter: 1739 loss: 3.31628544e-05
Iter: 1740 loss: 3.34666111e-05
Iter: 1741 loss: 3.31473493e-05
Iter: 1742 loss: 3.30975236e-05
Iter: 1743 loss: 3.30024704e-05
Iter: 1744 loss: 3.49885449e-05
Iter: 1745 loss: 3.30023395e-05
Iter: 1746 loss: 3.2947959e-05
Iter: 1747 loss: 3.29441e-05
Iter: 1748 loss: 3.28853312e-05
Iter: 1749 loss: 3.30732437e-05
Iter: 1750 loss: 3.2868782e-05
Iter: 1751 loss: 3.28426249e-05
Iter: 1752 loss: 3.29926734e-05
Iter: 1753 loss: 3.28389287e-05
Iter: 1754 loss: 3.28069036e-05
Iter: 1755 loss: 3.2818265e-05
Iter: 1756 loss: 3.27844755e-05
Iter: 1757 loss: 3.27492853e-05
Iter: 1758 loss: 3.28406823e-05
Iter: 1759 loss: 3.27379457e-05
Iter: 1760 loss: 3.26935551e-05
Iter: 1761 loss: 3.30245493e-05
Iter: 1762 loss: 3.26895752e-05
Iter: 1763 loss: 3.26726658e-05
Iter: 1764 loss: 3.26265435e-05
Iter: 1765 loss: 3.28855931e-05
Iter: 1766 loss: 3.26126828e-05
Iter: 1767 loss: 3.25300061e-05
Iter: 1768 loss: 3.25780202e-05
Iter: 1769 loss: 3.24756547e-05
Iter: 1770 loss: 3.23802815e-05
Iter: 1771 loss: 3.22195556e-05
Iter: 1772 loss: 3.22188716e-05
Iter: 1773 loss: 3.20933068e-05
Iter: 1774 loss: 3.20839463e-05
Iter: 1775 loss: 3.1967953e-05
Iter: 1776 loss: 3.30413459e-05
Iter: 1777 loss: 3.19640712e-05
Iter: 1778 loss: 3.19160863e-05
Iter: 1779 loss: 3.20235e-05
Iter: 1780 loss: 3.18979874e-05
Iter: 1781 loss: 3.18294551e-05
Iter: 1782 loss: 3.16662699e-05
Iter: 1783 loss: 3.34474462e-05
Iter: 1784 loss: 3.16503501e-05
Iter: 1785 loss: 3.23851491e-05
Iter: 1786 loss: 3.16183432e-05
Iter: 1787 loss: 3.1593554e-05
Iter: 1788 loss: 3.16556361e-05
Iter: 1789 loss: 3.15851503e-05
Iter: 1790 loss: 3.1554755e-05
Iter: 1791 loss: 3.14697027e-05
Iter: 1792 loss: 3.19873325e-05
Iter: 1793 loss: 3.14454774e-05
Iter: 1794 loss: 3.13732198e-05
Iter: 1795 loss: 3.20124273e-05
Iter: 1796 loss: 3.13700439e-05
Iter: 1797 loss: 3.13058881e-05
Iter: 1798 loss: 3.15852922e-05
Iter: 1799 loss: 3.12930206e-05
Iter: 1800 loss: 3.12237389e-05
Iter: 1801 loss: 3.15700527e-05
Iter: 1802 loss: 3.12113189e-05
Iter: 1803 loss: 3.11783297e-05
Iter: 1804 loss: 3.11215554e-05
Iter: 1805 loss: 3.11215335e-05
Iter: 1806 loss: 3.10276e-05
Iter: 1807 loss: 3.16729784e-05
Iter: 1808 loss: 3.10180876e-05
Iter: 1809 loss: 3.09537536e-05
Iter: 1810 loss: 3.10246687e-05
Iter: 1811 loss: 3.09185198e-05
Iter: 1812 loss: 3.08530252e-05
Iter: 1813 loss: 3.10250543e-05
Iter: 1814 loss: 3.08300587e-05
Iter: 1815 loss: 3.07617374e-05
Iter: 1816 loss: 3.13946839e-05
Iter: 1817 loss: 3.07589944e-05
Iter: 1818 loss: 3.06941e-05
Iter: 1819 loss: 3.07578302e-05
Iter: 1820 loss: 3.06573784e-05
Iter: 1821 loss: 3.07141199e-05
Iter: 1822 loss: 3.06186048e-05
Iter: 1823 loss: 3.05963586e-05
Iter: 1824 loss: 3.06100883e-05
Iter: 1825 loss: 3.0582225e-05
Iter: 1826 loss: 3.05569629e-05
Iter: 1827 loss: 3.0515992e-05
Iter: 1828 loss: 3.05153335e-05
Iter: 1829 loss: 3.04378827e-05
Iter: 1830 loss: 3.08616363e-05
Iter: 1831 loss: 3.04263667e-05
Iter: 1832 loss: 3.03956094e-05
Iter: 1833 loss: 3.03806155e-05
Iter: 1834 loss: 3.03459419e-05
Iter: 1835 loss: 3.02625358e-05
Iter: 1836 loss: 3.1157615e-05
Iter: 1837 loss: 3.02544086e-05
Iter: 1838 loss: 3.01585642e-05
Iter: 1839 loss: 3.03607594e-05
Iter: 1840 loss: 3.01203472e-05
Iter: 1841 loss: 3.00372521e-05
Iter: 1842 loss: 2.986447e-05
Iter: 1843 loss: 3.29674658e-05
Iter: 1844 loss: 2.98607683e-05
Iter: 1845 loss: 2.97713359e-05
Iter: 1846 loss: 3.10614705e-05
Iter: 1847 loss: 2.9771345e-05
Iter: 1848 loss: 2.96820581e-05
Iter: 1849 loss: 2.97618899e-05
Iter: 1850 loss: 2.96299659e-05
Iter: 1851 loss: 2.95735135e-05
Iter: 1852 loss: 2.9790077e-05
Iter: 1853 loss: 2.95605932e-05
Iter: 1854 loss: 2.95482769e-05
Iter: 1855 loss: 2.95331829e-05
Iter: 1856 loss: 2.94964157e-05
Iter: 1857 loss: 2.95192385e-05
Iter: 1858 loss: 2.94746078e-05
Iter: 1859 loss: 2.94351412e-05
Iter: 1860 loss: 2.93724333e-05
Iter: 1861 loss: 2.93721314e-05
Iter: 1862 loss: 2.92829463e-05
Iter: 1863 loss: 3.02692497e-05
Iter: 1864 loss: 2.92821223e-05
Iter: 1865 loss: 2.92181558e-05
Iter: 1866 loss: 2.96458238e-05
Iter: 1867 loss: 2.92123423e-05
Iter: 1868 loss: 2.91721608e-05
Iter: 1869 loss: 2.95045029e-05
Iter: 1870 loss: 2.91691522e-05
Iter: 1871 loss: 2.91463657e-05
Iter: 1872 loss: 2.91020624e-05
Iter: 1873 loss: 2.99972744e-05
Iter: 1874 loss: 2.91015422e-05
Iter: 1875 loss: 2.90129574e-05
Iter: 1876 loss: 2.89154559e-05
Iter: 1877 loss: 2.8901286e-05
Iter: 1878 loss: 2.87731873e-05
Iter: 1879 loss: 2.94350029e-05
Iter: 1880 loss: 2.87516632e-05
Iter: 1881 loss: 2.86805916e-05
Iter: 1882 loss: 2.97564602e-05
Iter: 1883 loss: 2.86807644e-05
Iter: 1884 loss: 2.86311461e-05
Iter: 1885 loss: 2.8576249e-05
Iter: 1886 loss: 2.85685492e-05
Iter: 1887 loss: 2.84881207e-05
Iter: 1888 loss: 2.89160689e-05
Iter: 1889 loss: 2.84739581e-05
Iter: 1890 loss: 2.84854214e-05
Iter: 1891 loss: 2.84625021e-05
Iter: 1892 loss: 2.84502603e-05
Iter: 1893 loss: 2.84299786e-05
Iter: 1894 loss: 2.84293619e-05
Iter: 1895 loss: 2.83858517e-05
Iter: 1896 loss: 2.8347753e-05
Iter: 1897 loss: 2.83356731e-05
Iter: 1898 loss: 2.83205773e-05
Iter: 1899 loss: 2.83043009e-05
Iter: 1900 loss: 2.82665842e-05
Iter: 1901 loss: 2.83402787e-05
Iter: 1902 loss: 2.82512374e-05
Iter: 1903 loss: 2.82173369e-05
Iter: 1904 loss: 2.83334721e-05
Iter: 1905 loss: 2.82080928e-05
Iter: 1906 loss: 2.8162267e-05
Iter: 1907 loss: 2.81239809e-05
Iter: 1908 loss: 2.81111697e-05
Iter: 1909 loss: 2.80150343e-05
Iter: 1910 loss: 2.80980476e-05
Iter: 1911 loss: 2.79591695e-05
Iter: 1912 loss: 2.79147134e-05
Iter: 1913 loss: 2.79226697e-05
Iter: 1914 loss: 2.78810567e-05
Iter: 1915 loss: 2.77976087e-05
Iter: 1916 loss: 2.79186606e-05
Iter: 1917 loss: 2.77581021e-05
Iter: 1918 loss: 2.78264342e-05
Iter: 1919 loss: 2.77159779e-05
Iter: 1920 loss: 2.76906085e-05
Iter: 1921 loss: 2.76648534e-05
Iter: 1922 loss: 2.76597293e-05
Iter: 1923 loss: 2.76175815e-05
Iter: 1924 loss: 2.76153878e-05
Iter: 1925 loss: 2.76037972e-05
Iter: 1926 loss: 2.75731709e-05
Iter: 1927 loss: 2.78201969e-05
Iter: 1928 loss: 2.75671518e-05
Iter: 1929 loss: 2.75077546e-05
Iter: 1930 loss: 2.78303232e-05
Iter: 1931 loss: 2.74988161e-05
Iter: 1932 loss: 2.74489139e-05
Iter: 1933 loss: 2.77304607e-05
Iter: 1934 loss: 2.74417289e-05
Iter: 1935 loss: 2.74210652e-05
Iter: 1936 loss: 2.74612976e-05
Iter: 1937 loss: 2.74132326e-05
Iter: 1938 loss: 2.73793958e-05
Iter: 1939 loss: 2.73514488e-05
Iter: 1940 loss: 2.73419701e-05
Iter: 1941 loss: 2.72929901e-05
Iter: 1942 loss: 2.73295827e-05
Iter: 1943 loss: 2.72627331e-05
Iter: 1944 loss: 2.72208163e-05
Iter: 1945 loss: 2.77504605e-05
Iter: 1946 loss: 2.72204325e-05
Iter: 1947 loss: 2.71792269e-05
Iter: 1948 loss: 2.70656983e-05
Iter: 1949 loss: 2.77542167e-05
Iter: 1950 loss: 2.70350702e-05
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.4/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.8
+ date
Sun Nov  8 15:13:49 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322c45c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322c889d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322b9d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322d02730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322d02158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322ba3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322ade840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322b60598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322b60048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322ab9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322ab92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f33229e7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322a038c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322a471e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322a42620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322a669d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322a77d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5d05f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3322abd9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5d05048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f332299b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5cef268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f332299bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c85c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c85950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c50ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32b0487d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c502f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c0a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c370d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32d5c12400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32b04db158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32b04db510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32b04d4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32b04321e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f32b0457c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.48700154
test_loss: 0.49055308
train_loss: 0.46593893
test_loss: 0.47122043
train_loss: 0.45372126
test_loss: 0.44546172
train_loss: 0.41408134
test_loss: 0.4154609
train_loss: 0.37890992
test_loss: 0.38370243
train_loss: 0.34845036
test_loss: 0.35039428
train_loss: 0.32031336
test_loss: 0.31547594
train_loss: 0.28060904
test_loss: 0.2792708
train_loss: 0.24340805
test_loss: 0.24191205
train_loss: 0.20383784
test_loss: 0.2049451
train_loss: 0.17433244
test_loss: 0.17094672
train_loss: 0.14078113
test_loss: 0.14164941
train_loss: 0.117814064
test_loss: 0.11757349
train_loss: 0.09768741
test_loss: 0.09838612
train_loss: 0.08473359
test_loss: 0.08388011
train_loss: 0.073400885
test_loss: 0.07401935
train_loss: 0.06907702
test_loss: 0.068387024
train_loss: 0.06412223
test_loss: 0.06561275
train_loss: 0.06319759
test_loss: 0.06436497
train_loss: 0.061797943
test_loss: 0.06373756
train_loss: 0.063238606
test_loss: 0.06337673
train_loss: 0.062373415
test_loss: 0.06313545
train_loss: 0.062313378
test_loss: 0.06296047
train_loss: 0.063229285
test_loss: 0.06283252
train_loss: 0.061690606
test_loss: 0.06270845
train_loss: 0.06133627
test_loss: 0.06260966
train_loss: 0.06401385
test_loss: 0.06251477
train_loss: 0.061234362
test_loss: 0.062401872
train_loss: 0.061885215
test_loss: 0.06237953
train_loss: 0.060625672
test_loss: 0.06228263
train_loss: 0.06166519
test_loss: 0.062166102
train_loss: 0.062761635
test_loss: 0.062118955
train_loss: 0.061633147
test_loss: 0.06196964
train_loss: 0.061406583
test_loss: 0.061848667
train_loss: 0.0609881
test_loss: 0.061727222
train_loss: 0.05978163
test_loss: 0.061569344
train_loss: 0.06078554
test_loss: 0.061368898
train_loss: 0.061763607
test_loss: 0.061185453
train_loss: 0.0602391
test_loss: 0.06096214
train_loss: 0.059894003
test_loss: 0.06076329
train_loss: 0.0603165
test_loss: 0.06047938
train_loss: 0.059510242
test_loss: 0.060210742
train_loss: 0.059585176
test_loss: 0.059971772
train_loss: 0.05827367
test_loss: 0.05974709
train_loss: 0.058508508
test_loss: 0.059596784
train_loss: 0.05890901
test_loss: 0.059209086
train_loss: 0.05888218
test_loss: 0.05903517
train_loss: 0.059137058
test_loss: 0.058711573
train_loss: 0.057280034
test_loss: 0.058460075
train_loss: 0.05726093
test_loss: 0.05821323
train_loss: 0.05720301
test_loss: 0.05794056
train_loss: 0.056390993
test_loss: 0.057677176
train_loss: 0.055792052
test_loss: 0.057357263
train_loss: 0.055929117
test_loss: 0.056880735
train_loss: 0.055755265
test_loss: 0.05640816
train_loss: 0.055945
test_loss: 0.05580481
train_loss: 0.054710906
test_loss: 0.05513195
train_loss: 0.055085957
test_loss: 0.054155853
train_loss: 0.052616913
test_loss: 0.05300797
train_loss: 0.050750665
test_loss: 0.051659092
train_loss: 0.049333982
test_loss: 0.05013545
train_loss: 0.04847934
test_loss: 0.048538487
train_loss: 0.045003727
test_loss: 0.046736628
train_loss: 0.04403948
test_loss: 0.044645414
train_loss: 0.040943757
test_loss: 0.042352766
train_loss: 0.038799558
test_loss: 0.040002376
train_loss: 0.03654048
test_loss: 0.037369397
train_loss: 0.03324253
test_loss: 0.034536988
train_loss: 0.030760368
test_loss: 0.031505186
train_loss: 0.027180826
test_loss: 0.027818833
train_loss: 0.024026113
test_loss: 0.024110215
train_loss: 0.020581508
test_loss: 0.020638198
train_loss: 0.016916005
test_loss: 0.017626416
train_loss: 0.014167286
test_loss: 0.01488016
train_loss: 0.012286585
test_loss: 0.012740644
train_loss: 0.010830435
test_loss: 0.011348542
train_loss: 0.010408332
test_loss: 0.010404031
train_loss: 0.009167238
test_loss: 0.009952922
train_loss: 0.008464707
test_loss: 0.008823276
train_loss: 0.008206389
test_loss: 0.00836352
train_loss: 0.0074788234
test_loss: 0.0081259655
train_loss: 0.0076480247
test_loss: 0.0074417195
train_loss: 0.006783188
test_loss: 0.007281548
train_loss: 0.006686287
test_loss: 0.006776295
train_loss: 0.0062102415
test_loss: 0.006652292
train_loss: 0.0061057913
test_loss: 0.006919803
train_loss: 0.005667397
test_loss: 0.005738978
train_loss: 0.0058682337
test_loss: 0.008888647
train_loss: 0.0055620503
test_loss: 0.00539806
train_loss: 0.0053197467
test_loss: 0.0050974702
train_loss: 0.004852638
test_loss: 0.005012508
train_loss: 0.0049937614
test_loss: 0.005018332
train_loss: 0.0045273434
test_loss: 0.0047184457
train_loss: 0.0050580488
test_loss: 0.005175895
train_loss: 0.004486068
test_loss: 0.0044619753
train_loss: 0.0046114447
test_loss: 0.004771621
train_loss: 0.005036513
test_loss: 0.004658835
train_loss: 0.003910587
test_loss: 0.004994685
train_loss: 0.004007464
test_loss: 0.0046958607
train_loss: 0.0040905373
test_loss: 0.0038870124
train_loss: 0.0052944412
test_loss: 0.0042480035
train_loss: 0.0046311156
test_loss: 0.0050274907
train_loss: 0.0036046864
test_loss: 0.0038097447
train_loss: 0.003908188
test_loss: 0.0038455522
train_loss: 0.0041842186
test_loss: 0.0042547663
train_loss: 0.0043501286
test_loss: 0.0038560752
train_loss: 0.0039767893
test_loss: 0.0042661536
train_loss: 0.003826624
test_loss: 0.004215628
train_loss: 0.003465327
test_loss: 0.004624725
train_loss: 0.0038219683
test_loss: 0.0034987049
train_loss: 0.0053857774
test_loss: 0.004026689
train_loss: 0.0038281952
test_loss: 0.0039653345
train_loss: 0.0033102836
test_loss: 0.0032615236
train_loss: 0.0029956256
test_loss: 0.0031763392
train_loss: 0.003590158
test_loss: 0.0036499684
train_loss: 0.0038162065
test_loss: 0.0040912116
train_loss: 0.0032672146
test_loss: 0.0038594855
train_loss: 0.003323847
test_loss: 0.0034757413
train_loss: 0.0029866288
test_loss: 0.0034355775
train_loss: 0.0035709587
test_loss: 0.0035322758
train_loss: 0.0038000164
test_loss: 0.0035233467
train_loss: 0.0036928132
test_loss: 0.0036026067
train_loss: 0.0031949936
test_loss: 0.0036052282
train_loss: 0.0036500741
test_loss: 0.0031205853
train_loss: 0.0030318233
test_loss: 0.0030286526
train_loss: 0.0033137002
test_loss: 0.0032224553
train_loss: 0.0030444781
test_loss: 0.0033301893
train_loss: 0.0033955644
test_loss: 0.0030449145
train_loss: 0.0032897547
test_loss: 0.0034647603
train_loss: 0.0033704618
test_loss: 0.0030433035
train_loss: 0.0028136584
test_loss: 0.0033419922
train_loss: 0.0040235957
test_loss: 0.0031555176
train_loss: 0.0030614631
test_loss: 0.003387405
train_loss: 0.0032269084
test_loss: 0.0033848847
train_loss: 0.002965702
test_loss: 0.0028464391
train_loss: 0.0028990281
test_loss: 0.0028269156
train_loss: 0.002921071
test_loss: 0.003083124
train_loss: 0.0030659232
test_loss: 0.0031089785
train_loss: 0.0032058014
test_loss: 0.0031333163
train_loss: 0.0028606462
test_loss: 0.0034927907
train_loss: 0.0033998708
test_loss: 0.0031778142
train_loss: 0.0032468615
test_loss: 0.00318104
train_loss: 0.0033862435
test_loss: 0.0034372122
train_loss: 0.0026705363
test_loss: 0.0031310993
train_loss: 0.0026409233
test_loss: 0.0031941775
train_loss: 0.0028027869
test_loss: 0.0030709498
train_loss: 0.0031692935
test_loss: 0.0029560712
train_loss: 0.0036115162
test_loss: 0.0030456954
train_loss: 0.0031118798
test_loss: 0.0032579429
train_loss: 0.0027854443
test_loss: 0.0026760935
train_loss: 0.002881075
test_loss: 0.0030882293
train_loss: 0.0030552018
test_loss: 0.0029760327
train_loss: 0.0031863407
test_loss: 0.0035426384
train_loss: 0.0033876218
test_loss: 0.0027618017
train_loss: 0.0031223397
test_loss: 0.003000641
train_loss: 0.0030620391
test_loss: 0.0027911116
train_loss: 0.0029491354
test_loss: 0.0034141787
train_loss: 0.0027412665
test_loss: 0.0028492175
train_loss: 0.0032791668
test_loss: 0.0032314148
train_loss: 0.0034319086
test_loss: 0.002742403
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 0 --phi 0.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi0.8/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc551c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc4ec950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc5897b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc49a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc49a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc5892f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc45c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc3c0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc3c0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc375158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc3759d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc324e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc352d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc3091e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc31bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc2d9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc2d9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc2d9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc24d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc1eba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29cc20a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3d3f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3d426a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3d72f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3d0b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3cf4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3cc0620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3cf4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3c441e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3c680d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f29b3c72510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f298c3d61e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f298c3c3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f298c3859d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f298c332268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f298c2dcf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.16335759e-05
Iter: 2 loss: 4.751503e-05
Iter: 3 loss: 9.12757241e-06
Iter: 4 loss: 7.81941435e-06
Iter: 5 loss: 8.61165245e-06
Iter: 6 loss: 6.97721e-06
Iter: 7 loss: 5.22997698e-06
Iter: 8 loss: 1.19700599e-05
Iter: 9 loss: 4.8192851e-06
Iter: 10 loss: 4.31621629e-06
Iter: 11 loss: 7.30855845e-06
Iter: 12 loss: 4.25394774e-06
Iter: 13 loss: 3.88240551e-06
Iter: 14 loss: 3.64620746e-06
Iter: 15 loss: 3.49985726e-06
Iter: 16 loss: 3.28820829e-06
Iter: 17 loss: 6.36246796e-06
Iter: 18 loss: 3.28798455e-06
Iter: 19 loss: 3.14566e-06
Iter: 20 loss: 3.38220161e-06
Iter: 21 loss: 3.08107019e-06
Iter: 22 loss: 2.98669329e-06
Iter: 23 loss: 3.78687901e-06
Iter: 24 loss: 2.9812511e-06
Iter: 25 loss: 2.91431888e-06
Iter: 26 loss: 2.94844062e-06
Iter: 27 loss: 2.86971044e-06
Iter: 28 loss: 2.79647429e-06
Iter: 29 loss: 3.08357676e-06
Iter: 30 loss: 2.77971344e-06
Iter: 31 loss: 2.73266187e-06
Iter: 32 loss: 2.71302179e-06
Iter: 33 loss: 2.68834219e-06
Iter: 34 loss: 2.64269738e-06
Iter: 35 loss: 3.2434225e-06
Iter: 36 loss: 2.64236132e-06
Iter: 37 loss: 2.61169521e-06
Iter: 38 loss: 2.61114337e-06
Iter: 39 loss: 2.59840931e-06
Iter: 40 loss: 2.67185396e-06
Iter: 41 loss: 2.59664034e-06
Iter: 42 loss: 2.58428759e-06
Iter: 43 loss: 2.55598388e-06
Iter: 44 loss: 2.92391883e-06
Iter: 45 loss: 2.55414443e-06
Iter: 46 loss: 2.52763539e-06
Iter: 47 loss: 2.92706454e-06
Iter: 48 loss: 2.52754398e-06
Iter: 49 loss: 2.5146046e-06
Iter: 50 loss: 2.48943616e-06
Iter: 51 loss: 3.00989677e-06
Iter: 52 loss: 2.48923197e-06
Iter: 53 loss: 2.46737955e-06
Iter: 54 loss: 2.46739091e-06
Iter: 55 loss: 2.45428282e-06
Iter: 56 loss: 2.45367255e-06
Iter: 57 loss: 2.44361877e-06
Iter: 58 loss: 2.42525448e-06
Iter: 59 loss: 2.51729625e-06
Iter: 60 loss: 2.42226452e-06
Iter: 61 loss: 2.40679765e-06
Iter: 62 loss: 2.41012708e-06
Iter: 63 loss: 2.39563678e-06
Iter: 64 loss: 2.37593417e-06
Iter: 65 loss: 2.44454532e-06
Iter: 66 loss: 2.37068252e-06
Iter: 67 loss: 2.35718858e-06
Iter: 68 loss: 2.35575453e-06
Iter: 69 loss: 2.34589925e-06
Iter: 70 loss: 2.34649451e-06
Iter: 71 loss: 2.33970627e-06
Iter: 72 loss: 2.33282685e-06
Iter: 73 loss: 2.33798119e-06
Iter: 74 loss: 2.32853654e-06
Iter: 75 loss: 2.32287971e-06
Iter: 76 loss: 2.39197902e-06
Iter: 77 loss: 2.32280763e-06
Iter: 78 loss: 2.32002e-06
Iter: 79 loss: 2.31475337e-06
Iter: 80 loss: 2.44245712e-06
Iter: 81 loss: 2.31480976e-06
Iter: 82 loss: 2.30736146e-06
Iter: 83 loss: 2.34810705e-06
Iter: 84 loss: 2.3064481e-06
Iter: 85 loss: 2.30268915e-06
Iter: 86 loss: 2.2965296e-06
Iter: 87 loss: 2.29650823e-06
Iter: 88 loss: 2.28875433e-06
Iter: 89 loss: 2.38057919e-06
Iter: 90 loss: 2.28865383e-06
Iter: 91 loss: 2.28407453e-06
Iter: 92 loss: 2.28609656e-06
Iter: 93 loss: 2.28111458e-06
Iter: 94 loss: 2.27445184e-06
Iter: 95 loss: 2.29411535e-06
Iter: 96 loss: 2.27238934e-06
Iter: 97 loss: 2.26686461e-06
Iter: 98 loss: 2.26966063e-06
Iter: 99 loss: 2.26319094e-06
Iter: 100 loss: 2.2559866e-06
Iter: 101 loss: 2.27851569e-06
Iter: 102 loss: 2.25385156e-06
Iter: 103 loss: 2.24851237e-06
Iter: 104 loss: 2.24817677e-06
Iter: 105 loss: 2.24420933e-06
Iter: 106 loss: 2.24977975e-06
Iter: 107 loss: 2.24214227e-06
Iter: 108 loss: 2.24008932e-06
Iter: 109 loss: 2.23680422e-06
Iter: 110 loss: 2.23677716e-06
Iter: 111 loss: 2.2328461e-06
Iter: 112 loss: 2.26800307e-06
Iter: 113 loss: 2.2324964e-06
Iter: 114 loss: 2.23068719e-06
Iter: 115 loss: 2.22996732e-06
Iter: 116 loss: 2.2289621e-06
Iter: 117 loss: 2.22515382e-06
Iter: 118 loss: 2.22627091e-06
Iter: 119 loss: 2.222391e-06
Iter: 120 loss: 2.21912887e-06
Iter: 121 loss: 2.21932441e-06
Iter: 122 loss: 2.2165641e-06
Iter: 123 loss: 2.21308323e-06
Iter: 124 loss: 2.21299479e-06
Iter: 125 loss: 2.21110872e-06
Iter: 126 loss: 2.20847e-06
Iter: 127 loss: 2.2083791e-06
Iter: 128 loss: 2.20403854e-06
Iter: 129 loss: 2.23141433e-06
Iter: 130 loss: 2.20360607e-06
Iter: 131 loss: 2.20111747e-06
Iter: 132 loss: 2.20175343e-06
Iter: 133 loss: 2.19940625e-06
Iter: 134 loss: 2.19617e-06
Iter: 135 loss: 2.20694983e-06
Iter: 136 loss: 2.19533308e-06
Iter: 137 loss: 2.19250501e-06
Iter: 138 loss: 2.19231561e-06
Iter: 139 loss: 2.19023536e-06
Iter: 140 loss: 2.18953664e-06
Iter: 141 loss: 2.18884543e-06
Iter: 142 loss: 2.18701143e-06
Iter: 143 loss: 2.18579498e-06
Iter: 144 loss: 2.18505875e-06
Iter: 145 loss: 2.18374953e-06
Iter: 146 loss: 2.18766536e-06
Iter: 147 loss: 2.18349055e-06
Iter: 148 loss: 2.1815174e-06
Iter: 149 loss: 2.18032392e-06
Iter: 150 loss: 2.17951856e-06
Iter: 151 loss: 2.17764182e-06
Iter: 152 loss: 2.17813977e-06
Iter: 153 loss: 2.17630577e-06
Iter: 154 loss: 2.17394131e-06
Iter: 155 loss: 2.20729453e-06
Iter: 156 loss: 2.17388629e-06
Iter: 157 loss: 2.17258525e-06
Iter: 158 loss: 2.17037336e-06
Iter: 159 loss: 2.17038132e-06
Iter: 160 loss: 2.16811668e-06
Iter: 161 loss: 2.16811713e-06
Iter: 162 loss: 2.16692e-06
Iter: 163 loss: 2.16713465e-06
Iter: 164 loss: 2.16601984e-06
Iter: 165 loss: 2.16428452e-06
Iter: 166 loss: 2.18114678e-06
Iter: 167 loss: 2.16430226e-06
Iter: 168 loss: 2.16333092e-06
Iter: 169 loss: 2.16146236e-06
Iter: 170 loss: 2.20526181e-06
Iter: 171 loss: 2.16146282e-06
Iter: 172 loss: 2.15966611e-06
Iter: 173 loss: 2.1597034e-06
Iter: 174 loss: 2.15879368e-06
Iter: 175 loss: 2.15875798e-06
Iter: 176 loss: 2.15829164e-06
Iter: 177 loss: 2.15690716e-06
Iter: 178 loss: 2.16189369e-06
Iter: 179 loss: 2.15634054e-06
Iter: 180 loss: 2.15587534e-06
Iter: 181 loss: 2.15539558e-06
Iter: 182 loss: 2.15466844e-06
Iter: 183 loss: 2.15264072e-06
Iter: 184 loss: 2.15967339e-06
Iter: 185 loss: 2.151751e-06
Iter: 186 loss: 2.14993361e-06
Iter: 187 loss: 2.16852231e-06
Iter: 188 loss: 2.14984061e-06
Iter: 189 loss: 2.14825286e-06
Iter: 190 loss: 2.16361809e-06
Iter: 191 loss: 2.14813281e-06
Iter: 192 loss: 2.14720512e-06
Iter: 193 loss: 2.14580609e-06
Iter: 194 loss: 2.14583497e-06
Iter: 195 loss: 2.14433021e-06
Iter: 196 loss: 2.16705735e-06
Iter: 197 loss: 2.14434681e-06
Iter: 198 loss: 2.14337751e-06
Iter: 199 loss: 2.14295096e-06
Iter: 200 loss: 2.14240185e-06
Iter: 201 loss: 2.14102329e-06
Iter: 202 loss: 2.15109935e-06
Iter: 203 loss: 2.14091096e-06
Iter: 204 loss: 2.13990506e-06
Iter: 205 loss: 2.14027727e-06
Iter: 206 loss: 2.1391977e-06
Iter: 207 loss: 2.1382873e-06
Iter: 208 loss: 2.13823705e-06
Iter: 209 loss: 2.13766452e-06
Iter: 210 loss: 2.13667431e-06
Iter: 211 loss: 2.15699242e-06
Iter: 212 loss: 2.13661838e-06
Iter: 213 loss: 2.13573412e-06
Iter: 214 loss: 2.13874637e-06
Iter: 215 loss: 2.13542808e-06
Iter: 216 loss: 2.13430076e-06
Iter: 217 loss: 2.14122247e-06
Iter: 218 loss: 2.13413796e-06
Iter: 219 loss: 2.13343606e-06
Iter: 220 loss: 2.13178532e-06
Iter: 221 loss: 2.1495905e-06
Iter: 222 loss: 2.13170711e-06
Iter: 223 loss: 2.13002181e-06
Iter: 224 loss: 2.13572275e-06
Iter: 225 loss: 2.12971781e-06
Iter: 226 loss: 2.12837222e-06
Iter: 227 loss: 2.12831173e-06
Iter: 228 loss: 2.12767418e-06
Iter: 229 loss: 2.12599161e-06
Iter: 230 loss: 2.14727697e-06
Iter: 231 loss: 2.12587588e-06
Iter: 232 loss: 2.12498367e-06
Iter: 233 loss: 2.1247838e-06
Iter: 234 loss: 2.1239025e-06
Iter: 235 loss: 2.12363625e-06
Iter: 236 loss: 2.12312375e-06
Iter: 237 loss: 2.12203349e-06
Iter: 238 loss: 2.13094427e-06
Iter: 239 loss: 2.122046e-06
Iter: 240 loss: 2.12121768e-06
Iter: 241 loss: 2.12370219e-06
Iter: 242 loss: 2.12098416e-06
Iter: 243 loss: 2.11977249e-06
Iter: 244 loss: 2.12454825e-06
Iter: 245 loss: 2.11959627e-06
Iter: 246 loss: 2.11908468e-06
Iter: 247 loss: 2.11789597e-06
Iter: 248 loss: 2.12703799e-06
Iter: 249 loss: 2.11771703e-06
Iter: 250 loss: 2.11684801e-06
Iter: 251 loss: 2.11666793e-06
Iter: 252 loss: 2.11625229e-06
Iter: 253 loss: 2.11520887e-06
Iter: 254 loss: 2.12312852e-06
Iter: 255 loss: 2.11505653e-06
Iter: 256 loss: 2.1137339e-06
Iter: 257 loss: 2.1145604e-06
Iter: 258 loss: 2.1129722e-06
Iter: 259 loss: 2.11176848e-06
Iter: 260 loss: 2.12570353e-06
Iter: 261 loss: 2.11175848e-06
Iter: 262 loss: 2.11084171e-06
Iter: 263 loss: 2.11834868e-06
Iter: 264 loss: 2.11079123e-06
Iter: 265 loss: 2.11005408e-06
Iter: 266 loss: 2.11264705e-06
Iter: 267 loss: 2.10999406e-06
Iter: 268 loss: 2.10933581e-06
Iter: 269 loss: 2.10785402e-06
Iter: 270 loss: 2.1239016e-06
Iter: 271 loss: 2.1076421e-06
Iter: 272 loss: 2.10713347e-06
Iter: 273 loss: 2.1069452e-06
Iter: 274 loss: 2.10602957e-06
Iter: 275 loss: 2.10567532e-06
Iter: 276 loss: 2.10538747e-06
Iter: 277 loss: 2.10506369e-06
Iter: 278 loss: 2.10479698e-06
Iter: 279 loss: 2.10438566e-06
Iter: 280 loss: 2.10412645e-06
Iter: 281 loss: 2.10403527e-06
Iter: 282 loss: 2.10315989e-06
Iter: 283 loss: 2.10245207e-06
Iter: 284 loss: 2.10231883e-06
Iter: 285 loss: 2.10154076e-06
Iter: 286 loss: 2.10494431e-06
Iter: 287 loss: 2.10134e-06
Iter: 288 loss: 2.10041026e-06
Iter: 289 loss: 2.10393773e-06
Iter: 290 loss: 2.10007693e-06
Iter: 291 loss: 2.09951122e-06
Iter: 292 loss: 2.0982643e-06
Iter: 293 loss: 2.11213501e-06
Iter: 294 loss: 2.09806512e-06
Iter: 295 loss: 2.09690461e-06
Iter: 296 loss: 2.10188159e-06
Iter: 297 loss: 2.09661061e-06
Iter: 298 loss: 2.09560903e-06
Iter: 299 loss: 2.09848031e-06
Iter: 300 loss: 2.09509199e-06
Iter: 301 loss: 2.09458358e-06
Iter: 302 loss: 2.09445466e-06
Iter: 303 loss: 2.09393829e-06
Iter: 304 loss: 2.09306677e-06
Iter: 305 loss: 2.10916642e-06
Iter: 306 loss: 2.09302198e-06
Iter: 307 loss: 2.09225641e-06
Iter: 308 loss: 2.1026417e-06
Iter: 309 loss: 2.09229165e-06
Iter: 310 loss: 2.09157133e-06
Iter: 311 loss: 2.09096879e-06
Iter: 312 loss: 2.09075233e-06
Iter: 313 loss: 2.09044379e-06
Iter: 314 loss: 2.09036352e-06
Iter: 315 loss: 2.08982169e-06
Iter: 316 loss: 2.08961592e-06
Iter: 317 loss: 2.08946631e-06
Iter: 318 loss: 2.08883694e-06
Iter: 319 loss: 2.08822166e-06
Iter: 320 loss: 2.08813367e-06
Iter: 321 loss: 2.08729034e-06
Iter: 322 loss: 2.09445352e-06
Iter: 323 loss: 2.08733e-06
Iter: 324 loss: 2.08669144e-06
Iter: 325 loss: 2.08760093e-06
Iter: 326 loss: 2.08644701e-06
Iter: 327 loss: 2.08573738e-06
Iter: 328 loss: 2.08570327e-06
Iter: 329 loss: 2.08523829e-06
Iter: 330 loss: 2.0844202e-06
Iter: 331 loss: 2.08337019e-06
Iter: 332 loss: 2.08331357e-06
Iter: 333 loss: 2.08212441e-06
Iter: 334 loss: 2.08906954e-06
Iter: 335 loss: 2.08190841e-06
Iter: 336 loss: 2.08169627e-06
Iter: 337 loss: 2.08129177e-06
Iter: 338 loss: 2.08100664e-06
Iter: 339 loss: 2.0802313e-06
Iter: 340 loss: 2.08438655e-06
Iter: 341 loss: 2.07992753e-06
Iter: 342 loss: 2.07978292e-06
Iter: 343 loss: 2.07948051e-06
Iter: 344 loss: 2.07904327e-06
Iter: 345 loss: 2.07860512e-06
Iter: 346 loss: 2.07851735e-06
Iter: 347 loss: 2.07810149e-06
Iter: 348 loss: 2.07803669e-06
Iter: 349 loss: 2.07780772e-06
Iter: 350 loss: 2.07747235e-06
Iter: 351 loss: 2.07739276e-06
Iter: 352 loss: 2.07681137e-06
Iter: 353 loss: 2.0771065e-06
Iter: 354 loss: 2.0765774e-06
Iter: 355 loss: 2.07598168e-06
Iter: 356 loss: 2.07605626e-06
Iter: 357 loss: 2.07557741e-06
Iter: 358 loss: 2.075064e-06
Iter: 359 loss: 2.07499852e-06
Iter: 360 loss: 2.0741727e-06
Iter: 361 loss: 2.0740074e-06
Iter: 362 loss: 2.07357402e-06
Iter: 363 loss: 2.07257244e-06
Iter: 364 loss: 2.07239145e-06
Iter: 365 loss: 2.07172752e-06
Iter: 366 loss: 2.07038579e-06
Iter: 367 loss: 2.07462836e-06
Iter: 368 loss: 2.06998402e-06
Iter: 369 loss: 2.06991308e-06
Iter: 370 loss: 2.06939421e-06
Iter: 371 loss: 2.06891696e-06
Iter: 372 loss: 2.06808886e-06
Iter: 373 loss: 2.08982851e-06
Iter: 374 loss: 2.06805908e-06
Iter: 375 loss: 2.06726781e-06
Iter: 376 loss: 2.06796017e-06
Iter: 377 loss: 2.06669938e-06
Iter: 378 loss: 2.06687309e-06
Iter: 379 loss: 2.06634149e-06
Iter: 380 loss: 2.06614686e-06
Iter: 381 loss: 2.06648019e-06
Iter: 382 loss: 2.06592676e-06
Iter: 383 loss: 2.06539085e-06
Iter: 384 loss: 2.06464551e-06
Iter: 385 loss: 2.06464847e-06
Iter: 386 loss: 2.06424693e-06
Iter: 387 loss: 2.06438403e-06
Iter: 388 loss: 2.06389518e-06
Iter: 389 loss: 2.06323671e-06
Iter: 390 loss: 2.06856794e-06
Iter: 391 loss: 2.06311438e-06
Iter: 392 loss: 2.06270829e-06
Iter: 393 loss: 2.06654067e-06
Iter: 394 loss: 2.06271375e-06
Iter: 395 loss: 2.06228447e-06
Iter: 396 loss: 2.06147388e-06
Iter: 397 loss: 2.07658809e-06
Iter: 398 loss: 2.06143727e-06
Iter: 399 loss: 2.060518e-06
Iter: 400 loss: 2.06477148e-06
Iter: 401 loss: 2.06041614e-06
Iter: 402 loss: 2.059689e-06
Iter: 403 loss: 2.05889137e-06
Iter: 404 loss: 2.05884885e-06
Iter: 405 loss: 2.05770402e-06
Iter: 406 loss: 2.06482309e-06
Iter: 407 loss: 2.05768174e-06
Iter: 408 loss: 2.05666e-06
Iter: 409 loss: 2.0686723e-06
Iter: 410 loss: 2.05665606e-06
Iter: 411 loss: 2.05615788e-06
Iter: 412 loss: 2.0549578e-06
Iter: 413 loss: 2.06354639e-06
Iter: 414 loss: 2.05472793e-06
Iter: 415 loss: 2.05594051e-06
Iter: 416 loss: 2.0543298e-06
Iter: 417 loss: 2.05396668e-06
Iter: 418 loss: 2.05324432e-06
Iter: 419 loss: 2.06949107e-06
Iter: 420 loss: 2.05322885e-06
Iter: 421 loss: 2.05251627e-06
Iter: 422 loss: 2.05321362e-06
Iter: 423 loss: 2.05221932e-06
Iter: 424 loss: 2.0515e-06
Iter: 425 loss: 2.0565808e-06
Iter: 426 loss: 2.05139986e-06
Iter: 427 loss: 2.05083256e-06
Iter: 428 loss: 2.05371475e-06
Iter: 429 loss: 2.05064316e-06
Iter: 430 loss: 2.05021479e-06
Iter: 431 loss: 2.05222113e-06
Iter: 432 loss: 2.0501277e-06
Iter: 433 loss: 2.04956314e-06
Iter: 434 loss: 2.04916728e-06
Iter: 435 loss: 2.04898038e-06
Iter: 436 loss: 2.04830167e-06
Iter: 437 loss: 2.04955904e-06
Iter: 438 loss: 2.04812886e-06
Iter: 439 loss: 2.04721823e-06
Iter: 440 loss: 2.05061e-06
Iter: 441 loss: 2.04698813e-06
Iter: 442 loss: 2.0464829e-06
Iter: 443 loss: 2.04609682e-06
Iter: 444 loss: 2.04594016e-06
Iter: 445 loss: 2.04541334e-06
Iter: 446 loss: 2.04532807e-06
Iter: 447 loss: 2.04490789e-06
Iter: 448 loss: 2.04408025e-06
Iter: 449 loss: 2.05935021e-06
Iter: 450 loss: 2.04404137e-06
Iter: 451 loss: 2.04450726e-06
Iter: 452 loss: 2.0438365e-06
Iter: 453 loss: 2.04353501e-06
Iter: 454 loss: 2.04296157e-06
Iter: 455 loss: 2.04819844e-06
Iter: 456 loss: 2.04288244e-06
Iter: 457 loss: 2.04213393e-06
Iter: 458 loss: 2.04325693e-06
Iter: 459 loss: 2.04185267e-06
Iter: 460 loss: 2.04153821e-06
Iter: 461 loss: 2.04089747e-06
Iter: 462 loss: 2.04083472e-06
Iter: 463 loss: 2.04007983e-06
Iter: 464 loss: 2.04812295e-06
Iter: 465 loss: 2.04005937e-06
Iter: 466 loss: 2.03960894e-06
Iter: 467 loss: 2.03881928e-06
Iter: 468 loss: 2.03898526e-06
Iter: 469 loss: 2.0380794e-06
Iter: 470 loss: 2.04385333e-06
Iter: 471 loss: 2.03792956e-06
Iter: 472 loss: 2.03728041e-06
Iter: 473 loss: 2.03832883e-06
Iter: 474 loss: 2.03699847e-06
Iter: 475 loss: 2.03623085e-06
Iter: 476 loss: 2.03790614e-06
Iter: 477 loss: 2.03599802e-06
Iter: 478 loss: 2.03526201e-06
Iter: 479 loss: 2.04094226e-06
Iter: 480 loss: 2.03520108e-06
Iter: 481 loss: 2.03463946e-06
Iter: 482 loss: 2.03501349e-06
Iter: 483 loss: 2.03416357e-06
Iter: 484 loss: 2.03381614e-06
Iter: 485 loss: 2.03376203e-06
Iter: 486 loss: 2.03331024e-06
Iter: 487 loss: 2.03390027e-06
Iter: 488 loss: 2.03314266e-06
Iter: 489 loss: 2.03269951e-06
Iter: 490 loss: 2.03302557e-06
Iter: 491 loss: 2.03226818e-06
Iter: 492 loss: 2.03186187e-06
Iter: 493 loss: 2.0315133e-06
Iter: 494 loss: 2.03144737e-06
Iter: 495 loss: 2.03123273e-06
Iter: 496 loss: 2.03114791e-06
Iter: 497 loss: 2.03084801e-06
Iter: 498 loss: 2.03057925e-06
Iter: 499 loss: 2.03051923e-06
Iter: 500 loss: 2.03003356e-06
Iter: 501 loss: 2.0325615e-06
Iter: 502 loss: 2.02998854e-06
Iter: 503 loss: 2.02960587e-06
Iter: 504 loss: 2.02926367e-06
Iter: 505 loss: 2.02913e-06
Iter: 506 loss: 2.02836327e-06
Iter: 507 loss: 2.03086893e-06
Iter: 508 loss: 2.02829438e-06
Iter: 509 loss: 2.02766614e-06
Iter: 510 loss: 2.02760293e-06
Iter: 511 loss: 2.02719025e-06
Iter: 512 loss: 2.02651881e-06
Iter: 513 loss: 2.02948831e-06
Iter: 514 loss: 2.02628939e-06
Iter: 515 loss: 2.02570186e-06
Iter: 516 loss: 2.03210743e-06
Iter: 517 loss: 2.0256457e-06
Iter: 518 loss: 2.0253849e-06
Iter: 519 loss: 2.02765295e-06
Iter: 520 loss: 2.02528486e-06
Iter: 521 loss: 2.02495949e-06
Iter: 522 loss: 2.025085e-06
Iter: 523 loss: 2.02460842e-06
Iter: 524 loss: 2.02434899e-06
Iter: 525 loss: 2.02388242e-06
Iter: 526 loss: 2.02383217e-06
Iter: 527 loss: 2.02308206e-06
Iter: 528 loss: 2.02840488e-06
Iter: 529 loss: 2.02314482e-06
Iter: 530 loss: 2.02271985e-06
Iter: 531 loss: 2.02221327e-06
Iter: 532 loss: 2.02218234e-06
Iter: 533 loss: 2.02162e-06
Iter: 534 loss: 2.02163801e-06
Iter: 535 loss: 2.02127535e-06
Iter: 536 loss: 2.02124966e-06
Iter: 537 loss: 2.02095021e-06
Iter: 538 loss: 2.02049887e-06
Iter: 539 loss: 2.02344154e-06
Iter: 540 loss: 2.02041929e-06
Iter: 541 loss: 2.01999501e-06
Iter: 542 loss: 2.01996522e-06
Iter: 543 loss: 2.01974717e-06
Iter: 544 loss: 2.0191228e-06
Iter: 545 loss: 2.01889088e-06
Iter: 546 loss: 2.01852481e-06
Iter: 547 loss: 2.01774537e-06
Iter: 548 loss: 2.02202114e-06
Iter: 549 loss: 2.01753483e-06
Iter: 550 loss: 2.01696389e-06
Iter: 551 loss: 2.02323872e-06
Iter: 552 loss: 2.01692137e-06
Iter: 553 loss: 2.01666035e-06
Iter: 554 loss: 2.01661442e-06
Iter: 555 loss: 2.01620833e-06
Iter: 556 loss: 2.01574403e-06
Iter: 557 loss: 2.01582498e-06
Iter: 558 loss: 2.0154198e-06
Iter: 559 loss: 2.01532339e-06
Iter: 560 loss: 2.01488297e-06
Iter: 561 loss: 2.01410148e-06
Iter: 562 loss: 2.01773719e-06
Iter: 563 loss: 2.01385456e-06
Iter: 564 loss: 2.01338753e-06
Iter: 565 loss: 2.01373268e-06
Iter: 566 loss: 2.01304101e-06
Iter: 567 loss: 2.01244461e-06
Iter: 568 loss: 2.02039382e-06
Iter: 569 loss: 2.012398e-06
Iter: 570 loss: 2.01203648e-06
Iter: 571 loss: 2.01173702e-06
Iter: 572 loss: 2.0114926e-06
Iter: 573 loss: 2.01073522e-06
Iter: 574 loss: 2.01244166e-06
Iter: 575 loss: 2.01037301e-06
Iter: 576 loss: 2.00978866e-06
Iter: 577 loss: 2.01060379e-06
Iter: 578 loss: 2.0094883e-06
Iter: 579 loss: 2.00875138e-06
Iter: 580 loss: 2.00968816e-06
Iter: 581 loss: 2.00834893e-06
Iter: 582 loss: 2.00772729e-06
Iter: 583 loss: 2.01040825e-06
Iter: 584 loss: 2.00750173e-06
Iter: 585 loss: 2.00721115e-06
Iter: 586 loss: 2.00706882e-06
Iter: 587 loss: 2.00674276e-06
Iter: 588 loss: 2.00834256e-06
Iter: 589 loss: 2.0065911e-06
Iter: 590 loss: 2.00640261e-06
Iter: 591 loss: 2.00599948e-06
Iter: 592 loss: 2.01560806e-06
Iter: 593 loss: 2.00596969e-06
Iter: 594 loss: 2.00557542e-06
Iter: 595 loss: 2.00970862e-06
Iter: 596 loss: 2.00552745e-06
Iter: 597 loss: 2.00511931e-06
Iter: 598 loss: 2.00474915e-06
Iter: 599 loss: 2.00458e-06
Iter: 600 loss: 2.00420186e-06
Iter: 601 loss: 2.00415593e-06
Iter: 602 loss: 2.00369868e-06
Iter: 603 loss: 2.00358681e-06
Iter: 604 loss: 2.00327668e-06
Iter: 605 loss: 2.00288878e-06
Iter: 606 loss: 2.00670593e-06
Iter: 607 loss: 2.00282648e-06
Iter: 608 loss: 2.00250952e-06
Iter: 609 loss: 2.0020284e-06
Iter: 610 loss: 2.0019761e-06
Iter: 611 loss: 2.0013722e-06
Iter: 612 loss: 2.00590534e-06
Iter: 613 loss: 2.00131626e-06
Iter: 614 loss: 2.00092154e-06
Iter: 615 loss: 2.00132354e-06
Iter: 616 loss: 2.00053773e-06
Iter: 617 loss: 2.00011755e-06
Iter: 618 loss: 2.00011254e-06
Iter: 619 loss: 1.99978808e-06
Iter: 620 loss: 2.00257728e-06
Iter: 621 loss: 1.99977626e-06
Iter: 622 loss: 1.99948727e-06
Iter: 623 loss: 1.9990714e-06
Iter: 624 loss: 2.00738941e-06
Iter: 625 loss: 1.99906071e-06
Iter: 626 loss: 1.9987192e-06
Iter: 627 loss: 2.0018806e-06
Iter: 628 loss: 1.99851866e-06
Iter: 629 loss: 1.99816031e-06
Iter: 630 loss: 1.99815145e-06
Iter: 631 loss: 1.99783176e-06
Iter: 632 loss: 1.99736019e-06
Iter: 633 loss: 2.00098839e-06
Iter: 634 loss: 1.99725241e-06
Iter: 635 loss: 1.99683313e-06
Iter: 636 loss: 1.99750048e-06
Iter: 637 loss: 1.99655392e-06
Iter: 638 loss: 1.9961517e-06
Iter: 639 loss: 1.99719261e-06
Iter: 640 loss: 1.99609781e-06
Iter: 641 loss: 1.99559599e-06
Iter: 642 loss: 1.99541046e-06
Iter: 643 loss: 1.99501324e-06
Iter: 644 loss: 1.99442843e-06
Iter: 645 loss: 1.99529359e-06
Iter: 646 loss: 1.99419e-06
Iter: 647 loss: 1.99332271e-06
Iter: 648 loss: 1.99414535e-06
Iter: 649 loss: 1.99281931e-06
Iter: 650 loss: 1.99225747e-06
Iter: 651 loss: 1.99222586e-06
Iter: 652 loss: 1.99172268e-06
Iter: 653 loss: 1.99421e-06
Iter: 654 loss: 1.99163128e-06
Iter: 655 loss: 1.99125225e-06
Iter: 656 loss: 1.99037731e-06
Iter: 657 loss: 1.99034662e-06
Iter: 658 loss: 1.98970565e-06
Iter: 659 loss: 1.9930626e-06
Iter: 660 loss: 1.9895981e-06
Iter: 661 loss: 1.98888051e-06
Iter: 662 loss: 1.98987209e-06
Iter: 663 loss: 1.98853581e-06
Iter: 664 loss: 1.98775115e-06
Iter: 665 loss: 1.99101146e-06
Iter: 666 loss: 1.98759676e-06
Iter: 667 loss: 1.98689941e-06
Iter: 668 loss: 1.98981252e-06
Iter: 669 loss: 1.98680141e-06
Iter: 670 loss: 1.98638895e-06
Iter: 671 loss: 1.98701287e-06
Iter: 672 loss: 1.98607813e-06
Iter: 673 loss: 1.9854308e-06
Iter: 674 loss: 1.98624366e-06
Iter: 675 loss: 1.98522298e-06
Iter: 676 loss: 1.9844797e-06
Iter: 677 loss: 1.98426915e-06
Iter: 678 loss: 1.98387693e-06
Iter: 679 loss: 1.9829572e-06
Iter: 680 loss: 1.98744237e-06
Iter: 681 loss: 1.98269254e-06
Iter: 682 loss: 1.98212319e-06
Iter: 683 loss: 1.98840144e-06
Iter: 684 loss: 1.98215889e-06
Iter: 685 loss: 1.98168209e-06
Iter: 686 loss: 1.98604494e-06
Iter: 687 loss: 1.98164889e-06
Iter: 688 loss: 1.98107296e-06
Iter: 689 loss: 1.98088946e-06
Iter: 690 loss: 1.98077578e-06
Iter: 691 loss: 1.98022849e-06
Iter: 692 loss: 1.98059774e-06
Iter: 693 loss: 1.98008411e-06
Iter: 694 loss: 1.97929921e-06
Iter: 695 loss: 1.9827346e-06
Iter: 696 loss: 1.97921077e-06
Iter: 697 loss: 1.97867325e-06
Iter: 698 loss: 1.97933241e-06
Iter: 699 loss: 1.97839427e-06
Iter: 700 loss: 1.97775535e-06
Iter: 701 loss: 1.98252496e-06
Iter: 702 loss: 1.97767395e-06
Iter: 703 loss: 1.97726195e-06
Iter: 704 loss: 1.97709664e-06
Iter: 705 loss: 1.97684631e-06
Iter: 706 loss: 1.97620079e-06
Iter: 707 loss: 1.97904774e-06
Iter: 708 loss: 1.97609756e-06
Iter: 709 loss: 1.97543068e-06
Iter: 710 loss: 1.97491158e-06
Iter: 711 loss: 1.97473901e-06
Iter: 712 loss: 1.97369491e-06
Iter: 713 loss: 1.97796589e-06
Iter: 714 loss: 1.97348186e-06
Iter: 715 loss: 1.97257532e-06
Iter: 716 loss: 1.97454733e-06
Iter: 717 loss: 1.97220743e-06
Iter: 718 loss: 1.97186455e-06
Iter: 719 loss: 1.97162899e-06
Iter: 720 loss: 1.9712329e-06
Iter: 721 loss: 1.97091663e-06
Iter: 722 loss: 1.97092277e-06
Iter: 723 loss: 1.97026657e-06
Iter: 724 loss: 1.96943188e-06
Iter: 725 loss: 1.96929864e-06
Iter: 726 loss: 1.96844121e-06
Iter: 727 loss: 1.96843439e-06
Iter: 728 loss: 1.96786209e-06
Iter: 729 loss: 1.96786e-06
Iter: 730 loss: 1.96731571e-06
Iter: 731 loss: 1.96658698e-06
Iter: 732 loss: 1.97524014e-06
Iter: 733 loss: 1.96664541e-06
Iter: 734 loss: 1.96619703e-06
Iter: 735 loss: 1.96564474e-06
Iter: 736 loss: 1.96556721e-06
Iter: 737 loss: 1.96484962e-06
Iter: 738 loss: 1.97129134e-06
Iter: 739 loss: 1.96485325e-06
Iter: 740 loss: 1.96421979e-06
Iter: 741 loss: 1.96394535e-06
Iter: 742 loss: 1.96362089e-06
Iter: 743 loss: 1.96282508e-06
Iter: 744 loss: 1.96553174e-06
Iter: 745 loss: 1.96274596e-06
Iter: 746 loss: 1.96207566e-06
Iter: 747 loss: 1.96329665e-06
Iter: 748 loss: 1.96174506e-06
Iter: 749 loss: 1.9617155e-06
Iter: 750 loss: 1.96144651e-06
Iter: 751 loss: 1.96121664e-06
Iter: 752 loss: 1.96088149e-06
Iter: 753 loss: 1.96079964e-06
Iter: 754 loss: 1.96034489e-06
Iter: 755 loss: 1.95996222e-06
Iter: 756 loss: 1.95989128e-06
Iter: 757 loss: 1.9593715e-06
Iter: 758 loss: 1.96588758e-06
Iter: 759 loss: 1.95934626e-06
Iter: 760 loss: 1.95888856e-06
Iter: 761 loss: 1.95844063e-06
Iter: 762 loss: 1.95842222e-06
Iter: 763 loss: 1.95780444e-06
Iter: 764 loss: 1.95783628e-06
Iter: 765 loss: 1.95738357e-06
Iter: 766 loss: 1.95682856e-06
Iter: 767 loss: 1.95682742e-06
Iter: 768 loss: 1.9561171e-06
Iter: 769 loss: 1.96057158e-06
Iter: 770 loss: 1.95601615e-06
Iter: 771 loss: 1.95532e-06
Iter: 772 loss: 1.95549478e-06
Iter: 773 loss: 1.95497614e-06
Iter: 774 loss: 1.95423854e-06
Iter: 775 loss: 1.95515827e-06
Iter: 776 loss: 1.95396979e-06
Iter: 777 loss: 1.95304483e-06
Iter: 778 loss: 1.95431221e-06
Iter: 779 loss: 1.95279426e-06
Iter: 780 loss: 1.95259759e-06
Iter: 781 loss: 1.95237931e-06
Iter: 782 loss: 1.95198504e-06
Iter: 783 loss: 1.95167559e-06
Iter: 784 loss: 1.95157963e-06
Iter: 785 loss: 1.95092684e-06
Iter: 786 loss: 1.95036273e-06
Iter: 787 loss: 1.95022722e-06
Iter: 788 loss: 1.94945619e-06
Iter: 789 loss: 1.95238795e-06
Iter: 790 loss: 1.94936774e-06
Iter: 791 loss: 1.94841459e-06
Iter: 792 loss: 1.95046255e-06
Iter: 793 loss: 1.94804943e-06
Iter: 794 loss: 1.94755648e-06
Iter: 795 loss: 1.95320854e-06
Iter: 796 loss: 1.94753102e-06
Iter: 797 loss: 1.94693962e-06
Iter: 798 loss: 1.94683616e-06
Iter: 799 loss: 1.94634936e-06
Iter: 800 loss: 1.9458239e-06
Iter: 801 loss: 1.9473473e-06
Iter: 802 loss: 1.94562335e-06
Iter: 803 loss: 1.9447582e-06
Iter: 804 loss: 1.94671156e-06
Iter: 805 loss: 1.94450649e-06
Iter: 806 loss: 1.94382937e-06
Iter: 807 loss: 1.94390282e-06
Iter: 808 loss: 1.94326572e-06
Iter: 809 loss: 1.94227414e-06
Iter: 810 loss: 1.94432414e-06
Iter: 811 loss: 1.94172844e-06
Iter: 812 loss: 1.94068843e-06
Iter: 813 loss: 1.94712129e-06
Iter: 814 loss: 1.94065296e-06
Iter: 815 loss: 1.93984988e-06
Iter: 816 loss: 1.95349662e-06
Iter: 817 loss: 1.93977849e-06
Iter: 818 loss: 1.93947039e-06
Iter: 819 loss: 1.93889855e-06
Iter: 820 loss: 1.9388076e-06
Iter: 821 loss: 1.93814026e-06
Iter: 822 loss: 1.93782739e-06
Iter: 823 loss: 1.93739334e-06
Iter: 824 loss: 1.93681649e-06
Iter: 825 loss: 1.93680489e-06
Iter: 826 loss: 1.93625e-06
Iter: 827 loss: 1.93566507e-06
Iter: 828 loss: 1.93549067e-06
Iter: 829 loss: 1.93487813e-06
Iter: 830 loss: 1.93486608e-06
Iter: 831 loss: 1.93446067e-06
Iter: 832 loss: 1.93367032e-06
Iter: 833 loss: 1.94853806e-06
Iter: 834 loss: 1.93370943e-06
Iter: 835 loss: 1.93295091e-06
Iter: 836 loss: 1.94233508e-06
Iter: 837 loss: 1.93292863e-06
Iter: 838 loss: 1.93247274e-06
Iter: 839 loss: 1.93210758e-06
Iter: 840 loss: 1.93183314e-06
Iter: 841 loss: 1.93090386e-06
Iter: 842 loss: 1.93237111e-06
Iter: 843 loss: 1.93055644e-06
Iter: 844 loss: 1.92967809e-06
Iter: 845 loss: 1.93034612e-06
Iter: 846 loss: 1.92908465e-06
Iter: 847 loss: 1.92867105e-06
Iter: 848 loss: 1.92859852e-06
Iter: 849 loss: 1.9277993e-06
Iter: 850 loss: 1.92881453e-06
Iter: 851 loss: 1.92758762e-06
Iter: 852 loss: 1.92715402e-06
Iter: 853 loss: 1.9267477e-06
Iter: 854 loss: 1.92657239e-06
Iter: 855 loss: 1.92567177e-06
Iter: 856 loss: 1.92700463e-06
Iter: 857 loss: 1.92527887e-06
Iter: 858 loss: 1.9244676e-06
Iter: 859 loss: 1.93215556e-06
Iter: 860 loss: 1.92444145e-06
Iter: 861 loss: 1.9238289e-06
Iter: 862 loss: 1.92360449e-06
Iter: 863 loss: 1.92327775e-06
Iter: 864 loss: 1.92263224e-06
Iter: 865 loss: 1.92257221e-06
Iter: 866 loss: 1.92210223e-06
Iter: 867 loss: 1.92137077e-06
Iter: 868 loss: 1.93257074e-06
Iter: 869 loss: 1.92128891e-06
Iter: 870 loss: 1.92055813e-06
Iter: 871 loss: 1.92053903e-06
Iter: 872 loss: 1.9199756e-06
Iter: 873 loss: 1.91911545e-06
Iter: 874 loss: 1.91912227e-06
Iter: 875 loss: 1.91824302e-06
Iter: 876 loss: 1.92395441e-06
Iter: 877 loss: 1.91817207e-06
Iter: 878 loss: 1.91746585e-06
Iter: 879 loss: 1.91738354e-06
Iter: 880 loss: 1.91682739e-06
Iter: 881 loss: 1.9169695e-06
Iter: 882 loss: 1.91657955e-06
Iter: 883 loss: 1.91618278e-06
Iter: 884 loss: 1.91579238e-06
Iter: 885 loss: 1.91571894e-06
Iter: 886 loss: 1.91532604e-06
Iter: 887 loss: 1.91521485e-06
Iter: 888 loss: 1.91492154e-06
Iter: 889 loss: 1.91444883e-06
Iter: 890 loss: 1.91641584e-06
Iter: 891 loss: 1.91435629e-06
Iter: 892 loss: 1.9138256e-06
Iter: 893 loss: 1.91445088e-06
Iter: 894 loss: 1.91343452e-06
Iter: 895 loss: 1.91285199e-06
Iter: 896 loss: 1.9171539e-06
Iter: 897 loss: 1.91284744e-06
Iter: 898 loss: 1.91245977e-06
Iter: 899 loss: 1.91256436e-06
Iter: 900 loss: 1.91225126e-06
Iter: 901 loss: 1.91175332e-06
Iter: 902 loss: 1.91304139e-06
Iter: 903 loss: 1.91147569e-06
Iter: 904 loss: 1.91101162e-06
Iter: 905 loss: 1.91067079e-06
Iter: 906 loss: 1.91055028e-06
Iter: 907 loss: 1.90995138e-06
Iter: 908 loss: 1.91491381e-06
Iter: 909 loss: 1.90994183e-06
Iter: 910 loss: 1.90936453e-06
Iter: 911 loss: 1.91028789e-06
Iter: 912 loss: 1.90918649e-06
Iter: 913 loss: 1.90842e-06
Iter: 914 loss: 1.91183972e-06
Iter: 915 loss: 1.90829405e-06
Iter: 916 loss: 1.90797664e-06
Iter: 917 loss: 1.91287972e-06
Iter: 918 loss: 1.90793298e-06
Iter: 919 loss: 1.90751166e-06
Iter: 920 loss: 1.90668356e-06
Iter: 921 loss: 1.92181506e-06
Iter: 922 loss: 1.90666333e-06
Iter: 923 loss: 1.906026e-06
Iter: 924 loss: 1.90616038e-06
Iter: 925 loss: 1.90553055e-06
Iter: 926 loss: 1.90496166e-06
Iter: 927 loss: 1.91547269e-06
Iter: 928 loss: 1.90493972e-06
Iter: 929 loss: 1.90435946e-06
Iter: 930 loss: 1.90526725e-06
Iter: 931 loss: 1.90422452e-06
Iter: 932 loss: 1.90369747e-06
Iter: 933 loss: 1.90815695e-06
Iter: 934 loss: 1.90366313e-06
Iter: 935 loss: 1.90326136e-06
Iter: 936 loss: 1.9025606e-06
Iter: 937 loss: 1.90251194e-06
Iter: 938 loss: 1.90174035e-06
Iter: 939 loss: 1.9022691e-06
Iter: 940 loss: 1.90116725e-06
Iter: 941 loss: 1.90081391e-06
Iter: 942 loss: 1.90065725e-06
Iter: 943 loss: 1.90017772e-06
Iter: 944 loss: 1.90062565e-06
Iter: 945 loss: 1.89988725e-06
Iter: 946 loss: 1.89921366e-06
Iter: 947 loss: 1.89990271e-06
Iter: 948 loss: 1.89883531e-06
Iter: 949 loss: 1.89842558e-06
Iter: 950 loss: 1.89846116e-06
Iter: 951 loss: 1.89785646e-06
Iter: 952 loss: 1.89741081e-06
Iter: 953 loss: 1.8971798e-06
Iter: 954 loss: 1.89662865e-06
Iter: 955 loss: 1.9000297e-06
Iter: 956 loss: 1.89665298e-06
Iter: 957 loss: 1.89618095e-06
Iter: 958 loss: 1.89518505e-06
Iter: 959 loss: 1.91256049e-06
Iter: 960 loss: 1.89517959e-06
Iter: 961 loss: 1.89433786e-06
Iter: 962 loss: 1.89692878e-06
Iter: 963 loss: 1.89402363e-06
Iter: 964 loss: 1.89380967e-06
Iter: 965 loss: 1.89351556e-06
Iter: 966 loss: 1.89318678e-06
Iter: 967 loss: 1.8925715e-06
Iter: 968 loss: 1.90638968e-06
Iter: 969 loss: 1.89250682e-06
Iter: 970 loss: 1.89199181e-06
Iter: 971 loss: 1.89489867e-06
Iter: 972 loss: 1.89183811e-06
Iter: 973 loss: 1.89127513e-06
Iter: 974 loss: 1.89280331e-06
Iter: 975 loss: 1.89114076e-06
Iter: 976 loss: 1.89062541e-06
Iter: 977 loss: 1.89130628e-06
Iter: 978 loss: 1.890372e-06
Iter: 979 loss: 1.88984131e-06
Iter: 980 loss: 1.89445609e-06
Iter: 981 loss: 1.88972263e-06
Iter: 982 loss: 1.88916476e-06
Iter: 983 loss: 1.88951196e-06
Iter: 984 loss: 1.88879483e-06
Iter: 985 loss: 1.88846e-06
Iter: 986 loss: 1.88845684e-06
Iter: 987 loss: 1.88810588e-06
Iter: 988 loss: 1.88747276e-06
Iter: 989 loss: 1.88752915e-06
Iter: 990 loss: 1.88710919e-06
Iter: 991 loss: 1.89082834e-06
Iter: 992 loss: 1.88702711e-06
Iter: 993 loss: 1.88664512e-06
Iter: 994 loss: 1.88588422e-06
Iter: 995 loss: 1.89896241e-06
Iter: 996 loss: 1.88582385e-06
Iter: 997 loss: 1.88525769e-06
Iter: 998 loss: 1.89002162e-06
Iter: 999 loss: 1.88515537e-06
Iter: 1000 loss: 1.8847727e-06
Iter: 1001 loss: 1.88469e-06
Iter: 1002 loss: 1.88443437e-06
Iter: 1003 loss: 1.88380159e-06
Iter: 1004 loss: 1.89267371e-06
Iter: 1005 loss: 1.88371462e-06
Iter: 1006 loss: 1.88331819e-06
Iter: 1007 loss: 1.88805677e-06
Iter: 1008 loss: 1.88322269e-06
Iter: 1009 loss: 1.88270155e-06
Iter: 1010 loss: 1.8835234e-06
Iter: 1011 loss: 1.88253318e-06
Iter: 1012 loss: 1.88210959e-06
Iter: 1013 loss: 1.88458432e-06
Iter: 1014 loss: 1.8819261e-06
Iter: 1015 loss: 1.88161914e-06
Iter: 1016 loss: 1.88375486e-06
Iter: 1017 loss: 1.88161903e-06
Iter: 1018 loss: 1.88128774e-06
Iter: 1019 loss: 1.88179592e-06
Iter: 1020 loss: 1.88110016e-06
Iter: 1021 loss: 1.88073398e-06
Iter: 1022 loss: 1.88073841e-06
Iter: 1023 loss: 1.88059732e-06
Iter: 1024 loss: 1.88025956e-06
Iter: 1025 loss: 1.88577133e-06
Iter: 1026 loss: 1.88022273e-06
Iter: 1027 loss: 1.87973455e-06
Iter: 1028 loss: 1.88285048e-06
Iter: 1029 loss: 1.87975752e-06
Iter: 1030 loss: 1.87948933e-06
Iter: 1031 loss: 1.87890032e-06
Iter: 1032 loss: 1.88951492e-06
Iter: 1033 loss: 1.87903311e-06
Iter: 1034 loss: 1.87879778e-06
Iter: 1035 loss: 1.87857052e-06
Iter: 1036 loss: 1.87833393e-06
Iter: 1037 loss: 1.87811042e-06
Iter: 1038 loss: 1.87801083e-06
Iter: 1039 loss: 1.87759758e-06
Iter: 1040 loss: 1.87744672e-06
Iter: 1041 loss: 1.87714795e-06
Iter: 1042 loss: 1.87684054e-06
Iter: 1043 loss: 1.88315937e-06
Iter: 1044 loss: 1.87682792e-06
Iter: 1045 loss: 1.87647925e-06
Iter: 1046 loss: 1.87601336e-06
Iter: 1047 loss: 1.88940635e-06
Iter: 1048 loss: 1.87604712e-06
Iter: 1049 loss: 1.87547653e-06
Iter: 1050 loss: 1.8754389e-06
Iter: 1051 loss: 1.87518629e-06
Iter: 1052 loss: 1.87498654e-06
Iter: 1053 loss: 1.87488126e-06
Iter: 1054 loss: 1.87459159e-06
Iter: 1055 loss: 1.87454259e-06
Iter: 1056 loss: 1.87427952e-06
Iter: 1057 loss: 1.87452179e-06
Iter: 1058 loss: 1.87414889e-06
Iter: 1059 loss: 1.87388741e-06
Iter: 1060 loss: 1.87369051e-06
Iter: 1061 loss: 1.87359933e-06
Iter: 1062 loss: 1.87299281e-06
Iter: 1063 loss: 1.87631895e-06
Iter: 1064 loss: 1.87289265e-06
Iter: 1065 loss: 1.87241653e-06
Iter: 1066 loss: 1.87204728e-06
Iter: 1067 loss: 1.87188982e-06
Iter: 1068 loss: 1.87166893e-06
Iter: 1069 loss: 1.87159458e-06
Iter: 1070 loss: 1.87120668e-06
Iter: 1071 loss: 1.87053479e-06
Iter: 1072 loss: 1.88438821e-06
Iter: 1073 loss: 1.87049227e-06
Iter: 1074 loss: 1.87007686e-06
Iter: 1075 loss: 1.87124988e-06
Iter: 1076 loss: 1.86980049e-06
Iter: 1077 loss: 1.86900115e-06
Iter: 1078 loss: 1.87148555e-06
Iter: 1079 loss: 1.86891691e-06
Iter: 1080 loss: 1.86813361e-06
Iter: 1081 loss: 1.87115552e-06
Iter: 1082 loss: 1.86805573e-06
Iter: 1083 loss: 1.86738976e-06
Iter: 1084 loss: 1.8680729e-06
Iter: 1085 loss: 1.86694979e-06
Iter: 1086 loss: 1.86650891e-06
Iter: 1087 loss: 1.86671321e-06
Iter: 1088 loss: 1.86616353e-06
Iter: 1089 loss: 1.86593365e-06
Iter: 1090 loss: 1.86571151e-06
Iter: 1091 loss: 1.8655245e-06
Iter: 1092 loss: 1.86496061e-06
Iter: 1093 loss: 1.86687657e-06
Iter: 1094 loss: 1.86472562e-06
Iter: 1095 loss: 1.86422835e-06
Iter: 1096 loss: 1.86412296e-06
Iter: 1097 loss: 1.86371642e-06
Iter: 1098 loss: 1.86300326e-06
Iter: 1099 loss: 1.86291982e-06
Iter: 1100 loss: 1.86258796e-06
Iter: 1101 loss: 1.86854925e-06
Iter: 1102 loss: 1.86260172e-06
Iter: 1103 loss: 1.86203943e-06
Iter: 1104 loss: 1.86301963e-06
Iter: 1105 loss: 1.86186458e-06
Iter: 1106 loss: 1.86138686e-06
Iter: 1107 loss: 1.86083957e-06
Iter: 1108 loss: 1.86079956e-06
Iter: 1109 loss: 1.86021066e-06
Iter: 1110 loss: 1.86029e-06
Iter: 1111 loss: 1.85975694e-06
Iter: 1112 loss: 1.85900649e-06
Iter: 1113 loss: 1.86269835e-06
Iter: 1114 loss: 1.85882607e-06
Iter: 1115 loss: 1.85822682e-06
Iter: 1116 loss: 1.86287116e-06
Iter: 1117 loss: 1.8583371e-06
Iter: 1118 loss: 1.85769545e-06
Iter: 1119 loss: 1.85885574e-06
Iter: 1120 loss: 1.85745239e-06
Iter: 1121 loss: 1.85712611e-06
Iter: 1122 loss: 1.86039551e-06
Iter: 1123 loss: 1.85714316e-06
Iter: 1124 loss: 1.85663839e-06
Iter: 1125 loss: 1.85691579e-06
Iter: 1126 loss: 1.85639851e-06
Iter: 1127 loss: 1.85611725e-06
Iter: 1128 loss: 1.85552653e-06
Iter: 1129 loss: 1.85554836e-06
Iter: 1130 loss: 1.85508804e-06
Iter: 1131 loss: 1.8622718e-06
Iter: 1132 loss: 1.85500653e-06
Iter: 1133 loss: 1.85468491e-06
Iter: 1134 loss: 1.85388353e-06
Iter: 1135 loss: 1.87307194e-06
Iter: 1136 loss: 1.85393321e-06
Iter: 1137 loss: 1.85329361e-06
Iter: 1138 loss: 1.85326189e-06
Iter: 1139 loss: 1.8527511e-06
Iter: 1140 loss: 1.85251031e-06
Iter: 1141 loss: 1.85215868e-06
Iter: 1142 loss: 1.85159263e-06
Iter: 1143 loss: 1.8545104e-06
Iter: 1144 loss: 1.85138197e-06
Iter: 1145 loss: 1.85083297e-06
Iter: 1146 loss: 1.850285e-06
Iter: 1147 loss: 1.8502667e-06
Iter: 1148 loss: 1.84960049e-06
Iter: 1149 loss: 1.84959879e-06
Iter: 1150 loss: 1.84920168e-06
Iter: 1151 loss: 1.85053409e-06
Iter: 1152 loss: 1.84913938e-06
Iter: 1153 loss: 1.84877558e-06
Iter: 1154 loss: 1.85014744e-06
Iter: 1155 loss: 1.84868281e-06
Iter: 1156 loss: 1.8484069e-06
Iter: 1157 loss: 1.85115232e-06
Iter: 1158 loss: 1.84836642e-06
Iter: 1159 loss: 1.84814621e-06
Iter: 1160 loss: 1.84772841e-06
Iter: 1161 loss: 1.85204283e-06
Iter: 1162 loss: 1.84769897e-06
Iter: 1163 loss: 1.84729663e-06
Iter: 1164 loss: 1.85253896e-06
Iter: 1165 loss: 1.84727344e-06
Iter: 1166 loss: 1.84696921e-06
Iter: 1167 loss: 1.84663406e-06
Iter: 1168 loss: 1.8464948e-06
Iter: 1169 loss: 1.84596502e-06
Iter: 1170 loss: 1.84603232e-06
Iter: 1171 loss: 1.84558598e-06
Iter: 1172 loss: 1.84464193e-06
Iter: 1173 loss: 1.85410727e-06
Iter: 1174 loss: 1.84469127e-06
Iter: 1175 loss: 1.84418718e-06
Iter: 1176 loss: 1.84427427e-06
Iter: 1177 loss: 1.843723e-06
Iter: 1178 loss: 1.84317378e-06
Iter: 1179 loss: 1.84322141e-06
Iter: 1180 loss: 1.84251667e-06
Iter: 1181 loss: 1.84186399e-06
Iter: 1182 loss: 1.84380644e-06
Iter: 1183 loss: 1.84176065e-06
Iter: 1184 loss: 1.84108728e-06
Iter: 1185 loss: 1.8484659e-06
Iter: 1186 loss: 1.84106318e-06
Iter: 1187 loss: 1.84070245e-06
Iter: 1188 loss: 1.8417727e-06
Iter: 1189 loss: 1.84047428e-06
Iter: 1190 loss: 1.84003397e-06
Iter: 1191 loss: 1.84326109e-06
Iter: 1192 loss: 1.83997042e-06
Iter: 1193 loss: 1.83956695e-06
Iter: 1194 loss: 1.83895577e-06
Iter: 1195 loss: 1.85365855e-06
Iter: 1196 loss: 1.8389527e-06
Iter: 1197 loss: 1.83837517e-06
Iter: 1198 loss: 1.84027465e-06
Iter: 1199 loss: 1.83810312e-06
Iter: 1200 loss: 1.83752513e-06
Iter: 1201 loss: 1.8403e-06
Iter: 1202 loss: 1.83738246e-06
Iter: 1203 loss: 1.83680163e-06
Iter: 1204 loss: 1.836092e-06
Iter: 1205 loss: 1.83607528e-06
Iter: 1206 loss: 1.83564657e-06
Iter: 1207 loss: 1.8356875e-06
Iter: 1208 loss: 1.83514442e-06
Iter: 1209 loss: 1.83474515e-06
Iter: 1210 loss: 1.83457701e-06
Iter: 1211 loss: 1.83395571e-06
Iter: 1212 loss: 1.83664463e-06
Iter: 1213 loss: 1.83378415e-06
Iter: 1214 loss: 1.8333119e-06
Iter: 1215 loss: 1.83295629e-06
Iter: 1216 loss: 1.8328966e-06
Iter: 1217 loss: 1.83224063e-06
Iter: 1218 loss: 1.83884936e-06
Iter: 1219 loss: 1.83224199e-06
Iter: 1220 loss: 1.83181965e-06
Iter: 1221 loss: 1.83462544e-06
Iter: 1222 loss: 1.83181351e-06
Iter: 1223 loss: 1.83144755e-06
Iter: 1224 loss: 1.835425e-06
Iter: 1225 loss: 1.83148927e-06
Iter: 1226 loss: 1.8312262e-06
Iter: 1227 loss: 1.83095358e-06
Iter: 1228 loss: 1.83100678e-06
Iter: 1229 loss: 1.83050406e-06
Iter: 1230 loss: 1.83006387e-06
Iter: 1231 loss: 1.82999588e-06
Iter: 1232 loss: 1.82974418e-06
Iter: 1233 loss: 1.82969302e-06
Iter: 1234 loss: 1.82931194e-06
Iter: 1235 loss: 1.82922133e-06
Iter: 1236 loss: 1.82901317e-06
Iter: 1237 loss: 1.82854046e-06
Iter: 1238 loss: 1.82864471e-06
Iter: 1239 loss: 1.82829729e-06
Iter: 1240 loss: 1.82790518e-06
Iter: 1241 loss: 1.83101281e-06
Iter: 1242 loss: 1.82788824e-06
Iter: 1243 loss: 1.8276362e-06
Iter: 1244 loss: 1.82755878e-06
Iter: 1245 loss: 1.82735766e-06
Iter: 1246 loss: 1.82713484e-06
Iter: 1247 loss: 1.82721385e-06
Iter: 1248 loss: 1.82687802e-06
Iter: 1249 loss: 1.82787858e-06
Iter: 1250 loss: 1.82672954e-06
Iter: 1251 loss: 1.82644271e-06
Iter: 1252 loss: 1.82634972e-06
Iter: 1253 loss: 1.82620329e-06
Iter: 1254 loss: 1.8259251e-06
Iter: 1255 loss: 1.82593431e-06
Iter: 1256 loss: 1.82581778e-06
Iter: 1257 loss: 1.8264775e-06
Iter: 1258 loss: 1.8257806e-06
Iter: 1259 loss: 1.82556369e-06
Iter: 1260 loss: 1.82660892e-06
Iter: 1261 loss: 1.8255605e-06
Iter: 1262 loss: 1.82546569e-06
Iter: 1263 loss: 1.82520193e-06
Iter: 1264 loss: 1.82992392e-06
Iter: 1265 loss: 1.82518579e-06
Iter: 1266 loss: 1.82498388e-06
Iter: 1267 loss: 1.82572501e-06
Iter: 1268 loss: 1.82499275e-06
Iter: 1269 loss: 1.8247249e-06
Iter: 1270 loss: 1.82574513e-06
Iter: 1271 loss: 1.82468114e-06
Iter: 1272 loss: 1.82454505e-06
Iter: 1273 loss: 1.82424583e-06
Iter: 1274 loss: 1.82424901e-06
Iter: 1275 loss: 1.82413851e-06
Iter: 1276 loss: 1.82407416e-06
Iter: 1277 loss: 1.82387043e-06
Iter: 1278 loss: 1.82366648e-06
Iter: 1279 loss: 1.82360782e-06
Iter: 1280 loss: 1.82346866e-06
Iter: 1281 loss: 1.82347299e-06
Iter: 1282 loss: 1.82329632e-06
Iter: 1283 loss: 1.82304575e-06
Iter: 1284 loss: 1.82635438e-06
Iter: 1285 loss: 1.82297072e-06
Iter: 1286 loss: 1.82282542e-06
Iter: 1287 loss: 1.82274061e-06
Iter: 1288 loss: 1.82264625e-06
Iter: 1289 loss: 1.82255098e-06
Iter: 1290 loss: 1.82253484e-06
Iter: 1291 loss: 1.82239364e-06
Iter: 1292 loss: 1.82271094e-06
Iter: 1293 loss: 1.82238477e-06
Iter: 1294 loss: 1.82225278e-06
Iter: 1295 loss: 1.82212352e-06
Iter: 1296 loss: 1.82211966e-06
Iter: 1297 loss: 1.82187455e-06
Iter: 1298 loss: 1.82244935e-06
Iter: 1299 loss: 1.82192e-06
Iter: 1300 loss: 1.82171027e-06
Iter: 1301 loss: 1.82200233e-06
Iter: 1302 loss: 1.82165434e-06
Iter: 1303 loss: 1.82155895e-06
Iter: 1304 loss: 1.82142674e-06
Iter: 1305 loss: 1.82132953e-06
Iter: 1306 loss: 1.82115173e-06
Iter: 1307 loss: 1.82234976e-06
Iter: 1308 loss: 1.82116821e-06
Iter: 1309 loss: 1.82102872e-06
Iter: 1310 loss: 1.82100757e-06
Iter: 1311 loss: 1.82093299e-06
Iter: 1312 loss: 1.82063252e-06
Iter: 1313 loss: 1.82275699e-06
Iter: 1314 loss: 1.82069437e-06
Iter: 1315 loss: 1.8205086e-06
Iter: 1316 loss: 1.82097187e-06
Iter: 1317 loss: 1.82046961e-06
Iter: 1318 loss: 1.82018084e-06
Iter: 1319 loss: 1.82202723e-06
Iter: 1320 loss: 1.8201597e-06
Iter: 1321 loss: 1.82000531e-06
Iter: 1322 loss: 1.81965549e-06
Iter: 1323 loss: 1.82426561e-06
Iter: 1324 loss: 1.81964833e-06
Iter: 1325 loss: 1.81997655e-06
Iter: 1326 loss: 1.81949486e-06
Iter: 1327 loss: 1.81939959e-06
Iter: 1328 loss: 1.8191688e-06
Iter: 1329 loss: 1.82423469e-06
Iter: 1330 loss: 1.81916437e-06
Iter: 1331 loss: 1.81897576e-06
Iter: 1332 loss: 1.81946154e-06
Iter: 1333 loss: 1.8187734e-06
Iter: 1334 loss: 1.8185051e-06
Iter: 1335 loss: 1.82021552e-06
Iter: 1336 loss: 1.81840301e-06
Iter: 1337 loss: 1.81824851e-06
Iter: 1338 loss: 1.81845678e-06
Iter: 1339 loss: 1.81812e-06
Iter: 1340 loss: 1.81794894e-06
Iter: 1341 loss: 1.81793257e-06
Iter: 1342 loss: 1.81780126e-06
Iter: 1343 loss: 1.81774533e-06
Iter: 1344 loss: 1.81763812e-06
Iter: 1345 loss: 1.81739574e-06
Iter: 1346 loss: 1.81831285e-06
Iter: 1347 loss: 1.81740222e-06
Iter: 1348 loss: 1.81719588e-06
Iter: 1349 loss: 1.81910514e-06
Iter: 1350 loss: 1.81716302e-06
Iter: 1351 loss: 1.81710789e-06
Iter: 1352 loss: 1.81697658e-06
Iter: 1353 loss: 1.81771293e-06
Iter: 1354 loss: 1.81681844e-06
Iter: 1355 loss: 1.81665291e-06
Iter: 1356 loss: 1.81654809e-06
Iter: 1357 loss: 1.81648591e-06
Iter: 1358 loss: 1.81696043e-06
Iter: 1359 loss: 1.81650034e-06
Iter: 1360 loss: 1.81635085e-06
Iter: 1361 loss: 1.81674739e-06
Iter: 1362 loss: 1.81629116e-06
Iter: 1363 loss: 1.81624875e-06
Iter: 1364 loss: 1.81613768e-06
Iter: 1365 loss: 1.8161428e-06
Iter: 1366 loss: 1.8158396e-06
Iter: 1367 loss: 1.81588962e-06
Iter: 1368 loss: 1.815676e-06
Iter: 1369 loss: 1.81551604e-06
Iter: 1370 loss: 1.81610801e-06
Iter: 1371 loss: 1.81533585e-06
Iter: 1372 loss: 1.81523228e-06
Iter: 1373 loss: 1.81523433e-06
Iter: 1374 loss: 1.81508835e-06
Iter: 1375 loss: 1.8149517e-06
Iter: 1376 loss: 1.81497285e-06
Iter: 1377 loss: 1.81476162e-06
Iter: 1378 loss: 1.81598944e-06
Iter: 1379 loss: 1.81466453e-06
Iter: 1380 loss: 1.81446114e-06
Iter: 1381 loss: 1.81628923e-06
Iter: 1382 loss: 1.81442147e-06
Iter: 1383 loss: 1.8143719e-06
Iter: 1384 loss: 1.81463304e-06
Iter: 1385 loss: 1.81428072e-06
Iter: 1386 loss: 1.8141601e-06
Iter: 1387 loss: 1.8142207e-06
Iter: 1388 loss: 1.81402584e-06
Iter: 1389 loss: 1.81385224e-06
Iter: 1390 loss: 1.81365294e-06
Iter: 1391 loss: 1.81360144e-06
Iter: 1392 loss: 1.81368557e-06
Iter: 1393 loss: 1.81349799e-06
Iter: 1394 loss: 1.81347275e-06
Iter: 1395 loss: 1.81335304e-06
Iter: 1396 loss: 1.81418989e-06
Iter: 1397 loss: 1.81320127e-06
Iter: 1398 loss: 1.81297742e-06
Iter: 1399 loss: 1.81390033e-06
Iter: 1400 loss: 1.8129299e-06
Iter: 1401 loss: 1.81271389e-06
Iter: 1402 loss: 1.81250061e-06
Iter: 1403 loss: 1.81244025e-06
Iter: 1404 loss: 1.81227574e-06
Iter: 1405 loss: 1.81332518e-06
Iter: 1406 loss: 1.81223913e-06
Iter: 1407 loss: 1.81208918e-06
Iter: 1408 loss: 1.81296173e-06
Iter: 1409 loss: 1.81209464e-06
Iter: 1410 loss: 1.81197083e-06
Iter: 1411 loss: 1.81183054e-06
Iter: 1412 loss: 1.81180815e-06
Iter: 1413 loss: 1.81163e-06
Iter: 1414 loss: 1.81210567e-06
Iter: 1415 loss: 1.81153951e-06
Iter: 1416 loss: 1.81143798e-06
Iter: 1417 loss: 1.81130576e-06
Iter: 1418 loss: 1.81125415e-06
Iter: 1419 loss: 1.81112728e-06
Iter: 1420 loss: 1.81288328e-06
Iter: 1421 loss: 1.8110884e-06
Iter: 1422 loss: 1.81103496e-06
Iter: 1423 loss: 1.81090093e-06
Iter: 1424 loss: 1.81085966e-06
Iter: 1425 loss: 1.8107487e-06
Iter: 1426 loss: 1.81068526e-06
Iter: 1427 loss: 1.81054429e-06
Iter: 1428 loss: 1.81070754e-06
Iter: 1429 loss: 1.81045812e-06
Iter: 1430 loss: 1.81036148e-06
Iter: 1431 loss: 1.81023404e-06
Iter: 1432 loss: 1.81023006e-06
Iter: 1433 loss: 1.81013365e-06
Iter: 1434 loss: 1.81211612e-06
Iter: 1435 loss: 1.8101573e-06
Iter: 1436 loss: 1.81004248e-06
Iter: 1437 loss: 1.80977872e-06
Iter: 1438 loss: 1.81262692e-06
Iter: 1439 loss: 1.80967379e-06
Iter: 1440 loss: 1.80950144e-06
Iter: 1441 loss: 1.80957659e-06
Iter: 1442 loss: 1.80938514e-06
Iter: 1443 loss: 1.80946608e-06
Iter: 1444 loss: 1.80923053e-06
Iter: 1445 loss: 1.80901372e-06
Iter: 1446 loss: 1.8088374e-06
Iter: 1447 loss: 1.80877521e-06
Iter: 1448 loss: 1.80863094e-06
Iter: 1449 loss: 1.80865732e-06
Iter: 1450 loss: 1.80843381e-06
Iter: 1451 loss: 1.80821348e-06
Iter: 1452 loss: 1.80825884e-06
Iter: 1453 loss: 1.8079777e-06
Iter: 1454 loss: 1.81094049e-06
Iter: 1455 loss: 1.80787538e-06
Iter: 1456 loss: 1.80770064e-06
Iter: 1457 loss: 1.8096805e-06
Iter: 1458 loss: 1.80766e-06
Iter: 1459 loss: 1.80748179e-06
Iter: 1460 loss: 1.80801533e-06
Iter: 1461 loss: 1.80729251e-06
Iter: 1462 loss: 1.80707423e-06
Iter: 1463 loss: 1.80681036e-06
Iter: 1464 loss: 1.81443863e-06
Iter: 1465 loss: 1.80681729e-06
Iter: 1466 loss: 1.80640518e-06
Iter: 1467 loss: 1.80722736e-06
Iter: 1468 loss: 1.80621873e-06
Iter: 1469 loss: 1.80567747e-06
Iter: 1470 loss: 1.80704194e-06
Iter: 1471 loss: 1.80555753e-06
Iter: 1472 loss: 1.80506754e-06
Iter: 1473 loss: 1.80757957e-06
Iter: 1474 loss: 1.80497636e-06
Iter: 1475 loss: 1.80460847e-06
Iter: 1476 loss: 1.80491611e-06
Iter: 1477 loss: 1.80440247e-06
Iter: 1478 loss: 1.80386883e-06
Iter: 1479 loss: 1.80604252e-06
Iter: 1480 loss: 1.80377958e-06
Iter: 1481 loss: 1.80339111e-06
Iter: 1482 loss: 1.8040389e-06
Iter: 1483 loss: 1.80333393e-06
Iter: 1484 loss: 1.80299855e-06
Iter: 1485 loss: 1.80383722e-06
Iter: 1486 loss: 1.80294376e-06
Iter: 1487 loss: 1.80278744e-06
Iter: 1488 loss: 1.80279596e-06
Iter: 1489 loss: 1.80246502e-06
Iter: 1490 loss: 1.80241125e-06
Iter: 1491 loss: 1.80226687e-06
Iter: 1492 loss: 1.80211646e-06
Iter: 1493 loss: 1.80210304e-06
Iter: 1494 loss: 1.80202835e-06
Iter: 1495 loss: 1.80178199e-06
Iter: 1496 loss: 1.80311611e-06
Iter: 1497 loss: 1.80161703e-06
Iter: 1498 loss: 1.80130144e-06
Iter: 1499 loss: 1.80245149e-06
Iter: 1500 loss: 1.80126244e-06
Iter: 1501 loss: 1.80088432e-06
Iter: 1502 loss: 1.80143752e-06
Iter: 1503 loss: 1.80070174e-06
Iter: 1504 loss: 1.80022278e-06
Iter: 1505 loss: 1.80488883e-06
Iter: 1506 loss: 1.8001524e-06
Iter: 1507 loss: 1.80001234e-06
Iter: 1508 loss: 1.79991309e-06
Iter: 1509 loss: 1.79985147e-06
Iter: 1510 loss: 1.7994472e-06
Iter: 1511 loss: 1.79986478e-06
Iter: 1512 loss: 1.7991415e-06
Iter: 1513 loss: 1.79879476e-06
Iter: 1514 loss: 1.79960512e-06
Iter: 1515 loss: 1.79851236e-06
Iter: 1516 loss: 1.79804522e-06
Iter: 1517 loss: 1.79916901e-06
Iter: 1518 loss: 1.79786639e-06
Iter: 1519 loss: 1.7975766e-06
Iter: 1520 loss: 1.79757455e-06
Iter: 1521 loss: 1.79727897e-06
Iter: 1522 loss: 1.79717949e-06
Iter: 1523 loss: 1.7970018e-06
Iter: 1524 loss: 1.79678159e-06
Iter: 1525 loss: 1.79676022e-06
Iter: 1526 loss: 1.79659241e-06
Iter: 1527 loss: 1.79626477e-06
Iter: 1528 loss: 1.79799144e-06
Iter: 1529 loss: 1.79598362e-06
Iter: 1530 loss: 1.79560823e-06
Iter: 1531 loss: 1.79801054e-06
Iter: 1532 loss: 1.79559083e-06
Iter: 1533 loss: 1.79503991e-06
Iter: 1534 loss: 1.79689789e-06
Iter: 1535 loss: 1.7949103e-06
Iter: 1536 loss: 1.79451729e-06
Iter: 1537 loss: 1.79480639e-06
Iter: 1538 loss: 1.79419249e-06
Iter: 1539 loss: 1.79358244e-06
Iter: 1540 loss: 1.79476058e-06
Iter: 1541 loss: 1.79343238e-06
Iter: 1542 loss: 1.79287031e-06
Iter: 1543 loss: 1.79515439e-06
Iter: 1544 loss: 1.79273866e-06
Iter: 1545 loss: 1.79214078e-06
Iter: 1546 loss: 1.79476172e-06
Iter: 1547 loss: 1.79207723e-06
Iter: 1548 loss: 1.79157814e-06
Iter: 1549 loss: 1.79146923e-06
Iter: 1550 loss: 1.79115409e-06
Iter: 1551 loss: 1.79049925e-06
Iter: 1552 loss: 1.79332255e-06
Iter: 1553 loss: 1.79037761e-06
Iter: 1554 loss: 1.79003337e-06
Iter: 1555 loss: 1.78996856e-06
Iter: 1556 loss: 1.78963592e-06
Iter: 1557 loss: 1.79030587e-06
Iter: 1558 loss: 1.78956361e-06
Iter: 1559 loss: 1.78919367e-06
Iter: 1560 loss: 1.79190545e-06
Iter: 1561 loss: 1.78921232e-06
Iter: 1562 loss: 1.7890336e-06
Iter: 1563 loss: 1.78859773e-06
Iter: 1564 loss: 1.79040978e-06
Iter: 1565 loss: 1.78826372e-06
Iter: 1566 loss: 1.78786979e-06
Iter: 1567 loss: 1.7879463e-06
Iter: 1568 loss: 1.78750929e-06
Iter: 1569 loss: 1.78744631e-06
Iter: 1570 loss: 1.78719029e-06
Iter: 1571 loss: 1.78677078e-06
Iter: 1572 loss: 1.78772871e-06
Iter: 1573 loss: 1.78658047e-06
Iter: 1574 loss: 1.78608991e-06
Iter: 1575 loss: 1.78863388e-06
Iter: 1576 loss: 1.785981e-06
Iter: 1577 loss: 1.78569667e-06
Iter: 1578 loss: 1.7876265e-06
Iter: 1579 loss: 1.78555194e-06
Iter: 1580 loss: 1.78533162e-06
Iter: 1581 loss: 1.78504263e-06
Iter: 1582 loss: 1.78484504e-06
Iter: 1583 loss: 1.78449864e-06
Iter: 1584 loss: 1.78620485e-06
Iter: 1585 loss: 1.78442588e-06
Iter: 1586 loss: 1.78395828e-06
Iter: 1587 loss: 1.78602204e-06
Iter: 1588 loss: 1.78392759e-06
Iter: 1589 loss: 1.78343862e-06
Iter: 1590 loss: 1.7853572e-06
Iter: 1591 loss: 1.78327764e-06
Iter: 1592 loss: 1.78295102e-06
Iter: 1593 loss: 1.78505104e-06
Iter: 1594 loss: 1.78291532e-06
Iter: 1595 loss: 1.78256596e-06
Iter: 1596 loss: 1.78182768e-06
Iter: 1597 loss: 1.79180824e-06
Iter: 1598 loss: 1.78173923e-06
Iter: 1599 loss: 1.78116386e-06
Iter: 1600 loss: 1.78307744e-06
Iter: 1601 loss: 1.78095456e-06
Iter: 1602 loss: 1.78031576e-06
Iter: 1603 loss: 1.78667244e-06
Iter: 1604 loss: 1.78037703e-06
Iter: 1605 loss: 1.78007781e-06
Iter: 1606 loss: 1.77930315e-06
Iter: 1607 loss: 1.79271e-06
Iter: 1608 loss: 1.77937477e-06
Iter: 1609 loss: 1.7787155e-06
Iter: 1610 loss: 1.77871527e-06
Iter: 1611 loss: 1.77829884e-06
Iter: 1612 loss: 1.77893162e-06
Iter: 1613 loss: 1.7780211e-06
Iter: 1614 loss: 1.77742049e-06
Iter: 1615 loss: 1.77830157e-06
Iter: 1616 loss: 1.77713036e-06
Iter: 1617 loss: 1.77643324e-06
Iter: 1618 loss: 1.77815446e-06
Iter: 1619 loss: 1.77621587e-06
Iter: 1620 loss: 1.77549828e-06
Iter: 1621 loss: 1.77658239e-06
Iter: 1622 loss: 1.77523975e-06
Iter: 1623 loss: 1.77462618e-06
Iter: 1624 loss: 1.77460515e-06
Iter: 1625 loss: 1.77413892e-06
Iter: 1626 loss: 1.77535685e-06
Iter: 1627 loss: 1.7741861e-06
Iter: 1628 loss: 1.77376342e-06
Iter: 1629 loss: 1.77308505e-06
Iter: 1630 loss: 1.77312222e-06
Iter: 1631 loss: 1.77251309e-06
Iter: 1632 loss: 1.77203651e-06
Iter: 1633 loss: 1.77184836e-06
Iter: 1634 loss: 1.77127185e-06
Iter: 1635 loss: 1.77122138e-06
Iter: 1636 loss: 1.77066659e-06
Iter: 1637 loss: 1.76972299e-06
Iter: 1638 loss: 1.78742698e-06
Iter: 1639 loss: 1.76974163e-06
Iter: 1640 loss: 1.76915114e-06
Iter: 1641 loss: 1.76916728e-06
Iter: 1642 loss: 1.76860544e-06
Iter: 1643 loss: 1.76959929e-06
Iter: 1644 loss: 1.76834544e-06
Iter: 1645 loss: 1.76770095e-06
Iter: 1646 loss: 1.76984713e-06
Iter: 1647 loss: 1.76754827e-06
Iter: 1648 loss: 1.76708033e-06
Iter: 1649 loss: 1.76722529e-06
Iter: 1650 loss: 1.76677065e-06
Iter: 1651 loss: 1.7659421e-06
Iter: 1652 loss: 1.76871754e-06
Iter: 1653 loss: 1.76566164e-06
Iter: 1654 loss: 1.76522906e-06
Iter: 1655 loss: 1.76525941e-06
Iter: 1656 loss: 1.76486321e-06
Iter: 1657 loss: 1.76595631e-06
Iter: 1658 loss: 1.76473043e-06
Iter: 1659 loss: 1.76437175e-06
Iter: 1660 loss: 1.76406297e-06
Iter: 1661 loss: 1.76389972e-06
Iter: 1662 loss: 1.76349045e-06
Iter: 1663 loss: 1.76297749e-06
Iter: 1664 loss: 1.76283663e-06
Iter: 1665 loss: 1.76252456e-06
Iter: 1666 loss: 1.76251e-06
Iter: 1667 loss: 1.76212689e-06
Iter: 1668 loss: 1.76152116e-06
Iter: 1669 loss: 1.7615647e-06
Iter: 1670 loss: 1.76096535e-06
Iter: 1671 loss: 1.76166884e-06
Iter: 1672 loss: 1.76069659e-06
Iter: 1673 loss: 1.75990704e-06
Iter: 1674 loss: 1.76547042e-06
Iter: 1675 loss: 1.75977038e-06
Iter: 1676 loss: 1.75934838e-06
Iter: 1677 loss: 1.76106289e-06
Iter: 1678 loss: 1.75918342e-06
Iter: 1679 loss: 1.7586367e-06
Iter: 1680 loss: 1.75894093e-06
Iter: 1681 loss: 1.75824414e-06
Iter: 1682 loss: 1.75763296e-06
Iter: 1683 loss: 1.76125263e-06
Iter: 1684 loss: 1.75751302e-06
Iter: 1685 loss: 1.75716593e-06
Iter: 1686 loss: 1.7610156e-06
Iter: 1687 loss: 1.75704531e-06
Iter: 1688 loss: 1.75678588e-06
Iter: 1689 loss: 1.75823016e-06
Iter: 1690 loss: 1.75670095e-06
Iter: 1691 loss: 1.75632567e-06
Iter: 1692 loss: 1.75672562e-06
Iter: 1693 loss: 1.75604032e-06
Iter: 1694 loss: 1.75582647e-06
Iter: 1695 loss: 1.75510309e-06
Iter: 1696 loss: 1.76598633e-06
Iter: 1697 loss: 1.7551115e-06
Iter: 1698 loss: 1.75451294e-06
Iter: 1699 loss: 1.75969603e-06
Iter: 1700 loss: 1.7544711e-06
Iter: 1701 loss: 1.75383e-06
Iter: 1702 loss: 1.75482069e-06
Iter: 1703 loss: 1.7534519e-06
Iter: 1704 loss: 1.75290643e-06
Iter: 1705 loss: 1.75197795e-06
Iter: 1706 loss: 1.75195737e-06
Iter: 1707 loss: 1.75118885e-06
Iter: 1708 loss: 1.75113541e-06
Iter: 1709 loss: 1.75048183e-06
Iter: 1710 loss: 1.75043715e-06
Iter: 1711 loss: 1.74993716e-06
Iter: 1712 loss: 1.74902709e-06
Iter: 1713 loss: 1.75186972e-06
Iter: 1714 loss: 1.7487572e-06
Iter: 1715 loss: 1.74805359e-06
Iter: 1716 loss: 1.74978481e-06
Iter: 1717 loss: 1.74772708e-06
Iter: 1718 loss: 1.74700187e-06
Iter: 1719 loss: 1.75063428e-06
Iter: 1720 loss: 1.74685556e-06
Iter: 1721 loss: 1.74608522e-06
Iter: 1722 loss: 1.75263608e-06
Iter: 1723 loss: 1.74612842e-06
Iter: 1724 loss: 1.74553509e-06
Iter: 1725 loss: 1.74627746e-06
Iter: 1726 loss: 1.74523723e-06
Iter: 1727 loss: 1.74481829e-06
Iter: 1728 loss: 1.74382603e-06
Iter: 1729 loss: 1.75828518e-06
Iter: 1730 loss: 1.74387401e-06
Iter: 1731 loss: 1.7427725e-06
Iter: 1732 loss: 1.7499035e-06
Iter: 1733 loss: 1.74266791e-06
Iter: 1734 loss: 1.74171544e-06
Iter: 1735 loss: 1.74899708e-06
Iter: 1736 loss: 1.7417193e-06
Iter: 1737 loss: 1.74115405e-06
Iter: 1738 loss: 1.74004856e-06
Iter: 1739 loss: 1.76198591e-06
Iter: 1740 loss: 1.74002525e-06
Iter: 1741 loss: 1.73936542e-06
Iter: 1742 loss: 1.73929652e-06
Iter: 1743 loss: 1.73849241e-06
Iter: 1744 loss: 1.73949059e-06
Iter: 1745 loss: 1.73815397e-06
Iter: 1746 loss: 1.73736146e-06
Iter: 1747 loss: 1.7405481e-06
Iter: 1748 loss: 1.73728893e-06
Iter: 1749 loss: 1.73664762e-06
Iter: 1750 loss: 1.73639262e-06
Iter: 1751 loss: 1.7360037e-06
Iter: 1752 loss: 1.7350842e-06
Iter: 1753 loss: 1.74251409e-06
Iter: 1754 loss: 1.73506817e-06
Iter: 1755 loss: 1.73433261e-06
Iter: 1756 loss: 1.74379034e-06
Iter: 1757 loss: 1.73434682e-06
Iter: 1758 loss: 1.73393641e-06
Iter: 1759 loss: 1.73443198e-06
Iter: 1760 loss: 1.73366436e-06
Iter: 1761 loss: 1.73314675e-06
Iter: 1762 loss: 1.73229273e-06
Iter: 1763 loss: 1.73233025e-06
Iter: 1764 loss: 1.73150306e-06
Iter: 1765 loss: 1.73333865e-06
Iter: 1766 loss: 1.73117701e-06
Iter: 1767 loss: 1.73041883e-06
Iter: 1768 loss: 1.73959938e-06
Iter: 1769 loss: 1.7304028e-06
Iter: 1770 loss: 1.72993043e-06
Iter: 1771 loss: 1.72914133e-06
Iter: 1772 loss: 1.72903037e-06
Iter: 1773 loss: 1.72853652e-06
Iter: 1774 loss: 1.73344108e-06
Iter: 1775 loss: 1.72841692e-06
Iter: 1776 loss: 1.72786235e-06
Iter: 1777 loss: 1.7309485e-06
Iter: 1778 loss: 1.72770706e-06
Iter: 1779 loss: 1.72732018e-06
Iter: 1780 loss: 1.72847967e-06
Iter: 1781 loss: 1.72723639e-06
Iter: 1782 loss: 1.72675902e-06
Iter: 1783 loss: 1.72653563e-06
Iter: 1784 loss: 1.72640182e-06
Iter: 1785 loss: 1.72575278e-06
Iter: 1786 loss: 1.73201215e-06
Iter: 1787 loss: 1.72578495e-06
Iter: 1788 loss: 1.72536522e-06
Iter: 1789 loss: 1.72780324e-06
Iter: 1790 loss: 1.72538216e-06
Iter: 1791 loss: 1.72494754e-06
Iter: 1792 loss: 1.72534942e-06
Iter: 1793 loss: 1.7246349e-06
Iter: 1794 loss: 1.72420334e-06
Iter: 1795 loss: 1.72393936e-06
Iter: 1796 loss: 1.72365685e-06
Iter: 1797 loss: 1.72315174e-06
Iter: 1798 loss: 1.72313253e-06
Iter: 1799 loss: 1.722668e-06
Iter: 1800 loss: 1.72231535e-06
Iter: 1801 loss: 1.72219234e-06
Iter: 1802 loss: 1.72182718e-06
Iter: 1803 loss: 1.7215383e-06
Iter: 1804 loss: 1.72138198e-06
Iter: 1805 loss: 1.72082741e-06
Iter: 1806 loss: 1.72119587e-06
Iter: 1807 loss: 1.72049863e-06
Iter: 1808 loss: 1.72005389e-06
Iter: 1809 loss: 1.72000068e-06
Iter: 1810 loss: 1.71968645e-06
Iter: 1811 loss: 1.71970237e-06
Iter: 1812 loss: 1.71939064e-06
Iter: 1813 loss: 1.71887564e-06
Iter: 1814 loss: 1.71908141e-06
Iter: 1815 loss: 1.71856232e-06
Iter: 1816 loss: 1.71790327e-06
Iter: 1817 loss: 1.72120451e-06
Iter: 1818 loss: 1.71789247e-06
Iter: 1819 loss: 1.717342e-06
Iter: 1820 loss: 1.721025e-06
Iter: 1821 loss: 1.7172564e-06
Iter: 1822 loss: 1.71668876e-06
Iter: 1823 loss: 1.7193172e-06
Iter: 1824 loss: 1.71666738e-06
Iter: 1825 loss: 1.71629131e-06
Iter: 1826 loss: 1.71597617e-06
Iter: 1827 loss: 1.71593831e-06
Iter: 1828 loss: 1.71535385e-06
Iter: 1829 loss: 1.71496924e-06
Iter: 1830 loss: 1.71481418e-06
Iter: 1831 loss: 1.71434328e-06
Iter: 1832 loss: 1.71433021e-06
Iter: 1833 loss: 1.71388717e-06
Iter: 1834 loss: 1.71377962e-06
Iter: 1835 loss: 1.71344641e-06
Iter: 1836 loss: 1.7128757e-06
Iter: 1837 loss: 1.71296188e-06
Iter: 1838 loss: 1.71243482e-06
Iter: 1839 loss: 1.71208194e-06
Iter: 1840 loss: 1.71200918e-06
Iter: 1841 loss: 1.71165868e-06
Iter: 1842 loss: 1.71137845e-06
Iter: 1843 loss: 1.71120053e-06
Iter: 1844 loss: 1.71067541e-06
Iter: 1845 loss: 1.71369049e-06
Iter: 1846 loss: 1.71048271e-06
Iter: 1847 loss: 1.71013551e-06
Iter: 1848 loss: 1.71019462e-06
Iter: 1849 loss: 1.70991757e-06
Iter: 1850 loss: 1.70935277e-06
Iter: 1851 loss: 1.71400222e-06
Iter: 1852 loss: 1.70926683e-06
Iter: 1853 loss: 1.70893418e-06
Iter: 1854 loss: 1.71394584e-06
Iter: 1855 loss: 1.70884528e-06
Iter: 1856 loss: 1.70868066e-06
Iter: 1857 loss: 1.70822329e-06
Iter: 1858 loss: 1.71789907e-06
Iter: 1859 loss: 1.70820363e-06
Iter: 1860 loss: 1.70761655e-06
Iter: 1861 loss: 1.70755959e-06
Iter: 1862 loss: 1.70711758e-06
Iter: 1863 loss: 1.7065106e-06
Iter: 1864 loss: 1.7084792e-06
Iter: 1865 loss: 1.70637963e-06
Iter: 1866 loss: 1.70554222e-06
Iter: 1867 loss: 1.70878502e-06
Iter: 1868 loss: 1.70525436e-06
Iter: 1869 loss: 1.7047e-06
Iter: 1870 loss: 1.70426563e-06
Iter: 1871 loss: 1.7040843e-06
Iter: 1872 loss: 1.70353451e-06
Iter: 1873 loss: 1.71084889e-06
Iter: 1874 loss: 1.70356191e-06
Iter: 1875 loss: 1.70293822e-06
Iter: 1876 loss: 1.70436476e-06
Iter: 1877 loss: 1.70275734e-06
Iter: 1878 loss: 1.70234739e-06
Iter: 1879 loss: 1.70413614e-06
Iter: 1880 loss: 1.70225871e-06
Iter: 1881 loss: 1.70183466e-06
Iter: 1882 loss: 1.70132e-06
Iter: 1883 loss: 1.70136343e-06
Iter: 1884 loss: 1.70104988e-06
Iter: 1885 loss: 1.70092301e-06
Iter: 1886 loss: 1.70064334e-06
Iter: 1887 loss: 1.70325359e-06
Iter: 1888 loss: 1.70064607e-06
Iter: 1889 loss: 1.70053795e-06
Iter: 1890 loss: 1.6999719e-06
Iter: 1891 loss: 1.70990904e-06
Iter: 1892 loss: 1.70003807e-06
Iter: 1893 loss: 1.69953648e-06
Iter: 1894 loss: 1.70024919e-06
Iter: 1895 loss: 1.69938426e-06
Iter: 1896 loss: 1.6988804e-06
Iter: 1897 loss: 1.69892451e-06
Iter: 1898 loss: 1.69845589e-06
Iter: 1899 loss: 1.69816121e-06
Iter: 1900 loss: 1.69811142e-06
Iter: 1901 loss: 1.69780844e-06
Iter: 1902 loss: 1.69722057e-06
Iter: 1903 loss: 1.71100487e-06
Iter: 1904 loss: 1.69728366e-06
Iter: 1905 loss: 1.69667567e-06
Iter: 1906 loss: 1.69908674e-06
Iter: 1907 loss: 1.69656767e-06
Iter: 1908 loss: 1.69612315e-06
Iter: 1909 loss: 1.70244084e-06
Iter: 1910 loss: 1.69602185e-06
Iter: 1911 loss: 1.69590896e-06
Iter: 1912 loss: 1.69574082e-06
Iter: 1913 loss: 1.69549935e-06
Iter: 1914 loss: 1.69492455e-06
Iter: 1915 loss: 1.69654118e-06
Iter: 1916 loss: 1.69487498e-06
Iter: 1917 loss: 1.69455245e-06
Iter: 1918 loss: 1.69544933e-06
Iter: 1919 loss: 1.69436555e-06
Iter: 1920 loss: 1.69389659e-06
Iter: 1921 loss: 1.69709301e-06
Iter: 1922 loss: 1.69380553e-06
Iter: 1923 loss: 1.69349528e-06
Iter: 1924 loss: 1.69601481e-06
Iter: 1925 loss: 1.69335192e-06
Iter: 1926 loss: 1.69323766e-06
Iter: 1927 loss: 1.69277882e-06
Iter: 1928 loss: 1.69881594e-06
Iter: 1929 loss: 1.69280565e-06
Iter: 1930 loss: 1.69225586e-06
Iter: 1931 loss: 1.69292059e-06
Iter: 1932 loss: 1.69203986e-06
Iter: 1933 loss: 1.69151576e-06
Iter: 1934 loss: 1.69423106e-06
Iter: 1935 loss: 1.69147984e-06
Iter: 1936 loss: 1.69097029e-06
Iter: 1937 loss: 1.69216219e-06
Iter: 1938 loss: 1.69079476e-06
Iter: 1939 loss: 1.69032137e-06
Iter: 1940 loss: 1.68983081e-06
Iter: 1941 loss: 1.68980887e-06
Iter: 1942 loss: 1.68935856e-06
Iter: 1943 loss: 1.68929114e-06
Iter: 1944 loss: 1.6889162e-06
Iter: 1945 loss: 1.68943279e-06
Iter: 1946 loss: 1.68873567e-06
Iter: 1947 loss: 1.68829604e-06
Iter: 1948 loss: 1.68947099e-06
Iter: 1949 loss: 1.68819884e-06
Iter: 1950 loss: 1.68778752e-06
Iter: 1951 loss: 1.68742258e-06
Iter: 1952 loss: 1.68722511e-06
Iter: 1953 loss: 1.68715565e-06
Iter: 1954 loss: 1.68697829e-06
Iter: 1955 loss: 1.6868396e-06
Iter: 1956 loss: 1.68745419e-06
Iter: 1957 loss: 1.68668657e-06
Iter: 1958 loss: 1.68636836e-06
Iter: 1959 loss: 1.68576412e-06
Iter: 1960 loss: 1.69491591e-06
Iter: 1961 loss: 1.68571273e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi0.8/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.2 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi1.2
+ date
Sun Nov  8 16:08:36 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.2/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.2/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca3205510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca31caa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca317d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca30f8950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca30f81e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca317d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca30c30d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca3070840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca3070a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca30708c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca2fb92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca2fb1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80aa28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80b121e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80b2b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80ac79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80af2d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80a3af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1ca2fcc9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80a3a048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80a48510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80a47268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c80a48e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c22ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c22a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c242ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c17dd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c2422f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c291158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c2920d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c27c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c167158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c167620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c146ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c0d9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1c5c0cec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.5695986
test_loss: 0.5710741
train_loss: 0.57224786
test_loss: 0.56098855
train_loss: 0.5456649
test_loss: 0.54509676
train_loss: 0.53210753
test_loss: 0.53135645
train_loss: 0.52548456
test_loss: 0.518053
train_loss: 0.51060617
test_loss: 0.5032406
train_loss: 0.4892332
test_loss: 0.48673877
train_loss: 0.4679435
test_loss: 0.4682132
train_loss: 0.44679934
test_loss: 0.44760665
train_loss: 0.42751688
test_loss: 0.42512062
train_loss: 0.39989322
test_loss: 0.4012321
train_loss: 0.36605656
test_loss: 0.3763993
train_loss: 0.3471506
test_loss: 0.35075182
train_loss: 0.32751507
test_loss: 0.3245464
train_loss: 0.2955656
test_loss: 0.29831547
train_loss: 0.27212614
test_loss: 0.27279094
train_loss: 0.2496007
test_loss: 0.24841914
train_loss: 0.22547728
test_loss: 0.22531644
train_loss: 0.20366155
test_loss: 0.20357409
train_loss: 0.17828578
test_loss: 0.18358687
train_loss: 0.16346666
test_loss: 0.16536866
train_loss: 0.14697894
test_loss: 0.14903224
train_loss: 0.13299134
test_loss: 0.13441189
train_loss: 0.12362486
test_loss: 0.12158845
train_loss: 0.1119269
test_loss: 0.11047218
train_loss: 0.09956954
test_loss: 0.10112107
train_loss: 0.09273802
test_loss: 0.09324651
train_loss: 0.084904045
test_loss: 0.08670721
train_loss: 0.08079732
test_loss: 0.08136625
train_loss: 0.07701712
test_loss: 0.07702452
train_loss: 0.07317398
test_loss: 0.073501244
train_loss: 0.07108335
test_loss: 0.07067264
train_loss: 0.06814204
test_loss: 0.06841536
train_loss: 0.064887166
test_loss: 0.06658039
train_loss: 0.06402535
test_loss: 0.06514167
train_loss: 0.062880255
test_loss: 0.06397947
train_loss: 0.06324915
test_loss: 0.06303394
train_loss: 0.06316714
test_loss: 0.062286846
train_loss: 0.06112205
test_loss: 0.06170393
train_loss: 0.060798846
test_loss: 0.06112357
train_loss: 0.059041474
test_loss: 0.060622204
train_loss: 0.060487542
test_loss: 0.060173027
train_loss: 0.059531838
test_loss: 0.05978976
train_loss: 0.058521934
test_loss: 0.05934441
train_loss: 0.059516184
test_loss: 0.058934156
train_loss: 0.059129804
test_loss: 0.058506288
train_loss: 0.05767198
test_loss: 0.05813046
train_loss: 0.05811645
test_loss: 0.057653356
train_loss: 0.05693554
test_loss: 0.057228357
train_loss: 0.057181895
test_loss: 0.05675406
train_loss: 0.055779777
test_loss: 0.05623215
train_loss: 0.054919336
test_loss: 0.055634186
train_loss: 0.055814877
test_loss: 0.055037897
train_loss: 0.053423457
test_loss: 0.054484446
train_loss: 0.052942604
test_loss: 0.05374127
train_loss: 0.05186294
test_loss: 0.052969094
train_loss: 0.051981278
test_loss: 0.052169792
train_loss: 0.05162906
test_loss: 0.051320348
train_loss: 0.05125639
test_loss: 0.050399408
train_loss: 0.04920467
test_loss: 0.04933722
train_loss: 0.047418267
test_loss: 0.048181836
train_loss: 0.04631288
test_loss: 0.047036882
train_loss: 0.04445795
test_loss: 0.045589693
train_loss: 0.044503987
test_loss: 0.04426423
train_loss: 0.040697195
test_loss: 0.04264487
train_loss: 0.040890038
test_loss: 0.04097256
train_loss: 0.038973678
test_loss: 0.039286487
train_loss: 0.037517283
test_loss: 0.037493628
train_loss: 0.034896865
test_loss: 0.035679493
train_loss: 0.032746438
test_loss: 0.033713996
train_loss: 0.031862065
test_loss: 0.031898636
train_loss: 0.029613413
test_loss: 0.029960021
train_loss: 0.027871352
test_loss: 0.02804627
train_loss: 0.025938245
test_loss: 0.026325108
train_loss: 0.02382031
test_loss: 0.024567446
train_loss: 0.022057274
test_loss: 0.022785245
train_loss: 0.020512862
test_loss: 0.020981925
train_loss: 0.019066606
test_loss: 0.019349346
train_loss: 0.017465467
test_loss: 0.017797058
train_loss: 0.01622062
test_loss: 0.016498936
train_loss: 0.014454227
test_loss: 0.015142073
train_loss: 0.013765982
test_loss: 0.013974327
train_loss: 0.0126751745
test_loss: 0.013265864
train_loss: 0.012234824
test_loss: 0.012165733
train_loss: 0.011135288
test_loss: 0.011335201
train_loss: 0.010403376
test_loss: 0.010638425
train_loss: 0.009866477
test_loss: 0.010052005
train_loss: 0.00921313
test_loss: 0.009776063
train_loss: 0.008706191
test_loss: 0.008999391
train_loss: 0.008297058
test_loss: 0.008537353
train_loss: 0.008098235
test_loss: 0.00834007
train_loss: 0.0076199314
test_loss: 0.007703788
train_loss: 0.007367864
test_loss: 0.007376488
train_loss: 0.0068341373
test_loss: 0.006949796
train_loss: 0.006677628
test_loss: 0.0067910287
train_loss: 0.006405992
test_loss: 0.006457399
train_loss: 0.0062778015
test_loss: 0.006392007
train_loss: 0.0058482527
test_loss: 0.0060234405
train_loss: 0.005756977
test_loss: 0.005985077
train_loss: 0.005491655
test_loss: 0.005526581
train_loss: 0.005590763
test_loss: 0.005794786
train_loss: 0.005169606
test_loss: 0.005594763
train_loss: 0.005074039
test_loss: 0.005173009
train_loss: 0.005067433
test_loss: 0.005167766
train_loss: 0.0049376045
test_loss: 0.005022757
train_loss: 0.004656262
test_loss: 0.00501529
train_loss: 0.0047948877
test_loss: 0.0048630415
train_loss: 0.0045489855
test_loss: 0.0045730663
train_loss: 0.0047616726
test_loss: 0.0047063977
train_loss: 0.004430929
test_loss: 0.004585722
train_loss: 0.0042232396
test_loss: 0.0041426932
train_loss: 0.004460404
test_loss: 0.004970487
train_loss: 0.003978499
test_loss: 0.004129446
train_loss: 0.0040687053
test_loss: 0.003979482
train_loss: 0.00401025
test_loss: 0.003911974
train_loss: 0.0038188905
test_loss: 0.0039098086
train_loss: 0.0037688501
test_loss: 0.0038609614
train_loss: 0.0038207301
test_loss: 0.004136412
train_loss: 0.0040354524
test_loss: 0.003958831
train_loss: 0.0034057088
test_loss: 0.0038567241
train_loss: 0.003645197
test_loss: 0.0036135465
train_loss: 0.003480949
test_loss: 0.003591999
train_loss: 0.003404402
test_loss: 0.0035106956
train_loss: 0.0032562914
test_loss: 0.0032887019
train_loss: 0.0033224546
test_loss: 0.0044641066
train_loss: 0.0030765294
test_loss: 0.0035750347
train_loss: 0.003479437
test_loss: 0.0033002973
train_loss: 0.0041239136
test_loss: 0.0035844608
train_loss: 0.0035391683
test_loss: 0.0032739479
train_loss: 0.0032328288
test_loss: 0.0037606512
train_loss: 0.003264803
test_loss: 0.0030364161
train_loss: 0.003040174
test_loss: 0.0031262627
train_loss: 0.0030974336
test_loss: 0.0035083613
train_loss: 0.0032621673
test_loss: 0.003437607
train_loss: 0.0032600998
test_loss: 0.0035367492
train_loss: 0.0030848605
test_loss: 0.003515752
train_loss: 0.00315239
test_loss: 0.003365854
train_loss: 0.0034986157
test_loss: 0.0030667395
train_loss: 0.0036662333
test_loss: 0.00360342
train_loss: 0.0032494785
test_loss: 0.0029832819
train_loss: 0.0033016605
test_loss: 0.0034195553
train_loss: 0.0031836526
test_loss: 0.003240679
train_loss: 0.002853762
test_loss: 0.0031576364
train_loss: 0.0031145639
test_loss: 0.0031363498
train_loss: 0.003192049
test_loss: 0.003023346
train_loss: 0.0027857185
test_loss: 0.0027876014
train_loss: 0.0029206316
test_loss: 0.0032783006
train_loss: 0.003095498
test_loss: 0.0031408367
train_loss: 0.0028017382
test_loss: 0.0029300335
train_loss: 0.002532518
test_loss: 0.0024712489
train_loss: 0.0030090786
test_loss: 0.0029467125
train_loss: 0.0034315572
test_loss: 0.0031183276
train_loss: 0.0031426367
test_loss: 0.0035552336
train_loss: 0.0035572909
test_loss: 0.0031637778
train_loss: 0.0030408693
test_loss: 0.0025927075
train_loss: 0.0027304422
test_loss: 0.0025988496
train_loss: 0.0027080947
test_loss: 0.00347945
train_loss: 0.0030028792
test_loss: 0.0029836795
train_loss: 0.0030778656
test_loss: 0.0032519225
train_loss: 0.0028049846
test_loss: 0.002603316
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi1.2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.2/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi1.2/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb863598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb7f68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb8e67b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb7bc620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb7bc6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb7bcea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb72f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb709730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb709268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb6c1158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb6c19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb709048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb6a0ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb6521e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb662f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb61b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb61bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb61b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fcb5949d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa5377a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa539d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa533a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa533d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa536cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa530c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa52eff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa52bd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa52ef378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa523a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa525f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa5259510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa52211e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa52127b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa51ba598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa518f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1fa51a9f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.06594016e-05
Iter: 2 loss: 0.000250945566
Iter: 3 loss: 9.06954301e-06
Iter: 4 loss: 8.24353538e-06
Iter: 5 loss: 7.88336729e-06
Iter: 6 loss: 7.46309706e-06
Iter: 7 loss: 6.58571889e-06
Iter: 8 loss: 6.88927776e-06
Iter: 9 loss: 5.96630753e-06
Iter: 10 loss: 5.21558968e-06
Iter: 11 loss: 5.21549146e-06
Iter: 12 loss: 4.91055926e-06
Iter: 13 loss: 6.39161499e-06
Iter: 14 loss: 4.85699456e-06
Iter: 15 loss: 4.71775729e-06
Iter: 16 loss: 4.51620144e-06
Iter: 17 loss: 4.51026426e-06
Iter: 18 loss: 4.34700678e-06
Iter: 19 loss: 6.57213877e-06
Iter: 20 loss: 4.34635967e-06
Iter: 21 loss: 4.27138e-06
Iter: 22 loss: 5.15331794e-06
Iter: 23 loss: 4.27036139e-06
Iter: 24 loss: 4.20873357e-06
Iter: 25 loss: 4.14671831e-06
Iter: 26 loss: 4.13420139e-06
Iter: 27 loss: 4.06867684e-06
Iter: 28 loss: 4.94092274e-06
Iter: 29 loss: 4.0683808e-06
Iter: 30 loss: 4.03854e-06
Iter: 31 loss: 3.98476186e-06
Iter: 32 loss: 5.28701185e-06
Iter: 33 loss: 3.98472821e-06
Iter: 34 loss: 3.94158315e-06
Iter: 35 loss: 4.16416333e-06
Iter: 36 loss: 3.93482605e-06
Iter: 37 loss: 4.01593616e-06
Iter: 38 loss: 3.92488346e-06
Iter: 39 loss: 3.92076436e-06
Iter: 40 loss: 3.90783498e-06
Iter: 41 loss: 3.92582251e-06
Iter: 42 loss: 3.89836441e-06
Iter: 43 loss: 3.876748e-06
Iter: 44 loss: 4.09589529e-06
Iter: 45 loss: 3.87597629e-06
Iter: 46 loss: 3.86641295e-06
Iter: 47 loss: 3.97428812e-06
Iter: 48 loss: 3.86621105e-06
Iter: 49 loss: 3.8603157e-06
Iter: 50 loss: 3.8518665e-06
Iter: 51 loss: 3.85151088e-06
Iter: 52 loss: 3.83872793e-06
Iter: 53 loss: 3.85305975e-06
Iter: 54 loss: 3.83162751e-06
Iter: 55 loss: 3.82054941e-06
Iter: 56 loss: 3.83690121e-06
Iter: 57 loss: 3.81520613e-06
Iter: 58 loss: 3.80066695e-06
Iter: 59 loss: 3.93545588e-06
Iter: 60 loss: 3.80013103e-06
Iter: 61 loss: 3.79170956e-06
Iter: 62 loss: 3.80476877e-06
Iter: 63 loss: 3.78753111e-06
Iter: 64 loss: 3.77847982e-06
Iter: 65 loss: 3.77283504e-06
Iter: 66 loss: 3.76925209e-06
Iter: 67 loss: 3.7534312e-06
Iter: 68 loss: 3.82122e-06
Iter: 69 loss: 3.75024683e-06
Iter: 70 loss: 3.74013666e-06
Iter: 71 loss: 3.73784746e-06
Iter: 72 loss: 3.73152761e-06
Iter: 73 loss: 3.72850445e-06
Iter: 74 loss: 3.72235627e-06
Iter: 75 loss: 3.72075101e-06
Iter: 76 loss: 3.71548e-06
Iter: 77 loss: 3.71418e-06
Iter: 78 loss: 3.70950124e-06
Iter: 79 loss: 3.69706231e-06
Iter: 80 loss: 3.83509087e-06
Iter: 81 loss: 3.69677036e-06
Iter: 82 loss: 3.69193299e-06
Iter: 83 loss: 3.73265061e-06
Iter: 84 loss: 3.69166014e-06
Iter: 85 loss: 3.68861583e-06
Iter: 86 loss: 3.68132328e-06
Iter: 87 loss: 3.76292201e-06
Iter: 88 loss: 3.68065912e-06
Iter: 89 loss: 3.67086705e-06
Iter: 90 loss: 3.73147645e-06
Iter: 91 loss: 3.66982113e-06
Iter: 92 loss: 3.66522431e-06
Iter: 93 loss: 3.68221868e-06
Iter: 94 loss: 3.66415975e-06
Iter: 95 loss: 3.65817186e-06
Iter: 96 loss: 3.66894733e-06
Iter: 97 loss: 3.65552614e-06
Iter: 98 loss: 3.65094138e-06
Iter: 99 loss: 3.6621725e-06
Iter: 100 loss: 3.6492504e-06
Iter: 101 loss: 3.64463176e-06
Iter: 102 loss: 3.64975813e-06
Iter: 103 loss: 3.64200173e-06
Iter: 104 loss: 3.63728736e-06
Iter: 105 loss: 3.6535439e-06
Iter: 106 loss: 3.6360218e-06
Iter: 107 loss: 3.6334568e-06
Iter: 108 loss: 3.6334543e-06
Iter: 109 loss: 3.62962464e-06
Iter: 110 loss: 3.6254371e-06
Iter: 111 loss: 3.62492119e-06
Iter: 112 loss: 3.62284027e-06
Iter: 113 loss: 3.62239302e-06
Iter: 114 loss: 3.62103901e-06
Iter: 115 loss: 3.61653474e-06
Iter: 116 loss: 3.63014465e-06
Iter: 117 loss: 3.61515208e-06
Iter: 118 loss: 3.61255525e-06
Iter: 119 loss: 3.62782157e-06
Iter: 120 loss: 3.6123115e-06
Iter: 121 loss: 3.61005073e-06
Iter: 122 loss: 3.60419813e-06
Iter: 123 loss: 3.64931066e-06
Iter: 124 loss: 3.60299168e-06
Iter: 125 loss: 3.59884348e-06
Iter: 126 loss: 3.59855903e-06
Iter: 127 loss: 3.59597152e-06
Iter: 128 loss: 3.59894511e-06
Iter: 129 loss: 3.59479236e-06
Iter: 130 loss: 3.59008641e-06
Iter: 131 loss: 3.58623902e-06
Iter: 132 loss: 3.58489706e-06
Iter: 133 loss: 3.58139505e-06
Iter: 134 loss: 3.62002879e-06
Iter: 135 loss: 3.5813141e-06
Iter: 136 loss: 3.57828799e-06
Iter: 137 loss: 3.57445288e-06
Iter: 138 loss: 3.57411091e-06
Iter: 139 loss: 3.56958822e-06
Iter: 140 loss: 3.60167974e-06
Iter: 141 loss: 3.56911505e-06
Iter: 142 loss: 3.56619557e-06
Iter: 143 loss: 3.57414456e-06
Iter: 144 loss: 3.56512942e-06
Iter: 145 loss: 3.56286682e-06
Iter: 146 loss: 3.56259216e-06
Iter: 147 loss: 3.56191731e-06
Iter: 148 loss: 3.5596936e-06
Iter: 149 loss: 3.56193459e-06
Iter: 150 loss: 3.55798e-06
Iter: 151 loss: 3.55594943e-06
Iter: 152 loss: 3.55541465e-06
Iter: 153 loss: 3.55448901e-06
Iter: 154 loss: 3.55131215e-06
Iter: 155 loss: 3.56224155e-06
Iter: 156 loss: 3.55004704e-06
Iter: 157 loss: 3.54745794e-06
Iter: 158 loss: 3.54738381e-06
Iter: 159 loss: 3.54520421e-06
Iter: 160 loss: 3.54701115e-06
Iter: 161 loss: 3.54395547e-06
Iter: 162 loss: 3.54129247e-06
Iter: 163 loss: 3.5451194e-06
Iter: 164 loss: 3.53995665e-06
Iter: 165 loss: 3.53795713e-06
Iter: 166 loss: 3.55252178e-06
Iter: 167 loss: 3.53778569e-06
Iter: 168 loss: 3.53644418e-06
Iter: 169 loss: 3.53273776e-06
Iter: 170 loss: 3.56684723e-06
Iter: 171 loss: 3.53222981e-06
Iter: 172 loss: 3.5281314e-06
Iter: 173 loss: 3.53279984e-06
Iter: 174 loss: 3.52585494e-06
Iter: 175 loss: 3.52311554e-06
Iter: 176 loss: 3.52287498e-06
Iter: 177 loss: 3.52125312e-06
Iter: 178 loss: 3.52127381e-06
Iter: 179 loss: 3.51939821e-06
Iter: 180 loss: 3.51607173e-06
Iter: 181 loss: 3.51605922e-06
Iter: 182 loss: 3.51375593e-06
Iter: 183 loss: 3.52029315e-06
Iter: 184 loss: 3.51299695e-06
Iter: 185 loss: 3.51105678e-06
Iter: 186 loss: 3.54060285e-06
Iter: 187 loss: 3.51102767e-06
Iter: 188 loss: 3.51005451e-06
Iter: 189 loss: 3.50714708e-06
Iter: 190 loss: 3.51854214e-06
Iter: 191 loss: 3.50592836e-06
Iter: 192 loss: 3.50264418e-06
Iter: 193 loss: 3.52942811e-06
Iter: 194 loss: 3.50247547e-06
Iter: 195 loss: 3.49898733e-06
Iter: 196 loss: 3.51415815e-06
Iter: 197 loss: 3.49819652e-06
Iter: 198 loss: 3.49637139e-06
Iter: 199 loss: 3.49482707e-06
Iter: 200 loss: 3.49421498e-06
Iter: 201 loss: 3.49090078e-06
Iter: 202 loss: 3.53417772e-06
Iter: 203 loss: 3.49094671e-06
Iter: 204 loss: 3.48959293e-06
Iter: 205 loss: 3.4899158e-06
Iter: 206 loss: 3.48856588e-06
Iter: 207 loss: 3.48613185e-06
Iter: 208 loss: 3.48414096e-06
Iter: 209 loss: 3.48333538e-06
Iter: 210 loss: 3.48258027e-06
Iter: 211 loss: 3.48212211e-06
Iter: 212 loss: 3.48067852e-06
Iter: 213 loss: 3.48295362e-06
Iter: 214 loss: 3.47995183e-06
Iter: 215 loss: 3.4785171e-06
Iter: 216 loss: 3.47674086e-06
Iter: 217 loss: 3.47652781e-06
Iter: 218 loss: 3.47537934e-06
Iter: 219 loss: 3.47530499e-06
Iter: 220 loss: 3.47401578e-06
Iter: 221 loss: 3.4708969e-06
Iter: 222 loss: 3.50782534e-06
Iter: 223 loss: 3.47063497e-06
Iter: 224 loss: 3.46859952e-06
Iter: 225 loss: 3.4676325e-06
Iter: 226 loss: 3.46658385e-06
Iter: 227 loss: 3.4655e-06
Iter: 228 loss: 3.46484239e-06
Iter: 229 loss: 3.46374168e-06
Iter: 230 loss: 3.46228239e-06
Iter: 231 loss: 3.46213938e-06
Iter: 232 loss: 3.45987701e-06
Iter: 233 loss: 3.47037872e-06
Iter: 234 loss: 3.4596e-06
Iter: 235 loss: 3.45741569e-06
Iter: 236 loss: 3.46566412e-06
Iter: 237 loss: 3.45704257e-06
Iter: 238 loss: 3.45558146e-06
Iter: 239 loss: 3.45506578e-06
Iter: 240 loss: 3.45424814e-06
Iter: 241 loss: 3.45219905e-06
Iter: 242 loss: 3.46256866e-06
Iter: 243 loss: 3.4519785e-06
Iter: 244 loss: 3.45180388e-06
Iter: 245 loss: 3.45114e-06
Iter: 246 loss: 3.45066428e-06
Iter: 247 loss: 3.44893078e-06
Iter: 248 loss: 3.45162562e-06
Iter: 249 loss: 3.44767705e-06
Iter: 250 loss: 3.4465661e-06
Iter: 251 loss: 3.44628597e-06
Iter: 252 loss: 3.44539012e-06
Iter: 253 loss: 3.44651585e-06
Iter: 254 loss: 3.44479758e-06
Iter: 255 loss: 3.44365048e-06
Iter: 256 loss: 3.44147202e-06
Iter: 257 loss: 3.48554386e-06
Iter: 258 loss: 3.44133309e-06
Iter: 259 loss: 3.43897386e-06
Iter: 260 loss: 3.44442788e-06
Iter: 261 loss: 3.43804777e-06
Iter: 262 loss: 3.43541797e-06
Iter: 263 loss: 3.47108494e-06
Iter: 264 loss: 3.43540978e-06
Iter: 265 loss: 3.43418219e-06
Iter: 266 loss: 3.43249576e-06
Iter: 267 loss: 3.43232091e-06
Iter: 268 loss: 3.43012721e-06
Iter: 269 loss: 3.43013926e-06
Iter: 270 loss: 3.42892281e-06
Iter: 271 loss: 3.42981366e-06
Iter: 272 loss: 3.42807175e-06
Iter: 273 loss: 3.42653129e-06
Iter: 274 loss: 3.42769385e-06
Iter: 275 loss: 3.42551334e-06
Iter: 276 loss: 3.42384624e-06
Iter: 277 loss: 3.44601267e-06
Iter: 278 loss: 3.42375188e-06
Iter: 279 loss: 3.42225917e-06
Iter: 280 loss: 3.42676208e-06
Iter: 281 loss: 3.42170779e-06
Iter: 282 loss: 3.42109934e-06
Iter: 283 loss: 3.41975419e-06
Iter: 284 loss: 3.43437182e-06
Iter: 285 loss: 3.41952591e-06
Iter: 286 loss: 3.41722762e-06
Iter: 287 loss: 3.43776924e-06
Iter: 288 loss: 3.41716895e-06
Iter: 289 loss: 3.41619489e-06
Iter: 290 loss: 3.41595955e-06
Iter: 291 loss: 3.41537498e-06
Iter: 292 loss: 3.41381792e-06
Iter: 293 loss: 3.41155101e-06
Iter: 294 loss: 3.41132773e-06
Iter: 295 loss: 3.40947645e-06
Iter: 296 loss: 3.42869134e-06
Iter: 297 loss: 3.40935867e-06
Iter: 298 loss: 3.40701536e-06
Iter: 299 loss: 3.41290252e-06
Iter: 300 loss: 3.40632459e-06
Iter: 301 loss: 3.40512111e-06
Iter: 302 loss: 3.40806855e-06
Iter: 303 loss: 3.40455472e-06
Iter: 304 loss: 3.40280803e-06
Iter: 305 loss: 3.40646466e-06
Iter: 306 loss: 3.40201495e-06
Iter: 307 loss: 3.40088286e-06
Iter: 308 loss: 3.40702172e-06
Iter: 309 loss: 3.40075667e-06
Iter: 310 loss: 3.39972303e-06
Iter: 311 loss: 3.40605743e-06
Iter: 312 loss: 3.39970165e-06
Iter: 313 loss: 3.39865892e-06
Iter: 314 loss: 3.40106112e-06
Iter: 315 loss: 3.39824805e-06
Iter: 316 loss: 3.3973015e-06
Iter: 317 loss: 3.39575809e-06
Iter: 318 loss: 3.39565577e-06
Iter: 319 loss: 3.39485769e-06
Iter: 320 loss: 3.39480425e-06
Iter: 321 loss: 3.39388225e-06
Iter: 322 loss: 3.39197732e-06
Iter: 323 loss: 3.43601823e-06
Iter: 324 loss: 3.39201097e-06
Iter: 325 loss: 3.39045164e-06
Iter: 326 loss: 3.39323128e-06
Iter: 327 loss: 3.38988184e-06
Iter: 328 loss: 3.38757e-06
Iter: 329 loss: 3.38798918e-06
Iter: 330 loss: 3.38599239e-06
Iter: 331 loss: 3.38391078e-06
Iter: 332 loss: 3.3889e-06
Iter: 333 loss: 3.38326208e-06
Iter: 334 loss: 3.38027303e-06
Iter: 335 loss: 3.39799021e-06
Iter: 336 loss: 3.37990787e-06
Iter: 337 loss: 3.37878328e-06
Iter: 338 loss: 3.37953202e-06
Iter: 339 loss: 3.37810116e-06
Iter: 340 loss: 3.37567826e-06
Iter: 341 loss: 3.3762708e-06
Iter: 342 loss: 3.37409392e-06
Iter: 343 loss: 3.3726003e-06
Iter: 344 loss: 3.37027905e-06
Iter: 345 loss: 3.37026177e-06
Iter: 346 loss: 3.37542042e-06
Iter: 347 loss: 3.36962239e-06
Iter: 348 loss: 3.36911216e-06
Iter: 349 loss: 3.36768062e-06
Iter: 350 loss: 3.38061818e-06
Iter: 351 loss: 3.36759672e-06
Iter: 352 loss: 3.36560333e-06
Iter: 353 loss: 3.37647248e-06
Iter: 354 loss: 3.36528865e-06
Iter: 355 loss: 3.36398671e-06
Iter: 356 loss: 3.36460289e-06
Iter: 357 loss: 3.36326934e-06
Iter: 358 loss: 3.36185485e-06
Iter: 359 loss: 3.3611127e-06
Iter: 360 loss: 3.3603767e-06
Iter: 361 loss: 3.35855339e-06
Iter: 362 loss: 3.37125903e-06
Iter: 363 loss: 3.35826917e-06
Iter: 364 loss: 3.35680238e-06
Iter: 365 loss: 3.35450864e-06
Iter: 366 loss: 3.35441359e-06
Iter: 367 loss: 3.35412255e-06
Iter: 368 loss: 3.35330287e-06
Iter: 369 loss: 3.35230447e-06
Iter: 370 loss: 3.35016375e-06
Iter: 371 loss: 3.39099347e-06
Iter: 372 loss: 3.35024356e-06
Iter: 373 loss: 3.34881656e-06
Iter: 374 loss: 3.34878109e-06
Iter: 375 loss: 3.34716e-06
Iter: 376 loss: 3.34713559e-06
Iter: 377 loss: 3.34596643e-06
Iter: 378 loss: 3.34390347e-06
Iter: 379 loss: 3.35262121e-06
Iter: 380 loss: 3.34356628e-06
Iter: 381 loss: 3.34254923e-06
Iter: 382 loss: 3.35199343e-06
Iter: 383 loss: 3.34254651e-06
Iter: 384 loss: 3.34135621e-06
Iter: 385 loss: 3.34236029e-06
Iter: 386 loss: 3.34057768e-06
Iter: 387 loss: 3.33981666e-06
Iter: 388 loss: 3.34070569e-06
Iter: 389 loss: 3.33947469e-06
Iter: 390 loss: 3.33832713e-06
Iter: 391 loss: 3.34083643e-06
Iter: 392 loss: 3.3378617e-06
Iter: 393 loss: 3.33672233e-06
Iter: 394 loss: 3.33575326e-06
Iter: 395 loss: 3.33533217e-06
Iter: 396 loss: 3.33382013e-06
Iter: 397 loss: 3.33582375e-06
Iter: 398 loss: 3.33296816e-06
Iter: 399 loss: 3.33114349e-06
Iter: 400 loss: 3.33607659e-06
Iter: 401 loss: 3.33050798e-06
Iter: 402 loss: 3.32886771e-06
Iter: 403 loss: 3.33105913e-06
Iter: 404 loss: 3.32799891e-06
Iter: 405 loss: 3.32582022e-06
Iter: 406 loss: 3.34437664e-06
Iter: 407 loss: 3.32567015e-06
Iter: 408 loss: 3.32459967e-06
Iter: 409 loss: 3.32202762e-06
Iter: 410 loss: 3.34978768e-06
Iter: 411 loss: 3.32181276e-06
Iter: 412 loss: 3.32032118e-06
Iter: 413 loss: 3.31992396e-06
Iter: 414 loss: 3.31868682e-06
Iter: 415 loss: 3.3250908e-06
Iter: 416 loss: 3.3185072e-06
Iter: 417 loss: 3.31716296e-06
Iter: 418 loss: 3.32613126e-06
Iter: 419 loss: 3.31697515e-06
Iter: 420 loss: 3.31594674e-06
Iter: 421 loss: 3.31640967e-06
Iter: 422 loss: 3.31523574e-06
Iter: 423 loss: 3.31412775e-06
Iter: 424 loss: 3.31348269e-06
Iter: 425 loss: 3.31305955e-06
Iter: 426 loss: 3.31109641e-06
Iter: 427 loss: 3.32352465e-06
Iter: 428 loss: 3.31089086e-06
Iter: 429 loss: 3.30953117e-06
Iter: 430 loss: 3.30901503e-06
Iter: 431 loss: 3.30825605e-06
Iter: 432 loss: 3.30651619e-06
Iter: 433 loss: 3.30750959e-06
Iter: 434 loss: 3.3052811e-06
Iter: 435 loss: 3.30296e-06
Iter: 436 loss: 3.31106958e-06
Iter: 437 loss: 3.30242847e-06
Iter: 438 loss: 3.30008197e-06
Iter: 439 loss: 3.30086573e-06
Iter: 440 loss: 3.29849263e-06
Iter: 441 loss: 3.29708246e-06
Iter: 442 loss: 3.29695649e-06
Iter: 443 loss: 3.29555223e-06
Iter: 444 loss: 3.29195859e-06
Iter: 445 loss: 3.32203081e-06
Iter: 446 loss: 3.29135105e-06
Iter: 447 loss: 3.2900316e-06
Iter: 448 loss: 3.28971896e-06
Iter: 449 loss: 3.28825763e-06
Iter: 450 loss: 3.30770035e-06
Iter: 451 loss: 3.28824e-06
Iter: 452 loss: 3.28734518e-06
Iter: 453 loss: 3.2890382e-06
Iter: 454 loss: 3.28713077e-06
Iter: 455 loss: 3.28634883e-06
Iter: 456 loss: 3.28492752e-06
Iter: 457 loss: 3.31251226e-06
Iter: 458 loss: 3.28484316e-06
Iter: 459 loss: 3.28306078e-06
Iter: 460 loss: 3.30471903e-06
Iter: 461 loss: 3.28300484e-06
Iter: 462 loss: 3.28210081e-06
Iter: 463 loss: 3.28174178e-06
Iter: 464 loss: 3.28119654e-06
Iter: 465 loss: 3.27977909e-06
Iter: 466 loss: 3.28076271e-06
Iter: 467 loss: 3.279059e-06
Iter: 468 loss: 3.27692896e-06
Iter: 469 loss: 3.28227429e-06
Iter: 470 loss: 3.27637417e-06
Iter: 471 loss: 3.2743319e-06
Iter: 472 loss: 3.27576936e-06
Iter: 473 loss: 3.27316729e-06
Iter: 474 loss: 3.27186217e-06
Iter: 475 loss: 3.27191628e-06
Iter: 476 loss: 3.27032444e-06
Iter: 477 loss: 3.26703912e-06
Iter: 478 loss: 3.32417744e-06
Iter: 479 loss: 3.26703912e-06
Iter: 480 loss: 3.2648727e-06
Iter: 481 loss: 3.28246597e-06
Iter: 482 loss: 3.26478698e-06
Iter: 483 loss: 3.26345412e-06
Iter: 484 loss: 3.26328973e-06
Iter: 485 loss: 3.26232725e-06
Iter: 486 loss: 3.26186364e-06
Iter: 487 loss: 3.26140048e-06
Iter: 488 loss: 3.26012741e-06
Iter: 489 loss: 3.26178838e-06
Iter: 490 loss: 3.259506e-06
Iter: 491 loss: 3.25793735e-06
Iter: 492 loss: 3.26005738e-06
Iter: 493 loss: 3.25699102e-06
Iter: 494 loss: 3.25500832e-06
Iter: 495 loss: 3.26561235e-06
Iter: 496 loss: 3.25482461e-06
Iter: 497 loss: 3.25369865e-06
Iter: 498 loss: 3.25141309e-06
Iter: 499 loss: 3.27540943e-06
Iter: 500 loss: 3.25085762e-06
Iter: 501 loss: 3.24827715e-06
Iter: 502 loss: 3.28470264e-06
Iter: 503 loss: 3.24823759e-06
Iter: 504 loss: 3.24647272e-06
Iter: 505 loss: 3.24401844e-06
Iter: 506 loss: 3.24395387e-06
Iter: 507 loss: 3.24159646e-06
Iter: 508 loss: 3.26674103e-06
Iter: 509 loss: 3.24156554e-06
Iter: 510 loss: 3.23900531e-06
Iter: 511 loss: 3.24238226e-06
Iter: 512 loss: 3.23766744e-06
Iter: 513 loss: 3.23605218e-06
Iter: 514 loss: 3.23600511e-06
Iter: 515 loss: 3.23484346e-06
Iter: 516 loss: 3.23458289e-06
Iter: 517 loss: 3.23370568e-06
Iter: 518 loss: 3.23303425e-06
Iter: 519 loss: 3.23244285e-06
Iter: 520 loss: 3.23220365e-06
Iter: 521 loss: 3.23118024e-06
Iter: 522 loss: 3.23322774e-06
Iter: 523 loss: 3.23080212e-06
Iter: 524 loss: 3.22938286e-06
Iter: 525 loss: 3.22927121e-06
Iter: 526 loss: 3.22820097e-06
Iter: 527 loss: 3.22649703e-06
Iter: 528 loss: 3.24244274e-06
Iter: 529 loss: 3.22640494e-06
Iter: 530 loss: 3.2253356e-06
Iter: 531 loss: 3.22278765e-06
Iter: 532 loss: 3.26051713e-06
Iter: 533 loss: 3.22274263e-06
Iter: 534 loss: 3.22047049e-06
Iter: 535 loss: 3.24261032e-06
Iter: 536 loss: 3.22037931e-06
Iter: 537 loss: 3.21885682e-06
Iter: 538 loss: 3.22065648e-06
Iter: 539 loss: 3.2178225e-06
Iter: 540 loss: 3.21569678e-06
Iter: 541 loss: 3.21952712e-06
Iter: 542 loss: 3.21480093e-06
Iter: 543 loss: 3.2128878e-06
Iter: 544 loss: 3.2287121e-06
Iter: 545 loss: 3.21274524e-06
Iter: 546 loss: 3.21055063e-06
Iter: 547 loss: 3.20900699e-06
Iter: 548 loss: 3.20832351e-06
Iter: 549 loss: 3.20718118e-06
Iter: 550 loss: 3.20706204e-06
Iter: 551 loss: 3.20542085e-06
Iter: 552 loss: 3.20390495e-06
Iter: 553 loss: 3.20341405e-06
Iter: 554 loss: 3.20198933e-06
Iter: 555 loss: 3.21159519e-06
Iter: 556 loss: 3.20194795e-06
Iter: 557 loss: 3.20072741e-06
Iter: 558 loss: 3.19966762e-06
Iter: 559 loss: 3.1992447e-06
Iter: 560 loss: 3.19672245e-06
Iter: 561 loss: 3.20444269e-06
Iter: 562 loss: 3.19585206e-06
Iter: 563 loss: 3.19386481e-06
Iter: 564 loss: 3.20072877e-06
Iter: 565 loss: 3.19324272e-06
Iter: 566 loss: 3.19200808e-06
Iter: 567 loss: 3.18957427e-06
Iter: 568 loss: 3.22998699e-06
Iter: 569 loss: 3.18951743e-06
Iter: 570 loss: 3.18782395e-06
Iter: 571 loss: 3.18755656e-06
Iter: 572 loss: 3.18637967e-06
Iter: 573 loss: 3.18448338e-06
Iter: 574 loss: 3.18447792e-06
Iter: 575 loss: 3.18231582e-06
Iter: 576 loss: 3.20778486e-06
Iter: 577 loss: 3.18232401e-06
Iter: 578 loss: 3.18061075e-06
Iter: 579 loss: 3.18880097e-06
Iter: 580 loss: 3.1805007e-06
Iter: 581 loss: 3.17911986e-06
Iter: 582 loss: 3.17919603e-06
Iter: 583 loss: 3.17799845e-06
Iter: 584 loss: 3.17757031e-06
Iter: 585 loss: 3.17703189e-06
Iter: 586 loss: 3.17675176e-06
Iter: 587 loss: 3.17552986e-06
Iter: 588 loss: 3.17647391e-06
Iter: 589 loss: 3.17450531e-06
Iter: 590 loss: 3.17225863e-06
Iter: 591 loss: 3.18987645e-06
Iter: 592 loss: 3.17202625e-06
Iter: 593 loss: 3.1708e-06
Iter: 594 loss: 3.18280718e-06
Iter: 595 loss: 3.17059312e-06
Iter: 596 loss: 3.16962496e-06
Iter: 597 loss: 3.16813498e-06
Iter: 598 loss: 3.16811884e-06
Iter: 599 loss: 3.16612841e-06
Iter: 600 loss: 3.17210061e-06
Iter: 601 loss: 3.1655909e-06
Iter: 602 loss: 3.16413434e-06
Iter: 603 loss: 3.16275919e-06
Iter: 604 loss: 3.16241949e-06
Iter: 605 loss: 3.15990474e-06
Iter: 606 loss: 3.18976936e-06
Iter: 607 loss: 3.15997931e-06
Iter: 608 loss: 3.15874695e-06
Iter: 609 loss: 3.15743955e-06
Iter: 610 loss: 3.15738134e-06
Iter: 611 loss: 3.15508169e-06
Iter: 612 loss: 3.17915578e-06
Iter: 613 loss: 3.15498505e-06
Iter: 614 loss: 3.15367106e-06
Iter: 615 loss: 3.1555021e-06
Iter: 616 loss: 3.15294642e-06
Iter: 617 loss: 3.15250372e-06
Iter: 618 loss: 3.15209218e-06
Iter: 619 loss: 3.15151055e-06
Iter: 620 loss: 3.14920771e-06
Iter: 621 loss: 3.1555478e-06
Iter: 622 loss: 3.14808858e-06
Iter: 623 loss: 3.14719978e-06
Iter: 624 loss: 3.14683666e-06
Iter: 625 loss: 3.14592421e-06
Iter: 626 loss: 3.14401132e-06
Iter: 627 loss: 3.1650527e-06
Iter: 628 loss: 3.14381123e-06
Iter: 629 loss: 3.14178351e-06
Iter: 630 loss: 3.14176532e-06
Iter: 631 loss: 3.14042063e-06
Iter: 632 loss: 3.14588033e-06
Iter: 633 loss: 3.1401089e-06
Iter: 634 loss: 3.13954479e-06
Iter: 635 loss: 3.13807413e-06
Iter: 636 loss: 3.15437251e-06
Iter: 637 loss: 3.13801547e-06
Iter: 638 loss: 3.1360396e-06
Iter: 639 loss: 3.14741783e-06
Iter: 640 loss: 3.13572036e-06
Iter: 641 loss: 3.13367491e-06
Iter: 642 loss: 3.1408249e-06
Iter: 643 loss: 3.13324335e-06
Iter: 644 loss: 3.13157079e-06
Iter: 645 loss: 3.13404325e-06
Iter: 646 loss: 3.13071359e-06
Iter: 647 loss: 3.12835573e-06
Iter: 648 loss: 3.1328957e-06
Iter: 649 loss: 3.12727798e-06
Iter: 650 loss: 3.12693055e-06
Iter: 651 loss: 3.12635893e-06
Iter: 652 loss: 3.1256327e-06
Iter: 653 loss: 3.12470593e-06
Iter: 654 loss: 3.12444513e-06
Iter: 655 loss: 3.12353086e-06
Iter: 656 loss: 3.12377688e-06
Iter: 657 loss: 3.12269276e-06
Iter: 658 loss: 3.12117459e-06
Iter: 659 loss: 3.12720249e-06
Iter: 660 loss: 3.12071734e-06
Iter: 661 loss: 3.1196887e-06
Iter: 662 loss: 3.12083466e-06
Iter: 663 loss: 3.11914459e-06
Iter: 664 loss: 3.11725921e-06
Iter: 665 loss: 3.11524741e-06
Iter: 666 loss: 3.11485792e-06
Iter: 667 loss: 3.1117529e-06
Iter: 668 loss: 3.12198108e-06
Iter: 669 loss: 3.11081726e-06
Iter: 670 loss: 3.10856967e-06
Iter: 671 loss: 3.10571045e-06
Iter: 672 loss: 3.1055265e-06
Iter: 673 loss: 3.10307405e-06
Iter: 674 loss: 3.10283235e-06
Iter: 675 loss: 3.10127643e-06
Iter: 676 loss: 3.10392988e-06
Iter: 677 loss: 3.10048449e-06
Iter: 678 loss: 3.09864481e-06
Iter: 679 loss: 3.10914834e-06
Iter: 680 loss: 3.09848906e-06
Iter: 681 loss: 3.09705342e-06
Iter: 682 loss: 3.10090854e-06
Iter: 683 loss: 3.09655115e-06
Iter: 684 loss: 3.09484176e-06
Iter: 685 loss: 3.11258054e-06
Iter: 686 loss: 3.09477423e-06
Iter: 687 loss: 3.09391e-06
Iter: 688 loss: 3.09262032e-06
Iter: 689 loss: 3.09272264e-06
Iter: 690 loss: 3.09127904e-06
Iter: 691 loss: 3.10024075e-06
Iter: 692 loss: 3.09094276e-06
Iter: 693 loss: 3.08973972e-06
Iter: 694 loss: 3.08889958e-06
Iter: 695 loss: 3.08836502e-06
Iter: 696 loss: 3.0873216e-06
Iter: 697 loss: 3.08729159e-06
Iter: 698 loss: 3.08637914e-06
Iter: 699 loss: 3.08460471e-06
Iter: 700 loss: 3.11808208e-06
Iter: 701 loss: 3.08456219e-06
Iter: 702 loss: 3.0821484e-06
Iter: 703 loss: 3.08671088e-06
Iter: 704 loss: 3.08105973e-06
Iter: 705 loss: 3.07900655e-06
Iter: 706 loss: 3.08172957e-06
Iter: 707 loss: 3.07798769e-06
Iter: 708 loss: 3.07556502e-06
Iter: 709 loss: 3.09319012e-06
Iter: 710 loss: 3.07540881e-06
Iter: 711 loss: 3.07368236e-06
Iter: 712 loss: 3.07880509e-06
Iter: 713 loss: 3.07307664e-06
Iter: 714 loss: 3.07119512e-06
Iter: 715 loss: 3.07940354e-06
Iter: 716 loss: 3.07068285e-06
Iter: 717 loss: 3.0695935e-06
Iter: 718 loss: 3.06954053e-06
Iter: 719 loss: 3.06853781e-06
Iter: 720 loss: 3.0664828e-06
Iter: 721 loss: 3.09347161e-06
Iter: 722 loss: 3.06626544e-06
Iter: 723 loss: 3.06462334e-06
Iter: 724 loss: 3.0865292e-06
Iter: 725 loss: 3.06451921e-06
Iter: 726 loss: 3.06316679e-06
Iter: 727 loss: 3.06453876e-06
Iter: 728 loss: 3.06250058e-06
Iter: 729 loss: 3.06123729e-06
Iter: 730 loss: 3.06484185e-06
Iter: 731 loss: 3.06072798e-06
Iter: 732 loss: 3.0589224e-06
Iter: 733 loss: 3.05801359e-06
Iter: 734 loss: 3.05716821e-06
Iter: 735 loss: 3.05518051e-06
Iter: 736 loss: 3.06177571e-06
Iter: 737 loss: 3.05456251e-06
Iter: 738 loss: 3.05256549e-06
Iter: 739 loss: 3.05072081e-06
Iter: 740 loss: 3.04999367e-06
Iter: 741 loss: 3.04795753e-06
Iter: 742 loss: 3.04799e-06
Iter: 743 loss: 3.04618561e-06
Iter: 744 loss: 3.04666173e-06
Iter: 745 loss: 3.0448673e-06
Iter: 746 loss: 3.0432484e-06
Iter: 747 loss: 3.04326704e-06
Iter: 748 loss: 3.04226114e-06
Iter: 749 loss: 3.05358958e-06
Iter: 750 loss: 3.04235846e-06
Iter: 751 loss: 3.04124524e-06
Iter: 752 loss: 3.03954903e-06
Iter: 753 loss: 3.07992354e-06
Iter: 754 loss: 3.03947854e-06
Iter: 755 loss: 3.03803654e-06
Iter: 756 loss: 3.0461747e-06
Iter: 757 loss: 3.03788192e-06
Iter: 758 loss: 3.03608431e-06
Iter: 759 loss: 3.03675188e-06
Iter: 760 loss: 3.03494699e-06
Iter: 761 loss: 3.03311026e-06
Iter: 762 loss: 3.03968227e-06
Iter: 763 loss: 3.03269871e-06
Iter: 764 loss: 3.03123534e-06
Iter: 765 loss: 3.04496e-06
Iter: 766 loss: 3.03108095e-06
Iter: 767 loss: 3.03005845e-06
Iter: 768 loss: 3.02833132e-06
Iter: 769 loss: 3.06204493e-06
Iter: 770 loss: 3.02819399e-06
Iter: 771 loss: 3.02606668e-06
Iter: 772 loss: 3.04112655e-06
Iter: 773 loss: 3.02587387e-06
Iter: 774 loss: 3.02438957e-06
Iter: 775 loss: 3.0230035e-06
Iter: 776 loss: 3.02267881e-06
Iter: 777 loss: 3.01985551e-06
Iter: 778 loss: 3.04397827e-06
Iter: 779 loss: 3.01981163e-06
Iter: 780 loss: 3.01792124e-06
Iter: 781 loss: 3.02212266e-06
Iter: 782 loss: 3.01729847e-06
Iter: 783 loss: 3.01534055e-06
Iter: 784 loss: 3.03727256e-06
Iter: 785 loss: 3.01534146e-06
Iter: 786 loss: 3.01349428e-06
Iter: 787 loss: 3.01618866e-06
Iter: 788 loss: 3.01271e-06
Iter: 789 loss: 3.01151317e-06
Iter: 790 loss: 3.01081559e-06
Iter: 791 loss: 3.01028831e-06
Iter: 792 loss: 3.0082856e-06
Iter: 793 loss: 3.02075023e-06
Iter: 794 loss: 3.00814486e-06
Iter: 795 loss: 3.00666375e-06
Iter: 796 loss: 3.00532201e-06
Iter: 797 loss: 3.00506281e-06
Iter: 798 loss: 3.00316901e-06
Iter: 799 loss: 3.03253591e-06
Iter: 800 loss: 3.00313741e-06
Iter: 801 loss: 3.00177817e-06
Iter: 802 loss: 3.00051715e-06
Iter: 803 loss: 3.00022452e-06
Iter: 804 loss: 2.99822887e-06
Iter: 805 loss: 2.99984185e-06
Iter: 806 loss: 2.9970729e-06
Iter: 807 loss: 2.99424619e-06
Iter: 808 loss: 3.00062288e-06
Iter: 809 loss: 2.99308022e-06
Iter: 810 loss: 2.99125782e-06
Iter: 811 loss: 3.0054432e-06
Iter: 812 loss: 2.99116209e-06
Iter: 813 loss: 2.98940972e-06
Iter: 814 loss: 2.9887367e-06
Iter: 815 loss: 2.98763462e-06
Iter: 816 loss: 2.98632312e-06
Iter: 817 loss: 2.98612508e-06
Iter: 818 loss: 2.9848959e-06
Iter: 819 loss: 2.99183034e-06
Iter: 820 loss: 2.98474924e-06
Iter: 821 loss: 2.98383725e-06
Iter: 822 loss: 2.98125678e-06
Iter: 823 loss: 2.99384055e-06
Iter: 824 loss: 2.98043415e-06
Iter: 825 loss: 2.97955012e-06
Iter: 826 loss: 2.97912516e-06
Iter: 827 loss: 2.97786414e-06
Iter: 828 loss: 2.97595307e-06
Iter: 829 loss: 2.97585734e-06
Iter: 830 loss: 2.97386123e-06
Iter: 831 loss: 2.98268924e-06
Iter: 832 loss: 2.97341967e-06
Iter: 833 loss: 2.97168481e-06
Iter: 834 loss: 2.99311364e-06
Iter: 835 loss: 2.97175461e-06
Iter: 836 loss: 2.97091174e-06
Iter: 837 loss: 2.96986309e-06
Iter: 838 loss: 2.96985672e-06
Iter: 839 loss: 2.96837811e-06
Iter: 840 loss: 2.96890312e-06
Iter: 841 loss: 2.96726557e-06
Iter: 842 loss: 2.96454164e-06
Iter: 843 loss: 2.97014185e-06
Iter: 844 loss: 2.9634225e-06
Iter: 845 loss: 2.96124563e-06
Iter: 846 loss: 2.98115856e-06
Iter: 847 loss: 2.96118969e-06
Iter: 848 loss: 2.95921473e-06
Iter: 849 loss: 2.95735344e-06
Iter: 850 loss: 2.95685368e-06
Iter: 851 loss: 2.95849304e-06
Iter: 852 loss: 2.95572909e-06
Iter: 853 loss: 2.9550074e-06
Iter: 854 loss: 2.95428526e-06
Iter: 855 loss: 2.95416544e-06
Iter: 856 loss: 2.95303812e-06
Iter: 857 loss: 2.95016662e-06
Iter: 858 loss: 2.97132055e-06
Iter: 859 loss: 2.94941901e-06
Iter: 860 loss: 2.95146538e-06
Iter: 861 loss: 2.94788651e-06
Iter: 862 loss: 2.94708047e-06
Iter: 863 loss: 2.94601978e-06
Iter: 864 loss: 2.94591609e-06
Iter: 865 loss: 2.94325e-06
Iter: 866 loss: 2.94760184e-06
Iter: 867 loss: 2.94200936e-06
Iter: 868 loss: 2.94039637e-06
Iter: 869 loss: 2.94946381e-06
Iter: 870 loss: 2.94023289e-06
Iter: 871 loss: 2.93906e-06
Iter: 872 loss: 2.93603898e-06
Iter: 873 loss: 2.95999644e-06
Iter: 874 loss: 2.93537187e-06
Iter: 875 loss: 2.93253947e-06
Iter: 876 loss: 2.93720609e-06
Iter: 877 loss: 2.93121e-06
Iter: 878 loss: 2.9293119e-06
Iter: 879 loss: 2.92910909e-06
Iter: 880 loss: 2.92775348e-06
Iter: 881 loss: 2.92800223e-06
Iter: 882 loss: 2.9267585e-06
Iter: 883 loss: 2.92491404e-06
Iter: 884 loss: 2.9422481e-06
Iter: 885 loss: 2.92485424e-06
Iter: 886 loss: 2.92339428e-06
Iter: 887 loss: 2.93908897e-06
Iter: 888 loss: 2.92336927e-06
Iter: 889 loss: 2.92248433e-06
Iter: 890 loss: 2.92068216e-06
Iter: 891 loss: 2.94774327e-06
Iter: 892 loss: 2.92053119e-06
Iter: 893 loss: 2.91881315e-06
Iter: 894 loss: 2.92883442e-06
Iter: 895 loss: 2.91852939e-06
Iter: 896 loss: 2.91714014e-06
Iter: 897 loss: 2.92438926e-06
Iter: 898 loss: 2.91693868e-06
Iter: 899 loss: 2.91532342e-06
Iter: 900 loss: 2.91488504e-06
Iter: 901 loss: 2.91380752e-06
Iter: 902 loss: 2.91263473e-06
Iter: 903 loss: 2.93123389e-06
Iter: 904 loss: 2.91260562e-06
Iter: 905 loss: 2.91155357e-06
Iter: 906 loss: 2.90879848e-06
Iter: 907 loss: 2.92943469e-06
Iter: 908 loss: 2.90822754e-06
Iter: 909 loss: 2.9062312e-06
Iter: 910 loss: 2.90624894e-06
Iter: 911 loss: 2.90474895e-06
Iter: 912 loss: 2.90194112e-06
Iter: 913 loss: 2.90194475e-06
Iter: 914 loss: 2.8993129e-06
Iter: 915 loss: 2.89918444e-06
Iter: 916 loss: 2.89790023e-06
Iter: 917 loss: 2.90406979e-06
Iter: 918 loss: 2.89778109e-06
Iter: 919 loss: 2.89647642e-06
Iter: 920 loss: 2.91152719e-06
Iter: 921 loss: 2.89656464e-06
Iter: 922 loss: 2.89541322e-06
Iter: 923 loss: 2.89275613e-06
Iter: 924 loss: 2.92195364e-06
Iter: 925 loss: 2.89244554e-06
Iter: 926 loss: 2.88981823e-06
Iter: 927 loss: 2.9056398e-06
Iter: 928 loss: 2.88949741e-06
Iter: 929 loss: 2.88784531e-06
Iter: 930 loss: 2.89102718e-06
Iter: 931 loss: 2.88705314e-06
Iter: 932 loss: 2.88487445e-06
Iter: 933 loss: 2.89831905e-06
Iter: 934 loss: 2.88476485e-06
Iter: 935 loss: 2.883457e-06
Iter: 936 loss: 2.88394176e-06
Iter: 937 loss: 2.8826571e-06
Iter: 938 loss: 2.88012438e-06
Iter: 939 loss: 2.87986859e-06
Iter: 940 loss: 2.87819535e-06
Iter: 941 loss: 2.87600164e-06
Iter: 942 loss: 2.87457351e-06
Iter: 943 loss: 2.87381772e-06
Iter: 944 loss: 2.87002968e-06
Iter: 945 loss: 2.89488435e-06
Iter: 946 loss: 2.86955606e-06
Iter: 947 loss: 2.86750128e-06
Iter: 948 loss: 2.87605735e-06
Iter: 949 loss: 2.86712725e-06
Iter: 950 loss: 2.86485192e-06
Iter: 951 loss: 2.87442617e-06
Iter: 952 loss: 2.86423415e-06
Iter: 953 loss: 2.86272552e-06
Iter: 954 loss: 2.8627112e-06
Iter: 955 loss: 2.86129307e-06
Iter: 956 loss: 2.86051613e-06
Iter: 957 loss: 2.85987608e-06
Iter: 958 loss: 2.85861506e-06
Iter: 959 loss: 2.85783358e-06
Iter: 960 loss: 2.85733927e-06
Iter: 961 loss: 2.8551608e-06
Iter: 962 loss: 2.8705133e-06
Iter: 963 loss: 2.85496094e-06
Iter: 964 loss: 2.85358033e-06
Iter: 965 loss: 2.86577892e-06
Iter: 966 loss: 2.85356464e-06
Iter: 967 loss: 2.85249371e-06
Iter: 968 loss: 2.85069018e-06
Iter: 969 loss: 2.89155378e-06
Iter: 970 loss: 2.85069291e-06
Iter: 971 loss: 2.84855923e-06
Iter: 972 loss: 2.87609419e-06
Iter: 973 loss: 2.84846647e-06
Iter: 974 loss: 2.84738508e-06
Iter: 975 loss: 2.84520161e-06
Iter: 976 loss: 2.88619663e-06
Iter: 977 loss: 2.84510043e-06
Iter: 978 loss: 2.84237512e-06
Iter: 979 loss: 2.85710257e-06
Iter: 980 loss: 2.8418508e-06
Iter: 981 loss: 2.83904637e-06
Iter: 982 loss: 2.83889e-06
Iter: 983 loss: 2.83660574e-06
Iter: 984 loss: 2.83520103e-06
Iter: 985 loss: 2.83467398e-06
Iter: 986 loss: 2.83340205e-06
Iter: 987 loss: 2.8460222e-06
Iter: 988 loss: 2.83335794e-06
Iter: 989 loss: 2.83203372e-06
Iter: 990 loss: 2.83213376e-06
Iter: 991 loss: 2.8309405e-06
Iter: 992 loss: 2.82923838e-06
Iter: 993 loss: 2.82577093e-06
Iter: 994 loss: 2.90331946e-06
Iter: 995 loss: 2.82580891e-06
Iter: 996 loss: 2.82355927e-06
Iter: 997 loss: 2.82334713e-06
Iter: 998 loss: 2.82174778e-06
Iter: 999 loss: 2.82710289e-06
Iter: 1000 loss: 2.82127803e-06
Iter: 1001 loss: 2.81906796e-06
Iter: 1002 loss: 2.8161985e-06
Iter: 1003 loss: 2.81606708e-06
Iter: 1004 loss: 2.81441748e-06
Iter: 1005 loss: 2.81418374e-06
Iter: 1006 loss: 2.81279745e-06
Iter: 1007 loss: 2.80944732e-06
Iter: 1008 loss: 2.84234466e-06
Iter: 1009 loss: 2.8089878e-06
Iter: 1010 loss: 2.8058862e-06
Iter: 1011 loss: 2.85147553e-06
Iter: 1012 loss: 2.80588301e-06
Iter: 1013 loss: 2.80396671e-06
Iter: 1014 loss: 2.80475274e-06
Iter: 1015 loss: 2.80252425e-06
Iter: 1016 loss: 2.79971619e-06
Iter: 1017 loss: 2.81316647e-06
Iter: 1018 loss: 2.79931987e-06
Iter: 1019 loss: 2.79746109e-06
Iter: 1020 loss: 2.79731239e-06
Iter: 1021 loss: 2.79614642e-06
Iter: 1022 loss: 2.79931851e-06
Iter: 1023 loss: 2.79557867e-06
Iter: 1024 loss: 2.79443884e-06
Iter: 1025 loss: 2.7929791e-06
Iter: 1026 loss: 2.7929168e-06
Iter: 1027 loss: 2.79101e-06
Iter: 1028 loss: 2.79766164e-06
Iter: 1029 loss: 2.79060032e-06
Iter: 1030 loss: 2.78854554e-06
Iter: 1031 loss: 2.7989181e-06
Iter: 1032 loss: 2.78836046e-06
Iter: 1033 loss: 2.78669859e-06
Iter: 1034 loss: 2.79240362e-06
Iter: 1035 loss: 2.78636116e-06
Iter: 1036 loss: 2.78530706e-06
Iter: 1037 loss: 2.78553944e-06
Iter: 1038 loss: 2.78470429e-06
Iter: 1039 loss: 2.78238167e-06
Iter: 1040 loss: 2.7820538e-06
Iter: 1041 loss: 2.78055177e-06
Iter: 1042 loss: 2.77865297e-06
Iter: 1043 loss: 2.78535936e-06
Iter: 1044 loss: 2.77807112e-06
Iter: 1045 loss: 2.77591198e-06
Iter: 1046 loss: 2.77674371e-06
Iter: 1047 loss: 2.77436857e-06
Iter: 1048 loss: 2.77140475e-06
Iter: 1049 loss: 2.77907975e-06
Iter: 1050 loss: 2.77053527e-06
Iter: 1051 loss: 2.7688634e-06
Iter: 1052 loss: 2.76862647e-06
Iter: 1053 loss: 2.76706896e-06
Iter: 1054 loss: 2.7709616e-06
Iter: 1055 loss: 2.76649666e-06
Iter: 1056 loss: 2.76506239e-06
Iter: 1057 loss: 2.76230276e-06
Iter: 1058 loss: 2.82524343e-06
Iter: 1059 loss: 2.76223909e-06
Iter: 1060 loss: 2.75937828e-06
Iter: 1061 loss: 2.7689041e-06
Iter: 1062 loss: 2.7586841e-06
Iter: 1063 loss: 2.75609659e-06
Iter: 1064 loss: 2.78040352e-06
Iter: 1065 loss: 2.75597245e-06
Iter: 1066 loss: 2.75402226e-06
Iter: 1067 loss: 2.75554248e-06
Iter: 1068 loss: 2.7528381e-06
Iter: 1069 loss: 2.75018465e-06
Iter: 1070 loss: 2.75093907e-06
Iter: 1071 loss: 2.7483029e-06
Iter: 1072 loss: 2.74601825e-06
Iter: 1073 loss: 2.74601371e-06
Iter: 1074 loss: 2.74481658e-06
Iter: 1075 loss: 2.74221065e-06
Iter: 1076 loss: 2.76879177e-06
Iter: 1077 loss: 2.74176637e-06
Iter: 1078 loss: 2.73880778e-06
Iter: 1079 loss: 2.78147627e-06
Iter: 1080 loss: 2.73882051e-06
Iter: 1081 loss: 2.73651494e-06
Iter: 1082 loss: 2.73340538e-06
Iter: 1083 loss: 2.73328214e-06
Iter: 1084 loss: 2.7358642e-06
Iter: 1085 loss: 2.73153455e-06
Iter: 1086 loss: 2.72987e-06
Iter: 1087 loss: 2.74174704e-06
Iter: 1088 loss: 2.72973966e-06
Iter: 1089 loss: 2.72875423e-06
Iter: 1090 loss: 2.72873422e-06
Iter: 1091 loss: 2.72790658e-06
Iter: 1092 loss: 2.72601801e-06
Iter: 1093 loss: 2.72394436e-06
Iter: 1094 loss: 2.7236847e-06
Iter: 1095 loss: 2.72156e-06
Iter: 1096 loss: 2.72196121e-06
Iter: 1097 loss: 2.71999488e-06
Iter: 1098 loss: 2.71670251e-06
Iter: 1099 loss: 2.75275147e-06
Iter: 1100 loss: 2.71664749e-06
Iter: 1101 loss: 2.71549061e-06
Iter: 1102 loss: 2.7147621e-06
Iter: 1103 loss: 2.71424346e-06
Iter: 1104 loss: 2.71198087e-06
Iter: 1105 loss: 2.71068393e-06
Iter: 1106 loss: 2.70968872e-06
Iter: 1107 loss: 2.70725377e-06
Iter: 1108 loss: 2.74090144e-06
Iter: 1109 loss: 2.70726741e-06
Iter: 1110 loss: 2.70501505e-06
Iter: 1111 loss: 2.7102376e-06
Iter: 1112 loss: 2.70430746e-06
Iter: 1113 loss: 2.70260603e-06
Iter: 1114 loss: 2.7023541e-06
Iter: 1115 loss: 2.70121427e-06
Iter: 1116 loss: 2.69878365e-06
Iter: 1117 loss: 2.7111073e-06
Iter: 1118 loss: 2.69845646e-06
Iter: 1119 loss: 2.69736529e-06
Iter: 1120 loss: 2.69744714e-06
Iter: 1121 loss: 2.6961734e-06
Iter: 1122 loss: 2.69666634e-06
Iter: 1123 loss: 2.69525481e-06
Iter: 1124 loss: 2.69415432e-06
Iter: 1125 loss: 2.69146472e-06
Iter: 1126 loss: 2.72368834e-06
Iter: 1127 loss: 2.69123893e-06
Iter: 1128 loss: 2.68973281e-06
Iter: 1129 loss: 2.68955591e-06
Iter: 1130 loss: 2.6879668e-06
Iter: 1131 loss: 2.6860514e-06
Iter: 1132 loss: 2.68588542e-06
Iter: 1133 loss: 2.68357894e-06
Iter: 1134 loss: 2.68357144e-06
Iter: 1135 loss: 2.68257327e-06
Iter: 1136 loss: 2.6797552e-06
Iter: 1137 loss: 2.6983264e-06
Iter: 1138 loss: 2.67918495e-06
Iter: 1139 loss: 2.67674909e-06
Iter: 1140 loss: 2.67676296e-06
Iter: 1141 loss: 2.67474115e-06
Iter: 1142 loss: 2.68623671e-06
Iter: 1143 loss: 2.67456e-06
Iter: 1144 loss: 2.67288578e-06
Iter: 1145 loss: 2.67360906e-06
Iter: 1146 loss: 2.67172231e-06
Iter: 1147 loss: 2.66902975e-06
Iter: 1148 loss: 2.67271707e-06
Iter: 1149 loss: 2.66757206e-06
Iter: 1150 loss: 2.66564484e-06
Iter: 1151 loss: 2.69417023e-06
Iter: 1152 loss: 2.66568168e-06
Iter: 1153 loss: 2.66398661e-06
Iter: 1154 loss: 2.67685573e-06
Iter: 1155 loss: 2.66388315e-06
Iter: 1156 loss: 2.66290817e-06
Iter: 1157 loss: 2.66014513e-06
Iter: 1158 loss: 2.67062342e-06
Iter: 1159 loss: 2.6589496e-06
Iter: 1160 loss: 2.65621793e-06
Iter: 1161 loss: 2.68296844e-06
Iter: 1162 loss: 2.65613335e-06
Iter: 1163 loss: 2.65429912e-06
Iter: 1164 loss: 2.67177074e-06
Iter: 1165 loss: 2.65419567e-06
Iter: 1166 loss: 2.65250537e-06
Iter: 1167 loss: 2.65445647e-06
Iter: 1168 loss: 2.65142944e-06
Iter: 1169 loss: 2.64964365e-06
Iter: 1170 loss: 2.66680695e-06
Iter: 1171 loss: 2.64961955e-06
Iter: 1172 loss: 2.64852451e-06
Iter: 1173 loss: 2.64547793e-06
Iter: 1174 loss: 2.66995221e-06
Iter: 1175 loss: 2.6448663e-06
Iter: 1176 loss: 2.64231221e-06
Iter: 1177 loss: 2.66571988e-06
Iter: 1178 loss: 2.6422365e-06
Iter: 1179 loss: 2.63967354e-06
Iter: 1180 loss: 2.65474705e-06
Iter: 1181 loss: 2.63933975e-06
Iter: 1182 loss: 2.6380194e-06
Iter: 1183 loss: 2.63936568e-06
Iter: 1184 loss: 2.63726702e-06
Iter: 1185 loss: 2.63576385e-06
Iter: 1186 loss: 2.64631126e-06
Iter: 1187 loss: 2.63564652e-06
Iter: 1188 loss: 2.63462471e-06
Iter: 1189 loss: 2.65082372e-06
Iter: 1190 loss: 2.63459856e-06
Iter: 1191 loss: 2.63401353e-06
Iter: 1192 loss: 2.63289121e-06
Iter: 1193 loss: 2.65543076e-06
Iter: 1194 loss: 2.63297807e-06
Iter: 1195 loss: 2.63121933e-06
Iter: 1196 loss: 2.63053698e-06
Iter: 1197 loss: 2.62980893e-06
Iter: 1198 loss: 2.62799563e-06
Iter: 1199 loss: 2.63289439e-06
Iter: 1200 loss: 2.62738376e-06
Iter: 1201 loss: 2.62454455e-06
Iter: 1202 loss: 2.6342409e-06
Iter: 1203 loss: 2.62382082e-06
Iter: 1204 loss: 2.62261688e-06
Iter: 1205 loss: 2.62252615e-06
Iter: 1206 loss: 2.62166168e-06
Iter: 1207 loss: 2.61969853e-06
Iter: 1208 loss: 2.64228379e-06
Iter: 1209 loss: 2.61950117e-06
Iter: 1210 loss: 2.61723153e-06
Iter: 1211 loss: 2.62907633e-06
Iter: 1212 loss: 2.61705645e-06
Iter: 1213 loss: 2.61551713e-06
Iter: 1214 loss: 2.6295686e-06
Iter: 1215 loss: 2.61543391e-06
Iter: 1216 loss: 2.61416199e-06
Iter: 1217 loss: 2.611637e-06
Iter: 1218 loss: 2.66201914e-06
Iter: 1219 loss: 2.61154514e-06
Iter: 1220 loss: 2.61049695e-06
Iter: 1221 loss: 2.61028117e-06
Iter: 1222 loss: 2.60924162e-06
Iter: 1223 loss: 2.6188e-06
Iter: 1224 loss: 2.60917363e-06
Iter: 1225 loss: 2.60835077e-06
Iter: 1226 loss: 2.60684169e-06
Iter: 1227 loss: 2.60682282e-06
Iter: 1228 loss: 2.60534148e-06
Iter: 1229 loss: 2.60671027e-06
Iter: 1230 loss: 2.60438719e-06
Iter: 1231 loss: 2.60230945e-06
Iter: 1232 loss: 2.60430625e-06
Iter: 1233 loss: 2.60127445e-06
Iter: 1234 loss: 2.5997565e-06
Iter: 1235 loss: 2.62113281e-06
Iter: 1236 loss: 2.59986882e-06
Iter: 1237 loss: 2.5980512e-06
Iter: 1238 loss: 2.59738e-06
Iter: 1239 loss: 2.59647777e-06
Iter: 1240 loss: 2.59525382e-06
Iter: 1241 loss: 2.59513558e-06
Iter: 1242 loss: 2.5944189e-06
Iter: 1243 loss: 2.5918157e-06
Iter: 1244 loss: 2.59727608e-06
Iter: 1245 loss: 2.59033504e-06
Iter: 1246 loss: 2.59241028e-06
Iter: 1247 loss: 2.58941532e-06
Iter: 1248 loss: 2.58858677e-06
Iter: 1249 loss: 2.58696537e-06
Iter: 1250 loss: 2.60882075e-06
Iter: 1251 loss: 2.58671434e-06
Iter: 1252 loss: 2.58543491e-06
Iter: 1253 loss: 2.585522e-06
Iter: 1254 loss: 2.58424939e-06
Iter: 1255 loss: 2.59357103e-06
Iter: 1256 loss: 2.58408545e-06
Iter: 1257 loss: 2.58270575e-06
Iter: 1258 loss: 2.58054297e-06
Iter: 1259 loss: 2.58044656e-06
Iter: 1260 loss: 2.57866714e-06
Iter: 1261 loss: 2.58659702e-06
Iter: 1262 loss: 2.57834017e-06
Iter: 1263 loss: 2.57715715e-06
Iter: 1264 loss: 2.57558281e-06
Iter: 1265 loss: 2.57546844e-06
Iter: 1266 loss: 2.57297415e-06
Iter: 1267 loss: 2.58424757e-06
Iter: 1268 loss: 2.57219108e-06
Iter: 1269 loss: 2.57093302e-06
Iter: 1270 loss: 2.56786461e-06
Iter: 1271 loss: 2.61467426e-06
Iter: 1272 loss: 2.56782323e-06
Iter: 1273 loss: 2.5764316e-06
Iter: 1274 loss: 2.56729163e-06
Iter: 1275 loss: 2.56679732e-06
Iter: 1276 loss: 2.56560384e-06
Iter: 1277 loss: 2.57599754e-06
Iter: 1278 loss: 2.56544126e-06
Iter: 1279 loss: 2.56406179e-06
Iter: 1280 loss: 2.57336114e-06
Iter: 1281 loss: 2.5640345e-06
Iter: 1282 loss: 2.56309249e-06
Iter: 1283 loss: 2.56339877e-06
Iter: 1284 loss: 2.56247586e-06
Iter: 1285 loss: 2.56108137e-06
Iter: 1286 loss: 2.56559451e-06
Iter: 1287 loss: 2.56079193e-06
Iter: 1288 loss: 2.56001113e-06
Iter: 1289 loss: 2.56772341e-06
Iter: 1290 loss: 2.5599802e-06
Iter: 1291 loss: 2.55929126e-06
Iter: 1292 loss: 2.56275825e-06
Iter: 1293 loss: 2.55926193e-06
Iter: 1294 loss: 2.55834721e-06
Iter: 1295 loss: 2.55682562e-06
Iter: 1296 loss: 2.58357659e-06
Iter: 1297 loss: 2.5567615e-06
Iter: 1298 loss: 2.55551959e-06
Iter: 1299 loss: 2.56430371e-06
Iter: 1300 loss: 2.55536679e-06
Iter: 1301 loss: 2.55438476e-06
Iter: 1302 loss: 2.55317718e-06
Iter: 1303 loss: 2.55303235e-06
Iter: 1304 loss: 2.55131044e-06
Iter: 1305 loss: 2.56588874e-06
Iter: 1306 loss: 2.55117061e-06
Iter: 1307 loss: 2.54999941e-06
Iter: 1308 loss: 2.54723591e-06
Iter: 1309 loss: 2.58331465e-06
Iter: 1310 loss: 2.54709562e-06
Iter: 1311 loss: 2.54898896e-06
Iter: 1312 loss: 2.54649103e-06
Iter: 1313 loss: 2.54568226e-06
Iter: 1314 loss: 2.5440745e-06
Iter: 1315 loss: 2.57489478e-06
Iter: 1316 loss: 2.54411839e-06
Iter: 1317 loss: 2.54288943e-06
Iter: 1318 loss: 2.54778706e-06
Iter: 1319 loss: 2.5428385e-06
Iter: 1320 loss: 2.54204087e-06
Iter: 1321 loss: 2.540585e-06
Iter: 1322 loss: 2.56543035e-06
Iter: 1323 loss: 2.54049337e-06
Iter: 1324 loss: 2.53836834e-06
Iter: 1325 loss: 2.54628321e-06
Iter: 1326 loss: 2.53781241e-06
Iter: 1327 loss: 2.53723579e-06
Iter: 1328 loss: 2.53719372e-06
Iter: 1329 loss: 2.53668918e-06
Iter: 1330 loss: 2.53624785e-06
Iter: 1331 loss: 2.53610165e-06
Iter: 1332 loss: 2.53488906e-06
Iter: 1333 loss: 2.53568192e-06
Iter: 1334 loss: 2.53407643e-06
Iter: 1335 loss: 2.53257667e-06
Iter: 1336 loss: 2.54728593e-06
Iter: 1337 loss: 2.53254029e-06
Iter: 1338 loss: 2.53189842e-06
Iter: 1339 loss: 2.53023541e-06
Iter: 1340 loss: 2.55960231e-06
Iter: 1341 loss: 2.53022108e-06
Iter: 1342 loss: 2.52890982e-06
Iter: 1343 loss: 2.52877635e-06
Iter: 1344 loss: 2.52785185e-06
Iter: 1345 loss: 2.52746509e-06
Iter: 1346 loss: 2.52697555e-06
Iter: 1347 loss: 2.52551854e-06
Iter: 1348 loss: 2.5300144e-06
Iter: 1349 loss: 2.52493624e-06
Iter: 1350 loss: 2.52454515e-06
Iter: 1351 loss: 2.52430482e-06
Iter: 1352 loss: 2.52385325e-06
Iter: 1353 loss: 2.52244854e-06
Iter: 1354 loss: 2.51976689e-06
Iter: 1355 loss: 2.51977235e-06
Iter: 1356 loss: 2.53009694e-06
Iter: 1357 loss: 2.51902111e-06
Iter: 1358 loss: 2.51850224e-06
Iter: 1359 loss: 2.51871506e-06
Iter: 1360 loss: 2.51818938e-06
Iter: 1361 loss: 2.51744041e-06
Iter: 1362 loss: 2.51850702e-06
Iter: 1363 loss: 2.51707093e-06
Iter: 1364 loss: 2.51610936e-06
Iter: 1365 loss: 2.51658093e-06
Iter: 1366 loss: 2.51550091e-06
Iter: 1367 loss: 2.51429674e-06
Iter: 1368 loss: 2.51633742e-06
Iter: 1369 loss: 2.51382926e-06
Iter: 1370 loss: 2.51290248e-06
Iter: 1371 loss: 2.52017139e-06
Iter: 1372 loss: 2.51279698e-06
Iter: 1373 loss: 2.5117115e-06
Iter: 1374 loss: 2.51057804e-06
Iter: 1375 loss: 2.51044048e-06
Iter: 1376 loss: 2.50934431e-06
Iter: 1377 loss: 2.52032214e-06
Iter: 1378 loss: 2.50935045e-06
Iter: 1379 loss: 2.50848166e-06
Iter: 1380 loss: 2.50673452e-06
Iter: 1381 loss: 2.5435229e-06
Iter: 1382 loss: 2.50675225e-06
Iter: 1383 loss: 2.50712628e-06
Iter: 1384 loss: 2.5059378e-06
Iter: 1385 loss: 2.50540143e-06
Iter: 1386 loss: 2.50422363e-06
Iter: 1387 loss: 2.51226902e-06
Iter: 1388 loss: 2.50394396e-06
Iter: 1389 loss: 2.50289122e-06
Iter: 1390 loss: 2.51354891e-06
Iter: 1391 loss: 2.50288485e-06
Iter: 1392 loss: 2.50216021e-06
Iter: 1393 loss: 2.50217977e-06
Iter: 1394 loss: 2.50175071e-06
Iter: 1395 loss: 2.50130233e-06
Iter: 1396 loss: 2.50117682e-06
Iter: 1397 loss: 2.50065159e-06
Iter: 1398 loss: 2.5013469e-06
Iter: 1399 loss: 2.50036737e-06
Iter: 1400 loss: 2.49950858e-06
Iter: 1401 loss: 2.50144058e-06
Iter: 1402 loss: 2.49908317e-06
Iter: 1403 loss: 2.49814707e-06
Iter: 1404 loss: 2.50004405e-06
Iter: 1405 loss: 2.4977594e-06
Iter: 1406 loss: 2.4966489e-06
Iter: 1407 loss: 2.49959567e-06
Iter: 1408 loss: 2.49621667e-06
Iter: 1409 loss: 2.49540767e-06
Iter: 1410 loss: 2.49426034e-06
Iter: 1411 loss: 2.49417644e-06
Iter: 1412 loss: 2.49331742e-06
Iter: 1413 loss: 2.49328423e-06
Iter: 1414 loss: 2.49283676e-06
Iter: 1415 loss: 2.49145364e-06
Iter: 1416 loss: 2.50268931e-06
Iter: 1417 loss: 2.49120603e-06
Iter: 1418 loss: 2.49021559e-06
Iter: 1419 loss: 2.49019217e-06
Iter: 1420 loss: 2.48976175e-06
Iter: 1421 loss: 2.48973083e-06
Iter: 1422 loss: 2.48942706e-06
Iter: 1423 loss: 2.48850802e-06
Iter: 1424 loss: 2.49526784e-06
Iter: 1425 loss: 2.48836545e-06
Iter: 1426 loss: 2.4877927e-06
Iter: 1427 loss: 2.48763286e-06
Iter: 1428 loss: 2.48716742e-06
Iter: 1429 loss: 2.4883193e-06
Iter: 1430 loss: 2.48695596e-06
Iter: 1431 loss: 2.486531e-06
Iter: 1432 loss: 2.48573861e-06
Iter: 1433 loss: 2.48576225e-06
Iter: 1434 loss: 2.48499191e-06
Iter: 1435 loss: 2.49001187e-06
Iter: 1436 loss: 2.48493484e-06
Iter: 1437 loss: 2.48409333e-06
Iter: 1438 loss: 2.48484594e-06
Iter: 1439 loss: 2.48375477e-06
Iter: 1440 loss: 2.48302649e-06
Iter: 1441 loss: 2.48666265e-06
Iter: 1442 loss: 2.48290917e-06
Iter: 1443 loss: 2.48195624e-06
Iter: 1444 loss: 2.48093147e-06
Iter: 1445 loss: 2.4807905e-06
Iter: 1446 loss: 2.47988783e-06
Iter: 1447 loss: 2.48458082e-06
Iter: 1448 loss: 2.47970729e-06
Iter: 1449 loss: 2.47851494e-06
Iter: 1450 loss: 2.47794287e-06
Iter: 1451 loss: 2.47729895e-06
Iter: 1452 loss: 2.4773442e-06
Iter: 1453 loss: 2.477002e-06
Iter: 1454 loss: 2.47652315e-06
Iter: 1455 loss: 2.47531216e-06
Iter: 1456 loss: 2.48465e-06
Iter: 1457 loss: 2.47495e-06
Iter: 1458 loss: 2.47438516e-06
Iter: 1459 loss: 2.47432149e-06
Iter: 1460 loss: 2.47348339e-06
Iter: 1461 loss: 2.48050492e-06
Iter: 1462 loss: 2.47347e-06
Iter: 1463 loss: 2.47310322e-06
Iter: 1464 loss: 2.47224534e-06
Iter: 1465 loss: 2.4722151e-06
Iter: 1466 loss: 2.47147136e-06
Iter: 1467 loss: 2.47237313e-06
Iter: 1468 loss: 2.47093294e-06
Iter: 1469 loss: 2.46996387e-06
Iter: 1470 loss: 2.47741764e-06
Iter: 1471 loss: 2.46986701e-06
Iter: 1472 loss: 2.46917853e-06
Iter: 1473 loss: 2.46969034e-06
Iter: 1474 loss: 2.46883883e-06
Iter: 1475 loss: 2.46790751e-06
Iter: 1476 loss: 2.47279058e-06
Iter: 1477 loss: 2.4676965e-06
Iter: 1478 loss: 2.46709169e-06
Iter: 1479 loss: 2.46588206e-06
Iter: 1480 loss: 2.46578202e-06
Iter: 1481 loss: 2.46473837e-06
Iter: 1482 loss: 2.47438174e-06
Iter: 1483 loss: 2.46476588e-06
Iter: 1484 loss: 2.46368927e-06
Iter: 1485 loss: 2.46347804e-06
Iter: 1486 loss: 2.46277091e-06
Iter: 1487 loss: 2.46274294e-06
Iter: 1488 loss: 2.46218087e-06
Iter: 1489 loss: 2.46191939e-06
Iter: 1490 loss: 2.46105492e-06
Iter: 1491 loss: 2.46144509e-06
Iter: 1492 loss: 2.46028208e-06
Iter: 1493 loss: 2.46302898e-06
Iter: 1494 loss: 2.46001787e-06
Iter: 1495 loss: 2.45977571e-06
Iter: 1496 loss: 2.45946626e-06
Iter: 1497 loss: 2.45946603e-06
Iter: 1498 loss: 2.45865363e-06
Iter: 1499 loss: 2.45802539e-06
Iter: 1500 loss: 2.45778574e-06
Iter: 1501 loss: 2.45684123e-06
Iter: 1502 loss: 2.45673937e-06
Iter: 1503 loss: 2.45618912e-06
Iter: 1504 loss: 2.45743786e-06
Iter: 1505 loss: 2.45599858e-06
Iter: 1506 loss: 2.45546562e-06
Iter: 1507 loss: 2.45661158e-06
Iter: 1508 loss: 2.45534238e-06
Iter: 1509 loss: 2.45459842e-06
Iter: 1510 loss: 2.45599449e-06
Iter: 1511 loss: 2.45416413e-06
Iter: 1512 loss: 2.45381102e-06
Iter: 1513 loss: 2.45298429e-06
Iter: 1514 loss: 2.46813534e-06
Iter: 1515 loss: 2.45289107e-06
Iter: 1516 loss: 2.45186948e-06
Iter: 1517 loss: 2.46011177e-06
Iter: 1518 loss: 2.45176466e-06
Iter: 1519 loss: 2.45119918e-06
Iter: 1520 loss: 2.45112619e-06
Iter: 1521 loss: 2.45047886e-06
Iter: 1522 loss: 2.44970124e-06
Iter: 1523 loss: 2.44961257e-06
Iter: 1524 loss: 2.44899525e-06
Iter: 1525 loss: 2.45166098e-06
Iter: 1526 loss: 2.44883108e-06
Iter: 1527 loss: 2.4482556e-06
Iter: 1528 loss: 2.45027218e-06
Iter: 1529 loss: 2.44801345e-06
Iter: 1530 loss: 2.44718376e-06
Iter: 1531 loss: 2.44785747e-06
Iter: 1532 loss: 2.44657258e-06
Iter: 1533 loss: 2.44614307e-06
Iter: 1534 loss: 2.44571675e-06
Iter: 1535 loss: 2.44567309e-06
Iter: 1536 loss: 2.4446158e-06
Iter: 1537 loss: 2.44429066e-06
Iter: 1538 loss: 2.44376474e-06
Iter: 1539 loss: 2.44291823e-06
Iter: 1540 loss: 2.4429396e-06
Iter: 1541 loss: 2.44233979e-06
Iter: 1542 loss: 2.44258786e-06
Iter: 1543 loss: 2.44193416e-06
Iter: 1544 loss: 2.4407841e-06
Iter: 1545 loss: 2.43991281e-06
Iter: 1546 loss: 2.43956447e-06
Iter: 1547 loss: 2.43881277e-06
Iter: 1548 loss: 2.4411188e-06
Iter: 1549 loss: 2.4384924e-06
Iter: 1550 loss: 2.43745376e-06
Iter: 1551 loss: 2.43918544e-06
Iter: 1552 loss: 2.43705108e-06
Iter: 1553 loss: 2.43701948e-06
Iter: 1554 loss: 2.43670729e-06
Iter: 1555 loss: 2.43639101e-06
Iter: 1556 loss: 2.43556156e-06
Iter: 1557 loss: 2.43998466e-06
Iter: 1558 loss: 2.43526665e-06
Iter: 1559 loss: 2.43412569e-06
Iter: 1560 loss: 2.43953491e-06
Iter: 1561 loss: 2.43405316e-06
Iter: 1562 loss: 2.4333176e-06
Iter: 1563 loss: 2.43486488e-06
Iter: 1564 loss: 2.4330443e-06
Iter: 1565 loss: 2.43270051e-06
Iter: 1566 loss: 2.43259092e-06
Iter: 1567 loss: 2.43242175e-06
Iter: 1568 loss: 2.43204795e-06
Iter: 1569 loss: 2.43453155e-06
Iter: 1570 loss: 2.43168142e-06
Iter: 1571 loss: 2.43074464e-06
Iter: 1572 loss: 2.4339497e-06
Iter: 1573 loss: 2.43048271e-06
Iter: 1574 loss: 2.43002705e-06
Iter: 1575 loss: 2.43319937e-06
Iter: 1576 loss: 2.43000181e-06
Iter: 1577 loss: 2.42957276e-06
Iter: 1578 loss: 2.42911779e-06
Iter: 1579 loss: 2.42904866e-06
Iter: 1580 loss: 2.42832562e-06
Iter: 1581 loss: 2.43222075e-06
Iter: 1582 loss: 2.42817305e-06
Iter: 1583 loss: 2.42774149e-06
Iter: 1584 loss: 2.42677561e-06
Iter: 1585 loss: 2.43700765e-06
Iter: 1586 loss: 2.42661372e-06
Iter: 1587 loss: 2.42629221e-06
Iter: 1588 loss: 2.42603392e-06
Iter: 1589 loss: 2.42537317e-06
Iter: 1590 loss: 2.42482247e-06
Iter: 1591 loss: 2.42455826e-06
Iter: 1592 loss: 2.42375336e-06
Iter: 1593 loss: 2.42655096e-06
Iter: 1594 loss: 2.4234937e-06
Iter: 1595 loss: 2.42272608e-06
Iter: 1596 loss: 2.42260899e-06
Iter: 1597 loss: 2.42205579e-06
Iter: 1598 loss: 2.42251826e-06
Iter: 1599 loss: 2.42166789e-06
Iter: 1600 loss: 2.42140527e-06
Iter: 1601 loss: 2.42075316e-06
Iter: 1602 loss: 2.42996657e-06
Iter: 1603 loss: 2.42071656e-06
Iter: 1604 loss: 2.42002511e-06
Iter: 1605 loss: 2.42183796e-06
Iter: 1606 loss: 2.41978569e-06
Iter: 1607 loss: 2.41915313e-06
Iter: 1608 loss: 2.42081023e-06
Iter: 1609 loss: 2.41895236e-06
Iter: 1610 loss: 2.4182616e-06
Iter: 1611 loss: 2.41764019e-06
Iter: 1612 loss: 2.41735e-06
Iter: 1613 loss: 2.41598013e-06
Iter: 1614 loss: 2.43082263e-06
Iter: 1615 loss: 2.41599764e-06
Iter: 1616 loss: 2.41523799e-06
Iter: 1617 loss: 2.4153328e-06
Iter: 1618 loss: 2.41473526e-06
Iter: 1619 loss: 2.41353064e-06
Iter: 1620 loss: 2.41317866e-06
Iter: 1621 loss: 2.41244697e-06
Iter: 1622 loss: 2.41204361e-06
Iter: 1623 loss: 2.41160706e-06
Iter: 1624 loss: 2.41129237e-06
Iter: 1625 loss: 2.41018756e-06
Iter: 1626 loss: 2.41727253e-06
Iter: 1627 loss: 2.40973168e-06
Iter: 1628 loss: 2.40814393e-06
Iter: 1629 loss: 2.41583712e-06
Iter: 1630 loss: 2.40784084e-06
Iter: 1631 loss: 2.40746749e-06
Iter: 1632 loss: 2.40736176e-06
Iter: 1633 loss: 2.40657619e-06
Iter: 1634 loss: 2.4053536e-06
Iter: 1635 loss: 2.40529494e-06
Iter: 1636 loss: 2.40427494e-06
Iter: 1637 loss: 2.41050475e-06
Iter: 1638 loss: 2.40420945e-06
Iter: 1639 loss: 2.4033593e-06
Iter: 1640 loss: 2.40265763e-06
Iter: 1641 loss: 2.40247368e-06
Iter: 1642 loss: 2.40096301e-06
Iter: 1643 loss: 2.40584359e-06
Iter: 1644 loss: 2.40053669e-06
Iter: 1645 loss: 2.39965561e-06
Iter: 1646 loss: 2.41364e-06
Iter: 1647 loss: 2.39959e-06
Iter: 1648 loss: 2.39903534e-06
Iter: 1649 loss: 2.39740689e-06
Iter: 1650 loss: 2.4185731e-06
Iter: 1651 loss: 2.39733276e-06
Iter: 1652 loss: 2.39625683e-06
Iter: 1653 loss: 2.39626888e-06
Iter: 1654 loss: 2.39566134e-06
Iter: 1655 loss: 2.39707106e-06
Iter: 1656 loss: 2.39548399e-06
Iter: 1657 loss: 2.39433712e-06
Iter: 1658 loss: 2.39327846e-06
Iter: 1659 loss: 2.39305655e-06
Iter: 1660 loss: 2.39207839e-06
Iter: 1661 loss: 2.39460269e-06
Iter: 1662 loss: 2.39169367e-06
Iter: 1663 loss: 2.39075189e-06
Iter: 1664 loss: 2.39618817e-06
Iter: 1665 loss: 2.3905834e-06
Iter: 1666 loss: 2.38930079e-06
Iter: 1667 loss: 2.39420297e-06
Iter: 1668 loss: 2.38906023e-06
Iter: 1669 loss: 2.38849771e-06
Iter: 1670 loss: 2.3882144e-06
Iter: 1671 loss: 2.38790972e-06
Iter: 1672 loss: 2.3866537e-06
Iter: 1673 loss: 2.38751227e-06
Iter: 1674 loss: 2.38587359e-06
Iter: 1675 loss: 2.38484085e-06
Iter: 1676 loss: 2.39093492e-06
Iter: 1677 loss: 2.38471239e-06
Iter: 1678 loss: 2.38389066e-06
Iter: 1679 loss: 2.38758093e-06
Iter: 1680 loss: 2.38379425e-06
Iter: 1681 loss: 2.38277426e-06
Iter: 1682 loss: 2.3829798e-06
Iter: 1683 loss: 2.38198731e-06
Iter: 1684 loss: 2.38130383e-06
Iter: 1685 loss: 2.38436905e-06
Iter: 1686 loss: 2.38117809e-06
Iter: 1687 loss: 2.38048733e-06
Iter: 1688 loss: 2.38225221e-06
Iter: 1689 loss: 2.38009e-06
Iter: 1690 loss: 2.37927861e-06
Iter: 1691 loss: 2.3849359e-06
Iter: 1692 loss: 2.37923e-06
Iter: 1693 loss: 2.3787004e-06
Iter: 1694 loss: 2.37788709e-06
Iter: 1695 loss: 2.39030669e-06
Iter: 1696 loss: 2.37782683e-06
Iter: 1697 loss: 2.37694735e-06
Iter: 1698 loss: 2.3878415e-06
Iter: 1699 loss: 2.3768348e-06
Iter: 1700 loss: 2.37631775e-06
Iter: 1701 loss: 2.37639779e-06
Iter: 1702 loss: 2.37596851e-06
Iter: 1703 loss: 2.37522886e-06
Iter: 1704 loss: 2.3792179e-06
Iter: 1705 loss: 2.37493578e-06
Iter: 1706 loss: 2.37372842e-06
Iter: 1707 loss: 2.38462985e-06
Iter: 1708 loss: 2.3737191e-06
Iter: 1709 loss: 2.37309837e-06
Iter: 1710 loss: 2.37355516e-06
Iter: 1711 loss: 2.37272866e-06
Iter: 1712 loss: 2.37188669e-06
Iter: 1713 loss: 2.37566292e-06
Iter: 1714 loss: 2.37173936e-06
Iter: 1715 loss: 2.37080349e-06
Iter: 1716 loss: 2.372484e-06
Iter: 1717 loss: 2.37051881e-06
Iter: 1718 loss: 2.36996721e-06
Iter: 1719 loss: 2.37024733e-06
Iter: 1720 loss: 2.36946971e-06
Iter: 1721 loss: 2.36863207e-06
Iter: 1722 loss: 2.37576751e-06
Iter: 1723 loss: 2.36863389e-06
Iter: 1724 loss: 2.36813548e-06
Iter: 1725 loss: 2.3712264e-06
Iter: 1726 loss: 2.36815094e-06
Iter: 1727 loss: 2.36785036e-06
Iter: 1728 loss: 2.36676783e-06
Iter: 1729 loss: 2.37294535e-06
Iter: 1730 loss: 2.36632695e-06
Iter: 1731 loss: 2.36584378e-06
Iter: 1732 loss: 2.3656844e-06
Iter: 1733 loss: 2.36548613e-06
Iter: 1734 loss: 2.36539177e-06
Iter: 1735 loss: 2.36509891e-06
Iter: 1736 loss: 2.36420692e-06
Iter: 1737 loss: 2.36668757e-06
Iter: 1738 loss: 2.36365349e-06
Iter: 1739 loss: 2.36309256e-06
Iter: 1740 loss: 2.36292271e-06
Iter: 1741 loss: 2.36239111e-06
Iter: 1742 loss: 2.36177857e-06
Iter: 1743 loss: 2.36165738e-06
Iter: 1744 loss: 2.36107621e-06
Iter: 1745 loss: 2.36995538e-06
Iter: 1746 loss: 2.36104552e-06
Iter: 1747 loss: 2.36045526e-06
Iter: 1748 loss: 2.36281539e-06
Iter: 1749 loss: 2.36032906e-06
Iter: 1750 loss: 2.3599905e-06
Iter: 1751 loss: 2.35928746e-06
Iter: 1752 loss: 2.35933067e-06
Iter: 1753 loss: 2.35847938e-06
Iter: 1754 loss: 2.36870164e-06
Iter: 1755 loss: 2.35844891e-06
Iter: 1756 loss: 2.35793732e-06
Iter: 1757 loss: 2.35864422e-06
Iter: 1758 loss: 2.35768084e-06
Iter: 1759 loss: 2.3568025e-06
Iter: 1760 loss: 2.35660832e-06
Iter: 1761 loss: 2.35612356e-06
Iter: 1762 loss: 2.3555267e-06
Iter: 1763 loss: 2.35552898e-06
Iter: 1764 loss: 2.35500625e-06
Iter: 1765 loss: 2.35456059e-06
Iter: 1766 loss: 2.35438029e-06
Iter: 1767 loss: 2.35387324e-06
Iter: 1768 loss: 2.35308153e-06
Iter: 1769 loss: 2.35305788e-06
Iter: 1770 loss: 2.35268976e-06
Iter: 1771 loss: 2.3533978e-06
Iter: 1772 loss: 2.35227299e-06
Iter: 1773 loss: 2.3513935e-06
Iter: 1774 loss: 2.3521261e-06
Iter: 1775 loss: 2.35094535e-06
Iter: 1776 loss: 2.35019388e-06
Iter: 1777 loss: 2.35702169e-06
Iter: 1778 loss: 2.35019161e-06
Iter: 1779 loss: 2.3496591e-06
Iter: 1780 loss: 2.35071093e-06
Iter: 1781 loss: 2.3493933e-06
Iter: 1782 loss: 2.34868344e-06
Iter: 1783 loss: 2.34844538e-06
Iter: 1784 loss: 2.34801246e-06
Iter: 1785 loss: 2.34744675e-06
Iter: 1786 loss: 2.34742333e-06
Iter: 1787 loss: 2.34699633e-06
Iter: 1788 loss: 2.34720028e-06
Iter: 1789 loss: 2.34669778e-06
Iter: 1790 loss: 2.34591653e-06
Iter: 1791 loss: 2.34740105e-06
Iter: 1792 loss: 2.34557069e-06
Iter: 1793 loss: 2.34502977e-06
Iter: 1794 loss: 2.34394793e-06
Iter: 1795 loss: 2.36611481e-06
Iter: 1796 loss: 2.3439884e-06
Iter: 1797 loss: 2.34542335e-06
Iter: 1798 loss: 2.34374147e-06
Iter: 1799 loss: 2.34339495e-06
Iter: 1800 loss: 2.34286153e-06
Iter: 1801 loss: 2.34294976e-06
Iter: 1802 loss: 2.34234449e-06
Iter: 1803 loss: 2.34123286e-06
Iter: 1804 loss: 2.36104256e-06
Iter: 1805 loss: 2.34116442e-06
Iter: 1806 loss: 2.340425e-06
Iter: 1807 loss: 2.34039726e-06
Iter: 1808 loss: 2.33985929e-06
Iter: 1809 loss: 2.34144704e-06
Iter: 1810 loss: 2.33968558e-06
Iter: 1811 loss: 2.33921264e-06
Iter: 1812 loss: 2.3379298e-06
Iter: 1813 loss: 2.35325206e-06
Iter: 1814 loss: 2.33787068e-06
Iter: 1815 loss: 2.33655646e-06
Iter: 1816 loss: 2.34656954e-06
Iter: 1817 loss: 2.33644778e-06
Iter: 1818 loss: 2.33557103e-06
Iter: 1819 loss: 2.34114486e-06
Iter: 1820 loss: 2.33546598e-06
Iter: 1821 loss: 2.33457035e-06
Iter: 1822 loss: 2.33672654e-06
Iter: 1823 loss: 2.33432456e-06
Iter: 1824 loss: 2.3335781e-06
Iter: 1825 loss: 2.33835749e-06
Iter: 1826 loss: 2.33347646e-06
Iter: 1827 loss: 2.33296123e-06
Iter: 1828 loss: 2.33186279e-06
Iter: 1829 loss: 2.3450782e-06
Iter: 1830 loss: 2.331735e-06
Iter: 1831 loss: 2.33103765e-06
Iter: 1832 loss: 2.33097171e-06
Iter: 1833 loss: 2.33018727e-06
Iter: 1834 loss: 2.33216906e-06
Iter: 1835 loss: 2.32986827e-06
Iter: 1836 loss: 2.32920456e-06
Iter: 1837 loss: 2.32907132e-06
Iter: 1838 loss: 2.32874527e-06
Iter: 1839 loss: 2.32796037e-06
Iter: 1840 loss: 2.33051333e-06
Iter: 1841 loss: 2.32777938e-06
Iter: 1842 loss: 2.32685397e-06
Iter: 1843 loss: 2.32751518e-06
Iter: 1844 loss: 2.32621596e-06
Iter: 1845 loss: 2.32535695e-06
Iter: 1846 loss: 2.33504579e-06
Iter: 1847 loss: 2.32548268e-06
Iter: 1848 loss: 2.32492403e-06
Iter: 1849 loss: 2.32437947e-06
Iter: 1850 loss: 2.32425964e-06
Iter: 1851 loss: 2.32330717e-06
Iter: 1852 loss: 2.32510456e-06
Iter: 1853 loss: 2.32289358e-06
Iter: 1854 loss: 2.32258753e-06
Iter: 1855 loss: 2.32243428e-06
Iter: 1856 loss: 2.32223761e-06
Iter: 1857 loss: 2.32181628e-06
Iter: 1858 loss: 2.32178058e-06
Iter: 1859 loss: 2.32116781e-06
Iter: 1860 loss: 2.32109483e-06
Iter: 1861 loss: 2.32058142e-06
Iter: 1862 loss: 2.31992226e-06
Iter: 1863 loss: 2.31888316e-06
Iter: 1864 loss: 2.31889089e-06
Iter: 1865 loss: 2.32056163e-06
Iter: 1866 loss: 2.31854688e-06
Iter: 1867 loss: 2.31827335e-06
Iter: 1868 loss: 2.317596e-06
Iter: 1869 loss: 2.32121329e-06
Iter: 1870 loss: 2.31729655e-06
Iter: 1871 loss: 2.31631384e-06
Iter: 1872 loss: 2.31991362e-06
Iter: 1873 loss: 2.31614536e-06
Iter: 1874 loss: 2.31548779e-06
Iter: 1875 loss: 2.32342722e-06
Iter: 1876 loss: 2.3154314e-06
Iter: 1877 loss: 2.31494892e-06
Iter: 1878 loss: 2.31486592e-06
Iter: 1879 loss: 2.31449053e-06
Iter: 1880 loss: 2.31387185e-06
Iter: 1881 loss: 2.32000866e-06
Iter: 1882 loss: 2.31376498e-06
Iter: 1883 loss: 2.31338959e-06
Iter: 1884 loss: 2.31299259e-06
Iter: 1885 loss: 2.31289255e-06
Iter: 1886 loss: 2.31217518e-06
Iter: 1887 loss: 2.31448576e-06
Iter: 1888 loss: 2.31199192e-06
Iter: 1889 loss: 2.31103695e-06
Iter: 1890 loss: 2.31776494e-06
Iter: 1891 loss: 2.31106151e-06
Iter: 1892 loss: 2.31065496e-06
Iter: 1893 loss: 2.31104968e-06
Iter: 1894 loss: 2.31040394e-06
Iter: 1895 loss: 2.30973524e-06
Iter: 1896 loss: 2.30838191e-06
Iter: 1897 loss: 2.33281844e-06
Iter: 1898 loss: 2.30834235e-06
Iter: 1899 loss: 2.30712158e-06
Iter: 1900 loss: 2.30835121e-06
Iter: 1901 loss: 2.3065727e-06
Iter: 1902 loss: 2.30805381e-06
Iter: 1903 loss: 2.30598062e-06
Iter: 1904 loss: 2.305831e-06
Iter: 1905 loss: 2.30531441e-06
Iter: 1906 loss: 2.30556475e-06
Iter: 1907 loss: 2.3047287e-06
Iter: 1908 loss: 2.3037046e-06
Iter: 1909 loss: 2.3124278e-06
Iter: 1910 loss: 2.30366959e-06
Iter: 1911 loss: 2.30303704e-06
Iter: 1912 loss: 2.30866635e-06
Iter: 1913 loss: 2.30307023e-06
Iter: 1914 loss: 2.3025591e-06
Iter: 1915 loss: 2.30190835e-06
Iter: 1916 loss: 2.30183787e-06
Iter: 1917 loss: 2.30110572e-06
Iter: 1918 loss: 2.30113892e-06
Iter: 1919 loss: 2.30062142e-06
Iter: 1920 loss: 2.29975853e-06
Iter: 1921 loss: 2.31474587e-06
Iter: 1922 loss: 2.29982902e-06
Iter: 1923 loss: 2.29889929e-06
Iter: 1924 loss: 2.30759156e-06
Iter: 1925 loss: 2.29890907e-06
Iter: 1926 loss: 2.29842817e-06
Iter: 1927 loss: 2.30354863e-06
Iter: 1928 loss: 2.29833449e-06
Iter: 1929 loss: 2.29800867e-06
Iter: 1930 loss: 2.29735633e-06
Iter: 1931 loss: 2.30894875e-06
Iter: 1932 loss: 2.29726948e-06
Iter: 1933 loss: 2.29622674e-06
Iter: 1934 loss: 2.30057822e-06
Iter: 1935 loss: 2.29599095e-06
Iter: 1936 loss: 2.29555508e-06
Iter: 1937 loss: 2.29551642e-06
Iter: 1938 loss: 2.29519742e-06
Iter: 1939 loss: 2.29425086e-06
Iter: 1940 loss: 2.30428236e-06
Iter: 1941 loss: 2.29429406e-06
Iter: 1942 loss: 2.29406169e-06
Iter: 1943 loss: 2.29346983e-06
Iter: 1944 loss: 2.29278612e-06
Iter: 1945 loss: 2.29264015e-06
Iter: 1946 loss: 2.29207444e-06
Iter: 1947 loss: 2.29189754e-06
Iter: 1948 loss: 2.29122497e-06
Iter: 1949 loss: 2.29216266e-06
Iter: 1950 loss: 2.29088209e-06
Iter: 1951 loss: 2.29008901e-06
Iter: 1952 loss: 2.29038733e-06
Iter: 1953 loss: 2.28959425e-06
Iter: 1954 loss: 2.28897693e-06
Iter: 1955 loss: 2.28893441e-06
Iter: 1956 loss: 2.28842691e-06
Iter: 1957 loss: 2.28786939e-06
Iter: 1958 loss: 2.28765907e-06
Iter: 1959 loss: 2.28694807e-06
Iter: 1960 loss: 2.29194211e-06
Iter: 1961 loss: 2.28691215e-06
Iter: 1962 loss: 2.28644922e-06
Iter: 1963 loss: 2.28922181e-06
Iter: 1964 loss: 2.28625595e-06
Iter: 1965 loss: 2.28556905e-06
Iter: 1966 loss: 2.28494719e-06
Iter: 1967 loss: 2.2847255e-06
Iter: 1968 loss: 2.28405e-06
Iter: 1969 loss: 2.28520184e-06
Iter: 1970 loss: 2.28368162e-06
Iter: 1971 loss: 2.28260637e-06
Iter: 1972 loss: 2.28452518e-06
Iter: 1973 loss: 2.28220779e-06
Iter: 1974 loss: 2.28221052e-06
Iter: 1975 loss: 2.28184263e-06
Iter: 1976 loss: 2.28141653e-06
Iter: 1977 loss: 2.28023782e-06
Iter: 1978 loss: 2.28515296e-06
Iter: 1979 loss: 2.27967894e-06
Iter: 1980 loss: 2.27876444e-06
Iter: 1981 loss: 2.27753503e-06
Iter: 1982 loss: 2.27748887e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.2/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.6 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi1.6
+ date
Sun Nov  8 17:02:06 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.6/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.6/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 1.6 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.6/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fddabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fea5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fd507b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fe94840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fdfce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fda4b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fd087b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fd080d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fda17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fccfae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fc2e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fc9b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fc9ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fbb91e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fbb6620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fc9b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fba6d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0737914ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fccf488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0737914048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fb4b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07378ec268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fb4ba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07378b0c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07378b0f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073782eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07377cfd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073782e2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073787c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073786c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073786d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fc4d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fc4d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f073fc54ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f07377691e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0737648c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 1.998633
test_loss: 1.998452
train_loss: 1.9965433
test_loss: 1.9979308
train_loss: 1.9960592
test_loss: 2.0500638
train_loss: 1.9953499
test_loss: 1.9987673
train_loss: 1.9960108
test_loss: 1.9986805
train_loss: 1.9975679
test_loss: 1.9985403
train_loss: 1.9991276
test_loss: 1.9989731
train_loss: 2.0052404
test_loss: 2.006364
train_loss: 1.9985421
test_loss: 1.9982796
train_loss: 1.9956007
test_loss: 2.0012972
train_loss: 1.9978731
test_loss: 2.0017343
train_loss: 1.9972674
test_loss: 2.1885815
train_loss: 2.001655
test_loss: 2.025048
train_loss: 1.9956888
test_loss: 1.9984123
train_loss: 1.9983443
test_loss: 2.050519
train_loss: 2.0077984
test_loss: 2.0425532
train_loss: 1.9961774
test_loss: 2.0005493
train_loss: 1.9988669
test_loss: 2.002442
train_loss: 1.9935384
test_loss: 1.9986261
train_loss: 1.9977889
test_loss: 2.0011659
train_loss: 1.9953328
test_loss: 2.0001745
train_loss: 1.9982862
test_loss: 1.9982936
train_loss: 0.6244039
test_loss: 0.6228638
train_loss: 0.60525656
test_loss: 0.6038077
train_loss: 0.6146868
test_loss: 0.60329115
train_loss: 0.61557525
test_loss: 0.60269654
train_loss: 0.6004701
test_loss: 0.60209626
train_loss: 0.6075699
test_loss: 0.6013987
train_loss: 0.5997176
test_loss: 0.6007036
train_loss: 0.6014317
test_loss: 0.59998524
train_loss: 0.6037266
test_loss: 0.5991685
train_loss: 0.5939817
test_loss: 0.59836346
train_loss: 0.59049046
test_loss: 0.5974862
train_loss: 0.60117817
test_loss: 0.5965582
train_loss: 0.5935143
test_loss: 0.5956223
train_loss: 0.58513397
test_loss: 0.594672
train_loss: 0.60069126
test_loss: 0.593599
train_loss: 0.6002486
test_loss: 0.5924433
train_loss: 0.595321
test_loss: 0.5912874
train_loss: 0.58231354
test_loss: 0.59003836
train_loss: 0.5858921
test_loss: 0.5887023
train_loss: 0.58438367
test_loss: 0.5873682
train_loss: 0.5823122
test_loss: 0.58591336
train_loss: 0.5768196
test_loss: 0.58442855
train_loss: 0.59594995
test_loss: 0.5827989
train_loss: 0.570702
test_loss: 0.5809825
train_loss: 0.58413815
test_loss: 0.5792475
train_loss: 0.58363044
test_loss: 0.57741773
train_loss: 0.57563907
test_loss: 0.5754657
train_loss: 0.5766423
test_loss: 0.5733461
train_loss: 0.5735136
test_loss: 0.57115185
train_loss: 0.564019
test_loss: 0.56886053
train_loss: 0.5706148
test_loss: 0.5664622
train_loss: 0.5735185
test_loss: 0.5638588
train_loss: 0.56191325
test_loss: 0.5612112
train_loss: 0.56793505
test_loss: 0.55841136
train_loss: 0.54228306
test_loss: 0.5554344
train_loss: 0.55832106
test_loss: 0.55228025
train_loss: 0.5482426
test_loss: 0.5489676
train_loss: 0.5495631
test_loss: 0.545479
train_loss: 0.541025
test_loss: 0.54175293
train_loss: 0.5400116
test_loss: 0.5379758
train_loss: 0.5334794
test_loss: 0.53386915
train_loss: 0.5299883
test_loss: 0.529572
train_loss: 0.52008003
test_loss: 0.524996
train_loss: 0.5139618
test_loss: 0.5202534
train_loss: 0.5199343
test_loss: 0.51521784
train_loss: 0.5030961
test_loss: 0.5098382
train_loss: 0.51170266
test_loss: 0.50422287
train_loss: 0.5013186
test_loss: 0.49826565
train_loss: 0.49458224
test_loss: 0.4920077
train_loss: 0.47469294
test_loss: 0.48521376
train_loss: 0.4739022
test_loss: 0.47829747
train_loss: 0.4719712
test_loss: 0.47091627
train_loss: 0.4593897
test_loss: 0.46313617
train_loss: 0.45877147
test_loss: 0.45482
train_loss: 0.45376694
test_loss: 0.44607148
train_loss: 0.44651186
test_loss: 0.4367383
train_loss: 0.4355089
test_loss: 0.42697778
train_loss: 0.42254946
test_loss: 0.41659576
train_loss: 0.39848703
test_loss: 0.40564996
train_loss: 0.40050763
test_loss: 0.3941463
train_loss: 0.37497002
test_loss: 0.38202432
train_loss: 0.35817707
test_loss: 0.3692436
train_loss: 0.36249268
test_loss: 0.3558869
train_loss: 0.34727874
test_loss: 0.34192917
train_loss: 0.3303848
test_loss: 0.32748178
train_loss: 0.31999156
test_loss: 0.3126918
train_loss: 0.3025805
test_loss: 0.29758015
train_loss: 0.28624424
test_loss: 0.28243122
train_loss: 0.26508933
test_loss: 0.26728874
train_loss: 0.2577454
test_loss: 0.2523979
train_loss: 0.2373364
test_loss: 0.23782697
train_loss: 0.22958934
test_loss: 0.22374907
train_loss: 0.21249467
test_loss: 0.21032503
train_loss: 0.19503409
test_loss: 0.19759983
train_loss: 0.1850571
test_loss: 0.18556651
train_loss: 0.17115304
test_loss: 0.17421201
train_loss: 0.16029458
test_loss: 0.16349708
train_loss: 0.1527381
test_loss: 0.153368
train_loss: 0.1446783
test_loss: 0.14372896
train_loss: 0.13500464
test_loss: 0.13453183
train_loss: 0.12608461
test_loss: 0.12575597
train_loss: 0.11708344
test_loss: 0.11730253
train_loss: 0.10795726
test_loss: 0.10916064
train_loss: 0.10018198
test_loss: 0.101286665
train_loss: 0.09341987
test_loss: 0.093730405
train_loss: 0.08537453
test_loss: 0.086570196
train_loss: 0.07806104
test_loss: 0.08005335
train_loss: 0.07435007
test_loss: 0.07443224
train_loss: 0.06820649
test_loss: 0.06997849
train_loss: 0.0647428
test_loss: 0.066757865
train_loss: 0.06360029
test_loss: 0.064550884
train_loss: 0.06288516
test_loss: 0.06304226
train_loss: 0.06196857
test_loss: 0.06199521
train_loss: 0.06038864
test_loss: 0.061233044
train_loss: 0.060918495
test_loss: 0.060650043
train_loss: 0.0613772
test_loss: 0.0601709
train_loss: 0.057480086
test_loss: 0.059776563
train_loss: 0.06072011
test_loss: 0.05942248
train_loss: 0.060057536
test_loss: 0.05914769
train_loss: 0.059430696
test_loss: 0.05886621
train_loss: 0.058308654
test_loss: 0.05864164
train_loss: 0.05827873
test_loss: 0.05842933
train_loss: 0.058059413
test_loss: 0.058239516
train_loss: 0.05743669
test_loss: 0.058061857
train_loss: 0.057507884
test_loss: 0.057883296
train_loss: 0.05691094
test_loss: 0.057730295
train_loss: 0.057632513
test_loss: 0.05760574
train_loss: 0.057276677
test_loss: 0.057462838
train_loss: 0.057359178
test_loss: 0.057333235
train_loss: 0.05685453
test_loss: 0.05719245
train_loss: 0.054728195
test_loss: 0.057063513
train_loss: 0.05671607
test_loss: 0.056930978
train_loss: 0.057127856
test_loss: 0.056794427
train_loss: 0.05589562
test_loss: 0.056654368
train_loss: 0.057145208
test_loss: 0.05651105
train_loss: 0.055494312
test_loss: 0.0563946
train_loss: 0.055874802
test_loss: 0.056199532
train_loss: 0.056229692
test_loss: 0.05605593
train_loss: 0.054901198
test_loss: 0.05585137
train_loss: 0.05416821
test_loss: 0.05569109
train_loss: 0.055238932
test_loss: 0.055487126
train_loss: 0.05483754
test_loss: 0.05525335
train_loss: 0.055086907
test_loss: 0.054953106
train_loss: 0.056255307
test_loss: 0.054716922
train_loss: 0.055058524
test_loss: 0.054342516
train_loss: 0.053561673
test_loss: 0.053979594
train_loss: 0.05235944
test_loss: 0.053607605
train_loss: 0.051475734
test_loss: 0.053033628
train_loss: 0.051096547
test_loss: 0.052450232
train_loss: 0.051423192
test_loss: 0.05175225
train_loss: 0.050098903
test_loss: 0.050915934
train_loss: 0.04900292
test_loss: 0.05006003
train_loss: 0.048059396
test_loss: 0.048993304
train_loss: 0.046633944
test_loss: 0.04778919
train_loss: 0.044880725
test_loss: 0.046410378
train_loss: 0.044162273
test_loss: 0.044716015
train_loss: 0.042992294
test_loss: 0.04328116
train_loss: 0.039354205
test_loss: 0.041041333
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi1.6/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.6/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 0 --phi 1.6 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi1.6/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb629c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb67a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb702840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb661ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb5e9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb7022f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb5909d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb5711e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb5d07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb4f10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb4f1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb4f19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb4cfd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb4841e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb493f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb44b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f491db120d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb40d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48fb3c8ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48d6ef5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48d6f187b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48d6eb52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48d6ec16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48d6e7de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48d6e7dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48d6e69f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48d6e690d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48d6e1ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48b05621e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48b05850d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48b057d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48b05321e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48b053f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48b04db9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48b049e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f48b0476f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.00284224423
Iter: 2 loss: 0.0030052769
Iter: 3 loss: 0.00279327971
Iter: 4 loss: 0.00277526351
Iter: 5 loss: 0.00277526188
Iter: 6 loss: 0.00276520895
Iter: 7 loss: 0.00277813082
Iter: 8 loss: 0.00276003103
Iter: 9 loss: 0.00274834549
Iter: 10 loss: 0.00278397906
Iter: 11 loss: 0.00274488865
Iter: 12 loss: 0.00273211394
Iter: 13 loss: 0.00271944725
Iter: 14 loss: 0.00271680159
Iter: 15 loss: 0.0026905227
Iter: 16 loss: 0.00272062048
Iter: 17 loss: 0.002676534
Iter: 18 loss: 0.00263723172
Iter: 19 loss: 0.00272854231
Iter: 20 loss: 0.0026228244
Iter: 21 loss: 0.00257238327
Iter: 22 loss: 0.00255673099
Iter: 23 loss: 0.00252766581
Iter: 24 loss: 0.0024586143
Iter: 25 loss: 0.00248883199
Iter: 26 loss: 0.0024118165
Iter: 27 loss: 0.00234800717
Iter: 28 loss: 0.00234779436
Iter: 29 loss: 0.00230171694
Iter: 30 loss: 0.00228787633
Iter: 31 loss: 0.00226047449
Iter: 32 loss: 0.00220612506
Iter: 33 loss: 0.00226037251
Iter: 34 loss: 0.00217569945
Iter: 35 loss: 0.00211510016
Iter: 36 loss: 0.0022570272
Iter: 37 loss: 0.00209374679
Iter: 38 loss: 0.00206431188
Iter: 39 loss: 0.00206019729
Iter: 40 loss: 0.00202828413
Iter: 41 loss: 0.00201630546
Iter: 42 loss: 0.00199878542
Iter: 43 loss: 0.0019574631
Iter: 44 loss: 0.0022140448
Iter: 45 loss: 0.00195284432
Iter: 46 loss: 0.00191595708
Iter: 47 loss: 0.00202402472
Iter: 48 loss: 0.00190443371
Iter: 49 loss: 0.0018649539
Iter: 50 loss: 0.00186462887
Iter: 51 loss: 0.00183313177
Iter: 52 loss: 0.00177585799
Iter: 53 loss: 0.00194524752
Iter: 54 loss: 0.0017585489
Iter: 55 loss: 0.00170397176
Iter: 56 loss: 0.0019435354
Iter: 57 loss: 0.00169310882
Iter: 58 loss: 0.00163264247
Iter: 59 loss: 0.00183599046
Iter: 60 loss: 0.00161431974
Iter: 61 loss: 0.00156224868
Iter: 62 loss: 0.00189418113
Iter: 63 loss: 0.00155694387
Iter: 64 loss: 0.00150533835
Iter: 65 loss: 0.00167061808
Iter: 66 loss: 0.00149122381
Iter: 67 loss: 0.00144853094
Iter: 68 loss: 0.00142963417
Iter: 69 loss: 0.00140790688
Iter: 70 loss: 0.00135303801
Iter: 71 loss: 0.00180788757
Iter: 72 loss: 0.00134989351
Iter: 73 loss: 0.00131734973
Iter: 74 loss: 0.00131711969
Iter: 75 loss: 0.00128660665
Iter: 76 loss: 0.00131326518
Iter: 77 loss: 0.0012681951
Iter: 78 loss: 0.00123445934
Iter: 79 loss: 0.00144925085
Iter: 80 loss: 0.00123097934
Iter: 81 loss: 0.00119164272
Iter: 82 loss: 0.00133958203
Iter: 83 loss: 0.00118227769
Iter: 84 loss: 0.00115162763
Iter: 85 loss: 0.00154950272
Iter: 86 loss: 0.00115137175
Iter: 87 loss: 0.00112540065
Iter: 88 loss: 0.00110953255
Iter: 89 loss: 0.00109914271
Iter: 90 loss: 0.00106451649
Iter: 91 loss: 0.00138502277
Iter: 92 loss: 0.00106314127
Iter: 93 loss: 0.00103202509
Iter: 94 loss: 0.00122967898
Iter: 95 loss: 0.00102752051
Iter: 96 loss: 0.000998552889
Iter: 97 loss: 0.0010133445
Iter: 98 loss: 0.000979511882
Iter: 99 loss: 0.000935284188
Iter: 100 loss: 0.00133556407
Iter: 101 loss: 0.000932741386
Iter: 102 loss: 0.000893989
Iter: 103 loss: 0.00114155421
Iter: 104 loss: 0.000889981398
Iter: 105 loss: 0.000860297878
Iter: 106 loss: 0.00101032713
Iter: 107 loss: 0.000855509
Iter: 108 loss: 0.000830815057
Iter: 109 loss: 0.000836754567
Iter: 110 loss: 0.000812828948
Iter: 111 loss: 0.000788532547
Iter: 112 loss: 0.000853750331
Iter: 113 loss: 0.000780531671
Iter: 114 loss: 0.000764047611
Iter: 115 loss: 0.000763931428
Iter: 116 loss: 0.000749097264
Iter: 117 loss: 0.000769545324
Iter: 118 loss: 0.000741543132
Iter: 119 loss: 0.00072343694
Iter: 120 loss: 0.000860596192
Iter: 121 loss: 0.000722069875
Iter: 122 loss: 0.000699379132
Iter: 123 loss: 0.000801895163
Iter: 124 loss: 0.000695200288
Iter: 125 loss: 0.000678165234
Iter: 126 loss: 0.000755592
Iter: 127 loss: 0.000674684707
Iter: 128 loss: 0.000657347264
Iter: 129 loss: 0.000659599784
Iter: 130 loss: 0.000644286105
Iter: 131 loss: 0.000625708839
Iter: 132 loss: 0.000678893935
Iter: 133 loss: 0.000620064326
Iter: 134 loss: 0.000601742
Iter: 135 loss: 0.000760653638
Iter: 136 loss: 0.000600745785
Iter: 137 loss: 0.00058829214
Iter: 138 loss: 0.00057782745
Iter: 139 loss: 0.000574384467
Iter: 140 loss: 0.000553609338
Iter: 141 loss: 0.00057692593
Iter: 142 loss: 0.000542243943
Iter: 143 loss: 0.000529403507
Iter: 144 loss: 0.000529356534
Iter: 145 loss: 0.000516768079
Iter: 146 loss: 0.000539846485
Iter: 147 loss: 0.000511432241
Iter: 148 loss: 0.000500221038
Iter: 149 loss: 0.000505265431
Iter: 150 loss: 0.000492559629
Iter: 151 loss: 0.000484550168
Iter: 152 loss: 0.000482994627
Iter: 153 loss: 0.000474822475
Iter: 154 loss: 0.00048387557
Iter: 155 loss: 0.000470420462
Iter: 156 loss: 0.000458335126
Iter: 157 loss: 0.00046623731
Iter: 158 loss: 0.000450774067
Iter: 159 loss: 0.000443956407
Iter: 160 loss: 0.00044180837
Iter: 161 loss: 0.000435735972
Iter: 162 loss: 0.000428952218
Iter: 163 loss: 0.000428032159
Iter: 164 loss: 0.000413926202
Iter: 165 loss: 0.000437303504
Iter: 166 loss: 0.000407634
Iter: 167 loss: 0.000396263698
Iter: 168 loss: 0.000396227028
Iter: 169 loss: 0.000388408836
Iter: 170 loss: 0.000386963307
Iter: 171 loss: 0.000381732767
Iter: 172 loss: 0.000368641719
Iter: 173 loss: 0.000414800073
Iter: 174 loss: 0.000364967855
Iter: 175 loss: 0.000355210272
Iter: 176 loss: 0.000355132215
Iter: 177 loss: 0.000347170222
Iter: 178 loss: 0.000355185301
Iter: 179 loss: 0.00034271687
Iter: 180 loss: 0.000331219024
Iter: 181 loss: 0.000355575641
Iter: 182 loss: 0.000326838315
Iter: 183 loss: 0.000316490739
Iter: 184 loss: 0.000411390269
Iter: 185 loss: 0.000316021557
Iter: 186 loss: 0.000308924209
Iter: 187 loss: 0.000337417587
Iter: 188 loss: 0.000307321257
Iter: 189 loss: 0.000299026869
Iter: 190 loss: 0.000307502982
Iter: 191 loss: 0.000294444413
Iter: 192 loss: 0.000285410817
Iter: 193 loss: 0.000331753865
Iter: 194 loss: 0.000283918838
Iter: 195 loss: 0.000277523475
Iter: 196 loss: 0.000289319054
Iter: 197 loss: 0.000274778751
Iter: 198 loss: 0.000268985488
Iter: 199 loss: 0.000266461
Iter: 200 loss: 0.000263535592
Iter: 201 loss: 0.000255779247
Iter: 202 loss: 0.000325027388
Iter: 203 loss: 0.000255269872
Iter: 204 loss: 0.000247939955
Iter: 205 loss: 0.000272712816
Iter: 206 loss: 0.000245979056
Iter: 207 loss: 0.000239739165
Iter: 208 loss: 0.000239735818
Iter: 209 loss: 0.000234471983
Iter: 210 loss: 0.000232978229
Iter: 211 loss: 0.000229810539
Iter: 212 loss: 0.000224537333
Iter: 213 loss: 0.000271142984
Iter: 214 loss: 0.000224283984
Iter: 215 loss: 0.00021940883
Iter: 216 loss: 0.000219412483
Iter: 217 loss: 0.000215516338
Iter: 218 loss: 0.000209060236
Iter: 219 loss: 0.000219287802
Iter: 220 loss: 0.000206064171
Iter: 221 loss: 0.000202498457
Iter: 222 loss: 0.000201515257
Iter: 223 loss: 0.000198249123
Iter: 224 loss: 0.000201945644
Iter: 225 loss: 0.00019649422
Iter: 226 loss: 0.000191979911
Iter: 227 loss: 0.000219465888
Iter: 228 loss: 0.000191392755
Iter: 229 loss: 0.000188178819
Iter: 230 loss: 0.000188515885
Iter: 231 loss: 0.000185722529
Iter: 232 loss: 0.000181056035
Iter: 233 loss: 0.000202068011
Iter: 234 loss: 0.000180168121
Iter: 235 loss: 0.000176109083
Iter: 236 loss: 0.000195407396
Iter: 237 loss: 0.000175371606
Iter: 238 loss: 0.000171901891
Iter: 239 loss: 0.000169971579
Iter: 240 loss: 0.000168477738
Iter: 241 loss: 0.000165313395
Iter: 242 loss: 0.000165299833
Iter: 243 loss: 0.00016230598
Iter: 244 loss: 0.000164622848
Iter: 245 loss: 0.000160496566
Iter: 246 loss: 0.000156884198
Iter: 247 loss: 0.00016955327
Iter: 248 loss: 0.000155960442
Iter: 249 loss: 0.000152860666
Iter: 250 loss: 0.000153879737
Iter: 251 loss: 0.000150675914
Iter: 252 loss: 0.00014757381
Iter: 253 loss: 0.000164604309
Iter: 254 loss: 0.000147126659
Iter: 255 loss: 0.000144222184
Iter: 256 loss: 0.000169732841
Iter: 257 loss: 0.000144078978
Iter: 258 loss: 0.00014187819
Iter: 259 loss: 0.000162062061
Iter: 260 loss: 0.000141769822
Iter: 261 loss: 0.000140013028
Iter: 262 loss: 0.000140199263
Iter: 263 loss: 0.000138671981
Iter: 264 loss: 0.000136471353
Iter: 265 loss: 0.000138658375
Iter: 266 loss: 0.000135237409
Iter: 267 loss: 0.000132395056
Iter: 268 loss: 0.000154564419
Iter: 269 loss: 0.000132193978
Iter: 270 loss: 0.000130218614
Iter: 271 loss: 0.000132383764
Iter: 272 loss: 0.000129154228
Iter: 273 loss: 0.000127231033
Iter: 274 loss: 0.00012843631
Iter: 275 loss: 0.000126004452
Iter: 276 loss: 0.00012393744
Iter: 277 loss: 0.000155769827
Iter: 278 loss: 0.000123937032
Iter: 279 loss: 0.000122533791
Iter: 280 loss: 0.000126688901
Iter: 281 loss: 0.000122108118
Iter: 282 loss: 0.000120677541
Iter: 283 loss: 0.000121961035
Iter: 284 loss: 0.000119847711
Iter: 285 loss: 0.000118355645
Iter: 286 loss: 0.000120949255
Iter: 287 loss: 0.000117696385
Iter: 288 loss: 0.000116185882
Iter: 289 loss: 0.000128374755
Iter: 290 loss: 0.000116090894
Iter: 291 loss: 0.000114570983
Iter: 292 loss: 0.000117616582
Iter: 293 loss: 0.000113948525
Iter: 294 loss: 0.00011257983
Iter: 295 loss: 0.000125813094
Iter: 296 loss: 0.000112532405
Iter: 297 loss: 0.000111713351
Iter: 298 loss: 0.00011029921
Iter: 299 loss: 0.000110298497
Iter: 300 loss: 0.000108623397
Iter: 301 loss: 0.000118277923
Iter: 302 loss: 0.000108401589
Iter: 303 loss: 0.000106812571
Iter: 304 loss: 0.000113205431
Iter: 305 loss: 0.000106465566
Iter: 306 loss: 0.000105247629
Iter: 307 loss: 0.000104858693
Iter: 308 loss: 0.000104149498
Iter: 309 loss: 0.000102658618
Iter: 310 loss: 0.000114604991
Iter: 311 loss: 0.000102560101
Iter: 312 loss: 0.000100958147
Iter: 313 loss: 0.000103247541
Iter: 314 loss: 0.000100183635
Iter: 315 loss: 9.85934e-05
Iter: 316 loss: 0.000107381828
Iter: 317 loss: 9.83660721e-05
Iter: 318 loss: 9.70222172e-05
Iter: 319 loss: 9.64131468e-05
Iter: 320 loss: 9.57432785e-05
Iter: 321 loss: 9.42351471e-05
Iter: 322 loss: 0.000104959479
Iter: 323 loss: 9.41024045e-05
Iter: 324 loss: 9.28029913e-05
Iter: 325 loss: 0.000100756792
Iter: 326 loss: 9.26477951e-05
Iter: 327 loss: 9.18133592e-05
Iter: 328 loss: 9.75127768e-05
Iter: 329 loss: 9.17333e-05
Iter: 330 loss: 9.09948212e-05
Iter: 331 loss: 9.0857342e-05
Iter: 332 loss: 9.03628825e-05
Iter: 333 loss: 8.94677141e-05
Iter: 334 loss: 8.91124146e-05
Iter: 335 loss: 8.86343187e-05
Iter: 336 loss: 8.76674458e-05
Iter: 337 loss: 8.76597187e-05
Iter: 338 loss: 8.68839e-05
Iter: 339 loss: 8.62071902e-05
Iter: 340 loss: 8.60010041e-05
Iter: 341 loss: 8.48486e-05
Iter: 342 loss: 8.55009e-05
Iter: 343 loss: 8.41002329e-05
Iter: 344 loss: 8.31428406e-05
Iter: 345 loss: 8.30742865e-05
Iter: 346 loss: 8.24041854e-05
Iter: 347 loss: 8.24711751e-05
Iter: 348 loss: 8.18873596e-05
Iter: 349 loss: 8.09289777e-05
Iter: 350 loss: 8.44259339e-05
Iter: 351 loss: 8.06910539e-05
Iter: 352 loss: 7.99714326e-05
Iter: 353 loss: 8.04571173e-05
Iter: 354 loss: 7.95187225e-05
Iter: 355 loss: 7.88387406e-05
Iter: 356 loss: 7.88383e-05
Iter: 357 loss: 7.83255746e-05
Iter: 358 loss: 7.86195378e-05
Iter: 359 loss: 7.79921102e-05
Iter: 360 loss: 7.72486601e-05
Iter: 361 loss: 8.15891e-05
Iter: 362 loss: 7.71498e-05
Iter: 363 loss: 7.64969736e-05
Iter: 364 loss: 7.62016e-05
Iter: 365 loss: 7.58751848e-05
Iter: 366 loss: 7.52039487e-05
Iter: 367 loss: 7.73422944e-05
Iter: 368 loss: 7.50119652e-05
Iter: 369 loss: 7.42382545e-05
Iter: 370 loss: 7.86451783e-05
Iter: 371 loss: 7.41314871e-05
Iter: 372 loss: 7.36459697e-05
Iter: 373 loss: 7.35972935e-05
Iter: 374 loss: 7.32418048e-05
Iter: 375 loss: 7.26126455e-05
Iter: 376 loss: 7.3984178e-05
Iter: 377 loss: 7.23726262e-05
Iter: 378 loss: 7.175887e-05
Iter: 379 loss: 8.13294173e-05
Iter: 380 loss: 7.17589428e-05
Iter: 381 loss: 7.14163471e-05
Iter: 382 loss: 7.13468689e-05
Iter: 383 loss: 7.11203829e-05
Iter: 384 loss: 7.05467828e-05
Iter: 385 loss: 7.2150091e-05
Iter: 386 loss: 7.03633632e-05
Iter: 387 loss: 6.99547309e-05
Iter: 388 loss: 7.19988093e-05
Iter: 389 loss: 6.98868098e-05
Iter: 390 loss: 6.94101764e-05
Iter: 391 loss: 7.03248079e-05
Iter: 392 loss: 6.9211e-05
Iter: 393 loss: 6.87255e-05
Iter: 394 loss: 6.9744885e-05
Iter: 395 loss: 6.85327395e-05
Iter: 396 loss: 6.8010806e-05
Iter: 397 loss: 7.06177816e-05
Iter: 398 loss: 6.79230288e-05
Iter: 399 loss: 6.75490446e-05
Iter: 400 loss: 6.69797155e-05
Iter: 401 loss: 6.69694e-05
Iter: 402 loss: 6.64790714e-05
Iter: 403 loss: 7.37039663e-05
Iter: 404 loss: 6.64785912e-05
Iter: 405 loss: 6.60243677e-05
Iter: 406 loss: 6.62665116e-05
Iter: 407 loss: 6.57250057e-05
Iter: 408 loss: 6.53110037e-05
Iter: 409 loss: 6.52622548e-05
Iter: 410 loss: 6.49647773e-05
Iter: 411 loss: 6.4491418e-05
Iter: 412 loss: 6.86160638e-05
Iter: 413 loss: 6.44663e-05
Iter: 414 loss: 6.40102735e-05
Iter: 415 loss: 6.60697697e-05
Iter: 416 loss: 6.39234277e-05
Iter: 417 loss: 6.35593678e-05
Iter: 418 loss: 6.3584419e-05
Iter: 419 loss: 6.32766751e-05
Iter: 420 loss: 6.28042253e-05
Iter: 421 loss: 6.62382517e-05
Iter: 422 loss: 6.27661211e-05
Iter: 423 loss: 6.2456631e-05
Iter: 424 loss: 6.40233e-05
Iter: 425 loss: 6.24049862e-05
Iter: 426 loss: 6.20287828e-05
Iter: 427 loss: 6.16486359e-05
Iter: 428 loss: 6.1573206e-05
Iter: 429 loss: 6.13278389e-05
Iter: 430 loss: 6.13117663e-05
Iter: 431 loss: 6.10986e-05
Iter: 432 loss: 6.07274487e-05
Iter: 433 loss: 6.07272159e-05
Iter: 434 loss: 6.02365872e-05
Iter: 435 loss: 6.04388915e-05
Iter: 436 loss: 5.98982588e-05
Iter: 437 loss: 5.95274469e-05
Iter: 438 loss: 5.95272504e-05
Iter: 439 loss: 5.91385251e-05
Iter: 440 loss: 5.87439899e-05
Iter: 441 loss: 5.86679234e-05
Iter: 442 loss: 5.81431814e-05
Iter: 443 loss: 5.83534638e-05
Iter: 444 loss: 5.77811516e-05
Iter: 445 loss: 5.75374361e-05
Iter: 446 loss: 5.74102669e-05
Iter: 447 loss: 5.714074e-05
Iter: 448 loss: 5.70123157e-05
Iter: 449 loss: 5.68806245e-05
Iter: 450 loss: 5.64828952e-05
Iter: 451 loss: 5.77676401e-05
Iter: 452 loss: 5.63719077e-05
Iter: 453 loss: 5.60777189e-05
Iter: 454 loss: 6.0346676e-05
Iter: 455 loss: 5.60781227e-05
Iter: 456 loss: 5.58863831e-05
Iter: 457 loss: 5.65756091e-05
Iter: 458 loss: 5.58380634e-05
Iter: 459 loss: 5.56702e-05
Iter: 460 loss: 5.53015416e-05
Iter: 461 loss: 6.06359754e-05
Iter: 462 loss: 5.52840538e-05
Iter: 463 loss: 5.5088698e-05
Iter: 464 loss: 5.50280565e-05
Iter: 465 loss: 5.48265089e-05
Iter: 466 loss: 5.44459726e-05
Iter: 467 loss: 6.27274e-05
Iter: 468 loss: 5.44451141e-05
Iter: 469 loss: 5.40873552e-05
Iter: 470 loss: 5.47264171e-05
Iter: 471 loss: 5.39308385e-05
Iter: 472 loss: 5.36036605e-05
Iter: 473 loss: 5.4725373e-05
Iter: 474 loss: 5.35167055e-05
Iter: 475 loss: 5.31263941e-05
Iter: 476 loss: 5.47158e-05
Iter: 477 loss: 5.30406905e-05
Iter: 478 loss: 5.28008241e-05
Iter: 479 loss: 5.2425512e-05
Iter: 480 loss: 5.24201387e-05
Iter: 481 loss: 5.21994043e-05
Iter: 482 loss: 5.21585353e-05
Iter: 483 loss: 5.18828019e-05
Iter: 484 loss: 5.16287801e-05
Iter: 485 loss: 5.15620923e-05
Iter: 486 loss: 5.12739061e-05
Iter: 487 loss: 5.24448296e-05
Iter: 488 loss: 5.12089246e-05
Iter: 489 loss: 5.0923325e-05
Iter: 490 loss: 5.22460396e-05
Iter: 491 loss: 5.08699595e-05
Iter: 492 loss: 5.06915821e-05
Iter: 493 loss: 5.28198725e-05
Iter: 494 loss: 5.06899742e-05
Iter: 495 loss: 5.05196731e-05
Iter: 496 loss: 5.02126459e-05
Iter: 497 loss: 5.75160666e-05
Iter: 498 loss: 5.02127368e-05
Iter: 499 loss: 4.99453599e-05
Iter: 500 loss: 5.02007788e-05
Iter: 501 loss: 4.97933361e-05
Iter: 502 loss: 4.96178509e-05
Iter: 503 loss: 4.95956047e-05
Iter: 504 loss: 4.94532869e-05
Iter: 505 loss: 4.91256214e-05
Iter: 506 loss: 5.33284692e-05
Iter: 507 loss: 4.91028914e-05
Iter: 508 loss: 4.88223886e-05
Iter: 509 loss: 5.28519195e-05
Iter: 510 loss: 4.88219084e-05
Iter: 511 loss: 4.86358222e-05
Iter: 512 loss: 4.8325659e-05
Iter: 513 loss: 4.83250624e-05
Iter: 514 loss: 4.81264069e-05
Iter: 515 loss: 4.81216e-05
Iter: 516 loss: 4.79450064e-05
Iter: 517 loss: 4.79021546e-05
Iter: 518 loss: 4.77900612e-05
Iter: 519 loss: 4.75263296e-05
Iter: 520 loss: 4.81081843e-05
Iter: 521 loss: 4.74237531e-05
Iter: 522 loss: 4.72174506e-05
Iter: 523 loss: 4.73869368e-05
Iter: 524 loss: 4.70932209e-05
Iter: 525 loss: 4.67858226e-05
Iter: 526 loss: 4.8652877e-05
Iter: 527 loss: 4.6747984e-05
Iter: 528 loss: 4.65423509e-05
Iter: 529 loss: 4.90102975e-05
Iter: 530 loss: 4.65399971e-05
Iter: 531 loss: 4.6416204e-05
Iter: 532 loss: 4.61379968e-05
Iter: 533 loss: 4.99479211e-05
Iter: 534 loss: 4.61223935e-05
Iter: 535 loss: 4.58358918e-05
Iter: 536 loss: 4.66925412e-05
Iter: 537 loss: 4.57497372e-05
Iter: 538 loss: 4.55588051e-05
Iter: 539 loss: 4.5545592e-05
Iter: 540 loss: 4.54417968e-05
Iter: 541 loss: 4.52577515e-05
Iter: 542 loss: 4.98261616e-05
Iter: 543 loss: 4.52569184e-05
Iter: 544 loss: 4.5038094e-05
Iter: 545 loss: 4.66753154e-05
Iter: 546 loss: 4.50198713e-05
Iter: 547 loss: 4.48529972e-05
Iter: 548 loss: 4.44705256e-05
Iter: 549 loss: 4.93613115e-05
Iter: 550 loss: 4.44439e-05
Iter: 551 loss: 4.43050594e-05
Iter: 552 loss: 4.42468518e-05
Iter: 553 loss: 4.40569602e-05
Iter: 554 loss: 4.3950502e-05
Iter: 555 loss: 4.38666684e-05
Iter: 556 loss: 4.36745031e-05
Iter: 557 loss: 4.4471788e-05
Iter: 558 loss: 4.36320843e-05
Iter: 559 loss: 4.34427748e-05
Iter: 560 loss: 4.35106631e-05
Iter: 561 loss: 4.3310265e-05
Iter: 562 loss: 4.31747321e-05
Iter: 563 loss: 4.31702065e-05
Iter: 564 loss: 4.30341461e-05
Iter: 565 loss: 4.2946449e-05
Iter: 566 loss: 4.2894455e-05
Iter: 567 loss: 4.2705622e-05
Iter: 568 loss: 4.27502091e-05
Iter: 569 loss: 4.25676189e-05
Iter: 570 loss: 4.23923302e-05
Iter: 571 loss: 4.32502711e-05
Iter: 572 loss: 4.23612291e-05
Iter: 573 loss: 4.21515e-05
Iter: 574 loss: 4.30241889e-05
Iter: 575 loss: 4.21061122e-05
Iter: 576 loss: 4.19889111e-05
Iter: 577 loss: 4.18948039e-05
Iter: 578 loss: 4.18604031e-05
Iter: 579 loss: 4.16210532e-05
Iter: 580 loss: 4.2217911e-05
Iter: 581 loss: 4.15381437e-05
Iter: 582 loss: 4.13791495e-05
Iter: 583 loss: 4.13710295e-05
Iter: 584 loss: 4.12501468e-05
Iter: 585 loss: 4.10826433e-05
Iter: 586 loss: 4.10792745e-05
Iter: 587 loss: 4.09769345e-05
Iter: 588 loss: 4.08788655e-05
Iter: 589 loss: 4.08564665e-05
Iter: 590 loss: 4.07047773e-05
Iter: 591 loss: 4.15948089e-05
Iter: 592 loss: 4.06855e-05
Iter: 593 loss: 4.05414066e-05
Iter: 594 loss: 4.04223247e-05
Iter: 595 loss: 4.0380437e-05
Iter: 596 loss: 4.02988153e-05
Iter: 597 loss: 4.02516489e-05
Iter: 598 loss: 4.0179606e-05
Iter: 599 loss: 4.0005878e-05
Iter: 600 loss: 4.1820731e-05
Iter: 601 loss: 3.99858327e-05
Iter: 602 loss: 3.97595213e-05
Iter: 603 loss: 4.01780926e-05
Iter: 604 loss: 3.9662973e-05
Iter: 605 loss: 3.95369643e-05
Iter: 606 loss: 4.05548308e-05
Iter: 607 loss: 3.95280149e-05
Iter: 608 loss: 3.93978662e-05
Iter: 609 loss: 3.93005976e-05
Iter: 610 loss: 3.92580587e-05
Iter: 611 loss: 3.90880959e-05
Iter: 612 loss: 3.90690184e-05
Iter: 613 loss: 3.8947077e-05
Iter: 614 loss: 3.873479e-05
Iter: 615 loss: 3.984863e-05
Iter: 616 loss: 3.87013424e-05
Iter: 617 loss: 3.85264429e-05
Iter: 618 loss: 3.86641477e-05
Iter: 619 loss: 3.84194e-05
Iter: 620 loss: 3.82945873e-05
Iter: 621 loss: 3.82918079e-05
Iter: 622 loss: 3.81610153e-05
Iter: 623 loss: 3.81063437e-05
Iter: 624 loss: 3.8038288e-05
Iter: 625 loss: 3.78798577e-05
Iter: 626 loss: 3.83752922e-05
Iter: 627 loss: 3.78334e-05
Iter: 628 loss: 3.76385142e-05
Iter: 629 loss: 3.82370636e-05
Iter: 630 loss: 3.75805357e-05
Iter: 631 loss: 3.74674491e-05
Iter: 632 loss: 3.74653646e-05
Iter: 633 loss: 3.7388847e-05
Iter: 634 loss: 3.72534814e-05
Iter: 635 loss: 3.72535396e-05
Iter: 636 loss: 3.70586749e-05
Iter: 637 loss: 3.80435e-05
Iter: 638 loss: 3.70255148e-05
Iter: 639 loss: 3.6891528e-05
Iter: 640 loss: 3.75051059e-05
Iter: 641 loss: 3.68657e-05
Iter: 642 loss: 3.67393222e-05
Iter: 643 loss: 3.67623725e-05
Iter: 644 loss: 3.66444146e-05
Iter: 645 loss: 3.65095548e-05
Iter: 646 loss: 3.63301297e-05
Iter: 647 loss: 3.63188847e-05
Iter: 648 loss: 3.60843733e-05
Iter: 649 loss: 3.68905021e-05
Iter: 650 loss: 3.60232734e-05
Iter: 651 loss: 3.58027246e-05
Iter: 652 loss: 3.75006784e-05
Iter: 653 loss: 3.5786903e-05
Iter: 654 loss: 3.56541859e-05
Iter: 655 loss: 3.67469838e-05
Iter: 656 loss: 3.56458149e-05
Iter: 657 loss: 3.5504112e-05
Iter: 658 loss: 3.56578239e-05
Iter: 659 loss: 3.54282565e-05
Iter: 660 loss: 3.5314828e-05
Iter: 661 loss: 3.54942458e-05
Iter: 662 loss: 3.52612915e-05
Iter: 663 loss: 3.51054514e-05
Iter: 664 loss: 3.59284277e-05
Iter: 665 loss: 3.50801129e-05
Iter: 666 loss: 3.49683251e-05
Iter: 667 loss: 3.59709957e-05
Iter: 668 loss: 3.49613911e-05
Iter: 669 loss: 3.48938192e-05
Iter: 670 loss: 3.48798239e-05
Iter: 671 loss: 3.48337271e-05
Iter: 672 loss: 3.47289424e-05
Iter: 673 loss: 3.50828559e-05
Iter: 674 loss: 3.46995294e-05
Iter: 675 loss: 3.46110974e-05
Iter: 676 loss: 3.48721478e-05
Iter: 677 loss: 3.45844237e-05
Iter: 678 loss: 3.44843793e-05
Iter: 679 loss: 3.43411593e-05
Iter: 680 loss: 3.43367865e-05
Iter: 681 loss: 3.41760024e-05
Iter: 682 loss: 3.43598331e-05
Iter: 683 loss: 3.40910919e-05
Iter: 684 loss: 3.39089092e-05
Iter: 685 loss: 3.44164437e-05
Iter: 686 loss: 3.38497703e-05
Iter: 687 loss: 3.36672892e-05
Iter: 688 loss: 3.48813701e-05
Iter: 689 loss: 3.36477387e-05
Iter: 690 loss: 3.35103541e-05
Iter: 691 loss: 3.49668735e-05
Iter: 692 loss: 3.35072473e-05
Iter: 693 loss: 3.33960452e-05
Iter: 694 loss: 3.33070711e-05
Iter: 695 loss: 3.32741256e-05
Iter: 696 loss: 3.31651827e-05
Iter: 697 loss: 3.47702371e-05
Iter: 698 loss: 3.31646406e-05
Iter: 699 loss: 3.30639377e-05
Iter: 700 loss: 3.33016033e-05
Iter: 701 loss: 3.30271832e-05
Iter: 702 loss: 3.29094437e-05
Iter: 703 loss: 3.30696057e-05
Iter: 704 loss: 3.28507449e-05
Iter: 705 loss: 3.27546659e-05
Iter: 706 loss: 3.3285949e-05
Iter: 707 loss: 3.27413873e-05
Iter: 708 loss: 3.26545414e-05
Iter: 709 loss: 3.26385198e-05
Iter: 710 loss: 3.25803412e-05
Iter: 711 loss: 3.24558605e-05
Iter: 712 loss: 3.29780305e-05
Iter: 713 loss: 3.24292851e-05
Iter: 714 loss: 3.23372333e-05
Iter: 715 loss: 3.22116102e-05
Iter: 716 loss: 3.22057313e-05
Iter: 717 loss: 3.20483e-05
Iter: 718 loss: 3.22965607e-05
Iter: 719 loss: 3.19747051e-05
Iter: 720 loss: 3.18028688e-05
Iter: 721 loss: 3.26751178e-05
Iter: 722 loss: 3.17744089e-05
Iter: 723 loss: 3.16522819e-05
Iter: 724 loss: 3.33421704e-05
Iter: 725 loss: 3.16515143e-05
Iter: 726 loss: 3.15395155e-05
Iter: 727 loss: 3.16502628e-05
Iter: 728 loss: 3.14766367e-05
Iter: 729 loss: 3.13566416e-05
Iter: 730 loss: 3.14260178e-05
Iter: 731 loss: 3.12774391e-05
Iter: 732 loss: 3.11630756e-05
Iter: 733 loss: 3.11608055e-05
Iter: 734 loss: 3.10843825e-05
Iter: 735 loss: 3.12115299e-05
Iter: 736 loss: 3.10500218e-05
Iter: 737 loss: 3.09639945e-05
Iter: 738 loss: 3.0971e-05
Iter: 739 loss: 3.08987837e-05
Iter: 740 loss: 3.07643058e-05
Iter: 741 loss: 3.11390686e-05
Iter: 742 loss: 3.07206028e-05
Iter: 743 loss: 3.0620391e-05
Iter: 744 loss: 3.08438539e-05
Iter: 745 loss: 3.05812864e-05
Iter: 746 loss: 3.04530076e-05
Iter: 747 loss: 3.04003079e-05
Iter: 748 loss: 3.03321031e-05
Iter: 749 loss: 3.01904165e-05
Iter: 750 loss: 3.02881763e-05
Iter: 751 loss: 3.01010987e-05
Iter: 752 loss: 2.99422754e-05
Iter: 753 loss: 3.03871166e-05
Iter: 754 loss: 2.98907689e-05
Iter: 755 loss: 2.97387705e-05
Iter: 756 loss: 3.08807903e-05
Iter: 757 loss: 2.97258302e-05
Iter: 758 loss: 2.96075068e-05
Iter: 759 loss: 3.08397066e-05
Iter: 760 loss: 2.96050839e-05
Iter: 761 loss: 2.95225109e-05
Iter: 762 loss: 2.94370657e-05
Iter: 763 loss: 2.94216661e-05
Iter: 764 loss: 2.93216144e-05
Iter: 765 loss: 2.93217163e-05
Iter: 766 loss: 2.92366603e-05
Iter: 767 loss: 2.94715392e-05
Iter: 768 loss: 2.920933e-05
Iter: 769 loss: 2.9130415e-05
Iter: 770 loss: 2.92703444e-05
Iter: 771 loss: 2.90955286e-05
Iter: 772 loss: 2.90150401e-05
Iter: 773 loss: 2.92339118e-05
Iter: 774 loss: 2.89889213e-05
Iter: 775 loss: 2.89067902e-05
Iter: 776 loss: 2.88584124e-05
Iter: 777 loss: 2.88245119e-05
Iter: 778 loss: 2.87192343e-05
Iter: 779 loss: 2.96096096e-05
Iter: 780 loss: 2.87135881e-05
Iter: 781 loss: 2.86413342e-05
Iter: 782 loss: 2.84899324e-05
Iter: 783 loss: 3.09771931e-05
Iter: 784 loss: 2.84862672e-05
Iter: 785 loss: 2.83246245e-05
Iter: 786 loss: 2.88849333e-05
Iter: 787 loss: 2.8281931e-05
Iter: 788 loss: 2.8118895e-05
Iter: 789 loss: 2.87350122e-05
Iter: 790 loss: 2.80796e-05
Iter: 791 loss: 2.79657579e-05
Iter: 792 loss: 2.79649357e-05
Iter: 793 loss: 2.78692241e-05
Iter: 794 loss: 2.78292391e-05
Iter: 795 loss: 2.77782528e-05
Iter: 796 loss: 2.76739e-05
Iter: 797 loss: 2.85962087e-05
Iter: 798 loss: 2.76689625e-05
Iter: 799 loss: 2.75707243e-05
Iter: 800 loss: 2.78984608e-05
Iter: 801 loss: 2.7544258e-05
Iter: 802 loss: 2.7457063e-05
Iter: 803 loss: 2.77089912e-05
Iter: 804 loss: 2.74298909e-05
Iter: 805 loss: 2.73517726e-05
Iter: 806 loss: 2.74929844e-05
Iter: 807 loss: 2.73177211e-05
Iter: 808 loss: 2.72247416e-05
Iter: 809 loss: 2.72161506e-05
Iter: 810 loss: 2.71467052e-05
Iter: 811 loss: 2.70476976e-05
Iter: 812 loss: 2.74864778e-05
Iter: 813 loss: 2.70285564e-05
Iter: 814 loss: 2.69261764e-05
Iter: 815 loss: 2.68928488e-05
Iter: 816 loss: 2.68336462e-05
Iter: 817 loss: 2.67072355e-05
Iter: 818 loss: 2.66363659e-05
Iter: 819 loss: 2.65813178e-05
Iter: 820 loss: 2.64195223e-05
Iter: 821 loss: 2.75706589e-05
Iter: 822 loss: 2.64047903e-05
Iter: 823 loss: 2.628521e-05
Iter: 824 loss: 2.73301957e-05
Iter: 825 loss: 2.62783451e-05
Iter: 826 loss: 2.61576133e-05
Iter: 827 loss: 2.65288709e-05
Iter: 828 loss: 2.61219848e-05
Iter: 829 loss: 2.60269571e-05
Iter: 830 loss: 2.61030073e-05
Iter: 831 loss: 2.59708268e-05
Iter: 832 loss: 2.58722666e-05
Iter: 833 loss: 2.58724449e-05
Iter: 834 loss: 2.58091986e-05
Iter: 835 loss: 2.58944601e-05
Iter: 836 loss: 2.57777883e-05
Iter: 837 loss: 2.57020802e-05
Iter: 838 loss: 2.58146647e-05
Iter: 839 loss: 2.56656458e-05
Iter: 840 loss: 2.55840205e-05
Iter: 841 loss: 2.57783649e-05
Iter: 842 loss: 2.55542946e-05
Iter: 843 loss: 2.54853494e-05
Iter: 844 loss: 2.5469908e-05
Iter: 845 loss: 2.54259685e-05
Iter: 846 loss: 2.53201397e-05
Iter: 847 loss: 2.57699357e-05
Iter: 848 loss: 2.52979735e-05
Iter: 849 loss: 2.52181489e-05
Iter: 850 loss: 2.51040274e-05
Iter: 851 loss: 2.5100755e-05
Iter: 852 loss: 2.49457989e-05
Iter: 853 loss: 2.51575293e-05
Iter: 854 loss: 2.48685792e-05
Iter: 855 loss: 2.4732457e-05
Iter: 856 loss: 2.65071612e-05
Iter: 857 loss: 2.47312928e-05
Iter: 858 loss: 2.46303443e-05
Iter: 859 loss: 2.56534913e-05
Iter: 860 loss: 2.46278214e-05
Iter: 861 loss: 2.45497431e-05
Iter: 862 loss: 2.45069914e-05
Iter: 863 loss: 2.4472969e-05
Iter: 864 loss: 2.44141902e-05
Iter: 865 loss: 2.44093644e-05
Iter: 866 loss: 2.43526611e-05
Iter: 867 loss: 2.43572031e-05
Iter: 868 loss: 2.43081286e-05
Iter: 869 loss: 2.42268816e-05
Iter: 870 loss: 2.45416886e-05
Iter: 871 loss: 2.42062124e-05
Iter: 872 loss: 2.41402322e-05
Iter: 873 loss: 2.42731185e-05
Iter: 874 loss: 2.41116068e-05
Iter: 875 loss: 2.40438676e-05
Iter: 876 loss: 2.4005345e-05
Iter: 877 loss: 2.39753354e-05
Iter: 878 loss: 2.38958455e-05
Iter: 879 loss: 2.44542571e-05
Iter: 880 loss: 2.38885259e-05
Iter: 881 loss: 2.38151879e-05
Iter: 882 loss: 2.37289696e-05
Iter: 883 loss: 2.37197091e-05
Iter: 884 loss: 2.35974367e-05
Iter: 885 loss: 2.36145188e-05
Iter: 886 loss: 2.35039515e-05
Iter: 887 loss: 2.33601895e-05
Iter: 888 loss: 2.4225792e-05
Iter: 889 loss: 2.33432656e-05
Iter: 890 loss: 2.32536895e-05
Iter: 891 loss: 2.32520724e-05
Iter: 892 loss: 2.31727317e-05
Iter: 893 loss: 2.31774738e-05
Iter: 894 loss: 2.31113336e-05
Iter: 895 loss: 2.3026596e-05
Iter: 896 loss: 2.35891966e-05
Iter: 897 loss: 2.30192873e-05
Iter: 898 loss: 2.29290017e-05
Iter: 899 loss: 2.31983395e-05
Iter: 900 loss: 2.29011639e-05
Iter: 901 loss: 2.28389399e-05
Iter: 902 loss: 2.32149068e-05
Iter: 903 loss: 2.28311583e-05
Iter: 904 loss: 2.27804594e-05
Iter: 905 loss: 2.28015269e-05
Iter: 906 loss: 2.27452801e-05
Iter: 907 loss: 2.26720258e-05
Iter: 908 loss: 2.26891752e-05
Iter: 909 loss: 2.26183183e-05
Iter: 910 loss: 2.25387921e-05
Iter: 911 loss: 2.27016535e-05
Iter: 912 loss: 2.25060903e-05
Iter: 913 loss: 2.24119194e-05
Iter: 914 loss: 2.25857439e-05
Iter: 915 loss: 2.23722855e-05
Iter: 916 loss: 2.22786148e-05
Iter: 917 loss: 2.21691189e-05
Iter: 918 loss: 2.21568407e-05
Iter: 919 loss: 2.20092406e-05
Iter: 920 loss: 2.23423394e-05
Iter: 921 loss: 2.19535759e-05
Iter: 922 loss: 2.18616769e-05
Iter: 923 loss: 2.1858883e-05
Iter: 924 loss: 2.17637207e-05
Iter: 925 loss: 2.20279089e-05
Iter: 926 loss: 2.17331508e-05
Iter: 927 loss: 2.16596854e-05
Iter: 928 loss: 2.1860491e-05
Iter: 929 loss: 2.16356275e-05
Iter: 930 loss: 2.15789696e-05
Iter: 931 loss: 2.15789078e-05
Iter: 932 loss: 2.15414293e-05
Iter: 933 loss: 2.15148684e-05
Iter: 934 loss: 2.15019427e-05
Iter: 935 loss: 2.14293104e-05
Iter: 936 loss: 2.16645603e-05
Iter: 937 loss: 2.14082302e-05
Iter: 938 loss: 2.13504518e-05
Iter: 939 loss: 2.14022657e-05
Iter: 940 loss: 2.13165968e-05
Iter: 941 loss: 2.12454397e-05
Iter: 942 loss: 2.11975257e-05
Iter: 943 loss: 2.117037e-05
Iter: 944 loss: 2.10833077e-05
Iter: 945 loss: 2.19707e-05
Iter: 946 loss: 2.10811122e-05
Iter: 947 loss: 2.10162889e-05
Iter: 948 loss: 2.09420159e-05
Iter: 949 loss: 2.09330628e-05
Iter: 950 loss: 2.08255206e-05
Iter: 951 loss: 2.08816637e-05
Iter: 952 loss: 2.07546109e-05
Iter: 953 loss: 2.06284076e-05
Iter: 954 loss: 2.08977635e-05
Iter: 955 loss: 2.05803663e-05
Iter: 956 loss: 2.05440592e-05
Iter: 957 loss: 2.05148317e-05
Iter: 958 loss: 2.04515654e-05
Iter: 959 loss: 2.03831914e-05
Iter: 960 loss: 2.0373478e-05
Iter: 961 loss: 2.03301097e-05
Iter: 962 loss: 2.03228155e-05
Iter: 963 loss: 2.02731098e-05
Iter: 964 loss: 2.03140953e-05
Iter: 965 loss: 2.02432493e-05
Iter: 966 loss: 2.01910352e-05
Iter: 967 loss: 2.03957279e-05
Iter: 968 loss: 2.01786752e-05
Iter: 969 loss: 2.01206658e-05
Iter: 970 loss: 2.01852781e-05
Iter: 971 loss: 2.00890645e-05
Iter: 972 loss: 2.00319537e-05
Iter: 973 loss: 2.0095049e-05
Iter: 974 loss: 2.00003924e-05
Iter: 975 loss: 1.99346905e-05
Iter: 976 loss: 1.98553353e-05
Iter: 977 loss: 1.98470498e-05
Iter: 978 loss: 1.97445697e-05
Iter: 979 loss: 2.12473788e-05
Iter: 980 loss: 1.97443969e-05
Iter: 981 loss: 1.96777146e-05
Iter: 982 loss: 1.96269484e-05
Iter: 983 loss: 1.96048677e-05
Iter: 984 loss: 1.95050307e-05
Iter: 985 loss: 1.96309593e-05
Iter: 986 loss: 1.94531276e-05
Iter: 987 loss: 1.93418055e-05
Iter: 988 loss: 1.99482929e-05
Iter: 989 loss: 1.93251e-05
Iter: 990 loss: 1.92576899e-05
Iter: 991 loss: 1.92558618e-05
Iter: 992 loss: 1.92064981e-05
Iter: 993 loss: 1.91538129e-05
Iter: 994 loss: 1.91462896e-05
Iter: 995 loss: 1.90753708e-05
Iter: 996 loss: 1.90736682e-05
Iter: 997 loss: 1.90297542e-05
Iter: 998 loss: 1.90606625e-05
Iter: 999 loss: 1.90021528e-05
Iter: 1000 loss: 1.89479942e-05
Iter: 1001 loss: 1.91584913e-05
Iter: 1002 loss: 1.89354032e-05
Iter: 1003 loss: 1.88875783e-05
Iter: 1004 loss: 1.88447448e-05
Iter: 1005 loss: 1.88326321e-05
Iter: 1006 loss: 1.87555579e-05
Iter: 1007 loss: 1.8930019e-05
Iter: 1008 loss: 1.87257283e-05
Iter: 1009 loss: 1.86472334e-05
Iter: 1010 loss: 1.87201731e-05
Iter: 1011 loss: 1.86019279e-05
Iter: 1012 loss: 1.85060599e-05
Iter: 1013 loss: 1.9073952e-05
Iter: 1014 loss: 1.84941364e-05
Iter: 1015 loss: 1.84332239e-05
Iter: 1016 loss: 1.8314644e-05
Iter: 1017 loss: 2.06865225e-05
Iter: 1018 loss: 1.83125085e-05
Iter: 1019 loss: 1.81782434e-05
Iter: 1020 loss: 1.84766177e-05
Iter: 1021 loss: 1.81279647e-05
Iter: 1022 loss: 1.80444731e-05
Iter: 1023 loss: 1.80398729e-05
Iter: 1024 loss: 1.7960052e-05
Iter: 1025 loss: 1.83094926e-05
Iter: 1026 loss: 1.79428589e-05
Iter: 1027 loss: 1.78947375e-05
Iter: 1028 loss: 1.80868046e-05
Iter: 1029 loss: 1.78844093e-05
Iter: 1030 loss: 1.78089722e-05
Iter: 1031 loss: 1.78186892e-05
Iter: 1032 loss: 1.77516431e-05
Iter: 1033 loss: 1.76867561e-05
Iter: 1034 loss: 1.82819931e-05
Iter: 1035 loss: 1.76834164e-05
Iter: 1036 loss: 1.76329304e-05
Iter: 1037 loss: 1.76193698e-05
Iter: 1038 loss: 1.75875557e-05
Iter: 1039 loss: 1.75172754e-05
Iter: 1040 loss: 1.75707974e-05
Iter: 1041 loss: 1.74748711e-05
Iter: 1042 loss: 1.73968638e-05
Iter: 1043 loss: 1.76514204e-05
Iter: 1044 loss: 1.73751596e-05
Iter: 1045 loss: 1.73099525e-05
Iter: 1046 loss: 1.74265297e-05
Iter: 1047 loss: 1.72820164e-05
Iter: 1048 loss: 1.72134696e-05
Iter: 1049 loss: 1.74855541e-05
Iter: 1050 loss: 1.71979755e-05
Iter: 1051 loss: 1.7142127e-05
Iter: 1052 loss: 1.70672793e-05
Iter: 1053 loss: 1.70633648e-05
Iter: 1054 loss: 1.69780196e-05
Iter: 1055 loss: 1.70497169e-05
Iter: 1056 loss: 1.69278883e-05
Iter: 1057 loss: 1.69192172e-05
Iter: 1058 loss: 1.68777351e-05
Iter: 1059 loss: 1.68267525e-05
Iter: 1060 loss: 1.67800426e-05
Iter: 1061 loss: 1.67673988e-05
Iter: 1062 loss: 1.67213839e-05
Iter: 1063 loss: 1.71116335e-05
Iter: 1064 loss: 1.67188355e-05
Iter: 1065 loss: 1.6653401e-05
Iter: 1066 loss: 1.67301914e-05
Iter: 1067 loss: 1.66184636e-05
Iter: 1068 loss: 1.657951e-05
Iter: 1069 loss: 1.69602554e-05
Iter: 1070 loss: 1.65782349e-05
Iter: 1071 loss: 1.65499405e-05
Iter: 1072 loss: 1.64836874e-05
Iter: 1073 loss: 1.72936725e-05
Iter: 1074 loss: 1.64791636e-05
Iter: 1075 loss: 1.63983605e-05
Iter: 1076 loss: 1.68740953e-05
Iter: 1077 loss: 1.63876193e-05
Iter: 1078 loss: 1.63228633e-05
Iter: 1079 loss: 1.63793775e-05
Iter: 1080 loss: 1.62848337e-05
Iter: 1081 loss: 1.62230317e-05
Iter: 1082 loss: 1.66072896e-05
Iter: 1083 loss: 1.62160686e-05
Iter: 1084 loss: 1.61660391e-05
Iter: 1085 loss: 1.61671887e-05
Iter: 1086 loss: 1.61268035e-05
Iter: 1087 loss: 1.60484651e-05
Iter: 1088 loss: 1.61090647e-05
Iter: 1089 loss: 1.60007694e-05
Iter: 1090 loss: 1.59114388e-05
Iter: 1091 loss: 1.59223255e-05
Iter: 1092 loss: 1.5842681e-05
Iter: 1093 loss: 1.57423365e-05
Iter: 1094 loss: 1.62137458e-05
Iter: 1095 loss: 1.57244176e-05
Iter: 1096 loss: 1.56585393e-05
Iter: 1097 loss: 1.56533497e-05
Iter: 1098 loss: 1.56280803e-05
Iter: 1099 loss: 1.58197217e-05
Iter: 1100 loss: 1.56264796e-05
Iter: 1101 loss: 1.55930684e-05
Iter: 1102 loss: 1.55350717e-05
Iter: 1103 loss: 1.55339894e-05
Iter: 1104 loss: 1.54963673e-05
Iter: 1105 loss: 1.54958634e-05
Iter: 1106 loss: 1.54714307e-05
Iter: 1107 loss: 1.54068184e-05
Iter: 1108 loss: 1.58891817e-05
Iter: 1109 loss: 1.53940036e-05
Iter: 1110 loss: 1.53598139e-05
Iter: 1111 loss: 1.53563851e-05
Iter: 1112 loss: 1.53228557e-05
Iter: 1113 loss: 1.52905632e-05
Iter: 1114 loss: 1.52837329e-05
Iter: 1115 loss: 1.52215989e-05
Iter: 1116 loss: 1.5597594e-05
Iter: 1117 loss: 1.52145712e-05
Iter: 1118 loss: 1.51689446e-05
Iter: 1119 loss: 1.51908043e-05
Iter: 1120 loss: 1.51372569e-05
Iter: 1121 loss: 1.50814431e-05
Iter: 1122 loss: 1.51100621e-05
Iter: 1123 loss: 1.50437936e-05
Iter: 1124 loss: 1.49729876e-05
Iter: 1125 loss: 1.52570146e-05
Iter: 1126 loss: 1.49570333e-05
Iter: 1127 loss: 1.49177795e-05
Iter: 1128 loss: 1.49165444e-05
Iter: 1129 loss: 1.48795034e-05
Iter: 1130 loss: 1.48982199e-05
Iter: 1131 loss: 1.48552972e-05
Iter: 1132 loss: 1.48234449e-05
Iter: 1133 loss: 1.48202507e-05
Iter: 1134 loss: 1.4807064e-05
Iter: 1135 loss: 1.47832779e-05
Iter: 1136 loss: 1.53653982e-05
Iter: 1137 loss: 1.47831961e-05
Iter: 1138 loss: 1.47353558e-05
Iter: 1139 loss: 1.46693437e-05
Iter: 1140 loss: 1.46665861e-05
Iter: 1141 loss: 1.46217571e-05
Iter: 1142 loss: 1.46423627e-05
Iter: 1143 loss: 1.45914928e-05
Iter: 1144 loss: 1.45259601e-05
Iter: 1145 loss: 1.51103541e-05
Iter: 1146 loss: 1.45221629e-05
Iter: 1147 loss: 1.44896294e-05
Iter: 1148 loss: 1.44429e-05
Iter: 1149 loss: 1.44413352e-05
Iter: 1150 loss: 1.4394398e-05
Iter: 1151 loss: 1.4817615e-05
Iter: 1152 loss: 1.43929537e-05
Iter: 1153 loss: 1.43429897e-05
Iter: 1154 loss: 1.43590842e-05
Iter: 1155 loss: 1.43070547e-05
Iter: 1156 loss: 1.42598728e-05
Iter: 1157 loss: 1.42184035e-05
Iter: 1158 loss: 1.42057561e-05
Iter: 1159 loss: 1.41643841e-05
Iter: 1160 loss: 1.47957853e-05
Iter: 1161 loss: 1.41646451e-05
Iter: 1162 loss: 1.41273331e-05
Iter: 1163 loss: 1.42322715e-05
Iter: 1164 loss: 1.41152095e-05
Iter: 1165 loss: 1.40979109e-05
Iter: 1166 loss: 1.40937791e-05
Iter: 1167 loss: 1.40785114e-05
Iter: 1168 loss: 1.40364455e-05
Iter: 1169 loss: 1.4264373e-05
Iter: 1170 loss: 1.40235879e-05
Iter: 1171 loss: 1.39873873e-05
Iter: 1172 loss: 1.39872136e-05
Iter: 1173 loss: 1.39629874e-05
Iter: 1174 loss: 1.3926996e-05
Iter: 1175 loss: 1.39264166e-05
Iter: 1176 loss: 1.38864762e-05
Iter: 1177 loss: 1.40188877e-05
Iter: 1178 loss: 1.3874338e-05
Iter: 1179 loss: 1.38269e-05
Iter: 1180 loss: 1.39752883e-05
Iter: 1181 loss: 1.38131654e-05
Iter: 1182 loss: 1.37804145e-05
Iter: 1183 loss: 1.37141697e-05
Iter: 1184 loss: 1.48962235e-05
Iter: 1185 loss: 1.37127181e-05
Iter: 1186 loss: 1.36415438e-05
Iter: 1187 loss: 1.419825e-05
Iter: 1188 loss: 1.36368817e-05
Iter: 1189 loss: 1.35849632e-05
Iter: 1190 loss: 1.41539731e-05
Iter: 1191 loss: 1.35843484e-05
Iter: 1192 loss: 1.35561659e-05
Iter: 1193 loss: 1.38428759e-05
Iter: 1194 loss: 1.35554146e-05
Iter: 1195 loss: 1.35317678e-05
Iter: 1196 loss: 1.35151631e-05
Iter: 1197 loss: 1.35063419e-05
Iter: 1198 loss: 1.34998718e-05
Iter: 1199 loss: 1.34876318e-05
Iter: 1200 loss: 1.34701841e-05
Iter: 1201 loss: 1.34558768e-05
Iter: 1202 loss: 1.34498277e-05
Iter: 1203 loss: 1.34281381e-05
Iter: 1204 loss: 1.34397051e-05
Iter: 1205 loss: 1.34122656e-05
Iter: 1206 loss: 1.33784761e-05
Iter: 1207 loss: 1.3522962e-05
Iter: 1208 loss: 1.33717213e-05
Iter: 1209 loss: 1.33399535e-05
Iter: 1210 loss: 1.3309952e-05
Iter: 1211 loss: 1.33027679e-05
Iter: 1212 loss: 1.32573605e-05
Iter: 1213 loss: 1.33978956e-05
Iter: 1214 loss: 1.32444075e-05
Iter: 1215 loss: 1.31945471e-05
Iter: 1216 loss: 1.34529801e-05
Iter: 1217 loss: 1.31870293e-05
Iter: 1218 loss: 1.3139982e-05
Iter: 1219 loss: 1.3209833e-05
Iter: 1220 loss: 1.31184788e-05
Iter: 1221 loss: 1.30847147e-05
Iter: 1222 loss: 1.31056595e-05
Iter: 1223 loss: 1.30634844e-05
Iter: 1224 loss: 1.30314938e-05
Iter: 1225 loss: 1.34888523e-05
Iter: 1226 loss: 1.30316857e-05
Iter: 1227 loss: 1.29979744e-05
Iter: 1228 loss: 1.29660966e-05
Iter: 1229 loss: 1.29577993e-05
Iter: 1230 loss: 1.29468153e-05
Iter: 1231 loss: 1.29346026e-05
Iter: 1232 loss: 1.29107666e-05
Iter: 1233 loss: 1.29542877e-05
Iter: 1234 loss: 1.2900251e-05
Iter: 1235 loss: 1.28807696e-05
Iter: 1236 loss: 1.28666652e-05
Iter: 1237 loss: 1.28591264e-05
Iter: 1238 loss: 1.2820271e-05
Iter: 1239 loss: 1.31505403e-05
Iter: 1240 loss: 1.28186075e-05
Iter: 1241 loss: 1.27863159e-05
Iter: 1242 loss: 1.28310858e-05
Iter: 1243 loss: 1.27713884e-05
Iter: 1244 loss: 1.27424337e-05
Iter: 1245 loss: 1.27510675e-05
Iter: 1246 loss: 1.27221228e-05
Iter: 1247 loss: 1.26841387e-05
Iter: 1248 loss: 1.27912244e-05
Iter: 1249 loss: 1.26710092e-05
Iter: 1250 loss: 1.26234099e-05
Iter: 1251 loss: 1.27960902e-05
Iter: 1252 loss: 1.26113318e-05
Iter: 1253 loss: 1.25793676e-05
Iter: 1254 loss: 1.2521903e-05
Iter: 1255 loss: 1.39280028e-05
Iter: 1256 loss: 1.25223287e-05
Iter: 1257 loss: 1.24836024e-05
Iter: 1258 loss: 1.2483345e-05
Iter: 1259 loss: 1.24641529e-05
Iter: 1260 loss: 1.24640337e-05
Iter: 1261 loss: 1.2452605e-05
Iter: 1262 loss: 1.24216513e-05
Iter: 1263 loss: 1.26037048e-05
Iter: 1264 loss: 1.24132093e-05
Iter: 1265 loss: 1.24283852e-05
Iter: 1266 loss: 1.23985992e-05
Iter: 1267 loss: 1.23884438e-05
Iter: 1268 loss: 1.2375669e-05
Iter: 1269 loss: 1.23758782e-05
Iter: 1270 loss: 1.23454211e-05
Iter: 1271 loss: 1.23684113e-05
Iter: 1272 loss: 1.23265199e-05
Iter: 1273 loss: 1.22787769e-05
Iter: 1274 loss: 1.269929e-05
Iter: 1275 loss: 1.22771e-05
Iter: 1276 loss: 1.22448837e-05
Iter: 1277 loss: 1.22430101e-05
Iter: 1278 loss: 1.22193769e-05
Iter: 1279 loss: 1.21937683e-05
Iter: 1280 loss: 1.21384046e-05
Iter: 1281 loss: 1.30005164e-05
Iter: 1282 loss: 1.21361882e-05
Iter: 1283 loss: 1.2084869e-05
Iter: 1284 loss: 1.25814404e-05
Iter: 1285 loss: 1.20836594e-05
Iter: 1286 loss: 1.20415152e-05
Iter: 1287 loss: 1.20419027e-05
Iter: 1288 loss: 1.20079703e-05
Iter: 1289 loss: 1.19575534e-05
Iter: 1290 loss: 1.20377026e-05
Iter: 1291 loss: 1.19337628e-05
Iter: 1292 loss: 1.19016222e-05
Iter: 1293 loss: 1.22309921e-05
Iter: 1294 loss: 1.190066e-05
Iter: 1295 loss: 1.18706184e-05
Iter: 1296 loss: 1.19809647e-05
Iter: 1297 loss: 1.18643375e-05
Iter: 1298 loss: 1.18614798e-05
Iter: 1299 loss: 1.1849751e-05
Iter: 1300 loss: 1.18405796e-05
Iter: 1301 loss: 1.1834617e-05
Iter: 1302 loss: 1.18326e-05
Iter: 1303 loss: 1.18165972e-05
Iter: 1304 loss: 1.17766849e-05
Iter: 1305 loss: 1.21142075e-05
Iter: 1306 loss: 1.17703148e-05
Iter: 1307 loss: 1.17416157e-05
Iter: 1308 loss: 1.17409e-05
Iter: 1309 loss: 1.17126765e-05
Iter: 1310 loss: 1.17916579e-05
Iter: 1311 loss: 1.17039563e-05
Iter: 1312 loss: 1.16790261e-05
Iter: 1313 loss: 1.17672898e-05
Iter: 1314 loss: 1.16726442e-05
Iter: 1315 loss: 1.16564788e-05
Iter: 1316 loss: 1.16846022e-05
Iter: 1317 loss: 1.1649031e-05
Iter: 1318 loss: 1.16237834e-05
Iter: 1319 loss: 1.16273786e-05
Iter: 1320 loss: 1.16038027e-05
Iter: 1321 loss: 1.15737676e-05
Iter: 1322 loss: 1.16300425e-05
Iter: 1323 loss: 1.15605517e-05
Iter: 1324 loss: 1.15277926e-05
Iter: 1325 loss: 1.15406892e-05
Iter: 1326 loss: 1.15048852e-05
Iter: 1327 loss: 1.14751983e-05
Iter: 1328 loss: 1.19354354e-05
Iter: 1329 loss: 1.1474729e-05
Iter: 1330 loss: 1.14622053e-05
Iter: 1331 loss: 1.15803396e-05
Iter: 1332 loss: 1.14609e-05
Iter: 1333 loss: 1.14461354e-05
Iter: 1334 loss: 1.14301047e-05
Iter: 1335 loss: 1.14266968e-05
Iter: 1336 loss: 1.14076556e-05
Iter: 1337 loss: 1.15181265e-05
Iter: 1338 loss: 1.14049117e-05
Iter: 1339 loss: 1.13892966e-05
Iter: 1340 loss: 1.13446513e-05
Iter: 1341 loss: 1.15233624e-05
Iter: 1342 loss: 1.13261112e-05
Iter: 1343 loss: 1.12839916e-05
Iter: 1344 loss: 1.15639295e-05
Iter: 1345 loss: 1.12804046e-05
Iter: 1346 loss: 1.12617035e-05
Iter: 1347 loss: 1.12552771e-05
Iter: 1348 loss: 1.12427506e-05
Iter: 1349 loss: 1.12257294e-05
Iter: 1350 loss: 1.12237876e-05
Iter: 1351 loss: 1.12007183e-05
Iter: 1352 loss: 1.13203587e-05
Iter: 1353 loss: 1.11971485e-05
Iter: 1354 loss: 1.11760583e-05
Iter: 1355 loss: 1.11601539e-05
Iter: 1356 loss: 1.11535501e-05
Iter: 1357 loss: 1.1119957e-05
Iter: 1358 loss: 1.12254529e-05
Iter: 1359 loss: 1.11091285e-05
Iter: 1360 loss: 1.10858628e-05
Iter: 1361 loss: 1.10863293e-05
Iter: 1362 loss: 1.10666933e-05
Iter: 1363 loss: 1.10927176e-05
Iter: 1364 loss: 1.10557148e-05
Iter: 1365 loss: 1.10462352e-05
Iter: 1366 loss: 1.10366209e-05
Iter: 1367 loss: 1.10348556e-05
Iter: 1368 loss: 1.10171877e-05
Iter: 1369 loss: 1.10638157e-05
Iter: 1370 loss: 1.10117e-05
Iter: 1371 loss: 1.099e-05
Iter: 1372 loss: 1.10421679e-05
Iter: 1373 loss: 1.09818229e-05
Iter: 1374 loss: 1.09656321e-05
Iter: 1375 loss: 1.09382981e-05
Iter: 1376 loss: 1.09384891e-05
Iter: 1377 loss: 1.09280527e-05
Iter: 1378 loss: 1.09177863e-05
Iter: 1379 loss: 1.09018529e-05
Iter: 1380 loss: 1.09020475e-05
Iter: 1381 loss: 1.08884597e-05
Iter: 1382 loss: 1.08553468e-05
Iter: 1383 loss: 1.08978384e-05
Iter: 1384 loss: 1.08386412e-05
Iter: 1385 loss: 1.08040067e-05
Iter: 1386 loss: 1.12679572e-05
Iter: 1387 loss: 1.08041868e-05
Iter: 1388 loss: 1.0783715e-05
Iter: 1389 loss: 1.0777263e-05
Iter: 1390 loss: 1.0765586e-05
Iter: 1391 loss: 1.07258675e-05
Iter: 1392 loss: 1.08762288e-05
Iter: 1393 loss: 1.07163214e-05
Iter: 1394 loss: 1.0766682e-05
Iter: 1395 loss: 1.0709844e-05
Iter: 1396 loss: 1.07052492e-05
Iter: 1397 loss: 1.06939424e-05
Iter: 1398 loss: 1.08214008e-05
Iter: 1399 loss: 1.0692811e-05
Iter: 1400 loss: 1.06767111e-05
Iter: 1401 loss: 1.06478446e-05
Iter: 1402 loss: 1.06477019e-05
Iter: 1403 loss: 1.06189127e-05
Iter: 1404 loss: 1.09044931e-05
Iter: 1405 loss: 1.06178e-05
Iter: 1406 loss: 1.05892268e-05
Iter: 1407 loss: 1.05663457e-05
Iter: 1408 loss: 1.05583513e-05
Iter: 1409 loss: 1.05489544e-05
Iter: 1410 loss: 1.05328763e-05
Iter: 1411 loss: 1.05225954e-05
Iter: 1412 loss: 1.05070412e-05
Iter: 1413 loss: 1.05065956e-05
Iter: 1414 loss: 1.04855508e-05
Iter: 1415 loss: 1.07645119e-05
Iter: 1416 loss: 1.04856617e-05
Iter: 1417 loss: 1.04739975e-05
Iter: 1418 loss: 1.04644341e-05
Iter: 1419 loss: 1.04613337e-05
Iter: 1420 loss: 1.04437331e-05
Iter: 1421 loss: 1.04433766e-05
Iter: 1422 loss: 1.04285937e-05
Iter: 1423 loss: 1.04100645e-05
Iter: 1424 loss: 1.05363033e-05
Iter: 1425 loss: 1.04076589e-05
Iter: 1426 loss: 1.03904295e-05
Iter: 1427 loss: 1.04882274e-05
Iter: 1428 loss: 1.03883167e-05
Iter: 1429 loss: 1.03671555e-05
Iter: 1430 loss: 1.04526116e-05
Iter: 1431 loss: 1.03633265e-05
Iter: 1432 loss: 1.03566881e-05
Iter: 1433 loss: 1.03450793e-05
Iter: 1434 loss: 1.05441832e-05
Iter: 1435 loss: 1.03446282e-05
Iter: 1436 loss: 1.03248703e-05
Iter: 1437 loss: 1.03364164e-05
Iter: 1438 loss: 1.03124567e-05
Iter: 1439 loss: 1.02948143e-05
Iter: 1440 loss: 1.05361269e-05
Iter: 1441 loss: 1.02948543e-05
Iter: 1442 loss: 1.02829508e-05
Iter: 1443 loss: 1.03810562e-05
Iter: 1444 loss: 1.02819176e-05
Iter: 1445 loss: 1.02718604e-05
Iter: 1446 loss: 1.02909971e-05
Iter: 1447 loss: 1.0267313e-05
Iter: 1448 loss: 1.02578324e-05
Iter: 1449 loss: 1.02888152e-05
Iter: 1450 loss: 1.02548211e-05
Iter: 1451 loss: 1.02412569e-05
Iter: 1452 loss: 1.02298554e-05
Iter: 1453 loss: 1.02253907e-05
Iter: 1454 loss: 1.02017693e-05
Iter: 1455 loss: 1.02668291e-05
Iter: 1456 loss: 1.01940886e-05
Iter: 1457 loss: 1.01709511e-05
Iter: 1458 loss: 1.02130671e-05
Iter: 1459 loss: 1.01609612e-05
Iter: 1460 loss: 1.01416927e-05
Iter: 1461 loss: 1.0367552e-05
Iter: 1462 loss: 1.01418436e-05
Iter: 1463 loss: 1.01286732e-05
Iter: 1464 loss: 1.01282531e-05
Iter: 1465 loss: 1.01230162e-05
Iter: 1466 loss: 1.01051883e-05
Iter: 1467 loss: 1.01178975e-05
Iter: 1468 loss: 1.00906454e-05
Iter: 1469 loss: 1.00693433e-05
Iter: 1470 loss: 1.02531067e-05
Iter: 1471 loss: 1.0068512e-05
Iter: 1472 loss: 1.00487305e-05
Iter: 1473 loss: 1.0048223e-05
Iter: 1474 loss: 1.0032496e-05
Iter: 1475 loss: 1.00130355e-05
Iter: 1476 loss: 1.00119978e-05
Iter: 1477 loss: 9.99677832e-06
Iter: 1478 loss: 1.00344678e-05
Iter: 1479 loss: 9.99001804e-06
Iter: 1480 loss: 9.97348798e-06
Iter: 1481 loss: 1.0001384e-05
Iter: 1482 loss: 9.96570816e-06
Iter: 1483 loss: 9.93987123e-06
Iter: 1484 loss: 9.97927418e-06
Iter: 1485 loss: 9.92831337e-06
Iter: 1486 loss: 9.90693297e-06
Iter: 1487 loss: 1.00157722e-05
Iter: 1488 loss: 9.90247463e-06
Iter: 1489 loss: 9.88823e-06
Iter: 1490 loss: 9.92035075e-06
Iter: 1491 loss: 9.88211195e-06
Iter: 1492 loss: 9.8663113e-06
Iter: 1493 loss: 9.84203143e-06
Iter: 1494 loss: 9.84156668e-06
Iter: 1495 loss: 9.82990059e-06
Iter: 1496 loss: 9.82906e-06
Iter: 1497 loss: 9.81500671e-06
Iter: 1498 loss: 9.85088082e-06
Iter: 1499 loss: 9.80975892e-06
Iter: 1500 loss: 9.80273126e-06
Iter: 1501 loss: 9.78583739e-06
Iter: 1502 loss: 1.00089e-05
Iter: 1503 loss: 9.78465141e-06
Iter: 1504 loss: 9.76707452e-06
Iter: 1505 loss: 9.7988177e-06
Iter: 1506 loss: 9.75990952e-06
Iter: 1507 loss: 9.73978877e-06
Iter: 1508 loss: 9.9617173e-06
Iter: 1509 loss: 9.73958504e-06
Iter: 1510 loss: 9.72526686e-06
Iter: 1511 loss: 9.84324288e-06
Iter: 1512 loss: 9.72467387e-06
Iter: 1513 loss: 9.71316695e-06
Iter: 1514 loss: 9.69830307e-06
Iter: 1515 loss: 9.69717075e-06
Iter: 1516 loss: 9.6889089e-06
Iter: 1517 loss: 9.68844142e-06
Iter: 1518 loss: 9.67953292e-06
Iter: 1519 loss: 9.66894368e-06
Iter: 1520 loss: 9.66836342e-06
Iter: 1521 loss: 9.65263916e-06
Iter: 1522 loss: 9.72438374e-06
Iter: 1523 loss: 9.64931314e-06
Iter: 1524 loss: 9.63577077e-06
Iter: 1525 loss: 9.6594049e-06
Iter: 1526 loss: 9.62948798e-06
Iter: 1527 loss: 9.61474507e-06
Iter: 1528 loss: 9.62402373e-06
Iter: 1529 loss: 9.605179e-06
Iter: 1530 loss: 9.60867874e-06
Iter: 1531 loss: 9.60117632e-06
Iter: 1532 loss: 9.59582212e-06
Iter: 1533 loss: 9.57908742e-06
Iter: 1534 loss: 9.60410216e-06
Iter: 1535 loss: 9.56796885e-06
Iter: 1536 loss: 9.55423093e-06
Iter: 1537 loss: 9.59536828e-06
Iter: 1538 loss: 9.55023734e-06
Iter: 1539 loss: 9.53167364e-06
Iter: 1540 loss: 9.506065e-06
Iter: 1541 loss: 9.50498361e-06
Iter: 1542 loss: 9.51867696e-06
Iter: 1543 loss: 9.49439891e-06
Iter: 1544 loss: 9.48552133e-06
Iter: 1545 loss: 9.47111675e-06
Iter: 1546 loss: 9.47134413e-06
Iter: 1547 loss: 9.45657484e-06
Iter: 1548 loss: 9.47206354e-06
Iter: 1549 loss: 9.44866952e-06
Iter: 1550 loss: 9.4317129e-06
Iter: 1551 loss: 9.65384515e-06
Iter: 1552 loss: 9.43202394e-06
Iter: 1553 loss: 9.42039242e-06
Iter: 1554 loss: 9.40994141e-06
Iter: 1555 loss: 9.40737664e-06
Iter: 1556 loss: 9.38718e-06
Iter: 1557 loss: 9.54272582e-06
Iter: 1558 loss: 9.38668381e-06
Iter: 1559 loss: 9.37092591e-06
Iter: 1560 loss: 9.38599806e-06
Iter: 1561 loss: 9.36152537e-06
Iter: 1562 loss: 9.35214484e-06
Iter: 1563 loss: 9.35232856e-06
Iter: 1564 loss: 9.34168656e-06
Iter: 1565 loss: 9.34598393e-06
Iter: 1566 loss: 9.33408046e-06
Iter: 1567 loss: 9.32425792e-06
Iter: 1568 loss: 9.31064096e-06
Iter: 1569 loss: 9.30986607e-06
Iter: 1570 loss: 9.28961708e-06
Iter: 1571 loss: 9.30139504e-06
Iter: 1572 loss: 9.27643487e-06
Iter: 1573 loss: 9.25682252e-06
Iter: 1574 loss: 9.26753455e-06
Iter: 1575 loss: 9.24416781e-06
Iter: 1576 loss: 9.2432565e-06
Iter: 1577 loss: 9.23234802e-06
Iter: 1578 loss: 9.22527943e-06
Iter: 1579 loss: 9.21299215e-06
Iter: 1580 loss: 9.49594505e-06
Iter: 1581 loss: 9.21242372e-06
Iter: 1582 loss: 9.20590355e-06
Iter: 1583 loss: 9.20445473e-06
Iter: 1584 loss: 9.19702143e-06
Iter: 1585 loss: 9.19722333e-06
Iter: 1586 loss: 9.19030117e-06
Iter: 1587 loss: 9.17841862e-06
Iter: 1588 loss: 9.18833484e-06
Iter: 1589 loss: 9.17111174e-06
Iter: 1590 loss: 9.16002682e-06
Iter: 1591 loss: 9.29313865e-06
Iter: 1592 loss: 9.15977e-06
Iter: 1593 loss: 9.15138389e-06
Iter: 1594 loss: 9.13713302e-06
Iter: 1595 loss: 9.13729946e-06
Iter: 1596 loss: 9.13250733e-06
Iter: 1597 loss: 9.12628821e-06
Iter: 1598 loss: 9.12228461e-06
Iter: 1599 loss: 9.11434927e-06
Iter: 1600 loss: 9.27422388e-06
Iter: 1601 loss: 9.11438838e-06
Iter: 1602 loss: 9.10356266e-06
Iter: 1603 loss: 9.09091796e-06
Iter: 1604 loss: 9.09010123e-06
Iter: 1605 loss: 9.0681242e-06
Iter: 1606 loss: 9.07759932e-06
Iter: 1607 loss: 9.05204797e-06
Iter: 1608 loss: 9.03699583e-06
Iter: 1609 loss: 9.0352878e-06
Iter: 1610 loss: 9.01597741e-06
Iter: 1611 loss: 9.00615669e-06
Iter: 1612 loss: 8.99799124e-06
Iter: 1613 loss: 8.97831251e-06
Iter: 1614 loss: 9.05535e-06
Iter: 1615 loss: 8.97422069e-06
Iter: 1616 loss: 8.95071935e-06
Iter: 1617 loss: 9.07139838e-06
Iter: 1618 loss: 8.94750519e-06
Iter: 1619 loss: 8.93257129e-06
Iter: 1620 loss: 8.94614823e-06
Iter: 1621 loss: 8.92378193e-06
Iter: 1622 loss: 8.9085388e-06
Iter: 1623 loss: 9.06049172e-06
Iter: 1624 loss: 8.90874071e-06
Iter: 1625 loss: 8.8935285e-06
Iter: 1626 loss: 8.89166e-06
Iter: 1627 loss: 8.88149407e-06
Iter: 1628 loss: 8.88032537e-06
Iter: 1629 loss: 8.87430087e-06
Iter: 1630 loss: 8.86853e-06
Iter: 1631 loss: 8.85543614e-06
Iter: 1632 loss: 8.99951738e-06
Iter: 1633 loss: 8.85488407e-06
Iter: 1634 loss: 8.84015208e-06
Iter: 1635 loss: 8.8762863e-06
Iter: 1636 loss: 8.83553184e-06
Iter: 1637 loss: 8.82467248e-06
Iter: 1638 loss: 8.82770291e-06
Iter: 1639 loss: 8.81676351e-06
Iter: 1640 loss: 8.80543848e-06
Iter: 1641 loss: 8.89563125e-06
Iter: 1642 loss: 8.80468815e-06
Iter: 1643 loss: 8.79264553e-06
Iter: 1644 loss: 8.87606438e-06
Iter: 1645 loss: 8.79139589e-06
Iter: 1646 loss: 8.78563515e-06
Iter: 1647 loss: 8.7770768e-06
Iter: 1648 loss: 8.77714137e-06
Iter: 1649 loss: 8.76734339e-06
Iter: 1650 loss: 8.76692411e-06
Iter: 1651 loss: 8.76073682e-06
Iter: 1652 loss: 8.75337628e-06
Iter: 1653 loss: 8.75279e-06
Iter: 1654 loss: 8.7379949e-06
Iter: 1655 loss: 8.71972225e-06
Iter: 1656 loss: 8.71834072e-06
Iter: 1657 loss: 8.74303e-06
Iter: 1658 loss: 8.70826443e-06
Iter: 1659 loss: 8.70363328e-06
Iter: 1660 loss: 8.69574797e-06
Iter: 1661 loss: 8.6958371e-06
Iter: 1662 loss: 8.67891322e-06
Iter: 1663 loss: 8.68069674e-06
Iter: 1664 loss: 8.66615483e-06
Iter: 1665 loss: 8.65460061e-06
Iter: 1666 loss: 8.66131631e-06
Iter: 1667 loss: 8.64659523e-06
Iter: 1668 loss: 8.63165405e-06
Iter: 1669 loss: 8.63781861e-06
Iter: 1670 loss: 8.62250454e-06
Iter: 1671 loss: 8.60434739e-06
Iter: 1672 loss: 8.63410605e-06
Iter: 1673 loss: 8.5954207e-06
Iter: 1674 loss: 8.58551e-06
Iter: 1675 loss: 8.58412386e-06
Iter: 1676 loss: 8.57385e-06
Iter: 1677 loss: 8.56030147e-06
Iter: 1678 loss: 8.55852e-06
Iter: 1679 loss: 8.54454902e-06
Iter: 1680 loss: 8.58361091e-06
Iter: 1681 loss: 8.54088194e-06
Iter: 1682 loss: 8.52006633e-06
Iter: 1683 loss: 8.6029e-06
Iter: 1684 loss: 8.51588084e-06
Iter: 1685 loss: 8.50633114e-06
Iter: 1686 loss: 8.48779473e-06
Iter: 1687 loss: 8.85172267e-06
Iter: 1688 loss: 8.48771379e-06
Iter: 1689 loss: 8.47922638e-06
Iter: 1690 loss: 8.47695901e-06
Iter: 1691 loss: 8.46747753e-06
Iter: 1692 loss: 8.45443356e-06
Iter: 1693 loss: 8.45404156e-06
Iter: 1694 loss: 8.45139584e-06
Iter: 1695 loss: 8.44636634e-06
Iter: 1696 loss: 8.44268652e-06
Iter: 1697 loss: 8.43318867e-06
Iter: 1698 loss: 8.48253421e-06
Iter: 1699 loss: 8.43011185e-06
Iter: 1700 loss: 8.41267774e-06
Iter: 1701 loss: 8.40868688e-06
Iter: 1702 loss: 8.39801e-06
Iter: 1703 loss: 8.3865707e-06
Iter: 1704 loss: 8.37367952e-06
Iter: 1705 loss: 8.37222797e-06
Iter: 1706 loss: 8.34786442e-06
Iter: 1707 loss: 8.42685495e-06
Iter: 1708 loss: 8.34003367e-06
Iter: 1709 loss: 8.33590639e-06
Iter: 1710 loss: 8.33150443e-06
Iter: 1711 loss: 8.32374099e-06
Iter: 1712 loss: 8.3068262e-06
Iter: 1713 loss: 8.57366649e-06
Iter: 1714 loss: 8.30633508e-06
Iter: 1715 loss: 8.29987857e-06
Iter: 1716 loss: 8.29650708e-06
Iter: 1717 loss: 8.29167766e-06
Iter: 1718 loss: 8.27705753e-06
Iter: 1719 loss: 8.31897341e-06
Iter: 1720 loss: 8.26962059e-06
Iter: 1721 loss: 8.25379539e-06
Iter: 1722 loss: 8.45364684e-06
Iter: 1723 loss: 8.25364714e-06
Iter: 1724 loss: 8.24174185e-06
Iter: 1725 loss: 8.24132712e-06
Iter: 1726 loss: 8.23582559e-06
Iter: 1727 loss: 8.22654511e-06
Iter: 1728 loss: 8.22662696e-06
Iter: 1729 loss: 8.21217873e-06
Iter: 1730 loss: 8.30021418e-06
Iter: 1731 loss: 8.21064896e-06
Iter: 1732 loss: 8.20422247e-06
Iter: 1733 loss: 8.18424269e-06
Iter: 1734 loss: 8.22539459e-06
Iter: 1735 loss: 8.17143609e-06
Iter: 1736 loss: 8.17075306e-06
Iter: 1737 loss: 8.16054671e-06
Iter: 1738 loss: 8.15093426e-06
Iter: 1739 loss: 8.14687337e-06
Iter: 1740 loss: 8.14219584e-06
Iter: 1741 loss: 8.12947928e-06
Iter: 1742 loss: 8.23065784e-06
Iter: 1743 loss: 8.12858707e-06
Iter: 1744 loss: 8.11843893e-06
Iter: 1745 loss: 8.14418308e-06
Iter: 1746 loss: 8.11496193e-06
Iter: 1747 loss: 8.10413803e-06
Iter: 1748 loss: 8.08901223e-06
Iter: 1749 loss: 8.08849654e-06
Iter: 1750 loss: 8.07447759e-06
Iter: 1751 loss: 8.08160348e-06
Iter: 1752 loss: 8.06425396e-06
Iter: 1753 loss: 8.05405216e-06
Iter: 1754 loss: 8.19062825e-06
Iter: 1755 loss: 8.05402487e-06
Iter: 1756 loss: 8.04701449e-06
Iter: 1757 loss: 8.08624645e-06
Iter: 1758 loss: 8.04647061e-06
Iter: 1759 loss: 8.03899275e-06
Iter: 1760 loss: 8.02652812e-06
Iter: 1761 loss: 8.33570812e-06
Iter: 1762 loss: 8.02646173e-06
Iter: 1763 loss: 8.01187707e-06
Iter: 1764 loss: 8.16775355e-06
Iter: 1765 loss: 8.01150236e-06
Iter: 1766 loss: 7.99987e-06
Iter: 1767 loss: 7.98003748e-06
Iter: 1768 loss: 7.97988832e-06
Iter: 1769 loss: 7.96645872e-06
Iter: 1770 loss: 8.08591722e-06
Iter: 1771 loss: 7.96509266e-06
Iter: 1772 loss: 7.9528445e-06
Iter: 1773 loss: 7.98995916e-06
Iter: 1774 loss: 7.94885727e-06
Iter: 1775 loss: 7.94069547e-06
Iter: 1776 loss: 7.97014854e-06
Iter: 1777 loss: 7.93858817e-06
Iter: 1778 loss: 7.92724495e-06
Iter: 1779 loss: 7.93058553e-06
Iter: 1780 loss: 7.91955426e-06
Iter: 1781 loss: 7.90481317e-06
Iter: 1782 loss: 7.96756467e-06
Iter: 1783 loss: 7.90118611e-06
Iter: 1784 loss: 7.89060687e-06
Iter: 1785 loss: 7.89083606e-06
Iter: 1786 loss: 7.88138459e-06
Iter: 1787 loss: 7.86623696e-06
Iter: 1788 loss: 7.92015089e-06
Iter: 1789 loss: 7.86181317e-06
Iter: 1790 loss: 7.84929853e-06
Iter: 1791 loss: 7.84947861e-06
Iter: 1792 loss: 7.83943415e-06
Iter: 1793 loss: 7.8652929e-06
Iter: 1794 loss: 7.83555151e-06
Iter: 1795 loss: 7.82864117e-06
Iter: 1796 loss: 7.83359428e-06
Iter: 1797 loss: 7.82467578e-06
Iter: 1798 loss: 7.81292329e-06
Iter: 1799 loss: 7.82041207e-06
Iter: 1800 loss: 7.80543e-06
Iter: 1801 loss: 7.79555e-06
Iter: 1802 loss: 7.78249523e-06
Iter: 1803 loss: 7.78193316e-06
Iter: 1804 loss: 7.7689474e-06
Iter: 1805 loss: 7.76889283e-06
Iter: 1806 loss: 7.75830267e-06
Iter: 1807 loss: 7.74830369e-06
Iter: 1808 loss: 7.74580531e-06
Iter: 1809 loss: 7.73330248e-06
Iter: 1810 loss: 7.73320335e-06
Iter: 1811 loss: 7.72497151e-06
Iter: 1812 loss: 7.71849227e-06
Iter: 1813 loss: 7.71611394e-06
Iter: 1814 loss: 7.70655788e-06
Iter: 1815 loss: 7.7431414e-06
Iter: 1816 loss: 7.70465158e-06
Iter: 1817 loss: 7.69908729e-06
Iter: 1818 loss: 7.68985592e-06
Iter: 1819 loss: 7.68984e-06
Iter: 1820 loss: 7.67771053e-06
Iter: 1821 loss: 7.83947871e-06
Iter: 1822 loss: 7.67751e-06
Iter: 1823 loss: 7.67294478e-06
Iter: 1824 loss: 7.7408049e-06
Iter: 1825 loss: 7.67308484e-06
Iter: 1826 loss: 7.66932e-06
Iter: 1827 loss: 7.66740777e-06
Iter: 1828 loss: 7.66586e-06
Iter: 1829 loss: 7.65809e-06
Iter: 1830 loss: 7.67191e-06
Iter: 1831 loss: 7.65495e-06
Iter: 1832 loss: 7.6484e-06
Iter: 1833 loss: 7.63645585e-06
Iter: 1834 loss: 7.63611479e-06
Iter: 1835 loss: 7.62475202e-06
Iter: 1836 loss: 7.71138e-06
Iter: 1837 loss: 7.62345098e-06
Iter: 1838 loss: 7.61025512e-06
Iter: 1839 loss: 7.62496666e-06
Iter: 1840 loss: 7.60349531e-06
Iter: 1841 loss: 7.59543673e-06
Iter: 1842 loss: 7.6833694e-06
Iter: 1843 loss: 7.59621889e-06
Iter: 1844 loss: 7.58706165e-06
Iter: 1845 loss: 7.57097587e-06
Iter: 1846 loss: 7.91698585e-06
Iter: 1847 loss: 7.57065072e-06
Iter: 1848 loss: 7.56181726e-06
Iter: 1849 loss: 7.55971814e-06
Iter: 1850 loss: 7.55274186e-06
Iter: 1851 loss: 7.53383847e-06
Iter: 1852 loss: 7.69006692e-06
Iter: 1853 loss: 7.53028326e-06
Iter: 1854 loss: 7.5394114e-06
Iter: 1855 loss: 7.52145297e-06
Iter: 1856 loss: 7.51656262e-06
Iter: 1857 loss: 7.50606e-06
Iter: 1858 loss: 7.67147867e-06
Iter: 1859 loss: 7.50590561e-06
Iter: 1860 loss: 7.48975117e-06
Iter: 1861 loss: 7.56028385e-06
Iter: 1862 loss: 7.48583352e-06
Iter: 1863 loss: 7.48430102e-06
Iter: 1864 loss: 7.48050752e-06
Iter: 1865 loss: 7.47431886e-06
Iter: 1866 loss: 7.46783917e-06
Iter: 1867 loss: 7.46744126e-06
Iter: 1868 loss: 7.45723764e-06
Iter: 1869 loss: 7.44324825e-06
Iter: 1870 loss: 7.4422851e-06
Iter: 1871 loss: 7.42489101e-06
Iter: 1872 loss: 7.47608237e-06
Iter: 1873 loss: 7.41947497e-06
Iter: 1874 loss: 7.40441919e-06
Iter: 1875 loss: 7.58833858e-06
Iter: 1876 loss: 7.40402811e-06
Iter: 1877 loss: 7.39515326e-06
Iter: 1878 loss: 7.41799386e-06
Iter: 1879 loss: 7.39183815e-06
Iter: 1880 loss: 7.38061317e-06
Iter: 1881 loss: 7.36863149e-06
Iter: 1882 loss: 7.36651236e-06
Iter: 1883 loss: 7.35068897e-06
Iter: 1884 loss: 7.35093545e-06
Iter: 1885 loss: 7.34358946e-06
Iter: 1886 loss: 7.32217222e-06
Iter: 1887 loss: 7.43267356e-06
Iter: 1888 loss: 7.31497494e-06
Iter: 1889 loss: 7.30223701e-06
Iter: 1890 loss: 7.30181455e-06
Iter: 1891 loss: 7.28624082e-06
Iter: 1892 loss: 7.32656281e-06
Iter: 1893 loss: 7.28118266e-06
Iter: 1894 loss: 7.27492943e-06
Iter: 1895 loss: 7.26580674e-06
Iter: 1896 loss: 7.26557619e-06
Iter: 1897 loss: 7.26200733e-06
Iter: 1898 loss: 7.25719792e-06
Iter: 1899 loss: 7.24810707e-06
Iter: 1900 loss: 7.26999906e-06
Iter: 1901 loss: 7.24449637e-06
Iter: 1902 loss: 7.23237099e-06
Iter: 1903 loss: 7.2257335e-06
Iter: 1904 loss: 7.22004415e-06
Iter: 1905 loss: 7.20905473e-06
Iter: 1906 loss: 7.20847402e-06
Iter: 1907 loss: 7.19715e-06
Iter: 1908 loss: 7.21306833e-06
Iter: 1909 loss: 7.19222589e-06
Iter: 1910 loss: 7.18022102e-06
Iter: 1911 loss: 7.22024515e-06
Iter: 1912 loss: 7.17771036e-06
Iter: 1913 loss: 7.16914974e-06
Iter: 1914 loss: 7.17367584e-06
Iter: 1915 loss: 7.16334853e-06
Iter: 1916 loss: 7.15112355e-06
Iter: 1917 loss: 7.20831667e-06
Iter: 1918 loss: 7.14870293e-06
Iter: 1919 loss: 7.13343e-06
Iter: 1920 loss: 7.15601527e-06
Iter: 1921 loss: 7.12640303e-06
Iter: 1922 loss: 7.11370558e-06
Iter: 1923 loss: 7.1275872e-06
Iter: 1924 loss: 7.10650875e-06
Iter: 1925 loss: 7.09695814e-06
Iter: 1926 loss: 7.09663482e-06
Iter: 1927 loss: 7.09117194e-06
Iter: 1928 loss: 7.08601829e-06
Iter: 1929 loss: 7.08487278e-06
Iter: 1930 loss: 7.07042955e-06
Iter: 1931 loss: 7.14429e-06
Iter: 1932 loss: 7.0679107e-06
Iter: 1933 loss: 7.06443e-06
Iter: 1934 loss: 7.05434331e-06
Iter: 1935 loss: 7.09023107e-06
Iter: 1936 loss: 7.0500555e-06
Iter: 1937 loss: 7.03565456e-06
Iter: 1938 loss: 7.07634445e-06
Iter: 1939 loss: 7.03147089e-06
Iter: 1940 loss: 7.02216903e-06
Iter: 1941 loss: 7.14914586e-06
Iter: 1942 loss: 7.02255e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi1.6/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi2
+ date
Sun Nov  8 17:55:54 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b848400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b861730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b7f4d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b754730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b754268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b753950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b754598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b6dbae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b6db840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b6db6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b66f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b682f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b5df840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b5741e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b569620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b6eebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b5f78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b64eea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b6a11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b64e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b4d9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b4c2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b4d9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b64f048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b546620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa07a00dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa09b553510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa07a00d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa079f7d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa079efb0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa079eef400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa079f360d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa079f366a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa079f1dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0545ee1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa0545a6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 1.9966216
test_loss: 2.0033963
train_loss: 2.0083997
test_loss: 1.9975841
train_loss: 1.9993957
test_loss: 1.9983679
train_loss: 1.9981891
test_loss: 1.9983733
train_loss: 1.9982054
test_loss: 1.9976069
train_loss: 1.998494
test_loss: 1.9969194
train_loss: 1.9991436
test_loss: 2.021733
train_loss: 1.997785
test_loss: 1.995891
train_loss: 1.9921166
test_loss: 1.9956453
train_loss: 1.9984663
test_loss: 1.9969611
train_loss: 1.9976294
test_loss: 1.9964161
train_loss: 1.9968843
test_loss: 1.9973835
train_loss: 1.993947
test_loss: 1.996946
train_loss: 1.9922194
test_loss: 1.997586
train_loss: 1.9951805
test_loss: 1.997165
train_loss: 1.9915154
test_loss: 1.9968194
train_loss: 1.9944963
test_loss: 1.9969381
train_loss: 1.99192
test_loss: 1.996515
train_loss: 1.994995
test_loss: 1.997164
train_loss: 1.9912
test_loss: 1.9975231
train_loss: 1.992662
test_loss: 1.9968565
train_loss: 1.9954197
test_loss: 1.9980264
train_loss: 1.991204
test_loss: 1.9978253
train_loss: 1.99238
test_loss: 1.9968631
train_loss: 1.991786
test_loss: 1.9975077
train_loss: 1.9932045
test_loss: 1.9957794
train_loss: 1.9932638
test_loss: 1.9973577
train_loss: 1.9923055
test_loss: 1.9965874
train_loss: 1.9893861
test_loss: 1.9971712
train_loss: 1.9913915
test_loss: 1.9969348
train_loss: 1.9915462
test_loss: 1.9971596
train_loss: 1.9915859
test_loss: 1.9978741
train_loss: 1.9902457
test_loss: 1.9992123
train_loss: 1.9993968
test_loss: 1.9960005
train_loss: 1.9940329
test_loss: 1.996654
train_loss: 1.9959878
test_loss: 1.9966574
train_loss: 1.9982055
test_loss: 1.996545
train_loss: 1.9933846
test_loss: 1.9964968
train_loss: 1.9926932
test_loss: 1.9968855
train_loss: 1.9956539
test_loss: 1.9953731
train_loss: 1.9928374
test_loss: 1.9948646
train_loss: 1.9953485
test_loss: 1.9960814
train_loss: 1.9947506
test_loss: 1.9968394
train_loss: 1.9981353
test_loss: 1.9942551
train_loss: 1.9961654
test_loss: 1.9960233
train_loss: 0.9628587
test_loss: 0.9913236
train_loss: 0.975467
test_loss: 0.9919135
train_loss: 0.977656
test_loss: 0.99153006
train_loss: 0.97896457
test_loss: 0.9911385
train_loss: 0.9634063
test_loss: 0.99077743
train_loss: 0.97935283
test_loss: 0.9903694
train_loss: 0.9776237
test_loss: 0.98988307
train_loss: 0.960079
test_loss: 0.9894828
train_loss: 0.9641493
test_loss: 0.98893756
train_loss: 0.9718827
test_loss: 0.9884722
train_loss: 0.9827312
test_loss: 0.98790973
train_loss: 0.9904506
test_loss: 0.9873295
train_loss: 0.99023163
test_loss: 0.986763
train_loss: 0.94902885
test_loss: 0.9861963
train_loss: 0.95893764
test_loss: 0.98546773
train_loss: 0.97655153
test_loss: 0.98475426
train_loss: 0.9591582
test_loss: 0.9840585
train_loss: 0.96906847
test_loss: 0.9833428
train_loss: 0.9682096
test_loss: 0.98244756
train_loss: 0.96094674
test_loss: 0.9816047
train_loss: 0.96707475
test_loss: 0.9807612
train_loss: 0.9581495
test_loss: 0.97981834
train_loss: 0.9570561
test_loss: 0.97878045
train_loss: 0.9370196
test_loss: 0.97776484
train_loss: 0.9529043
test_loss: 0.9766537
train_loss: 0.9716172
test_loss: 0.97549
train_loss: 0.95676494
test_loss: 0.9741219
train_loss: 0.94586354
test_loss: 0.97288346
train_loss: 0.94872314
test_loss: 0.9714397
train_loss: 0.9625615
test_loss: 0.9699182
train_loss: 0.9501992
test_loss: 0.9683571
train_loss: 0.95003533
test_loss: 0.9667064
train_loss: 0.9453477
test_loss: 0.96495336
train_loss: 0.9605502
test_loss: 0.9631247
train_loss: 0.9501947
test_loss: 0.9610997
train_loss: 0.9404166
test_loss: 0.9589909
train_loss: 0.9369618
test_loss: 0.9567101
train_loss: 0.94037986
test_loss: 0.9541631
train_loss: 0.9408305
test_loss: 0.9516374
train_loss: 0.92871034
test_loss: 0.94887835
train_loss: 0.9476006
test_loss: 0.94574356
train_loss: 0.9311605
test_loss: 0.94261765
train_loss: 0.93711877
test_loss: 0.9391367
train_loss: 0.9267431
test_loss: 0.9352326
train_loss: 0.92793787
test_loss: 0.9310451
train_loss: 0.94131947
test_loss: 0.9262997
train_loss: 0.9153664
test_loss: 0.9212316
train_loss: 0.8924988
test_loss: 0.9154202
train_loss: 0.89433813
test_loss: 0.908745
train_loss: 0.9028803
test_loss: 0.9011553
train_loss: 0.87334514
test_loss: 0.89208275
train_loss: 0.8725991
test_loss: 0.8810432
train_loss: 0.85229063
test_loss: 0.86700577
train_loss: 0.8443297
test_loss: 0.8484374
train_loss: 0.8149255
test_loss: 0.8232701
train_loss: 0.79838586
test_loss: 0.7898161
train_loss: 0.7444963
test_loss: 0.7469395
train_loss: 0.69290876
test_loss: 0.6925777
train_loss: 0.6208521
test_loss: 0.6268371
train_loss: 0.54176956
test_loss: 0.5533482
train_loss: 0.47523463
test_loss: 0.4857055
train_loss: 0.42674685
test_loss: 0.43229324
train_loss: 0.39507285
test_loss: 0.39237636
train_loss: 0.35360682
test_loss: 0.3621357
train_loss: 0.32305843
test_loss: 0.33770734
train_loss: 0.30943617
test_loss: 0.31628954
train_loss: 0.28836393
test_loss: 0.29562598
train_loss: 0.27565172
test_loss: 0.27472684
train_loss: 0.24443504
test_loss: 0.25335124
train_loss: 0.2289778
test_loss: 0.23163496
train_loss: 0.20789905
test_loss: 0.20876193
train_loss: 0.18674064
test_loss: 0.18330164
train_loss: 0.15440086
test_loss: 0.1563206
train_loss: 0.1309399
test_loss: 0.13162012
train_loss: 0.1040529
test_loss: 0.11201995
train_loss: 0.092060685
test_loss: 0.09661938
train_loss: 0.08417906
test_loss: 0.084524736
train_loss: 0.07610855
test_loss: 0.07547977
train_loss: 0.07150933
test_loss: 0.06903407
train_loss: 0.0645889
test_loss: 0.06461913
train_loss: 0.06263431
test_loss: 0.061608262
train_loss: 0.057283897
test_loss: 0.05969781
train_loss: 0.057638388
test_loss: 0.058228984
train_loss: 0.054854274
test_loss: 0.057203684
train_loss: 0.05552606
test_loss: 0.056325752
train_loss: 0.055033006
test_loss: 0.055826943
train_loss: 0.054043896
test_loss: 0.05522318
train_loss: 0.05359436
test_loss: 0.054863583
train_loss: 0.054312024
test_loss: 0.05480988
train_loss: 0.05366545
test_loss: 0.054408424
train_loss: 0.05465631
test_loss: 0.0542338
train_loss: 0.053116657
test_loss: 0.054066904
train_loss: 0.053107403
test_loss: 0.053871132
train_loss: 0.05359625
test_loss: 0.05391868
train_loss: 0.052504007
test_loss: 0.053698644
train_loss: 0.052197285
test_loss: 0.053646497
train_loss: 0.05292528
test_loss: 0.053586494
train_loss: 0.05286427
test_loss: 0.05356743
train_loss: 0.05364018
test_loss: 0.053492043
train_loss: 0.053153004
test_loss: 0.053360276
train_loss: 0.053995952
test_loss: 0.0533978
train_loss: 0.054098785
test_loss: 0.05334957
train_loss: 0.05496089
test_loss: 0.053482268
train_loss: 0.052412737
test_loss: 0.053286932
train_loss: 0.052668564
test_loss: 0.053318106
train_loss: 0.053602513
test_loss: 0.05344969
train_loss: 0.052793816
test_loss: 0.053299546
train_loss: 0.053533304
test_loss: 0.053294618
train_loss: 0.05319608
test_loss: 0.053174198
train_loss: 0.05310724
test_loss: 0.053337052
train_loss: 0.052833844
test_loss: 0.05327212
train_loss: 0.051664103
test_loss: 0.05325295
train_loss: 0.05183319
test_loss: 0.053305987
train_loss: 0.05236901
test_loss: 0.05330919
train_loss: 0.051751807
test_loss: 0.05318973
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi2/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 0 --phi 2 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi2/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f46005bd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f460055a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f460052dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f460062f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f460062fea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f460062f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eef128c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4600506158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eeede598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eee74158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eee749d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eee34ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eee5cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eee021e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eedba0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eedbac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eed77ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eed2df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eed2fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eed2d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eed0d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eecc2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eed0df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45eecd9ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45b87fd6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45b87f3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45b8837510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45b87f3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45b873f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45b87640d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45b8759400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45b873c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45b873c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45b870aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45907951e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f45907aef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.00451963162
Iter: 2 loss: 0.00866468716
Iter: 3 loss: 0.00449155597
Iter: 4 loss: 0.00448048767
Iter: 5 loss: 0.00447795
Iter: 6 loss: 0.00447077816
Iter: 7 loss: 0.00446051173
Iter: 8 loss: 0.00447294069
Iter: 9 loss: 0.0044552004
Iter: 10 loss: 0.0044469675
Iter: 11 loss: 0.0044558933
Iter: 12 loss: 0.00444247201
Iter: 13 loss: 0.00443443703
Iter: 14 loss: 0.00446386868
Iter: 15 loss: 0.00443243794
Iter: 16 loss: 0.00442840438
Iter: 17 loss: 0.0044254344
Iter: 18 loss: 0.00442407699
Iter: 19 loss: 0.00441875216
Iter: 20 loss: 0.00448962487
Iter: 21 loss: 0.00441872608
Iter: 22 loss: 0.00441563688
Iter: 23 loss: 0.00441956101
Iter: 24 loss: 0.00441405876
Iter: 25 loss: 0.00441093743
Iter: 26 loss: 0.0044191638
Iter: 27 loss: 0.00440988876
Iter: 28 loss: 0.00440832973
Iter: 29 loss: 0.00443189684
Iter: 30 loss: 0.00440833438
Iter: 31 loss: 0.00440688664
Iter: 32 loss: 0.00440547615
Iter: 33 loss: 0.00440516928
Iter: 34 loss: 0.00440316088
Iter: 35 loss: 0.00441625
Iter: 36 loss: 0.00440294854
Iter: 37 loss: 0.00440124888
Iter: 38 loss: 0.00440200698
Iter: 39 loss: 0.00440009404
Iter: 40 loss: 0.00439907657
Iter: 41 loss: 0.00441185292
Iter: 42 loss: 0.00439907098
Iter: 43 loss: 0.00439844
Iter: 44 loss: 0.00440586172
Iter: 45 loss: 0.00439844094
Iter: 46 loss: 0.00439769961
Iter: 47 loss: 0.00439889636
Iter: 48 loss: 0.00439736852
Iter: 49 loss: 0.00439711846
Iter: 50 loss: 0.00439671148
Iter: 51 loss: 0.00439670309
Iter: 52 loss: 0.00439597201
Iter: 53 loss: 0.00439801905
Iter: 54 loss: 0.00439573498
Iter: 55 loss: 0.00439522788
Iter: 56 loss: 0.00439578574
Iter: 57 loss: 0.00439494895
Iter: 58 loss: 0.00439432
Iter: 59 loss: 0.0043947231
Iter: 60 loss: 0.00439390866
Iter: 61 loss: 0.004393321
Iter: 62 loss: 0.0043933196
Iter: 63 loss: 0.00439296477
Iter: 64 loss: 0.00439245533
Iter: 65 loss: 0.0043924409
Iter: 66 loss: 0.0043918509
Iter: 67 loss: 0.00439826958
Iter: 68 loss: 0.00439184
Iter: 69 loss: 0.00439144252
Iter: 70 loss: 0.00439324556
Iter: 71 loss: 0.00439136
Iter: 72 loss: 0.00439099222
Iter: 73 loss: 0.00439067464
Iter: 74 loss: 0.0043905722
Iter: 75 loss: 0.00439002318
Iter: 76 loss: 0.00439021597
Iter: 77 loss: 0.00438964088
Iter: 78 loss: 0.00439041108
Iter: 79 loss: 0.00438946299
Iter: 80 loss: 0.00438929256
Iter: 81 loss: 0.0043891659
Iter: 82 loss: 0.00438910676
Iter: 83 loss: 0.00438894238
Iter: 84 loss: 0.00438865088
Iter: 85 loss: 0.0043957606
Iter: 86 loss: 0.00438864
Iter: 87 loss: 0.00438840222
Iter: 88 loss: 0.00438839057
Iter: 89 loss: 0.00438826066
Iter: 90 loss: 0.00438834447
Iter: 91 loss: 0.00438817032
Iter: 92 loss: 0.00438798824
Iter: 93 loss: 0.00438779499
Iter: 94 loss: 0.0043877596
Iter: 95 loss: 0.00438752584
Iter: 96 loss: 0.0043892893
Iter: 97 loss: 0.0043875007
Iter: 98 loss: 0.00438730605
Iter: 99 loss: 0.00438816193
Iter: 100 loss: 0.004387266
Iter: 101 loss: 0.0043871128
Iter: 102 loss: 0.00438786112
Iter: 103 loss: 0.00438709371
Iter: 104 loss: 0.00438695727
Iter: 105 loss: 0.00438681385
Iter: 106 loss: 0.00438679615
Iter: 107 loss: 0.00438667461
Iter: 108 loss: 0.00438666437
Iter: 109 loss: 0.0043865582
Iter: 110 loss: 0.00438641571
Iter: 111 loss: 0.00438640732
Iter: 112 loss: 0.00438627182
Iter: 113 loss: 0.00438765064
Iter: 114 loss: 0.00438627
Iter: 115 loss: 0.00438623689
Iter: 116 loss: 0.00438622199
Iter: 117 loss: 0.00438616518
Iter: 118 loss: 0.00438602455
Iter: 119 loss: 0.00438694283
Iter: 120 loss: 0.00438598171
Iter: 121 loss: 0.00438589696
Iter: 122 loss: 0.00438578054
Iter: 123 loss: 0.0043857675
Iter: 124 loss: 0.00438570138
Iter: 125 loss: 0.00438567344
Iter: 126 loss: 0.00438559428
Iter: 127 loss: 0.00438569952
Iter: 128 loss: 0.00438556261
Iter: 129 loss: 0.00438548252
Iter: 130 loss: 0.00438553654
Iter: 131 loss: 0.00438542338
Iter: 132 loss: 0.00438535213
Iter: 133 loss: 0.00438538
Iter: 134 loss: 0.00438529626
Iter: 135 loss: 0.00438519754
Iter: 136 loss: 0.00438593095
Iter: 137 loss: 0.00438518543
Iter: 138 loss: 0.00438513141
Iter: 139 loss: 0.00438559195
Iter: 140 loss: 0.00438512582
Iter: 141 loss: 0.00438507181
Iter: 142 loss: 0.00438504945
Iter: 143 loss: 0.00438501965
Iter: 144 loss: 0.00438495
Iter: 145 loss: 0.00438523
Iter: 146 loss: 0.00438494515
Iter: 147 loss: 0.00438487064
Iter: 148 loss: 0.0043850597
Iter: 149 loss: 0.00438485108
Iter: 150 loss: 0.00438479427
Iter: 151 loss: 0.00438479474
Iter: 152 loss: 0.00438476074
Iter: 153 loss: 0.00438470766
Iter: 154 loss: 0.00438470719
Iter: 155 loss: 0.00438466296
Iter: 156 loss: 0.00438455632
Iter: 157 loss: 0.00438627368
Iter: 158 loss: 0.00438455492
Iter: 159 loss: 0.00438446738
Iter: 160 loss: 0.00438572
Iter: 161 loss: 0.00438446552
Iter: 162 loss: 0.00438439613
Iter: 163 loss: 0.004384486
Iter: 164 loss: 0.00438435469
Iter: 165 loss: 0.00438425876
Iter: 166 loss: 0.00438423734
Iter: 167 loss: 0.00438418239
Iter: 168 loss: 0.00438402873
Iter: 169 loss: 0.00438495213
Iter: 170 loss: 0.00438401
Iter: 171 loss: 0.00438393187
Iter: 172 loss: 0.00438392675
Iter: 173 loss: 0.00438389182
Iter: 174 loss: 0.00438380148
Iter: 175 loss: 0.00438443292
Iter: 176 loss: 0.00438378
Iter: 177 loss: 0.004383686
Iter: 178 loss: 0.00438495073
Iter: 179 loss: 0.00438368041
Iter: 180 loss: 0.0043835938
Iter: 181 loss: 0.00438355748
Iter: 182 loss: 0.00438351836
Iter: 183 loss: 0.00438345876
Iter: 184 loss: 0.00438344479
Iter: 185 loss: 0.00438340288
Iter: 186 loss: 0.00438385224
Iter: 187 loss: 0.00438339915
Iter: 188 loss: 0.00438337
Iter: 189 loss: 0.00438329345
Iter: 190 loss: 0.00438384246
Iter: 191 loss: 0.00438327109
Iter: 192 loss: 0.00438316865
Iter: 193 loss: 0.00438333815
Iter: 194 loss: 0.00438312627
Iter: 195 loss: 0.00438301777
Iter: 196 loss: 0.00438305549
Iter: 197 loss: 0.00438294793
Iter: 198 loss: 0.00438281335
Iter: 199 loss: 0.00438424107
Iter: 200 loss: 0.00438280869
Iter: 201 loss: 0.00438272487
Iter: 202 loss: 0.00438292418
Iter: 203 loss: 0.00438268762
Iter: 204 loss: 0.00438259169
Iter: 205 loss: 0.00438259961
Iter: 206 loss: 0.00438252371
Iter: 207 loss: 0.00438242452
Iter: 208 loss: 0.00438279146
Iter: 209 loss: 0.00438239891
Iter: 210 loss: 0.00438227737
Iter: 211 loss: 0.00438281801
Iter: 212 loss: 0.00438225688
Iter: 213 loss: 0.00438217539
Iter: 214 loss: 0.00438225735
Iter: 215 loss: 0.00438212929
Iter: 216 loss: 0.00438201847
Iter: 217 loss: 0.00438199
Iter: 218 loss: 0.00438192673
Iter: 219 loss: 0.00438186852
Iter: 220 loss: 0.00438186154
Iter: 221 loss: 0.00438181357
Iter: 222 loss: 0.00438243756
Iter: 223 loss: 0.00438180938
Iter: 224 loss: 0.00438177586
Iter: 225 loss: 0.00438169576
Iter: 226 loss: 0.00438292045
Iter: 227 loss: 0.00438168552
Iter: 228 loss: 0.00438161707
Iter: 229 loss: 0.0043819095
Iter: 230 loss: 0.00438159052
Iter: 231 loss: 0.00438149
Iter: 232 loss: 0.00438130647
Iter: 233 loss: 0.00438541081
Iter: 234 loss: 0.00438131718
Iter: 235 loss: 0.00438117515
Iter: 236 loss: 0.00438307505
Iter: 237 loss: 0.00438118074
Iter: 238 loss: 0.00438106712
Iter: 239 loss: 0.00438128924
Iter: 240 loss: 0.00438101729
Iter: 241 loss: 0.00438088924
Iter: 242 loss: 0.00438126735
Iter: 243 loss: 0.00438084127
Iter: 244 loss: 0.00438073045
Iter: 245 loss: 0.00438059634
Iter: 246 loss: 0.00438058795
Iter: 247 loss: 0.00438051065
Iter: 248 loss: 0.00438047759
Iter: 249 loss: 0.00438039377
Iter: 250 loss: 0.00438037049
Iter: 251 loss: 0.0043803174
Iter: 252 loss: 0.00438018935
Iter: 253 loss: 0.00438020611
Iter: 254 loss: 0.00438008225
Iter: 255 loss: 0.0043800124
Iter: 256 loss: 0.00438000355
Iter: 257 loss: 0.00437990436
Iter: 258 loss: 0.00437997375
Iter: 259 loss: 0.00437985081
Iter: 260 loss: 0.00437975209
Iter: 261 loss: 0.00437968597
Iter: 262 loss: 0.00437965943
Iter: 263 loss: 0.00437953416
Iter: 264 loss: 0.00438009389
Iter: 265 loss: 0.00437951507
Iter: 266 loss: 0.00437939819
Iter: 267 loss: 0.00437938794
Iter: 268 loss: 0.00437930413
Iter: 269 loss: 0.00437917607
Iter: 270 loss: 0.00437974837
Iter: 271 loss: 0.00437914766
Iter: 272 loss: 0.00437901542
Iter: 273 loss: 0.00437914906
Iter: 274 loss: 0.00437893393
Iter: 275 loss: 0.00437880494
Iter: 276 loss: 0.0043802117
Iter: 277 loss: 0.00437880494
Iter: 278 loss: 0.00437870529
Iter: 279 loss: 0.00437860191
Iter: 280 loss: 0.00437859166
Iter: 281 loss: 0.00437840633
Iter: 282 loss: 0.0043793628
Iter: 283 loss: 0.0043783877
Iter: 284 loss: 0.00437828712
Iter: 285 loss: 0.00437912252
Iter: 286 loss: 0.00437827501
Iter: 287 loss: 0.00437814742
Iter: 288 loss: 0.00437803613
Iter: 289 loss: 0.00437801
Iter: 290 loss: 0.00437795
Iter: 291 loss: 0.00437793834
Iter: 292 loss: 0.00437784474
Iter: 293 loss: 0.00437771901
Iter: 294 loss: 0.00437771622
Iter: 295 loss: 0.00437758
Iter: 296 loss: 0.00437750109
Iter: 297 loss: 0.00437744521
Iter: 298 loss: 0.00437730271
Iter: 299 loss: 0.00437886594
Iter: 300 loss: 0.0043773111
Iter: 301 loss: 0.00437717186
Iter: 302 loss: 0.00437700329
Iter: 303 loss: 0.00437700469
Iter: 304 loss: 0.00437678862
Iter: 305 loss: 0.00437767431
Iter: 306 loss: 0.00437674392
Iter: 307 loss: 0.0043765204
Iter: 308 loss: 0.00437678955
Iter: 309 loss: 0.00437640259
Iter: 310 loss: 0.0043762
Iter: 311 loss: 0.00437817
Iter: 312 loss: 0.0043761963
Iter: 313 loss: 0.0043760268
Iter: 314 loss: 0.00437598582
Iter: 315 loss: 0.00437587081
Iter: 316 loss: 0.00437569385
Iter: 317 loss: 0.00437737722
Iter: 318 loss: 0.00437568408
Iter: 319 loss: 0.00437555276
Iter: 320 loss: 0.0043757637
Iter: 321 loss: 0.00437549781
Iter: 322 loss: 0.00437526731
Iter: 323 loss: 0.00437532924
Iter: 324 loss: 0.00437510479
Iter: 325 loss: 0.00437504146
Iter: 326 loss: 0.00437501213
Iter: 327 loss: 0.00437491713
Iter: 328 loss: 0.00437488873
Iter: 329 loss: 0.00437483424
Iter: 330 loss: 0.00437472668
Iter: 331 loss: 0.00437448
Iter: 332 loss: 0.00437763706
Iter: 333 loss: 0.00437446591
Iter: 334 loss: 0.00437421352
Iter: 335 loss: 0.00437524775
Iter: 336 loss: 0.00437415158
Iter: 337 loss: 0.00437393412
Iter: 338 loss: 0.00437515276
Iter: 339 loss: 0.00437390152
Iter: 340 loss: 0.00437370781
Iter: 341 loss: 0.00437490083
Iter: 342 loss: 0.00437369198
Iter: 343 loss: 0.00437352899
Iter: 344 loss: 0.00437334646
Iter: 345 loss: 0.00437331852
Iter: 346 loss: 0.00437311549
Iter: 347 loss: 0.00437413063
Iter: 348 loss: 0.00437307311
Iter: 349 loss: 0.00437285705
Iter: 350 loss: 0.00437339209
Iter: 351 loss: 0.00437277695
Iter: 352 loss: 0.00437258417
Iter: 353 loss: 0.00437284913
Iter: 354 loss: 0.00437248638
Iter: 355 loss: 0.00437220884
Iter: 356 loss: 0.00437236903
Iter: 357 loss: 0.0043720305
Iter: 358 loss: 0.00437186193
Iter: 359 loss: 0.00437184796
Iter: 360 loss: 0.00437171385
Iter: 361 loss: 0.00437353086
Iter: 362 loss: 0.0043717213
Iter: 363 loss: 0.00437159697
Iter: 364 loss: 0.00437149685
Iter: 365 loss: 0.00437146565
Iter: 366 loss: 0.00437129103
Iter: 367 loss: 0.00437165285
Iter: 368 loss: 0.00437122863
Iter: 369 loss: 0.00437104609
Iter: 370 loss: 0.00437071919
Iter: 371 loss: 0.00437844777
Iter: 372 loss: 0.00437072478
Iter: 373 loss: 0.00437046913
Iter: 374 loss: 0.00437046681
Iter: 375 loss: 0.00437023072
Iter: 376 loss: 0.00437021535
Iter: 377 loss: 0.00437004026
Iter: 378 loss: 0.00436975202
Iter: 379 loss: 0.00437085703
Iter: 380 loss: 0.00436968356
Iter: 381 loss: 0.0043694363
Iter: 382 loss: 0.00436979346
Iter: 383 loss: 0.00436931523
Iter: 384 loss: 0.00436903955
Iter: 385 loss: 0.00437120488
Iter: 386 loss: 0.00436901953
Iter: 387 loss: 0.00436880719
Iter: 388 loss: 0.00436869636
Iter: 389 loss: 0.00436861208
Iter: 390 loss: 0.00436825957
Iter: 391 loss: 0.00436882
Iter: 392 loss: 0.00436809193
Iter: 393 loss: 0.00436799321
Iter: 394 loss: 0.00436794478
Iter: 395 loss: 0.00436780229
Iter: 396 loss: 0.00436800066
Iter: 397 loss: 0.00436772
Iter: 398 loss: 0.004367569
Iter: 399 loss: 0.00436757039
Iter: 400 loss: 0.00436744234
Iter: 401 loss: 0.00436721649
Iter: 402 loss: 0.00436774734
Iter: 403 loss: 0.00436713453
Iter: 404 loss: 0.00436692964
Iter: 405 loss: 0.00436666748
Iter: 406 loss: 0.0043666549
Iter: 407 loss: 0.00436628889
Iter: 408 loss: 0.0043702065
Iter: 409 loss: 0.00436627865
Iter: 410 loss: 0.00436600205
Iter: 411 loss: 0.00436695572
Iter: 412 loss: 0.00436591823
Iter: 413 loss: 0.00436563091
Iter: 414 loss: 0.00436629634
Iter: 415 loss: 0.004365528
Iter: 416 loss: 0.00436530355
Iter: 417 loss: 0.00436582975
Iter: 418 loss: 0.00436521415
Iter: 419 loss: 0.00436492916
Iter: 420 loss: 0.00436619436
Iter: 421 loss: 0.00436487561
Iter: 422 loss: 0.00436463952
Iter: 423 loss: 0.00436515221
Iter: 424 loss: 0.0043645585
Iter: 425 loss: 0.00436433731
Iter: 426 loss: 0.00436418178
Iter: 427 loss: 0.00436410075
Iter: 428 loss: 0.00436468609
Iter: 429 loss: 0.00436402578
Iter: 430 loss: 0.00436393498
Iter: 431 loss: 0.00436390797
Iter: 432 loss: 0.00436385907
Iter: 433 loss: 0.00436353125
Iter: 434 loss: 0.00436352519
Iter: 435 loss: 0.00436327653
Iter: 436 loss: 0.00436315313
Iter: 437 loss: 0.00436306
Iter: 438 loss: 0.00436288631
Iter: 439 loss: 0.00436270656
Iter: 440 loss: 0.00436267164
Iter: 441 loss: 0.00436243508
Iter: 442 loss: 0.00436357781
Iter: 443 loss: 0.00436239736
Iter: 444 loss: 0.00436219806
Iter: 445 loss: 0.00436252262
Iter: 446 loss: 0.0043621175
Iter: 447 loss: 0.00436182367
Iter: 448 loss: 0.00436192332
Iter: 449 loss: 0.00436162157
Iter: 450 loss: 0.0043613473
Iter: 451 loss: 0.00436195498
Iter: 452 loss: 0.00436123926
Iter: 453 loss: 0.00436088629
Iter: 454 loss: 0.00436263625
Iter: 455 loss: 0.00436083227
Iter: 456 loss: 0.00436051469
Iter: 457 loss: 0.00436222786
Iter: 458 loss: 0.004360463
Iter: 459 loss: 0.0043602963
Iter: 460 loss: 0.00436027907
Iter: 461 loss: 0.00436011329
Iter: 462 loss: 0.00436054263
Iter: 463 loss: 0.00436006393
Iter: 464 loss: 0.00435994286
Iter: 465 loss: 0.00435960665
Iter: 466 loss: 0.00436047604
Iter: 467 loss: 0.00435941713
Iter: 468 loss: 0.00435905671
Iter: 469 loss: 0.00435904786
Iter: 470 loss: 0.00435879547
Iter: 471 loss: 0.00436047371
Iter: 472 loss: 0.00435876334
Iter: 473 loss: 0.00435852772
Iter: 474 loss: 0.00435837638
Iter: 475 loss: 0.00435828045
Iter: 476 loss: 0.00435798801
Iter: 477 loss: 0.0043589687
Iter: 478 loss: 0.00435790885
Iter: 479 loss: 0.00435748603
Iter: 480 loss: 0.00436069723
Iter: 481 loss: 0.00435745809
Iter: 482 loss: 0.00435705762
Iter: 483 loss: 0.0043589971
Iter: 484 loss: 0.00435699429
Iter: 485 loss: 0.00435670465
Iter: 486 loss: 0.0043579014
Iter: 487 loss: 0.0043566376
Iter: 488 loss: 0.0043563135
Iter: 489 loss: 0.00435647368
Iter: 490 loss: 0.00435609277
Iter: 491 loss: 0.0043559107
Iter: 492 loss: 0.00435586879
Iter: 493 loss: 0.00435568392
Iter: 494 loss: 0.00435612304
Iter: 495 loss: 0.00435563736
Iter: 496 loss: 0.00435538217
Iter: 497 loss: 0.0043560667
Iter: 498 loss: 0.00435530348
Iter: 499 loss: 0.00435498683
Iter: 500 loss: 0.00435548928
Iter: 501 loss: 0.00435482943
Iter: 502 loss: 0.00435459102
Iter: 503 loss: 0.00435570441
Iter: 504 loss: 0.00435454492
Iter: 505 loss: 0.00435432233
Iter: 506 loss: 0.00435476378
Iter: 507 loss: 0.0043542413
Iter: 508 loss: 0.00435399078
Iter: 509 loss: 0.00435407413
Iter: 510 loss: 0.0043538129
Iter: 511 loss: 0.00435351
Iter: 512 loss: 0.00435378496
Iter: 513 loss: 0.0043533342
Iter: 514 loss: 0.00435288297
Iter: 515 loss: 0.00435487553
Iter: 516 loss: 0.00435277168
Iter: 517 loss: 0.00435237074
Iter: 518 loss: 0.00435356796
Iter: 519 loss: 0.00435226411
Iter: 520 loss: 0.00435182266
Iter: 521 loss: 0.00435280846
Iter: 522 loss: 0.00435165735
Iter: 523 loss: 0.00435130671
Iter: 524 loss: 0.00435341103
Iter: 525 loss: 0.00435126107
Iter: 526 loss: 0.00435091462
Iter: 527 loss: 0.00435097422
Iter: 528 loss: 0.00435066968
Iter: 529 loss: 0.00435027387
Iter: 530 loss: 0.00435343
Iter: 531 loss: 0.0043502585
Iter: 532 loss: 0.00435002334
Iter: 533 loss: 0.00435001869
Iter: 534 loss: 0.00434966
Iter: 535 loss: 0.00434999354
Iter: 536 loss: 0.00434945058
Iter: 537 loss: 0.0043490082
Iter: 538 loss: 0.00435451511
Iter: 539 loss: 0.00434900681
Iter: 540 loss: 0.00434879307
Iter: 541 loss: 0.00435111113
Iter: 542 loss: 0.0043487926
Iter: 543 loss: 0.00434856955
Iter: 544 loss: 0.00434883265
Iter: 545 loss: 0.00434844242
Iter: 546 loss: 0.0043479153
Iter: 547 loss: 0.00434837025
Iter: 548 loss: 0.00434759771
Iter: 549 loss: 0.00434699375
Iter: 550 loss: 0.0043525463
Iter: 551 loss: 0.00434697326
Iter: 552 loss: 0.00434660632
Iter: 553 loss: 0.00434659142
Iter: 554 loss: 0.00434630271
Iter: 555 loss: 0.00434583565
Iter: 556 loss: 0.00434658676
Iter: 557 loss: 0.0043456303
Iter: 558 loss: 0.00434513716
Iter: 559 loss: 0.00434923
Iter: 560 loss: 0.00434509944
Iter: 561 loss: 0.00434481166
Iter: 562 loss: 0.00434623
Iter: 563 loss: 0.00434476696
Iter: 564 loss: 0.00434452668
Iter: 565 loss: 0.0043445155
Iter: 566 loss: 0.00434423517
Iter: 567 loss: 0.00434629619
Iter: 568 loss: 0.00434420817
Iter: 569 loss: 0.00434386916
Iter: 570 loss: 0.00434570108
Iter: 571 loss: 0.00434381375
Iter: 572 loss: 0.00434365822
Iter: 573 loss: 0.00434399769
Iter: 574 loss: 0.00434359536
Iter: 575 loss: 0.00434341934
Iter: 576 loss: 0.00434447266
Iter: 577 loss: 0.0043434035
Iter: 578 loss: 0.00434326613
Iter: 579 loss: 0.0043431744
Iter: 580 loss: 0.0043431269
Iter: 581 loss: 0.00434293412
Iter: 582 loss: 0.00434306264
Iter: 583 loss: 0.00434282
Iter: 584 loss: 0.00434254855
Iter: 585 loss: 0.0043442375
Iter: 586 loss: 0.00434253
Iter: 587 loss: 0.00434226962
Iter: 588 loss: 0.0043438212
Iter: 589 loss: 0.00434223562
Iter: 590 loss: 0.00434205774
Iter: 591 loss: 0.0043421709
Iter: 592 loss: 0.00434194971
Iter: 593 loss: 0.00434158649
Iter: 594 loss: 0.00434479862
Iter: 595 loss: 0.00434157252
Iter: 596 loss: 0.00434125774
Iter: 597 loss: 0.0043429425
Iter: 598 loss: 0.00434122
Iter: 599 loss: 0.00434104446
Iter: 600 loss: 0.00434108172
Iter: 601 loss: 0.0043409
Iter: 602 loss: 0.00434073154
Iter: 603 loss: 0.00434308406
Iter: 604 loss: 0.00434072688
Iter: 605 loss: 0.00434057228
Iter: 606 loss: 0.00434118835
Iter: 607 loss: 0.00434054155
Iter: 608 loss: 0.00434041908
Iter: 609 loss: 0.00434077112
Iter: 610 loss: 0.00434038322
Iter: 611 loss: 0.00434025
Iter: 612 loss: 0.00434048939
Iter: 613 loss: 0.00434019975
Iter: 614 loss: 0.0043400852
Iter: 615 loss: 0.00433991756
Iter: 616 loss: 0.00433991477
Iter: 617 loss: 0.0043397015
Iter: 618 loss: 0.00434047543
Iter: 619 loss: 0.00433964329
Iter: 620 loss: 0.0043394668
Iter: 621 loss: 0.00434032455
Iter: 622 loss: 0.00433942955
Iter: 623 loss: 0.00433923164
Iter: 624 loss: 0.00433940906
Iter: 625 loss: 0.00433911849
Iter: 626 loss: 0.00433893129
Iter: 627 loss: 0.00433889497
Iter: 628 loss: 0.00433876971
Iter: 629 loss: 0.00433850661
Iter: 630 loss: 0.00434082793
Iter: 631 loss: 0.00433849311
Iter: 632 loss: 0.00433827331
Iter: 633 loss: 0.00433930475
Iter: 634 loss: 0.00433823373
Iter: 635 loss: 0.00433804281
Iter: 636 loss: 0.00433787191
Iter: 637 loss: 0.00433782116
Iter: 638 loss: 0.0043375832
Iter: 639 loss: 0.00433806097
Iter: 640 loss: 0.00433748867
Iter: 641 loss: 0.00433736434
Iter: 642 loss: 0.00433734339
Iter: 643 loss: 0.00433720089
Iter: 644 loss: 0.00433760788
Iter: 645 loss: 0.0043371655
Iter: 646 loss: 0.00433706352
Iter: 647 loss: 0.00433711847
Iter: 648 loss: 0.00433700811
Iter: 649 loss: 0.00433688425
Iter: 650 loss: 0.00433675386
Iter: 651 loss: 0.00433672685
Iter: 652 loss: 0.00433652243
Iter: 653 loss: 0.00433660578
Iter: 654 loss: 0.00433637155
Iter: 655 loss: 0.00433608796
Iter: 656 loss: 0.00433753897
Iter: 657 loss: 0.00433604326
Iter: 658 loss: 0.0043358868
Iter: 659 loss: 0.00433589052
Iter: 660 loss: 0.00433576
Iter: 661 loss: 0.0043355925
Iter: 662 loss: 0.00433557946
Iter: 663 loss: 0.0043353457
Iter: 664 loss: 0.00433569495
Iter: 665 loss: 0.00433523394
Iter: 666 loss: 0.004334989
Iter: 667 loss: 0.00433636643
Iter: 668 loss: 0.00433495641
Iter: 669 loss: 0.0043347124
Iter: 670 loss: 0.00433580764
Iter: 671 loss: 0.00433466956
Iter: 672 loss: 0.00433448842
Iter: 673 loss: 0.00433450844
Iter: 674 loss: 0.00433435384
Iter: 675 loss: 0.00433417875
Iter: 676 loss: 0.00433418248
Iter: 677 loss: 0.00433397293
Iter: 678 loss: 0.00433397386
Iter: 679 loss: 0.00433380529
Iter: 680 loss: 0.00433364417
Iter: 681 loss: 0.00433395617
Iter: 682 loss: 0.00433357619
Iter: 683 loss: 0.0043333685
Iter: 684 loss: 0.00433312822
Iter: 685 loss: 0.00433310028
Iter: 686 loss: 0.00433275057
Iter: 687 loss: 0.00433352
Iter: 688 loss: 0.00433260435
Iter: 689 loss: 0.00433217268
Iter: 690 loss: 0.00433254149
Iter: 691 loss: 0.00433191471
Iter: 692 loss: 0.00433159852
Iter: 693 loss: 0.00433160271
Iter: 694 loss: 0.0043312991
Iter: 695 loss: 0.00433133729
Iter: 696 loss: 0.00433108164
Iter: 697 loss: 0.00433074823
Iter: 698 loss: 0.00433143321
Iter: 699 loss: 0.00433061
Iter: 700 loss: 0.00433027418
Iter: 701 loss: 0.00433077523
Iter: 702 loss: 0.00433011446
Iter: 703 loss: 0.00432972191
Iter: 704 loss: 0.00433396967
Iter: 705 loss: 0.00432971586
Iter: 706 loss: 0.00432948954
Iter: 707 loss: 0.00432997663
Iter: 708 loss: 0.00432940945
Iter: 709 loss: 0.00432909653
Iter: 710 loss: 0.00433119433
Iter: 711 loss: 0.00432907976
Iter: 712 loss: 0.00432890793
Iter: 713 loss: 0.0043286779
Iter: 714 loss: 0.00432866625
Iter: 715 loss: 0.00432827603
Iter: 716 loss: 0.00432877522
Iter: 717 loss: 0.00432808371
Iter: 718 loss: 0.00432755798
Iter: 719 loss: 0.00432864344
Iter: 720 loss: 0.00432735588
Iter: 721 loss: 0.00432690186
Iter: 722 loss: 0.0043272404
Iter: 723 loss: 0.00432661688
Iter: 724 loss: 0.00432595378
Iter: 725 loss: 0.00432680035
Iter: 726 loss: 0.00432561245
Iter: 727 loss: 0.00432504807
Iter: 728 loss: 0.00432505086
Iter: 729 loss: 0.00432459544
Iter: 730 loss: 0.00432453025
Iter: 731 loss: 0.00432420336
Iter: 732 loss: 0.00432361243
Iter: 733 loss: 0.00432391139
Iter: 734 loss: 0.00432321709
Iter: 735 loss: 0.00432253536
Iter: 736 loss: 0.00432779267
Iter: 737 loss: 0.00432248507
Iter: 738 loss: 0.00432187039
Iter: 739 loss: 0.0043248022
Iter: 740 loss: 0.00432175584
Iter: 741 loss: 0.00432147458
Iter: 742 loss: 0.00432145782
Iter: 743 loss: 0.00432121521
Iter: 744 loss: 0.00432054512
Iter: 745 loss: 0.0043245866
Iter: 746 loss: 0.00432037096
Iter: 747 loss: 0.00431950949
Iter: 748 loss: 0.00432173116
Iter: 749 loss: 0.00431922916
Iter: 750 loss: 0.00431840774
Iter: 751 loss: 0.00432272349
Iter: 752 loss: 0.00431828201
Iter: 753 loss: 0.00431750342
Iter: 754 loss: 0.00431718817
Iter: 755 loss: 0.00431677513
Iter: 756 loss: 0.00431573251
Iter: 757 loss: 0.00431692414
Iter: 758 loss: 0.00431518909
Iter: 759 loss: 0.00431382563
Iter: 760 loss: 0.00431827316
Iter: 761 loss: 0.00431344286
Iter: 762 loss: 0.00431229966
Iter: 763 loss: 0.00432122685
Iter: 764 loss: 0.00431222795
Iter: 765 loss: 0.00431098184
Iter: 766 loss: 0.0043090675
Iter: 767 loss: 0.00430902839
Iter: 768 loss: 0.004306891
Iter: 769 loss: 0.00431313226
Iter: 770 loss: 0.0043062414
Iter: 771 loss: 0.00430416828
Iter: 772 loss: 0.00430751406
Iter: 773 loss: 0.0043032323
Iter: 774 loss: 0.00430183392
Iter: 775 loss: 0.00430170633
Iter: 776 loss: 0.00430064322
Iter: 777 loss: 0.00430217
Iter: 778 loss: 0.00430013
Iter: 779 loss: 0.00429893238
Iter: 780 loss: 0.0042972127
Iter: 781 loss: 0.00429716054
Iter: 782 loss: 0.00429544924
Iter: 783 loss: 0.00431072
Iter: 784 loss: 0.00429536402
Iter: 785 loss: 0.00429398753
Iter: 786 loss: 0.00429692119
Iter: 787 loss: 0.0042934576
Iter: 788 loss: 0.0042920392
Iter: 789 loss: 0.00429318566
Iter: 790 loss: 0.00429120101
Iter: 791 loss: 0.00428971974
Iter: 792 loss: 0.00429146085
Iter: 793 loss: 0.00428893976
Iter: 794 loss: 0.00428724103
Iter: 795 loss: 0.00429354887
Iter: 796 loss: 0.00428685
Iter: 797 loss: 0.00428517722
Iter: 798 loss: 0.00429476146
Iter: 799 loss: 0.00428496115
Iter: 800 loss: 0.00428335695
Iter: 801 loss: 0.00428332528
Iter: 802 loss: 0.00428208
Iter: 803 loss: 0.00428014249
Iter: 804 loss: 0.0042820368
Iter: 805 loss: 0.0042790547
Iter: 806 loss: 0.00427794363
Iter: 807 loss: 0.00427781
Iter: 808 loss: 0.00427653594
Iter: 809 loss: 0.00427695084
Iter: 810 loss: 0.00427564606
Iter: 811 loss: 0.00427432172
Iter: 812 loss: 0.00427668775
Iter: 813 loss: 0.00427375687
Iter: 814 loss: 0.0042725876
Iter: 815 loss: 0.00427141972
Iter: 816 loss: 0.00427117525
Iter: 817 loss: 0.00426925346
Iter: 818 loss: 0.00428187335
Iter: 819 loss: 0.00426907698
Iter: 820 loss: 0.00426766276
Iter: 821 loss: 0.00427268725
Iter: 822 loss: 0.00426731305
Iter: 823 loss: 0.00426615681
Iter: 824 loss: 0.00426489115
Iter: 825 loss: 0.00426469
Iter: 826 loss: 0.00426303921
Iter: 827 loss: 0.00427518785
Iter: 828 loss: 0.00426291209
Iter: 829 loss: 0.00426153699
Iter: 830 loss: 0.0042642504
Iter: 831 loss: 0.0042609782
Iter: 832 loss: 0.00425953511
Iter: 833 loss: 0.00426803622
Iter: 834 loss: 0.00425934652
Iter: 835 loss: 0.00425830344
Iter: 836 loss: 0.00425695907
Iter: 837 loss: 0.00425686967
Iter: 838 loss: 0.00425490923
Iter: 839 loss: 0.00426199473
Iter: 840 loss: 0.0042544282
Iter: 841 loss: 0.0042542587
Iter: 842 loss: 0.00425334135
Iter: 843 loss: 0.00425271131
Iter: 844 loss: 0.0042510191
Iter: 845 loss: 0.00426182477
Iter: 846 loss: 0.0042506014
Iter: 847 loss: 0.00424814271
Iter: 848 loss: 0.00424979161
Iter: 849 loss: 0.00424660649
Iter: 850 loss: 0.00424362952
Iter: 851 loss: 0.00424362905
Iter: 852 loss: 0.00424158433
Iter: 853 loss: 0.00424914621
Iter: 854 loss: 0.00424109353
Iter: 855 loss: 0.0042396104
Iter: 856 loss: 0.00424002111
Iter: 857 loss: 0.00423855102
Iter: 858 loss: 0.0042363666
Iter: 859 loss: 0.00424252916
Iter: 860 loss: 0.00423569465
Iter: 861 loss: 0.00423316844
Iter: 862 loss: 0.00423955917
Iter: 863 loss: 0.00423230976
Iter: 864 loss: 0.00423043035
Iter: 865 loss: 0.00423544
Iter: 866 loss: 0.00422982685
Iter: 867 loss: 0.00422756653
Iter: 868 loss: 0.00423046807
Iter: 869 loss: 0.00422644429
Iter: 870 loss: 0.00422354694
Iter: 871 loss: 0.0042252643
Iter: 872 loss: 0.00422169827
Iter: 873 loss: 0.00422221282
Iter: 874 loss: 0.00422045309
Iter: 875 loss: 0.00421895646
Iter: 876 loss: 0.00422049593
Iter: 877 loss: 0.00421816669
Iter: 878 loss: 0.00421658624
Iter: 879 loss: 0.00421975693
Iter: 880 loss: 0.00421585329
Iter: 881 loss: 0.00421184208
Iter: 882 loss: 0.00421340577
Iter: 883 loss: 0.00420925114
Iter: 884 loss: 0.00420302432
Iter: 885 loss: 0.00422080886
Iter: 886 loss: 0.00420138147
Iter: 887 loss: 0.00419592392
Iter: 888 loss: 0.00419631926
Iter: 889 loss: 0.00419194577
Iter: 890 loss: 0.00418420602
Iter: 891 loss: 0.00447104219
Iter: 892 loss: 0.00418420183
Iter: 893 loss: 0.00417344365
Iter: 894 loss: 0.00417331047
Iter: 895 loss: 0.00415675063
Iter: 896 loss: 0.48239255
Iter: 897 loss: 0.00415674504
Iter: 898 loss: 0.00413449155
Iter: 899 loss: 2.232862
Iter: 900 loss: 0.00413449
Iter: 901 loss: 0.00411011046
Iter: 902 loss: 0.00410947483
Iter: 903 loss: 0.00408074027
Iter: 904 loss: 2.73259401
Iter: 905 loss: 0.00408074
Iter: 906 loss: 0.67698729
Iter: 907 loss: 0.00408073328
Iter: 908 loss: 0.173268661
Iter: 909 loss: 0.00408072677
Iter: 910 loss: 0.173259988
Iter: 911 loss: 0.0333204195
Iter: 912 loss: 0.00408070721
Iter: 913 loss: 0.00406876439
Iter: 914 loss: 0.00406875648
Iter: 915 loss: 0.385429054
Iter: 916 loss: 0.00406875787
Iter: 917 loss: 0.173238873
Iter: 918 loss: 0.00406874577
Iter: 919 loss: 0.0332685262
Iter: 920 loss: 0.00406872854
Iter: 921 loss: 0.0332809351
Iter: 922 loss: 0.00406871643
Iter: 923 loss: 0.00406221254
Iter: 924 loss: 0.00406220555
Iter: 925 loss: 0.0332848839
Iter: 926 loss: 0.00406220416
Iter: 927 loss: 0.0332868174
Iter: 928 loss: 0.00406220555
Iter: 929 loss: 0.00406143162
Iter: 930 loss: 0.00406143069
Iter: 931 loss: 0.0332875624
Iter: 932 loss: 0.00406143442
Iter: 933 loss: 0.00406123791
Iter: 934 loss: 0.00406123791
Iter: 935 loss: 0.0332877114
Iter: 936 loss: 0.00406123511
Iter: 937 loss: 0.00406118715
Iter: 938 loss: 0.00406118762
Iter: 939 loss: 0.00406116387
Iter: 940 loss: 0.00406116387
Iter: 941 loss: 0.0332876034
Iter: 942 loss: 0.00406116433
Iter: 943 loss: 0.0332876816
Iter: 944 loss: 0.00406116433
Iter: 945 loss: 0.00406116107
Iter: 946 loss: 0.00406116
Iter: 947 loss: 0.00406116154
Iter: 948 loss: 0.033287622
Iter: 949 loss: 0.00406116154
Iter: 950 loss: 0.00406116154
Iter: 951 loss: 1.60725951
Iter: 952 loss: 0.0332876742
Iter: 953 loss: 0.00406116154
Iter: 954 loss: 0.0332876779
Iter: 955 loss: 0.00406116154
Iter: 956 loss: 0.0332876705
Iter: 957 loss: 0.00406116154
Iter: 958 loss: 0.0332876705
Iter: 959 loss: 0.00406116154
Iter: 960 loss: 0.0332876481
Iter: 961 loss: 0.00406116154
Iter: 962 loss: 0.0332876183
Iter: 963 loss: 0.00406116154
Iter: 964 loss: 0.0332876295
Iter: 965 loss: 0.00406116154
Iter: 966 loss: 0.0332876146
Iter: 967 loss: 0.00406116154
Iter: 968 loss: 0.00406116154
Iter: 969 loss: 0.00406116154
Iter: 970 loss: 0.0332876295
Iter: 971 loss: 0.00406116154
Iter: 972 loss: 0.00406116154
Iter: 973 loss: 0.00406116154
Iter: 974 loss: 0.0332876295
Iter: 975 loss: 0.00406116154
Iter: 976 loss: 0.0332876295
Iter: 977 loss: 0.00406116154
Iter: 978 loss: 0.0332876295
Iter: 979 loss: 0.00406116154
Iter: 980 loss: 0.00406116154
Iter: 981 loss: 0.00406116154
Iter: 982 loss: 0.00406116154
Iter: 983 loss: 0.00406116154
Iter: 984 loss: 0.00406116154
Iter: 985 loss: 0.00406116154
Iter: 986 loss: 0.00406116154
Iter: 987 loss: 0.00406116154
Iter: 988 loss: 0.00406116154
Iter: 989 loss: 0.00406116154
Iter: 990 loss: 0.00406116154
Iter: 991 loss: 0.00406116154
Iter: 992 loss: 0.0332876295
Iter: 993 loss: 0.00406116154
Iter: 994 loss: 0.0332876295
Iter: 995 loss: 0.00406116154
Iter: 996 loss: 0.00406116154
Iter: 997 loss: 0.00406116154
Iter: 998 loss: 0.0332876295
Iter: 999 loss: 0.00406116154
Iter: 1000 loss: 0.00406116154
Iter: 1001 loss: 0.00406116154
Iter: 1002 loss: 0.00406116154
Iter: 1003 loss: 0.00406116154
Iter: 1004 loss: 0.0332876295
Iter: 1005 loss: 0.00406116154
Iter: 1006 loss: 0.0332876295
Iter: 1007 loss: 0.00406116154
Iter: 1008 loss: 0.0332876295
Iter: 1009 loss: 0.00406116154
Iter: 1010 loss: 0.00406116154
Iter: 1011 loss: 0.00406116154
Iter: 1012 loss: 0.00406116154
Iter: 1013 loss: 0.00406116154
Iter: 1014 loss: 0.0332876295
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.4 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi2.4
+ date
Sun Nov  8 18:49:21 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.4/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.4/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 2.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aef2c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aef6b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aefc29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aeea0730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aeea0158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aefc22f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aeee50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aee6c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aee6c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aede8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aede82f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aed07f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aed278c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aedac1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aeda2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aeceb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aecf5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f738976cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aedef9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f738976c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aec43510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aec81268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73aec43e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73896a5c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73896a5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7389722ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7364735d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73897222f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73896ee158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f738970b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73896e6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7389690158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7389690510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7389698ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73646a51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73646c4c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 202, in <module>
    grads = tape.gradient(loss, model.trainable_weights)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1266, in _backward_function_wrapper
    processed_args, remapped_captures)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input is not invertible.
	 [[node gradients/MatrixDeterminant_grad/MatrixInverse (defined at biholoNN_train.py:200) ]] [Op:__inference___backward_volume_form_4446_8945]

Function call stack:
__backward_volume_form_4446

+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi2.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi2.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d9cc378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d9449d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d982950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840da3f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840da3f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840da3f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840da3f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d89a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d89a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d8270d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d82f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d80c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d827d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d840950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d7d0e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d7d0b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d788e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d840510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d80ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d6e51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d6c08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d6622f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d66d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f840d695f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83f31e5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83f31b5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83f31f4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83f31b5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83f31181e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83f31230d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83f3116510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83f30df1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83f30cf7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83cc0349d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83b020e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f83b021ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.00862749293
Iter: 2 loss: 0.187691391
Iter: 3 loss: 0.0185063556
Iter: 4 loss: 0.00845702458
Iter: 5 loss: 0.00794572197
Iter: 6 loss: 0.010940779
Iter: 7 loss: 0.00784397684
Iter: 8 loss: 0.00771799823
Iter: 9 loss: 0.00823429786
Iter: 10 loss: 0.00768808741
Iter: 11 loss: 0.00747918617
Iter: 12 loss: 0.00735512
Iter: 13 loss: 0.0072536543
Iter: 14 loss: 0.00710205082
Iter: 15 loss: 0.00868456252
Iter: 16 loss: 0.00709516741
Iter: 17 loss: 0.00698614214
Iter: 18 loss: 0.00720572099
Iter: 19 loss: 0.00693741115
Iter: 20 loss: 0.00680507906
Iter: 21 loss: 0.00697509386
Iter: 22 loss: 0.00673907669
Iter: 23 loss: 0.00660100859
Iter: 24 loss: 0.00638990197
Iter: 25 loss: 0.00638601603
Iter: 26 loss: 0.00619459525
Iter: 27 loss: 0.00689570606
Iter: 28 loss: 0.0061500459
Iter: 29 loss: 0.00598205626
Iter: 30 loss: 0.00694419257
Iter: 31 loss: 0.00596367382
Iter: 32 loss: 0.00581795536
Iter: 33 loss: 0.00612512371
Iter: 34 loss: 0.00576297473
Iter: 35 loss: 0.00561964652
Iter: 36 loss: 0.00607396
Iter: 37 loss: 0.00556998048
Iter: 38 loss: 0.00549319154
Iter: 39 loss: 0.0054904297
Iter: 40 loss: 0.0054241484
Iter: 41 loss: 0.00559031405
Iter: 42 loss: 0.00540285418
Iter: 43 loss: 0.00533381943
Iter: 44 loss: 0.00526674837
Iter: 45 loss: 0.00524998177
Iter: 46 loss: 0.0051482264
Iter: 47 loss: 0.00512670912
Iter: 48 loss: 0.00501668407
Iter: 49 loss: 0.00623775786
Iter: 50 loss: 0.00501489127
Iter: 51 loss: 0.00496587111
Iter: 52 loss: 0.00513302721
Iter: 53 loss: 0.00495309569
Iter: 54 loss: 0.00492077507
Iter: 55 loss: 0.00494144484
Iter: 56 loss: 0.00490007224
Iter: 57 loss: 0.00486723613
Iter: 58 loss: 0.00482394919
Iter: 59 loss: 0.00482143834
Iter: 60 loss: 0.00474060653
Iter: 61 loss: 0.0048849266
Iter: 62 loss: 0.00470572617
Iter: 63 loss: 0.0046477518
Iter: 64 loss: 0.00482842606
Iter: 65 loss: 0.00463074166
Iter: 66 loss: 0.00458650663
Iter: 67 loss: 0.00493242126
Iter: 68 loss: 0.00458332244
Iter: 69 loss: 0.00460206
Iter: 70 loss: 0.00457379222
Iter: 71 loss: 0.00455121137
Iter: 72 loss: 0.00463886186
Iter: 73 loss: 0.004546165
Iter: 74 loss: 0.00453445967
Iter: 75 loss: 0.00450353138
Iter: 76 loss: 0.00473537529
Iter: 77 loss: 0.0044971155
Iter: 78 loss: 0.00447368342
Iter: 79 loss: 0.0045030266
Iter: 80 loss: 0.00446143514
Iter: 81 loss: 0.00445034681
Iter: 82 loss: 0.00444436
Iter: 83 loss: 0.00443913881
Iter: 84 loss: 0.00442509074
Iter: 85 loss: 0.0044605406
Iter: 86 loss: 0.00442015845
Iter: 87 loss: 0.00440470828
Iter: 88 loss: 0.00451949798
Iter: 89 loss: 0.00440364704
Iter: 90 loss: 0.0043828534
Iter: 91 loss: 0.00436097616
Iter: 92 loss: 0.00435712794
Iter: 93 loss: 0.00433277432
Iter: 94 loss: 0.00433205254
Iter: 95 loss: 0.00430636341
Iter: 96 loss: 0.00440225843
Iter: 97 loss: 0.00430020131
Iter: 98 loss: 0.00428560795
Iter: 99 loss: 0.00426750351
Iter: 100 loss: 0.00426585972
Iter: 101 loss: 0.00425811484
Iter: 102 loss: 0.00425667223
Iter: 103 loss: 0.00424846262
Iter: 104 loss: 0.00430171425
Iter: 105 loss: 0.00424754806
Iter: 106 loss: 0.00423801178
Iter: 107 loss: 0.00422108825
Iter: 108 loss: 0.00422109
Iter: 109 loss: 0.00420456426
Iter: 110 loss: 0.0043548774
Iter: 111 loss: 0.00420338567
Iter: 112 loss: 0.00418413058
Iter: 113 loss: 0.00416637585
Iter: 114 loss: 0.00416172203
Iter: 115 loss: 0.00411965186
Iter: 116 loss: 0.00434098952
Iter: 117 loss: 0.00411231443
Iter: 118 loss: 0.00410160795
Iter: 119 loss: 0.00409180205
Iter: 120 loss: 0.00407132506
Iter: 121 loss: 0.00408127531
Iter: 122 loss: 0.00405784603
Iter: 123 loss: 0.00403940724
Iter: 124 loss: 0.00403848151
Iter: 125 loss: 0.00402666209
Iter: 126 loss: 0.00404264126
Iter: 127 loss: 0.00402084133
Iter: 128 loss: 0.00401183031
Iter: 129 loss: 0.00400444027
Iter: 130 loss: 0.00400178228
Iter: 131 loss: 0.00399361
Iter: 132 loss: 0.00398337655
Iter: 133 loss: 0.00398249645
Iter: 134 loss: 0.00397382304
Iter: 135 loss: 0.00396934943
Iter: 136 loss: 0.00394641142
Iter: 137 loss: 0.00400230754
Iter: 138 loss: 0.00393807376
Iter: 139 loss: 0.00392089039
Iter: 140 loss: 0.00399554335
Iter: 141 loss: 0.00391582772
Iter: 142 loss: 0.00389316492
Iter: 143 loss: 0.00392822083
Iter: 144 loss: 0.00388274342
Iter: 145 loss: 0.003850871
Iter: 146 loss: 0.00412229402
Iter: 147 loss: 0.00384738599
Iter: 148 loss: 0.00382433785
Iter: 149 loss: 0.00402608048
Iter: 150 loss: 0.0038229432
Iter: 151 loss: 0.00380322291
Iter: 152 loss: 0.00392906787
Iter: 153 loss: 0.00379920681
Iter: 154 loss: 0.00377834868
Iter: 155 loss: 0.00377168157
Iter: 156 loss: 0.00376007613
Iter: 157 loss: 0.00373815466
Iter: 158 loss: 0.00374812307
Iter: 159 loss: 0.00372279715
Iter: 160 loss: 0.00371278496
Iter: 161 loss: 0.00370802404
Iter: 162 loss: 0.00369307725
Iter: 163 loss: 0.00388197275
Iter: 164 loss: 0.00369288749
Iter: 165 loss: 0.0036729537
Iter: 166 loss: 0.00388575508
Iter: 167 loss: 0.00367238093
Iter: 168 loss: 0.00365693378
Iter: 169 loss: 0.00369085139
Iter: 170 loss: 0.00365101919
Iter: 171 loss: 0.00364326872
Iter: 172 loss: 0.00362852751
Iter: 173 loss: 0.00413297489
Iter: 174 loss: 0.00362850167
Iter: 175 loss: 0.00362888724
Iter: 176 loss: 0.0036156287
Iter: 177 loss: 0.00359918061
Iter: 178 loss: 0.00370402634
Iter: 179 loss: 0.00359654753
Iter: 180 loss: 0.00358007848
Iter: 181 loss: 0.00358945155
Iter: 182 loss: 0.0035697578
Iter: 183 loss: 0.0035519572
Iter: 184 loss: 0.00355194742
Iter: 185 loss: 0.0035438356
Iter: 186 loss: 0.00357207563
Iter: 187 loss: 0.00354177691
Iter: 188 loss: 0.00352315023
Iter: 189 loss: 0.00355452788
Iter: 190 loss: 0.00351437251
Iter: 191 loss: 0.00349809765
Iter: 192 loss: 0.00361591
Iter: 193 loss: 0.00349678565
Iter: 194 loss: 0.00348286843
Iter: 195 loss: 0.00351482769
Iter: 196 loss: 0.00347768096
Iter: 197 loss: 0.0034775
Iter: 198 loss: 0.00347304414
Iter: 199 loss: 0.00346691557
Iter: 200 loss: 0.00351799489
Iter: 201 loss: 0.00346645294
Iter: 202 loss: 0.00346237142
Iter: 203 loss: 0.00347268209
Iter: 204 loss: 0.00346095324
Iter: 205 loss: 0.00345637556
Iter: 206 loss: 0.00345981983
Iter: 207 loss: 0.00345346774
Iter: 208 loss: 0.00344584719
Iter: 209 loss: 0.00346442685
Iter: 210 loss: 0.00344310631
Iter: 211 loss: 0.00343277724
Iter: 212 loss: 0.00345485937
Iter: 213 loss: 0.00342886825
Iter: 214 loss: 0.00342070172
Iter: 215 loss: 0.00342334947
Iter: 216 loss: 0.0034147054
Iter: 217 loss: 0.00340521173
Iter: 218 loss: 0.003421325
Iter: 219 loss: 0.00340095116
Iter: 220 loss: 0.00338812894
Iter: 221 loss: 0.00337558892
Iter: 222 loss: 0.00337253418
Iter: 223 loss: 0.0033607271
Iter: 224 loss: 0.00341314357
Iter: 225 loss: 0.00335870357
Iter: 226 loss: 0.0033460455
Iter: 227 loss: 0.00345138507
Iter: 228 loss: 0.00334472791
Iter: 229 loss: 0.00333681819
Iter: 230 loss: 0.00334177213
Iter: 231 loss: 0.00333172921
Iter: 232 loss: 0.00332620135
Iter: 233 loss: 0.00332614314
Iter: 234 loss: 0.00331786484
Iter: 235 loss: 0.00331562385
Iter: 236 loss: 0.00331058889
Iter: 237 loss: 0.00330213737
Iter: 238 loss: 0.00330708362
Iter: 239 loss: 0.00329664466
Iter: 240 loss: 0.00328840199
Iter: 241 loss: 0.003282452
Iter: 242 loss: 0.00327972556
Iter: 243 loss: 0.00326127489
Iter: 244 loss: 0.00328403316
Iter: 245 loss: 0.00325050484
Iter: 246 loss: 0.00323471148
Iter: 247 loss: 0.0034478493
Iter: 248 loss: 0.00323413801
Iter: 249 loss: 0.00322261173
Iter: 250 loss: 0.0032220413
Iter: 251 loss: 0.00321627059
Iter: 252 loss: 0.0032193854
Iter: 253 loss: 0.00321194343
Iter: 254 loss: 0.00319933868
Iter: 255 loss: 0.00323106395
Iter: 256 loss: 0.00319495774
Iter: 257 loss: 0.00318711624
Iter: 258 loss: 0.00320059294
Iter: 259 loss: 0.00318362052
Iter: 260 loss: 0.00317911687
Iter: 261 loss: 0.00317299
Iter: 262 loss: 0.00317268725
Iter: 263 loss: 0.00315929553
Iter: 264 loss: 0.00318705896
Iter: 265 loss: 0.00315368199
Iter: 266 loss: 0.00317484746
Iter: 267 loss: 0.00314977299
Iter: 268 loss: 0.00314639765
Iter: 269 loss: 0.00314866379
Iter: 270 loss: 0.00314425025
Iter: 271 loss: 0.00313787488
Iter: 272 loss: 0.00313109183
Iter: 273 loss: 0.00312976469
Iter: 274 loss: 0.00312278
Iter: 275 loss: 0.0031242813
Iter: 276 loss: 0.00311805354
Iter: 277 loss: 0.00310525717
Iter: 278 loss: 0.00309922267
Iter: 279 loss: 0.00309374323
Iter: 280 loss: 0.00308262324
Iter: 281 loss: 0.00312025659
Iter: 282 loss: 0.00307971938
Iter: 283 loss: 0.00307447603
Iter: 284 loss: 0.00308562908
Iter: 285 loss: 0.00307278708
Iter: 286 loss: 0.00306376512
Iter: 287 loss: 0.00307673868
Iter: 288 loss: 0.00305906381
Iter: 289 loss: 0.00305252522
Iter: 290 loss: 0.00309707085
Iter: 291 loss: 0.00305192475
Iter: 292 loss: 0.00304477243
Iter: 293 loss: 0.00308664516
Iter: 294 loss: 0.00304376287
Iter: 295 loss: 0.00303964876
Iter: 296 loss: 0.0030336366
Iter: 297 loss: 0.00303345569
Iter: 298 loss: 0.00302449195
Iter: 299 loss: 0.00303092133
Iter: 300 loss: 0.00301874802
Iter: 301 loss: 0.00301149674
Iter: 302 loss: 0.00309266988
Iter: 303 loss: 0.00301138964
Iter: 304 loss: 0.0030040415
Iter: 305 loss: 0.00301965699
Iter: 306 loss: 0.00300119398
Iter: 307 loss: 0.00299420767
Iter: 308 loss: 0.00308492966
Iter: 309 loss: 0.00299390778
Iter: 310 loss: 0.00298686232
Iter: 311 loss: 0.00298654987
Iter: 312 loss: 0.00298113795
Iter: 313 loss: 0.00296679768
Iter: 314 loss: 0.00295918621
Iter: 315 loss: 0.00295244367
Iter: 316 loss: 0.0029395388
Iter: 317 loss: 0.00300461706
Iter: 318 loss: 0.00293746125
Iter: 319 loss: 0.00292911171
Iter: 320 loss: 0.00291470415
Iter: 321 loss: 0.00291469414
Iter: 322 loss: 0.00290713343
Iter: 323 loss: 0.00302556716
Iter: 324 loss: 0.00290711899
Iter: 325 loss: 0.00290222955
Iter: 326 loss: 0.00295318337
Iter: 327 loss: 0.00290208962
Iter: 328 loss: 0.0028985939
Iter: 329 loss: 0.00292012142
Iter: 330 loss: 0.00289820647
Iter: 331 loss: 0.00289390236
Iter: 332 loss: 0.00288979849
Iter: 333 loss: 0.00288880384
Iter: 334 loss: 0.00288137281
Iter: 335 loss: 0.00287495134
Iter: 336 loss: 0.0028729164
Iter: 337 loss: 0.00286346674
Iter: 338 loss: 0.00290979352
Iter: 339 loss: 0.00286177336
Iter: 340 loss: 0.0028613708
Iter: 341 loss: 0.0028554257
Iter: 342 loss: 0.00285297376
Iter: 343 loss: 0.00284915324
Iter: 344 loss: 0.00284910342
Iter: 345 loss: 0.0028443723
Iter: 346 loss: 0.00284254784
Iter: 347 loss: 0.00283993222
Iter: 348 loss: 0.0028315112
Iter: 349 loss: 0.00284249941
Iter: 350 loss: 0.00282699591
Iter: 351 loss: 0.00280890497
Iter: 352 loss: 0.00293028913
Iter: 353 loss: 0.00280720787
Iter: 354 loss: 0.00280430983
Iter: 355 loss: 0.00280004088
Iter: 356 loss: 0.0027946285
Iter: 357 loss: 0.00280618249
Iter: 358 loss: 0.00279234303
Iter: 359 loss: 0.00278615067
Iter: 360 loss: 0.00278296927
Iter: 361 loss: 0.00278020673
Iter: 362 loss: 0.00277880323
Iter: 363 loss: 0.00277585443
Iter: 364 loss: 0.00277121551
Iter: 365 loss: 0.00277754082
Iter: 366 loss: 0.0027688432
Iter: 367 loss: 0.00276446
Iter: 368 loss: 0.00276434328
Iter: 369 loss: 0.00276091695
Iter: 370 loss: 0.00275469664
Iter: 371 loss: 0.00282949791
Iter: 372 loss: 0.00275451364
Iter: 373 loss: 0.00274700345
Iter: 374 loss: 0.0027676
Iter: 375 loss: 0.00274498644
Iter: 376 loss: 0.00273634749
Iter: 377 loss: 0.00275596138
Iter: 378 loss: 0.0027325605
Iter: 379 loss: 0.00272536371
Iter: 380 loss: 0.00274949335
Iter: 381 loss: 0.0027236829
Iter: 382 loss: 0.00271555292
Iter: 383 loss: 0.0027235446
Iter: 384 loss: 0.00271079084
Iter: 385 loss: 0.00270556193
Iter: 386 loss: 0.00270621851
Iter: 387 loss: 0.00270157261
Iter: 388 loss: 0.00269428128
Iter: 389 loss: 0.00279783923
Iter: 390 loss: 0.00269425195
Iter: 391 loss: 0.00268753618
Iter: 392 loss: 0.00269008288
Iter: 393 loss: 0.00268276385
Iter: 394 loss: 0.00267381687
Iter: 395 loss: 0.00266442937
Iter: 396 loss: 0.00266283425
Iter: 397 loss: 0.00265329261
Iter: 398 loss: 0.00274623907
Iter: 399 loss: 0.00265286164
Iter: 400 loss: 0.00265352754
Iter: 401 loss: 0.00264866138
Iter: 402 loss: 0.00264670327
Iter: 403 loss: 0.00265402161
Iter: 404 loss: 0.00264629349
Iter: 405 loss: 0.00264176307
Iter: 406 loss: 0.00263372646
Iter: 407 loss: 0.00263371225
Iter: 408 loss: 0.00262838649
Iter: 409 loss: 0.00262347981
Iter: 410 loss: 0.00262221275
Iter: 411 loss: 0.00261031
Iter: 412 loss: 0.00271328236
Iter: 413 loss: 0.00260954909
Iter: 414 loss: 0.00259901816
Iter: 415 loss: 0.00266392669
Iter: 416 loss: 0.00259744166
Iter: 417 loss: 0.00259395526
Iter: 418 loss: 0.00259466376
Iter: 419 loss: 0.00259135454
Iter: 420 loss: 0.00258426135
Iter: 421 loss: 0.0026230684
Iter: 422 loss: 0.00258260872
Iter: 423 loss: 0.00257265661
Iter: 424 loss: 0.00260374579
Iter: 425 loss: 0.00256950478
Iter: 426 loss: 0.00255748956
Iter: 427 loss: 0.00264903158
Iter: 428 loss: 0.00255643413
Iter: 429 loss: 0.0025502136
Iter: 430 loss: 0.00262605515
Iter: 431 loss: 0.00255014119
Iter: 432 loss: 0.00254702894
Iter: 433 loss: 0.00254673022
Iter: 434 loss: 0.00254283403
Iter: 435 loss: 0.00254625315
Iter: 436 loss: 0.00254040025
Iter: 437 loss: 0.00253443141
Iter: 438 loss: 0.00262006558
Iter: 439 loss: 0.00253443047
Iter: 440 loss: 0.00252995314
Iter: 441 loss: 0.00253252592
Iter: 442 loss: 0.00252701226
Iter: 443 loss: 0.00252016704
Iter: 444 loss: 0.00255210605
Iter: 445 loss: 0.00251883501
Iter: 446 loss: 0.00251176069
Iter: 447 loss: 0.00253757462
Iter: 448 loss: 0.00250997255
Iter: 449 loss: 0.00250120088
Iter: 450 loss: 0.00251254532
Iter: 451 loss: 0.00249670958
Iter: 452 loss: 0.00248628948
Iter: 453 loss: 0.00250419625
Iter: 454 loss: 0.0024814338
Iter: 455 loss: 0.00247145304
Iter: 456 loss: 0.00257813488
Iter: 457 loss: 0.00247104117
Iter: 458 loss: 0.00246256148
Iter: 459 loss: 0.00246173237
Iter: 460 loss: 0.00245568249
Iter: 461 loss: 0.00249578524
Iter: 462 loss: 0.00245453347
Iter: 463 loss: 0.00245122
Iter: 464 loss: 0.00247005979
Iter: 465 loss: 0.0024507232
Iter: 466 loss: 0.00244717533
Iter: 467 loss: 0.00244713388
Iter: 468 loss: 0.00244415272
Iter: 469 loss: 0.00246559642
Iter: 470 loss: 0.0024439015
Iter: 471 loss: 0.00244266726
Iter: 472 loss: 0.00244253222
Iter: 473 loss: 0.00243999157
Iter: 474 loss: 0.00243549352
Iter: 475 loss: 0.0024354863
Iter: 476 loss: 0.00243089069
Iter: 477 loss: 0.00244254433
Iter: 478 loss: 0.00242993445
Iter: 479 loss: 0.00242613419
Iter: 480 loss: 0.00243538781
Iter: 481 loss: 0.00242473744
Iter: 482 loss: 0.00242348248
Iter: 483 loss: 0.00242337631
Iter: 484 loss: 0.00242243614
Iter: 485 loss: 0.00241878768
Iter: 486 loss: 0.002411847
Iter: 487 loss: 0.00256722141
Iter: 488 loss: 0.00241181906
Iter: 489 loss: 0.0025167889
Iter: 490 loss: 0.00240497524
Iter: 491 loss: 0.00239688763
Iter: 492 loss: 0.00242585735
Iter: 493 loss: 0.00239421963
Iter: 494 loss: 0.00238271337
Iter: 495 loss: 0.00254518073
Iter: 496 loss: 0.00238266774
Iter: 497 loss: 0.00237441156
Iter: 498 loss: 0.00241632131
Iter: 499 loss: 0.00237316359
Iter: 500 loss: 0.00237501599
Iter: 501 loss: 0.00237057265
Iter: 502 loss: 0.00236716447
Iter: 503 loss: 0.00237007812
Iter: 504 loss: 0.0023651
Iter: 505 loss: 0.00235833
Iter: 506 loss: 0.00240841834
Iter: 507 loss: 0.00235768547
Iter: 508 loss: 0.00235840119
Iter: 509 loss: 0.00235343026
Iter: 510 loss: 0.00234974315
Iter: 511 loss: 0.00234223227
Iter: 512 loss: 0.00249869423
Iter: 513 loss: 0.00234211585
Iter: 514 loss: 0.00233687251
Iter: 515 loss: 0.00236834539
Iter: 516 loss: 0.00233629206
Iter: 517 loss: 0.00233233673
Iter: 518 loss: 0.00234190887
Iter: 519 loss: 0.00233069132
Iter: 520 loss: 0.00232164841
Iter: 521 loss: 0.00244300161
Iter: 522 loss: 0.00232146028
Iter: 523 loss: 0.00231694151
Iter: 524 loss: 0.00234563951
Iter: 525 loss: 0.00231635012
Iter: 526 loss: 0.00231395569
Iter: 527 loss: 0.00231456687
Iter: 528 loss: 0.00231216475
Iter: 529 loss: 0.00230854889
Iter: 530 loss: 0.00230265502
Iter: 531 loss: 0.00230260962
Iter: 532 loss: 0.00234798156
Iter: 533 loss: 0.00229948759
Iter: 534 loss: 0.00229464658
Iter: 535 loss: 0.0023297444
Iter: 536 loss: 0.00229427498
Iter: 537 loss: 0.00228864886
Iter: 538 loss: 0.0022825459
Iter: 539 loss: 0.00228167628
Iter: 540 loss: 0.00227237819
Iter: 541 loss: 0.00235168799
Iter: 542 loss: 0.00227176538
Iter: 543 loss: 0.0022631688
Iter: 544 loss: 0.00229609455
Iter: 545 loss: 0.00226103771
Iter: 546 loss: 0.00225175335
Iter: 547 loss: 0.00227497448
Iter: 548 loss: 0.00224759872
Iter: 549 loss: 0.00225522276
Iter: 550 loss: 0.00224183
Iter: 551 loss: 0.00223420118
Iter: 552 loss: 0.00229702517
Iter: 553 loss: 0.00223329104
Iter: 554 loss: 0.00223444356
Iter: 555 loss: 0.00223072898
Iter: 556 loss: 0.00222599483
Iter: 557 loss: 0.0022408329
Iter: 558 loss: 0.00222427421
Iter: 559 loss: 0.00222019805
Iter: 560 loss: 0.00223119394
Iter: 561 loss: 0.00221888861
Iter: 562 loss: 0.00221050018
Iter: 563 loss: 0.00232985523
Iter: 564 loss: 0.00221035723
Iter: 565 loss: 0.00220373808
Iter: 566 loss: 0.00220253505
Iter: 567 loss: 0.00220065704
Iter: 568 loss: 0.00222493149
Iter: 569 loss: 0.00220065028
Iter: 570 loss: 0.00219771871
Iter: 571 loss: 0.00219089235
Iter: 572 loss: 0.00235035573
Iter: 573 loss: 0.00219058292
Iter: 574 loss: 0.00218420825
Iter: 575 loss: 0.00218382617
Iter: 576 loss: 0.00217992184
Iter: 577 loss: 0.00219173916
Iter: 578 loss: 0.00217881426
Iter: 579 loss: 0.00217455532
Iter: 580 loss: 0.002171495
Iter: 581 loss: 0.00216988358
Iter: 582 loss: 0.00216475222
Iter: 583 loss: 0.00218408881
Iter: 584 loss: 0.00216366746
Iter: 585 loss: 0.00215950329
Iter: 586 loss: 0.00215909025
Iter: 587 loss: 0.00215384
Iter: 588 loss: 0.00221783319
Iter: 589 loss: 0.00215374189
Iter: 590 loss: 0.00215080101
Iter: 591 loss: 0.00215199613
Iter: 592 loss: 0.00214883359
Iter: 593 loss: 0.00214421493
Iter: 594 loss: 0.00214963709
Iter: 595 loss: 0.00214147614
Iter: 596 loss: 0.00213786028
Iter: 597 loss: 0.00213802559
Iter: 598 loss: 0.00213504024
Iter: 599 loss: 0.00212711492
Iter: 600 loss: 0.00227849884
Iter: 601 loss: 0.00212710304
Iter: 602 loss: 0.00212250743
Iter: 603 loss: 0.00212238
Iter: 604 loss: 0.00211943127
Iter: 605 loss: 0.00213860092
Iter: 606 loss: 0.00211910298
Iter: 607 loss: 0.00211652741
Iter: 608 loss: 0.00212562154
Iter: 609 loss: 0.00211560167
Iter: 610 loss: 0.00211101654
Iter: 611 loss: 0.00211361493
Iter: 612 loss: 0.00210804702
Iter: 613 loss: 0.00210189121
Iter: 614 loss: 0.00210187864
Iter: 615 loss: 0.00209501339
Iter: 616 loss: 0.00211148825
Iter: 617 loss: 0.00209253817
Iter: 618 loss: 0.00208587619
Iter: 619 loss: 0.00223208684
Iter: 620 loss: 0.00208573556
Iter: 621 loss: 0.00208036485
Iter: 622 loss: 0.0021356463
Iter: 623 loss: 0.00208024704
Iter: 624 loss: 0.0020751392
Iter: 625 loss: 0.00208442984
Iter: 626 loss: 0.00207270728
Iter: 627 loss: 0.00206719176
Iter: 628 loss: 0.00208854256
Iter: 629 loss: 0.00206616521
Iter: 630 loss: 0.00206129509
Iter: 631 loss: 0.00207882491
Iter: 632 loss: 0.00205976237
Iter: 633 loss: 0.00205582986
Iter: 634 loss: 0.00205611251
Iter: 635 loss: 0.00205271109
Iter: 636 loss: 0.00205050875
Iter: 637 loss: 0.00204954157
Iter: 638 loss: 0.00204697112
Iter: 639 loss: 0.00205748505
Iter: 640 loss: 0.00204642816
Iter: 641 loss: 0.00204386143
Iter: 642 loss: 0.00204486959
Iter: 643 loss: 0.0020417762
Iter: 644 loss: 0.00203615241
Iter: 645 loss: 0.00204230612
Iter: 646 loss: 0.0020330851
Iter: 647 loss: 0.00202740589
Iter: 648 loss: 0.00206181756
Iter: 649 loss: 0.00202672346
Iter: 650 loss: 0.00202233577
Iter: 651 loss: 0.00204624794
Iter: 652 loss: 0.00202167174
Iter: 653 loss: 0.00201959885
Iter: 654 loss: 0.00201694923
Iter: 655 loss: 0.00201672595
Iter: 656 loss: 0.00201702514
Iter: 657 loss: 0.00201146561
Iter: 658 loss: 0.00200673891
Iter: 659 loss: 0.00203745649
Iter: 660 loss: 0.00200619758
Iter: 661 loss: 0.00200467324
Iter: 662 loss: 0.0020108223
Iter: 663 loss: 0.00200423482
Iter: 664 loss: 0.0019991342
Iter: 665 loss: 0.00200784858
Iter: 666 loss: 0.00199687877
Iter: 667 loss: 0.00201365189
Iter: 668 loss: 0.00199208548
Iter: 669 loss: 0.00198834785
Iter: 670 loss: 0.00199153251
Iter: 671 loss: 0.0019862398
Iter: 672 loss: 0.00198160019
Iter: 673 loss: 0.00198799279
Iter: 674 loss: 0.00197915267
Iter: 675 loss: 0.00197521271
Iter: 676 loss: 0.00197639037
Iter: 677 loss: 0.00197241548
Iter: 678 loss: 0.00196710508
Iter: 679 loss: 0.00206677616
Iter: 680 loss: 0.00196706667
Iter: 681 loss: 0.00196293672
Iter: 682 loss: 0.00200097635
Iter: 683 loss: 0.00196279795
Iter: 684 loss: 0.00196069
Iter: 685 loss: 0.00195728173
Iter: 686 loss: 0.00195725588
Iter: 687 loss: 0.00195440976
Iter: 688 loss: 0.00195436366
Iter: 689 loss: 0.00195187214
Iter: 690 loss: 0.00196514302
Iter: 691 loss: 0.00195151649
Iter: 692 loss: 0.0019509613
Iter: 693 loss: 0.00194848992
Iter: 694 loss: 0.00194555498
Iter: 695 loss: 0.00194914732
Iter: 696 loss: 0.0019440013
Iter: 697 loss: 0.00194158987
Iter: 698 loss: 0.00193536305
Iter: 699 loss: 0.00198660884
Iter: 700 loss: 0.00193424779
Iter: 701 loss: 0.00192899397
Iter: 702 loss: 0.00195145211
Iter: 703 loss: 0.00192781596
Iter: 704 loss: 0.00192188611
Iter: 705 loss: 0.00200443435
Iter: 706 loss: 0.00192186411
Iter: 707 loss: 0.00191840075
Iter: 708 loss: 0.00191446533
Iter: 709 loss: 0.00191400526
Iter: 710 loss: 0.00190872164
Iter: 711 loss: 0.00189794588
Iter: 712 loss: 0.00248062983
Iter: 713 loss: 0.00189792225
Iter: 714 loss: 0.0018868685
Iter: 715 loss: 0.00199050899
Iter: 716 loss: 0.00188615709
Iter: 717 loss: 0.00188147
Iter: 718 loss: 0.00188132667
Iter: 719 loss: 0.00188042596
Iter: 720 loss: 0.0018791568
Iter: 721 loss: 0.00187685317
Iter: 722 loss: 0.0018760619
Iter: 723 loss: 0.00187474419
Iter: 724 loss: 0.00187081576
Iter: 725 loss: 0.00189424818
Iter: 726 loss: 0.00187033881
Iter: 727 loss: 0.00186697836
Iter: 728 loss: 0.00186697638
Iter: 729 loss: 0.00186383515
Iter: 730 loss: 0.00187469041
Iter: 731 loss: 0.00186303246
Iter: 732 loss: 0.00186013966
Iter: 733 loss: 0.00186667952
Iter: 734 loss: 0.00185887702
Iter: 735 loss: 0.00185589283
Iter: 736 loss: 0.00185985176
Iter: 737 loss: 0.00185438967
Iter: 738 loss: 0.00185072178
Iter: 739 loss: 0.00186196098
Iter: 740 loss: 0.00184962759
Iter: 741 loss: 0.00184519181
Iter: 742 loss: 0.00184180541
Iter: 743 loss: 0.00184037595
Iter: 744 loss: 0.00183566974
Iter: 745 loss: 0.0018422564
Iter: 746 loss: 0.00183315366
Iter: 747 loss: 0.00183104246
Iter: 748 loss: 0.0018333348
Iter: 749 loss: 0.00183007552
Iter: 750 loss: 0.00182496884
Iter: 751 loss: 0.00181786087
Iter: 752 loss: 0.00181743898
Iter: 753 loss: 0.00181016827
Iter: 754 loss: 0.00183869433
Iter: 755 loss: 0.00180868432
Iter: 756 loss: 0.0018031525
Iter: 757 loss: 0.00187511405
Iter: 758 loss: 0.00180308463
Iter: 759 loss: 0.00180035
Iter: 760 loss: 0.00180264201
Iter: 761 loss: 0.00179871824
Iter: 762 loss: 0.0017946919
Iter: 763 loss: 0.00181475957
Iter: 764 loss: 0.00179403
Iter: 765 loss: 0.00179081946
Iter: 766 loss: 0.00179078721
Iter: 767 loss: 0.00178983528
Iter: 768 loss: 0.00178851141
Iter: 769 loss: 0.001788461
Iter: 770 loss: 0.0017859498
Iter: 771 loss: 0.00179081573
Iter: 772 loss: 0.00178473513
Iter: 773 loss: 0.00178188877
Iter: 774 loss: 0.00179192901
Iter: 775 loss: 0.00178112066
Iter: 776 loss: 0.00178323593
Iter: 777 loss: 0.00177896372
Iter: 778 loss: 0.00177625776
Iter: 779 loss: 0.00180171081
Iter: 780 loss: 0.00177615206
Iter: 781 loss: 0.00177516951
Iter: 782 loss: 0.00177342934
Iter: 783 loss: 0.00177342258
Iter: 784 loss: 0.00177039
Iter: 785 loss: 0.00176493719
Iter: 786 loss: 0.00189991738
Iter: 787 loss: 0.00176493067
Iter: 788 loss: 0.00175899954
Iter: 789 loss: 0.00175899453
Iter: 790 loss: 0.0017550959
Iter: 791 loss: 0.00178040669
Iter: 792 loss: 0.00175465946
Iter: 793 loss: 0.00175148528
Iter: 794 loss: 0.00175415992
Iter: 795 loss: 0.00174962438
Iter: 796 loss: 0.00174671481
Iter: 797 loss: 0.00174331584
Iter: 798 loss: 0.00174293388
Iter: 799 loss: 0.0017452616
Iter: 800 loss: 0.00174176833
Iter: 801 loss: 0.00174026424
Iter: 802 loss: 0.00174194237
Iter: 803 loss: 0.00173943979
Iter: 804 loss: 0.00173630205
Iter: 805 loss: 0.00174696173
Iter: 806 loss: 0.00173545978
Iter: 807 loss: 0.00173018407
Iter: 808 loss: 0.00174424867
Iter: 809 loss: 0.00172845169
Iter: 810 loss: 0.00172515505
Iter: 811 loss: 0.00173380016
Iter: 812 loss: 0.00172404945
Iter: 813 loss: 0.00171986502
Iter: 814 loss: 0.0017132645
Iter: 815 loss: 0.0017131858
Iter: 816 loss: 0.00170884386
Iter: 817 loss: 0.00171362027
Iter: 818 loss: 0.00170648587
Iter: 819 loss: 0.00170290819
Iter: 820 loss: 0.00173440413
Iter: 821 loss: 0.00170269306
Iter: 822 loss: 0.00169987162
Iter: 823 loss: 0.00169604132
Iter: 824 loss: 0.00169586239
Iter: 825 loss: 0.00169240078
Iter: 826 loss: 0.00169925962
Iter: 827 loss: 0.00169093884
Iter: 828 loss: 0.00168905815
Iter: 829 loss: 0.00168927456
Iter: 830 loss: 0.0016876437
Iter: 831 loss: 0.00168412714
Iter: 832 loss: 0.00168411736
Iter: 833 loss: 0.00168063969
Iter: 834 loss: 0.00170110201
Iter: 835 loss: 0.00168007391
Iter: 836 loss: 0.00167697819
Iter: 837 loss: 0.00169368612
Iter: 838 loss: 0.00167650613
Iter: 839 loss: 0.00167299807
Iter: 840 loss: 0.00168576045
Iter: 841 loss: 0.00167212659
Iter: 842 loss: 0.00167089759
Iter: 843 loss: 0.00167082867
Iter: 844 loss: 0.00166990503
Iter: 845 loss: 0.00166646158
Iter: 846 loss: 0.00167832512
Iter: 847 loss: 0.00166551606
Iter: 848 loss: 0.00165929855
Iter: 849 loss: 0.00167887495
Iter: 850 loss: 0.00165748654
Iter: 851 loss: 0.00165332423
Iter: 852 loss: 0.00165687548
Iter: 853 loss: 0.00165086077
Iter: 854 loss: 0.00164450053
Iter: 855 loss: 0.00168044586
Iter: 856 loss: 0.00164358062
Iter: 857 loss: 0.00164144579
Iter: 858 loss: 0.00164594816
Iter: 859 loss: 0.00164060434
Iter: 860 loss: 0.00163813028
Iter: 861 loss: 0.0016356156
Iter: 862 loss: 0.00163511652
Iter: 863 loss: 0.00163252454
Iter: 864 loss: 0.00164664967
Iter: 865 loss: 0.00163206097
Iter: 866 loss: 0.00163042184
Iter: 867 loss: 0.00163728592
Iter: 868 loss: 0.00163007481
Iter: 869 loss: 0.00162523007
Iter: 870 loss: 0.0016565409
Iter: 871 loss: 0.00162464194
Iter: 872 loss: 0.00162896095
Iter: 873 loss: 0.00162171759
Iter: 874 loss: 0.00161998603
Iter: 875 loss: 0.00162854069
Iter: 876 loss: 0.00161969475
Iter: 877 loss: 0.00161702943
Iter: 878 loss: 0.00161494024
Iter: 879 loss: 0.00161411136
Iter: 880 loss: 0.00161106023
Iter: 881 loss: 0.00162928202
Iter: 882 loss: 0.00161046279
Iter: 883 loss: 0.00160881528
Iter: 884 loss: 0.00161146501
Iter: 885 loss: 0.00160808279
Iter: 886 loss: 0.00160690118
Iter: 887 loss: 0.00160747929
Iter: 888 loss: 0.00160610338
Iter: 889 loss: 0.00160479092
Iter: 890 loss: 0.00160138961
Iter: 891 loss: 0.00162610214
Iter: 892 loss: 0.00160075049
Iter: 893 loss: 0.00159852696
Iter: 894 loss: 0.00161664968
Iter: 895 loss: 0.00159828621
Iter: 896 loss: 0.00159717281
Iter: 897 loss: 0.00159537466
Iter: 898 loss: 0.0015953586
Iter: 899 loss: 0.00161000551
Iter: 900 loss: 0.00159361714
Iter: 901 loss: 0.00159091479
Iter: 902 loss: 0.0015972734
Iter: 903 loss: 0.00158989336
Iter: 904 loss: 0.00158811128
Iter: 905 loss: 0.00158595177
Iter: 906 loss: 0.0015857151
Iter: 907 loss: 0.00158266234
Iter: 908 loss: 0.00158944912
Iter: 909 loss: 0.00158152916
Iter: 910 loss: 0.00157823693
Iter: 911 loss: 0.00160358381
Iter: 912 loss: 0.0015778885
Iter: 913 loss: 0.00157633866
Iter: 914 loss: 0.00158326817
Iter: 915 loss: 0.00157607021
Iter: 916 loss: 0.00157458615
Iter: 917 loss: 0.001571181
Iter: 918 loss: 0.00161306211
Iter: 919 loss: 0.0015709186
Iter: 920 loss: 0.00156714651
Iter: 921 loss: 0.00156714837
Iter: 922 loss: 0.0015640182
Iter: 923 loss: 0.00157599733
Iter: 924 loss: 0.00156321668
Iter: 925 loss: 0.00156040234
Iter: 926 loss: 0.00156509457
Iter: 927 loss: 0.00155910093
Iter: 928 loss: 0.00155541557
Iter: 929 loss: 0.00155712117
Iter: 930 loss: 0.0015529861
Iter: 931 loss: 0.00155113591
Iter: 932 loss: 0.00155033311
Iter: 933 loss: 0.00154932821
Iter: 934 loss: 0.00154689653
Iter: 935 loss: 0.00157816615
Iter: 936 loss: 0.0015468921
Iter: 937 loss: 0.00154523214
Iter: 938 loss: 0.00156069547
Iter: 939 loss: 0.00154513679
Iter: 940 loss: 0.00154438661
Iter: 941 loss: 0.00154344807
Iter: 942 loss: 0.00154337694
Iter: 943 loss: 0.00154154806
Iter: 944 loss: 0.001540325
Iter: 945 loss: 0.00153961359
Iter: 946 loss: 0.00153674162
Iter: 947 loss: 0.00156520563
Iter: 948 loss: 0.0015366422
Iter: 949 loss: 0.00153457047
Iter: 950 loss: 0.00154105248
Iter: 951 loss: 0.00153399608
Iter: 952 loss: 0.00153295835
Iter: 953 loss: 0.00154443551
Iter: 954 loss: 0.00153291225
Iter: 955 loss: 0.00153228827
Iter: 956 loss: 0.00153324683
Iter: 957 loss: 0.00153198931
Iter: 958 loss: 0.00153060129
Iter: 959 loss: 0.00152728066
Iter: 960 loss: 0.00156549306
Iter: 961 loss: 0.00152695877
Iter: 962 loss: 0.00152178737
Iter: 963 loss: 0.00158368528
Iter: 964 loss: 0.00152165268
Iter: 965 loss: 0.00152300973
Iter: 966 loss: 0.00151822215
Iter: 967 loss: 0.00151486695
Iter: 968 loss: 0.00151452923
Iter: 969 loss: 0.00150873861
Iter: 970 loss: 0.0015952281
Iter: 971 loss: 0.00150842057
Iter: 972 loss: 0.00150673499
Iter: 973 loss: 0.00150281098
Iter: 974 loss: 0.00150063518
Iter: 975 loss: 0.00150850206
Iter: 976 loss: 0.00149995019
Iter: 977 loss: 0.00149737578
Iter: 978 loss: 0.00150129502
Iter: 979 loss: 0.00149617507
Iter: 980 loss: 0.00149299041
Iter: 981 loss: 0.0014981227
Iter: 982 loss: 0.00149151869
Iter: 983 loss: 0.00153194927
Iter: 984 loss: 0.00149008946
Iter: 985 loss: 0.00148823147
Iter: 986 loss: 0.00148265925
Iter: 987 loss: 0.00150128128
Iter: 988 loss: 0.00147990976
Iter: 989 loss: 0.00147348177
Iter: 990 loss: 0.00150453218
Iter: 991 loss: 0.00147224031
Iter: 992 loss: 0.00147030037
Iter: 993 loss: 0.00146971492
Iter: 994 loss: 0.00146846008
Iter: 995 loss: 0.00146761583
Iter: 996 loss: 0.001467144
Iter: 997 loss: 0.00146515516
Iter: 998 loss: 0.0014690191
Iter: 999 loss: 0.00146436493
Iter: 1000 loss: 0.00146328169
Iter: 1001 loss: 0.00147049199
Iter: 1002 loss: 0.00146308239
Iter: 1003 loss: 0.00146158994
Iter: 1004 loss: 0.00146418228
Iter: 1005 loss: 0.00146095757
Iter: 1006 loss: 0.00145893916
Iter: 1007 loss: 0.00146405993
Iter: 1008 loss: 0.00145825231
Iter: 1009 loss: 0.00145638431
Iter: 1010 loss: 0.0014549077
Iter: 1011 loss: 0.00145433
Iter: 1012 loss: 0.00145141827
Iter: 1013 loss: 0.00144995155
Iter: 1014 loss: 0.00144857494
Iter: 1015 loss: 0.00144379283
Iter: 1016 loss: 0.00144530344
Iter: 1017 loss: 0.00144041074
Iter: 1018 loss: 0.00146299321
Iter: 1019 loss: 0.00143877661
Iter: 1020 loss: 0.00143785356
Iter: 1021 loss: 0.00145055389
Iter: 1022 loss: 0.00143781235
Iter: 1023 loss: 0.00143711478
Iter: 1024 loss: 0.00143490359
Iter: 1025 loss: 0.00143812504
Iter: 1026 loss: 0.00143330346
Iter: 1027 loss: 0.0014283763
Iter: 1028 loss: 0.00144372345
Iter: 1029 loss: 0.00142683
Iter: 1030 loss: 0.00150772545
Iter: 1031 loss: 0.00142616848
Iter: 1032 loss: 0.00142574334
Iter: 1033 loss: 0.00142631645
Iter: 1034 loss: 0.00142553262
Iter: 1035 loss: 0.00142445508
Iter: 1036 loss: 0.00142189907
Iter: 1037 loss: 0.00144992094
Iter: 1038 loss: 0.00142165041
Iter: 1039 loss: 0.00141870836
Iter: 1040 loss: 0.00141747459
Iter: 1041 loss: 0.0014159017
Iter: 1042 loss: 0.00141436839
Iter: 1043 loss: 0.0014143663
Iter: 1044 loss: 0.00141323416
Iter: 1045 loss: 0.00141214347
Iter: 1046 loss: 0.00141191692
Iter: 1047 loss: 0.00140875718
Iter: 1048 loss: 0.00142878527
Iter: 1049 loss: 0.00140820991
Iter: 1050 loss: 0.00141138863
Iter: 1051 loss: 0.0014062929
Iter: 1052 loss: 0.00140438008
Iter: 1053 loss: 0.00140795566
Iter: 1054 loss: 0.00140357332
Iter: 1055 loss: 0.00140158948
Iter: 1056 loss: 0.00140104361
Iter: 1057 loss: 0.00139978691
Iter: 1058 loss: 0.00139403809
Iter: 1059 loss: 0.00140825682
Iter: 1060 loss: 0.00139197335
Iter: 1061 loss: 0.00139219291
Iter: 1062 loss: 0.00138835306
Iter: 1063 loss: 0.0013869293
Iter: 1064 loss: 0.00139738037
Iter: 1065 loss: 0.00138680614
Iter: 1066 loss: 0.00138338702
Iter: 1067 loss: 0.00138341449
Iter: 1068 loss: 0.00138053193
Iter: 1069 loss: 0.00137754914
Iter: 1070 loss: 0.00137676054
Iter: 1071 loss: 0.00137577974
Iter: 1072 loss: 0.00137551443
Iter: 1073 loss: 0.001374949
Iter: 1074 loss: 0.00137357321
Iter: 1075 loss: 0.00138622755
Iter: 1076 loss: 0.0013733845
Iter: 1077 loss: 0.00137126469
Iter: 1078 loss: 0.0013735611
Iter: 1079 loss: 0.00137006934
Iter: 1080 loss: 0.00136810809
Iter: 1081 loss: 0.00136989669
Iter: 1082 loss: 0.00136698992
Iter: 1083 loss: 0.00136381947
Iter: 1084 loss: 0.00136712263
Iter: 1085 loss: 0.0013618716
Iter: 1086 loss: 0.00135858613
Iter: 1087 loss: 0.00135862082
Iter: 1088 loss: 0.0013559846
Iter: 1089 loss: 0.00135265884
Iter: 1090 loss: 0.00136655488
Iter: 1091 loss: 0.0013521217
Iter: 1092 loss: 0.00135004707
Iter: 1093 loss: 0.00134997279
Iter: 1094 loss: 0.00134880852
Iter: 1095 loss: 0.0013539819
Iter: 1096 loss: 0.00134856452
Iter: 1097 loss: 0.00134580419
Iter: 1098 loss: 0.001346551
Iter: 1099 loss: 0.00134362921
Iter: 1100 loss: 0.00133932196
Iter: 1101 loss: 0.00133921974
Iter: 1102 loss: 0.00133718434
Iter: 1103 loss: 0.00134301977
Iter: 1104 loss: 0.0013365557
Iter: 1105 loss: 0.00133463112
Iter: 1106 loss: 0.00133100653
Iter: 1107 loss: 0.00141414709
Iter: 1108 loss: 0.00133099291
Iter: 1109 loss: 0.00132981013
Iter: 1110 loss: 0.00132931955
Iter: 1111 loss: 0.0013259137
Iter: 1112 loss: 0.00135178934
Iter: 1113 loss: 0.00132561894
Iter: 1114 loss: 0.00132295908
Iter: 1115 loss: 0.00132320274
Iter: 1116 loss: 0.00132087583
Iter: 1117 loss: 0.0013190693
Iter: 1118 loss: 0.00131932308
Iter: 1119 loss: 0.00131769758
Iter: 1120 loss: 0.00131508661
Iter: 1121 loss: 0.00131026236
Iter: 1122 loss: 0.00146720884
Iter: 1123 loss: 0.00131025421
Iter: 1124 loss: 0.00130843569
Iter: 1125 loss: 0.00130804849
Iter: 1126 loss: 0.00130746549
Iter: 1127 loss: 0.00131115247
Iter: 1128 loss: 0.00130740087
Iter: 1129 loss: 0.00130411785
Iter: 1130 loss: 0.00131200009
Iter: 1131 loss: 0.00130289653
Iter: 1132 loss: 0.00130009651
Iter: 1133 loss: 0.00131074851
Iter: 1134 loss: 0.00129939895
Iter: 1135 loss: 0.00129831443
Iter: 1136 loss: 0.00130061689
Iter: 1137 loss: 0.00129787694
Iter: 1138 loss: 0.00129650813
Iter: 1139 loss: 0.00131175644
Iter: 1140 loss: 0.00129647925
Iter: 1141 loss: 0.00129515468
Iter: 1142 loss: 0.00129843806
Iter: 1143 loss: 0.00129467715
Iter: 1144 loss: 0.00129286642
Iter: 1145 loss: 0.0013028119
Iter: 1146 loss: 0.00129257247
Iter: 1147 loss: 0.0012911174
Iter: 1148 loss: 0.00131384144
Iter: 1149 loss: 0.00129111414
Iter: 1150 loss: 0.00129041087
Iter: 1151 loss: 0.00128955941
Iter: 1152 loss: 0.00128948141
Iter: 1153 loss: 0.00128995231
Iter: 1154 loss: 0.00128880749
Iter: 1155 loss: 0.00128719129
Iter: 1156 loss: 0.00130514789
Iter: 1157 loss: 0.00128711131
Iter: 1158 loss: 0.001286702
Iter: 1159 loss: 0.00128893903
Iter: 1160 loss: 0.00128664554
Iter: 1161 loss: 0.00128576544
Iter: 1162 loss: 0.00128378824
Iter: 1163 loss: 0.00131687487
Iter: 1164 loss: 0.00128369068
Iter: 1165 loss: 0.00129040424
Iter: 1166 loss: 0.00128297647
Iter: 1167 loss: 0.00128207006
Iter: 1168 loss: 0.00128370058
Iter: 1169 loss: 0.00128168077
Iter: 1170 loss: 0.00128066365
Iter: 1171 loss: 0.00128088298
Iter: 1172 loss: 0.00127981184
Iter: 1173 loss: 0.00127882371
Iter: 1174 loss: 0.00128679373
Iter: 1175 loss: 0.00127876922
Iter: 1176 loss: 0.00127781474
Iter: 1177 loss: 0.00127737178
Iter: 1178 loss: 0.00127684651
Iter: 1179 loss: 0.00127568538
Iter: 1180 loss: 0.00128391781
Iter: 1181 loss: 0.00127559784
Iter: 1182 loss: 0.0012752153
Iter: 1183 loss: 0.00127551518
Iter: 1184 loss: 0.00127495895
Iter: 1185 loss: 0.00127432041
Iter: 1186 loss: 0.00127331819
Iter: 1187 loss: 0.00127330888
Iter: 1188 loss: 0.0012778563
Iter: 1189 loss: 0.00127273402
Iter: 1190 loss: 0.00127166952
Iter: 1191 loss: 0.0012728835
Iter: 1192 loss: 0.00127112
Iter: 1193 loss: 0.0012702581
Iter: 1194 loss: 0.00127060292
Iter: 1195 loss: 0.0012696163
Iter: 1196 loss: 0.00126902177
Iter: 1197 loss: 0.00126901828
Iter: 1198 loss: 0.0012685965
Iter: 1199 loss: 0.00126834633
Iter: 1200 loss: 0.00126816717
Iter: 1201 loss: 0.00126710266
Iter: 1202 loss: 0.00126768695
Iter: 1203 loss: 0.00126639986
Iter: 1204 loss: 0.00126536889
Iter: 1205 loss: 0.00126380112
Iter: 1206 loss: 0.00126376562
Iter: 1207 loss: 0.0012609011
Iter: 1208 loss: 0.00125780306
Iter: 1209 loss: 0.00125726871
Iter: 1210 loss: 0.00125577266
Iter: 1211 loss: 0.00127080688
Iter: 1212 loss: 0.00125573925
Iter: 1213 loss: 0.00125509035
Iter: 1214 loss: 0.00125578931
Iter: 1215 loss: 0.00125470199
Iter: 1216 loss: 0.00125369802
Iter: 1217 loss: 0.00125396461
Iter: 1218 loss: 0.00125295774
Iter: 1219 loss: 0.00124862324
Iter: 1220 loss: 0.00131672481
Iter: 1221 loss: 0.00124858203
Iter: 1222 loss: 0.00124641822
Iter: 1223 loss: 0.00127125904
Iter: 1224 loss: 0.00124634989
Iter: 1225 loss: 0.00124781416
Iter: 1226 loss: 0.00124516967
Iter: 1227 loss: 0.00124446943
Iter: 1228 loss: 0.00124376721
Iter: 1229 loss: 0.00124362414
Iter: 1230 loss: 0.00124225905
Iter: 1231 loss: 0.00124181388
Iter: 1232 loss: 0.00124098349
Iter: 1233 loss: 0.00123916171
Iter: 1234 loss: 0.00124851125
Iter: 1235 loss: 0.0012388532
Iter: 1236 loss: 0.00123803038
Iter: 1237 loss: 0.00124215521
Iter: 1238 loss: 0.00123793387
Iter: 1239 loss: 0.00123734644
Iter: 1240 loss: 0.00123813655
Iter: 1241 loss: 0.00123700267
Iter: 1242 loss: 0.00123649929
Iter: 1243 loss: 0.00123723852
Iter: 1244 loss: 0.00123621849
Iter: 1245 loss: 0.00123459497
Iter: 1246 loss: 0.00123422267
Iter: 1247 loss: 0.001233196
Iter: 1248 loss: 0.00123217062
Iter: 1249 loss: 0.00124655035
Iter: 1250 loss: 0.00123213697
Iter: 1251 loss: 0.00123172626
Iter: 1252 loss: 0.00123375724
Iter: 1253 loss: 0.00123167457
Iter: 1254 loss: 0.00123039063
Iter: 1255 loss: 0.00124752708
Iter: 1256 loss: 0.00123036304
Iter: 1257 loss: 0.00122890179
Iter: 1258 loss: 0.0012302103
Iter: 1259 loss: 0.00122801831
Iter: 1260 loss: 0.0012248836
Iter: 1261 loss: 0.00123102986
Iter: 1262 loss: 0.00122360233
Iter: 1263 loss: 0.00121957704
Iter: 1264 loss: 0.00123469415
Iter: 1265 loss: 0.0012186463
Iter: 1266 loss: 0.0012174712
Iter: 1267 loss: 0.00121743989
Iter: 1268 loss: 0.00121689448
Iter: 1269 loss: 0.00121811882
Iter: 1270 loss: 0.00121667073
Iter: 1271 loss: 0.00121573161
Iter: 1272 loss: 0.00121410168
Iter: 1273 loss: 0.00121409923
Iter: 1274 loss: 0.0012124842
Iter: 1275 loss: 0.0012193385
Iter: 1276 loss: 0.00121222297
Iter: 1277 loss: 0.00121154753
Iter: 1278 loss: 0.0012165365
Iter: 1279 loss: 0.0012114899
Iter: 1280 loss: 0.00121118617
Iter: 1281 loss: 0.0012110211
Iter: 1282 loss: 0.00121019082
Iter: 1283 loss: 0.00121012749
Iter: 1284 loss: 0.00120950141
Iter: 1285 loss: 0.00120865158
Iter: 1286 loss: 0.00120657403
Iter: 1287 loss: 0.00122631085
Iter: 1288 loss: 0.0012063157
Iter: 1289 loss: 0.00120405504
Iter: 1290 loss: 0.00120738009
Iter: 1291 loss: 0.00120285875
Iter: 1292 loss: 0.00120094512
Iter: 1293 loss: 0.00120126433
Iter: 1294 loss: 0.00119949249
Iter: 1295 loss: 0.00119696278
Iter: 1296 loss: 0.00120486098
Iter: 1297 loss: 0.00119582901
Iter: 1298 loss: 0.00119157822
Iter: 1299 loss: 0.00120890082
Iter: 1300 loss: 0.00119092572
Iter: 1301 loss: 0.00118914433
Iter: 1302 loss: 0.00118902593
Iter: 1303 loss: 0.00118723628
Iter: 1304 loss: 0.00118706096
Iter: 1305 loss: 0.00118658738
Iter: 1306 loss: 0.00118903571
Iter: 1307 loss: 0.00118649192
Iter: 1308 loss: 0.00118599925
Iter: 1309 loss: 0.00118563185
Iter: 1310 loss: 0.00118549529
Iter: 1311 loss: 0.00118405302
Iter: 1312 loss: 0.00118380296
Iter: 1313 loss: 0.00118281052
Iter: 1314 loss: 0.00118117535
Iter: 1315 loss: 0.00119201175
Iter: 1316 loss: 0.00118099689
Iter: 1317 loss: 0.00117989525
Iter: 1318 loss: 0.00119708048
Iter: 1319 loss: 0.00117985706
Iter: 1320 loss: 0.0011783957
Iter: 1321 loss: 0.00118337816
Iter: 1322 loss: 0.00117798266
Iter: 1323 loss: 0.00117705343
Iter: 1324 loss: 0.00117771165
Iter: 1325 loss: 0.00117661245
Iter: 1326 loss: 0.00117562618
Iter: 1327 loss: 0.00117605412
Iter: 1328 loss: 0.00117486529
Iter: 1329 loss: 0.00117422361
Iter: 1330 loss: 0.00117620477
Iter: 1331 loss: 0.00117403746
Iter: 1332 loss: 0.00117268274
Iter: 1333 loss: 0.00117579068
Iter: 1334 loss: 0.00117218611
Iter: 1335 loss: 0.00117140892
Iter: 1336 loss: 0.00117140636
Iter: 1337 loss: 0.00117067713
Iter: 1338 loss: 0.00117280614
Iter: 1339 loss: 0.00117037422
Iter: 1340 loss: 0.00116842252
Iter: 1341 loss: 0.00117460056
Iter: 1342 loss: 0.00116790261
Iter: 1343 loss: 0.00116700376
Iter: 1344 loss: 0.00117020775
Iter: 1345 loss: 0.0011667764
Iter: 1346 loss: 0.00116621517
Iter: 1347 loss: 0.00116582937
Iter: 1348 loss: 0.00116561656
Iter: 1349 loss: 0.00116230128
Iter: 1350 loss: 0.00116967456
Iter: 1351 loss: 0.00116100116
Iter: 1352 loss: 0.00115656538
Iter: 1353 loss: 0.00117542478
Iter: 1354 loss: 0.00115567329
Iter: 1355 loss: 0.00115388108
Iter: 1356 loss: 0.00115567469
Iter: 1357 loss: 0.00115286338
Iter: 1358 loss: 0.00115133426
Iter: 1359 loss: 0.00115679635
Iter: 1360 loss: 0.00115095195
Iter: 1361 loss: 0.00115031097
Iter: 1362 loss: 0.00115112425
Iter: 1363 loss: 0.00114994915
Iter: 1364 loss: 0.00114917627
Iter: 1365 loss: 0.00116774708
Iter: 1366 loss: 0.0011491325
Iter: 1367 loss: 0.00114925439
Iter: 1368 loss: 0.00114872213
Iter: 1369 loss: 0.00114814762
Iter: 1370 loss: 0.00115056778
Iter: 1371 loss: 0.00114802353
Iter: 1372 loss: 0.00114713924
Iter: 1373 loss: 0.00116932765
Iter: 1374 loss: 0.00114714121
Iter: 1375 loss: 0.00115079898
Iter: 1376 loss: 0.00114659918
Iter: 1377 loss: 0.00114592479
Iter: 1378 loss: 0.00115888147
Iter: 1379 loss: 0.00114588614
Iter: 1380 loss: 0.00114616519
Iter: 1381 loss: 0.00114524411
Iter: 1382 loss: 0.00114429556
Iter: 1383 loss: 0.00115410541
Iter: 1384 loss: 0.00114411255
Iter: 1385 loss: 0.00114241801
Iter: 1386 loss: 0.00116644148
Iter: 1387 loss: 0.00114233536
Iter: 1388 loss: 0.00114147016
Iter: 1389 loss: 0.00114103302
Iter: 1390 loss: 0.00114011008
Iter: 1391 loss: 0.00114088878
Iter: 1392 loss: 0.00113950158
Iter: 1393 loss: 0.0011389436
Iter: 1394 loss: 0.00113893859
Iter: 1395 loss: 0.00113881216
Iter: 1396 loss: 0.00113908062
Iter: 1397 loss: 0.0011387571
Iter: 1398 loss: 0.00113860099
Iter: 1399 loss: 0.00113817723
Iter: 1400 loss: 0.00114208693
Iter: 1401 loss: 0.00113807863
Iter: 1402 loss: 0.00113677967
Iter: 1403 loss: 0.0011425371
Iter: 1404 loss: 0.00113652623
Iter: 1405 loss: 0.00113471772
Iter: 1406 loss: 0.00113467709
Iter: 1407 loss: 0.00113497139
Iter: 1408 loss: 0.00113421492
Iter: 1409 loss: 0.00113417
Iter: 1410 loss: 0.00113391387
Iter: 1411 loss: 0.00113366614
Iter: 1412 loss: 0.0011338715
Iter: 1413 loss: 0.00113350945
Iter: 1414 loss: 0.00113325915
Iter: 1415 loss: 0.0011333396
Iter: 1416 loss: 0.00113308337
Iter: 1417 loss: 0.00113283494
Iter: 1418 loss: 0.00113205449
Iter: 1419 loss: 0.00113563181
Iter: 1420 loss: 0.00113168219
Iter: 1421 loss: 0.00113216811
Iter: 1422 loss: 0.00113115087
Iter: 1423 loss: 0.00113058556
Iter: 1424 loss: 0.00113059
Iter: 1425 loss: 0.00113107287
Iter: 1426 loss: 0.00113004365
Iter: 1427 loss: 0.00112986076
Iter: 1428 loss: 0.00113049638
Iter: 1429 loss: 0.00112981116
Iter: 1430 loss: 0.00112963817
Iter: 1431 loss: 0.00112926879
Iter: 1432 loss: 0.00113324984
Iter: 1433 loss: 0.00112925423
Iter: 1434 loss: 0.0011283803
Iter: 1435 loss: 0.00112654036
Iter: 1436 loss: 0.00112654536
Iter: 1437 loss: 0.00112602406
Iter: 1438 loss: 0.0011260031
Iter: 1439 loss: 0.00112585281
Iter: 1440 loss: 0.00112584443
Iter: 1441 loss: 0.00112574187
Iter: 1442 loss: 0.00112545164
Iter: 1443 loss: 0.00112715736
Iter: 1444 loss: 0.00112531916
Iter: 1445 loss: 0.00112476258
Iter: 1446 loss: 0.00112476037
Iter: 1447 loss: 0.00112463965
Iter: 1448 loss: 0.00112410728
Iter: 1449 loss: 0.00112380949
Iter: 1450 loss: 0.00112956157
Iter: 1451 loss: 0.00112381065
Iter: 1452 loss: 0.00112328888
Iter: 1453 loss: 0.00112790149
Iter: 1454 loss: 0.0011232628
Iter: 1455 loss: 0.00112271472
Iter: 1456 loss: 0.001122216
Iter: 1457 loss: 0.00112207502
Iter: 1458 loss: 0.00112154242
Iter: 1459 loss: 0.0011221515
Iter: 1460 loss: 0.0011212409
Iter: 1461 loss: 0.00112103461
Iter: 1462 loss: 0.0011215352
Iter: 1463 loss: 0.00112095918
Iter: 1464 loss: 0.00112079526
Iter: 1465 loss: 0.00112056953
Iter: 1466 loss: 0.0011205622
Iter: 1467 loss: 0.00112037605
Iter: 1468 loss: 0.00112096732
Iter: 1469 loss: 0.00112033496
Iter: 1470 loss: 0.00111963623
Iter: 1471 loss: 0.00111957686
Iter: 1472 loss: 0.00111878267
Iter: 1473 loss: 0.0011171425
Iter: 1474 loss: 0.0012976781
Iter: 1475 loss: 0.00111715507
Iter: 1476 loss: 0.00111685973
Iter: 1477 loss: 0.00111638242
Iter: 1478 loss: 0.00111586647
Iter: 1479 loss: 0.00112097617
Iter: 1480 loss: 0.0011158085
Iter: 1481 loss: 0.0011155461
Iter: 1482 loss: 0.00111726415
Iter: 1483 loss: 0.00111551536
Iter: 1484 loss: 0.00111695216
Iter: 1485 loss: 0.00111526554
Iter: 1486 loss: 0.00111489696
Iter: 1487 loss: 0.0011193864
Iter: 1488 loss: 0.00111490232
Iter: 1489 loss: 0.00111452735
Iter: 1490 loss: 0.0011147341
Iter: 1491 loss: 0.00111427414
Iter: 1492 loss: 0.00111398147
Iter: 1493 loss: 0.00111392024
Iter: 1494 loss: 0.00111371814
Iter: 1495 loss: 0.00111345539
Iter: 1496 loss: 0.0011133654
Iter: 1497 loss: 0.00111320475
Iter: 1498 loss: 0.00111281208
Iter: 1499 loss: 0.00111254316
Iter: 1500 loss: 0.001112425
Iter: 1501 loss: 0.00111211697
Iter: 1502 loss: 0.00111368881
Iter: 1503 loss: 0.00111205294
Iter: 1504 loss: 0.00111191405
Iter: 1505 loss: 0.00111264247
Iter: 1506 loss: 0.00111188879
Iter: 1507 loss: 0.00111173792
Iter: 1508 loss: 0.00111203548
Iter: 1509 loss: 0.00111166434
Iter: 1510 loss: 0.00111151207
Iter: 1511 loss: 0.00111154106
Iter: 1512 loss: 0.0011113981
Iter: 1513 loss: 0.00111130951
Iter: 1514 loss: 0.00111124781
Iter: 1515 loss: 0.0011112194
Iter: 1516 loss: 0.00111115421
Iter: 1517 loss: 0.00111102965
Iter: 1518 loss: 0.00111370045
Iter: 1519 loss: 0.00111103128
Iter: 1520 loss: 0.00112245954
Iter: 1521 loss: 0.0011108208
Iter: 1522 loss: 0.00111577578
Iter: 1523 loss: 0.00111060531
Iter: 1524 loss: 0.00111615658
Iter: 1525 loss: 0.00110925455
Iter: 1526 loss: 0.00110836688
Iter: 1527 loss: 0.00111248
Iter: 1528 loss: 0.00110817479
Iter: 1529 loss: 0.00110764499
Iter: 1530 loss: 0.00110707642
Iter: 1531 loss: 0.00110697
Iter: 1532 loss: 0.00110633858
Iter: 1533 loss: 0.0011062806
Iter: 1534 loss: 0.0011059324
Iter: 1535 loss: 0.00110954535
Iter: 1536 loss: 0.00110591645
Iter: 1537 loss: 0.00110574707
Iter: 1538 loss: 0.00110629364
Iter: 1539 loss: 0.00110569014
Iter: 1540 loss: 0.0011056246
Iter: 1541 loss: 0.0011058175
Iter: 1542 loss: 0.00110560213
Iter: 1543 loss: 0.0011055111
Iter: 1544 loss: 0.00110542914
Iter: 1545 loss: 0.0011054019
Iter: 1546 loss: 0.00110516162
Iter: 1547 loss: 0.00110497954
Iter: 1548 loss: 0.00110490504
Iter: 1549 loss: 0.00110481877
Iter: 1550 loss: 0.00110487267
Iter: 1551 loss: 0.00110476138
Iter: 1552 loss: 0.00110443006
Iter: 1553 loss: 0.00110394845
Iter: 1554 loss: 0.00110393879
Iter: 1555 loss: 0.00110552763
Iter: 1556 loss: 0.00110330223
Iter: 1557 loss: 0.00110291503
Iter: 1558 loss: 0.00110422284
Iter: 1559 loss: 0.00110278605
Iter: 1560 loss: 0.00110250642
Iter: 1561 loss: 0.0011046388
Iter: 1562 loss: 0.00110248302
Iter: 1563 loss: 0.00110244297
Iter: 1564 loss: 0.0011024035
Iter: 1565 loss: 0.00110226218
Iter: 1566 loss: 0.00110199559
Iter: 1567 loss: 0.00110813254
Iter: 1568 loss: 0.00110199256
Iter: 1569 loss: 0.00110168615
Iter: 1570 loss: 0.00110235659
Iter: 1571 loss: 0.00110154902
Iter: 1572 loss: 0.00110125216
Iter: 1573 loss: 0.00110538956
Iter: 1574 loss: 0.00110124773
Iter: 1575 loss: 0.00110111234
Iter: 1576 loss: 0.00110095041
Iter: 1577 loss: 0.0011009227
Iter: 1578 loss: 0.00110054039
Iter: 1579 loss: 0.00110034179
Iter: 1580 loss: 0.00110017299
Iter: 1581 loss: 0.00110092654
Iter: 1582 loss: 0.00110012665
Iter: 1583 loss: 0.00109993096
Iter: 1584 loss: 0.00110052747
Iter: 1585 loss: 0.00109987066
Iter: 1586 loss: 0.00109979883
Iter: 1587 loss: 0.00110130361
Iter: 1588 loss: 0.00109979848
Iter: 1589 loss: 0.00109976775
Iter: 1590 loss: 0.00109980965
Iter: 1591 loss: 0.00109975156
Iter: 1592 loss: 0.00110120594
Iter: 1593 loss: 0.00109952583
Iter: 1594 loss: 0.00109898695
Iter: 1595 loss: 0.00109997159
Iter: 1596 loss: 0.00109875668
Iter: 1597 loss: 0.00109851954
Iter: 1598 loss: 0.00109964225
Iter: 1599 loss: 0.00109849009
Iter: 1600 loss: 0.00109828811
Iter: 1601 loss: 0.00110080105
Iter: 1602 loss: 0.00109829195
Iter: 1603 loss: 0.00109825633
Iter: 1604 loss: 0.0010984299
Iter: 1605 loss: 0.00109824666
Iter: 1606 loss: 0.00109821314
Iter: 1607 loss: 0.00109820673
Iter: 1608 loss: 0.00109813153
Iter: 1609 loss: 0.00109793805
Iter: 1610 loss: 0.00109900418
Iter: 1611 loss: 0.00109788531
Iter: 1612 loss: 0.00109726191
Iter: 1613 loss: 0.0011046
Iter: 1614 loss: 0.00109724386
Iter: 1615 loss: 0.0010974101
Iter: 1616 loss: 0.00109700742
Iter: 1617 loss: 0.00109673059
Iter: 1618 loss: 0.00109696994
Iter: 1619 loss: 0.00109657086
Iter: 1620 loss: 0.00109624839
Iter: 1621 loss: 0.00110105705
Iter: 1622 loss: 0.00109622721
Iter: 1623 loss: 0.00109587738
Iter: 1624 loss: 0.00109542604
Iter: 1625 loss: 0.00109537668
Iter: 1626 loss: 0.00109493756
Iter: 1627 loss: 0.00109644607
Iter: 1628 loss: 0.00109484373
Iter: 1629 loss: 0.00109458866
Iter: 1630 loss: 0.00110012689
Iter: 1631 loss: 0.00109458668
Iter: 1632 loss: 0.00109450868
Iter: 1633 loss: 0.00109495292
Iter: 1634 loss: 0.00109449972
Iter: 1635 loss: 0.00109448237
Iter: 1636 loss: 0.0010944684
Iter: 1637 loss: 0.001094445
Iter: 1638 loss: 0.00109438342
Iter: 1639 loss: 0.00109450729
Iter: 1640 loss: 0.00109434512
Iter: 1641 loss: 0.00109420577
Iter: 1642 loss: 0.00109399413
Iter: 1643 loss: 0.00109398516
Iter: 1644 loss: 0.00109383056
Iter: 1645 loss: 0.00109458598
Iter: 1646 loss: 0.00109381403
Iter: 1647 loss: 0.00109373778
Iter: 1648 loss: 0.0010938592
Iter: 1649 loss: 0.00109371275
Iter: 1650 loss: 0.00109367841
Iter: 1651 loss: 0.00109385967
Iter: 1652 loss: 0.00109367433
Iter: 1653 loss: 0.00109356723
Iter: 1654 loss: 0.00109356712
Iter: 1655 loss: 0.00109347561
Iter: 1656 loss: 0.00109327526
Iter: 1657 loss: 0.0010930805
Iter: 1658 loss: 0.00109303743
Iter: 1659 loss: 0.00175424223
Iter: 1660 loss: 0.00109303137
Iter: 1661 loss: 0.00109290658
Iter: 1662 loss: 0.00109329075
Iter: 1663 loss: 0.00109287351
Iter: 1664 loss: 0.00109300669
Iter: 1665 loss: 0.00109282532
Iter: 1666 loss: 0.00109277712
Iter: 1667 loss: 0.00109304034
Iter: 1668 loss: 0.00109276292
Iter: 1669 loss: 0.00109290204
Iter: 1670 loss: 0.00109275
Iter: 1671 loss: 0.00109274301
Iter: 1672 loss: 0.00109277957
Iter: 1673 loss: 0.00109273661
Iter: 1674 loss: 0.00109273358
Iter: 1675 loss: 0.0010927479
Iter: 1676 loss: 0.00109272823
Iter: 1677 loss: 0.00109271775
Iter: 1678 loss: 0.00109272148
Iter: 1679 loss: 0.00109270448
Iter: 1680 loss: 0.001092677
Iter: 1681 loss: 0.00109310739
Iter: 1682 loss: 0.00109267607
Iter: 1683 loss: 0.00109261274
Iter: 1684 loss: 0.00109243614
Iter: 1685 loss: 0.00109269912
Iter: 1686 loss: 0.00109233963
Iter: 1687 loss: 0.00109200249
Iter: 1688 loss: 0.00109193835
Iter: 1689 loss: 0.00109171588
Iter: 1690 loss: 0.00109142903
Iter: 1691 loss: 0.0010938287
Iter: 1692 loss: 0.00109141762
Iter: 1693 loss: 0.00109137106
Iter: 1694 loss: 0.00109165837
Iter: 1695 loss: 0.00109136058
Iter: 1696 loss: 0.00109133776
Iter: 1697 loss: 0.00109159306
Iter: 1698 loss: 0.00109133462
Iter: 1699 loss: 0.00109126279
Iter: 1700 loss: 0.00109134009
Iter: 1701 loss: 0.0010912302
Iter: 1702 loss: 0.00109118549
Iter: 1703 loss: 0.00109127653
Iter: 1704 loss: 0.00109116628
Iter: 1705 loss: 0.0010911047
Iter: 1706 loss: 0.00109116768
Iter: 1707 loss: 0.00109107
Iter: 1708 loss: 0.00109100295
Iter: 1709 loss: 0.00109128165
Iter: 1710 loss: 0.00109098852
Iter: 1711 loss: 0.0010909657
Iter: 1712 loss: 0.00109096174
Iter: 1713 loss: 0.00109094719
Iter: 1714 loss: 0.00109092041
Iter: 1715 loss: 0.00109088537
Iter: 1716 loss: 0.00109088211
Iter: 1717 loss: 0.00109080691
Iter: 1718 loss: 0.00109105546
Iter: 1719 loss: 0.001090782
Iter: 1720 loss: 0.00109073904
Iter: 1721 loss: 0.0010909657
Iter: 1722 loss: 0.00109073252
Iter: 1723 loss: 0.00109069678
Iter: 1724 loss: 0.00109063101
Iter: 1725 loss: 0.00109154289
Iter: 1726 loss: 0.00109062821
Iter: 1727 loss: 0.00109011447
Iter: 1728 loss: 0.00109256012
Iter: 1729 loss: 0.00109001808
Iter: 1730 loss: 0.00108955894
Iter: 1731 loss: 0.00108959037
Iter: 1732 loss: 0.00108920014
Iter: 1733 loss: 0.00108866789
Iter: 1734 loss: 0.00109646586
Iter: 1735 loss: 0.00108864845
Iter: 1736 loss: 0.00108836044
Iter: 1737 loss: 0.00108835741
Iter: 1738 loss: 0.00108825846
Iter: 1739 loss: 0.00109073205
Iter: 1740 loss: 0.00108825532
Iter: 1741 loss: 0.00108824507
Iter: 1742 loss: 0.00108832179
Iter: 1743 loss: 0.00108823867
Iter: 1744 loss: 0.00108807965
Iter: 1745 loss: 0.00108759198
Iter: 1746 loss: 0.00109064823
Iter: 1747 loss: 0.00108741014
Iter: 1748 loss: 0.00108668313
Iter: 1749 loss: 0.0010876843
Iter: 1750 loss: 0.00108634273
Iter: 1751 loss: 0.00108586671
Iter: 1752 loss: 0.00108956813
Iter: 1753 loss: 0.00108586089
Iter: 1754 loss: 0.00108566612
Iter: 1755 loss: 0.00108556217
Iter: 1756 loss: 0.00108549441
Iter: 1757 loss: 0.00108594913
Iter: 1758 loss: 0.00108548487
Iter: 1759 loss: 0.0010999965
Iter: 1760 loss: 0.00108541548
Iter: 1761 loss: 0.00108487532
Iter: 1762 loss: 0.0010842859
Iter: 1763 loss: 0.00108421186
Iter: 1764 loss: 0.00108387147
Iter: 1765 loss: 0.0010854844
Iter: 1766 loss: 0.00108379882
Iter: 1767 loss: 0.00108371081
Iter: 1768 loss: 0.00108439988
Iter: 1769 loss: 0.00108370173
Iter: 1770 loss: 0.00116187579
Iter: 1771 loss: 0.00108370022
Iter: 1772 loss: 0.00108366436
Iter: 1773 loss: 0.00108409522
Iter: 1774 loss: 0.00108366483
Iter: 1775 loss: 0.00108359451
Iter: 1776 loss: 0.00108345901
Iter: 1777 loss: 0.00108591118
Iter: 1778 loss: 0.00108345
Iter: 1779 loss: 0.00108338147
Iter: 1780 loss: 0.001083432
Iter: 1781 loss: 0.00108333724
Iter: 1782 loss: 0.00108327321
Iter: 1783 loss: 0.00108311512
Iter: 1784 loss: 0.00108620082
Iter: 1785 loss: 0.00108311558
Iter: 1786 loss: 0.00108468812
Iter: 1787 loss: 0.00108303665
Iter: 1788 loss: 0.00108284887
Iter: 1789 loss: 0.00108268019
Iter: 1790 loss: 0.00108256796
Iter: 1791 loss: 0.00108192523
Iter: 1792 loss: 0.00108870619
Iter: 1793 loss: 0.0010818257
Iter: 1794 loss: 0.00108171022
Iter: 1795 loss: 0.00108169578
Iter: 1796 loss: 0.00108166877
Iter: 1797 loss: 0.00108166505
Iter: 1798 loss: 0.00108165352
Iter: 1799 loss: 0.00108161662
Iter: 1800 loss: 0.00108189741
Iter: 1801 loss: 0.00108160253
Iter: 1802 loss: 0.00109385245
Iter: 1803 loss: 0.00108154921
Iter: 1804 loss: 0.00108103035
Iter: 1805 loss: 0.00108496496
Iter: 1806 loss: 0.00108096818
Iter: 1807 loss: 0.00108061254
Iter: 1808 loss: 0.00108468393
Iter: 1809 loss: 0.00108060474
Iter: 1810 loss: 0.00108012115
Iter: 1811 loss: 0.0010801187
Iter: 1812 loss: 0.0010799797
Iter: 1813 loss: 0.00107996562
Iter: 1814 loss: 0.00107990066
Iter: 1815 loss: 0.00107988308
Iter: 1816 loss: 0.00107987691
Iter: 1817 loss: 0.00107986666
Iter: 1818 loss: 0.00107986224
Iter: 1819 loss: 0.00107983896
Iter: 1820 loss: 0.00107975956
Iter: 1821 loss: 0.0010805421
Iter: 1822 loss: 0.00107974559
Iter: 1823 loss: 0.00107971148
Iter: 1824 loss: 0.00107970322
Iter: 1825 loss: 0.00107966783
Iter: 1826 loss: 0.00107964769
Iter: 1827 loss: 0.00107964012
Iter: 1828 loss: 0.00107957784
Iter: 1829 loss: 0.00107948249
Iter: 1830 loss: 0.00108122511
Iter: 1831 loss: 0.00107948144
Iter: 1832 loss: 0.00107940426
Iter: 1833 loss: 0.00107930205
Iter: 1834 loss: 0.00107928738
Iter: 1835 loss: 0.00113904197
Iter: 1836 loss: 0.00107926177
Iter: 1837 loss: 0.00107912929
Iter: 1838 loss: 0.00107904198
Iter: 1839 loss: 0.00107952
Iter: 1840 loss: 0.00107903127
Iter: 1841 loss: 0.00107897935
Iter: 1842 loss: 0.00107910729
Iter: 1843 loss: 0.00107897189
Iter: 1844 loss: 0.00107887015
Iter: 1845 loss: 0.00107858435
Iter: 1846 loss: 0.00108030264
Iter: 1847 loss: 0.0010784748
Iter: 1848 loss: 0.0010792691
Iter: 1849 loss: 0.00107833685
Iter: 1850 loss: 0.00107802381
Iter: 1851 loss: 0.00107803277
Iter: 1852 loss: 0.00107728783
Iter: 1853 loss: 0.00108093815
Iter: 1854 loss: 0.00107708131
Iter: 1855 loss: 0.00107634487
Iter: 1856 loss: 0.00108106865
Iter: 1857 loss: 0.00107624312
Iter: 1858 loss: 0.00107572577
Iter: 1859 loss: 0.001078289
Iter: 1860 loss: 0.0010755955
Iter: 1861 loss: 0.00107551133
Iter: 1862 loss: 0.00107551378
Iter: 1863 loss: 0.00107549538
Iter: 1864 loss: 0.00107549841
Iter: 1865 loss: 0.00107547862
Iter: 1866 loss: 0.00107544125
Iter: 1867 loss: 0.0010758756
Iter: 1868 loss: 0.00107542612
Iter: 1869 loss: 0.00107533298
Iter: 1870 loss: 0.00107516046
Iter: 1871 loss: 0.00107679027
Iter: 1872 loss: 0.00107503729
Iter: 1873 loss: 0.0010748324
Iter: 1874 loss: 0.00107734604
Iter: 1875 loss: 0.00107480423
Iter: 1876 loss: 0.00107467477
Iter: 1877 loss: 0.00107482797
Iter: 1878 loss: 0.00107459759
Iter: 1879 loss: 0.001074353
Iter: 1880 loss: 0.00107644883
Iter: 1881 loss: 0.00107431854
Iter: 1882 loss: 0.0010742445
Iter: 1883 loss: 0.00107287406
Iter: 1884 loss: 0.0010724403
Iter: 1885 loss: 0.00107236218
Iter: 1886 loss: 0.00107228151
Iter: 1887 loss: 0.00107255054
Iter: 1888 loss: 0.00107225135
Iter: 1889 loss: 0.00107216276
Iter: 1890 loss: 0.00107191992
Iter: 1891 loss: 0.00107294286
Iter: 1892 loss: 0.00107184111
Iter: 1893 loss: 0.00107167102
Iter: 1894 loss: 0.00107197254
Iter: 1895 loss: 0.00107160548
Iter: 1896 loss: 0.00107145193
Iter: 1897 loss: 0.00107185589
Iter: 1898 loss: 0.00107140141
Iter: 1899 loss: 0.00107461773
Iter: 1900 loss: 0.00107134692
Iter: 1901 loss: 0.0196203515
Iter: 1902 loss: 0.00107133586
Iter: 1903 loss: 0.00107128103
Iter: 1904 loss: 0.00107208
Iter: 1905 loss: 0.00107127451
Iter: 1906 loss: 0.00107124366
Iter: 1907 loss: 0.00107123912
Iter: 1908 loss: 0.00107123074
Iter: 1909 loss: 0.0010712418
Iter: 1910 loss: 0.00107121281
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.4/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.8 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi2.8
+ date
Sun Nov  8 18:54:24 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.8/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.8/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 2.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.8/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32eff2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32efd99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32f0279d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32f062400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ef54ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ef54048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ef1aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32eecf598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32eecf048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ee04f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ee042f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32edb6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32edcc8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ee521e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ee5f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ed469d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ed65b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32eea1f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ee2c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32eea1048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ecb7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ed9b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ecb7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ec91c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ec91950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff30d776ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff30d6e2d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff30d7762f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ec69158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ec590d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff32ec43400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff30d721158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff30d721510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff30d73bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2e87001e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2e869fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 202, in <module>
    grads = tape.gradient(loss, model.trainable_weights)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1266, in _backward_function_wrapper
    processed_args, remapped_captures)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input is not invertible.
	 [[node gradients/MatrixDeterminant_grad/MatrixInverse (defined at biholoNN_train.py:200) ]] [Op:__inference___backward_volume_form_4446_8945]

Function call stack:
__backward_volume_form_4446

+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi2.8/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.8/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 0 --phi 2.8 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi2.8/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2faa7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2fa399d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2fb29840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f9f7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f9f7158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f9f7268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f9f7598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f94d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f94d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f8f60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f900268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f8dfea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f8dfc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f91b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f8a7f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f85f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f85f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f843378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f7bff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f79c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f79c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f7332f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f7426a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c2f769f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c0fd71378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c0fd34f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c0fd02620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c0fd34378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c0fc861e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c0fca60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c0fcaf510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c0fc721e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c0fc677b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c0fc02598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c0fbd4268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0c0fb88f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.75097248e-05
Iter: 2 loss: 0.000214171741
Iter: 3 loss: 1.29080408e-05
Iter: 4 loss: 1.00044635e-05
Iter: 5 loss: 1.25158567e-05
Iter: 6 loss: 8.30020872e-06
Iter: 7 loss: 7.25239715e-06
Iter: 8 loss: 7.82705047e-06
Iter: 9 loss: 6.56483644e-06
Iter: 10 loss: 5.46903266e-06
Iter: 11 loss: 7.92843639e-06
Iter: 12 loss: 5.05403705e-06
Iter: 13 loss: 4.54434394e-06
Iter: 14 loss: 6.36088407e-06
Iter: 15 loss: 4.41402972e-06
Iter: 16 loss: 4.06337e-06
Iter: 17 loss: 4.95850463e-06
Iter: 18 loss: 3.94249946e-06
Iter: 19 loss: 3.69862528e-06
Iter: 20 loss: 5.71654618e-06
Iter: 21 loss: 3.68371548e-06
Iter: 22 loss: 3.52608345e-06
Iter: 23 loss: 3.657658e-06
Iter: 24 loss: 3.43235638e-06
Iter: 25 loss: 3.24896428e-06
Iter: 26 loss: 3.47426931e-06
Iter: 27 loss: 3.15327111e-06
Iter: 28 loss: 3.01208115e-06
Iter: 29 loss: 4.33104833e-06
Iter: 30 loss: 3.0063552e-06
Iter: 31 loss: 2.9212797e-06
Iter: 32 loss: 3.2034925e-06
Iter: 33 loss: 2.89791024e-06
Iter: 34 loss: 2.85549208e-06
Iter: 35 loss: 2.8553568e-06
Iter: 36 loss: 2.8303316e-06
Iter: 37 loss: 2.82927726e-06
Iter: 38 loss: 2.81634175e-06
Iter: 39 loss: 2.77697882e-06
Iter: 40 loss: 2.88173101e-06
Iter: 41 loss: 2.75540333e-06
Iter: 42 loss: 2.70672626e-06
Iter: 43 loss: 3.18546222e-06
Iter: 44 loss: 2.70510895e-06
Iter: 45 loss: 2.67624227e-06
Iter: 46 loss: 2.78133e-06
Iter: 47 loss: 2.66877055e-06
Iter: 48 loss: 2.64051914e-06
Iter: 49 loss: 2.59801936e-06
Iter: 50 loss: 2.59734e-06
Iter: 51 loss: 2.55379155e-06
Iter: 52 loss: 2.92792652e-06
Iter: 53 loss: 2.5513641e-06
Iter: 54 loss: 2.51297502e-06
Iter: 55 loss: 2.58346608e-06
Iter: 56 loss: 2.49689e-06
Iter: 57 loss: 2.4658334e-06
Iter: 58 loss: 2.63191441e-06
Iter: 59 loss: 2.46128e-06
Iter: 60 loss: 2.4292267e-06
Iter: 61 loss: 2.42474061e-06
Iter: 62 loss: 2.40205e-06
Iter: 63 loss: 2.36939923e-06
Iter: 64 loss: 2.6383716e-06
Iter: 65 loss: 2.36747633e-06
Iter: 66 loss: 2.34330901e-06
Iter: 67 loss: 2.35948028e-06
Iter: 68 loss: 2.32812135e-06
Iter: 69 loss: 2.33011201e-06
Iter: 70 loss: 2.31901322e-06
Iter: 71 loss: 2.30847513e-06
Iter: 72 loss: 2.32653838e-06
Iter: 73 loss: 2.30387036e-06
Iter: 74 loss: 2.2978e-06
Iter: 75 loss: 2.28066574e-06
Iter: 76 loss: 2.34984236e-06
Iter: 77 loss: 2.27363216e-06
Iter: 78 loss: 2.25977101e-06
Iter: 79 loss: 2.25966346e-06
Iter: 80 loss: 2.24668406e-06
Iter: 81 loss: 2.24515225e-06
Iter: 82 loss: 2.23582924e-06
Iter: 83 loss: 2.21882556e-06
Iter: 84 loss: 2.27898454e-06
Iter: 85 loss: 2.21451887e-06
Iter: 86 loss: 2.20252582e-06
Iter: 87 loss: 2.22100562e-06
Iter: 88 loss: 2.19682693e-06
Iter: 89 loss: 2.18263449e-06
Iter: 90 loss: 2.20609354e-06
Iter: 91 loss: 2.17623938e-06
Iter: 92 loss: 2.16236913e-06
Iter: 93 loss: 2.24696805e-06
Iter: 94 loss: 2.16054832e-06
Iter: 95 loss: 2.14962301e-06
Iter: 96 loss: 2.18423656e-06
Iter: 97 loss: 2.14656779e-06
Iter: 98 loss: 2.13524709e-06
Iter: 99 loss: 2.13952967e-06
Iter: 100 loss: 2.12738905e-06
Iter: 101 loss: 2.11650945e-06
Iter: 102 loss: 2.19050708e-06
Iter: 103 loss: 2.11555061e-06
Iter: 104 loss: 2.1128044e-06
Iter: 105 loss: 2.10988719e-06
Iter: 106 loss: 2.10607232e-06
Iter: 107 loss: 2.09490281e-06
Iter: 108 loss: 2.1332994e-06
Iter: 109 loss: 2.09017e-06
Iter: 110 loss: 2.08136294e-06
Iter: 111 loss: 2.09773543e-06
Iter: 112 loss: 2.07761741e-06
Iter: 113 loss: 2.06754612e-06
Iter: 114 loss: 2.13711564e-06
Iter: 115 loss: 2.06648792e-06
Iter: 116 loss: 2.05920605e-06
Iter: 117 loss: 2.10268786e-06
Iter: 118 loss: 2.05865445e-06
Iter: 119 loss: 2.05405217e-06
Iter: 120 loss: 2.04516232e-06
Iter: 121 loss: 2.21606138e-06
Iter: 122 loss: 2.04503954e-06
Iter: 123 loss: 2.03322907e-06
Iter: 124 loss: 2.11124598e-06
Iter: 125 loss: 2.03218769e-06
Iter: 126 loss: 2.02422916e-06
Iter: 127 loss: 2.0307898e-06
Iter: 128 loss: 2.01963621e-06
Iter: 129 loss: 2.00935165e-06
Iter: 130 loss: 2.06049e-06
Iter: 131 loss: 2.00760087e-06
Iter: 132 loss: 1.99955662e-06
Iter: 133 loss: 2.02778e-06
Iter: 134 loss: 1.99757369e-06
Iter: 135 loss: 1.98872067e-06
Iter: 136 loss: 1.9872241e-06
Iter: 137 loss: 1.98111366e-06
Iter: 138 loss: 1.98685098e-06
Iter: 139 loss: 1.97767099e-06
Iter: 140 loss: 1.97352529e-06
Iter: 141 loss: 1.96938845e-06
Iter: 142 loss: 1.96855603e-06
Iter: 143 loss: 1.9649226e-06
Iter: 144 loss: 1.95746725e-06
Iter: 145 loss: 2.09483028e-06
Iter: 146 loss: 1.95737e-06
Iter: 147 loss: 1.9492295e-06
Iter: 148 loss: 1.98801399e-06
Iter: 149 loss: 1.94759741e-06
Iter: 150 loss: 1.94208178e-06
Iter: 151 loss: 2.00118939e-06
Iter: 152 loss: 1.94178142e-06
Iter: 153 loss: 1.93773781e-06
Iter: 154 loss: 1.94543304e-06
Iter: 155 loss: 1.93587857e-06
Iter: 156 loss: 1.93195388e-06
Iter: 157 loss: 1.93459e-06
Iter: 158 loss: 1.92945549e-06
Iter: 159 loss: 1.9248323e-06
Iter: 160 loss: 1.92703669e-06
Iter: 161 loss: 1.92170864e-06
Iter: 162 loss: 1.91550362e-06
Iter: 163 loss: 1.94711834e-06
Iter: 164 loss: 1.91440085e-06
Iter: 165 loss: 1.91007371e-06
Iter: 166 loss: 1.93183746e-06
Iter: 167 loss: 1.9094939e-06
Iter: 168 loss: 1.90527089e-06
Iter: 169 loss: 1.90876335e-06
Iter: 170 loss: 1.90263893e-06
Iter: 171 loss: 1.89913112e-06
Iter: 172 loss: 1.93901906e-06
Iter: 173 loss: 1.89893717e-06
Iter: 174 loss: 1.89656987e-06
Iter: 175 loss: 1.89646323e-06
Iter: 176 loss: 1.89505727e-06
Iter: 177 loss: 1.89133539e-06
Iter: 178 loss: 1.90827905e-06
Iter: 179 loss: 1.88986928e-06
Iter: 180 loss: 1.8865984e-06
Iter: 181 loss: 1.88808008e-06
Iter: 182 loss: 1.8841647e-06
Iter: 183 loss: 1.8791327e-06
Iter: 184 loss: 1.90394962e-06
Iter: 185 loss: 1.87817102e-06
Iter: 186 loss: 1.87499154e-06
Iter: 187 loss: 1.9085187e-06
Iter: 188 loss: 1.87497812e-06
Iter: 189 loss: 1.8715898e-06
Iter: 190 loss: 1.8670828e-06
Iter: 191 loss: 1.8668552e-06
Iter: 192 loss: 1.86293391e-06
Iter: 193 loss: 1.89054572e-06
Iter: 194 loss: 1.86245177e-06
Iter: 195 loss: 1.85920715e-06
Iter: 196 loss: 1.8642362e-06
Iter: 197 loss: 1.85754357e-06
Iter: 198 loss: 1.85408442e-06
Iter: 199 loss: 1.86757404e-06
Iter: 200 loss: 1.85311399e-06
Iter: 201 loss: 1.84945679e-06
Iter: 202 loss: 1.85397778e-06
Iter: 203 loss: 1.84767646e-06
Iter: 204 loss: 1.84411022e-06
Iter: 205 loss: 1.87533419e-06
Iter: 206 loss: 1.84370492e-06
Iter: 207 loss: 1.84406667e-06
Iter: 208 loss: 1.84280645e-06
Iter: 209 loss: 1.84189139e-06
Iter: 210 loss: 1.83940574e-06
Iter: 211 loss: 1.86346074e-06
Iter: 212 loss: 1.83909458e-06
Iter: 213 loss: 1.83697443e-06
Iter: 214 loss: 1.83426869e-06
Iter: 215 loss: 1.83410498e-06
Iter: 216 loss: 1.83025668e-06
Iter: 217 loss: 1.85805709e-06
Iter: 218 loss: 1.82983308e-06
Iter: 219 loss: 1.82773624e-06
Iter: 220 loss: 1.83514e-06
Iter: 221 loss: 1.82701228e-06
Iter: 222 loss: 1.82409804e-06
Iter: 223 loss: 1.83420934e-06
Iter: 224 loss: 1.82338238e-06
Iter: 225 loss: 1.82116753e-06
Iter: 226 loss: 1.82318399e-06
Iter: 227 loss: 1.81980045e-06
Iter: 228 loss: 1.81717814e-06
Iter: 229 loss: 1.81707446e-06
Iter: 230 loss: 1.81495898e-06
Iter: 231 loss: 1.81168571e-06
Iter: 232 loss: 1.83709062e-06
Iter: 233 loss: 1.81133305e-06
Iter: 234 loss: 1.80892971e-06
Iter: 235 loss: 1.8085633e-06
Iter: 236 loss: 1.80680559e-06
Iter: 237 loss: 1.80364782e-06
Iter: 238 loss: 1.83974976e-06
Iter: 239 loss: 1.80369204e-06
Iter: 240 loss: 1.80286463e-06
Iter: 241 loss: 1.80274253e-06
Iter: 242 loss: 1.80151233e-06
Iter: 243 loss: 1.79969948e-06
Iter: 244 loss: 1.79969231e-06
Iter: 245 loss: 1.79773417e-06
Iter: 246 loss: 1.79661663e-06
Iter: 247 loss: 1.79593371e-06
Iter: 248 loss: 1.79364542e-06
Iter: 249 loss: 1.79570839e-06
Iter: 250 loss: 1.7925131e-06
Iter: 251 loss: 1.78939308e-06
Iter: 252 loss: 1.79591621e-06
Iter: 253 loss: 1.7882461e-06
Iter: 254 loss: 1.78606206e-06
Iter: 255 loss: 1.78607536e-06
Iter: 256 loss: 1.78452444e-06
Iter: 257 loss: 1.78477489e-06
Iter: 258 loss: 1.78316043e-06
Iter: 259 loss: 1.78102619e-06
Iter: 260 loss: 1.78298114e-06
Iter: 261 loss: 1.77965353e-06
Iter: 262 loss: 1.77688878e-06
Iter: 263 loss: 1.78059349e-06
Iter: 264 loss: 1.77540585e-06
Iter: 265 loss: 1.7726768e-06
Iter: 266 loss: 1.78143409e-06
Iter: 267 loss: 1.77189895e-06
Iter: 268 loss: 1.76913784e-06
Iter: 269 loss: 1.78128744e-06
Iter: 270 loss: 1.76844765e-06
Iter: 271 loss: 1.76650224e-06
Iter: 272 loss: 1.78029757e-06
Iter: 273 loss: 1.766266e-06
Iter: 274 loss: 1.76470428e-06
Iter: 275 loss: 1.76445451e-06
Iter: 276 loss: 1.76420292e-06
Iter: 277 loss: 1.7630241e-06
Iter: 278 loss: 1.77685024e-06
Iter: 279 loss: 1.76281947e-06
Iter: 280 loss: 1.76181629e-06
Iter: 281 loss: 1.76101253e-06
Iter: 282 loss: 1.7606535e-06
Iter: 283 loss: 1.75857747e-06
Iter: 284 loss: 1.76647609e-06
Iter: 285 loss: 1.75806531e-06
Iter: 286 loss: 1.75631271e-06
Iter: 287 loss: 1.75877369e-06
Iter: 288 loss: 1.75564287e-06
Iter: 289 loss: 1.75375601e-06
Iter: 290 loss: 1.77608047e-06
Iter: 291 loss: 1.75379023e-06
Iter: 292 loss: 1.75267223e-06
Iter: 293 loss: 1.75278058e-06
Iter: 294 loss: 1.75169771e-06
Iter: 295 loss: 1.74991578e-06
Iter: 296 loss: 1.74920524e-06
Iter: 297 loss: 1.74842557e-06
Iter: 298 loss: 1.74600041e-06
Iter: 299 loss: 1.76035564e-06
Iter: 300 loss: 1.74557294e-06
Iter: 301 loss: 1.74326863e-06
Iter: 302 loss: 1.74278193e-06
Iter: 303 loss: 1.74127e-06
Iter: 304 loss: 1.73884769e-06
Iter: 305 loss: 1.77296477e-06
Iter: 306 loss: 1.73888384e-06
Iter: 307 loss: 1.73932403e-06
Iter: 308 loss: 1.73829278e-06
Iter: 309 loss: 1.73760145e-06
Iter: 310 loss: 1.73579065e-06
Iter: 311 loss: 1.73836e-06
Iter: 312 loss: 1.73450746e-06
Iter: 313 loss: 1.73256785e-06
Iter: 314 loss: 1.74147965e-06
Iter: 315 loss: 1.73215744e-06
Iter: 316 loss: 1.73054536e-06
Iter: 317 loss: 1.73622288e-06
Iter: 318 loss: 1.73014973e-06
Iter: 319 loss: 1.72852253e-06
Iter: 320 loss: 1.72876094e-06
Iter: 321 loss: 1.72741966e-06
Iter: 322 loss: 1.7256292e-06
Iter: 323 loss: 1.7422617e-06
Iter: 324 loss: 1.72577688e-06
Iter: 325 loss: 1.72436796e-06
Iter: 326 loss: 1.73174351e-06
Iter: 327 loss: 1.7242445e-06
Iter: 328 loss: 1.72290515e-06
Iter: 329 loss: 1.72166199e-06
Iter: 330 loss: 1.7215001e-06
Iter: 331 loss: 1.7198015e-06
Iter: 332 loss: 1.73280591e-06
Iter: 333 loss: 1.71965723e-06
Iter: 334 loss: 1.71838599e-06
Iter: 335 loss: 1.71657985e-06
Iter: 336 loss: 1.71646457e-06
Iter: 337 loss: 1.71420231e-06
Iter: 338 loss: 1.73859382e-06
Iter: 339 loss: 1.71401302e-06
Iter: 340 loss: 1.71267641e-06
Iter: 341 loss: 1.72265493e-06
Iter: 342 loss: 1.71263196e-06
Iter: 343 loss: 1.71084412e-06
Iter: 344 loss: 1.72267073e-06
Iter: 345 loss: 1.7108373e-06
Iter: 346 loss: 1.7103855e-06
Iter: 347 loss: 1.7090664e-06
Iter: 348 loss: 1.71537658e-06
Iter: 349 loss: 1.70882e-06
Iter: 350 loss: 1.70665055e-06
Iter: 351 loss: 1.70671501e-06
Iter: 352 loss: 1.70502051e-06
Iter: 353 loss: 1.70346095e-06
Iter: 354 loss: 1.70323108e-06
Iter: 355 loss: 1.70212695e-06
Iter: 356 loss: 1.7008158e-06
Iter: 357 loss: 1.70067688e-06
Iter: 358 loss: 1.69840496e-06
Iter: 359 loss: 1.71630552e-06
Iter: 360 loss: 1.69859743e-06
Iter: 361 loss: 1.69719226e-06
Iter: 362 loss: 1.70589453e-06
Iter: 363 loss: 1.69690657e-06
Iter: 364 loss: 1.69582677e-06
Iter: 365 loss: 1.69557961e-06
Iter: 366 loss: 1.69465159e-06
Iter: 367 loss: 1.69325097e-06
Iter: 368 loss: 1.69438147e-06
Iter: 369 loss: 1.69246221e-06
Iter: 370 loss: 1.69080272e-06
Iter: 371 loss: 1.69505927e-06
Iter: 372 loss: 1.68998395e-06
Iter: 373 loss: 1.68842553e-06
Iter: 374 loss: 1.69844748e-06
Iter: 375 loss: 1.68816734e-06
Iter: 376 loss: 1.68878182e-06
Iter: 377 loss: 1.68778035e-06
Iter: 378 loss: 1.68736847e-06
Iter: 379 loss: 1.68619442e-06
Iter: 380 loss: 1.69243697e-06
Iter: 381 loss: 1.68582778e-06
Iter: 382 loss: 1.68495103e-06
Iter: 383 loss: 1.68548024e-06
Iter: 384 loss: 1.68439067e-06
Iter: 385 loss: 1.6832455e-06
Iter: 386 loss: 1.68611746e-06
Iter: 387 loss: 1.68298493e-06
Iter: 388 loss: 1.68160977e-06
Iter: 389 loss: 1.68467943e-06
Iter: 390 loss: 1.68122176e-06
Iter: 391 loss: 1.68017618e-06
Iter: 392 loss: 1.68561144e-06
Iter: 393 loss: 1.6797419e-06
Iter: 394 loss: 1.67881058e-06
Iter: 395 loss: 1.68022939e-06
Iter: 396 loss: 1.67809378e-06
Iter: 397 loss: 1.67686119e-06
Iter: 398 loss: 1.68611825e-06
Iter: 399 loss: 1.67665053e-06
Iter: 400 loss: 1.67576286e-06
Iter: 401 loss: 1.67686449e-06
Iter: 402 loss: 1.67503799e-06
Iter: 403 loss: 1.67400617e-06
Iter: 404 loss: 1.67393318e-06
Iter: 405 loss: 1.67287317e-06
Iter: 406 loss: 1.67180087e-06
Iter: 407 loss: 1.67384246e-06
Iter: 408 loss: 1.67112046e-06
Iter: 409 loss: 1.67007488e-06
Iter: 410 loss: 1.68442534e-06
Iter: 411 loss: 1.66987888e-06
Iter: 412 loss: 1.67039707e-06
Iter: 413 loss: 1.66975587e-06
Iter: 414 loss: 1.66960558e-06
Iter: 415 loss: 1.66896041e-06
Iter: 416 loss: 1.66818484e-06
Iter: 417 loss: 1.66787504e-06
Iter: 418 loss: 1.66650921e-06
Iter: 419 loss: 1.67344592e-06
Iter: 420 loss: 1.66625489e-06
Iter: 421 loss: 1.66522398e-06
Iter: 422 loss: 1.6652516e-06
Iter: 423 loss: 1.66438429e-06
Iter: 424 loss: 1.66285713e-06
Iter: 425 loss: 1.67250766e-06
Iter: 426 loss: 1.66273458e-06
Iter: 427 loss: 1.66188499e-06
Iter: 428 loss: 1.66232076e-06
Iter: 429 loss: 1.66119185e-06
Iter: 430 loss: 1.65999836e-06
Iter: 431 loss: 1.66684572e-06
Iter: 432 loss: 1.65995175e-06
Iter: 433 loss: 1.65915742e-06
Iter: 434 loss: 1.66569839e-06
Iter: 435 loss: 1.6591523e-06
Iter: 436 loss: 1.65851054e-06
Iter: 437 loss: 1.65766539e-06
Iter: 438 loss: 1.65745769e-06
Iter: 439 loss: 1.65645133e-06
Iter: 440 loss: 1.66024597e-06
Iter: 441 loss: 1.65616905e-06
Iter: 442 loss: 1.65524966e-06
Iter: 443 loss: 1.65918721e-06
Iter: 444 loss: 1.6550589e-06
Iter: 445 loss: 1.65424308e-06
Iter: 446 loss: 1.65485562e-06
Iter: 447 loss: 1.65382721e-06
Iter: 448 loss: 1.65285189e-06
Iter: 449 loss: 1.65288088e-06
Iter: 450 loss: 1.65253346e-06
Iter: 451 loss: 1.65183314e-06
Iter: 452 loss: 1.652695e-06
Iter: 453 loss: 1.6512746e-06
Iter: 454 loss: 1.65022834e-06
Iter: 455 loss: 1.65692006e-06
Iter: 456 loss: 1.65000904e-06
Iter: 457 loss: 1.64928713e-06
Iter: 458 loss: 1.64826793e-06
Iter: 459 loss: 1.64820381e-06
Iter: 460 loss: 1.64676362e-06
Iter: 461 loss: 1.66168968e-06
Iter: 462 loss: 1.64685548e-06
Iter: 463 loss: 1.64590699e-06
Iter: 464 loss: 1.64725282e-06
Iter: 465 loss: 1.64558435e-06
Iter: 466 loss: 1.64476069e-06
Iter: 467 loss: 1.64732205e-06
Iter: 468 loss: 1.64457356e-06
Iter: 469 loss: 1.6437059e-06
Iter: 470 loss: 1.64534595e-06
Iter: 471 loss: 1.64322216e-06
Iter: 472 loss: 1.64231892e-06
Iter: 473 loss: 1.64805226e-06
Iter: 474 loss: 1.64230642e-06
Iter: 475 loss: 1.64152584e-06
Iter: 476 loss: 1.64068717e-06
Iter: 477 loss: 1.64051733e-06
Iter: 478 loss: 1.63933441e-06
Iter: 479 loss: 1.63927e-06
Iter: 480 loss: 1.63975369e-06
Iter: 481 loss: 1.63900108e-06
Iter: 482 loss: 1.63879918e-06
Iter: 483 loss: 1.63828304e-06
Iter: 484 loss: 1.64075766e-06
Iter: 485 loss: 1.63818254e-06
Iter: 486 loss: 1.63740151e-06
Iter: 487 loss: 1.63773268e-06
Iter: 488 loss: 1.63697837e-06
Iter: 489 loss: 1.63607342e-06
Iter: 490 loss: 1.63907328e-06
Iter: 491 loss: 1.63580239e-06
Iter: 492 loss: 1.63469076e-06
Iter: 493 loss: 1.63508764e-06
Iter: 494 loss: 1.63401103e-06
Iter: 495 loss: 1.63302866e-06
Iter: 496 loss: 1.64430935e-06
Iter: 497 loss: 1.63296806e-06
Iter: 498 loss: 1.63217601e-06
Iter: 499 loss: 1.63125583e-06
Iter: 500 loss: 1.631229e-06
Iter: 501 loss: 1.62984475e-06
Iter: 502 loss: 1.64148173e-06
Iter: 503 loss: 1.62976664e-06
Iter: 504 loss: 1.62883134e-06
Iter: 505 loss: 1.6346039e-06
Iter: 506 loss: 1.62866377e-06
Iter: 507 loss: 1.62762558e-06
Iter: 508 loss: 1.63125355e-06
Iter: 509 loss: 1.62762262e-06
Iter: 510 loss: 1.62686422e-06
Iter: 511 loss: 1.62723302e-06
Iter: 512 loss: 1.62660228e-06
Iter: 513 loss: 1.62610809e-06
Iter: 514 loss: 1.62607284e-06
Iter: 515 loss: 1.62549577e-06
Iter: 516 loss: 1.62544211e-06
Iter: 517 loss: 1.62504261e-06
Iter: 518 loss: 1.62476499e-06
Iter: 519 loss: 1.624118e-06
Iter: 520 loss: 1.6388326e-06
Iter: 521 loss: 1.62385561e-06
Iter: 522 loss: 1.62309129e-06
Iter: 523 loss: 1.62549418e-06
Iter: 524 loss: 1.62269066e-06
Iter: 525 loss: 1.6218861e-06
Iter: 526 loss: 1.62673473e-06
Iter: 527 loss: 1.62195249e-06
Iter: 528 loss: 1.62121535e-06
Iter: 529 loss: 1.62033245e-06
Iter: 530 loss: 1.62023821e-06
Iter: 531 loss: 1.61926823e-06
Iter: 532 loss: 1.63164532e-06
Iter: 533 loss: 1.61910793e-06
Iter: 534 loss: 1.61836715e-06
Iter: 535 loss: 1.61688922e-06
Iter: 536 loss: 1.61684648e-06
Iter: 537 loss: 1.61546609e-06
Iter: 538 loss: 1.63005006e-06
Iter: 539 loss: 1.61546166e-06
Iter: 540 loss: 1.61445018e-06
Iter: 541 loss: 1.6189457e-06
Iter: 542 loss: 1.61409298e-06
Iter: 543 loss: 1.61294099e-06
Iter: 544 loss: 1.61807964e-06
Iter: 545 loss: 1.61269122e-06
Iter: 546 loss: 1.61181424e-06
Iter: 547 loss: 1.61204503e-06
Iter: 548 loss: 1.61132721e-06
Iter: 549 loss: 1.61087371e-06
Iter: 550 loss: 1.61059404e-06
Iter: 551 loss: 1.61034e-06
Iter: 552 loss: 1.60940749e-06
Iter: 553 loss: 1.6176997e-06
Iter: 554 loss: 1.60913828e-06
Iter: 555 loss: 1.60840136e-06
Iter: 556 loss: 1.6095828e-06
Iter: 557 loss: 1.60780007e-06
Iter: 558 loss: 1.6069082e-06
Iter: 559 loss: 1.60658851e-06
Iter: 560 loss: 1.60563172e-06
Iter: 561 loss: 1.60436969e-06
Iter: 562 loss: 1.61067226e-06
Iter: 563 loss: 1.60413742e-06
Iter: 564 loss: 1.60288e-06
Iter: 565 loss: 1.60551235e-06
Iter: 566 loss: 1.60244758e-06
Iter: 567 loss: 1.60134618e-06
Iter: 568 loss: 1.6086949e-06
Iter: 569 loss: 1.60121158e-06
Iter: 570 loss: 1.60043078e-06
Iter: 571 loss: 1.60237801e-06
Iter: 572 loss: 1.60015759e-06
Iter: 573 loss: 1.59933666e-06
Iter: 574 loss: 1.6007823e-06
Iter: 575 loss: 1.59891238e-06
Iter: 576 loss: 1.598106e-06
Iter: 577 loss: 1.60160425e-06
Iter: 578 loss: 1.59789533e-06
Iter: 579 loss: 1.59718547e-06
Iter: 580 loss: 1.59889214e-06
Iter: 581 loss: 1.59676847e-06
Iter: 582 loss: 1.5968651e-06
Iter: 583 loss: 1.59648107e-06
Iter: 584 loss: 1.59613501e-06
Iter: 585 loss: 1.59625779e-06
Iter: 586 loss: 1.59596584e-06
Iter: 587 loss: 1.59550927e-06
Iter: 588 loss: 1.59461081e-06
Iter: 589 loss: 1.60961258e-06
Iter: 590 loss: 1.59473575e-06
Iter: 591 loss: 1.59369802e-06
Iter: 592 loss: 1.59700289e-06
Iter: 593 loss: 1.59359286e-06
Iter: 594 loss: 1.59275896e-06
Iter: 595 loss: 1.59261435e-06
Iter: 596 loss: 1.59201431e-06
Iter: 597 loss: 1.59128217e-06
Iter: 598 loss: 1.60362299e-06
Iter: 599 loss: 1.5912367e-06
Iter: 600 loss: 1.59062677e-06
Iter: 601 loss: 1.58983244e-06
Iter: 602 loss: 1.58977377e-06
Iter: 603 loss: 1.5887897e-06
Iter: 604 loss: 1.59638466e-06
Iter: 605 loss: 1.58881312e-06
Iter: 606 loss: 1.58808643e-06
Iter: 607 loss: 1.58763089e-06
Iter: 608 loss: 1.58714829e-06
Iter: 609 loss: 1.58611363e-06
Iter: 610 loss: 1.59182855e-06
Iter: 611 loss: 1.58587704e-06
Iter: 612 loss: 1.58521311e-06
Iter: 613 loss: 1.58424382e-06
Iter: 614 loss: 1.58422699e-06
Iter: 615 loss: 1.58361695e-06
Iter: 616 loss: 1.58357182e-06
Iter: 617 loss: 1.58319654e-06
Iter: 618 loss: 1.5844214e-06
Iter: 619 loss: 1.58301873e-06
Iter: 620 loss: 1.58266221e-06
Iter: 621 loss: 1.58336923e-06
Iter: 622 loss: 1.58240186e-06
Iter: 623 loss: 1.58197201e-06
Iter: 624 loss: 1.58128182e-06
Iter: 625 loss: 1.58137743e-06
Iter: 626 loss: 1.5805839e-06
Iter: 627 loss: 1.58294017e-06
Iter: 628 loss: 1.58021135e-06
Iter: 629 loss: 1.57956663e-06
Iter: 630 loss: 1.58011403e-06
Iter: 631 loss: 1.57923523e-06
Iter: 632 loss: 1.57827685e-06
Iter: 633 loss: 1.58230841e-06
Iter: 634 loss: 1.57821671e-06
Iter: 635 loss: 1.57756642e-06
Iter: 636 loss: 1.57939189e-06
Iter: 637 loss: 1.57734394e-06
Iter: 638 loss: 1.57669592e-06
Iter: 639 loss: 1.57750719e-06
Iter: 640 loss: 1.57625755e-06
Iter: 641 loss: 1.57535578e-06
Iter: 642 loss: 1.57793522e-06
Iter: 643 loss: 1.57535953e-06
Iter: 644 loss: 1.57471982e-06
Iter: 645 loss: 1.57554132e-06
Iter: 646 loss: 1.57433431e-06
Iter: 647 loss: 1.57361035e-06
Iter: 648 loss: 1.57644035e-06
Iter: 649 loss: 1.57322484e-06
Iter: 650 loss: 1.57388854e-06
Iter: 651 loss: 1.57314344e-06
Iter: 652 loss: 1.57293414e-06
Iter: 653 loss: 1.57248076e-06
Iter: 654 loss: 1.57537477e-06
Iter: 655 loss: 1.57244529e-06
Iter: 656 loss: 1.57177283e-06
Iter: 657 loss: 1.57326338e-06
Iter: 658 loss: 1.57147315e-06
Iter: 659 loss: 1.57087413e-06
Iter: 660 loss: 1.57211093e-06
Iter: 661 loss: 1.57070303e-06
Iter: 662 loss: 1.57021282e-06
Iter: 663 loss: 1.57000909e-06
Iter: 664 loss: 1.56968656e-06
Iter: 665 loss: 1.56896931e-06
Iter: 666 loss: 1.57471413e-06
Iter: 667 loss: 1.56894225e-06
Iter: 668 loss: 1.56849683e-06
Iter: 669 loss: 1.5679276e-06
Iter: 670 loss: 1.56792885e-06
Iter: 671 loss: 1.56718454e-06
Iter: 672 loss: 1.57606178e-06
Iter: 673 loss: 1.56720102e-06
Iter: 674 loss: 1.56657643e-06
Iter: 675 loss: 1.56630813e-06
Iter: 676 loss: 1.56599458e-06
Iter: 677 loss: 1.56530086e-06
Iter: 678 loss: 1.56964802e-06
Iter: 679 loss: 1.56504007e-06
Iter: 680 loss: 1.56471674e-06
Iter: 681 loss: 1.56668079e-06
Iter: 682 loss: 1.564488e-06
Iter: 683 loss: 1.56398812e-06
Iter: 684 loss: 1.56620797e-06
Iter: 685 loss: 1.56396482e-06
Iter: 686 loss: 1.56373153e-06
Iter: 687 loss: 1.56363149e-06
Iter: 688 loss: 1.56351143e-06
Iter: 689 loss: 1.56317992e-06
Iter: 690 loss: 1.5636042e-06
Iter: 691 loss: 1.5627204e-06
Iter: 692 loss: 1.5621049e-06
Iter: 693 loss: 1.5675439e-06
Iter: 694 loss: 1.56208512e-06
Iter: 695 loss: 1.56155545e-06
Iter: 696 loss: 1.56163549e-06
Iter: 697 loss: 1.56105489e-06
Iter: 698 loss: 1.56067449e-06
Iter: 699 loss: 1.56101714e-06
Iter: 700 loss: 1.56017836e-06
Iter: 701 loss: 1.55939642e-06
Iter: 702 loss: 1.56195233e-06
Iter: 703 loss: 1.55906594e-06
Iter: 704 loss: 1.55841417e-06
Iter: 705 loss: 1.561933e-06
Iter: 706 loss: 1.55840746e-06
Iter: 707 loss: 1.55774819e-06
Iter: 708 loss: 1.56011799e-06
Iter: 709 loss: 1.55758232e-06
Iter: 710 loss: 1.55721932e-06
Iter: 711 loss: 1.5581079e-06
Iter: 712 loss: 1.55695886e-06
Iter: 713 loss: 1.55642488e-06
Iter: 714 loss: 1.55738667e-06
Iter: 715 loss: 1.55633268e-06
Iter: 716 loss: 1.55577459e-06
Iter: 717 loss: 1.55868975e-06
Iter: 718 loss: 1.55571115e-06
Iter: 719 loss: 1.55617886e-06
Iter: 720 loss: 1.5555845e-06
Iter: 721 loss: 1.55537418e-06
Iter: 722 loss: 1.55511111e-06
Iter: 723 loss: 1.55578232e-06
Iter: 724 loss: 1.55477449e-06
Iter: 725 loss: 1.55442603e-06
Iter: 726 loss: 1.55614589e-06
Iter: 727 loss: 1.55417956e-06
Iter: 728 loss: 1.55353132e-06
Iter: 729 loss: 1.55513135e-06
Iter: 730 loss: 1.55360203e-06
Iter: 731 loss: 1.55289763e-06
Iter: 732 loss: 1.55245823e-06
Iter: 733 loss: 1.55254349e-06
Iter: 734 loss: 1.55131431e-06
Iter: 735 loss: 1.55782732e-06
Iter: 736 loss: 1.55151361e-06
Iter: 737 loss: 1.55106204e-06
Iter: 738 loss: 1.55138412e-06
Iter: 739 loss: 1.55094096e-06
Iter: 740 loss: 1.55010889e-06
Iter: 741 loss: 1.55378018e-06
Iter: 742 loss: 1.54997656e-06
Iter: 743 loss: 1.54964152e-06
Iter: 744 loss: 1.55175644e-06
Iter: 745 loss: 1.54949225e-06
Iter: 746 loss: 1.54902705e-06
Iter: 747 loss: 1.54870656e-06
Iter: 748 loss: 1.54851773e-06
Iter: 749 loss: 1.5479211e-06
Iter: 750 loss: 1.54952147e-06
Iter: 751 loss: 1.54767963e-06
Iter: 752 loss: 1.54748057e-06
Iter: 753 loss: 1.54722852e-06
Iter: 754 loss: 1.54712359e-06
Iter: 755 loss: 1.54721693e-06
Iter: 756 loss: 1.54677878e-06
Iter: 757 loss: 1.54661166e-06
Iter: 758 loss: 1.54612644e-06
Iter: 759 loss: 1.54613383e-06
Iter: 760 loss: 1.54581596e-06
Iter: 761 loss: 1.54681788e-06
Iter: 762 loss: 1.54557483e-06
Iter: 763 loss: 1.54510508e-06
Iter: 764 loss: 1.5452913e-06
Iter: 765 loss: 1.54459917e-06
Iter: 766 loss: 1.54391182e-06
Iter: 767 loss: 1.54511827e-06
Iter: 768 loss: 1.54355098e-06
Iter: 769 loss: 1.54315831e-06
Iter: 770 loss: 1.54550617e-06
Iter: 771 loss: 1.54296993e-06
Iter: 772 loss: 1.54239638e-06
Iter: 773 loss: 1.54362908e-06
Iter: 774 loss: 1.54221334e-06
Iter: 775 loss: 1.54172403e-06
Iter: 776 loss: 1.54322356e-06
Iter: 777 loss: 1.54172426e-06
Iter: 778 loss: 1.54102486e-06
Iter: 779 loss: 1.54353859e-06
Iter: 780 loss: 1.54073894e-06
Iter: 781 loss: 1.54029351e-06
Iter: 782 loss: 1.54049417e-06
Iter: 783 loss: 1.53995711e-06
Iter: 784 loss: 1.53923065e-06
Iter: 785 loss: 1.54235192e-06
Iter: 786 loss: 1.53918154e-06
Iter: 787 loss: 1.53975463e-06
Iter: 788 loss: 1.53888368e-06
Iter: 789 loss: 1.53880978e-06
Iter: 790 loss: 1.53840585e-06
Iter: 791 loss: 1.5392485e-06
Iter: 792 loss: 1.53815654e-06
Iter: 793 loss: 1.53764267e-06
Iter: 794 loss: 1.54060046e-06
Iter: 795 loss: 1.5375698e-06
Iter: 796 loss: 1.5370598e-06
Iter: 797 loss: 1.53980204e-06
Iter: 798 loss: 1.53711039e-06
Iter: 799 loss: 1.53683072e-06
Iter: 800 loss: 1.53614849e-06
Iter: 801 loss: 1.53621386e-06
Iter: 802 loss: 1.53546466e-06
Iter: 803 loss: 1.53740461e-06
Iter: 804 loss: 1.53536109e-06
Iter: 805 loss: 1.53469341e-06
Iter: 806 loss: 1.5360489e-06
Iter: 807 loss: 1.53445296e-06
Iter: 808 loss: 1.5339e-06
Iter: 809 loss: 1.53607061e-06
Iter: 810 loss: 1.53381552e-06
Iter: 811 loss: 1.53325277e-06
Iter: 812 loss: 1.53529243e-06
Iter: 813 loss: 1.53317546e-06
Iter: 814 loss: 1.53273686e-06
Iter: 815 loss: 1.53405335e-06
Iter: 816 loss: 1.53255337e-06
Iter: 817 loss: 1.53224141e-06
Iter: 818 loss: 1.53302062e-06
Iter: 819 loss: 1.53184487e-06
Iter: 820 loss: 1.53162046e-06
Iter: 821 loss: 1.5315311e-06
Iter: 822 loss: 1.53114081e-06
Iter: 823 loss: 1.53208111e-06
Iter: 824 loss: 1.53099541e-06
Iter: 825 loss: 1.53076735e-06
Iter: 826 loss: 1.53008023e-06
Iter: 827 loss: 1.53336805e-06
Iter: 828 loss: 1.52999394e-06
Iter: 829 loss: 1.52926816e-06
Iter: 830 loss: 1.52930068e-06
Iter: 831 loss: 1.52868392e-06
Iter: 832 loss: 1.52869336e-06
Iter: 833 loss: 1.52835628e-06
Iter: 834 loss: 1.52762266e-06
Iter: 835 loss: 1.5314356e-06
Iter: 836 loss: 1.52758605e-06
Iter: 837 loss: 1.52726716e-06
Iter: 838 loss: 1.52629354e-06
Iter: 839 loss: 1.54187228e-06
Iter: 840 loss: 1.52631264e-06
Iter: 841 loss: 1.52563121e-06
Iter: 842 loss: 1.52562654e-06
Iter: 843 loss: 1.52511791e-06
Iter: 844 loss: 1.5256212e-06
Iter: 845 loss: 1.52493499e-06
Iter: 846 loss: 1.52427924e-06
Iter: 847 loss: 1.52592372e-06
Iter: 848 loss: 1.5240646e-06
Iter: 849 loss: 1.52342955e-06
Iter: 850 loss: 1.52472956e-06
Iter: 851 loss: 1.52307734e-06
Iter: 852 loss: 1.5230014e-06
Iter: 853 loss: 1.52280882e-06
Iter: 854 loss: 1.52260532e-06
Iter: 855 loss: 1.52316761e-06
Iter: 856 loss: 1.52250266e-06
Iter: 857 loss: 1.52226971e-06
Iter: 858 loss: 1.52177381e-06
Iter: 859 loss: 1.52747543e-06
Iter: 860 loss: 1.52169184e-06
Iter: 861 loss: 1.5208866e-06
Iter: 862 loss: 1.52270854e-06
Iter: 863 loss: 1.52075825e-06
Iter: 864 loss: 1.52009852e-06
Iter: 865 loss: 1.52195014e-06
Iter: 866 loss: 1.51999939e-06
Iter: 867 loss: 1.51940117e-06
Iter: 868 loss: 1.52226789e-06
Iter: 869 loss: 1.51939923e-06
Iter: 870 loss: 1.51876691e-06
Iter: 871 loss: 1.51937161e-06
Iter: 872 loss: 1.51853123e-06
Iter: 873 loss: 1.51806353e-06
Iter: 874 loss: 1.51808206e-06
Iter: 875 loss: 1.51768609e-06
Iter: 876 loss: 1.51691756e-06
Iter: 877 loss: 1.51905238e-06
Iter: 878 loss: 1.51665563e-06
Iter: 879 loss: 1.51604468e-06
Iter: 880 loss: 1.5181663e-06
Iter: 881 loss: 1.51571942e-06
Iter: 882 loss: 1.515024e-06
Iter: 883 loss: 1.51545134e-06
Iter: 884 loss: 1.51468271e-06
Iter: 885 loss: 1.5139367e-06
Iter: 886 loss: 1.52053144e-06
Iter: 887 loss: 1.513965e-06
Iter: 888 loss: 1.51333347e-06
Iter: 889 loss: 1.51476979e-06
Iter: 890 loss: 1.51315351e-06
Iter: 891 loss: 1.51339964e-06
Iter: 892 loss: 1.51290487e-06
Iter: 893 loss: 1.51272525e-06
Iter: 894 loss: 1.51225049e-06
Iter: 895 loss: 1.51615632e-06
Iter: 896 loss: 1.51216273e-06
Iter: 897 loss: 1.51188942e-06
Iter: 898 loss: 1.51110703e-06
Iter: 899 loss: 1.5111782e-06
Iter: 900 loss: 1.51056156e-06
Iter: 901 loss: 1.51315817e-06
Iter: 902 loss: 1.51035351e-06
Iter: 903 loss: 1.5096241e-06
Iter: 904 loss: 1.51256245e-06
Iter: 905 loss: 1.50945709e-06
Iter: 906 loss: 1.50887331e-06
Iter: 907 loss: 1.50950473e-06
Iter: 908 loss: 1.50879396e-06
Iter: 909 loss: 1.5082876e-06
Iter: 910 loss: 1.51471954e-06
Iter: 911 loss: 1.50821506e-06
Iter: 912 loss: 1.50779977e-06
Iter: 913 loss: 1.50829521e-06
Iter: 914 loss: 1.50743404e-06
Iter: 915 loss: 1.50693597e-06
Iter: 916 loss: 1.5066139e-06
Iter: 917 loss: 1.50630137e-06
Iter: 918 loss: 1.50567462e-06
Iter: 919 loss: 1.51290305e-06
Iter: 920 loss: 1.50570247e-06
Iter: 921 loss: 1.50517963e-06
Iter: 922 loss: 1.50459573e-06
Iter: 923 loss: 1.50447181e-06
Iter: 924 loss: 1.50464166e-06
Iter: 925 loss: 1.50424285e-06
Iter: 926 loss: 1.50392532e-06
Iter: 927 loss: 1.50541507e-06
Iter: 928 loss: 1.50387768e-06
Iter: 929 loss: 1.50372261e-06
Iter: 930 loss: 1.5034841e-06
Iter: 931 loss: 1.50671883e-06
Iter: 932 loss: 1.50320898e-06
Iter: 933 loss: 1.50269057e-06
Iter: 934 loss: 1.50471021e-06
Iter: 935 loss: 1.50259541e-06
Iter: 936 loss: 1.50206949e-06
Iter: 937 loss: 1.5021626e-06
Iter: 938 loss: 1.50172355e-06
Iter: 939 loss: 1.50109042e-06
Iter: 940 loss: 1.50335791e-06
Iter: 941 loss: 1.50107439e-06
Iter: 942 loss: 1.50054723e-06
Iter: 943 loss: 1.50068684e-06
Iter: 944 loss: 1.50010965e-06
Iter: 945 loss: 1.49948789e-06
Iter: 946 loss: 1.50571907e-06
Iter: 947 loss: 1.49956225e-06
Iter: 948 loss: 1.49914945e-06
Iter: 949 loss: 1.50312519e-06
Iter: 950 loss: 1.49905532e-06
Iter: 951 loss: 1.49872812e-06
Iter: 952 loss: 1.49854623e-06
Iter: 953 loss: 1.49835807e-06
Iter: 954 loss: 1.49797688e-06
Iter: 955 loss: 1.49883317e-06
Iter: 956 loss: 1.49763173e-06
Iter: 957 loss: 1.49713856e-06
Iter: 958 loss: 1.49815787e-06
Iter: 959 loss: 1.49694347e-06
Iter: 960 loss: 1.49699031e-06
Iter: 961 loss: 1.49670575e-06
Iter: 962 loss: 1.49645166e-06
Iter: 963 loss: 1.49594007e-06
Iter: 964 loss: 1.50192159e-06
Iter: 965 loss: 1.4957642e-06
Iter: 966 loss: 1.49540062e-06
Iter: 967 loss: 1.49520656e-06
Iter: 968 loss: 1.49493235e-06
Iter: 969 loss: 1.49429718e-06
Iter: 970 loss: 1.49518871e-06
Iter: 971 loss: 1.49396226e-06
Iter: 972 loss: 1.49328696e-06
Iter: 973 loss: 1.49730272e-06
Iter: 974 loss: 1.49333141e-06
Iter: 975 loss: 1.49269658e-06
Iter: 976 loss: 1.49281743e-06
Iter: 977 loss: 1.49212542e-06
Iter: 978 loss: 1.49129016e-06
Iter: 979 loss: 1.49450875e-06
Iter: 980 loss: 1.49125435e-06
Iter: 981 loss: 1.49050686e-06
Iter: 982 loss: 1.49138691e-06
Iter: 983 loss: 1.49008e-06
Iter: 984 loss: 1.48953109e-06
Iter: 985 loss: 1.48960976e-06
Iter: 986 loss: 1.48927086e-06
Iter: 987 loss: 1.4888542e-06
Iter: 988 loss: 1.48887955e-06
Iter: 989 loss: 1.48823733e-06
Iter: 990 loss: 1.48924528e-06
Iter: 991 loss: 1.48790446e-06
Iter: 992 loss: 1.48766333e-06
Iter: 993 loss: 1.48763377e-06
Iter: 994 loss: 1.48716435e-06
Iter: 995 loss: 1.48812876e-06
Iter: 996 loss: 1.48695779e-06
Iter: 997 loss: 1.4867594e-06
Iter: 998 loss: 1.48616266e-06
Iter: 999 loss: 1.49484731e-06
Iter: 1000 loss: 1.48611343e-06
Iter: 1001 loss: 1.48552397e-06
Iter: 1002 loss: 1.48654431e-06
Iter: 1003 loss: 1.48536742e-06
Iter: 1004 loss: 1.48462641e-06
Iter: 1005 loss: 1.48676327e-06
Iter: 1006 loss: 1.4844627e-06
Iter: 1007 loss: 1.48387562e-06
Iter: 1008 loss: 1.48554977e-06
Iter: 1009 loss: 1.48358095e-06
Iter: 1010 loss: 1.48290508e-06
Iter: 1011 loss: 1.48417303e-06
Iter: 1012 loss: 1.48268737e-06
Iter: 1013 loss: 1.48182653e-06
Iter: 1014 loss: 1.48442177e-06
Iter: 1015 loss: 1.48176491e-06
Iter: 1016 loss: 1.48111917e-06
Iter: 1017 loss: 1.48463209e-06
Iter: 1018 loss: 1.48106585e-06
Iter: 1019 loss: 1.48065612e-06
Iter: 1020 loss: 1.48148376e-06
Iter: 1021 loss: 1.48018512e-06
Iter: 1022 loss: 1.47972719e-06
Iter: 1023 loss: 1.48187917e-06
Iter: 1024 loss: 1.47965648e-06
Iter: 1025 loss: 1.47924698e-06
Iter: 1026 loss: 1.47874243e-06
Iter: 1027 loss: 1.47866194e-06
Iter: 1028 loss: 1.47933451e-06
Iter: 1029 loss: 1.47837409e-06
Iter: 1030 loss: 1.47824153e-06
Iter: 1031 loss: 1.47773255e-06
Iter: 1032 loss: 1.48207164e-06
Iter: 1033 loss: 1.47756191e-06
Iter: 1034 loss: 1.47714741e-06
Iter: 1035 loss: 1.47680089e-06
Iter: 1036 loss: 1.47661058e-06
Iter: 1037 loss: 1.47593232e-06
Iter: 1038 loss: 1.48100844e-06
Iter: 1039 loss: 1.47583376e-06
Iter: 1040 loss: 1.47538572e-06
Iter: 1041 loss: 1.47474952e-06
Iter: 1042 loss: 1.47457524e-06
Iter: 1043 loss: 1.47392768e-06
Iter: 1044 loss: 1.48515358e-06
Iter: 1045 loss: 1.47393382e-06
Iter: 1046 loss: 1.47345372e-06
Iter: 1047 loss: 1.47390028e-06
Iter: 1048 loss: 1.47314131e-06
Iter: 1049 loss: 1.47256367e-06
Iter: 1050 loss: 1.4763134e-06
Iter: 1051 loss: 1.4725872e-06
Iter: 1052 loss: 1.47222e-06
Iter: 1053 loss: 1.47184755e-06
Iter: 1054 loss: 1.47167111e-06
Iter: 1055 loss: 1.47108403e-06
Iter: 1056 loss: 1.47389903e-06
Iter: 1057 loss: 1.47100013e-06
Iter: 1058 loss: 1.47060587e-06
Iter: 1059 loss: 1.47487106e-06
Iter: 1060 loss: 1.47066271e-06
Iter: 1061 loss: 1.47045239e-06
Iter: 1062 loss: 1.4698237e-06
Iter: 1063 loss: 1.47694698e-06
Iter: 1064 loss: 1.46989532e-06
Iter: 1065 loss: 1.4706809e-06
Iter: 1066 loss: 1.46959e-06
Iter: 1067 loss: 1.46948082e-06
Iter: 1068 loss: 1.4691467e-06
Iter: 1069 loss: 1.47349249e-06
Iter: 1070 loss: 1.46902448e-06
Iter: 1071 loss: 1.46872719e-06
Iter: 1072 loss: 1.46832724e-06
Iter: 1073 loss: 1.46831201e-06
Iter: 1074 loss: 1.46777643e-06
Iter: 1075 loss: 1.47008927e-06
Iter: 1076 loss: 1.46759601e-06
Iter: 1077 loss: 1.46736227e-06
Iter: 1078 loss: 1.46852653e-06
Iter: 1079 loss: 1.46729872e-06
Iter: 1080 loss: 1.46672892e-06
Iter: 1081 loss: 1.46673e-06
Iter: 1082 loss: 1.4664206e-06
Iter: 1083 loss: 1.46591617e-06
Iter: 1084 loss: 1.46909156e-06
Iter: 1085 loss: 1.46592345e-06
Iter: 1086 loss: 1.46546017e-06
Iter: 1087 loss: 1.46561342e-06
Iter: 1088 loss: 1.46520313e-06
Iter: 1089 loss: 1.46468528e-06
Iter: 1090 loss: 1.46778211e-06
Iter: 1091 loss: 1.46467187e-06
Iter: 1092 loss: 1.46420689e-06
Iter: 1093 loss: 1.46370746e-06
Iter: 1094 loss: 1.46356319e-06
Iter: 1095 loss: 1.46307229e-06
Iter: 1096 loss: 1.46309276e-06
Iter: 1097 loss: 1.46292848e-06
Iter: 1098 loss: 1.46279069e-06
Iter: 1099 loss: 1.46257457e-06
Iter: 1100 loss: 1.46263631e-06
Iter: 1101 loss: 1.4623547e-06
Iter: 1102 loss: 1.46213483e-06
Iter: 1103 loss: 1.46183697e-06
Iter: 1104 loss: 1.46187506e-06
Iter: 1105 loss: 1.46138814e-06
Iter: 1106 loss: 1.46277239e-06
Iter: 1107 loss: 1.46127968e-06
Iter: 1108 loss: 1.46099649e-06
Iter: 1109 loss: 1.46100501e-06
Iter: 1110 loss: 1.46067066e-06
Iter: 1111 loss: 1.46032744e-06
Iter: 1112 loss: 1.46153548e-06
Iter: 1113 loss: 1.46007972e-06
Iter: 1114 loss: 1.45973354e-06
Iter: 1115 loss: 1.45984211e-06
Iter: 1116 loss: 1.45923639e-06
Iter: 1117 loss: 1.45879858e-06
Iter: 1118 loss: 1.46217349e-06
Iter: 1119 loss: 1.4586933e-06
Iter: 1120 loss: 1.45825618e-06
Iter: 1121 loss: 1.45844638e-06
Iter: 1122 loss: 1.45788783e-06
Iter: 1123 loss: 1.4574174e-06
Iter: 1124 loss: 1.46140735e-06
Iter: 1125 loss: 1.45746776e-06
Iter: 1126 loss: 1.45701745e-06
Iter: 1127 loss: 1.45725335e-06
Iter: 1128 loss: 1.45675881e-06
Iter: 1129 loss: 1.45635931e-06
Iter: 1130 loss: 1.45965373e-06
Iter: 1131 loss: 1.45624642e-06
Iter: 1132 loss: 1.45650313e-06
Iter: 1133 loss: 1.45619856e-06
Iter: 1134 loss: 1.45603383e-06
Iter: 1135 loss: 1.45582703e-06
Iter: 1136 loss: 1.45748413e-06
Iter: 1137 loss: 1.45571835e-06
Iter: 1138 loss: 1.45547062e-06
Iter: 1139 loss: 1.45521403e-06
Iter: 1140 loss: 1.45521176e-06
Iter: 1141 loss: 1.45471e-06
Iter: 1142 loss: 1.4598113e-06
Iter: 1143 loss: 1.4547885e-06
Iter: 1144 loss: 1.45438798e-06
Iter: 1145 loss: 1.45434706e-06
Iter: 1146 loss: 1.45402021e-06
Iter: 1147 loss: 1.45368699e-06
Iter: 1148 loss: 1.45439969e-06
Iter: 1149 loss: 1.45350919e-06
Iter: 1150 loss: 1.45299487e-06
Iter: 1151 loss: 1.4534894e-06
Iter: 1152 loss: 1.45281592e-06
Iter: 1153 loss: 1.45239039e-06
Iter: 1154 loss: 1.45415061e-06
Iter: 1155 loss: 1.45220838e-06
Iter: 1156 loss: 1.4517243e-06
Iter: 1157 loss: 1.452476e-06
Iter: 1158 loss: 1.45163494e-06
Iter: 1159 loss: 1.45101239e-06
Iter: 1160 loss: 1.45097749e-06
Iter: 1161 loss: 1.45069055e-06
Iter: 1162 loss: 1.45007402e-06
Iter: 1163 loss: 1.45418323e-06
Iter: 1164 loss: 1.45001241e-06
Iter: 1165 loss: 1.4495912e-06
Iter: 1166 loss: 1.45507352e-06
Iter: 1167 loss: 1.44955038e-06
Iter: 1168 loss: 1.44923638e-06
Iter: 1169 loss: 1.45032e-06
Iter: 1170 loss: 1.44910359e-06
Iter: 1171 loss: 1.4488578e-06
Iter: 1172 loss: 1.44844762e-06
Iter: 1173 loss: 1.45397075e-06
Iter: 1174 loss: 1.44842534e-06
Iter: 1175 loss: 1.44811099e-06
Iter: 1176 loss: 1.44899241e-06
Iter: 1177 loss: 1.44791852e-06
Iter: 1178 loss: 1.44735395e-06
Iter: 1179 loss: 1.44980959e-06
Iter: 1180 loss: 1.44733417e-06
Iter: 1181 loss: 1.44683031e-06
Iter: 1182 loss: 1.44835246e-06
Iter: 1183 loss: 1.44674482e-06
Iter: 1184 loss: 1.44653222e-06
Iter: 1185 loss: 1.44663227e-06
Iter: 1186 loss: 1.44623357e-06
Iter: 1187 loss: 1.4457778e-06
Iter: 1188 loss: 1.44670605e-06
Iter: 1189 loss: 1.44575074e-06
Iter: 1190 loss: 1.44540331e-06
Iter: 1191 loss: 1.44558817e-06
Iter: 1192 loss: 1.44497392e-06
Iter: 1193 loss: 1.44439673e-06
Iter: 1194 loss: 1.44760327e-06
Iter: 1195 loss: 1.44444596e-06
Iter: 1196 loss: 1.44395494e-06
Iter: 1197 loss: 1.44489275e-06
Iter: 1198 loss: 1.44389526e-06
Iter: 1199 loss: 1.44356125e-06
Iter: 1200 loss: 1.44385649e-06
Iter: 1201 loss: 1.44331557e-06
Iter: 1202 loss: 1.44330568e-06
Iter: 1203 loss: 1.44298895e-06
Iter: 1204 loss: 1.44297223e-06
Iter: 1205 loss: 1.44286798e-06
Iter: 1206 loss: 1.44278101e-06
Iter: 1207 loss: 1.44242267e-06
Iter: 1208 loss: 1.44212686e-06
Iter: 1209 loss: 1.4489608e-06
Iter: 1210 loss: 1.44229261e-06
Iter: 1211 loss: 1.44199475e-06
Iter: 1212 loss: 1.44252454e-06
Iter: 1213 loss: 1.44179671e-06
Iter: 1214 loss: 1.44151443e-06
Iter: 1215 loss: 1.44274668e-06
Iter: 1216 loss: 1.44138266e-06
Iter: 1217 loss: 1.44095156e-06
Iter: 1218 loss: 1.44201272e-06
Iter: 1219 loss: 1.44085652e-06
Iter: 1220 loss: 1.44058959e-06
Iter: 1221 loss: 1.44049011e-06
Iter: 1222 loss: 1.440196e-06
Iter: 1223 loss: 1.43979355e-06
Iter: 1224 loss: 1.44159139e-06
Iter: 1225 loss: 1.43971897e-06
Iter: 1226 loss: 1.43929083e-06
Iter: 1227 loss: 1.43974341e-06
Iter: 1228 loss: 1.43900911e-06
Iter: 1229 loss: 1.438715e-06
Iter: 1230 loss: 1.43953287e-06
Iter: 1231 loss: 1.43854959e-06
Iter: 1232 loss: 1.43804482e-06
Iter: 1233 loss: 1.43885188e-06
Iter: 1234 loss: 1.43795626e-06
Iter: 1235 loss: 1.43743523e-06
Iter: 1236 loss: 1.43883744e-06
Iter: 1237 loss: 1.43721934e-06
Iter: 1238 loss: 1.43675845e-06
Iter: 1239 loss: 1.43885632e-06
Iter: 1240 loss: 1.43676834e-06
Iter: 1241 loss: 1.43700288e-06
Iter: 1242 loss: 1.43664943e-06
Iter: 1243 loss: 1.43654051e-06
Iter: 1244 loss: 1.43637817e-06
Iter: 1245 loss: 1.43650459e-06
Iter: 1246 loss: 1.4359066e-06
Iter: 1247 loss: 1.43560101e-06
Iter: 1248 loss: 1.43727289e-06
Iter: 1249 loss: 1.43556622e-06
Iter: 1250 loss: 1.43525983e-06
Iter: 1251 loss: 1.43555587e-06
Iter: 1252 loss: 1.43500574e-06
Iter: 1253 loss: 1.43457385e-06
Iter: 1254 loss: 1.43665966e-06
Iter: 1255 loss: 1.43450688e-06
Iter: 1256 loss: 1.43409181e-06
Iter: 1257 loss: 1.43717455e-06
Iter: 1258 loss: 1.43413718e-06
Iter: 1259 loss: 1.43399177e-06
Iter: 1260 loss: 1.43371233e-06
Iter: 1261 loss: 1.43360967e-06
Iter: 1262 loss: 1.43325326e-06
Iter: 1263 loss: 1.43452087e-06
Iter: 1264 loss: 1.43314014e-06
Iter: 1265 loss: 1.4327743e-06
Iter: 1266 loss: 1.43332318e-06
Iter: 1267 loss: 1.43277839e-06
Iter: 1268 loss: 1.4322286e-06
Iter: 1269 loss: 1.43307454e-06
Iter: 1270 loss: 1.43211878e-06
Iter: 1271 loss: 1.43176044e-06
Iter: 1272 loss: 1.43322586e-06
Iter: 1273 loss: 1.43152715e-06
Iter: 1274 loss: 1.43108878e-06
Iter: 1275 loss: 1.43267971e-06
Iter: 1276 loss: 1.43093587e-06
Iter: 1277 loss: 1.43087402e-06
Iter: 1278 loss: 1.43079592e-06
Iter: 1279 loss: 1.43064369e-06
Iter: 1280 loss: 1.43051557e-06
Iter: 1281 loss: 1.4303215e-06
Iter: 1282 loss: 1.4302866e-06
Iter: 1283 loss: 1.42989961e-06
Iter: 1284 loss: 1.42987403e-06
Iter: 1285 loss: 1.42972976e-06
Iter: 1286 loss: 1.42980548e-06
Iter: 1287 loss: 1.42945873e-06
Iter: 1288 loss: 1.42915871e-06
Iter: 1289 loss: 1.42979854e-06
Iter: 1290 loss: 1.42893009e-06
Iter: 1291 loss: 1.42865861e-06
Iter: 1292 loss: 1.43272416e-06
Iter: 1293 loss: 1.42852434e-06
Iter: 1294 loss: 1.42825706e-06
Iter: 1295 loss: 1.42917338e-06
Iter: 1296 loss: 1.42813485e-06
Iter: 1297 loss: 1.42777196e-06
Iter: 1298 loss: 1.4280688e-06
Iter: 1299 loss: 1.427592e-06
Iter: 1300 loss: 1.42722115e-06
Iter: 1301 loss: 1.42767544e-06
Iter: 1302 loss: 1.42702629e-06
Iter: 1303 loss: 1.42654426e-06
Iter: 1304 loss: 1.42800104e-06
Iter: 1305 loss: 1.42640795e-06
Iter: 1306 loss: 1.4259299e-06
Iter: 1307 loss: 1.42796534e-06
Iter: 1308 loss: 1.42596059e-06
Iter: 1309 loss: 1.42559259e-06
Iter: 1310 loss: 1.42544786e-06
Iter: 1311 loss: 1.42530678e-06
Iter: 1312 loss: 1.42490205e-06
Iter: 1313 loss: 1.42929525e-06
Iter: 1314 loss: 1.42486283e-06
Iter: 1315 loss: 1.42481008e-06
Iter: 1316 loss: 1.42481588e-06
Iter: 1317 loss: 1.4246001e-06
Iter: 1318 loss: 1.42441877e-06
Iter: 1319 loss: 1.42374734e-06
Iter: 1320 loss: 1.42392446e-06
Iter: 1321 loss: 1.42324075e-06
Iter: 1322 loss: 1.42786894e-06
Iter: 1323 loss: 1.42321016e-06
Iter: 1324 loss: 1.42289741e-06
Iter: 1325 loss: 1.42285853e-06
Iter: 1326 loss: 1.42240629e-06
Iter: 1327 loss: 1.42197359e-06
Iter: 1328 loss: 1.42597582e-06
Iter: 1329 loss: 1.42182103e-06
Iter: 1330 loss: 1.42148019e-06
Iter: 1331 loss: 1.42519036e-06
Iter: 1332 loss: 1.42162332e-06
Iter: 1333 loss: 1.42132694e-06
Iter: 1334 loss: 1.42102419e-06
Iter: 1335 loss: 1.42094177e-06
Iter: 1336 loss: 1.42072338e-06
Iter: 1337 loss: 1.4225817e-06
Iter: 1338 loss: 1.42068438e-06
Iter: 1339 loss: 1.42020554e-06
Iter: 1340 loss: 1.42130534e-06
Iter: 1341 loss: 1.42010742e-06
Iter: 1342 loss: 1.41975408e-06
Iter: 1343 loss: 1.42007525e-06
Iter: 1344 loss: 1.41957912e-06
Iter: 1345 loss: 1.41923078e-06
Iter: 1346 loss: 1.41981707e-06
Iter: 1347 loss: 1.41900387e-06
Iter: 1348 loss: 1.41873238e-06
Iter: 1349 loss: 1.42141971e-06
Iter: 1350 loss: 1.41856378e-06
Iter: 1351 loss: 1.41830594e-06
Iter: 1352 loss: 1.41827377e-06
Iter: 1353 loss: 1.41833993e-06
Iter: 1354 loss: 1.41836654e-06
Iter: 1355 loss: 1.41825296e-06
Iter: 1356 loss: 1.41834983e-06
Iter: 1357 loss: 1.41830583e-06
Iter: 1358 loss: 1.41834505e-06
Iter: 1359 loss: 1.41815985e-06
Iter: 1360 loss: 1.41826172e-06
Iter: 1361 loss: 1.41834812e-06
Iter: 1362 loss: 1.41836e-06
Iter: 1363 loss: 1.41827968e-06
Iter: 1364 loss: 1.41828127e-06
Iter: 1365 loss: 1.4183297e-06
Iter: 1366 loss: 1.41827218e-06
Iter: 1367 loss: 1.41831106e-06
Iter: 1368 loss: 1.41831629e-06
Iter: 1369 loss: 1.41828821e-06
Iter: 1370 loss: 1.41826933e-06
Iter: 1371 loss: 1.41827127e-06
Iter: 1372 loss: 1.41827809e-06
Iter: 1373 loss: 1.41827115e-06
Iter: 1374 loss: 1.4182765e-06
Iter: 1375 loss: 1.41826922e-06
Iter: 1376 loss: 1.4182765e-06
Iter: 1377 loss: 1.4182765e-06
Iter: 1378 loss: 1.4182765e-06
Iter: 1379 loss: 1.41826922e-06
Iter: 1380 loss: 1.41755822e-06
Iter: 1381 loss: 1.42627698e-06
Iter: 1382 loss: 1.41762473e-06
Iter: 1383 loss: 1.41717646e-06
Iter: 1384 loss: 1.41811029e-06
Iter: 1385 loss: 1.41699434e-06
Iter: 1386 loss: 1.4167349e-06
Iter: 1387 loss: 1.42084116e-06
Iter: 1388 loss: 1.41678026e-06
Iter: 1389 loss: 1.41646888e-06
Iter: 1390 loss: 1.41823966e-06
Iter: 1391 loss: 1.41635121e-06
Iter: 1392 loss: 1.41626117e-06
Iter: 1393 loss: 1.41591295e-06
Iter: 1394 loss: 1.41591431e-06
Iter: 1395 loss: 1.41550515e-06
Iter: 1396 loss: 1.41597116e-06
Iter: 1397 loss: 1.41545775e-06
Iter: 1398 loss: 1.41501459e-06
Iter: 1399 loss: 1.41639816e-06
Iter: 1400 loss: 1.41487976e-06
Iter: 1401 loss: 1.41474823e-06
Iter: 1402 loss: 1.41493638e-06
Iter: 1403 loss: 1.41442433e-06
Iter: 1404 loss: 1.41414728e-06
Iter: 1405 loss: 1.41543956e-06
Iter: 1406 loss: 1.41415489e-06
Iter: 1407 loss: 1.413988e-06
Iter: 1408 loss: 1.41383316e-06
Iter: 1409 loss: 1.4137953e-06
Iter: 1410 loss: 1.41350642e-06
Iter: 1411 loss: 1.41357714e-06
Iter: 1412 loss: 1.41340411e-06
Iter: 1413 loss: 1.41299574e-06
Iter: 1414 loss: 1.41294754e-06
Iter: 1415 loss: 1.41268424e-06
Iter: 1416 loss: 1.41472697e-06
Iter: 1417 loss: 1.41264695e-06
Iter: 1418 loss: 1.41233568e-06
Iter: 1419 loss: 1.41233613e-06
Iter: 1420 loss: 1.41207715e-06
Iter: 1421 loss: 1.4117611e-06
Iter: 1422 loss: 1.41496e-06
Iter: 1423 loss: 1.41172677e-06
Iter: 1424 loss: 1.41142846e-06
Iter: 1425 loss: 1.4120825e-06
Iter: 1426 loss: 1.41121984e-06
Iter: 1427 loss: 1.41096552e-06
Iter: 1428 loss: 1.41120358e-06
Iter: 1429 loss: 1.41055307e-06
Iter: 1430 loss: 1.41021724e-06
Iter: 1431 loss: 1.4112328e-06
Iter: 1432 loss: 1.41022088e-06
Iter: 1433 loss: 1.40968166e-06
Iter: 1434 loss: 1.4102967e-06
Iter: 1435 loss: 1.40952363e-06
Iter: 1436 loss: 1.40903546e-06
Iter: 1437 loss: 1.41087753e-06
Iter: 1438 loss: 1.40891473e-06
Iter: 1439 loss: 1.40923032e-06
Iter: 1440 loss: 1.40884583e-06
Iter: 1441 loss: 1.40862585e-06
Iter: 1442 loss: 1.40864552e-06
Iter: 1443 loss: 1.408662e-06
Iter: 1444 loss: 1.40842087e-06
Iter: 1445 loss: 1.40823784e-06
Iter: 1446 loss: 1.41252315e-06
Iter: 1447 loss: 1.40816746e-06
Iter: 1448 loss: 1.4076918e-06
Iter: 1449 loss: 1.40865563e-06
Iter: 1450 loss: 1.40763905e-06
Iter: 1451 loss: 1.40727332e-06
Iter: 1452 loss: 1.40750672e-06
Iter: 1453 loss: 1.40680481e-06
Iter: 1454 loss: 1.4064392e-06
Iter: 1455 loss: 1.4107469e-06
Iter: 1456 loss: 1.40650707e-06
Iter: 1457 loss: 1.40628038e-06
Iter: 1458 loss: 1.40680947e-06
Iter: 1459 loss: 1.40619215e-06
Iter: 1460 loss: 1.40584802e-06
Iter: 1461 loss: 1.40604402e-06
Iter: 1462 loss: 1.40551151e-06
Iter: 1463 loss: 1.40526686e-06
Iter: 1464 loss: 1.40677366e-06
Iter: 1465 loss: 1.40518046e-06
Iter: 1466 loss: 1.40470013e-06
Iter: 1467 loss: 1.40455e-06
Iter: 1468 loss: 1.40435839e-06
Iter: 1469 loss: 1.40384861e-06
Iter: 1470 loss: 1.40981933e-06
Iter: 1471 loss: 1.40396673e-06
Iter: 1472 loss: 1.40363909e-06
Iter: 1473 loss: 1.40648217e-06
Iter: 1474 loss: 1.40369752e-06
Iter: 1475 loss: 1.40348936e-06
Iter: 1476 loss: 1.40420195e-06
Iter: 1477 loss: 1.40327097e-06
Iter: 1478 loss: 1.40321288e-06
Iter: 1479 loss: 1.40291058e-06
Iter: 1480 loss: 1.40290376e-06
Iter: 1481 loss: 1.40253496e-06
Iter: 1482 loss: 1.40298698e-06
Iter: 1483 loss: 1.40238683e-06
Iter: 1484 loss: 1.40210705e-06
Iter: 1485 loss: 1.4022994e-06
Iter: 1486 loss: 1.40191833e-06
Iter: 1487 loss: 1.40150792e-06
Iter: 1488 loss: 1.40354e-06
Iter: 1489 loss: 1.40150291e-06
Iter: 1490 loss: 1.40110876e-06
Iter: 1491 loss: 1.40258726e-06
Iter: 1492 loss: 1.40112411e-06
Iter: 1493 loss: 1.40076213e-06
Iter: 1494 loss: 1.40159909e-06
Iter: 1495 loss: 1.40070551e-06
Iter: 1496 loss: 1.40031329e-06
Iter: 1497 loss: 1.40067937e-06
Iter: 1498 loss: 1.4002577e-06
Iter: 1499 loss: 1.39999406e-06
Iter: 1500 loss: 1.40147426e-06
Iter: 1501 loss: 1.39984365e-06
Iter: 1502 loss: 1.3996937e-06
Iter: 1503 loss: 1.39969563e-06
Iter: 1504 loss: 1.39944439e-06
Iter: 1505 loss: 1.39912413e-06
Iter: 1506 loss: 1.40221596e-06
Iter: 1507 loss: 1.39912549e-06
Iter: 1508 loss: 1.39913686e-06
Iter: 1509 loss: 1.39890597e-06
Iter: 1510 loss: 1.39898634e-06
Iter: 1511 loss: 1.39891711e-06
Iter: 1512 loss: 1.39901397e-06
Iter: 1513 loss: 1.3990275e-06
Iter: 1514 loss: 1.39897952e-06
Iter: 1515 loss: 1.39897452e-06
Iter: 1516 loss: 1.39903977e-06
Iter: 1517 loss: 1.39900476e-06
Iter: 1518 loss: 1.3989677e-06
Iter: 1519 loss: 1.39892495e-06
Iter: 1520 loss: 1.39890483e-06
Iter: 1521 loss: 1.39891881e-06
Iter: 1522 loss: 1.39886356e-06
Iter: 1523 loss: 1.39887072e-06
Iter: 1524 loss: 1.39883377e-06
Iter: 1525 loss: 1.39885128e-06
Iter: 1526 loss: 1.39887038e-06
Iter: 1527 loss: 1.3989054e-06
Iter: 1528 loss: 1.39891233e-06
Iter: 1529 loss: 1.39891108e-06
Iter: 1530 loss: 1.39890517e-06
Iter: 1531 loss: 1.39890517e-06
Iter: 1532 loss: 1.39890517e-06
Iter: 1533 loss: 1.39890506e-06
Iter: 1534 loss: 1.39890506e-06
Iter: 1535 loss: 1.39891108e-06
Iter: 1536 loss: 1.39890506e-06
Iter: 1537 loss: 1.39890506e-06
Iter: 1538 loss: 1.39891108e-06
Iter: 1539 loss: 1.39891108e-06
Iter: 1540 loss: 1.39890506e-06
Iter: 1541 loss: 1.39890506e-06
Iter: 1542 loss: 1.39891108e-06
Iter: 1543 loss: 1.39891108e-06
Iter: 1544 loss: 1.39890506e-06
Iter: 1545 loss: 1.40386351e-06
Iter: 1546 loss: 1.39898111e-06
Iter: 1547 loss: 1.39894541e-06
Iter: 1548 loss: 1.39900772e-06
Iter: 1549 loss: 1.3990624e-06
Iter: 1550 loss: 1.39894883e-06
Iter: 1551 loss: 1.39903523e-06
Iter: 1552 loss: 1.39896781e-06
Iter: 1553 loss: 1.39894337e-06
Iter: 1554 loss: 1.3989652e-06
Iter: 1555 loss: 1.39896474e-06
Iter: 1556 loss: 1.39900749e-06
Iter: 1557 loss: 1.3989561e-06
Iter: 1558 loss: 1.39902045e-06
Iter: 1559 loss: 1.3989395e-06
Iter: 1560 loss: 1.39895837e-06
Iter: 1561 loss: 1.39894746e-06
Iter: 1562 loss: 1.39893677e-06
Iter: 1563 loss: 1.39892131e-06
Iter: 1564 loss: 1.39890267e-06
Iter: 1565 loss: 1.39892e-06
Iter: 1566 loss: 1.39891745e-06
Iter: 1567 loss: 1.39891199e-06
Iter: 1568 loss: 1.39890335e-06
Iter: 1569 loss: 1.3989121e-06
Iter: 1570 loss: 1.3989121e-06
Iter: 1571 loss: 1.39890335e-06
Iter: 1572 loss: 1.3989121e-06
Iter: 1573 loss: 1.39890335e-06
Iter: 1574 loss: 1.39890335e-06
Iter: 1575 loss: 1.39890335e-06
Iter: 1576 loss: 1.3989121e-06
Iter: 1577 loss: 1.3989121e-06
Iter: 1578 loss: 1.3989121e-06
Iter: 1579 loss: 1.3989121e-06
Iter: 1580 loss: 1.3989121e-06
Iter: 1581 loss: 1.3989121e-06
Iter: 1582 loss: 1.39890335e-06
Iter: 1583 loss: 1.39890335e-06
Iter: 1584 loss: 1.39890335e-06
Iter: 1585 loss: 1.39890335e-06
Iter: 1586 loss: 1.39890335e-06
Iter: 1587 loss: 1.39890335e-06
Iter: 1588 loss: 1.39890335e-06
Iter: 1589 loss: 1.3989121e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi2.8/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi3 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi3
+ date
Sun Nov  8 18:58:45 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi3/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi3/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 0 --phi 3 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi3/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb269468c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb2694b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb268eeb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb2688a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb26898488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb2686b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb0013e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb00151ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb00151400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb00151488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb000ef2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb000f3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb00019f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb00088158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb0008d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcb000197b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae03ebea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae0391ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae0327d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae0391268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae0376598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae037b1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae0376b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae0431f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae02f5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae02f4c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae02f48c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae02ab1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae023e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae023ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae01d9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae023ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae023e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae02788c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae00d1158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcae00e4f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Traceback (most recent call last):
  File "biholoNN_train.py", line 202, in <module>
    grads = tape.gradient(loss, model.trainable_weights)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py", line 1073, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1266, in _backward_function_wrapper
    processed_args, remapped_captures)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/function.py", line 550, in call
    ctx=ctx)
  File "/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input is not invertible.
	 [[node gradients/MatrixDeterminant_grad/MatrixInverse (defined at biholoNN_train.py:200) ]] [Op:__inference___backward_volume_form_4446_8945]

Function call stack:
__backward_volume_form_4446

+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi3/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi3/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 0 --phi 3 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi0_phi3/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fd72488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fd73a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fdbb8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fe97950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fe977b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fe97730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fe97620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fd0d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fcb3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fc690d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fc69950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fc36488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fc52ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fc06268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fc527b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fc52a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fbc5ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fbc5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fb21ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fb9a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fb02268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fb182f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fab76a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fac8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fa62598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d12daff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d4fa97840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d12daf378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d12d251e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d12d200d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d12d2f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d12cef1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d12cdb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8d12ca59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8cec3d5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8cec3f4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.671071e-05
Iter: 2 loss: 0.000214090483
Iter: 3 loss: 6.32712909e-06
Iter: 4 loss: 5.30235957e-06
Iter: 5 loss: 3.65599681e-06
Iter: 6 loss: 3.64340167e-06
Iter: 7 loss: 2.9600692e-06
Iter: 8 loss: 4.62078697e-06
Iter: 9 loss: 2.71588601e-06
Iter: 10 loss: 2.33586479e-06
Iter: 11 loss: 3.27291446e-06
Iter: 12 loss: 2.20132642e-06
Iter: 13 loss: 2.03199488e-06
Iter: 14 loss: 2.39594056e-06
Iter: 15 loss: 1.96592e-06
Iter: 16 loss: 1.87080616e-06
Iter: 17 loss: 1.99954889e-06
Iter: 18 loss: 1.82333065e-06
Iter: 19 loss: 1.77916604e-06
Iter: 20 loss: 1.76730191e-06
Iter: 21 loss: 1.73997489e-06
Iter: 22 loss: 1.68149052e-06
Iter: 23 loss: 1.74174284e-06
Iter: 24 loss: 1.64896142e-06
Iter: 25 loss: 1.59273679e-06
Iter: 26 loss: 1.73484477e-06
Iter: 27 loss: 1.57320062e-06
Iter: 28 loss: 1.54976874e-06
Iter: 29 loss: 1.56523083e-06
Iter: 30 loss: 1.53483393e-06
Iter: 31 loss: 1.51009351e-06
Iter: 32 loss: 1.52744678e-06
Iter: 33 loss: 1.49459152e-06
Iter: 34 loss: 1.46864363e-06
Iter: 35 loss: 1.76493904e-06
Iter: 36 loss: 1.46829746e-06
Iter: 37 loss: 1.45104923e-06
Iter: 38 loss: 1.46948287e-06
Iter: 39 loss: 1.44149089e-06
Iter: 40 loss: 1.42315753e-06
Iter: 41 loss: 1.46039861e-06
Iter: 42 loss: 1.41575629e-06
Iter: 43 loss: 1.3954101e-06
Iter: 44 loss: 1.49249411e-06
Iter: 45 loss: 1.3917047e-06
Iter: 46 loss: 1.37415805e-06
Iter: 47 loss: 1.44993794e-06
Iter: 48 loss: 1.37057577e-06
Iter: 49 loss: 1.35581445e-06
Iter: 50 loss: 1.36232495e-06
Iter: 51 loss: 1.34558763e-06
Iter: 52 loss: 1.32879848e-06
Iter: 53 loss: 1.58912894e-06
Iter: 54 loss: 1.32872447e-06
Iter: 55 loss: 1.3204924e-06
Iter: 56 loss: 1.31622573e-06
Iter: 57 loss: 1.31231866e-06
Iter: 58 loss: 1.30030821e-06
Iter: 59 loss: 1.28535476e-06
Iter: 60 loss: 1.28418719e-06
Iter: 61 loss: 1.27524584e-06
Iter: 62 loss: 1.27383146e-06
Iter: 63 loss: 1.26256907e-06
Iter: 64 loss: 1.25263978e-06
Iter: 65 loss: 1.24973019e-06
Iter: 66 loss: 1.23506334e-06
Iter: 67 loss: 1.28585896e-06
Iter: 68 loss: 1.23119196e-06
Iter: 69 loss: 1.22134759e-06
Iter: 70 loss: 1.23704058e-06
Iter: 71 loss: 1.21673838e-06
Iter: 72 loss: 1.20707352e-06
Iter: 73 loss: 1.34257562e-06
Iter: 74 loss: 1.2070609e-06
Iter: 75 loss: 1.20107882e-06
Iter: 76 loss: 1.20444611e-06
Iter: 77 loss: 1.19720073e-06
Iter: 78 loss: 1.18907667e-06
Iter: 79 loss: 1.21482458e-06
Iter: 80 loss: 1.18665821e-06
Iter: 81 loss: 1.17915704e-06
Iter: 82 loss: 1.23215523e-06
Iter: 83 loss: 1.17853335e-06
Iter: 84 loss: 1.17424588e-06
Iter: 85 loss: 1.19076833e-06
Iter: 86 loss: 1.17340278e-06
Iter: 87 loss: 1.16784361e-06
Iter: 88 loss: 1.1618348e-06
Iter: 89 loss: 1.16102524e-06
Iter: 90 loss: 1.15218268e-06
Iter: 91 loss: 1.17936247e-06
Iter: 92 loss: 1.14964951e-06
Iter: 93 loss: 1.14314e-06
Iter: 94 loss: 1.14729255e-06
Iter: 95 loss: 1.13925694e-06
Iter: 96 loss: 1.13276803e-06
Iter: 97 loss: 1.21969185e-06
Iter: 98 loss: 1.13271869e-06
Iter: 99 loss: 1.12812393e-06
Iter: 100 loss: 1.14819989e-06
Iter: 101 loss: 1.12718294e-06
Iter: 102 loss: 1.12343332e-06
Iter: 103 loss: 1.1165364e-06
Iter: 104 loss: 1.28460124e-06
Iter: 105 loss: 1.11652207e-06
Iter: 106 loss: 1.1092468e-06
Iter: 107 loss: 1.14759018e-06
Iter: 108 loss: 1.1080258e-06
Iter: 109 loss: 1.10262044e-06
Iter: 110 loss: 1.12704242e-06
Iter: 111 loss: 1.1015195e-06
Iter: 112 loss: 1.09614234e-06
Iter: 113 loss: 1.13274348e-06
Iter: 114 loss: 1.09565497e-06
Iter: 115 loss: 1.09131952e-06
Iter: 116 loss: 1.1107461e-06
Iter: 117 loss: 1.09069276e-06
Iter: 118 loss: 1.08801783e-06
Iter: 119 loss: 1.10501287e-06
Iter: 120 loss: 1.08779454e-06
Iter: 121 loss: 1.08591496e-06
Iter: 122 loss: 1.09028042e-06
Iter: 123 loss: 1.08519851e-06
Iter: 124 loss: 1.08283029e-06
Iter: 125 loss: 1.08207428e-06
Iter: 126 loss: 1.08064057e-06
Iter: 127 loss: 1.07781261e-06
Iter: 128 loss: 1.0819499e-06
Iter: 129 loss: 1.07651249e-06
Iter: 130 loss: 1.07394237e-06
Iter: 131 loss: 1.07312906e-06
Iter: 132 loss: 1.0715471e-06
Iter: 133 loss: 1.06995572e-06
Iter: 134 loss: 1.06935147e-06
Iter: 135 loss: 1.06768039e-06
Iter: 136 loss: 1.06505672e-06
Iter: 137 loss: 1.06506286e-06
Iter: 138 loss: 1.06107075e-06
Iter: 139 loss: 1.06738e-06
Iter: 140 loss: 1.0592081e-06
Iter: 141 loss: 1.05625179e-06
Iter: 142 loss: 1.07429469e-06
Iter: 143 loss: 1.05597428e-06
Iter: 144 loss: 1.05321237e-06
Iter: 145 loss: 1.05474714e-06
Iter: 146 loss: 1.05138042e-06
Iter: 147 loss: 1.04992864e-06
Iter: 148 loss: 1.04957144e-06
Iter: 149 loss: 1.04841126e-06
Iter: 150 loss: 1.05021741e-06
Iter: 151 loss: 1.04774267e-06
Iter: 152 loss: 1.04639798e-06
Iter: 153 loss: 1.0478492e-06
Iter: 154 loss: 1.04563492e-06
Iter: 155 loss: 1.04368735e-06
Iter: 156 loss: 1.04825449e-06
Iter: 157 loss: 1.04302512e-06
Iter: 158 loss: 1.04137666e-06
Iter: 159 loss: 1.04512196e-06
Iter: 160 loss: 1.04089577e-06
Iter: 161 loss: 1.03934826e-06
Iter: 162 loss: 1.03722687e-06
Iter: 163 loss: 1.03706952e-06
Iter: 164 loss: 1.03502634e-06
Iter: 165 loss: 1.06410994e-06
Iter: 166 loss: 1.0350476e-06
Iter: 167 loss: 1.0337335e-06
Iter: 168 loss: 1.03742275e-06
Iter: 169 loss: 1.03333116e-06
Iter: 170 loss: 1.0313147e-06
Iter: 171 loss: 1.03464572e-06
Iter: 172 loss: 1.03036768e-06
Iter: 173 loss: 1.02869899e-06
Iter: 174 loss: 1.02674164e-06
Iter: 175 loss: 1.0266001e-06
Iter: 176 loss: 1.02342847e-06
Iter: 177 loss: 1.03887385e-06
Iter: 178 loss: 1.02291563e-06
Iter: 179 loss: 1.02057959e-06
Iter: 180 loss: 1.03051309e-06
Iter: 181 loss: 1.02019e-06
Iter: 182 loss: 1.01774253e-06
Iter: 183 loss: 1.03465914e-06
Iter: 184 loss: 1.01741114e-06
Iter: 185 loss: 1.01580292e-06
Iter: 186 loss: 1.02098375e-06
Iter: 187 loss: 1.01522153e-06
Iter: 188 loss: 1.01390947e-06
Iter: 189 loss: 1.01740602e-06
Iter: 190 loss: 1.01342891e-06
Iter: 191 loss: 1.01210185e-06
Iter: 192 loss: 1.01306023e-06
Iter: 193 loss: 1.01120747e-06
Iter: 194 loss: 1.00941338e-06
Iter: 195 loss: 1.00996488e-06
Iter: 196 loss: 1.00817431e-06
Iter: 197 loss: 1.0063435e-06
Iter: 198 loss: 1.01244359e-06
Iter: 199 loss: 1.00589671e-06
Iter: 200 loss: 1.00420357e-06
Iter: 201 loss: 1.00873353e-06
Iter: 202 loss: 1.00361e-06
Iter: 203 loss: 1.0020151e-06
Iter: 204 loss: 1.01804744e-06
Iter: 205 loss: 1.00199736e-06
Iter: 206 loss: 1.00077341e-06
Iter: 207 loss: 1.00325906e-06
Iter: 208 loss: 1.00028012e-06
Iter: 209 loss: 9.99132226e-07
Iter: 210 loss: 9.9854833e-07
Iter: 211 loss: 9.98051e-07
Iter: 212 loss: 9.96426593e-07
Iter: 213 loss: 1.00318255e-06
Iter: 214 loss: 9.9598094e-07
Iter: 215 loss: 9.94997094e-07
Iter: 216 loss: 1.00536045e-06
Iter: 217 loss: 9.94983793e-07
Iter: 218 loss: 9.93865683e-07
Iter: 219 loss: 9.97016741e-07
Iter: 220 loss: 9.93529625e-07
Iter: 221 loss: 9.92770651e-07
Iter: 222 loss: 9.9504075e-07
Iter: 223 loss: 9.92556807e-07
Iter: 224 loss: 9.91753268e-07
Iter: 225 loss: 9.91436082e-07
Iter: 226 loss: 9.90972467e-07
Iter: 227 loss: 9.89613e-07
Iter: 228 loss: 9.91556817e-07
Iter: 229 loss: 9.88971124e-07
Iter: 230 loss: 9.87731937e-07
Iter: 231 loss: 9.93780532e-07
Iter: 232 loss: 9.87524231e-07
Iter: 233 loss: 9.86489908e-07
Iter: 234 loss: 9.84916369e-07
Iter: 235 loss: 9.84902158e-07
Iter: 236 loss: 9.83919335e-07
Iter: 237 loss: 9.83760856e-07
Iter: 238 loss: 9.83068e-07
Iter: 239 loss: 9.87583689e-07
Iter: 240 loss: 9.82930487e-07
Iter: 241 loss: 9.82379788e-07
Iter: 242 loss: 9.81097401e-07
Iter: 243 loss: 1.00753346e-06
Iter: 244 loss: 9.81162202e-07
Iter: 245 loss: 9.79904712e-07
Iter: 246 loss: 9.89623118e-07
Iter: 247 loss: 9.79753395e-07
Iter: 248 loss: 9.78716571e-07
Iter: 249 loss: 9.78498861e-07
Iter: 250 loss: 9.77856871e-07
Iter: 251 loss: 9.77712716e-07
Iter: 252 loss: 9.7713064e-07
Iter: 253 loss: 9.767233e-07
Iter: 254 loss: 9.76275146e-07
Iter: 255 loss: 9.76150204e-07
Iter: 256 loss: 9.75303351e-07
Iter: 257 loss: 9.76061642e-07
Iter: 258 loss: 9.74850309e-07
Iter: 259 loss: 9.7391387e-07
Iter: 260 loss: 9.7703e-07
Iter: 261 loss: 9.73633405e-07
Iter: 262 loss: 9.73041551e-07
Iter: 263 loss: 9.75157491e-07
Iter: 264 loss: 9.72837597e-07
Iter: 265 loss: 9.72282692e-07
Iter: 266 loss: 9.72572934e-07
Iter: 267 loss: 9.71708232e-07
Iter: 268 loss: 9.70935844e-07
Iter: 269 loss: 9.71870691e-07
Iter: 270 loss: 9.70474275e-07
Iter: 271 loss: 9.69616e-07
Iter: 272 loss: 9.74468776e-07
Iter: 273 loss: 9.6935787e-07
Iter: 274 loss: 9.68473159e-07
Iter: 275 loss: 9.74067461e-07
Iter: 276 loss: 9.68313202e-07
Iter: 277 loss: 9.6757833e-07
Iter: 278 loss: 9.67384608e-07
Iter: 279 loss: 9.66894163e-07
Iter: 280 loss: 9.66051857e-07
Iter: 281 loss: 9.66004109e-07
Iter: 282 loss: 9.65399522e-07
Iter: 283 loss: 9.64331434e-07
Iter: 284 loss: 9.75745479e-07
Iter: 285 loss: 9.64272317e-07
Iter: 286 loss: 9.63519824e-07
Iter: 287 loss: 9.63553e-07
Iter: 288 loss: 9.63223101e-07
Iter: 289 loss: 9.6236522e-07
Iter: 290 loss: 9.77233412e-07
Iter: 291 loss: 9.62357603e-07
Iter: 292 loss: 9.61308388e-07
Iter: 293 loss: 9.64675337e-07
Iter: 294 loss: 9.61023829e-07
Iter: 295 loss: 9.60263606e-07
Iter: 296 loss: 9.62270406e-07
Iter: 297 loss: 9.59989393e-07
Iter: 298 loss: 9.59154704e-07
Iter: 299 loss: 9.59721092e-07
Iter: 300 loss: 9.58600367e-07
Iter: 301 loss: 9.57589e-07
Iter: 302 loss: 9.62472541e-07
Iter: 303 loss: 9.5745e-07
Iter: 304 loss: 9.5680025e-07
Iter: 305 loss: 9.56217e-07
Iter: 306 loss: 9.56078225e-07
Iter: 307 loss: 9.55491e-07
Iter: 308 loss: 9.5541111e-07
Iter: 309 loss: 9.54905886e-07
Iter: 310 loss: 9.55876885e-07
Iter: 311 loss: 9.5462633e-07
Iter: 312 loss: 9.54140774e-07
Iter: 313 loss: 9.53231279e-07
Iter: 314 loss: 9.53209508e-07
Iter: 315 loss: 9.5249203e-07
Iter: 316 loss: 9.59742692e-07
Iter: 317 loss: 9.5246105e-07
Iter: 318 loss: 9.51802235e-07
Iter: 319 loss: 9.54679876e-07
Iter: 320 loss: 9.51717766e-07
Iter: 321 loss: 9.50792867e-07
Iter: 322 loss: 9.54210918e-07
Iter: 323 loss: 9.50600338e-07
Iter: 324 loss: 9.50366712e-07
Iter: 325 loss: 9.50369554e-07
Iter: 326 loss: 9.50142294e-07
Iter: 327 loss: 9.49604328e-07
Iter: 328 loss: 9.49311e-07
Iter: 329 loss: 9.4912707e-07
Iter: 330 loss: 9.48425395e-07
Iter: 331 loss: 9.52784035e-07
Iter: 332 loss: 9.48335355e-07
Iter: 333 loss: 9.47864407e-07
Iter: 334 loss: 9.48227409e-07
Iter: 335 loss: 9.47448825e-07
Iter: 336 loss: 9.46748855e-07
Iter: 337 loss: 9.48520949e-07
Iter: 338 loss: 9.46452644e-07
Iter: 339 loss: 9.45795477e-07
Iter: 340 loss: 9.47749e-07
Iter: 341 loss: 9.45692193e-07
Iter: 342 loss: 9.44987733e-07
Iter: 343 loss: 9.48867864e-07
Iter: 344 loss: 9.44894794e-07
Iter: 345 loss: 9.44435897e-07
Iter: 346 loss: 9.4677722e-07
Iter: 347 loss: 9.4424081e-07
Iter: 348 loss: 9.43895657e-07
Iter: 349 loss: 9.44172029e-07
Iter: 350 loss: 9.4365663e-07
Iter: 351 loss: 9.43126224e-07
Iter: 352 loss: 9.43059547e-07
Iter: 353 loss: 9.4274094e-07
Iter: 354 loss: 9.42304609e-07
Iter: 355 loss: 9.42225881e-07
Iter: 356 loss: 9.41628741e-07
Iter: 357 loss: 9.43224791e-07
Iter: 358 loss: 9.41464805e-07
Iter: 359 loss: 9.41203496e-07
Iter: 360 loss: 9.40835662e-07
Iter: 361 loss: 9.40840209e-07
Iter: 362 loss: 9.40112329e-07
Iter: 363 loss: 9.4245172e-07
Iter: 364 loss: 9.39918209e-07
Iter: 365 loss: 9.39458801e-07
Iter: 366 loss: 9.40766881e-07
Iter: 367 loss: 9.39346364e-07
Iter: 368 loss: 9.38906794e-07
Iter: 369 loss: 9.392229e-07
Iter: 370 loss: 9.38566529e-07
Iter: 371 loss: 9.38017934e-07
Iter: 372 loss: 9.42854854e-07
Iter: 373 loss: 9.38005883e-07
Iter: 374 loss: 9.37657148e-07
Iter: 375 loss: 9.3710662e-07
Iter: 376 loss: 9.37177219e-07
Iter: 377 loss: 9.36473157e-07
Iter: 378 loss: 9.43985e-07
Iter: 379 loss: 9.36544211e-07
Iter: 380 loss: 9.36152389e-07
Iter: 381 loss: 9.39302595e-07
Iter: 382 loss: 9.36108165e-07
Iter: 383 loss: 9.3570219e-07
Iter: 384 loss: 9.35170817e-07
Iter: 385 loss: 9.35276375e-07
Iter: 386 loss: 9.34639445e-07
Iter: 387 loss: 9.38924472e-07
Iter: 388 loss: 9.34506886e-07
Iter: 389 loss: 9.34407751e-07
Iter: 390 loss: 9.34370291e-07
Iter: 391 loss: 9.34196521e-07
Iter: 392 loss: 9.33592844e-07
Iter: 393 loss: 9.38867174e-07
Iter: 394 loss: 9.33564479e-07
Iter: 395 loss: 9.33131332e-07
Iter: 396 loss: 9.36467131e-07
Iter: 397 loss: 9.33101944e-07
Iter: 398 loss: 9.3284649e-07
Iter: 399 loss: 9.33204774e-07
Iter: 400 loss: 9.32575347e-07
Iter: 401 loss: 9.32236844e-07
Iter: 402 loss: 9.33336651e-07
Iter: 403 loss: 9.3220217e-07
Iter: 404 loss: 9.317788e-07
Iter: 405 loss: 9.31855084e-07
Iter: 406 loss: 9.31632712e-07
Iter: 407 loss: 9.31122088e-07
Iter: 408 loss: 9.32748719e-07
Iter: 409 loss: 9.30949454e-07
Iter: 410 loss: 9.30526e-07
Iter: 411 loss: 9.31214515e-07
Iter: 412 loss: 9.30337194e-07
Iter: 413 loss: 9.29911209e-07
Iter: 414 loss: 9.30755846e-07
Iter: 415 loss: 9.2978928e-07
Iter: 416 loss: 9.2939348e-07
Iter: 417 loss: 9.35161665e-07
Iter: 418 loss: 9.29378416e-07
Iter: 419 loss: 9.29060434e-07
Iter: 420 loss: 9.29177418e-07
Iter: 421 loss: 9.2882658e-07
Iter: 422 loss: 9.28542e-07
Iter: 423 loss: 9.29529278e-07
Iter: 424 loss: 9.28429927e-07
Iter: 425 loss: 9.28053055e-07
Iter: 426 loss: 9.31132604e-07
Iter: 427 loss: 9.28033e-07
Iter: 428 loss: 9.27825965e-07
Iter: 429 loss: 9.27387077e-07
Iter: 430 loss: 9.27394296e-07
Iter: 431 loss: 9.27087967e-07
Iter: 432 loss: 9.27700569e-07
Iter: 433 loss: 9.27005544e-07
Iter: 434 loss: 9.26539656e-07
Iter: 435 loss: 9.27971598e-07
Iter: 436 loss: 9.26465873e-07
Iter: 437 loss: 9.26058249e-07
Iter: 438 loss: 9.27266683e-07
Iter: 439 loss: 9.26001348e-07
Iter: 440 loss: 9.2566836e-07
Iter: 441 loss: 9.25882148e-07
Iter: 442 loss: 9.25542508e-07
Iter: 443 loss: 9.25108111e-07
Iter: 444 loss: 9.2654318e-07
Iter: 445 loss: 9.25031031e-07
Iter: 446 loss: 9.24769097e-07
Iter: 447 loss: 9.25568315e-07
Iter: 448 loss: 9.24556e-07
Iter: 449 loss: 9.24156268e-07
Iter: 450 loss: 9.24334415e-07
Iter: 451 loss: 9.23930941e-07
Iter: 452 loss: 9.23586413e-07
Iter: 453 loss: 9.2816822e-07
Iter: 454 loss: 9.23637458e-07
Iter: 455 loss: 9.23261666e-07
Iter: 456 loss: 9.24135065e-07
Iter: 457 loss: 9.23170035e-07
Iter: 458 loss: 9.22948857e-07
Iter: 459 loss: 9.24712424e-07
Iter: 460 loss: 9.22866491e-07
Iter: 461 loss: 9.22585969e-07
Iter: 462 loss: 9.22680385e-07
Iter: 463 loss: 9.22444542e-07
Iter: 464 loss: 9.22204947e-07
Iter: 465 loss: 9.21964784e-07
Iter: 466 loss: 9.21954666e-07
Iter: 467 loss: 9.21561536e-07
Iter: 468 loss: 9.22163963e-07
Iter: 469 loss: 9.21253559e-07
Iter: 470 loss: 9.20906928e-07
Iter: 471 loss: 9.25102427e-07
Iter: 472 loss: 9.20881746e-07
Iter: 473 loss: 9.20555863e-07
Iter: 474 loss: 9.19980778e-07
Iter: 475 loss: 9.20046091e-07
Iter: 476 loss: 9.19533363e-07
Iter: 477 loss: 9.24522226e-07
Iter: 478 loss: 9.19459524e-07
Iter: 479 loss: 9.19117497e-07
Iter: 480 loss: 9.19314232e-07
Iter: 481 loss: 9.18757223e-07
Iter: 482 loss: 9.18175147e-07
Iter: 483 loss: 9.21132084e-07
Iter: 484 loss: 9.18204364e-07
Iter: 485 loss: 9.17884e-07
Iter: 486 loss: 9.17772809e-07
Iter: 487 loss: 9.17530258e-07
Iter: 488 loss: 9.172727e-07
Iter: 489 loss: 9.17228704e-07
Iter: 490 loss: 9.17016223e-07
Iter: 491 loss: 9.18124556e-07
Iter: 492 loss: 9.16910551e-07
Iter: 493 loss: 9.16675333e-07
Iter: 494 loss: 9.17050443e-07
Iter: 495 loss: 9.16574436e-07
Iter: 496 loss: 9.16382248e-07
Iter: 497 loss: 9.16000943e-07
Iter: 498 loss: 9.16040165e-07
Iter: 499 loss: 9.15668124e-07
Iter: 500 loss: 9.17635e-07
Iter: 501 loss: 9.15513283e-07
Iter: 502 loss: 9.15287956e-07
Iter: 503 loss: 9.15698536e-07
Iter: 504 loss: 9.15075475e-07
Iter: 505 loss: 9.14779434e-07
Iter: 506 loss: 9.15548185e-07
Iter: 507 loss: 9.14643124e-07
Iter: 508 loss: 9.14104248e-07
Iter: 509 loss: 9.15397436e-07
Iter: 510 loss: 9.1405991e-07
Iter: 511 loss: 9.13676672e-07
Iter: 512 loss: 9.14398925e-07
Iter: 513 loss: 9.1355173e-07
Iter: 514 loss: 9.13154622e-07
Iter: 515 loss: 9.13746192e-07
Iter: 516 loss: 9.12946859e-07
Iter: 517 loss: 9.12614382e-07
Iter: 518 loss: 9.15483e-07
Iter: 519 loss: 9.12480346e-07
Iter: 520 loss: 9.12181065e-07
Iter: 521 loss: 9.11780717e-07
Iter: 522 loss: 9.11835059e-07
Iter: 523 loss: 9.11683458e-07
Iter: 524 loss: 9.11538109e-07
Iter: 525 loss: 9.11305619e-07
Iter: 526 loss: 9.11754285e-07
Iter: 527 loss: 9.11243433e-07
Iter: 528 loss: 9.11064603e-07
Iter: 529 loss: 9.105853e-07
Iter: 530 loss: 9.19671379e-07
Iter: 531 loss: 9.10676363e-07
Iter: 532 loss: 9.10064784e-07
Iter: 533 loss: 9.12188057e-07
Iter: 534 loss: 9.09971163e-07
Iter: 535 loss: 9.09714913e-07
Iter: 536 loss: 9.0959378e-07
Iter: 537 loss: 9.09462074e-07
Iter: 538 loss: 9.08845209e-07
Iter: 539 loss: 9.09598953e-07
Iter: 540 loss: 9.08595268e-07
Iter: 541 loss: 9.08278594e-07
Iter: 542 loss: 9.13337658e-07
Iter: 543 loss: 9.08295192e-07
Iter: 544 loss: 9.07919741e-07
Iter: 545 loss: 9.08647166e-07
Iter: 546 loss: 9.07768e-07
Iter: 547 loss: 9.07444701e-07
Iter: 548 loss: 9.08146092e-07
Iter: 549 loss: 9.07385e-07
Iter: 550 loss: 9.07039748e-07
Iter: 551 loss: 9.07574076e-07
Iter: 552 loss: 9.06888602e-07
Iter: 553 loss: 9.06481944e-07
Iter: 554 loss: 9.06705623e-07
Iter: 555 loss: 9.06216769e-07
Iter: 556 loss: 9.05813124e-07
Iter: 557 loss: 9.09127436e-07
Iter: 558 loss: 9.05782088e-07
Iter: 559 loss: 9.05507306e-07
Iter: 560 loss: 9.05535842e-07
Iter: 561 loss: 9.05368609e-07
Iter: 562 loss: 9.04992476e-07
Iter: 563 loss: 9.04972126e-07
Iter: 564 loss: 9.04631634e-07
Iter: 565 loss: 9.05356842e-07
Iter: 566 loss: 9.04473e-07
Iter: 567 loss: 9.04134708e-07
Iter: 568 loss: 9.04171031e-07
Iter: 569 loss: 9.038489e-07
Iter: 570 loss: 9.03447358e-07
Iter: 571 loss: 9.0709068e-07
Iter: 572 loss: 9.03432e-07
Iter: 573 loss: 9.03110958e-07
Iter: 574 loss: 9.03665239e-07
Iter: 575 loss: 9.02990905e-07
Iter: 576 loss: 9.02598174e-07
Iter: 577 loss: 9.02524903e-07
Iter: 578 loss: 9.02285763e-07
Iter: 579 loss: 9.01917e-07
Iter: 580 loss: 9.01944418e-07
Iter: 581 loss: 9.01705732e-07
Iter: 582 loss: 9.01880298e-07
Iter: 583 loss: 9.015e-07
Iter: 584 loss: 9.01164242e-07
Iter: 585 loss: 9.01863871e-07
Iter: 586 loss: 9.01047542e-07
Iter: 587 loss: 9.00810676e-07
Iter: 588 loss: 9.00875e-07
Iter: 589 loss: 9.00710802e-07
Iter: 590 loss: 9.00380826e-07
Iter: 591 loss: 9.0038975e-07
Iter: 592 loss: 9.0014737e-07
Iter: 593 loss: 9.01584485e-07
Iter: 594 loss: 9.00141345e-07
Iter: 595 loss: 9.00003e-07
Iter: 596 loss: 8.99802e-07
Iter: 597 loss: 9.03773582e-07
Iter: 598 loss: 8.99835072e-07
Iter: 599 loss: 8.99495092e-07
Iter: 600 loss: 8.99783231e-07
Iter: 601 loss: 8.99220424e-07
Iter: 602 loss: 8.98974577e-07
Iter: 603 loss: 9.01080512e-07
Iter: 604 loss: 8.98829171e-07
Iter: 605 loss: 8.98597875e-07
Iter: 606 loss: 8.98934843e-07
Iter: 607 loss: 8.98401083e-07
Iter: 608 loss: 8.98032056e-07
Iter: 609 loss: 8.9900152e-07
Iter: 610 loss: 8.97871189e-07
Iter: 611 loss: 8.97538939e-07
Iter: 612 loss: 8.99449674e-07
Iter: 613 loss: 8.97490509e-07
Iter: 614 loss: 8.97236589e-07
Iter: 615 loss: 8.97116252e-07
Iter: 616 loss: 8.96939582e-07
Iter: 617 loss: 8.96582776e-07
Iter: 618 loss: 8.99695294e-07
Iter: 619 loss: 8.96542247e-07
Iter: 620 loss: 8.96271217e-07
Iter: 621 loss: 8.97234202e-07
Iter: 622 loss: 8.96236884e-07
Iter: 623 loss: 8.95966139e-07
Iter: 624 loss: 8.9548746e-07
Iter: 625 loss: 8.95544645e-07
Iter: 626 loss: 8.95593246e-07
Iter: 627 loss: 8.95342737e-07
Iter: 628 loss: 8.95229675e-07
Iter: 629 loss: 8.95384915e-07
Iter: 630 loss: 8.9502754e-07
Iter: 631 loss: 8.9490868e-07
Iter: 632 loss: 8.9449162e-07
Iter: 633 loss: 8.99444501e-07
Iter: 634 loss: 8.94432844e-07
Iter: 635 loss: 8.94120546e-07
Iter: 636 loss: 8.95996834e-07
Iter: 637 loss: 8.94152265e-07
Iter: 638 loss: 8.93881179e-07
Iter: 639 loss: 8.94868663e-07
Iter: 640 loss: 8.93782044e-07
Iter: 641 loss: 8.93548e-07
Iter: 642 loss: 8.9414749e-07
Iter: 643 loss: 8.93416427e-07
Iter: 644 loss: 8.93182346e-07
Iter: 645 loss: 8.93387323e-07
Iter: 646 loss: 8.93029664e-07
Iter: 647 loss: 8.92732885e-07
Iter: 648 loss: 8.94000323e-07
Iter: 649 loss: 8.92615333e-07
Iter: 650 loss: 8.92329695e-07
Iter: 651 loss: 8.9328023e-07
Iter: 652 loss: 8.92287289e-07
Iter: 653 loss: 8.91927243e-07
Iter: 654 loss: 8.92156379e-07
Iter: 655 loss: 8.91733237e-07
Iter: 656 loss: 8.91358809e-07
Iter: 657 loss: 8.92621586e-07
Iter: 658 loss: 8.91254615e-07
Iter: 659 loss: 8.9088951e-07
Iter: 660 loss: 8.92757612e-07
Iter: 661 loss: 8.90925776e-07
Iter: 662 loss: 8.90589831e-07
Iter: 663 loss: 8.90810384e-07
Iter: 664 loss: 8.90338e-07
Iter: 665 loss: 8.903246e-07
Iter: 666 loss: 8.90180104e-07
Iter: 667 loss: 8.90015656e-07
Iter: 668 loss: 8.89731439e-07
Iter: 669 loss: 8.94943696e-07
Iter: 670 loss: 8.89723e-07
Iter: 671 loss: 8.89360081e-07
Iter: 672 loss: 8.89114631e-07
Iter: 673 loss: 8.89012426e-07
Iter: 674 loss: 8.88788691e-07
Iter: 675 loss: 8.88799548e-07
Iter: 676 loss: 8.88487591e-07
Iter: 677 loss: 8.88676141e-07
Iter: 678 loss: 8.88375951e-07
Iter: 679 loss: 8.88029e-07
Iter: 680 loss: 8.89025785e-07
Iter: 681 loss: 8.879e-07
Iter: 682 loss: 8.87801889e-07
Iter: 683 loss: 8.88460647e-07
Iter: 684 loss: 8.87630506e-07
Iter: 685 loss: 8.87402734e-07
Iter: 686 loss: 8.87610554e-07
Iter: 687 loss: 8.87202077e-07
Iter: 688 loss: 8.87047634e-07
Iter: 689 loss: 8.88370209e-07
Iter: 690 loss: 8.87016938e-07
Iter: 691 loss: 8.86674457e-07
Iter: 692 loss: 8.86660587e-07
Iter: 693 loss: 8.8652871e-07
Iter: 694 loss: 8.86244834e-07
Iter: 695 loss: 8.88057286e-07
Iter: 696 loss: 8.86132568e-07
Iter: 697 loss: 8.85970678e-07
Iter: 698 loss: 8.8751716e-07
Iter: 699 loss: 8.85923441e-07
Iter: 700 loss: 8.85754901e-07
Iter: 701 loss: 8.88101e-07
Iter: 702 loss: 8.85783038e-07
Iter: 703 loss: 8.85691804e-07
Iter: 704 loss: 8.85315842e-07
Iter: 705 loss: 8.88719342e-07
Iter: 706 loss: 8.85285658e-07
Iter: 707 loss: 8.85130476e-07
Iter: 708 loss: 8.8556942e-07
Iter: 709 loss: 8.84884798e-07
Iter: 710 loss: 8.84608312e-07
Iter: 711 loss: 8.85083523e-07
Iter: 712 loss: 8.84416e-07
Iter: 713 loss: 8.84107465e-07
Iter: 714 loss: 8.86103237e-07
Iter: 715 loss: 8.84059489e-07
Iter: 716 loss: 8.83724e-07
Iter: 717 loss: 8.84563462e-07
Iter: 718 loss: 8.83687562e-07
Iter: 719 loss: 8.83286305e-07
Iter: 720 loss: 8.83342864e-07
Iter: 721 loss: 8.83169037e-07
Iter: 722 loss: 8.82733332e-07
Iter: 723 loss: 8.84781e-07
Iter: 724 loss: 8.82651932e-07
Iter: 725 loss: 8.82291829e-07
Iter: 726 loss: 8.83520556e-07
Iter: 727 loss: 8.82162738e-07
Iter: 728 loss: 8.81783649e-07
Iter: 729 loss: 8.82239192e-07
Iter: 730 loss: 8.81648873e-07
Iter: 731 loss: 8.81305141e-07
Iter: 732 loss: 8.82717302e-07
Iter: 733 loss: 8.81178892e-07
Iter: 734 loss: 8.80855623e-07
Iter: 735 loss: 8.84414249e-07
Iter: 736 loss: 8.80882e-07
Iter: 737 loss: 8.80676282e-07
Iter: 738 loss: 8.81621e-07
Iter: 739 loss: 8.80574589e-07
Iter: 740 loss: 8.80484777e-07
Iter: 741 loss: 8.80326866e-07
Iter: 742 loss: 8.80240236e-07
Iter: 743 loss: 8.8013087e-07
Iter: 744 loss: 8.79909521e-07
Iter: 745 loss: 8.79840684e-07
Iter: 746 loss: 8.79582728e-07
Iter: 747 loss: 8.83196265e-07
Iter: 748 loss: 8.79564425e-07
Iter: 749 loss: 8.79387642e-07
Iter: 750 loss: 8.79435106e-07
Iter: 751 loss: 8.79295726e-07
Iter: 752 loss: 8.78964443e-07
Iter: 753 loss: 8.80913149e-07
Iter: 754 loss: 8.78959099e-07
Iter: 755 loss: 8.78762535e-07
Iter: 756 loss: 8.7857245e-07
Iter: 757 loss: 8.78588196e-07
Iter: 758 loss: 8.78125945e-07
Iter: 759 loss: 8.79351e-07
Iter: 760 loss: 8.77920684e-07
Iter: 761 loss: 8.77717412e-07
Iter: 762 loss: 8.80662242e-07
Iter: 763 loss: 8.77699335e-07
Iter: 764 loss: 8.77458319e-07
Iter: 765 loss: 8.7732235e-07
Iter: 766 loss: 8.77217246e-07
Iter: 767 loss: 8.76921547e-07
Iter: 768 loss: 8.78422611e-07
Iter: 769 loss: 8.76836282e-07
Iter: 770 loss: 8.76880904e-07
Iter: 771 loss: 8.76771e-07
Iter: 772 loss: 8.76678826e-07
Iter: 773 loss: 8.76406546e-07
Iter: 774 loss: 8.78628725e-07
Iter: 775 loss: 8.76409e-07
Iter: 776 loss: 8.76169338e-07
Iter: 777 loss: 8.77245043e-07
Iter: 778 loss: 8.7606918e-07
Iter: 779 loss: 8.75971182e-07
Iter: 780 loss: 8.76186732e-07
Iter: 781 loss: 8.75840556e-07
Iter: 782 loss: 8.75583851e-07
Iter: 783 loss: 8.75846297e-07
Iter: 784 loss: 8.75444812e-07
Iter: 785 loss: 8.75181286e-07
Iter: 786 loss: 8.75673322e-07
Iter: 787 loss: 8.75184583e-07
Iter: 788 loss: 8.74857449e-07
Iter: 789 loss: 8.76716967e-07
Iter: 790 loss: 8.74901218e-07
Iter: 791 loss: 8.74620468e-07
Iter: 792 loss: 8.75312423e-07
Iter: 793 loss: 8.74602279e-07
Iter: 794 loss: 8.744193e-07
Iter: 795 loss: 8.74533214e-07
Iter: 796 loss: 8.7426514e-07
Iter: 797 loss: 8.74069656e-07
Iter: 798 loss: 8.74608588e-07
Iter: 799 loss: 8.73926808e-07
Iter: 800 loss: 8.73734905e-07
Iter: 801 loss: 8.7594259e-07
Iter: 802 loss: 8.73701083e-07
Iter: 803 loss: 8.73473255e-07
Iter: 804 loss: 8.73650151e-07
Iter: 805 loss: 8.73405895e-07
Iter: 806 loss: 8.73233375e-07
Iter: 807 loss: 8.73242698e-07
Iter: 808 loss: 8.73128045e-07
Iter: 809 loss: 8.72934436e-07
Iter: 810 loss: 8.77678076e-07
Iter: 811 loss: 8.72975079e-07
Iter: 812 loss: 8.7273e-07
Iter: 813 loss: 8.73135605e-07
Iter: 814 loss: 8.72669091e-07
Iter: 815 loss: 8.72491228e-07
Iter: 816 loss: 8.72435919e-07
Iter: 817 loss: 8.72311546e-07
Iter: 818 loss: 8.72035059e-07
Iter: 819 loss: 8.74144234e-07
Iter: 820 loss: 8.71962811e-07
Iter: 821 loss: 8.71761699e-07
Iter: 822 loss: 8.71456791e-07
Iter: 823 loss: 8.7149931e-07
Iter: 824 loss: 8.71084467e-07
Iter: 825 loss: 8.71084353e-07
Iter: 826 loss: 8.70862777e-07
Iter: 827 loss: 8.710727e-07
Iter: 828 loss: 8.70791837e-07
Iter: 829 loss: 8.70541896e-07
Iter: 830 loss: 8.71124371e-07
Iter: 831 loss: 8.70458166e-07
Iter: 832 loss: 8.70216468e-07
Iter: 833 loss: 8.70750853e-07
Iter: 834 loss: 8.70177189e-07
Iter: 835 loss: 8.69894961e-07
Iter: 836 loss: 8.7088705e-07
Iter: 837 loss: 8.69927305e-07
Iter: 838 loss: 8.69765131e-07
Iter: 839 loss: 8.71699399e-07
Iter: 840 loss: 8.69768769e-07
Iter: 841 loss: 8.696029e-07
Iter: 842 loss: 8.70284339e-07
Iter: 843 loss: 8.69563564e-07
Iter: 844 loss: 8.69491203e-07
Iter: 845 loss: 8.69332951e-07
Iter: 846 loss: 8.7167291e-07
Iter: 847 loss: 8.69288897e-07
Iter: 848 loss: 8.6916009e-07
Iter: 849 loss: 8.70071688e-07
Iter: 850 loss: 8.69069879e-07
Iter: 851 loss: 8.68959e-07
Iter: 852 loss: 8.69192377e-07
Iter: 853 loss: 8.68869222e-07
Iter: 854 loss: 8.68688289e-07
Iter: 855 loss: 8.69074313e-07
Iter: 856 loss: 8.68621441e-07
Iter: 857 loss: 8.68410268e-07
Iter: 858 loss: 8.68823747e-07
Iter: 859 loss: 8.68317102e-07
Iter: 860 loss: 8.6809149e-07
Iter: 861 loss: 8.68701e-07
Iter: 862 loss: 8.68081429e-07
Iter: 863 loss: 8.67815459e-07
Iter: 864 loss: 8.68192501e-07
Iter: 865 loss: 8.67707172e-07
Iter: 866 loss: 8.67391577e-07
Iter: 867 loss: 8.68466202e-07
Iter: 868 loss: 8.67371114e-07
Iter: 869 loss: 8.67148117e-07
Iter: 870 loss: 8.67120605e-07
Iter: 871 loss: 8.66964513e-07
Iter: 872 loss: 8.66810694e-07
Iter: 873 loss: 8.66761354e-07
Iter: 874 loss: 8.66716846e-07
Iter: 875 loss: 8.67898621e-07
Iter: 876 loss: 8.66730772e-07
Iter: 877 loss: 8.66639425e-07
Iter: 878 loss: 8.66445e-07
Iter: 879 loss: 8.69131725e-07
Iter: 880 loss: 8.66377036e-07
Iter: 881 loss: 8.66238e-07
Iter: 882 loss: 8.66286655e-07
Iter: 883 loss: 8.66147843e-07
Iter: 884 loss: 8.65930588e-07
Iter: 885 loss: 8.66659e-07
Iter: 886 loss: 8.65926e-07
Iter: 887 loss: 8.65750394e-07
Iter: 888 loss: 8.65740276e-07
Iter: 889 loss: 8.65546212e-07
Iter: 890 loss: 8.65355332e-07
Iter: 891 loss: 8.67338e-07
Iter: 892 loss: 8.65311847e-07
Iter: 893 loss: 8.65179572e-07
Iter: 894 loss: 8.65138588e-07
Iter: 895 loss: 8.65028539e-07
Iter: 896 loss: 8.6484431e-07
Iter: 897 loss: 8.66177857e-07
Iter: 898 loss: 8.6477371e-07
Iter: 899 loss: 8.64589254e-07
Iter: 900 loss: 8.64977039e-07
Iter: 901 loss: 8.64544063e-07
Iter: 902 loss: 8.64346646e-07
Iter: 903 loss: 8.6492571e-07
Iter: 904 loss: 8.64247284e-07
Iter: 905 loss: 8.64104209e-07
Iter: 906 loss: 8.64306685e-07
Iter: 907 loss: 8.6396625e-07
Iter: 908 loss: 8.63901903e-07
Iter: 909 loss: 8.66209e-07
Iter: 910 loss: 8.63820617e-07
Iter: 911 loss: 8.63610126e-07
Iter: 912 loss: 8.64624e-07
Iter: 913 loss: 8.63585285e-07
Iter: 914 loss: 8.63499963e-07
Iter: 915 loss: 8.63330513e-07
Iter: 916 loss: 8.63257469e-07
Iter: 917 loss: 8.63188689e-07
Iter: 918 loss: 8.63473872e-07
Iter: 919 loss: 8.63057608e-07
Iter: 920 loss: 8.62966203e-07
Iter: 921 loss: 8.62825686e-07
Iter: 922 loss: 8.62790046e-07
Iter: 923 loss: 8.62477407e-07
Iter: 924 loss: 8.65065715e-07
Iter: 925 loss: 8.62473314e-07
Iter: 926 loss: 8.62257821e-07
Iter: 927 loss: 8.62359741e-07
Iter: 928 loss: 8.62189381e-07
Iter: 929 loss: 8.61919148e-07
Iter: 930 loss: 8.62737693e-07
Iter: 931 loss: 8.61760668e-07
Iter: 932 loss: 8.61610488e-07
Iter: 933 loss: 8.62070294e-07
Iter: 934 loss: 8.61487365e-07
Iter: 935 loss: 8.61262606e-07
Iter: 936 loss: 8.62397883e-07
Iter: 937 loss: 8.61276078e-07
Iter: 938 loss: 8.61138119e-07
Iter: 939 loss: 8.62011632e-07
Iter: 940 loss: 8.6111578e-07
Iter: 941 loss: 8.61056776e-07
Iter: 942 loss: 8.60958e-07
Iter: 943 loss: 8.60864702e-07
Iter: 944 loss: 8.60877321e-07
Iter: 945 loss: 8.60795637e-07
Iter: 946 loss: 8.60791e-07
Iter: 947 loss: 8.60664272e-07
Iter: 948 loss: 8.60715772e-07
Iter: 949 loss: 8.60552e-07
Iter: 950 loss: 8.6048567e-07
Iter: 951 loss: 8.60503235e-07
Iter: 952 loss: 8.60277794e-07
Iter: 953 loss: 8.61027274e-07
Iter: 954 loss: 8.60282739e-07
Iter: 955 loss: 8.60093792e-07
Iter: 956 loss: 8.59996533e-07
Iter: 957 loss: 8.59919396e-07
Iter: 958 loss: 8.59625573e-07
Iter: 959 loss: 8.60567127e-07
Iter: 960 loss: 8.59651323e-07
Iter: 961 loss: 8.59467093e-07
Iter: 962 loss: 8.60685816e-07
Iter: 963 loss: 8.59484203e-07
Iter: 964 loss: 8.59347836e-07
Iter: 965 loss: 8.59162697e-07
Iter: 966 loss: 8.59100965e-07
Iter: 967 loss: 8.58916678e-07
Iter: 968 loss: 8.6005889e-07
Iter: 969 loss: 8.58824365e-07
Iter: 970 loss: 8.58722615e-07
Iter: 971 loss: 8.58739497e-07
Iter: 972 loss: 8.5859557e-07
Iter: 973 loss: 8.58256271e-07
Iter: 974 loss: 8.59124555e-07
Iter: 975 loss: 8.58195676e-07
Iter: 976 loss: 8.57991211e-07
Iter: 977 loss: 8.59804913e-07
Iter: 978 loss: 8.57970804e-07
Iter: 979 loss: 8.57900773e-07
Iter: 980 loss: 8.57847681e-07
Iter: 981 loss: 8.57795669e-07
Iter: 982 loss: 8.57714667e-07
Iter: 983 loss: 8.57661348e-07
Iter: 984 loss: 8.57580346e-07
Iter: 985 loss: 8.57546695e-07
Iter: 986 loss: 8.5746484e-07
Iter: 987 loss: 8.57243e-07
Iter: 988 loss: 8.57646739e-07
Iter: 989 loss: 8.57226e-07
Iter: 990 loss: 8.57062787e-07
Iter: 991 loss: 8.57584894e-07
Iter: 992 loss: 8.56993381e-07
Iter: 993 loss: 8.56897316e-07
Iter: 994 loss: 8.573885e-07
Iter: 995 loss: 8.56840757e-07
Iter: 996 loss: 8.56555289e-07
Iter: 997 loss: 8.56623217e-07
Iter: 998 loss: 8.56524082e-07
Iter: 999 loss: 8.56315182e-07
Iter: 1000 loss: 8.57598479e-07
Iter: 1001 loss: 8.56309725e-07
Iter: 1002 loss: 8.56197914e-07
Iter: 1003 loss: 8.55976509e-07
Iter: 1004 loss: 8.55953431e-07
Iter: 1005 loss: 8.55815e-07
Iter: 1006 loss: 8.557966e-07
Iter: 1007 loss: 8.55656936e-07
Iter: 1008 loss: 8.55322583e-07
Iter: 1009 loss: 8.61819217e-07
Iter: 1010 loss: 8.55323094e-07
Iter: 1011 loss: 8.5519855e-07
Iter: 1012 loss: 8.5522646e-07
Iter: 1013 loss: 8.55077076e-07
Iter: 1014 loss: 8.57128384e-07
Iter: 1015 loss: 8.55078838e-07
Iter: 1016 loss: 8.55027281e-07
Iter: 1017 loss: 8.54901373e-07
Iter: 1018 loss: 8.57101497e-07
Iter: 1019 loss: 8.54792745e-07
Iter: 1020 loss: 8.54615337e-07
Iter: 1021 loss: 8.56348436e-07
Iter: 1022 loss: 8.54638415e-07
Iter: 1023 loss: 8.54482892e-07
Iter: 1024 loss: 8.54492953e-07
Iter: 1025 loss: 8.54439918e-07
Iter: 1026 loss: 8.54259383e-07
Iter: 1027 loss: 8.5466047e-07
Iter: 1028 loss: 8.54055315e-07
Iter: 1029 loss: 8.53947654e-07
Iter: 1030 loss: 8.54912287e-07
Iter: 1031 loss: 8.53959818e-07
Iter: 1032 loss: 8.53797815e-07
Iter: 1033 loss: 8.53860058e-07
Iter: 1034 loss: 8.53713857e-07
Iter: 1035 loss: 8.53477104e-07
Iter: 1036 loss: 8.53992105e-07
Iter: 1037 loss: 8.53460051e-07
Iter: 1038 loss: 8.53280369e-07
Iter: 1039 loss: 8.53340339e-07
Iter: 1040 loss: 8.53110691e-07
Iter: 1041 loss: 8.52887922e-07
Iter: 1042 loss: 8.54448331e-07
Iter: 1043 loss: 8.52903895e-07
Iter: 1044 loss: 8.52688515e-07
Iter: 1045 loss: 8.53643542e-07
Iter: 1046 loss: 8.52773326e-07
Iter: 1047 loss: 8.52586425e-07
Iter: 1048 loss: 8.53160202e-07
Iter: 1049 loss: 8.52529183e-07
Iter: 1050 loss: 8.52426865e-07
Iter: 1051 loss: 8.52967787e-07
Iter: 1052 loss: 8.52395374e-07
Iter: 1053 loss: 8.523715e-07
Iter: 1054 loss: 8.52060055e-07
Iter: 1055 loss: 8.52153107e-07
Iter: 1056 loss: 8.51989228e-07
Iter: 1057 loss: 8.52094956e-07
Iter: 1058 loss: 8.5181307e-07
Iter: 1059 loss: 8.51606956e-07
Iter: 1060 loss: 8.5273723e-07
Iter: 1061 loss: 8.51565233e-07
Iter: 1062 loss: 8.51430059e-07
Iter: 1063 loss: 8.51765549e-07
Iter: 1064 loss: 8.513436e-07
Iter: 1065 loss: 8.51104e-07
Iter: 1066 loss: 8.51229743e-07
Iter: 1067 loss: 8.50998788e-07
Iter: 1068 loss: 8.50775677e-07
Iter: 1069 loss: 8.53287e-07
Iter: 1070 loss: 8.50682227e-07
Iter: 1071 loss: 8.50609e-07
Iter: 1072 loss: 8.50408185e-07
Iter: 1073 loss: 8.50335255e-07
Iter: 1074 loss: 8.5006161e-07
Iter: 1075 loss: 8.51822278e-07
Iter: 1076 loss: 8.50088611e-07
Iter: 1077 loss: 8.49876244e-07
Iter: 1078 loss: 8.50148467e-07
Iter: 1079 loss: 8.49800131e-07
Iter: 1080 loss: 8.49631476e-07
Iter: 1081 loss: 8.51312336e-07
Iter: 1082 loss: 8.49595267e-07
Iter: 1083 loss: 8.49511935e-07
Iter: 1084 loss: 8.49600326e-07
Iter: 1085 loss: 8.49535866e-07
Iter: 1086 loss: 8.4952444e-07
Iter: 1087 loss: 8.49505284e-07
Iter: 1088 loss: 8.49469643e-07
Iter: 1089 loss: 8.49500168e-07
Iter: 1090 loss: 8.49567698e-07
Iter: 1091 loss: 8.49565311e-07
Iter: 1092 loss: 8.49539674e-07
Iter: 1093 loss: 8.49536207e-07
Iter: 1094 loss: 8.49530693e-07
Iter: 1095 loss: 8.49585604e-07
Iter: 1096 loss: 8.49577532e-07
Iter: 1097 loss: 8.49568551e-07
Iter: 1098 loss: 8.49581966e-07
Iter: 1099 loss: 8.49611183e-07
Iter: 1100 loss: 8.49600838e-07
Iter: 1101 loss: 8.49605385e-07
Iter: 1102 loss: 8.49577532e-07
Iter: 1103 loss: 8.49594187e-07
Iter: 1104 loss: 8.49600383e-07
Iter: 1105 loss: 8.49593505e-07
Iter: 1106 loss: 8.49593107e-07
Iter: 1107 loss: 8.49597768e-07
Iter: 1108 loss: 8.49595e-07
Iter: 1109 loss: 8.49597768e-07
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi0_phi3/300_100_100_100_1
+ for psi in $PSI
+ for layers in $LAYERS
+ MODEL=experiments.final/output120/f1_psi0_phi0
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi1_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi1_phi0
+ date
Sun Nov  8 19:02:21 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 1 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226e836a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226eae598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226e83bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226ee8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226f89730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226f89bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226f89598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226e33598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226e33bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41f28de7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226d8b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226e52a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226e33730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226db0b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226db0488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226db0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41f28a3378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41f28cf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41f28a3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41f28cfc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41cc098400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41cc0989d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41b00c2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41b00ccf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41b00d4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41cc069598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41cc069488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41b003fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41b003f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41647de0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226db42f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f4226db4e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f416476d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41647a4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41646c2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f41646dbea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.11028521
test_loss: 0.1082786
train_loss: 0.06187752
test_loss: 0.061761513
train_loss: 0.057752676
test_loss: 0.058571458
train_loss: 0.056451734
test_loss: 0.057295613
train_loss: 0.05523266
test_loss: 0.055761173
train_loss: 0.051784135
test_loss: 0.053254303
train_loss: 0.04819014
test_loss: 0.04866418
train_loss: 0.040164977
test_loss: 0.04114605
train_loss: 0.027732164
test_loss: 0.027206436
train_loss: 0.021210106
test_loss: 0.021584587
train_loss: 0.019595537
test_loss: 0.019644223
train_loss: 0.018441698
test_loss: 0.018732822
train_loss: 0.018152779
test_loss: 0.018021066
train_loss: 0.017538965
test_loss: 0.01781931
train_loss: 0.016923135
test_loss: 0.017398413
train_loss: 0.016817085
test_loss: 0.017335752
train_loss: 0.016921392
test_loss: 0.016910454
train_loss: 0.01684593
test_loss: 0.016828606
train_loss: 0.016397264
test_loss: 0.01671931
train_loss: 0.016234836
test_loss: 0.01660992
train_loss: 0.016446773
test_loss: 0.016626896
train_loss: 0.016654287
test_loss: 0.016343368
train_loss: 0.015865052
test_loss: 0.01643747
train_loss: 0.016246755
test_loss: 0.01641905
train_loss: 0.01636132
test_loss: 0.016331824
train_loss: 0.01622396
test_loss: 0.0162381
train_loss: 0.016005322
test_loss: 0.016263967
train_loss: 0.01582886
test_loss: 0.016227787
train_loss: 0.015824128
test_loss: 0.016378365
train_loss: 0.016005367
test_loss: 0.016151521
train_loss: 0.01568985
test_loss: 0.016061982
train_loss: 0.015812103
test_loss: 0.015846204
train_loss: 0.015990278
test_loss: 0.016276378
train_loss: 0.015469177
test_loss: 0.016344596
train_loss: 0.015392126
test_loss: 0.01616893
train_loss: 0.015576078
test_loss: 0.015587521
train_loss: 0.015248284
test_loss: 0.0157712
train_loss: 0.014906136
test_loss: 0.015435396
train_loss: 0.015473138
test_loss: 0.015714394
train_loss: 0.014586778
test_loss: 0.01524536
train_loss: 0.01439796
test_loss: 0.015174715
train_loss: 0.014551792
test_loss: 0.015175142
train_loss: 0.014465638
test_loss: 0.014345154
train_loss: 0.013434288
test_loss: 0.0141805615
train_loss: 0.013419459
test_loss: 0.013158213
train_loss: 0.0127529465
test_loss: 0.012962358
train_loss: 0.01124386
test_loss: 0.011622625
train_loss: 0.009235455
test_loss: 0.009444525
train_loss: 0.0097071845
test_loss: 0.008400945
train_loss: 0.0077886884
test_loss: 0.0075781187
train_loss: 0.007917804
test_loss: 0.0071113487
train_loss: 0.005356343
test_loss: 0.005607123
train_loss: 0.0060356846
test_loss: 0.005665381
train_loss: 0.0071795685
test_loss: 0.006343811
train_loss: 0.005563963
test_loss: 0.005384428
train_loss: 0.005231219
test_loss: 0.005470335
train_loss: 0.005285507
test_loss: 0.005342259
train_loss: 0.0057456745
test_loss: 0.005430339
train_loss: 0.0051061343
test_loss: 0.004827876
train_loss: 0.0042728987
test_loss: 0.004700614
train_loss: 0.0063746744
test_loss: 0.005090052
train_loss: 0.005063153
test_loss: 0.0045539844
train_loss: 0.004319719
test_loss: 0.0043490366
train_loss: 0.004356857
test_loss: 0.004517266
train_loss: 0.005026263
test_loss: 0.0045228307
train_loss: 0.0040319054
test_loss: 0.0045104376
train_loss: 0.004369053
test_loss: 0.004079274
train_loss: 0.003927306
test_loss: 0.0041531245
train_loss: 0.00409421
test_loss: 0.004024451
train_loss: 0.0044996818
test_loss: 0.004464207
train_loss: 0.0040657884
test_loss: 0.0041506556
train_loss: 0.0044431817
test_loss: 0.0047035944
train_loss: 0.0037591422
test_loss: 0.004169399
train_loss: 0.004718395
test_loss: 0.004069347
train_loss: 0.0039463257
test_loss: 0.0039396
train_loss: 0.0039685774
test_loss: 0.0045174267
train_loss: 0.004340656
test_loss: 0.0043193228
train_loss: 0.0037938145
test_loss: 0.0039785164
train_loss: 0.004589055
test_loss: 0.0043749777
train_loss: 0.004361109
test_loss: 0.004555103
train_loss: 0.003903894
test_loss: 0.004327867
train_loss: 0.0043494324
test_loss: 0.0042648534
train_loss: 0.0039397157
test_loss: 0.0039238096
train_loss: 0.0037596875
test_loss: 0.0038319114
train_loss: 0.0037245813
test_loss: 0.003716284
train_loss: 0.0042271814
test_loss: 0.0038532193
train_loss: 0.003560163
test_loss: 0.0038702488
train_loss: 0.0039165276
test_loss: 0.004167043
train_loss: 0.00378629
test_loss: 0.0039111706
train_loss: 0.0038652313
test_loss: 0.0039726016
train_loss: 0.00412998
test_loss: 0.0035759548
train_loss: 0.0041142018
test_loss: 0.0042572776
train_loss: 0.004142333
test_loss: 0.0037086357
train_loss: 0.00391045
test_loss: 0.0036340952
train_loss: 0.004033303
test_loss: 0.003735444
train_loss: 0.0044489894
test_loss: 0.004555563
train_loss: 0.0037194483
test_loss: 0.0035110225
train_loss: 0.0035849828
test_loss: 0.003639226
train_loss: 0.0037863806
test_loss: 0.0045142355
train_loss: 0.004392305
test_loss: 0.0039660763
train_loss: 0.0037714012
test_loss: 0.0035922031
train_loss: 0.003526032
test_loss: 0.0035354113
train_loss: 0.0032015755
test_loss: 0.003472664
train_loss: 0.0038922613
test_loss: 0.00391335
train_loss: 0.0036303985
test_loss: 0.003900398
train_loss: 0.0032189444
test_loss: 0.0037354059
train_loss: 0.004736261
test_loss: 0.0038228026
train_loss: 0.004032459
test_loss: 0.0040253866
train_loss: 0.0033756238
test_loss: 0.0039007072
train_loss: 0.0033074787
test_loss: 0.003258223
train_loss: 0.0034688786
test_loss: 0.0034440798
train_loss: 0.003434123
test_loss: 0.0037733738
train_loss: 0.003319035
test_loss: 0.0034322385
train_loss: 0.0035624807
test_loss: 0.004274419
train_loss: 0.0038913395
test_loss: 0.0033796048
train_loss: 0.0036793347
test_loss: 0.0041385954
train_loss: 0.0035803025
test_loss: 0.0036819614
train_loss: 0.003505691
test_loss: 0.003204032
train_loss: 0.0033811433
test_loss: 0.0035775092
train_loss: 0.0041189566
test_loss: 0.0036873247
train_loss: 0.0032711271
test_loss: 0.0031698025
train_loss: 0.0035696896
test_loss: 0.0037737142
train_loss: 0.0036081597
test_loss: 0.003140253
train_loss: 0.003495999
test_loss: 0.0031809143
train_loss: 0.0034578566
test_loss: 0.0035559947
train_loss: 0.0038096302
test_loss: 0.0035775024
train_loss: 0.003478387
test_loss: 0.003450153
train_loss: 0.0038662143
test_loss: 0.003757952
train_loss: 0.0038393284
test_loss: 0.0036486825
train_loss: 0.0035991096
test_loss: 0.0033317725
train_loss: 0.0032688037
test_loss: 0.003455168
train_loss: 0.0034297418
test_loss: 0.0035755495
train_loss: 0.003866368
test_loss: 0.0033715826
train_loss: 0.003310388
test_loss: 0.003181055
train_loss: 0.0034225013
test_loss: 0.0035263593
train_loss: 0.0033832262
test_loss: 0.0030912636
train_loss: 0.0032886968
test_loss: 0.0037196644
train_loss: 0.0038281926
test_loss: 0.0037676943
train_loss: 0.0031557416
test_loss: 0.0029725677
train_loss: 0.0032391823
test_loss: 0.0030242745
train_loss: 0.0033314838
test_loss: 0.0032588248
train_loss: 0.0034913663
test_loss: 0.003381869
train_loss: 0.0031046863
test_loss: 0.003347984
train_loss: 0.0034976797
test_loss: 0.0032989215
train_loss: 0.003225935
test_loss: 0.0030785606
train_loss: 0.0031731068
test_loss: 0.0032601792
train_loss: 0.003342745
test_loss: 0.003749387
train_loss: 0.0032250206
test_loss: 0.0034294322
train_loss: 0.0032837926
test_loss: 0.003579639
train_loss: 0.0034628282
test_loss: 0.0032148664
train_loss: 0.0034145415
test_loss: 0.00323291
train_loss: 0.0030464362
test_loss: 0.0034766234
train_loss: 0.0029451651
test_loss: 0.0031203178
train_loss: 0.0034743939
test_loss: 0.0037493135
train_loss: 0.0033432564
test_loss: 0.0030527243
train_loss: 0.0032156867
test_loss: 0.0031403522
train_loss: 0.00274413
test_loss: 0.003351002
train_loss: 0.00307868
test_loss: 0.003335217
train_loss: 0.0031553283
test_loss: 0.0031422738
train_loss: 0.003015699
test_loss: 0.003141253
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi1_phi0/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 1 --phi 0 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi1_phi0/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8a36840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b893d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b893d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b898d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b898d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8912048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8912268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b899a158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8912840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8899ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8857400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8912730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b87fbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b87fb840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b881f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b877cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b881fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8797730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8776840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8797598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b87321e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b86e8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8699598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8699510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b86b7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b8673598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28b86738c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f287b97f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f287b92d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f287b94fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f287b92d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f287b9147b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f287b914e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f287b880f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f287b8b07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f28547b0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 1.52795801e-05
Iter: 2 loss: 2.30368496e-05
Iter: 3 loss: 1.0942832e-05
Iter: 4 loss: 9.63732327e-06
Iter: 5 loss: 8.03235889e-06
Iter: 6 loss: 7.88864054e-06
Iter: 7 loss: 7.54289249e-06
Iter: 8 loss: 7.28157966e-06
Iter: 9 loss: 6.85164e-06
Iter: 10 loss: 6.35157e-06
Iter: 11 loss: 6.29362057e-06
Iter: 12 loss: 6.18893318e-06
Iter: 13 loss: 6.09424751e-06
Iter: 14 loss: 5.95822075e-06
Iter: 15 loss: 5.71765759e-06
Iter: 16 loss: 5.71772171e-06
Iter: 17 loss: 5.46962883e-06
Iter: 18 loss: 5.98471161e-06
Iter: 19 loss: 5.37063806e-06
Iter: 20 loss: 5.22498613e-06
Iter: 21 loss: 6.10411098e-06
Iter: 22 loss: 5.20711956e-06
Iter: 23 loss: 5.08530047e-06
Iter: 24 loss: 5.17786884e-06
Iter: 25 loss: 5.01105615e-06
Iter: 26 loss: 4.91606625e-06
Iter: 27 loss: 4.71973362e-06
Iter: 28 loss: 8.15324529e-06
Iter: 29 loss: 4.71567e-06
Iter: 30 loss: 4.5201873e-06
Iter: 31 loss: 5.89117e-06
Iter: 32 loss: 4.50232938e-06
Iter: 33 loss: 4.33312107e-06
Iter: 34 loss: 6.7987844e-06
Iter: 35 loss: 4.33292871e-06
Iter: 36 loss: 4.25317921e-06
Iter: 37 loss: 4.05876926e-06
Iter: 38 loss: 6.06008416e-06
Iter: 39 loss: 4.03583363e-06
Iter: 40 loss: 3.84110263e-06
Iter: 41 loss: 4.2876668e-06
Iter: 42 loss: 3.76825778e-06
Iter: 43 loss: 3.55143902e-06
Iter: 44 loss: 3.89939032e-06
Iter: 45 loss: 3.45099443e-06
Iter: 46 loss: 3.31775573e-06
Iter: 47 loss: 3.31379215e-06
Iter: 48 loss: 3.2496514e-06
Iter: 49 loss: 3.24287225e-06
Iter: 50 loss: 3.21508901e-06
Iter: 51 loss: 3.15582042e-06
Iter: 52 loss: 4.10137272e-06
Iter: 53 loss: 3.15395687e-06
Iter: 54 loss: 3.08827794e-06
Iter: 55 loss: 4.11359815e-06
Iter: 56 loss: 3.08830658e-06
Iter: 57 loss: 3.0608735e-06
Iter: 58 loss: 3.00878537e-06
Iter: 59 loss: 4.12630743e-06
Iter: 60 loss: 3.00855709e-06
Iter: 61 loss: 2.97079805e-06
Iter: 62 loss: 2.96767917e-06
Iter: 63 loss: 2.94274059e-06
Iter: 64 loss: 2.89238233e-06
Iter: 65 loss: 3.81796781e-06
Iter: 66 loss: 2.89165246e-06
Iter: 67 loss: 2.85641863e-06
Iter: 68 loss: 3.09632355e-06
Iter: 69 loss: 2.85303304e-06
Iter: 70 loss: 2.81717053e-06
Iter: 71 loss: 3.09076768e-06
Iter: 72 loss: 2.81450684e-06
Iter: 73 loss: 2.79297683e-06
Iter: 74 loss: 2.74481772e-06
Iter: 75 loss: 3.41355644e-06
Iter: 76 loss: 2.742177e-06
Iter: 77 loss: 2.74303e-06
Iter: 78 loss: 2.71878662e-06
Iter: 79 loss: 2.70063e-06
Iter: 80 loss: 2.65697054e-06
Iter: 81 loss: 3.12709608e-06
Iter: 82 loss: 2.65221661e-06
Iter: 83 loss: 2.61084051e-06
Iter: 84 loss: 2.75078355e-06
Iter: 85 loss: 2.59962962e-06
Iter: 86 loss: 2.56398403e-06
Iter: 87 loss: 2.56316207e-06
Iter: 88 loss: 2.54708129e-06
Iter: 89 loss: 2.51952724e-06
Iter: 90 loss: 2.51949746e-06
Iter: 91 loss: 2.48073457e-06
Iter: 92 loss: 2.81357529e-06
Iter: 93 loss: 2.47859543e-06
Iter: 94 loss: 2.46148602e-06
Iter: 95 loss: 2.43030081e-06
Iter: 96 loss: 3.16884916e-06
Iter: 97 loss: 2.4302517e-06
Iter: 98 loss: 2.42372016e-06
Iter: 99 loss: 2.41356065e-06
Iter: 100 loss: 2.40476447e-06
Iter: 101 loss: 2.38411258e-06
Iter: 102 loss: 2.6253565e-06
Iter: 103 loss: 2.38232224e-06
Iter: 104 loss: 2.36786173e-06
Iter: 105 loss: 2.36759706e-06
Iter: 106 loss: 2.35208881e-06
Iter: 107 loss: 2.34602953e-06
Iter: 108 loss: 2.33760807e-06
Iter: 109 loss: 2.32313664e-06
Iter: 110 loss: 2.31592139e-06
Iter: 111 loss: 2.30908313e-06
Iter: 112 loss: 2.29599959e-06
Iter: 113 loss: 2.29596367e-06
Iter: 114 loss: 2.28167301e-06
Iter: 115 loss: 2.30555179e-06
Iter: 116 loss: 2.27510986e-06
Iter: 117 loss: 2.26434713e-06
Iter: 118 loss: 2.24314363e-06
Iter: 119 loss: 2.66315919e-06
Iter: 120 loss: 2.24292558e-06
Iter: 121 loss: 2.22940116e-06
Iter: 122 loss: 2.22886911e-06
Iter: 123 loss: 2.21341065e-06
Iter: 124 loss: 2.26658858e-06
Iter: 125 loss: 2.2093684e-06
Iter: 126 loss: 2.20281572e-06
Iter: 127 loss: 2.19126446e-06
Iter: 128 loss: 2.19125855e-06
Iter: 129 loss: 2.17541037e-06
Iter: 130 loss: 2.37452718e-06
Iter: 131 loss: 2.17528577e-06
Iter: 132 loss: 2.16714648e-06
Iter: 133 loss: 2.14776605e-06
Iter: 134 loss: 2.36430628e-06
Iter: 135 loss: 2.14590136e-06
Iter: 136 loss: 2.14357533e-06
Iter: 137 loss: 2.1364076e-06
Iter: 138 loss: 2.12909231e-06
Iter: 139 loss: 2.11202519e-06
Iter: 140 loss: 2.32021625e-06
Iter: 141 loss: 2.11064253e-06
Iter: 142 loss: 2.09553423e-06
Iter: 143 loss: 2.12993882e-06
Iter: 144 loss: 2.08981464e-06
Iter: 145 loss: 2.0776597e-06
Iter: 146 loss: 2.17994057e-06
Iter: 147 loss: 2.07691619e-06
Iter: 148 loss: 2.06394452e-06
Iter: 149 loss: 2.13832436e-06
Iter: 150 loss: 2.06217396e-06
Iter: 151 loss: 2.05579045e-06
Iter: 152 loss: 2.04635785e-06
Iter: 153 loss: 2.04614253e-06
Iter: 154 loss: 2.03652303e-06
Iter: 155 loss: 2.04656976e-06
Iter: 156 loss: 2.03114314e-06
Iter: 157 loss: 2.02743331e-06
Iter: 158 loss: 2.02514866e-06
Iter: 159 loss: 2.01897842e-06
Iter: 160 loss: 2.0127427e-06
Iter: 161 loss: 2.01147623e-06
Iter: 162 loss: 2.00471572e-06
Iter: 163 loss: 2.03482205e-06
Iter: 164 loss: 2.00326599e-06
Iter: 165 loss: 1.99352735e-06
Iter: 166 loss: 2.01528019e-06
Iter: 167 loss: 1.98984162e-06
Iter: 168 loss: 1.9851509e-06
Iter: 169 loss: 1.97795248e-06
Iter: 170 loss: 1.97776239e-06
Iter: 171 loss: 1.97323652e-06
Iter: 172 loss: 1.97193845e-06
Iter: 173 loss: 1.96829251e-06
Iter: 174 loss: 1.95809298e-06
Iter: 175 loss: 2.01698776e-06
Iter: 176 loss: 1.95518965e-06
Iter: 177 loss: 1.95136931e-06
Iter: 178 loss: 1.94965423e-06
Iter: 179 loss: 1.94402446e-06
Iter: 180 loss: 1.94067798e-06
Iter: 181 loss: 1.93832307e-06
Iter: 182 loss: 1.93281e-06
Iter: 183 loss: 1.9355241e-06
Iter: 184 loss: 1.92913944e-06
Iter: 185 loss: 1.92338143e-06
Iter: 186 loss: 1.92322932e-06
Iter: 187 loss: 1.91965592e-06
Iter: 188 loss: 1.911656e-06
Iter: 189 loss: 2.02394949e-06
Iter: 190 loss: 1.91123218e-06
Iter: 191 loss: 1.90419883e-06
Iter: 192 loss: 1.93994561e-06
Iter: 193 loss: 1.90302637e-06
Iter: 194 loss: 1.89912839e-06
Iter: 195 loss: 1.89873754e-06
Iter: 196 loss: 1.89636307e-06
Iter: 197 loss: 1.89053549e-06
Iter: 198 loss: 1.94621066e-06
Iter: 199 loss: 1.88974775e-06
Iter: 200 loss: 1.88983881e-06
Iter: 201 loss: 1.88717445e-06
Iter: 202 loss: 1.88453168e-06
Iter: 203 loss: 1.87765204e-06
Iter: 204 loss: 1.92976177e-06
Iter: 205 loss: 1.87624789e-06
Iter: 206 loss: 1.87131843e-06
Iter: 207 loss: 1.92250172e-06
Iter: 208 loss: 1.87117507e-06
Iter: 209 loss: 1.86634259e-06
Iter: 210 loss: 1.89559546e-06
Iter: 211 loss: 1.86580166e-06
Iter: 212 loss: 1.86334535e-06
Iter: 213 loss: 1.85808312e-06
Iter: 214 loss: 1.94288623e-06
Iter: 215 loss: 1.85787144e-06
Iter: 216 loss: 1.85480906e-06
Iter: 217 loss: 1.854169e-06
Iter: 218 loss: 1.85131398e-06
Iter: 219 loss: 1.84480893e-06
Iter: 220 loss: 1.92561629e-06
Iter: 221 loss: 1.84427472e-06
Iter: 222 loss: 1.84160024e-06
Iter: 223 loss: 1.84070359e-06
Iter: 224 loss: 1.83729082e-06
Iter: 225 loss: 1.83108739e-06
Iter: 226 loss: 1.97815484e-06
Iter: 227 loss: 1.83108875e-06
Iter: 228 loss: 1.82556141e-06
Iter: 229 loss: 1.83710267e-06
Iter: 230 loss: 1.82340841e-06
Iter: 231 loss: 1.82004771e-06
Iter: 232 loss: 1.82000645e-06
Iter: 233 loss: 1.81588848e-06
Iter: 234 loss: 1.81220435e-06
Iter: 235 loss: 1.81119503e-06
Iter: 236 loss: 1.80958398e-06
Iter: 237 loss: 1.80921825e-06
Iter: 238 loss: 1.80730785e-06
Iter: 239 loss: 1.80435245e-06
Iter: 240 loss: 1.80430629e-06
Iter: 241 loss: 1.8016184e-06
Iter: 242 loss: 1.80189181e-06
Iter: 243 loss: 1.79955225e-06
Iter: 244 loss: 1.79616188e-06
Iter: 245 loss: 1.79608742e-06
Iter: 246 loss: 1.79467725e-06
Iter: 247 loss: 1.790326e-06
Iter: 248 loss: 1.80631548e-06
Iter: 249 loss: 1.78854714e-06
Iter: 250 loss: 1.78730784e-06
Iter: 251 loss: 1.78544815e-06
Iter: 252 loss: 1.78317055e-06
Iter: 253 loss: 1.78060839e-06
Iter: 254 loss: 1.7802945e-06
Iter: 255 loss: 1.77763263e-06
Iter: 256 loss: 1.79044014e-06
Iter: 257 loss: 1.77719642e-06
Iter: 258 loss: 1.77342611e-06
Iter: 259 loss: 1.77629613e-06
Iter: 260 loss: 1.77110303e-06
Iter: 261 loss: 1.76791536e-06
Iter: 262 loss: 1.76349045e-06
Iter: 263 loss: 1.76330127e-06
Iter: 264 loss: 1.76120807e-06
Iter: 265 loss: 1.76066123e-06
Iter: 266 loss: 1.7578526e-06
Iter: 267 loss: 1.76293236e-06
Iter: 268 loss: 1.7567088e-06
Iter: 269 loss: 1.75455557e-06
Iter: 270 loss: 1.77765935e-06
Iter: 271 loss: 1.75446587e-06
Iter: 272 loss: 1.75312277e-06
Iter: 273 loss: 1.7499126e-06
Iter: 274 loss: 1.78358141e-06
Iter: 275 loss: 1.74950719e-06
Iter: 276 loss: 1.74841807e-06
Iter: 277 loss: 1.7478435e-06
Iter: 278 loss: 1.7461374e-06
Iter: 279 loss: 1.74257252e-06
Iter: 280 loss: 1.79910921e-06
Iter: 281 loss: 1.74246566e-06
Iter: 282 loss: 1.73912554e-06
Iter: 283 loss: 1.74226864e-06
Iter: 284 loss: 1.73718604e-06
Iter: 285 loss: 1.73514877e-06
Iter: 286 loss: 1.73506101e-06
Iter: 287 loss: 1.73256387e-06
Iter: 288 loss: 1.73008402e-06
Iter: 289 loss: 1.72949478e-06
Iter: 290 loss: 1.72618752e-06
Iter: 291 loss: 1.72341788e-06
Iter: 292 loss: 1.7224794e-06
Iter: 293 loss: 1.7229828e-06
Iter: 294 loss: 1.72037016e-06
Iter: 295 loss: 1.71880367e-06
Iter: 296 loss: 1.71545139e-06
Iter: 297 loss: 1.76446122e-06
Iter: 298 loss: 1.71528154e-06
Iter: 299 loss: 1.71205829e-06
Iter: 300 loss: 1.71839349e-06
Iter: 301 loss: 1.71075555e-06
Iter: 302 loss: 1.70947737e-06
Iter: 303 loss: 1.70907913e-06
Iter: 304 loss: 1.70716112e-06
Iter: 305 loss: 1.70483054e-06
Iter: 306 loss: 1.70455974e-06
Iter: 307 loss: 1.70251747e-06
Iter: 308 loss: 1.72890554e-06
Iter: 309 loss: 1.70252895e-06
Iter: 310 loss: 1.70061969e-06
Iter: 311 loss: 1.6996e-06
Iter: 312 loss: 1.69875636e-06
Iter: 313 loss: 1.69700775e-06
Iter: 314 loss: 1.69510554e-06
Iter: 315 loss: 1.69488908e-06
Iter: 316 loss: 1.69418377e-06
Iter: 317 loss: 1.69322175e-06
Iter: 318 loss: 1.69222062e-06
Iter: 319 loss: 1.68959957e-06
Iter: 320 loss: 1.70740248e-06
Iter: 321 loss: 1.6889262e-06
Iter: 322 loss: 1.68629413e-06
Iter: 323 loss: 1.69897794e-06
Iter: 324 loss: 1.68579299e-06
Iter: 325 loss: 1.68471479e-06
Iter: 326 loss: 1.68446934e-06
Iter: 327 loss: 1.68365784e-06
Iter: 328 loss: 1.68185818e-06
Iter: 329 loss: 1.70534076e-06
Iter: 330 loss: 1.68170493e-06
Iter: 331 loss: 1.68044971e-06
Iter: 332 loss: 1.68037479e-06
Iter: 333 loss: 1.67915607e-06
Iter: 334 loss: 1.67635835e-06
Iter: 335 loss: 1.71343152e-06
Iter: 336 loss: 1.67613507e-06
Iter: 337 loss: 1.6731376e-06
Iter: 338 loss: 1.67714313e-06
Iter: 339 loss: 1.67158169e-06
Iter: 340 loss: 1.66944073e-06
Iter: 341 loss: 1.68671158e-06
Iter: 342 loss: 1.66926964e-06
Iter: 343 loss: 1.66716291e-06
Iter: 344 loss: 1.69119232e-06
Iter: 345 loss: 1.66710765e-06
Iter: 346 loss: 1.66598784e-06
Iter: 347 loss: 1.66514735e-06
Iter: 348 loss: 1.66480322e-06
Iter: 349 loss: 1.6626077e-06
Iter: 350 loss: 1.67321605e-06
Iter: 351 loss: 1.66222196e-06
Iter: 352 loss: 1.66093048e-06
Iter: 353 loss: 1.65876384e-06
Iter: 354 loss: 1.71154102e-06
Iter: 355 loss: 1.65876577e-06
Iter: 356 loss: 1.65951781e-06
Iter: 357 loss: 1.6579254e-06
Iter: 358 loss: 1.65735537e-06
Iter: 359 loss: 1.65571328e-06
Iter: 360 loss: 1.66484028e-06
Iter: 361 loss: 1.65520589e-06
Iter: 362 loss: 1.65313463e-06
Iter: 363 loss: 1.65578695e-06
Iter: 364 loss: 1.65208405e-06
Iter: 365 loss: 1.65086192e-06
Iter: 366 loss: 1.65075971e-06
Iter: 367 loss: 1.64956725e-06
Iter: 368 loss: 1.6485908e-06
Iter: 369 loss: 1.648178e-06
Iter: 370 loss: 1.64699634e-06
Iter: 371 loss: 1.65078143e-06
Iter: 372 loss: 1.64671951e-06
Iter: 373 loss: 1.64517428e-06
Iter: 374 loss: 1.65292704e-06
Iter: 375 loss: 1.64492747e-06
Iter: 376 loss: 1.64391156e-06
Iter: 377 loss: 1.64189908e-06
Iter: 378 loss: 1.68261045e-06
Iter: 379 loss: 1.64183245e-06
Iter: 380 loss: 1.64002586e-06
Iter: 381 loss: 1.65801805e-06
Iter: 382 loss: 1.63995946e-06
Iter: 383 loss: 1.63803622e-06
Iter: 384 loss: 1.65122754e-06
Iter: 385 loss: 1.63788081e-06
Iter: 386 loss: 1.63713912e-06
Iter: 387 loss: 1.63756158e-06
Iter: 388 loss: 1.63666391e-06
Iter: 389 loss: 1.63546167e-06
Iter: 390 loss: 1.63728646e-06
Iter: 391 loss: 1.6348497e-06
Iter: 392 loss: 1.63385323e-06
Iter: 393 loss: 1.63199331e-06
Iter: 394 loss: 1.67307428e-06
Iter: 395 loss: 1.63197376e-06
Iter: 396 loss: 1.63183313e-06
Iter: 397 loss: 1.63102095e-06
Iter: 398 loss: 1.63017069e-06
Iter: 399 loss: 1.62845652e-06
Iter: 400 loss: 1.65625079e-06
Iter: 401 loss: 1.62837205e-06
Iter: 402 loss: 1.62728566e-06
Iter: 403 loss: 1.63541858e-06
Iter: 404 loss: 1.62721653e-06
Iter: 405 loss: 1.62597439e-06
Iter: 406 loss: 1.63061497e-06
Iter: 407 loss: 1.62567926e-06
Iter: 408 loss: 1.62473452e-06
Iter: 409 loss: 1.62419065e-06
Iter: 410 loss: 1.62384208e-06
Iter: 411 loss: 1.62221397e-06
Iter: 412 loss: 1.63450045e-06
Iter: 413 loss: 1.62208119e-06
Iter: 414 loss: 1.62113145e-06
Iter: 415 loss: 1.61947321e-06
Iter: 416 loss: 1.61947492e-06
Iter: 417 loss: 1.6190462e-06
Iter: 418 loss: 1.61867206e-06
Iter: 419 loss: 1.61785192e-06
Iter: 420 loss: 1.61732191e-06
Iter: 421 loss: 1.61696039e-06
Iter: 422 loss: 1.61606886e-06
Iter: 423 loss: 1.61580783e-06
Iter: 424 loss: 1.61524895e-06
Iter: 425 loss: 1.61354194e-06
Iter: 426 loss: 1.62662946e-06
Iter: 427 loss: 1.61342268e-06
Iter: 428 loss: 1.61274897e-06
Iter: 429 loss: 1.61207788e-06
Iter: 430 loss: 1.61197113e-06
Iter: 431 loss: 1.61104424e-06
Iter: 432 loss: 1.61107391e-06
Iter: 433 loss: 1.61057824e-06
Iter: 434 loss: 1.60952516e-06
Iter: 435 loss: 1.62541392e-06
Iter: 436 loss: 1.60950344e-06
Iter: 437 loss: 1.60886407e-06
Iter: 438 loss: 1.60882121e-06
Iter: 439 loss: 1.60814147e-06
Iter: 440 loss: 1.60743116e-06
Iter: 441 loss: 1.60733e-06
Iter: 442 loss: 1.60641275e-06
Iter: 443 loss: 1.61361959e-06
Iter: 444 loss: 1.60633e-06
Iter: 445 loss: 1.60521063e-06
Iter: 446 loss: 1.60641093e-06
Iter: 447 loss: 1.60456921e-06
Iter: 448 loss: 1.60373668e-06
Iter: 449 loss: 1.6041372e-06
Iter: 450 loss: 1.60321565e-06
Iter: 451 loss: 1.60229786e-06
Iter: 452 loss: 1.6023273e-06
Iter: 453 loss: 1.60183583e-06
Iter: 454 loss: 1.60050445e-06
Iter: 455 loss: 1.61015566e-06
Iter: 456 loss: 1.60025763e-06
Iter: 457 loss: 1.60025365e-06
Iter: 458 loss: 1.59960325e-06
Iter: 459 loss: 1.59888987e-06
Iter: 460 loss: 1.59752835e-06
Iter: 461 loss: 1.62647939e-06
Iter: 462 loss: 1.5974822e-06
Iter: 463 loss: 1.59621959e-06
Iter: 464 loss: 1.60080776e-06
Iter: 465 loss: 1.59592025e-06
Iter: 466 loss: 1.5951673e-06
Iter: 467 loss: 1.59508488e-06
Iter: 468 loss: 1.59468141e-06
Iter: 469 loss: 1.59367687e-06
Iter: 470 loss: 1.60192781e-06
Iter: 471 loss: 1.59341653e-06
Iter: 472 loss: 1.59254944e-06
Iter: 473 loss: 1.60158174e-06
Iter: 474 loss: 1.59245883e-06
Iter: 475 loss: 1.591456e-06
Iter: 476 loss: 1.59357421e-06
Iter: 477 loss: 1.59102854e-06
Iter: 478 loss: 1.59039928e-06
Iter: 479 loss: 1.59115757e-06
Iter: 480 loss: 1.59007061e-06
Iter: 481 loss: 1.58912371e-06
Iter: 482 loss: 1.59261413e-06
Iter: 483 loss: 1.58889679e-06
Iter: 484 loss: 1.58828595e-06
Iter: 485 loss: 1.58722401e-06
Iter: 486 loss: 1.61272237e-06
Iter: 487 loss: 1.58722344e-06
Iter: 488 loss: 1.58750936e-06
Iter: 489 loss: 1.58672503e-06
Iter: 490 loss: 1.58630496e-06
Iter: 491 loss: 1.58514069e-06
Iter: 492 loss: 1.59335104e-06
Iter: 493 loss: 1.58493094e-06
Iter: 494 loss: 1.58442629e-06
Iter: 495 loss: 1.58420573e-06
Iter: 496 loss: 1.5836888e-06
Iter: 497 loss: 1.58293801e-06
Iter: 498 loss: 1.58292119e-06
Iter: 499 loss: 1.5824703e-06
Iter: 500 loss: 1.58245598e-06
Iter: 501 loss: 1.58195007e-06
Iter: 502 loss: 1.58102182e-06
Iter: 503 loss: 1.60020534e-06
Iter: 504 loss: 1.58103126e-06
Iter: 505 loss: 1.5799576e-06
Iter: 506 loss: 1.57995953e-06
Iter: 507 loss: 1.57917725e-06
Iter: 508 loss: 1.57903742e-06
Iter: 509 loss: 1.57850866e-06
Iter: 510 loss: 1.57795967e-06
Iter: 511 loss: 1.57766544e-06
Iter: 512 loss: 1.57739191e-06
Iter: 513 loss: 1.57673162e-06
Iter: 514 loss: 1.58165449e-06
Iter: 515 loss: 1.57674799e-06
Iter: 516 loss: 1.57600948e-06
Iter: 517 loss: 1.57657109e-06
Iter: 518 loss: 1.57562886e-06
Iter: 519 loss: 1.57508134e-06
Iter: 520 loss: 1.5787798e-06
Iter: 521 loss: 1.57499812e-06
Iter: 522 loss: 1.57437182e-06
Iter: 523 loss: 1.57438626e-06
Iter: 524 loss: 1.57384682e-06
Iter: 525 loss: 1.57323211e-06
Iter: 526 loss: 1.57576483e-06
Iter: 527 loss: 1.57317686e-06
Iter: 528 loss: 1.57243619e-06
Iter: 529 loss: 1.57388274e-06
Iter: 530 loss: 1.57216584e-06
Iter: 531 loss: 1.57176794e-06
Iter: 532 loss: 1.57149441e-06
Iter: 533 loss: 1.57140119e-06
Iter: 534 loss: 1.57050818e-06
Iter: 535 loss: 1.57549107e-06
Iter: 536 loss: 1.57039722e-06
Iter: 537 loss: 1.56986539e-06
Iter: 538 loss: 1.56885051e-06
Iter: 539 loss: 1.58596936e-06
Iter: 540 loss: 1.56878514e-06
Iter: 541 loss: 1.56780732e-06
Iter: 542 loss: 1.57180443e-06
Iter: 543 loss: 1.56763883e-06
Iter: 544 loss: 1.5669018e-06
Iter: 545 loss: 1.5669284e-06
Iter: 546 loss: 1.56643114e-06
Iter: 547 loss: 1.56565625e-06
Iter: 548 loss: 1.58364401e-06
Iter: 549 loss: 1.56563112e-06
Iter: 550 loss: 1.56511715e-06
Iter: 551 loss: 1.56506212e-06
Iter: 552 loss: 1.56462158e-06
Iter: 553 loss: 1.56385465e-06
Iter: 554 loss: 1.5638999e-06
Iter: 555 loss: 1.56351e-06
Iter: 556 loss: 1.56334102e-06
Iter: 557 loss: 1.5630693e-06
Iter: 558 loss: 1.56251087e-06
Iter: 559 loss: 1.57512318e-06
Iter: 560 loss: 1.56249087e-06
Iter: 561 loss: 1.56202179e-06
Iter: 562 loss: 1.56206067e-06
Iter: 563 loss: 1.56170267e-06
Iter: 564 loss: 1.56090732e-06
Iter: 565 loss: 1.56932231e-06
Iter: 566 loss: 1.56086048e-06
Iter: 567 loss: 1.5605342e-06
Iter: 568 loss: 1.56040903e-06
Iter: 569 loss: 1.55999408e-06
Iter: 570 loss: 1.56016131e-06
Iter: 571 loss: 1.55968246e-06
Iter: 572 loss: 1.5591163e-06
Iter: 573 loss: 1.55903797e-06
Iter: 574 loss: 1.55859107e-06
Iter: 575 loss: 1.55799535e-06
Iter: 576 loss: 1.5578679e-06
Iter: 577 loss: 1.55741986e-06
Iter: 578 loss: 1.55671955e-06
Iter: 579 loss: 1.55675696e-06
Iter: 580 loss: 1.5562415e-06
Iter: 581 loss: 1.55542602e-06
Iter: 582 loss: 1.55541431e-06
Iter: 583 loss: 1.55484349e-06
Iter: 584 loss: 1.55480325e-06
Iter: 585 loss: 1.55430394e-06
Iter: 586 loss: 1.55394878e-06
Iter: 587 loss: 1.55376074e-06
Iter: 588 loss: 1.55306566e-06
Iter: 589 loss: 1.55309783e-06
Iter: 590 loss: 1.55279099e-06
Iter: 591 loss: 1.55223506e-06
Iter: 592 loss: 1.56290241e-06
Iter: 593 loss: 1.55226928e-06
Iter: 594 loss: 1.55166822e-06
Iter: 595 loss: 1.55165867e-06
Iter: 596 loss: 1.55136524e-06
Iter: 597 loss: 1.55082694e-06
Iter: 598 loss: 1.55083103e-06
Iter: 599 loss: 1.55043028e-06
Iter: 600 loss: 1.55042426e-06
Iter: 601 loss: 1.55005182e-06
Iter: 602 loss: 1.55036435e-06
Iter: 603 loss: 1.54987038e-06
Iter: 604 loss: 1.54930569e-06
Iter: 605 loss: 1.54907025e-06
Iter: 606 loss: 1.54886698e-06
Iter: 607 loss: 1.54846771e-06
Iter: 608 loss: 1.54842007e-06
Iter: 609 loss: 1.54793656e-06
Iter: 610 loss: 1.54780423e-06
Iter: 611 loss: 1.54754593e-06
Iter: 612 loss: 1.54691747e-06
Iter: 613 loss: 1.54752126e-06
Iter: 614 loss: 1.54659483e-06
Iter: 615 loss: 1.54573365e-06
Iter: 616 loss: 1.55162627e-06
Iter: 617 loss: 1.54567647e-06
Iter: 618 loss: 1.54533268e-06
Iter: 619 loss: 1.54980262e-06
Iter: 620 loss: 1.54530676e-06
Iter: 621 loss: 1.54498878e-06
Iter: 622 loss: 1.54432394e-06
Iter: 623 loss: 1.55386829e-06
Iter: 624 loss: 1.54426789e-06
Iter: 625 loss: 1.54391034e-06
Iter: 626 loss: 1.54387249e-06
Iter: 627 loss: 1.54350255e-06
Iter: 628 loss: 1.54298414e-06
Iter: 629 loss: 1.54298209e-06
Iter: 630 loss: 1.54249847e-06
Iter: 631 loss: 1.54599957e-06
Iter: 632 loss: 1.54246345e-06
Iter: 633 loss: 1.54198347e-06
Iter: 634 loss: 1.54336158e-06
Iter: 635 loss: 1.5418392e-06
Iter: 636 loss: 1.54138286e-06
Iter: 637 loss: 1.54092106e-06
Iter: 638 loss: 1.54090299e-06
Iter: 639 loss: 1.54027703e-06
Iter: 640 loss: 1.54270492e-06
Iter: 641 loss: 1.54013719e-06
Iter: 642 loss: 1.53963947e-06
Iter: 643 loss: 1.54666054e-06
Iter: 644 loss: 1.53965584e-06
Iter: 645 loss: 1.53929648e-06
Iter: 646 loss: 1.53888959e-06
Iter: 647 loss: 1.53889982e-06
Iter: 648 loss: 1.53840301e-06
Iter: 649 loss: 1.54521422e-06
Iter: 650 loss: 1.53842177e-06
Iter: 651 loss: 1.53811402e-06
Iter: 652 loss: 1.53849305e-06
Iter: 653 loss: 1.53795838e-06
Iter: 654 loss: 1.53751455e-06
Iter: 655 loss: 1.5387825e-06
Iter: 656 loss: 1.53743122e-06
Iter: 657 loss: 1.53708947e-06
Iter: 658 loss: 1.53664371e-06
Iter: 659 loss: 1.53659403e-06
Iter: 660 loss: 1.53606265e-06
Iter: 661 loss: 1.5360589e-06
Iter: 662 loss: 1.53581664e-06
Iter: 663 loss: 1.53531e-06
Iter: 664 loss: 1.54383224e-06
Iter: 665 loss: 1.53530732e-06
Iter: 666 loss: 1.53504743e-06
Iter: 667 loss: 1.53495375e-06
Iter: 668 loss: 1.53467431e-06
Iter: 669 loss: 1.53414658e-06
Iter: 670 loss: 1.54344502e-06
Iter: 671 loss: 1.53418705e-06
Iter: 672 loss: 1.53355234e-06
Iter: 673 loss: 1.53475571e-06
Iter: 674 loss: 1.53333781e-06
Iter: 675 loss: 1.53290966e-06
Iter: 676 loss: 1.53261931e-06
Iter: 677 loss: 1.53244355e-06
Iter: 678 loss: 1.53183396e-06
Iter: 679 loss: 1.53987423e-06
Iter: 680 loss: 1.53180713e-06
Iter: 681 loss: 1.53141332e-06
Iter: 682 loss: 1.53138808e-06
Iter: 683 loss: 1.53116321e-06
Iter: 684 loss: 1.53058409e-06
Iter: 685 loss: 1.53558312e-06
Iter: 686 loss: 1.53056635e-06
Iter: 687 loss: 1.53026156e-06
Iter: 688 loss: 1.53145038e-06
Iter: 689 loss: 1.53019414e-06
Iter: 690 loss: 1.52986536e-06
Iter: 691 loss: 1.52934376e-06
Iter: 692 loss: 1.52937321e-06
Iter: 693 loss: 1.52900986e-06
Iter: 694 loss: 1.52897132e-06
Iter: 695 loss: 1.52854659e-06
Iter: 696 loss: 1.52824498e-06
Iter: 697 loss: 1.52819848e-06
Iter: 698 loss: 1.52781945e-06
Iter: 699 loss: 1.52883808e-06
Iter: 700 loss: 1.52773282e-06
Iter: 701 loss: 1.52716405e-06
Iter: 702 loss: 1.5305576e-06
Iter: 703 loss: 1.52712528e-06
Iter: 704 loss: 1.52686812e-06
Iter: 705 loss: 1.52667644e-06
Iter: 706 loss: 1.52659152e-06
Iter: 707 loss: 1.52615712e-06
Iter: 708 loss: 1.52840244e-06
Iter: 709 loss: 1.52606628e-06
Iter: 710 loss: 1.52570203e-06
Iter: 711 loss: 1.52611631e-06
Iter: 712 loss: 1.52550979e-06
Iter: 713 loss: 1.52504185e-06
Iter: 714 loss: 1.52652933e-06
Iter: 715 loss: 1.52487542e-06
Iter: 716 loss: 1.52456846e-06
Iter: 717 loss: 1.52821394e-06
Iter: 718 loss: 1.52457119e-06
Iter: 719 loss: 1.52436019e-06
Iter: 720 loss: 1.52464827e-06
Iter: 721 loss: 1.52422035e-06
Iter: 722 loss: 1.52391101e-06
Iter: 723 loss: 1.52462076e-06
Iter: 724 loss: 1.52379982e-06
Iter: 725 loss: 1.52359553e-06
Iter: 726 loss: 1.52351708e-06
Iter: 727 loss: 1.52334746e-06
Iter: 728 loss: 1.52290897e-06
Iter: 729 loss: 1.52622874e-06
Iter: 730 loss: 1.52292557e-06
Iter: 731 loss: 1.52272958e-06
Iter: 732 loss: 1.52220105e-06
Iter: 733 loss: 1.52827965e-06
Iter: 734 loss: 1.52220946e-06
Iter: 735 loss: 1.52215762e-06
Iter: 736 loss: 1.52194639e-06
Iter: 737 loss: 1.52172652e-06
Iter: 738 loss: 1.52126927e-06
Iter: 739 loss: 1.52897724e-06
Iter: 740 loss: 1.52128541e-06
Iter: 741 loss: 1.52082271e-06
Iter: 742 loss: 1.52116831e-06
Iter: 743 loss: 1.52055736e-06
Iter: 744 loss: 1.52011592e-06
Iter: 745 loss: 1.52015855e-06
Iter: 746 loss: 1.5198126e-06
Iter: 747 loss: 1.51976496e-06
Iter: 748 loss: 1.51957965e-06
Iter: 749 loss: 1.51910967e-06
Iter: 750 loss: 1.51986183e-06
Iter: 751 loss: 1.5189e-06
Iter: 752 loss: 1.51856307e-06
Iter: 753 loss: 1.51858615e-06
Iter: 754 loss: 1.51829727e-06
Iter: 755 loss: 1.5183432e-06
Iter: 756 loss: 1.51816778e-06
Iter: 757 loss: 1.51773816e-06
Iter: 758 loss: 1.5196922e-06
Iter: 759 loss: 1.51771553e-06
Iter: 760 loss: 1.5175433e-06
Iter: 761 loss: 1.51720371e-06
Iter: 762 loss: 1.51722384e-06
Iter: 763 loss: 1.51683651e-06
Iter: 764 loss: 1.52177154e-06
Iter: 765 loss: 1.51685822e-06
Iter: 766 loss: 1.51664676e-06
Iter: 767 loss: 1.51609697e-06
Iter: 768 loss: 1.52157122e-06
Iter: 769 loss: 1.51605013e-06
Iter: 770 loss: 1.5159435e-06
Iter: 771 loss: 1.5157766e-06
Iter: 772 loss: 1.51552854e-06
Iter: 773 loss: 1.51523341e-06
Iter: 774 loss: 1.51522715e-06
Iter: 775 loss: 1.51490235e-06
Iter: 776 loss: 1.51467873e-06
Iter: 777 loss: 1.51456697e-06
Iter: 778 loss: 1.51417078e-06
Iter: 779 loss: 1.51572067e-06
Iter: 780 loss: 1.51407517e-06
Iter: 781 loss: 1.51364895e-06
Iter: 782 loss: 1.51625989e-06
Iter: 783 loss: 1.51360427e-06
Iter: 784 loss: 1.51336587e-06
Iter: 785 loss: 1.51288896e-06
Iter: 786 loss: 1.52178086e-06
Iter: 787 loss: 1.51286429e-06
Iter: 788 loss: 1.51244649e-06
Iter: 789 loss: 1.51599443e-06
Iter: 790 loss: 1.51249651e-06
Iter: 791 loss: 1.51209542e-06
Iter: 792 loss: 1.51488166e-06
Iter: 793 loss: 1.51203528e-06
Iter: 794 loss: 1.51185293e-06
Iter: 795 loss: 1.51246184e-06
Iter: 796 loss: 1.51176448e-06
Iter: 797 loss: 1.51146651e-06
Iter: 798 loss: 1.51117138e-06
Iter: 799 loss: 1.51114227e-06
Iter: 800 loss: 1.51085112e-06
Iter: 801 loss: 1.51386189e-06
Iter: 802 loss: 1.51082122e-06
Iter: 803 loss: 1.51049835e-06
Iter: 804 loss: 1.51042809e-06
Iter: 805 loss: 1.51016059e-06
Iter: 806 loss: 1.50983885e-06
Iter: 807 loss: 1.50937035e-06
Iter: 808 loss: 1.50933693e-06
Iter: 809 loss: 1.5090742e-06
Iter: 810 loss: 1.50898e-06
Iter: 811 loss: 1.50862093e-06
Iter: 812 loss: 1.5089164e-06
Iter: 813 loss: 1.50842902e-06
Iter: 814 loss: 1.50821904e-06
Iter: 815 loss: 1.50812446e-06
Iter: 816 loss: 1.50799508e-06
Iter: 817 loss: 1.50764458e-06
Iter: 818 loss: 1.51180075e-06
Iter: 819 loss: 1.50757273e-06
Iter: 820 loss: 1.50739766e-06
Iter: 821 loss: 1.50766846e-06
Iter: 822 loss: 1.50725327e-06
Iter: 823 loss: 1.50707206e-06
Iter: 824 loss: 1.50820256e-06
Iter: 825 loss: 1.50704886e-06
Iter: 826 loss: 1.50680626e-06
Iter: 827 loss: 1.50652386e-06
Iter: 828 loss: 1.50651772e-06
Iter: 829 loss: 1.50618303e-06
Iter: 830 loss: 1.50998676e-06
Iter: 831 loss: 1.50619189e-06
Iter: 832 loss: 1.50589767e-06
Iter: 833 loss: 1.50550431e-06
Iter: 834 loss: 1.50549499e-06
Iter: 835 loss: 1.50527956e-06
Iter: 836 loss: 1.50520498e-06
Iter: 837 loss: 1.50507367e-06
Iter: 838 loss: 1.5047541e-06
Iter: 839 loss: 1.51112238e-06
Iter: 840 loss: 1.50475876e-06
Iter: 841 loss: 1.5043031e-06
Iter: 842 loss: 1.50904089e-06
Iter: 843 loss: 1.50429923e-06
Iter: 844 loss: 1.50398182e-06
Iter: 845 loss: 1.50422807e-06
Iter: 846 loss: 1.50379981e-06
Iter: 847 loss: 1.5034127e-06
Iter: 848 loss: 1.50379708e-06
Iter: 849 loss: 1.50320056e-06
Iter: 850 loss: 1.50290407e-06
Iter: 851 loss: 1.50620622e-06
Iter: 852 loss: 1.50292681e-06
Iter: 853 loss: 1.50261008e-06
Iter: 854 loss: 1.50258575e-06
Iter: 855 loss: 1.5023702e-06
Iter: 856 loss: 1.50212281e-06
Iter: 857 loss: 1.50212259e-06
Iter: 858 loss: 1.5019358e-06
Iter: 859 loss: 1.50188021e-06
Iter: 860 loss: 1.50180199e-06
Iter: 861 loss: 1.50150663e-06
Iter: 862 loss: 1.50255607e-06
Iter: 863 loss: 1.50143637e-06
Iter: 864 loss: 1.50116375e-06
Iter: 865 loss: 1.50159531e-06
Iter: 866 loss: 1.50105666e-06
Iter: 867 loss: 1.50084065e-06
Iter: 868 loss: 1.50168523e-06
Iter: 869 loss: 1.50076301e-06
Iter: 870 loss: 1.50056144e-06
Iter: 871 loss: 1.50056633e-06
Iter: 872 loss: 1.50032724e-06
Iter: 873 loss: 1.50013511e-06
Iter: 874 loss: 1.50141409e-06
Iter: 875 loss: 1.50007031e-06
Iter: 876 loss: 1.49990865e-06
Iter: 877 loss: 1.50028302e-06
Iter: 878 loss: 1.49981236e-06
Iter: 879 loss: 1.49959988e-06
Iter: 880 loss: 1.49965956e-06
Iter: 881 loss: 1.49940342e-06
Iter: 882 loss: 1.49920481e-06
Iter: 883 loss: 1.50014762e-06
Iter: 884 loss: 1.49911625e-06
Iter: 885 loss: 1.49885159e-06
Iter: 886 loss: 1.49996094e-06
Iter: 887 loss: 1.49877087e-06
Iter: 888 loss: 1.49860591e-06
Iter: 889 loss: 1.49927587e-06
Iter: 890 loss: 1.49860671e-06
Iter: 891 loss: 1.49834352e-06
Iter: 892 loss: 1.49881487e-06
Iter: 893 loss: 1.49819766e-06
Iter: 894 loss: 1.49803554e-06
Iter: 895 loss: 1.49833431e-06
Iter: 896 loss: 1.49795324e-06
Iter: 897 loss: 1.49773462e-06
Iter: 898 loss: 1.4985651e-06
Iter: 899 loss: 1.49763991e-06
Iter: 900 loss: 1.49742652e-06
Iter: 901 loss: 1.49746677e-06
Iter: 902 loss: 1.49726156e-06
Iter: 903 loss: 1.4968839e-06
Iter: 904 loss: 1.49870584e-06
Iter: 905 loss: 1.49686957e-06
Iter: 906 loss: 1.49663401e-06
Iter: 907 loss: 1.49660013e-06
Iter: 908 loss: 1.49644779e-06
Iter: 909 loss: 1.49617529e-06
Iter: 910 loss: 1.49894618e-06
Iter: 911 loss: 1.49617108e-06
Iter: 912 loss: 1.49600396e-06
Iter: 913 loss: 1.49591403e-06
Iter: 914 loss: 1.49582525e-06
Iter: 915 loss: 1.49552716e-06
Iter: 916 loss: 1.49602511e-06
Iter: 917 loss: 1.49541802e-06
Iter: 918 loss: 1.49515427e-06
Iter: 919 loss: 1.49757511e-06
Iter: 920 loss: 1.4950981e-06
Iter: 921 loss: 1.49489051e-06
Iter: 922 loss: 1.49504888e-06
Iter: 923 loss: 1.49475818e-06
Iter: 924 loss: 1.49453717e-06
Iter: 925 loss: 1.49629875e-06
Iter: 926 loss: 1.4944751e-06
Iter: 927 loss: 1.49432765e-06
Iter: 928 loss: 1.49416474e-06
Iter: 929 loss: 1.49412654e-06
Iter: 930 loss: 1.49379559e-06
Iter: 931 loss: 1.49638493e-06
Iter: 932 loss: 1.49380548e-06
Iter: 933 loss: 1.49359016e-06
Iter: 934 loss: 1.49347989e-06
Iter: 935 loss: 1.49339928e-06
Iter: 936 loss: 1.4930813e-06
Iter: 937 loss: 1.49536686e-06
Iter: 938 loss: 1.49301741e-06
Iter: 939 loss: 1.49284324e-06
Iter: 940 loss: 1.49254629e-06
Iter: 941 loss: 1.49251446e-06
Iter: 942 loss: 1.4922457e-06
Iter: 943 loss: 1.49229879e-06
Iter: 944 loss: 1.4920397e-06
Iter: 945 loss: 1.49198661e-06
Iter: 946 loss: 1.49192203e-06
Iter: 947 loss: 1.49164487e-06
Iter: 948 loss: 1.49198854e-06
Iter: 949 loss: 1.49146513e-06
Iter: 950 loss: 1.49118455e-06
Iter: 951 loss: 1.49390746e-06
Iter: 952 loss: 1.49117477e-06
Iter: 953 loss: 1.4909931e-06
Iter: 954 loss: 1.4911991e-06
Iter: 955 loss: 1.49082462e-06
Iter: 956 loss: 1.49063408e-06
Iter: 957 loss: 1.49340201e-06
Iter: 958 loss: 1.49066693e-06
Iter: 959 loss: 1.49056268e-06
Iter: 960 loss: 1.49028301e-06
Iter: 961 loss: 1.49431207e-06
Iter: 962 loss: 1.49029916e-06
Iter: 963 loss: 1.49001437e-06
Iter: 964 loss: 1.48999266e-06
Iter: 965 loss: 1.48982622e-06
Iter: 966 loss: 1.4897e-06
Iter: 967 loss: 1.48963886e-06
Iter: 968 loss: 1.48937954e-06
Iter: 969 loss: 1.49202424e-06
Iter: 970 loss: 1.48941433e-06
Iter: 971 loss: 1.48921026e-06
Iter: 972 loss: 1.48888057e-06
Iter: 973 loss: 1.48887557e-06
Iter: 974 loss: 1.48862796e-06
Iter: 975 loss: 1.48867275e-06
Iter: 976 loss: 1.48843606e-06
Iter: 977 loss: 1.48851382e-06
Iter: 978 loss: 1.48833794e-06
Iter: 979 loss: 1.48810227e-06
Iter: 980 loss: 1.48777087e-06
Iter: 981 loss: 1.48773029e-06
Iter: 982 loss: 1.48730646e-06
Iter: 983 loss: 1.48920378e-06
Iter: 984 loss: 1.48731488e-06
Iter: 985 loss: 1.48690992e-06
Iter: 986 loss: 1.48813342e-06
Iter: 987 loss: 1.48685297e-06
Iter: 988 loss: 1.48654567e-06
Iter: 989 loss: 1.4897156e-06
Iter: 990 loss: 1.48654306e-06
Iter: 991 loss: 1.48637014e-06
Iter: 992 loss: 1.48597746e-06
Iter: 993 loss: 1.49335744e-06
Iter: 994 loss: 1.48597826e-06
Iter: 995 loss: 1.48566687e-06
Iter: 996 loss: 1.48983747e-06
Iter: 997 loss: 1.48564777e-06
Iter: 998 loss: 1.48533456e-06
Iter: 999 loss: 1.48557615e-06
Iter: 1000 loss: 1.48516369e-06
Iter: 1001 loss: 1.48490562e-06
Iter: 1002 loss: 1.48716833e-06
Iter: 1003 loss: 1.48492609e-06
Iter: 1004 loss: 1.4846853e-06
Iter: 1005 loss: 1.48472168e-06
Iter: 1006 loss: 1.48451886e-06
Iter: 1007 loss: 1.48435527e-06
Iter: 1008 loss: 1.48581898e-06
Iter: 1009 loss: 1.48434947e-06
Iter: 1010 loss: 1.48418223e-06
Iter: 1011 loss: 1.4841994e-06
Iter: 1012 loss: 1.48403376e-06
Iter: 1013 loss: 1.48382219e-06
Iter: 1014 loss: 1.48502511e-06
Iter: 1015 loss: 1.48375989e-06
Iter: 1016 loss: 1.48364597e-06
Iter: 1017 loss: 1.48360868e-06
Iter: 1018 loss: 1.48350318e-06
Iter: 1019 loss: 1.48327206e-06
Iter: 1020 loss: 1.48442359e-06
Iter: 1021 loss: 1.48322192e-06
Iter: 1022 loss: 1.48306071e-06
Iter: 1023 loss: 1.48497691e-06
Iter: 1024 loss: 1.48305571e-06
Iter: 1025 loss: 1.48290178e-06
Iter: 1026 loss: 1.48279457e-06
Iter: 1027 loss: 1.48271056e-06
Iter: 1028 loss: 1.48251934e-06
Iter: 1029 loss: 1.48283561e-06
Iter: 1030 loss: 1.48247432e-06
Iter: 1031 loss: 1.48220852e-06
Iter: 1032 loss: 1.48356844e-06
Iter: 1033 loss: 1.48216839e-06
Iter: 1034 loss: 1.48205709e-06
Iter: 1035 loss: 1.48249012e-06
Iter: 1036 loss: 1.48198274e-06
Iter: 1037 loss: 1.48182835e-06
Iter: 1038 loss: 1.48191282e-06
Iter: 1039 loss: 1.48171011e-06
Iter: 1040 loss: 1.48153231e-06
Iter: 1041 loss: 1.48203503e-06
Iter: 1042 loss: 1.48146523e-06
Iter: 1043 loss: 1.48129197e-06
Iter: 1044 loss: 1.4819725e-06
Iter: 1045 loss: 1.48123399e-06
Iter: 1046 loss: 1.4810845e-06
Iter: 1047 loss: 1.48137701e-06
Iter: 1048 loss: 1.48104118e-06
Iter: 1049 loss: 1.48080755e-06
Iter: 1050 loss: 1.4808287e-06
Iter: 1051 loss: 1.48070626e-06
Iter: 1052 loss: 1.48041204e-06
Iter: 1053 loss: 1.48182949e-06
Iter: 1054 loss: 1.4803677e-06
Iter: 1055 loss: 1.48017762e-06
Iter: 1056 loss: 1.48144659e-06
Iter: 1057 loss: 1.48014601e-06
Iter: 1058 loss: 1.47991932e-06
Iter: 1059 loss: 1.48004915e-06
Iter: 1060 loss: 1.47974902e-06
Iter: 1061 loss: 1.47957905e-06
Iter: 1062 loss: 1.47947594e-06
Iter: 1063 loss: 1.47938795e-06
Iter: 1064 loss: 1.47915466e-06
Iter: 1065 loss: 1.47914625e-06
Iter: 1066 loss: 1.4789963e-06
Iter: 1067 loss: 1.47893059e-06
Iter: 1068 loss: 1.47885771e-06
Iter: 1069 loss: 1.47862829e-06
Iter: 1070 loss: 1.47985952e-06
Iter: 1071 loss: 1.47862772e-06
Iter: 1072 loss: 1.4784514e-06
Iter: 1073 loss: 1.47847891e-06
Iter: 1074 loss: 1.47835544e-06
Iter: 1075 loss: 1.47815365e-06
Iter: 1076 loss: 1.4794233e-06
Iter: 1077 loss: 1.47815365e-06
Iter: 1078 loss: 1.47797073e-06
Iter: 1079 loss: 1.4780652e-06
Iter: 1080 loss: 1.4778791e-06
Iter: 1081 loss: 1.47764911e-06
Iter: 1082 loss: 1.47791957e-06
Iter: 1083 loss: 1.47755338e-06
Iter: 1084 loss: 1.47738433e-06
Iter: 1085 loss: 1.47778246e-06
Iter: 1086 loss: 1.47726178e-06
Iter: 1087 loss: 1.47706032e-06
Iter: 1088 loss: 1.47835453e-06
Iter: 1089 loss: 1.47707988e-06
Iter: 1090 loss: 1.47686842e-06
Iter: 1091 loss: 1.47738433e-06
Iter: 1092 loss: 1.47679316e-06
Iter: 1093 loss: 1.47665173e-06
Iter: 1094 loss: 1.47642231e-06
Iter: 1095 loss: 1.4764214e-06
Iter: 1096 loss: 1.47633807e-06
Iter: 1097 loss: 1.47629271e-06
Iter: 1098 loss: 1.47615015e-06
Iter: 1099 loss: 1.47585956e-06
Iter: 1100 loss: 1.47591743e-06
Iter: 1101 loss: 1.47568937e-06
Iter: 1102 loss: 1.4756863e-06
Iter: 1103 loss: 1.47558876e-06
Iter: 1104 loss: 1.47551395e-06
Iter: 1105 loss: 1.47546234e-06
Iter: 1106 loss: 1.47527271e-06
Iter: 1107 loss: 1.4774497e-06
Iter: 1108 loss: 1.4752751e-06
Iter: 1109 loss: 1.47519347e-06
Iter: 1110 loss: 1.47523076e-06
Iter: 1111 loss: 1.47510184e-06
Iter: 1112 loss: 1.47495234e-06
Iter: 1113 loss: 1.47548121e-06
Iter: 1114 loss: 1.4749412e-06
Iter: 1115 loss: 1.47473497e-06
Iter: 1116 loss: 1.4756713e-06
Iter: 1117 loss: 1.47471405e-06
Iter: 1118 loss: 1.474594e-06
Iter: 1119 loss: 1.47472713e-06
Iter: 1120 loss: 1.47452829e-06
Iter: 1121 loss: 1.47431251e-06
Iter: 1122 loss: 1.47562321e-06
Iter: 1123 loss: 1.47430103e-06
Iter: 1124 loss: 1.47419223e-06
Iter: 1125 loss: 1.47393962e-06
Iter: 1126 loss: 1.4778575e-06
Iter: 1127 loss: 1.47393575e-06
Iter: 1128 loss: 1.47377773e-06
Iter: 1129 loss: 1.47378591e-06
Iter: 1130 loss: 1.47354785e-06
Iter: 1131 loss: 1.47343189e-06
Iter: 1132 loss: 1.47334231e-06
Iter: 1133 loss: 1.47318042e-06
Iter: 1134 loss: 1.47569062e-06
Iter: 1135 loss: 1.47320156e-06
Iter: 1136 loss: 1.47305786e-06
Iter: 1137 loss: 1.47278706e-06
Iter: 1138 loss: 1.47279411e-06
Iter: 1139 loss: 1.47258493e-06
Iter: 1140 loss: 1.47254923e-06
Iter: 1141 loss: 1.47240962e-06
Iter: 1142 loss: 1.4724701e-06
Iter: 1143 loss: 1.47226785e-06
Iter: 1144 loss: 1.4720689e-06
Iter: 1145 loss: 1.47229775e-06
Iter: 1146 loss: 1.47201217e-06
Iter: 1147 loss: 1.47175297e-06
Iter: 1148 loss: 1.47210551e-06
Iter: 1149 loss: 1.47168203e-06
Iter: 1150 loss: 1.47145204e-06
Iter: 1151 loss: 1.47237688e-06
Iter: 1152 loss: 1.4714393e-06
Iter: 1153 loss: 1.47129595e-06
Iter: 1154 loss: 1.47131027e-06
Iter: 1155 loss: 1.47117157e-06
Iter: 1156 loss: 1.47094011e-06
Iter: 1157 loss: 1.47590731e-06
Iter: 1158 loss: 1.47096557e-06
Iter: 1159 loss: 1.47077776e-06
Iter: 1160 loss: 1.47123887e-06
Iter: 1161 loss: 1.47066089e-06
Iter: 1162 loss: 1.47038531e-06
Iter: 1163 loss: 1.4730465e-06
Iter: 1164 loss: 1.47042874e-06
Iter: 1165 loss: 1.47035189e-06
Iter: 1166 loss: 1.47036894e-06
Iter: 1167 loss: 1.47025912e-06
Iter: 1168 loss: 1.47002072e-06
Iter: 1169 loss: 1.47043988e-06
Iter: 1170 loss: 1.46995671e-06
Iter: 1171 loss: 1.46984848e-06
Iter: 1172 loss: 1.47007745e-06
Iter: 1173 loss: 1.46977129e-06
Iter: 1174 loss: 1.46960656e-06
Iter: 1175 loss: 1.47019568e-06
Iter: 1176 loss: 1.46957655e-06
Iter: 1177 loss: 1.46939419e-06
Iter: 1178 loss: 1.46977322e-06
Iter: 1179 loss: 1.46931734e-06
Iter: 1180 loss: 1.4691841e-06
Iter: 1181 loss: 1.46923571e-06
Iter: 1182 loss: 1.46912225e-06
Iter: 1183 loss: 1.46887237e-06
Iter: 1184 loss: 1.46933962e-06
Iter: 1185 loss: 1.46878688e-06
Iter: 1186 loss: 1.46853904e-06
Iter: 1187 loss: 1.47034348e-06
Iter: 1188 loss: 1.46855177e-06
Iter: 1189 loss: 1.46836328e-06
Iter: 1190 loss: 1.46844036e-06
Iter: 1191 loss: 1.46824596e-06
Iter: 1192 loss: 1.46806008e-06
Iter: 1193 loss: 1.46781645e-06
Iter: 1194 loss: 1.46783236e-06
Iter: 1195 loss: 1.46766479e-06
Iter: 1196 loss: 1.46759385e-06
Iter: 1197 loss: 1.4674672e-06
Iter: 1198 loss: 1.46719958e-06
Iter: 1199 loss: 1.46720129e-06
Iter: 1200 loss: 1.46705634e-06
Iter: 1201 loss: 1.4670519e-06
Iter: 1202 loss: 1.46690923e-06
Iter: 1203 loss: 1.4667138e-06
Iter: 1204 loss: 1.46906586e-06
Iter: 1205 loss: 1.46670016e-06
Iter: 1206 loss: 1.46661785e-06
Iter: 1207 loss: 1.46657044e-06
Iter: 1208 loss: 1.46650723e-06
Iter: 1209 loss: 1.46640855e-06
Iter: 1210 loss: 1.46645448e-06
Iter: 1211 loss: 1.4662935e-06
Iter: 1212 loss: 1.46646312e-06
Iter: 1213 loss: 1.46627099e-06
Iter: 1214 loss: 1.46611069e-06
Iter: 1215 loss: 1.46615241e-06
Iter: 1216 loss: 1.46607317e-06
Iter: 1217 loss: 1.46590389e-06
Iter: 1218 loss: 1.46665298e-06
Iter: 1219 loss: 1.46584125e-06
Iter: 1220 loss: 1.46580942e-06
Iter: 1221 loss: 1.46549451e-06
Iter: 1222 loss: 1.46668651e-06
Iter: 1223 loss: 1.46537536e-06
Iter: 1224 loss: 1.46508341e-06
Iter: 1225 loss: 1.46670118e-06
Iter: 1226 loss: 1.46504885e-06
Iter: 1227 loss: 1.46476077e-06
Iter: 1228 loss: 1.4660277e-06
Iter: 1229 loss: 1.46470052e-06
Iter: 1230 loss: 1.46459411e-06
Iter: 1231 loss: 1.46674324e-06
Iter: 1232 loss: 1.46450748e-06
Iter: 1233 loss: 1.46439857e-06
Iter: 1234 loss: 1.46430955e-06
Iter: 1235 loss: 1.46421462e-06
Iter: 1236 loss: 1.46401794e-06
Iter: 1237 loss: 1.4645417e-06
Iter: 1238 loss: 1.46394564e-06
Iter: 1239 loss: 1.46376192e-06
Iter: 1240 loss: 1.46531136e-06
Iter: 1241 loss: 1.46367836e-06
Iter: 1242 loss: 1.46357411e-06
Iter: 1243 loss: 1.4632792e-06
Iter: 1244 loss: 1.46744242e-06
Iter: 1245 loss: 1.46328841e-06
Iter: 1246 loss: 1.46316768e-06
Iter: 1247 loss: 1.46308571e-06
Iter: 1248 loss: 1.46297089e-06
Iter: 1249 loss: 1.46274556e-06
Iter: 1250 loss: 1.46278956e-06
Iter: 1251 loss: 1.46264449e-06
Iter: 1252 loss: 1.46263858e-06
Iter: 1253 loss: 1.46252398e-06
Iter: 1254 loss: 1.46236584e-06
Iter: 1255 loss: 1.46692605e-06
Iter: 1256 loss: 1.46234947e-06
Iter: 1257 loss: 1.46214506e-06
Iter: 1258 loss: 1.46214029e-06
Iter: 1259 loss: 1.46194498e-06
Iter: 1260 loss: 1.46244111e-06
Iter: 1261 loss: 1.46185903e-06
Iter: 1262 loss: 1.46178127e-06
Iter: 1263 loss: 1.46162074e-06
Iter: 1264 loss: 1.46158959e-06
Iter: 1265 loss: 1.461382e-06
Iter: 1266 loss: 1.4614385e-06
Iter: 1267 loss: 1.46117884e-06
Iter: 1268 loss: 1.46099e-06
Iter: 1269 loss: 1.46362754e-06
Iter: 1270 loss: 1.46100706e-06
Iter: 1271 loss: 1.46079515e-06
Iter: 1272 loss: 1.46196885e-06
Iter: 1273 loss: 1.46076468e-06
Iter: 1274 loss: 1.46065315e-06
Iter: 1275 loss: 1.46041725e-06
Iter: 1276 loss: 1.46039588e-06
Iter: 1277 loss: 1.46028992e-06
Iter: 1278 loss: 1.46026491e-06
Iter: 1279 loss: 1.4601377e-06
Iter: 1280 loss: 1.45985962e-06
Iter: 1281 loss: 1.46295918e-06
Iter: 1282 loss: 1.45987156e-06
Iter: 1283 loss: 1.45965578e-06
Iter: 1284 loss: 1.45963634e-06
Iter: 1285 loss: 1.4594558e-06
Iter: 1286 loss: 1.45935144e-06
Iter: 1287 loss: 1.45926435e-06
Iter: 1288 loss: 1.45904926e-06
Iter: 1289 loss: 1.45900049e-06
Iter: 1290 loss: 1.45891784e-06
Iter: 1291 loss: 1.45886247e-06
Iter: 1292 loss: 1.4587805e-06
Iter: 1293 loss: 1.45864465e-06
Iter: 1294 loss: 1.45869171e-06
Iter: 1295 loss: 1.45851709e-06
Iter: 1296 loss: 1.45838635e-06
Iter: 1297 loss: 1.45839761e-06
Iter: 1298 loss: 1.45824924e-06
Iter: 1299 loss: 1.45806553e-06
Iter: 1300 loss: 1.45806723e-06
Iter: 1301 loss: 1.45793e-06
Iter: 1302 loss: 1.45826857e-06
Iter: 1303 loss: 1.45783213e-06
Iter: 1304 loss: 1.45777312e-06
Iter: 1305 loss: 1.45773015e-06
Iter: 1306 loss: 1.4576533e-06
Iter: 1307 loss: 1.45756417e-06
Iter: 1308 loss: 1.45889521e-06
Iter: 1309 loss: 1.45750937e-06
Iter: 1310 loss: 1.45743047e-06
Iter: 1311 loss: 1.45717036e-06
Iter: 1312 loss: 1.46061939e-06
Iter: 1313 loss: 1.45718809e-06
Iter: 1314 loss: 1.45701097e-06
Iter: 1315 loss: 1.45936815e-06
Iter: 1316 loss: 1.45702234e-06
Iter: 1317 loss: 1.45684612e-06
Iter: 1318 loss: 1.45808258e-06
Iter: 1319 loss: 1.45679564e-06
Iter: 1320 loss: 1.4567438e-06
Iter: 1321 loss: 1.45652643e-06
Iter: 1322 loss: 1.45823947e-06
Iter: 1323 loss: 1.45655781e-06
Iter: 1324 loss: 1.4563791e-06
Iter: 1325 loss: 1.45633544e-06
Iter: 1326 loss: 1.45626689e-06
Iter: 1327 loss: 1.45607089e-06
Iter: 1328 loss: 1.4587e-06
Iter: 1329 loss: 1.45604599e-06
Iter: 1330 loss: 1.45588524e-06
Iter: 1331 loss: 1.4559057e-06
Iter: 1332 loss: 1.45568242e-06
Iter: 1333 loss: 1.45553133e-06
Iter: 1334 loss: 1.45548643e-06
Iter: 1335 loss: 1.45529089e-06
Iter: 1336 loss: 1.45529725e-06
Iter: 1337 loss: 1.45515241e-06
Iter: 1338 loss: 1.45497563e-06
Iter: 1339 loss: 1.45485342e-06
Iter: 1340 loss: 1.45482136e-06
Iter: 1341 loss: 1.45452873e-06
Iter: 1342 loss: 1.45580043e-06
Iter: 1343 loss: 1.45453487e-06
Iter: 1344 loss: 1.4542552e-06
Iter: 1345 loss: 1.45524223e-06
Iter: 1346 loss: 1.45425111e-06
Iter: 1347 loss: 1.4540841e-06
Iter: 1348 loss: 1.45400384e-06
Iter: 1349 loss: 1.45397644e-06
Iter: 1350 loss: 1.45380864e-06
Iter: 1351 loss: 1.45578315e-06
Iter: 1352 loss: 1.45375884e-06
Iter: 1353 loss: 1.45364766e-06
Iter: 1354 loss: 1.4535002e-06
Iter: 1355 loss: 1.45753756e-06
Iter: 1356 loss: 1.45347394e-06
Iter: 1357 loss: 1.45330182e-06
Iter: 1358 loss: 1.45451986e-06
Iter: 1359 loss: 1.45327829e-06
Iter: 1360 loss: 1.45307467e-06
Iter: 1361 loss: 1.4541215e-06
Iter: 1362 loss: 1.45302624e-06
Iter: 1363 loss: 1.45289539e-06
Iter: 1364 loss: 1.45260219e-06
Iter: 1365 loss: 1.45683225e-06
Iter: 1366 loss: 1.45259537e-06
Iter: 1367 loss: 1.45251295e-06
Iter: 1368 loss: 1.45247054e-06
Iter: 1369 loss: 1.4522625e-06
Iter: 1370 loss: 1.45209833e-06
Iter: 1371 loss: 1.45212152e-06
Iter: 1372 loss: 1.4518738e-06
Iter: 1373 loss: 1.45244621e-06
Iter: 1374 loss: 1.45179342e-06
Iter: 1375 loss: 1.45162062e-06
Iter: 1376 loss: 1.45162016e-06
Iter: 1377 loss: 1.45148829e-06
Iter: 1378 loss: 1.45171475e-06
Iter: 1379 loss: 1.45143451e-06
Iter: 1380 loss: 1.45132481e-06
Iter: 1381 loss: 1.45187664e-06
Iter: 1382 loss: 1.4513198e-06
Iter: 1383 loss: 1.45119407e-06
Iter: 1384 loss: 1.45110516e-06
Iter: 1385 loss: 1.45108447e-06
Iter: 1386 loss: 1.4509219e-06
Iter: 1387 loss: 1.4531438e-06
Iter: 1388 loss: 1.45092133e-06
Iter: 1389 loss: 1.45083368e-06
Iter: 1390 loss: 1.45071226e-06
Iter: 1391 loss: 1.45067088e-06
Iter: 1392 loss: 1.45055833e-06
Iter: 1393 loss: 1.45257957e-06
Iter: 1394 loss: 1.45056038e-06
Iter: 1395 loss: 1.45043896e-06
Iter: 1396 loss: 1.45025251e-06
Iter: 1397 loss: 1.45025865e-06
Iter: 1398 loss: 1.45007073e-06
Iter: 1399 loss: 1.44992919e-06
Iter: 1400 loss: 1.44992691e-06
Iter: 1401 loss: 1.44980413e-06
Iter: 1402 loss: 1.44976866e-06
Iter: 1403 loss: 1.44963553e-06
Iter: 1404 loss: 1.44943078e-06
Iter: 1405 loss: 1.4494243e-06
Iter: 1406 loss: 1.4493171e-06
Iter: 1407 loss: 1.44932858e-06
Iter: 1408 loss: 1.44912497e-06
Iter: 1409 loss: 1.44892329e-06
Iter: 1410 loss: 1.44917647e-06
Iter: 1411 loss: 1.44883768e-06
Iter: 1412 loss: 1.44866431e-06
Iter: 1413 loss: 1.45105446e-06
Iter: 1414 loss: 1.44867522e-06
Iter: 1415 loss: 1.44856574e-06
Iter: 1416 loss: 1.44831313e-06
Iter: 1417 loss: 1.45320064e-06
Iter: 1418 loss: 1.44831222e-06
Iter: 1419 loss: 1.44800651e-06
Iter: 1420 loss: 1.44795433e-06
Iter: 1421 loss: 1.44782439e-06
Iter: 1422 loss: 1.44761987e-06
Iter: 1423 loss: 1.44762771e-06
Iter: 1424 loss: 1.4473701e-06
Iter: 1425 loss: 1.44794808e-06
Iter: 1426 loss: 1.44729802e-06
Iter: 1427 loss: 1.44719831e-06
Iter: 1428 loss: 1.44785179e-06
Iter: 1429 loss: 1.44714329e-06
Iter: 1430 loss: 1.44701175e-06
Iter: 1431 loss: 1.44685544e-06
Iter: 1432 loss: 1.44683781e-06
Iter: 1433 loss: 1.44667479e-06
Iter: 1434 loss: 1.44648561e-06
Iter: 1435 loss: 1.4464905e-06
Iter: 1436 loss: 1.44643718e-06
Iter: 1437 loss: 1.44637022e-06
Iter: 1438 loss: 1.44623243e-06
Iter: 1439 loss: 1.44596356e-06
Iter: 1440 loss: 1.45117804e-06
Iter: 1441 loss: 1.44597527e-06
Iter: 1442 loss: 1.44584567e-06
Iter: 1443 loss: 1.44584317e-06
Iter: 1444 loss: 1.44569719e-06
Iter: 1445 loss: 1.44605667e-06
Iter: 1446 loss: 1.4456208e-06
Iter: 1447 loss: 1.44549506e-06
Iter: 1448 loss: 1.4453226e-06
Iter: 1449 loss: 1.44983198e-06
Iter: 1450 loss: 1.44532316e-06
Iter: 1451 loss: 1.4451731e-06
Iter: 1452 loss: 1.44513342e-06
Iter: 1453 loss: 1.44504065e-06
Iter: 1454 loss: 1.44499143e-06
Iter: 1455 loss: 1.44498324e-06
Iter: 1456 loss: 1.44481021e-06
Iter: 1457 loss: 1.44557089e-06
Iter: 1458 loss: 1.44479e-06
Iter: 1459 loss: 1.44462797e-06
Iter: 1460 loss: 1.44512e-06
Iter: 1461 loss: 1.44459932e-06
Iter: 1462 loss: 1.44442288e-06
Iter: 1463 loss: 1.44453247e-06
Iter: 1464 loss: 1.44432806e-06
Iter: 1465 loss: 1.44415367e-06
Iter: 1466 loss: 1.44404169e-06
Iter: 1467 loss: 1.44400201e-06
Iter: 1468 loss: 1.44381909e-06
Iter: 1469 loss: 1.44532169e-06
Iter: 1470 loss: 1.443771e-06
Iter: 1471 loss: 1.4436057e-06
Iter: 1472 loss: 1.44347791e-06
Iter: 1473 loss: 1.44341334e-06
Iter: 1474 loss: 1.44322325e-06
Iter: 1475 loss: 1.44430851e-06
Iter: 1476 loss: 1.44321416e-06
Iter: 1477 loss: 1.44301009e-06
Iter: 1478 loss: 1.4442719e-06
Iter: 1479 loss: 1.44301782e-06
Iter: 1480 loss: 1.4429329e-06
Iter: 1481 loss: 1.44273804e-06
Iter: 1482 loss: 1.44275941e-06
Iter: 1483 loss: 1.44255841e-06
Iter: 1484 loss: 1.44458511e-06
Iter: 1485 loss: 1.44255557e-06
Iter: 1486 loss: 1.4424229e-06
Iter: 1487 loss: 1.44240698e-06
Iter: 1488 loss: 1.44233638e-06
Iter: 1489 loss: 1.44210753e-06
Iter: 1490 loss: 1.4435069e-06
Iter: 1491 loss: 1.44209412e-06
Iter: 1492 loss: 1.4419345e-06
Iter: 1493 loss: 1.4419677e-06
Iter: 1494 loss: 1.44186447e-06
Iter: 1495 loss: 1.44169496e-06
Iter: 1496 loss: 1.44354385e-06
Iter: 1497 loss: 1.44167939e-06
Iter: 1498 loss: 1.44155865e-06
Iter: 1499 loss: 1.44144178e-06
Iter: 1500 loss: 1.44142803e-06
Iter: 1501 loss: 1.44129626e-06
Iter: 1502 loss: 1.44264698e-06
Iter: 1503 loss: 1.44127876e-06
Iter: 1504 loss: 1.4411994e-06
Iter: 1505 loss: 1.44140404e-06
Iter: 1506 loss: 1.44116302e-06
Iter: 1507 loss: 1.44104308e-06
Iter: 1508 loss: 1.44086846e-06
Iter: 1509 loss: 1.44521698e-06
Iter: 1510 loss: 1.44086084e-06
Iter: 1511 loss: 1.44072908e-06
Iter: 1512 loss: 1.44073056e-06
Iter: 1513 loss: 1.4406055e-06
Iter: 1514 loss: 1.44047124e-06
Iter: 1515 loss: 1.44393357e-06
Iter: 1516 loss: 1.44047203e-06
Iter: 1517 loss: 1.44030639e-06
Iter: 1518 loss: 1.44167507e-06
Iter: 1519 loss: 1.44028434e-06
Iter: 1520 loss: 1.44016929e-06
Iter: 1521 loss: 1.44017099e-06
Iter: 1522 loss: 1.44001524e-06
Iter: 1523 loss: 1.43986449e-06
Iter: 1524 loss: 1.44133583e-06
Iter: 1525 loss: 1.43988791e-06
Iter: 1526 loss: 1.43974194e-06
Iter: 1527 loss: 1.44006026e-06
Iter: 1528 loss: 1.43968396e-06
Iter: 1529 loss: 1.43956299e-06
Iter: 1530 loss: 1.43994566e-06
Iter: 1531 loss: 1.43949455e-06
Iter: 1532 loss: 1.43939042e-06
Iter: 1533 loss: 1.44004025e-06
Iter: 1534 loss: 1.4393728e-06
Iter: 1535 loss: 1.43923467e-06
Iter: 1536 loss: 1.43914099e-06
Iter: 1537 loss: 1.4390821e-06
Iter: 1538 loss: 1.43892169e-06
Iter: 1539 loss: 1.44067008e-06
Iter: 1540 loss: 1.43895113e-06
Iter: 1541 loss: 1.43881334e-06
Iter: 1542 loss: 1.438612e-06
Iter: 1543 loss: 1.44395926e-06
Iter: 1544 loss: 1.43861143e-06
Iter: 1545 loss: 1.43839361e-06
Iter: 1546 loss: 1.43991542e-06
Iter: 1547 loss: 1.43836689e-06
Iter: 1548 loss: 1.4380862e-06
Iter: 1549 loss: 1.43922023e-06
Iter: 1550 loss: 1.43805141e-06
Iter: 1551 loss: 1.43794614e-06
Iter: 1552 loss: 1.43819534e-06
Iter: 1553 loss: 1.4378702e-06
Iter: 1554 loss: 1.43774253e-06
Iter: 1555 loss: 1.43829016e-06
Iter: 1556 loss: 1.43773741e-06
Iter: 1557 loss: 1.43764225e-06
Iter: 1558 loss: 1.43764396e-06
Iter: 1559 loss: 1.43753141e-06
Iter: 1560 loss: 1.43742795e-06
Iter: 1561 loss: 1.43822774e-06
Iter: 1562 loss: 1.4373984e-06
Iter: 1563 loss: 1.4373436e-06
Iter: 1564 loss: 1.43732723e-06
Iter: 1565 loss: 1.43723389e-06
Iter: 1566 loss: 1.43704119e-06
Iter: 1567 loss: 1.43823081e-06
Iter: 1568 loss: 1.4370471e-06
Iter: 1569 loss: 1.43694751e-06
Iter: 1570 loss: 1.43687657e-06
Iter: 1571 loss: 1.43681677e-06
Iter: 1572 loss: 1.43669399e-06
Iter: 1573 loss: 1.43756461e-06
Iter: 1574 loss: 1.43662078e-06
Iter: 1575 loss: 1.43649265e-06
Iter: 1576 loss: 1.43660634e-06
Iter: 1577 loss: 1.43641364e-06
Iter: 1578 loss: 1.43623856e-06
Iter: 1579 loss: 1.43623856e-06
Iter: 1580 loss: 1.4361035e-06
Iter: 1581 loss: 1.43591262e-06
Iter: 1582 loss: 1.43592524e-06
Iter: 1583 loss: 1.43583179e-06
Iter: 1584 loss: 1.43565649e-06
Iter: 1585 loss: 1.43980685e-06
Iter: 1586 loss: 1.43565569e-06
Iter: 1587 loss: 1.43550449e-06
Iter: 1588 loss: 1.43550858e-06
Iter: 1589 loss: 1.435434e-06
Iter: 1590 loss: 1.43545788e-06
Iter: 1591 loss: 1.43534521e-06
Iter: 1592 loss: 1.43524051e-06
Iter: 1593 loss: 1.43601824e-06
Iter: 1594 loss: 1.43520242e-06
Iter: 1595 loss: 1.43511079e-06
Iter: 1596 loss: 1.43507668e-06
Iter: 1597 loss: 1.43498062e-06
Iter: 1598 loss: 1.4348899e-06
Iter: 1599 loss: 1.43546288e-06
Iter: 1600 loss: 1.43485977e-06
Iter: 1601 loss: 1.43469799e-06
Iter: 1602 loss: 1.43499051e-06
Iter: 1603 loss: 1.43468537e-06
Iter: 1604 loss: 1.43453235e-06
Iter: 1605 loss: 1.43465854e-06
Iter: 1606 loss: 1.43447869e-06
Iter: 1607 loss: 1.43429872e-06
Iter: 1608 loss: 1.43472766e-06
Iter: 1609 loss: 1.43419493e-06
Iter: 1610 loss: 1.43403554e-06
Iter: 1611 loss: 1.43422358e-06
Iter: 1612 loss: 1.4339962e-06
Iter: 1613 loss: 1.4339048e-06
Iter: 1614 loss: 1.43387695e-06
Iter: 1615 loss: 1.4337611e-06
Iter: 1616 loss: 1.43346915e-06
Iter: 1617 loss: 1.43624015e-06
Iter: 1618 loss: 1.433438e-06
Iter: 1619 loss: 1.43340571e-06
Iter: 1620 loss: 1.43336661e-06
Iter: 1621 loss: 1.43324633e-06
Iter: 1622 loss: 1.43320494e-06
Iter: 1623 loss: 1.43312104e-06
Iter: 1624 loss: 1.4330227e-06
Iter: 1625 loss: 1.43349803e-06
Iter: 1626 loss: 1.43297382e-06
Iter: 1627 loss: 1.43281773e-06
Iter: 1628 loss: 1.43342845e-06
Iter: 1629 loss: 1.43278396e-06
Iter: 1630 loss: 1.43267e-06
Iter: 1631 loss: 1.43268767e-06
Iter: 1632 loss: 1.43258285e-06
Iter: 1633 loss: 1.43237185e-06
Iter: 1634 loss: 1.43455873e-06
Iter: 1635 loss: 1.43239095e-06
Iter: 1636 loss: 1.43234502e-06
Iter: 1637 loss: 1.43214459e-06
Iter: 1638 loss: 1.43216357e-06
Iter: 1639 loss: 1.43188845e-06
Iter: 1640 loss: 1.43306602e-06
Iter: 1641 loss: 1.43183627e-06
Iter: 1642 loss: 1.43167813e-06
Iter: 1643 loss: 1.43177033e-06
Iter: 1644 loss: 1.43160378e-06
Iter: 1645 loss: 1.43141369e-06
Iter: 1646 loss: 1.43366401e-06
Iter: 1647 loss: 1.43141779e-06
Iter: 1648 loss: 1.43123543e-06
Iter: 1649 loss: 1.43104785e-06
Iter: 1650 loss: 1.43104603e-06
Iter: 1651 loss: 1.43081513e-06
Iter: 1652 loss: 1.43143143e-06
Iter: 1653 loss: 1.43081184e-06
Iter: 1654 loss: 1.43057991e-06
Iter: 1655 loss: 1.4313631e-06
Iter: 1656 loss: 1.4305142e-06
Iter: 1657 loss: 1.43037175e-06
Iter: 1658 loss: 1.43031741e-06
Iter: 1659 loss: 1.43024761e-06
Iter: 1660 loss: 1.4300947e-06
Iter: 1661 loss: 1.43279817e-06
Iter: 1662 loss: 1.43008174e-06
Iter: 1663 loss: 1.42996737e-06
Iter: 1664 loss: 1.42973897e-06
Iter: 1665 loss: 1.43276134e-06
Iter: 1666 loss: 1.42970259e-06
Iter: 1667 loss: 1.42970021e-06
Iter: 1668 loss: 1.42962404e-06
Iter: 1669 loss: 1.42954434e-06
Iter: 1670 loss: 1.4294003e-06
Iter: 1671 loss: 1.4317327e-06
Iter: 1672 loss: 1.42944725e-06
Iter: 1673 loss: 1.42927718e-06
Iter: 1674 loss: 1.42927831e-06
Iter: 1675 loss: 1.42915212e-06
Iter: 1676 loss: 1.42900603e-06
Iter: 1677 loss: 1.42925398e-06
Iter: 1678 loss: 1.4289933e-06
Iter: 1679 loss: 1.4288355e-06
Iter: 1680 loss: 1.42969247e-06
Iter: 1681 loss: 1.42885187e-06
Iter: 1682 loss: 1.4287524e-06
Iter: 1683 loss: 1.42862712e-06
Iter: 1684 loss: 1.42859346e-06
Iter: 1685 loss: 1.42840622e-06
Iter: 1686 loss: 1.43059174e-06
Iter: 1687 loss: 1.42837882e-06
Iter: 1688 loss: 1.42825922e-06
Iter: 1689 loss: 1.4285697e-06
Iter: 1690 loss: 1.42825479e-06
Iter: 1691 loss: 1.4281045e-06
Iter: 1692 loss: 1.42817112e-06
Iter: 1693 loss: 1.42798126e-06
Iter: 1694 loss: 1.42786439e-06
Iter: 1695 loss: 1.42923363e-06
Iter: 1696 loss: 1.42787019e-06
Iter: 1697 loss: 1.42772069e-06
Iter: 1698 loss: 1.42757312e-06
Iter: 1699 loss: 1.42755687e-06
Iter: 1700 loss: 1.42738372e-06
Iter: 1701 loss: 1.42815463e-06
Iter: 1702 loss: 1.4273752e-06
Iter: 1703 loss: 1.42711372e-06
Iter: 1704 loss: 1.42770773e-06
Iter: 1705 loss: 1.42706654e-06
Iter: 1706 loss: 1.42698161e-06
Iter: 1707 loss: 1.42722934e-06
Iter: 1708 loss: 1.42688464e-06
Iter: 1709 loss: 1.42676458e-06
Iter: 1710 loss: 1.42711121e-06
Iter: 1711 loss: 1.42666636e-06
Iter: 1712 loss: 1.42650799e-06
Iter: 1713 loss: 1.42740896e-06
Iter: 1714 loss: 1.42646684e-06
Iter: 1715 loss: 1.42634622e-06
Iter: 1716 loss: 1.42627187e-06
Iter: 1717 loss: 1.42622093e-06
Iter: 1718 loss: 1.4260736e-06
Iter: 1719 loss: 1.42736076e-06
Iter: 1720 loss: 1.42605882e-06
Iter: 1721 loss: 1.42590909e-06
Iter: 1722 loss: 1.42621252e-06
Iter: 1723 loss: 1.42589465e-06
Iter: 1724 loss: 1.4257181e-06
Iter: 1725 loss: 1.42582189e-06
Iter: 1726 loss: 1.42558849e-06
Iter: 1727 loss: 1.42546048e-06
Iter: 1728 loss: 1.42572299e-06
Iter: 1729 loss: 1.42535282e-06
Iter: 1730 loss: 1.42514534e-06
Iter: 1731 loss: 1.42630097e-06
Iter: 1732 loss: 1.4250777e-06
Iter: 1733 loss: 1.42493604e-06
Iter: 1734 loss: 1.42473323e-06
Iter: 1735 loss: 1.42474505e-06
Iter: 1736 loss: 1.42455542e-06
Iter: 1737 loss: 1.42455247e-06
Iter: 1738 loss: 1.42441854e-06
Iter: 1739 loss: 1.42414888e-06
Iter: 1740 loss: 1.42595559e-06
Iter: 1741 loss: 1.42412659e-06
Iter: 1742 loss: 1.42391877e-06
Iter: 1743 loss: 1.42384283e-06
Iter: 1744 loss: 1.42373892e-06
Iter: 1745 loss: 1.42354463e-06
Iter: 1746 loss: 1.42349722e-06
Iter: 1747 loss: 1.4233018e-06
Iter: 1748 loss: 1.42330987e-06
Iter: 1749 loss: 1.42319436e-06
Iter: 1750 loss: 1.4229438e-06
Iter: 1751 loss: 1.4261874e-06
Iter: 1752 loss: 1.42289991e-06
Iter: 1753 loss: 1.42267697e-06
Iter: 1754 loss: 1.42397175e-06
Iter: 1755 loss: 1.4226473e-06
Iter: 1756 loss: 1.42243016e-06
Iter: 1757 loss: 1.42376643e-06
Iter: 1758 loss: 1.42240265e-06
Iter: 1759 loss: 1.42227827e-06
Iter: 1760 loss: 1.42202975e-06
Iter: 1761 loss: 1.42202066e-06
Iter: 1762 loss: 1.42185104e-06
Iter: 1763 loss: 1.42196586e-06
Iter: 1764 loss: 1.42170916e-06
Iter: 1765 loss: 1.42151453e-06
Iter: 1766 loss: 1.42296403e-06
Iter: 1767 loss: 1.42152896e-06
Iter: 1768 loss: 1.42136355e-06
Iter: 1769 loss: 1.42201748e-06
Iter: 1770 loss: 1.42130557e-06
Iter: 1771 loss: 1.42119006e-06
Iter: 1772 loss: 1.42278009e-06
Iter: 1773 loss: 1.42118074e-06
Iter: 1774 loss: 1.42110616e-06
Iter: 1775 loss: 1.42108672e-06
Iter: 1776 loss: 1.42102533e-06
Iter: 1777 loss: 1.42091835e-06
Iter: 1778 loss: 1.42175952e-06
Iter: 1779 loss: 1.42091574e-06
Iter: 1780 loss: 1.42082558e-06
Iter: 1781 loss: 1.42073134e-06
Iter: 1782 loss: 1.42071326e-06
Iter: 1783 loss: 1.42050294e-06
Iter: 1784 loss: 1.42198155e-06
Iter: 1785 loss: 1.42045769e-06
Iter: 1786 loss: 1.42040858e-06
Iter: 1787 loss: 1.42019803e-06
Iter: 1788 loss: 1.42485692e-06
Iter: 1789 loss: 1.42022873e-06
Iter: 1790 loss: 1.42003751e-06
Iter: 1791 loss: 1.42006e-06
Iter: 1792 loss: 1.41998282e-06
Iter: 1793 loss: 1.41976614e-06
Iter: 1794 loss: 1.41973487e-06
Iter: 1795 loss: 1.41963369e-06
Iter: 1796 loss: 1.41962482e-06
Iter: 1797 loss: 1.41953456e-06
Iter: 1798 loss: 1.4195283e-06
Iter: 1799 loss: 1.41947305e-06
Iter: 1800 loss: 1.41936448e-06
Iter: 1801 loss: 1.4191437e-06
Iter: 1802 loss: 1.41916166e-06
Iter: 1803 loss: 1.41892906e-06
Iter: 1804 loss: 1.41876092e-06
Iter: 1805 loss: 1.41870726e-06
Iter: 1806 loss: 1.41846544e-06
Iter: 1807 loss: 1.41844112e-06
Iter: 1808 loss: 1.41823944e-06
Iter: 1809 loss: 1.41909607e-06
Iter: 1810 loss: 1.41822943e-06
Iter: 1811 loss: 1.41808061e-06
Iter: 1812 loss: 1.41852547e-06
Iter: 1813 loss: 1.41803798e-06
Iter: 1814 loss: 1.4179052e-06
Iter: 1815 loss: 1.41766668e-06
Iter: 1816 loss: 1.41769203e-06
Iter: 1817 loss: 1.41743453e-06
Iter: 1818 loss: 1.41808709e-06
Iter: 1819 loss: 1.41740782e-06
Iter: 1820 loss: 1.41716725e-06
Iter: 1821 loss: 1.41980354e-06
Iter: 1822 loss: 1.41717669e-06
Iter: 1823 loss: 1.41701764e-06
Iter: 1824 loss: 1.41675787e-06
Iter: 1825 loss: 1.41885948e-06
Iter: 1826 loss: 1.41669398e-06
Iter: 1827 loss: 1.41647149e-06
Iter: 1828 loss: 1.41645e-06
Iter: 1829 loss: 1.41621592e-06
Iter: 1830 loss: 1.41645364e-06
Iter: 1831 loss: 1.41613975e-06
Iter: 1832 loss: 1.4159433e-06
Iter: 1833 loss: 1.41599992e-06
Iter: 1834 loss: 1.41582177e-06
Iter: 1835 loss: 1.41559417e-06
Iter: 1836 loss: 1.41811279e-06
Iter: 1837 loss: 1.41555847e-06
Iter: 1838 loss: 1.41539567e-06
Iter: 1839 loss: 1.41598161e-06
Iter: 1840 loss: 1.41533474e-06
Iter: 1841 loss: 1.41525061e-06
Iter: 1842 loss: 1.41534724e-06
Iter: 1843 loss: 1.41515989e-06
Iter: 1844 loss: 1.41496457e-06
Iter: 1845 loss: 1.41579289e-06
Iter: 1846 loss: 1.41496673e-06
Iter: 1847 loss: 1.41487521e-06
Iter: 1848 loss: 1.41516375e-06
Iter: 1849 loss: 1.4148336e-06
Iter: 1850 loss: 1.41473777e-06
Iter: 1851 loss: 1.41468263e-06
Iter: 1852 loss: 1.41460168e-06
Iter: 1853 loss: 1.41441285e-06
Iter: 1854 loss: 1.41458497e-06
Iter: 1855 loss: 1.41432383e-06
Iter: 1856 loss: 1.41409851e-06
Iter: 1857 loss: 1.41581131e-06
Iter: 1858 loss: 1.41407759e-06
Iter: 1859 loss: 1.41396788e-06
Iter: 1860 loss: 1.41371697e-06
Iter: 1861 loss: 1.41709847e-06
Iter: 1862 loss: 1.41368616e-06
Iter: 1863 loss: 1.41346675e-06
Iter: 1864 loss: 1.4150346e-06
Iter: 1865 loss: 1.41338194e-06
Iter: 1866 loss: 1.41334681e-06
Iter: 1867 loss: 1.41329667e-06
Iter: 1868 loss: 1.41321402e-06
Iter: 1869 loss: 1.41296812e-06
Iter: 1870 loss: 1.41497026e-06
Iter: 1871 loss: 1.41291468e-06
Iter: 1872 loss: 1.41295095e-06
Iter: 1873 loss: 1.41281271e-06
Iter: 1874 loss: 1.41276291e-06
Iter: 1875 loss: 1.4125543e-06
Iter: 1876 loss: 1.41478722e-06
Iter: 1877 loss: 1.41254213e-06
Iter: 1878 loss: 1.41245778e-06
Iter: 1879 loss: 1.41247585e-06
Iter: 1880 loss: 1.41233431e-06
Iter: 1881 loss: 1.4124638e-06
Iter: 1882 loss: 1.41231499e-06
Iter: 1883 loss: 1.41225451e-06
Iter: 1884 loss: 1.41249075e-06
Iter: 1885 loss: 1.41218516e-06
Iter: 1886 loss: 1.41205862e-06
Iter: 1887 loss: 1.41197427e-06
Iter: 1888 loss: 1.41197006e-06
Iter: 1889 loss: 1.41178e-06
Iter: 1890 loss: 1.41186854e-06
Iter: 1891 loss: 1.41168437e-06
Iter: 1892 loss: 1.41160683e-06
Iter: 1893 loss: 1.41154214e-06
Iter: 1894 loss: 1.41140936e-06
Iter: 1895 loss: 1.41117039e-06
Iter: 1896 loss: 1.41480609e-06
Iter: 1897 loss: 1.41116357e-06
Iter: 1898 loss: 1.41096871e-06
Iter: 1899 loss: 1.41099736e-06
Iter: 1900 loss: 1.41077783e-06
Iter: 1901 loss: 1.41067835e-06
Iter: 1902 loss: 1.41060525e-06
Iter: 1903 loss: 1.41040459e-06
Iter: 1904 loss: 1.41112105e-06
Iter: 1905 loss: 1.41036196e-06
Iter: 1906 loss: 1.4100458e-06
Iter: 1907 loss: 1.41066823e-06
Iter: 1908 loss: 1.40991165e-06
Iter: 1909 loss: 1.40975806e-06
Iter: 1910 loss: 1.40968041e-06
Iter: 1911 loss: 1.40956899e-06
Iter: 1912 loss: 1.40946486e-06
Iter: 1913 loss: 1.40945849e-06
Iter: 1914 loss: 1.40927864e-06
Iter: 1915 loss: 1.4090657e-06
Iter: 1916 loss: 1.40907923e-06
Iter: 1917 loss: 1.40880047e-06
Iter: 1918 loss: 1.41042369e-06
Iter: 1919 loss: 1.40884765e-06
Iter: 1920 loss: 1.4086819e-06
Iter: 1921 loss: 1.41002954e-06
Iter: 1922 loss: 1.40865609e-06
Iter: 1923 loss: 1.40855673e-06
Iter: 1924 loss: 1.40832708e-06
Iter: 1925 loss: 1.4125327e-06
Iter: 1926 loss: 1.40834709e-06
Iter: 1927 loss: 1.40813052e-06
Iter: 1928 loss: 1.40916245e-06
Iter: 1929 loss: 1.40811244e-06
Iter: 1930 loss: 1.40797363e-06
Iter: 1931 loss: 1.40894792e-06
Iter: 1932 loss: 1.40792736e-06
Iter: 1933 loss: 1.40769862e-06
Iter: 1934 loss: 1.40771476e-06
Iter: 1935 loss: 1.40753036e-06
Iter: 1936 loss: 1.40736552e-06
Iter: 1937 loss: 1.40945372e-06
Iter: 1938 loss: 1.40741827e-06
Iter: 1939 loss: 1.40725149e-06
Iter: 1940 loss: 1.40705095e-06
Iter: 1941 loss: 1.40706572e-06
Iter: 1942 loss: 1.40690986e-06
Iter: 1943 loss: 1.40686359e-06
Iter: 1944 loss: 1.40662678e-06
Iter: 1945 loss: 1.40645204e-06
Iter: 1946 loss: 1.40638736e-06
Iter: 1947 loss: 1.40617658e-06
Iter: 1948 loss: 1.4069401e-06
Iter: 1949 loss: 1.4061327e-06
Iter: 1950 loss: 1.40586553e-06
Iter: 1951 loss: 1.40760335e-06
Iter: 1952 loss: 1.40584575e-06
Iter: 1953 loss: 1.40568795e-06
Iter: 1954 loss: 1.40552811e-06
Iter: 1955 loss: 1.4055064e-06
Iter: 1956 loss: 1.40544944e-06
Iter: 1957 loss: 1.40537645e-06
Iter: 1958 loss: 1.40528743e-06
Iter: 1959 loss: 1.40508951e-06
Iter: 1960 loss: 1.40793122e-06
Iter: 1961 loss: 1.40510292e-06
Iter: 1962 loss: 1.40484099e-06
Iter: 1963 loss: 1.40487919e-06
Iter: 1964 loss: 1.40467944e-06
Iter: 1965 loss: 1.40457632e-06
Iter: 1966 loss: 1.40455154e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ MODEL=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0/300_100_100_100_1
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output122/f1_psi1_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0.4 /home/mrdouglas/Manifold/experiments.final/output122/f1_psi1_phi0.4
+ date
Sun Nov  8 19:56:44 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0.4/300_100_100_100_1_epochs800 ']'
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0.4/300_100_100_100_1 ']'
+ LOAD=
+ python biholoNN_train.py --seed 1234 --function f1 --psi 1 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0.4/ --save_name 300_100_100_100_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 800 --loss_func weighted_MAPE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75bece1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75bfe26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75bf3d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75bfbdc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75be7ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75bf1fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75be49ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75be49c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75bd88378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75bd77400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75be10730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb751364f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7513678c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75bdb7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb751392ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7513b3bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7513bb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7512f2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75131b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75131bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7512a8158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7511bc840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7511f99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75118ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75119d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75125b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75121a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb751150950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb751150840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb751152268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75112c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb751057510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb751057488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75103d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb75100f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb7510cfea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.44127765
test_loss: 0.43651667
train_loss: 0.44134575
test_loss: 0.43622756
train_loss: 0.44143122
test_loss: 0.43588442
train_loss: 0.4392025
test_loss: 0.43548858
train_loss: 0.4508046
test_loss: 0.43503454
train_loss: 0.4381448
test_loss: 0.43457526
train_loss: 0.44440427
test_loss: 0.4339613
train_loss: 0.4359228
test_loss: 0.43334532
train_loss: 0.4300864
test_loss: 0.43258905
train_loss: 0.43082583
test_loss: 0.43194026
train_loss: 0.42426977
test_loss: 0.43109956
train_loss: 0.43402848
test_loss: 0.43022206
train_loss: 0.42117923
test_loss: 0.42930934
train_loss: 0.4279049
test_loss: 0.42833602
train_loss: 0.42782003
test_loss: 0.4272718
train_loss: 0.4267766
test_loss: 0.4261984
train_loss: 0.43591577
test_loss: 0.4250202
train_loss: 0.4314428
test_loss: 0.42373314
train_loss: 0.42646182
test_loss: 0.42240894
train_loss: 0.42756683
test_loss: 0.42102918
train_loss: 0.42590103
test_loss: 0.41957352
train_loss: 0.42160496
test_loss: 0.41806462
train_loss: 0.42288503
test_loss: 0.4164077
train_loss: 0.4159822
test_loss: 0.41474524
train_loss: 0.41280508
test_loss: 0.41297334
train_loss: 0.41159248
test_loss: 0.41106746
train_loss: 0.41322076
test_loss: 0.409097
train_loss: 0.4092699
test_loss: 0.40710133
train_loss: 0.41112846
test_loss: 0.4049277
train_loss: 0.4055152
test_loss: 0.40270787
train_loss: 0.40122253
test_loss: 0.4003132
train_loss: 0.4006893
test_loss: 0.39789003
train_loss: 0.39178362
test_loss: 0.3953609
train_loss: 0.3971035
test_loss: 0.39267766
train_loss: 0.39301047
test_loss: 0.38988155
train_loss: 0.38629436
test_loss: 0.38696748
train_loss: 0.38631412
test_loss: 0.38392386
train_loss: 0.38124108
test_loss: 0.3807927
train_loss: 0.36959252
test_loss: 0.37748027
train_loss: 0.37602445
test_loss: 0.3740508
train_loss: 0.3750849
test_loss: 0.37044448
train_loss: 0.37075838
test_loss: 0.3667276
train_loss: 0.36371464
test_loss: 0.36280283
train_loss: 0.36653817
test_loss: 0.3587804
train_loss: 0.3553293
test_loss: 0.35452294
train_loss: 0.35101312
test_loss: 0.35014248
train_loss: 0.3434767
test_loss: 0.34554988
train_loss: 0.34291634
test_loss: 0.34078085
train_loss: 0.33373314
test_loss: 0.3357815
train_loss: 0.3304327
test_loss: 0.33060017
train_loss: 0.32273433
test_loss: 0.32519147
train_loss: 0.32296628
test_loss: 0.31953666
train_loss: 0.31378096
test_loss: 0.31363449
train_loss: 0.30934417
test_loss: 0.30745348
train_loss: 0.3027515
test_loss: 0.30103043
train_loss: 0.29678118
test_loss: 0.29435074
train_loss: 0.28724352
test_loss: 0.28735515
train_loss: 0.2846821
test_loss: 0.2800246
train_loss: 0.26976618
test_loss: 0.27239183
train_loss: 0.26196843
test_loss: 0.26444262
train_loss: 0.26261103
test_loss: 0.25610557
train_loss: 0.24875873
test_loss: 0.24739026
train_loss: 0.2390054
test_loss: 0.23828101
train_loss: 0.23017076
test_loss: 0.22879435
train_loss: 0.21818106
test_loss: 0.21896532
train_loss: 0.21055555
test_loss: 0.20891306
train_loss: 0.19913352
test_loss: 0.1987248
train_loss: 0.1904313
test_loss: 0.1885345
train_loss: 0.18105067
test_loss: 0.17838319
train_loss: 0.16995332
test_loss: 0.16833283
train_loss: 0.15781268
test_loss: 0.15844831
train_loss: 0.15068656
test_loss: 0.14882794
train_loss: 0.13940075
test_loss: 0.13955149
train_loss: 0.12999716
test_loss: 0.13069394
train_loss: 0.12316794
test_loss: 0.12238507
train_loss: 0.11442575
test_loss: 0.11465742
train_loss: 0.10631879
test_loss: 0.107555985
train_loss: 0.100761704
test_loss: 0.10115335
train_loss: 0.09676459
test_loss: 0.09546822
train_loss: 0.09123272
test_loss: 0.09053306
train_loss: 0.0835167
test_loss: 0.08629572
train_loss: 0.082956314
test_loss: 0.08271594
train_loss: 0.07797676
test_loss: 0.07969333
train_loss: 0.07606561
test_loss: 0.077178195
train_loss: 0.07546424
test_loss: 0.07508354
train_loss: 0.07277482
test_loss: 0.073327504
train_loss: 0.071432754
test_loss: 0.07188529
train_loss: 0.07014045
test_loss: 0.070662335
train_loss: 0.068405725
test_loss: 0.06962886
train_loss: 0.06767692
test_loss: 0.06875313
train_loss: 0.066889495
test_loss: 0.06800811
train_loss: 0.06663463
test_loss: 0.06736677
train_loss: 0.06562872
test_loss: 0.06681621
train_loss: 0.06564246
test_loss: 0.0663445
train_loss: 0.065145865
test_loss: 0.065935805
train_loss: 0.06475351
test_loss: 0.06556251
train_loss: 0.064174496
test_loss: 0.06522652
train_loss: 0.06343471
test_loss: 0.06494065
train_loss: 0.0635238
test_loss: 0.06467362
train_loss: 0.06380566
test_loss: 0.064455956
train_loss: 0.06395803
test_loss: 0.064247005
train_loss: 0.064047486
test_loss: 0.06405019
train_loss: 0.06261392
test_loss: 0.06390397
train_loss: 0.06348788
test_loss: 0.063741274
train_loss: 0.06402075
test_loss: 0.06361076
train_loss: 0.063078724
test_loss: 0.06346707
train_loss: 0.06199633
test_loss: 0.06336538
train_loss: 0.06292041
test_loss: 0.06325591
train_loss: 0.06300865
test_loss: 0.06316579
train_loss: 0.06282667
test_loss: 0.06307393
train_loss: 0.06255764
test_loss: 0.0629716
train_loss: 0.061494455
test_loss: 0.06289517
train_loss: 0.062685154
test_loss: 0.06282431
train_loss: 0.062087536
test_loss: 0.062726445
train_loss: 0.0634921
test_loss: 0.06265862
train_loss: 0.06303146
test_loss: 0.062584795
train_loss: 0.060756966
test_loss: 0.06251873
train_loss: 0.061891917
test_loss: 0.062443916
train_loss: 0.062305145
test_loss: 0.06237076
train_loss: 0.06169349
test_loss: 0.062292125
train_loss: 0.061274175
test_loss: 0.062244687
train_loss: 0.06082006
test_loss: 0.062148694
train_loss: 0.062386952
test_loss: 0.062081277
train_loss: 0.062309872
test_loss: 0.062011883
train_loss: 0.061726026
test_loss: 0.06185883
train_loss: 0.061474055
test_loss: 0.061797902
train_loss: 0.058567636
test_loss: 0.061629545
train_loss: 0.062402666
test_loss: 0.06151411
train_loss: 0.06044663
test_loss: 0.061387055
train_loss: 0.061486587
test_loss: 0.061232094
train_loss: 0.060368285
test_loss: 0.06098948
train_loss: 0.059872184
test_loss: 0.06082396
train_loss: 0.06102196
test_loss: 0.060563147
train_loss: 0.058956034
test_loss: 0.060259517
train_loss: 0.05865223
test_loss: 0.05991858
train_loss: 0.058559828
test_loss: 0.059446014
train_loss: 0.058268394
test_loss: 0.05898144
train_loss: 0.05665257
test_loss: 0.058394186
train_loss: 0.058537662
test_loss: 0.0577134
train_loss: 0.055675186
test_loss: 0.056914527
train_loss: 0.055366363
test_loss: 0.055960715
train_loss: 0.054111354
test_loss: 0.05477962
train_loss: 0.05259578
test_loss: 0.053501833
train_loss: 0.05158126
test_loss: 0.052111387
train_loss: 0.048592538
test_loss: 0.050471246
train_loss: 0.046775557
test_loss: 0.04853931
train_loss: 0.045233805
test_loss: 0.046359442
train_loss: 0.041808452
test_loss: 0.043825794
train_loss: 0.040056203
test_loss: 0.04086959
train_loss: 0.036331885
test_loss: 0.03738732
train_loss: 0.031749524
test_loss: 0.033579145
train_loss: 0.028600503
test_loss: 0.029702017
train_loss: 0.02619033
test_loss: 0.026526036
train_loss: 0.024421364
test_loss: 0.024232926
train_loss: 0.022237696
test_loss: 0.022797527
train_loss: 0.021616455
test_loss: 0.021659557
train_loss: 0.020693531
test_loss: 0.020877901
train_loss: 0.019333491
test_loss: 0.020250473
train_loss: 0.019770563
test_loss: 0.019829728
train_loss: 0.018960875
test_loss: 0.019347712
+ echo
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output122/f1_psi1_phi0.4/300_100_100_100_1 ']'
+ python biholoNN_train.py --seed 1234 --n_pairs 4000 --load_model /home/mrdouglas/Manifold/experiments.final/output120/f1_psi1_phi0.4/300_100_100_100_1 --optimizer lbfgs --function f1 --psi 1 --phi 0.4 --layers 300_100_100_100_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output122/f1_psi1_phi0.4/ --save_name 300_100_100_100_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_100_100_100_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10db2c2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10da5a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10db7c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10db7ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10dab4730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10dab4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d9f89d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d9a62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d9a68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d9f81e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d901f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d901510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d93a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d8f6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d8979d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d8b8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d8b2598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d84ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d829840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d829950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d7d5d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d7d5950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d783ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d76bae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd10d76ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0cea6aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0cea32620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0cea59ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0cea59840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0cea0c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0cea0c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0ce9dc400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0ce97a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0ce99bbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0a80cd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fd0a80cff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.000658708857
Iter: 2 loss: 0.000768418191
Iter: 3 loss: 0.000643813517
Iter: 4 loss: 0.000634828233
Iter: 5 loss: 0.000660436461
Iter: 6 loss: 0.000631984789
Iter: 7 loss: 0.000628194422
Iter: 8 loss: 0.000662887935
Iter: 9 loss: 0.000628022826
Iter: 10 loss: 0.000625852
Iter: 11 loss: 0.000623412488
Iter: 12 loss: 0.000623082509
Iter: 13 loss: 0.00062073255
Iter: 14 loss: 0.000648671936
Iter: 15 loss: 0.000620702282
Iter: 16 loss: 0.000618694117
Iter: 17 loss: 0.000620644423
Iter: 18 loss: 0.000617549638
Iter: 19 loss: 0.000615307246
Iter: 20 loss: 0.000624903943
Iter: 21 loss: 0.000614835182
Iter: 22 loss: 0.000612557225
Iter: 23 loss: 0.000611797
Iter: 24 loss: 0.000610483228
Iter: 25 loss: 0.000606853107
Iter: 26 loss: 0.000605052512
Iter: 27 loss: 0.000603317807
Iter: 28 loss: 0.000598368584
Iter: 29 loss: 0.000626473338
Iter: 30 loss: 0.000597684644
Iter: 31 loss: 0.000594927405
Iter: 32 loss: 0.000594826823
Iter: 33 loss: 0.000593114179
Iter: 34 loss: 0.000590647745
Iter: 35 loss: 0.000590574
Iter: 36 loss: 0.00058916409
Iter: 37 loss: 0.000589045638
Iter: 38 loss: 0.000587366521
Iter: 39 loss: 0.000586589449
Iter: 40 loss: 0.000585757545
Iter: 41 loss: 0.000584164402
Iter: 42 loss: 0.000597971
Iter: 43 loss: 0.000584081921
Iter: 44 loss: 0.000582640525
Iter: 45 loss: 0.00058146473
Iter: 46 loss: 0.000581043423
Iter: 47 loss: 0.00057937461
Iter: 48 loss: 0.000594837416
Iter: 49 loss: 0.000579300104
Iter: 50 loss: 0.000577600847
Iter: 51 loss: 0.000577429
Iter: 52 loss: 0.000576191931
Iter: 53 loss: 0.000574177073
Iter: 54 loss: 0.000586839
Iter: 55 loss: 0.000573949772
Iter: 56 loss: 0.000572018791
Iter: 57 loss: 0.000572023913
Iter: 58 loss: 0.000570480363
Iter: 59 loss: 0.000568294257
Iter: 60 loss: 0.000568756077
Iter: 61 loss: 0.000566686504
Iter: 62 loss: 0.000564205169
Iter: 63 loss: 0.00058023521
Iter: 64 loss: 0.000563934038
Iter: 65 loss: 0.00056170905
Iter: 66 loss: 0.000581042725
Iter: 67 loss: 0.000561595079
Iter: 68 loss: 0.00056010671
Iter: 69 loss: 0.000559125911
Iter: 70 loss: 0.000558563275
Iter: 71 loss: 0.000557610765
Iter: 72 loss: 0.000557314605
Iter: 73 loss: 0.000556387589
Iter: 74 loss: 0.000554828
Iter: 75 loss: 0.000554824481
Iter: 76 loss: 0.000553164282
Iter: 77 loss: 0.000563555048
Iter: 78 loss: 0.000552974874
Iter: 79 loss: 0.000551251927
Iter: 80 loss: 0.000553933787
Iter: 81 loss: 0.000550446566
Iter: 82 loss: 0.000548811
Iter: 83 loss: 0.000550840399
Iter: 84 loss: 0.000547948875
Iter: 85 loss: 0.000545614574
Iter: 86 loss: 0.000559864624
Iter: 87 loss: 0.000545334595
Iter: 88 loss: 0.000543757458
Iter: 89 loss: 0.000542389636
Iter: 90 loss: 0.000541968388
Iter: 91 loss: 0.000539680943
Iter: 92 loss: 0.000570927339
Iter: 93 loss: 0.000539666798
Iter: 94 loss: 0.000537558866
Iter: 95 loss: 0.000539216155
Iter: 96 loss: 0.000536284933
Iter: 97 loss: 0.000534085673
Iter: 98 loss: 0.000534458377
Iter: 99 loss: 0.000532414881
Iter: 100 loss: 0.000530363061
Iter: 101 loss: 0.000556075713
Iter: 102 loss: 0.000530344667
Iter: 103 loss: 0.000528349774
Iter: 104 loss: 0.000534043415
Iter: 105 loss: 0.000527726253
Iter: 106 loss: 0.000525787
Iter: 107 loss: 0.000544423296
Iter: 108 loss: 0.000525712967
Iter: 109 loss: 0.000524552481
Iter: 110 loss: 0.000525160518
Iter: 111 loss: 0.000523770461
Iter: 112 loss: 0.000521599664
Iter: 113 loss: 0.000523257884
Iter: 114 loss: 0.00052029232
Iter: 115 loss: 0.000518381421
Iter: 116 loss: 0.000518378045
Iter: 117 loss: 0.00051685411
Iter: 118 loss: 0.000516018947
Iter: 119 loss: 0.000515337
Iter: 120 loss: 0.000513088773
Iter: 121 loss: 0.000512968458
Iter: 122 loss: 0.000511261402
Iter: 123 loss: 0.00050910539
Iter: 124 loss: 0.000510595099
Iter: 125 loss: 0.000507740478
Iter: 126 loss: 0.00050566718
Iter: 127 loss: 0.000510570244
Iter: 128 loss: 0.000504911761
Iter: 129 loss: 0.000502484152
Iter: 130 loss: 0.00051700254
Iter: 131 loss: 0.000502190494
Iter: 132 loss: 0.000500286289
Iter: 133 loss: 0.000501278
Iter: 134 loss: 0.000498941517
Iter: 135 loss: 0.000496884284
Iter: 136 loss: 0.000504789467
Iter: 137 loss: 0.000496407
Iter: 138 loss: 0.000494472682
Iter: 139 loss: 0.000495556393
Iter: 140 loss: 0.000493213534
Iter: 141 loss: 0.000491420389
Iter: 142 loss: 0.000509006553
Iter: 143 loss: 0.000491332437
Iter: 144 loss: 0.000489541504
Iter: 145 loss: 0.0004971124
Iter: 146 loss: 0.000489153899
Iter: 147 loss: 0.000487403275
Iter: 148 loss: 0.000491410727
Iter: 149 loss: 0.000486750883
Iter: 150 loss: 0.000485318073
Iter: 151 loss: 0.000508131692
Iter: 152 loss: 0.000485268974
Iter: 153 loss: 0.000483958691
Iter: 154 loss: 0.000482686213
Iter: 155 loss: 0.000482405943
Iter: 156 loss: 0.000480029266
Iter: 157 loss: 0.000514091691
Iter: 158 loss: 0.000480008748
Iter: 159 loss: 0.000477984344
Iter: 160 loss: 0.000492638617
Iter: 161 loss: 0.00047782046
Iter: 162 loss: 0.000476822373
Iter: 163 loss: 0.000477450114
Iter: 164 loss: 0.000476117217
Iter: 165 loss: 0.000474612578
Iter: 166 loss: 0.000478054222
Iter: 167 loss: 0.00047406496
Iter: 168 loss: 0.000471896463
Iter: 169 loss: 0.00048591051
Iter: 170 loss: 0.000471650885
Iter: 171 loss: 0.000469906197
Iter: 172 loss: 0.000484386517
Iter: 173 loss: 0.000469802471
Iter: 174 loss: 0.000468831102
Iter: 175 loss: 0.00046797446
Iter: 176 loss: 0.00046772178
Iter: 177 loss: 0.000466687605
Iter: 178 loss: 0.000466640049
Iter: 179 loss: 0.000465760269
Iter: 180 loss: 0.000469095539
Iter: 181 loss: 0.000465550402
Iter: 182 loss: 0.000464747718
Iter: 183 loss: 0.000463658944
Iter: 184 loss: 0.000463604665
Iter: 185 loss: 0.000462666154
Iter: 186 loss: 0.000460965035
Iter: 187 loss: 0.000504038704
Iter: 188 loss: 0.000460964744
Iter: 189 loss: 0.000459249277
Iter: 190 loss: 0.000466007594
Iter: 191 loss: 0.000458861672
Iter: 192 loss: 0.000457338348
Iter: 193 loss: 0.00045793326
Iter: 194 loss: 0.000456288282
Iter: 195 loss: 0.000456467242
Iter: 196 loss: 0.000455426518
Iter: 197 loss: 0.000454885536
Iter: 198 loss: 0.000453997287
Iter: 199 loss: 0.000453993067
Iter: 200 loss: 0.000452316424
Iter: 201 loss: 0.000455545756
Iter: 202 loss: 0.000451594417
Iter: 203 loss: 0.000449921703
Iter: 204 loss: 0.000457215909
Iter: 205 loss: 0.000449586078
Iter: 206 loss: 0.000448020844
Iter: 207 loss: 0.000448579231
Iter: 208 loss: 0.000446837745
Iter: 209 loss: 0.000445883052
Iter: 210 loss: 0.000449636515
Iter: 211 loss: 0.000445669546
Iter: 212 loss: 0.000444908917
Iter: 213 loss: 0.000446628925
Iter: 214 loss: 0.000444571808
Iter: 215 loss: 0.000443723926
Iter: 216 loss: 0.000445665064
Iter: 217 loss: 0.000443406432
Iter: 218 loss: 0.000442510122
Iter: 219 loss: 0.000442504068
Iter: 220 loss: 0.000441519252
Iter: 221 loss: 0.000441240263
Iter: 222 loss: 0.000440633041
Iter: 223 loss: 0.000439537776
Iter: 224 loss: 0.00044196556
Iter: 225 loss: 0.00043911906
Iter: 226 loss: 0.000438443734
Iter: 227 loss: 0.000439091877
Iter: 228 loss: 0.000438001967
Iter: 229 loss: 0.000437033596
Iter: 230 loss: 0.000439040858
Iter: 231 loss: 0.000436648756
Iter: 232 loss: 0.000435956841
Iter: 233 loss: 0.000435906055
Iter: 234 loss: 0.000435382331
Iter: 235 loss: 0.00043430127
Iter: 236 loss: 0.000440788863
Iter: 237 loss: 0.000434167916
Iter: 238 loss: 0.000433497247
Iter: 239 loss: 0.000434145215
Iter: 240 loss: 0.00043311491
Iter: 241 loss: 0.000432495959
Iter: 242 loss: 0.000433917448
Iter: 243 loss: 0.000432271278
Iter: 244 loss: 0.000431388326
Iter: 245 loss: 0.000430823653
Iter: 246 loss: 0.000430480664
Iter: 247 loss: 0.000429726846
Iter: 248 loss: 0.00042934221
Iter: 249 loss: 0.000428709609
Iter: 250 loss: 0.000431144086
Iter: 251 loss: 0.000428531057
Iter: 252 loss: 0.000427947933
Iter: 253 loss: 0.000430399552
Iter: 254 loss: 0.000427834893
Iter: 255 loss: 0.0004272238
Iter: 256 loss: 0.000431549182
Iter: 257 loss: 0.000427162624
Iter: 258 loss: 0.000426536775
Iter: 259 loss: 0.000429231615
Iter: 260 loss: 0.000426407787
Iter: 261 loss: 0.000425819861
Iter: 262 loss: 0.000425088656
Iter: 263 loss: 0.000425029895
Iter: 264 loss: 0.000424174068
Iter: 265 loss: 0.000428767496
Iter: 266 loss: 0.000424044847
Iter: 267 loss: 0.000423555874
Iter: 268 loss: 0.000423061458
Iter: 269 loss: 0.000422964833
Iter: 270 loss: 0.000422293175
Iter: 271 loss: 0.000424087601
Iter: 272 loss: 0.000422071083
Iter: 273 loss: 0.000421580626
Iter: 274 loss: 0.000425796519
Iter: 275 loss: 0.00042155353
Iter: 276 loss: 0.000421315373
Iter: 277 loss: 0.000420846045
Iter: 278 loss: 0.000430184678
Iter: 279 loss: 0.00042084296
Iter: 280 loss: 0.000420240394
Iter: 281 loss: 0.000420058117
Iter: 282 loss: 0.00041969915
Iter: 283 loss: 0.000419172517
Iter: 284 loss: 0.000423317775
Iter: 285 loss: 0.000419146905
Iter: 286 loss: 0.000418538228
Iter: 287 loss: 0.000418519834
Iter: 288 loss: 0.000418397307
Iter: 289 loss: 0.00041805298
Iter: 290 loss: 0.000419734337
Iter: 291 loss: 0.000417939504
Iter: 292 loss: 0.000417098956
Iter: 293 loss: 0.000418800977
Iter: 294 loss: 0.000416759227
Iter: 295 loss: 0.000415925926
Iter: 296 loss: 0.000421967561
Iter: 297 loss: 0.000415831106
Iter: 298 loss: 0.00041539385
Iter: 299 loss: 0.00041493235
Iter: 300 loss: 0.000414855138
Iter: 301 loss: 0.000414009555
Iter: 302 loss: 0.000416418916
Iter: 303 loss: 0.000413686415
Iter: 304 loss: 0.000413343485
Iter: 305 loss: 0.000415850023
Iter: 306 loss: 0.000413315604
Iter: 307 loss: 0.000413184112
Iter: 308 loss: 0.000413098314
Iter: 309 loss: 0.000413044065
Iter: 310 loss: 0.000412590452
Iter: 311 loss: 0.00041405909
Iter: 312 loss: 0.000412464084
Iter: 313 loss: 0.000412034307
Iter: 314 loss: 0.000417404284
Iter: 315 loss: 0.000412030815
Iter: 316 loss: 0.000411881279
Iter: 317 loss: 0.000411871646
Iter: 318 loss: 0.000411752495
Iter: 319 loss: 0.000411474437
Iter: 320 loss: 0.000411115238
Iter: 321 loss: 0.000411091838
Iter: 322 loss: 0.000410726527
Iter: 323 loss: 0.000411366287
Iter: 324 loss: 0.000410564273
Iter: 325 loss: 0.000410343229
Iter: 326 loss: 0.000410346867
Iter: 327 loss: 0.000410169567
Iter: 328 loss: 0.000409809087
Iter: 329 loss: 0.000409798202
Iter: 330 loss: 0.000409516681
Iter: 331 loss: 0.000409292115
Iter: 332 loss: 0.000409206317
Iter: 333 loss: 0.000408989203
Iter: 334 loss: 0.000408806023
Iter: 335 loss: 0.000408749242
Iter: 336 loss: 0.000408275868
Iter: 337 loss: 0.000411581597
Iter: 338 loss: 0.000408233842
Iter: 339 loss: 0.000407568121
Iter: 340 loss: 0.000414989074
Iter: 341 loss: 0.000407542568
Iter: 342 loss: 0.00040744475
Iter: 343 loss: 0.000407314568
Iter: 344 loss: 0.000407307
Iter: 345 loss: 0.00040707388
Iter: 346 loss: 0.000406931213
Iter: 347 loss: 0.000406834122
Iter: 348 loss: 0.000406600244
Iter: 349 loss: 0.000406483712
Iter: 350 loss: 0.000406168314
Iter: 351 loss: 0.000406137551
Iter: 352 loss: 0.000405969447
Iter: 353 loss: 0.000405683764
Iter: 354 loss: 0.000405682717
Iter: 355 loss: 0.000405296858
Iter: 356 loss: 0.000404673221
Iter: 357 loss: 0.000404671126
Iter: 358 loss: 0.000403982762
Iter: 359 loss: 0.000406138483
Iter: 360 loss: 0.000403782935
Iter: 361 loss: 0.000403422018
Iter: 362 loss: 0.000403414451
Iter: 363 loss: 0.000402979087
Iter: 364 loss: 0.000404823746
Iter: 365 loss: 0.000402890204
Iter: 366 loss: 0.000402611447
Iter: 367 loss: 0.000403213577
Iter: 368 loss: 0.000402491889
Iter: 369 loss: 0.000402331265
Iter: 370 loss: 0.000402154634
Iter: 371 loss: 0.000402127887
Iter: 372 loss: 0.000401858124
Iter: 373 loss: 0.000402633101
Iter: 374 loss: 0.000401769881
Iter: 375 loss: 0.00040163481
Iter: 376 loss: 0.000401649362
Iter: 377 loss: 0.000401532918
Iter: 378 loss: 0.000401261816
Iter: 379 loss: 0.000400827063
Iter: 380 loss: 0.000400824531
Iter: 381 loss: 0.000400298188
Iter: 382 loss: 0.00040097523
Iter: 383 loss: 0.000400031393
Iter: 384 loss: 0.000399731682
Iter: 385 loss: 0.000399727956
Iter: 386 loss: 0.000399561104
Iter: 387 loss: 0.000399425509
Iter: 388 loss: 0.000399377634
Iter: 389 loss: 0.000398973498
Iter: 390 loss: 0.000399007782
Iter: 391 loss: 0.000398657401
Iter: 392 loss: 0.000398495729
Iter: 393 loss: 0.000398268399
Iter: 394 loss: 0.000398260308
Iter: 395 loss: 0.000398136675
Iter: 396 loss: 0.000398230623
Iter: 397 loss: 0.000398063392
Iter: 398 loss: 0.000397802825
Iter: 399 loss: 0.00039730288
Iter: 400 loss: 0.000408881984
Iter: 401 loss: 0.000397298951
Iter: 402 loss: 0.000396636
Iter: 403 loss: 0.000398386241
Iter: 404 loss: 0.000396417017
Iter: 405 loss: 0.000397310796
Iter: 406 loss: 0.000396262622
Iter: 407 loss: 0.000396024145
Iter: 408 loss: 0.000395464303
Iter: 409 loss: 0.000402070786
Iter: 410 loss: 0.000395416748
Iter: 411 loss: 0.000394976232
Iter: 412 loss: 0.000395166
Iter: 413 loss: 0.000394677103
Iter: 414 loss: 0.000394461676
Iter: 415 loss: 0.000394336384
Iter: 416 loss: 0.000394244096
Iter: 417 loss: 0.000393996655
Iter: 418 loss: 0.000395128765
Iter: 419 loss: 0.00039395073
Iter: 420 loss: 0.000394026516
Iter: 421 loss: 0.000393696711
Iter: 422 loss: 0.000393508293
Iter: 423 loss: 0.000393963739
Iter: 424 loss: 0.000393438677
Iter: 425 loss: 0.000393370341
Iter: 426 loss: 0.000393246126
Iter: 427 loss: 0.000396180374
Iter: 428 loss: 0.000393243623
Iter: 429 loss: 0.000393257738
Iter: 430 loss: 0.000393000577
Iter: 431 loss: 0.000392665126
Iter: 432 loss: 0.000393159775
Iter: 433 loss: 0.00039250619
Iter: 434 loss: 0.000392309681
Iter: 435 loss: 0.000391900045
Iter: 436 loss: 0.000400385412
Iter: 437 loss: 0.000391892361
Iter: 438 loss: 0.000391453796
Iter: 439 loss: 0.000392282789
Iter: 440 loss: 0.000391278358
Iter: 441 loss: 0.00039109966
Iter: 442 loss: 0.000390955189
Iter: 443 loss: 0.000390901172
Iter: 444 loss: 0.00039065443
Iter: 445 loss: 0.000390679343
Iter: 446 loss: 0.000390463858
Iter: 447 loss: 0.000390289526
Iter: 448 loss: 0.000392104965
Iter: 449 loss: 0.000390286645
Iter: 450 loss: 0.000389964815
Iter: 451 loss: 0.000390448113
Iter: 452 loss: 0.000389813154
Iter: 453 loss: 0.00038955381
Iter: 454 loss: 0.000389103661
Iter: 455 loss: 0.000389104
Iter: 456 loss: 0.000388841843
Iter: 457 loss: 0.000390028232
Iter: 458 loss: 0.000388791901
Iter: 459 loss: 0.000388594723
Iter: 460 loss: 0.000388871413
Iter: 461 loss: 0.000388493936
Iter: 462 loss: 0.0003882499
Iter: 463 loss: 0.000388719287
Iter: 464 loss: 0.000388146902
Iter: 465 loss: 0.000387790642
Iter: 466 loss: 0.000387733628
Iter: 467 loss: 0.000388748042
Iter: 468 loss: 0.00038747536
Iter: 469 loss: 0.000387333392
Iter: 470 loss: 0.000386970292
Iter: 471 loss: 0.000390159083
Iter: 472 loss: 0.000386914
Iter: 473 loss: 0.000386668369
Iter: 474 loss: 0.00038666738
Iter: 475 loss: 0.000386554602
Iter: 476 loss: 0.00038680993
Iter: 477 loss: 0.000386515225
Iter: 478 loss: 0.000386409432
Iter: 479 loss: 0.000386176864
Iter: 480 loss: 0.000389565132
Iter: 481 loss: 0.000386165804
Iter: 482 loss: 0.000385450781
Iter: 483 loss: 0.000390887668
Iter: 484 loss: 0.000385397114
Iter: 485 loss: 0.000386077503
Iter: 486 loss: 0.000385147228
Iter: 487 loss: 0.000384897285
Iter: 488 loss: 0.000388525717
Iter: 489 loss: 0.000384896819
Iter: 490 loss: 0.000384811312
Iter: 491 loss: 0.000384542858
Iter: 492 loss: 0.000384879153
Iter: 493 loss: 0.000384339015
Iter: 494 loss: 0.000383961713
Iter: 495 loss: 0.000384294515
Iter: 496 loss: 0.000383736275
Iter: 497 loss: 0.000383625593
Iter: 498 loss: 0.000383573235
Iter: 499 loss: 0.000383514882
Iter: 500 loss: 0.000383440696
Iter: 501 loss: 0.000383268518
Iter: 502 loss: 0.000385368883
Iter: 503 loss: 0.000383256178
Iter: 504 loss: 0.000382960017
Iter: 505 loss: 0.00038249424
Iter: 506 loss: 0.000382487924
Iter: 507 loss: 0.000382100436
Iter: 508 loss: 0.000384316023
Iter: 509 loss: 0.000382048427
Iter: 510 loss: 0.000382226484
Iter: 511 loss: 0.000381893129
Iter: 512 loss: 0.000381794
Iter: 513 loss: 0.000381587306
Iter: 514 loss: 0.00038522933
Iter: 515 loss: 0.000381584076
Iter: 516 loss: 0.000381442776
Iter: 517 loss: 0.000381484773
Iter: 518 loss: 0.000381340913
Iter: 519 loss: 0.000381181628
Iter: 520 loss: 0.000380878133
Iter: 521 loss: 0.000388362561
Iter: 522 loss: 0.000380878104
Iter: 523 loss: 0.000380779064
Iter: 524 loss: 0.000381080201
Iter: 525 loss: 0.000380751677
Iter: 526 loss: 0.000380699872
Iter: 527 loss: 0.000380549056
Iter: 528 loss: 0.000381050922
Iter: 529 loss: 0.000380479905
Iter: 530 loss: 0.000380240468
Iter: 531 loss: 0.000381755963
Iter: 532 loss: 0.000380213809
Iter: 533 loss: 0.000379764417
Iter: 534 loss: 0.000383619685
Iter: 535 loss: 0.000379741075
Iter: 536 loss: 0.000379610196
Iter: 537 loss: 0.000380302
Iter: 538 loss: 0.000379589386
Iter: 539 loss: 0.000379402481
Iter: 540 loss: 0.000379394041
Iter: 541 loss: 0.000379247736
Iter: 542 loss: 0.000379179372
Iter: 543 loss: 0.000379112491
Iter: 544 loss: 0.000378971919
Iter: 545 loss: 0.000378929777
Iter: 546 loss: 0.000378848083
Iter: 547 loss: 0.000378701661
Iter: 548 loss: 0.000378403551
Iter: 549 loss: 0.000383685401
Iter: 550 loss: 0.000378396886
Iter: 551 loss: 0.000378852
Iter: 552 loss: 0.000378078315
Iter: 553 loss: 0.00037794048
Iter: 554 loss: 0.000377907767
Iter: 555 loss: 0.000377816963
Iter: 556 loss: 0.000377737335
Iter: 557 loss: 0.000377724791
Iter: 558 loss: 0.00037767092
Iter: 559 loss: 0.000377528486
Iter: 560 loss: 0.000377079414
Iter: 561 loss: 0.000377731165
Iter: 562 loss: 0.000376759854
Iter: 563 loss: 0.000376262906
Iter: 564 loss: 0.000383076054
Iter: 565 loss: 0.000376260898
Iter: 566 loss: 0.000376659562
Iter: 567 loss: 0.000375959818
Iter: 568 loss: 0.000375820877
Iter: 569 loss: 0.000375339063
Iter: 570 loss: 0.000374843599
Iter: 571 loss: 0.000374655414
Iter: 572 loss: 0.000381837599
Iter: 573 loss: 0.00037447986
Iter: 574 loss: 0.000374301511
Iter: 575 loss: 0.000375788397
Iter: 576 loss: 0.000374290423
Iter: 577 loss: 0.000374239346
Iter: 578 loss: 0.00037405183
Iter: 579 loss: 0.00037348285
Iter: 580 loss: 0.000379890495
Iter: 581 loss: 0.000373423682
Iter: 582 loss: 0.000373135961
Iter: 583 loss: 0.000375054486
Iter: 584 loss: 0.000373107672
Iter: 585 loss: 0.000373054703
Iter: 586 loss: 0.000372901617
Iter: 587 loss: 0.000373481686
Iter: 588 loss: 0.000372834184
Iter: 589 loss: 0.000374234165
Iter: 590 loss: 0.000372755167
Iter: 591 loss: 0.00037255208
Iter: 592 loss: 0.000373716874
Iter: 593 loss: 0.000372524286
Iter: 594 loss: 0.000372312206
Iter: 595 loss: 0.000372081937
Iter: 596 loss: 0.0003720448
Iter: 597 loss: 0.000371359929
Iter: 598 loss: 0.00037628517
Iter: 599 loss: 0.000371296483
Iter: 600 loss: 0.00037098286
Iter: 601 loss: 0.000370979134
Iter: 602 loss: 0.000370742811
Iter: 603 loss: 0.000370481343
Iter: 604 loss: 0.000370442867
Iter: 605 loss: 0.000370103284
Iter: 606 loss: 0.000370063179
Iter: 607 loss: 0.000369817979
Iter: 608 loss: 0.000393757276
Iter: 609 loss: 0.000369760819
Iter: 610 loss: 0.000369699555
Iter: 611 loss: 0.000369697314
Iter: 612 loss: 0.000369649671
Iter: 613 loss: 0.000369560439
Iter: 614 loss: 0.000369406422
Iter: 615 loss: 0.000369408459
Iter: 616 loss: 0.000369077665
Iter: 617 loss: 0.000369275367
Iter: 618 loss: 0.000368868263
Iter: 619 loss: 0.000368994137
Iter: 620 loss: 0.00036870304
Iter: 621 loss: 0.000368662877
Iter: 622 loss: 0.000368731504
Iter: 623 loss: 0.000368644367
Iter: 624 loss: 0.000368578476
Iter: 625 loss: 0.000368376903
Iter: 626 loss: 0.000368916953
Iter: 627 loss: 0.00036826916
Iter: 628 loss: 0.000367988861
Iter: 629 loss: 0.000367593952
Iter: 630 loss: 0.000367578119
Iter: 631 loss: 0.000367625209
Iter: 632 loss: 0.000367315602
Iter: 633 loss: 0.000367248402
Iter: 634 loss: 0.000367154134
Iter: 635 loss: 0.000367150613
Iter: 636 loss: 0.00036701886
Iter: 637 loss: 0.000366744469
Iter: 638 loss: 0.000371373462
Iter: 639 loss: 0.000366739
Iter: 640 loss: 0.000366538414
Iter: 641 loss: 0.000366529741
Iter: 642 loss: 0.000366386666
Iter: 643 loss: 0.000368382025
Iter: 644 loss: 0.000366386113
Iter: 645 loss: 0.000366354245
Iter: 646 loss: 0.000366278109
Iter: 647 loss: 0.000367149187
Iter: 648 loss: 0.000366269232
Iter: 649 loss: 0.000366198772
Iter: 650 loss: 0.000366170105
Iter: 651 loss: 0.0003661316
Iter: 652 loss: 0.000365978747
Iter: 653 loss: 0.000365778804
Iter: 654 loss: 0.000365767046
Iter: 655 loss: 0.00036632549
Iter: 656 loss: 0.000365625572
Iter: 657 loss: 0.000365325337
Iter: 658 loss: 0.000365347019
Iter: 659 loss: 0.000365092594
Iter: 660 loss: 0.000364968961
Iter: 661 loss: 0.000364884501
Iter: 662 loss: 0.000364838634
Iter: 663 loss: 0.000364689768
Iter: 664 loss: 0.000364666863
Iter: 665 loss: 0.000364335254
Iter: 666 loss: 0.000364421518
Iter: 667 loss: 0.000364096079
Iter: 668 loss: 0.00036383851
Iter: 669 loss: 0.000364560023
Iter: 670 loss: 0.000363755506
Iter: 671 loss: 0.000363359373
Iter: 672 loss: 0.000363148
Iter: 673 loss: 0.000362967578
Iter: 674 loss: 0.000362808496
Iter: 675 loss: 0.000362755265
Iter: 676 loss: 0.000362496939
Iter: 677 loss: 0.000362464285
Iter: 678 loss: 0.000362425955
Iter: 679 loss: 0.000362283608
Iter: 680 loss: 0.000361958111
Iter: 681 loss: 0.000369817019
Iter: 682 loss: 0.000361957093
Iter: 683 loss: 0.000361206
Iter: 684 loss: 0.000366236956
Iter: 685 loss: 0.000361130398
Iter: 686 loss: 0.000360984413
Iter: 687 loss: 0.000360922888
Iter: 688 loss: 0.000360798382
Iter: 689 loss: 0.000362370454
Iter: 690 loss: 0.000360795151
Iter: 691 loss: 0.000360674399
Iter: 692 loss: 0.000360351783
Iter: 693 loss: 0.000362595747
Iter: 694 loss: 0.000360280392
Iter: 695 loss: 0.000360136124
Iter: 696 loss: 0.000360866252
Iter: 697 loss: 0.000360110018
Iter: 698 loss: 0.000360051927
Iter: 699 loss: 0.000359861326
Iter: 700 loss: 0.000360033737
Iter: 701 loss: 0.000359706
Iter: 702 loss: 0.000359597208
Iter: 703 loss: 0.000359852711
Iter: 704 loss: 0.000359555532
Iter: 705 loss: 0.000359441503
Iter: 706 loss: 0.000359067693
Iter: 707 loss: 0.000359324942
Iter: 708 loss: 0.000358744932
Iter: 709 loss: 0.000358426827
Iter: 710 loss: 0.000358425488
Iter: 711 loss: 0.00035819909
Iter: 712 loss: 0.000361125858
Iter: 713 loss: 0.000358200807
Iter: 714 loss: 0.000358015415
Iter: 715 loss: 0.000358729274
Iter: 716 loss: 0.00035796946
Iter: 717 loss: 0.000357943936
Iter: 718 loss: 0.000357850862
Iter: 719 loss: 0.000357769546
Iter: 720 loss: 0.000357726763
Iter: 721 loss: 0.000357242359
Iter: 722 loss: 0.000358795543
Iter: 723 loss: 0.000357105
Iter: 724 loss: 0.000358287245
Iter: 725 loss: 0.000356993667
Iter: 726 loss: 0.00035696011
Iter: 727 loss: 0.000356844073
Iter: 728 loss: 0.000356591889
Iter: 729 loss: 0.000356592354
Iter: 730 loss: 0.000356350327
Iter: 731 loss: 0.000358204299
Iter: 732 loss: 0.000356329954
Iter: 733 loss: 0.000356244855
Iter: 734 loss: 0.000356613018
Iter: 735 loss: 0.00035622876
Iter: 736 loss: 0.00035610341
Iter: 737 loss: 0.00035640772
Iter: 738 loss: 0.00035606054
Iter: 739 loss: 0.000355879194
Iter: 740 loss: 0.000355530443
Iter: 741 loss: 0.000362790481
Iter: 742 loss: 0.000355527445
Iter: 743 loss: 0.000355065829
Iter: 744 loss: 0.000355456723
Iter: 745 loss: 0.000354789547
Iter: 746 loss: 0.000354582327
Iter: 747 loss: 0.000356386
Iter: 748 loss: 0.00035456958
Iter: 749 loss: 0.000354348915
Iter: 750 loss: 0.0003557569
Iter: 751 loss: 0.000354325311
Iter: 752 loss: 0.000354245829
Iter: 753 loss: 0.000353980227
Iter: 754 loss: 0.000353666663
Iter: 755 loss: 0.000353582465
Iter: 756 loss: 0.000353230193
Iter: 757 loss: 0.000354170508
Iter: 758 loss: 0.000353115523
Iter: 759 loss: 0.000353018288
Iter: 760 loss: 0.00035271229
Iter: 761 loss: 0.000353092328
Iter: 762 loss: 0.000352480623
Iter: 763 loss: 0.00035218196
Iter: 764 loss: 0.000352181349
Iter: 765 loss: 0.000351903262
Iter: 766 loss: 0.000351497496
Iter: 767 loss: 0.000351485767
Iter: 768 loss: 0.000351118855
Iter: 769 loss: 0.000353740761
Iter: 770 loss: 0.000351089
Iter: 771 loss: 0.000351634983
Iter: 772 loss: 0.00035091571
Iter: 773 loss: 0.00035084167
Iter: 774 loss: 0.000350605114
Iter: 775 loss: 0.000350833463
Iter: 776 loss: 0.000350412214
Iter: 777 loss: 0.000350157497
Iter: 778 loss: 0.00034995543
Iter: 779 loss: 0.000349877664
Iter: 780 loss: 0.000349407492
Iter: 781 loss: 0.000355420285
Iter: 782 loss: 0.000349404523
Iter: 783 loss: 0.000349457492
Iter: 784 loss: 0.00034922405
Iter: 785 loss: 0.000349201786
Iter: 786 loss: 0.000349120703
Iter: 787 loss: 0.000349011825
Iter: 788 loss: 0.000348994305
Iter: 789 loss: 0.000349234208
Iter: 790 loss: 0.000348900096
Iter: 791 loss: 0.000348716334
Iter: 792 loss: 0.000349271606
Iter: 793 loss: 0.000348661473
Iter: 794 loss: 0.000348593545
Iter: 795 loss: 0.000348525704
Iter: 796 loss: 0.000348512782
Iter: 797 loss: 0.000348775677
Iter: 798 loss: 0.000348436879
Iter: 799 loss: 0.000348370901
Iter: 800 loss: 0.000348180474
Iter: 801 loss: 0.000349170325
Iter: 802 loss: 0.000348117377
Iter: 803 loss: 0.000348224217
Iter: 804 loss: 0.000347885303
Iter: 805 loss: 0.000347758876
Iter: 806 loss: 0.000347581168
Iter: 807 loss: 0.000347574241
Iter: 808 loss: 0.000347319263
Iter: 809 loss: 0.000347724941
Iter: 810 loss: 0.000347199501
Iter: 811 loss: 0.000347094145
Iter: 812 loss: 0.000346911722
Iter: 813 loss: 0.000346911431
Iter: 814 loss: 0.000346782384
Iter: 815 loss: 0.000347659341
Iter: 816 loss: 0.0003467676
Iter: 817 loss: 0.00034662598
Iter: 818 loss: 0.000347377238
Iter: 819 loss: 0.000346603512
Iter: 820 loss: 0.000346529647
Iter: 821 loss: 0.000346296816
Iter: 822 loss: 0.000346746878
Iter: 823 loss: 0.000346144487
Iter: 824 loss: 0.000345933397
Iter: 825 loss: 0.000348151778
Iter: 826 loss: 0.000345926936
Iter: 827 loss: 0.000345652807
Iter: 828 loss: 0.000345179811
Iter: 829 loss: 0.000345179811
Iter: 830 loss: 0.000350345683
Iter: 831 loss: 0.000345041917
Iter: 832 loss: 0.000344882865
Iter: 833 loss: 0.000345479697
Iter: 834 loss: 0.000344846281
Iter: 835 loss: 0.000344438478
Iter: 836 loss: 0.000344476051
Iter: 837 loss: 0.000344122935
Iter: 838 loss: 0.00034388635
Iter: 839 loss: 0.000343762717
Iter: 840 loss: 0.000343698601
Iter: 841 loss: 0.000343557214
Iter: 842 loss: 0.000345528
Iter: 843 loss: 0.000343547872
Iter: 844 loss: 0.000343360327
Iter: 845 loss: 0.000343110907
Iter: 846 loss: 0.000343099586
Iter: 847 loss: 0.000343061984
Iter: 848 loss: 0.000342947897
Iter: 849 loss: 0.000343072636
Iter: 850 loss: 0.000342859392
Iter: 851 loss: 0.000342538639
Iter: 852 loss: 0.000342326792
Iter: 853 loss: 0.000342207932
Iter: 854 loss: 0.000342043728
Iter: 855 loss: 0.000342043873
Iter: 856 loss: 0.000341945619
Iter: 857 loss: 0.000342111947
Iter: 858 loss: 0.000341900944
Iter: 859 loss: 0.000341602834
Iter: 860 loss: 0.000341495703
Iter: 861 loss: 0.000341329607
Iter: 862 loss: 0.000341086765
Iter: 863 loss: 0.000340801722
Iter: 864 loss: 0.000340772473
Iter: 865 loss: 0.000340653758
Iter: 866 loss: 0.000340592873
Iter: 867 loss: 0.000340538623
Iter: 868 loss: 0.000340538099
Iter: 869 loss: 0.000340497296
Iter: 870 loss: 0.000340344966
Iter: 871 loss: 0.000340284081
Iter: 872 loss: 0.000340204104
Iter: 873 loss: 0.000339777151
Iter: 874 loss: 0.000342935149
Iter: 875 loss: 0.000339742226
Iter: 876 loss: 0.0003397197
Iter: 877 loss: 0.000339632301
Iter: 878 loss: 0.000339556253
Iter: 879 loss: 0.000339514576
Iter: 880 loss: 0.000339166843
Iter: 881 loss: 0.000341234612
Iter: 882 loss: 0.000339121267
Iter: 883 loss: 0.000338964
Iter: 884 loss: 0.000340678846
Iter: 885 loss: 0.000338959391
Iter: 886 loss: 0.000338867714
Iter: 887 loss: 0.000338736456
Iter: 888 loss: 0.00033873244
Iter: 889 loss: 0.000338578422
Iter: 890 loss: 0.000339468941
Iter: 891 loss: 0.000338557
Iter: 892 loss: 0.000338430022
Iter: 893 loss: 0.000338102167
Iter: 894 loss: 0.00034101645
Iter: 895 loss: 0.000338052574
Iter: 896 loss: 0.000337604695
Iter: 897 loss: 0.000337014877
Iter: 898 loss: 0.000336979225
Iter: 899 loss: 0.000337706471
Iter: 900 loss: 0.000336755562
Iter: 901 loss: 0.000336611934
Iter: 902 loss: 0.000336221
Iter: 903 loss: 0.000338499347
Iter: 904 loss: 0.000336107623
Iter: 905 loss: 0.000335489458
Iter: 906 loss: 0.000344811764
Iter: 907 loss: 0.000335490447
Iter: 908 loss: 0.000335368211
Iter: 909 loss: 0.000335252844
Iter: 910 loss: 0.000335213379
Iter: 911 loss: 0.000335247663
Iter: 912 loss: 0.000335191609
Iter: 913 loss: 0.000335144694
Iter: 914 loss: 0.000335017365
Iter: 915 loss: 0.000335652236
Iter: 916 loss: 0.000334974844
Iter: 917 loss: 0.0003349084
Iter: 918 loss: 0.000334908691
Iter: 919 loss: 0.000334577577
Iter: 920 loss: 0.000339215738
Iter: 921 loss: 0.000334577955
Iter: 922 loss: 0.000334265846
Iter: 923 loss: 0.000334254582
Iter: 924 loss: 0.000334080425
Iter: 925 loss: 0.000335373566
Iter: 926 loss: 0.000334066688
Iter: 927 loss: 0.000333940436
Iter: 928 loss: 0.000333692209
Iter: 929 loss: 0.000338352926
Iter: 930 loss: 0.000333688455
Iter: 931 loss: 0.000334319047
Iter: 932 loss: 0.000333580305
Iter: 933 loss: 0.000333483855
Iter: 934 loss: 0.000333701115
Iter: 935 loss: 0.000333447417
Iter: 936 loss: 0.000333389151
Iter: 937 loss: 0.000333209
Iter: 938 loss: 0.000333528209
Iter: 939 loss: 0.000333085656
Iter: 940 loss: 0.0003329053
Iter: 941 loss: 0.000332648866
Iter: 942 loss: 0.000332641095
Iter: 943 loss: 0.00036781
Iter: 944 loss: 0.00033261138
Iter: 945 loss: 0.000332460506
Iter: 946 loss: 0.000333016447
Iter: 947 loss: 0.000332425203
Iter: 948 loss: 0.000332292111
Iter: 949 loss: 0.000333839882
Iter: 950 loss: 0.000332288299
Iter: 951 loss: 0.000332168594
Iter: 952 loss: 0.000331754563
Iter: 953 loss: 0.000331337913
Iter: 954 loss: 0.000331170886
Iter: 955 loss: 0.000330606941
Iter: 956 loss: 0.00033198786
Iter: 957 loss: 0.000330407463
Iter: 958 loss: 0.000329916249
Iter: 959 loss: 0.000331858115
Iter: 960 loss: 0.00032980094
Iter: 961 loss: 0.000329605275
Iter: 962 loss: 0.000329383591
Iter: 963 loss: 0.000329353556
Iter: 964 loss: 0.000333203701
Iter: 965 loss: 0.000329250586
Iter: 966 loss: 0.000328938768
Iter: 967 loss: 0.000329941278
Iter: 968 loss: 0.000328853697
Iter: 969 loss: 0.000328790426
Iter: 970 loss: 0.000328642724
Iter: 971 loss: 0.00033042155
Iter: 972 loss: 0.000328630849
Iter: 973 loss: 0.000328515132
Iter: 974 loss: 0.000328788155
Iter: 975 loss: 0.000328469847
Iter: 976 loss: 0.000328415044
Iter: 977 loss: 0.000328223046
Iter: 978 loss: 0.00032789784
Iter: 979 loss: 0.000327877467
Iter: 980 loss: 0.000330473878
Iter: 981 loss: 0.000327720598
Iter: 982 loss: 0.000327589049
Iter: 983 loss: 0.000328129245
Iter: 984 loss: 0.000327560643
Iter: 985 loss: 0.000327292597
Iter: 986 loss: 0.000327747199
Iter: 987 loss: 0.000327173591
Iter: 988 loss: 0.000326812878
Iter: 989 loss: 0.000326810725
Iter: 990 loss: 0.000326467503
Iter: 991 loss: 0.000326858921
Iter: 992 loss: 0.000326282083
Iter: 993 loss: 0.000325913716
Iter: 994 loss: 0.000325280096
Iter: 995 loss: 0.000325279485
Iter: 996 loss: 0.000325644942
Iter: 997 loss: 0.000324867695
Iter: 998 loss: 0.000324707304
Iter: 999 loss: 0.000325471337
Iter: 1000 loss: 0.000324677443
Iter: 1001 loss: 0.000324620982
Iter: 1002 loss: 0.000324474444
Iter: 1003 loss: 0.00032560539
Iter: 1004 loss: 0.000324442459
Iter: 1005 loss: 0.000324249442
Iter: 1006 loss: 0.000325134723
Iter: 1007 loss: 0.000324212539
Iter: 1008 loss: 0.000324108463
Iter: 1009 loss: 0.000324029
Iter: 1010 loss: 0.00032399554
Iter: 1011 loss: 0.000329178409
Iter: 1012 loss: 0.000323939195
Iter: 1013 loss: 0.000323840301
Iter: 1014 loss: 0.000324200228
Iter: 1015 loss: 0.000323816785
Iter: 1016 loss: 0.000323590677
Iter: 1017 loss: 0.000323521206
Iter: 1018 loss: 0.000323388958
Iter: 1019 loss: 0.000323028
Iter: 1020 loss: 0.000323020096
Iter: 1021 loss: 0.0003228485
Iter: 1022 loss: 0.000322426611
Iter: 1023 loss: 0.000326645939
Iter: 1024 loss: 0.000322374632
Iter: 1025 loss: 0.0003219668
Iter: 1026 loss: 0.000322330423
Iter: 1027 loss: 0.000321722124
Iter: 1028 loss: 0.000321687316
Iter: 1029 loss: 0.000321614498
Iter: 1030 loss: 0.000322982378
Iter: 1031 loss: 0.00032161345
Iter: 1032 loss: 0.000321525149
Iter: 1033 loss: 0.000321275729
Iter: 1034 loss: 0.000322534062
Iter: 1035 loss: 0.000321193103
Iter: 1036 loss: 0.000321759377
Iter: 1037 loss: 0.00032105163
Iter: 1038 loss: 0.000320968626
Iter: 1039 loss: 0.000320949941
Iter: 1040 loss: 0.00032081129
Iter: 1041 loss: 0.000320753956
Iter: 1042 loss: 0.000320625491
Iter: 1043 loss: 0.000320356427
Iter: 1044 loss: 0.000324664579
Iter: 1045 loss: 0.000320348248
Iter: 1046 loss: 0.000320144289
Iter: 1047 loss: 0.000320340332
Iter: 1048 loss: 0.00032002636
Iter: 1049 loss: 0.000319899467
Iter: 1050 loss: 0.000319741492
Iter: 1051 loss: 0.000319729646
Iter: 1052 loss: 0.000319629791
Iter: 1053 loss: 0.00031989452
Iter: 1054 loss: 0.000319594808
Iter: 1055 loss: 0.000319558836
Iter: 1056 loss: 0.000319496379
Iter: 1057 loss: 0.000319496117
Iter: 1058 loss: 0.000319452
Iter: 1059 loss: 0.000319442654
Iter: 1060 loss: 0.000319414423
Iter: 1061 loss: 0.000319186365
Iter: 1062 loss: 0.000318888633
Iter: 1063 loss: 0.000318867591
Iter: 1064 loss: 0.000318701379
Iter: 1065 loss: 0.000321014319
Iter: 1066 loss: 0.000318701641
Iter: 1067 loss: 0.000318633742
Iter: 1068 loss: 0.000318637467
Iter: 1069 loss: 0.0003185799
Iter: 1070 loss: 0.000318536302
Iter: 1071 loss: 0.000318503269
Iter: 1072 loss: 0.000318489765
Iter: 1073 loss: 0.000318433071
Iter: 1074 loss: 0.000318347535
Iter: 1075 loss: 0.000318344129
Iter: 1076 loss: 0.000318410253
Iter: 1077 loss: 0.000318225066
Iter: 1078 loss: 0.000318123901
Iter: 1079 loss: 0.000318063598
Iter: 1080 loss: 0.000318021717
Iter: 1081 loss: 0.000317986676
Iter: 1082 loss: 0.000317968952
Iter: 1083 loss: 0.000317953585
Iter: 1084 loss: 0.000318093575
Iter: 1085 loss: 0.000317639759
Iter: 1086 loss: 0.000317510479
Iter: 1087 loss: 0.000317831262
Iter: 1088 loss: 0.000317463331
Iter: 1089 loss: 0.000317410682
Iter: 1090 loss: 0.00031743024
Iter: 1091 loss: 0.00031737276
Iter: 1092 loss: 0.000317278202
Iter: 1093 loss: 0.000317028083
Iter: 1094 loss: 0.000318941689
Iter: 1095 loss: 0.000316980091
Iter: 1096 loss: 0.000316862599
Iter: 1097 loss: 0.000316805701
Iter: 1098 loss: 0.000316272752
Iter: 1099 loss: 0.000316245074
Iter: 1100 loss: 0.000318601233
Iter: 1101 loss: 0.00031618934
Iter: 1102 loss: 0.000316133373
Iter: 1103 loss: 0.000316004851
Iter: 1104 loss: 0.000317575526
Iter: 1105 loss: 0.000315998681
Iter: 1106 loss: 0.000316038786
Iter: 1107 loss: 0.000315948535
Iter: 1108 loss: 0.000315901096
Iter: 1109 loss: 0.00031608998
Iter: 1110 loss: 0.000315892044
Iter: 1111 loss: 0.000315863639
Iter: 1112 loss: 0.000315775891
Iter: 1113 loss: 0.000315862766
Iter: 1114 loss: 0.000315702753
Iter: 1115 loss: 0.000315547484
Iter: 1116 loss: 0.000315987418
Iter: 1117 loss: 0.000315498823
Iter: 1118 loss: 0.000321858679
Iter: 1119 loss: 0.000315404235
Iter: 1120 loss: 0.000315263518
Iter: 1121 loss: 0.000314923818
Iter: 1122 loss: 0.000318656908
Iter: 1123 loss: 0.000314887788
Iter: 1124 loss: 0.000314737263
Iter: 1125 loss: 0.000315038662
Iter: 1126 loss: 0.000314677076
Iter: 1127 loss: 0.000314613222
Iter: 1128 loss: 0.000314560049
Iter: 1129 loss: 0.000314539677
Iter: 1130 loss: 0.000314464269
Iter: 1131 loss: 0.000314272707
Iter: 1132 loss: 0.000316058926
Iter: 1133 loss: 0.000314244244
Iter: 1134 loss: 0.000314135686
Iter: 1135 loss: 0.000313859142
Iter: 1136 loss: 0.000316059391
Iter: 1137 loss: 0.000313802186
Iter: 1138 loss: 0.000313695229
Iter: 1139 loss: 0.000314140372
Iter: 1140 loss: 0.000313673227
Iter: 1141 loss: 0.000323524757
Iter: 1142 loss: 0.000313645694
Iter: 1143 loss: 0.000313596334
Iter: 1144 loss: 0.000314318604
Iter: 1145 loss: 0.000313596363
Iter: 1146 loss: 0.000313582248
Iter: 1147 loss: 0.000313586701
Iter: 1148 loss: 0.000313571538
Iter: 1149 loss: 0.000313339406
Iter: 1150 loss: 0.000313210243
Iter: 1151 loss: 0.00031310966
Iter: 1152 loss: 0.000315166835
Iter: 1153 loss: 0.00031300055
Iter: 1154 loss: 0.000312934746
Iter: 1155 loss: 0.000313723547
Iter: 1156 loss: 0.000312933698
Iter: 1157 loss: 0.000312902615
Iter: 1158 loss: 0.000312850694
Iter: 1159 loss: 0.000312851742
Iter: 1160 loss: 0.00031275145
Iter: 1161 loss: 0.000312541553
Iter: 1162 loss: 0.000316009216
Iter: 1163 loss: 0.000312535762
Iter: 1164 loss: 0.000312323187
Iter: 1165 loss: 0.00031231629
Iter: 1166 loss: 0.000312027958
Iter: 1167 loss: 0.000312012562
Iter: 1168 loss: 0.000311790733
Iter: 1169 loss: 0.000311647193
Iter: 1170 loss: 0.000311353419
Iter: 1171 loss: 0.000316693331
Iter: 1172 loss: 0.000311347248
Iter: 1173 loss: 0.000311417622
Iter: 1174 loss: 0.000311215234
Iter: 1175 loss: 0.000311195385
Iter: 1176 loss: 0.00031118287
Iter: 1177 loss: 0.000311158015
Iter: 1178 loss: 0.000311339798
Iter: 1179 loss: 0.000311157317
Iter: 1180 loss: 0.000311134412
Iter: 1181 loss: 0.000311062788
Iter: 1182 loss: 0.000310945092
Iter: 1183 loss: 0.000310937903
Iter: 1184 loss: 0.000310207368
Iter: 1185 loss: 0.000312350719
Iter: 1186 loss: 0.000309985771
Iter: 1187 loss: 0.000309993629
Iter: 1188 loss: 0.000309758208
Iter: 1189 loss: 0.000309624505
Iter: 1190 loss: 0.00030961365
Iter: 1191 loss: 0.000309594499
Iter: 1192 loss: 0.00030952043
Iter: 1193 loss: 0.000309219584
Iter: 1194 loss: 0.000309653929
Iter: 1195 loss: 0.000309006078
Iter: 1196 loss: 0.000308760791
Iter: 1197 loss: 0.000308752118
Iter: 1198 loss: 0.000308564631
Iter: 1199 loss: 0.000308461109
Iter: 1200 loss: 0.000309037918
Iter: 1201 loss: 0.000308445131
Iter: 1202 loss: 0.000308688323
Iter: 1203 loss: 0.000308396935
Iter: 1204 loss: 0.000308368879
Iter: 1205 loss: 0.000308369927
Iter: 1206 loss: 0.000308345567
Iter: 1207 loss: 0.000308312126
Iter: 1208 loss: 0.000308208313
Iter: 1209 loss: 0.000308256393
Iter: 1210 loss: 0.000308112969
Iter: 1211 loss: 0.000308029208
Iter: 1212 loss: 0.000307962939
Iter: 1213 loss: 0.000307710259
Iter: 1214 loss: 0.000307412411
Iter: 1215 loss: 0.000307378301
Iter: 1216 loss: 0.000306579255
Iter: 1217 loss: 0.000308515271
Iter: 1218 loss: 0.00030629267
Iter: 1219 loss: 0.000306905655
Iter: 1220 loss: 0.000306221104
Iter: 1221 loss: 0.000306175585
Iter: 1222 loss: 0.000306178525
Iter: 1223 loss: 0.000306142116
Iter: 1224 loss: 0.000306082773
Iter: 1225 loss: 0.000305924797
Iter: 1226 loss: 0.00030693505
Iter: 1227 loss: 0.000305883295
Iter: 1228 loss: 0.000305655
Iter: 1229 loss: 0.000306347123
Iter: 1230 loss: 0.000305589841
Iter: 1231 loss: 0.000305800932
Iter: 1232 loss: 0.000305421301
Iter: 1233 loss: 0.000305332127
Iter: 1234 loss: 0.000305178808
Iter: 1235 loss: 0.000305179856
Iter: 1236 loss: 0.00030508585
Iter: 1237 loss: 0.000305001362
Iter: 1238 loss: 0.000304701651
Iter: 1239 loss: 0.000305841211
Iter: 1240 loss: 0.000304626417
Iter: 1241 loss: 0.000315322395
Iter: 1242 loss: 0.000304417626
Iter: 1243 loss: 0.000304349378
Iter: 1244 loss: 0.000304304122
Iter: 1245 loss: 0.00030427787
Iter: 1246 loss: 0.000303970184
Iter: 1247 loss: 0.00030422915
Iter: 1248 loss: 0.000303786306
Iter: 1249 loss: 0.000304699875
Iter: 1250 loss: 0.000303647801
Iter: 1251 loss: 0.000303542853
Iter: 1252 loss: 0.00030353808
Iter: 1253 loss: 0.000303516979
Iter: 1254 loss: 0.000303457811
Iter: 1255 loss: 0.000303912
Iter: 1256 loss: 0.000303445209
Iter: 1257 loss: 0.00030332015
Iter: 1258 loss: 0.000303203065
Iter: 1259 loss: 0.000303173
Iter: 1260 loss: 0.000303140143
Iter: 1261 loss: 0.000303003093
Iter: 1262 loss: 0.000302542117
Iter: 1263 loss: 0.00030558469
Iter: 1264 loss: 0.000302423956
Iter: 1265 loss: 0.000320772815
Iter: 1266 loss: 0.000302224769
Iter: 1267 loss: 0.000302149623
Iter: 1268 loss: 0.000302660454
Iter: 1269 loss: 0.00030214453
Iter: 1270 loss: 0.00030211141
Iter: 1271 loss: 0.000302009081
Iter: 1272 loss: 0.000302260771
Iter: 1273 loss: 0.000301952823
Iter: 1274 loss: 0.000301845081
Iter: 1275 loss: 0.000301843655
Iter: 1276 loss: 0.000301695
Iter: 1277 loss: 0.000301320455
Iter: 1278 loss: 0.00030477473
Iter: 1279 loss: 0.000301267428
Iter: 1280 loss: 0.000301046181
Iter: 1281 loss: 0.000302317785
Iter: 1282 loss: 0.000301014923
Iter: 1283 loss: 0.000300998508
Iter: 1284 loss: 0.000300963729
Iter: 1285 loss: 0.000301535969
Iter: 1286 loss: 0.000300962478
Iter: 1287 loss: 0.000300901738
Iter: 1288 loss: 0.000300866319
Iter: 1289 loss: 0.000300809974
Iter: 1290 loss: 0.000300998334
Iter: 1291 loss: 0.000300794869
Iter: 1292 loss: 0.000300767715
Iter: 1293 loss: 0.000300665619
Iter: 1294 loss: 0.000300571322
Iter: 1295 loss: 0.000300525862
Iter: 1296 loss: 0.000300286134
Iter: 1297 loss: 0.000300283602
Iter: 1298 loss: 0.000300172949
Iter: 1299 loss: 0.000300149317
Iter: 1300 loss: 0.000383822015
Iter: 1301 loss: 0.000300019921
Iter: 1302 loss: 0.000299920561
Iter: 1303 loss: 0.000299604464
Iter: 1304 loss: 0.000300060201
Iter: 1305 loss: 0.000299370062
Iter: 1306 loss: 0.000299110019
Iter: 1307 loss: 0.000300817192
Iter: 1308 loss: 0.000299080042
Iter: 1309 loss: 0.000299059786
Iter: 1310 loss: 0.000299020379
Iter: 1311 loss: 0.000299828418
Iter: 1312 loss: 0.000299020234
Iter: 1313 loss: 0.000298951229
Iter: 1314 loss: 0.000298802275
Iter: 1315 loss: 0.000300925574
Iter: 1316 loss: 0.000298794708
Iter: 1317 loss: 0.000318962208
Iter: 1318 loss: 0.000298385829
Iter: 1319 loss: 0.000298026571
Iter: 1320 loss: 0.000301625172
Iter: 1321 loss: 0.000298016588
Iter: 1322 loss: 0.00029778434
Iter: 1323 loss: 0.000298208382
Iter: 1324 loss: 0.000297684543
Iter: 1325 loss: 0.000297772931
Iter: 1326 loss: 0.00029756641
Iter: 1327 loss: 0.000297505263
Iter: 1328 loss: 0.000297429564
Iter: 1329 loss: 0.000297421444
Iter: 1330 loss: 0.000297011575
Iter: 1331 loss: 0.000296414539
Iter: 1332 loss: 0.000296399317
Iter: 1333 loss: 0.000295796024
Iter: 1334 loss: 0.00030384757
Iter: 1335 loss: 0.000295792532
Iter: 1336 loss: 0.000295692385
Iter: 1337 loss: 0.000295576814
Iter: 1338 loss: 0.000295742415
Iter: 1339 loss: 0.000295531092
Iter: 1340 loss: 0.000295509584
Iter: 1341 loss: 0.00029568121
Iter: 1342 loss: 0.000295508304
Iter: 1343 loss: 0.000295481645
Iter: 1344 loss: 0.000295467617
Iter: 1345 loss: 0.000295456179
Iter: 1346 loss: 0.000295422913
Iter: 1347 loss: 0.000295312406
Iter: 1348 loss: 0.000295315374
Iter: 1349 loss: 0.000295201171
Iter: 1350 loss: 0.000294956932
Iter: 1351 loss: 0.000295005622
Iter: 1352 loss: 0.000294777274
Iter: 1353 loss: 0.000294812839
Iter: 1354 loss: 0.00029465789
Iter: 1355 loss: 0.000308059738
Iter: 1356 loss: 0.000294553436
Iter: 1357 loss: 0.000294509897
Iter: 1358 loss: 0.000294748286
Iter: 1359 loss: 0.000294504862
Iter: 1360 loss: 0.000294474303
Iter: 1361 loss: 0.00029436333
Iter: 1362 loss: 0.000294108817
Iter: 1363 loss: 0.000300195679
Iter: 1364 loss: 0.000294107391
Iter: 1365 loss: 0.000293816091
Iter: 1366 loss: 0.000293393066
Iter: 1367 loss: 0.000293381716
Iter: 1368 loss: 0.000293018413
Iter: 1369 loss: 0.000295697449
Iter: 1370 loss: 0.000292983721
Iter: 1371 loss: 0.000293704186
Iter: 1372 loss: 0.000292946148
Iter: 1373 loss: 0.000292924669
Iter: 1374 loss: 0.000292934477
Iter: 1375 loss: 0.00029290872
Iter: 1376 loss: 0.000292898854
Iter: 1377 loss: 0.000292889308
Iter: 1378 loss: 0.000292874174
Iter: 1379 loss: 0.00029292432
Iter: 1380 loss: 0.000292871089
Iter: 1381 loss: 0.000292855548
Iter: 1382 loss: 0.000292814977
Iter: 1383 loss: 0.000292836456
Iter: 1384 loss: 0.000292776
Iter: 1385 loss: 0.000292681972
Iter: 1386 loss: 0.000293918274
Iter: 1387 loss: 0.000292680343
Iter: 1388 loss: 0.000296626764
Iter: 1389 loss: 0.000292631157
Iter: 1390 loss: 0.000292576442
Iter: 1391 loss: 0.000292510755
Iter: 1392 loss: 0.000292503391
Iter: 1393 loss: 0.000292399141
Iter: 1394 loss: 0.000292840181
Iter: 1395 loss: 0.000292377721
Iter: 1396 loss: 0.000292269455
Iter: 1397 loss: 0.000291970558
Iter: 1398 loss: 0.000293496
Iter: 1399 loss: 0.000291872682
Iter: 1400 loss: 0.000291522942
Iter: 1401 loss: 0.00029094232
Iter: 1402 loss: 0.000290940632
Iter: 1403 loss: 0.000290798576
Iter: 1404 loss: 0.000291872682
Iter: 1405 loss: 0.000290790835
Iter: 1406 loss: 0.000290756638
Iter: 1407 loss: 0.000290840922
Iter: 1408 loss: 0.000290744065
Iter: 1409 loss: 0.00029071013
Iter: 1410 loss: 0.000291110831
Iter: 1411 loss: 0.000290711178
Iter: 1412 loss: 0.000290667784
Iter: 1413 loss: 0.000290951692
Iter: 1414 loss: 0.000290664088
Iter: 1415 loss: 0.000290628581
Iter: 1416 loss: 0.000290518277
Iter: 1417 loss: 0.000290765893
Iter: 1418 loss: 0.000290451775
Iter: 1419 loss: 0.000290292315
Iter: 1420 loss: 0.000290021475
Iter: 1421 loss: 0.000290020602
Iter: 1422 loss: 0.000289885385
Iter: 1423 loss: 0.00028959653
Iter: 1424 loss: 0.000294064317
Iter: 1425 loss: 0.000289584976
Iter: 1426 loss: 0.00028932502
Iter: 1427 loss: 0.000289599353
Iter: 1428 loss: 0.000289180316
Iter: 1429 loss: 0.000289105286
Iter: 1430 loss: 0.000288889685
Iter: 1431 loss: 0.000289825752
Iter: 1432 loss: 0.000288807903
Iter: 1433 loss: 0.000288593845
Iter: 1434 loss: 0.000289264106
Iter: 1435 loss: 0.000288527604
Iter: 1436 loss: 0.000288478186
Iter: 1437 loss: 0.000288315146
Iter: 1438 loss: 0.000288195675
Iter: 1439 loss: 0.000288102892
Iter: 1440 loss: 0.000287979667
Iter: 1441 loss: 0.000287965522
Iter: 1442 loss: 0.000288044685
Iter: 1443 loss: 0.000287849689
Iter: 1444 loss: 0.000287819566
Iter: 1445 loss: 0.000287914416
Iter: 1446 loss: 0.000287809875
Iter: 1447 loss: 0.000287777861
Iter: 1448 loss: 0.000287743751
Iter: 1449 loss: 0.00028773793
Iter: 1450 loss: 0.000287696777
Iter: 1451 loss: 0.000287744857
Iter: 1452 loss: 0.000287677132
Iter: 1453 loss: 0.00028761907
Iter: 1454 loss: 0.000287455186
Iter: 1455 loss: 0.000288179028
Iter: 1456 loss: 0.000287391333
Iter: 1457 loss: 0.000289919379
Iter: 1458 loss: 0.000287195027
Iter: 1459 loss: 0.000287408446
Iter: 1460 loss: 0.000286996015
Iter: 1461 loss: 0.000286895782
Iter: 1462 loss: 0.000288427982
Iter: 1463 loss: 0.000286894239
Iter: 1464 loss: 0.000286864204
Iter: 1465 loss: 0.000286746887
Iter: 1466 loss: 0.000286367722
Iter: 1467 loss: 0.000289567804
Iter: 1468 loss: 0.000286304479
Iter: 1469 loss: 0.000286204711
Iter: 1470 loss: 0.000286953931
Iter: 1471 loss: 0.000286194438
Iter: 1472 loss: 0.000286181632
Iter: 1473 loss: 0.000286136754
Iter: 1474 loss: 0.000286134135
Iter: 1475 loss: 0.000286087889
Iter: 1476 loss: 0.000286446681
Iter: 1477 loss: 0.000286037568
Iter: 1478 loss: 0.000285890448
Iter: 1479 loss: 0.000285574613
Iter: 1480 loss: 0.000290648837
Iter: 1481 loss: 0.000285566814
Iter: 1482 loss: 0.000285356655
Iter: 1483 loss: 0.000285349874
Iter: 1484 loss: 0.000285698945
Iter: 1485 loss: 0.000285188959
Iter: 1486 loss: 0.000285015558
Iter: 1487 loss: 0.000285095593
Iter: 1488 loss: 0.000284899434
Iter: 1489 loss: 0.000284792535
Iter: 1490 loss: 0.000284520385
Iter: 1491 loss: 0.000286798
Iter: 1492 loss: 0.000284473354
Iter: 1493 loss: 0.000286099268
Iter: 1494 loss: 0.00028436014
Iter: 1495 loss: 0.000284282432
Iter: 1496 loss: 0.000284249458
Iter: 1497 loss: 0.000284213107
Iter: 1498 loss: 0.000284051814
Iter: 1499 loss: 0.000283810688
Iter: 1500 loss: 0.000283807138
Iter: 1501 loss: 0.000283662579
Iter: 1502 loss: 0.00028365929
Iter: 1503 loss: 0.000283532427
Iter: 1504 loss: 0.000283821952
Iter: 1505 loss: 0.000283486152
Iter: 1506 loss: 0.000283440371
Iter: 1507 loss: 0.000283339148
Iter: 1508 loss: 0.000284737704
Iter: 1509 loss: 0.000283332804
Iter: 1510 loss: 0.000283274741
Iter: 1511 loss: 0.000283301692
Iter: 1512 loss: 0.000283235917
Iter: 1513 loss: 0.000283174857
Iter: 1514 loss: 0.000283061236
Iter: 1515 loss: 0.00028545034
Iter: 1516 loss: 0.000283059489
Iter: 1517 loss: 0.000282699126
Iter: 1518 loss: 0.000286339782
Iter: 1519 loss: 0.000282689027
Iter: 1520 loss: 0.000282568741
Iter: 1521 loss: 0.00028245413
Iter: 1522 loss: 0.000282213936
Iter: 1523 loss: 0.000282468565
Iter: 1524 loss: 0.000282078981
Iter: 1525 loss: 0.000281981425
Iter: 1526 loss: 0.000282186666
Iter: 1527 loss: 0.000281944347
Iter: 1528 loss: 0.000281939108
Iter: 1529 loss: 0.000281924324
Iter: 1530 loss: 0.000281923974
Iter: 1531 loss: 0.000281910645
Iter: 1532 loss: 0.000281837041
Iter: 1533 loss: 0.000281665067
Iter: 1534 loss: 0.000283657
Iter: 1535 loss: 0.000281649933
Iter: 1536 loss: 0.000281594926
Iter: 1537 loss: 0.000281594112
Iter: 1538 loss: 0.00028156428
Iter: 1539 loss: 0.000281518645
Iter: 1540 loss: 0.000281517569
Iter: 1541 loss: 0.000281491783
Iter: 1542 loss: 0.000281486427
Iter: 1543 loss: 0.000281463959
Iter: 1544 loss: 0.000281395973
Iter: 1545 loss: 0.000281528
Iter: 1546 loss: 0.000281349523
Iter: 1547 loss: 0.000282107532
Iter: 1548 loss: 0.000281246263
Iter: 1549 loss: 0.000281104818
Iter: 1550 loss: 0.000281542
Iter: 1551 loss: 0.000281061017
Iter: 1552 loss: 0.000281037181
Iter: 1553 loss: 0.000280951383
Iter: 1554 loss: 0.000280677748
Iter: 1555 loss: 0.000283435802
Iter: 1556 loss: 0.000280645385
Iter: 1557 loss: 0.000280648092
Iter: 1558 loss: 0.000280438981
Iter: 1559 loss: 0.000280385604
Iter: 1560 loss: 0.000280370703
Iter: 1561 loss: 0.000280355976
Iter: 1562 loss: 0.000280387816
Iter: 1563 loss: 0.00028035068
Iter: 1564 loss: 0.000280297128
Iter: 1565 loss: 0.000280133187
Iter: 1566 loss: 0.000280629058
Iter: 1567 loss: 0.00028004896
Iter: 1568 loss: 0.00027989212
Iter: 1569 loss: 0.0002805205
Iter: 1570 loss: 0.000279857486
Iter: 1571 loss: 0.000279716449
Iter: 1572 loss: 0.000279684435
Iter: 1573 loss: 0.000279656611
Iter: 1574 loss: 0.000279603351
Iter: 1575 loss: 0.000280838809
Iter: 1576 loss: 0.000279604661
Iter: 1577 loss: 0.000279527361
Iter: 1578 loss: 0.00027941115
Iter: 1579 loss: 0.000279409112
Iter: 1580 loss: 0.00027923187
Iter: 1581 loss: 0.000280178443
Iter: 1582 loss: 0.000279203115
Iter: 1583 loss: 0.0002789516
Iter: 1584 loss: 0.000278746447
Iter: 1585 loss: 0.000278671156
Iter: 1586 loss: 0.000278565683
Iter: 1587 loss: 0.000278764113
Iter: 1588 loss: 0.000278519408
Iter: 1589 loss: 0.000283641362
Iter: 1590 loss: 0.000278484338
Iter: 1591 loss: 0.000278430729
Iter: 1592 loss: 0.000278498832
Iter: 1593 loss: 0.000278401
Iter: 1594 loss: 0.000278489315
Iter: 1595 loss: 0.00027832968
Iter: 1596 loss: 0.000278310559
Iter: 1597 loss: 0.000278335414
Iter: 1598 loss: 0.000278301362
Iter: 1599 loss: 0.000278287713
Iter: 1600 loss: 0.000278298045
Iter: 1601 loss: 0.000278278196
Iter: 1602 loss: 0.000278258492
Iter: 1603 loss: 0.000278222316
Iter: 1604 loss: 0.000279105298
Iter: 1605 loss: 0.000278222637
Iter: 1606 loss: 0.000278954336
Iter: 1607 loss: 0.000278161286
Iter: 1608 loss: 0.0002780348
Iter: 1609 loss: 0.000278026186
Iter: 1610 loss: 0.00027768212
Iter: 1611 loss: 0.000278427382
Iter: 1612 loss: 0.000277549
Iter: 1613 loss: 0.000298265368
Iter: 1614 loss: 0.000277463289
Iter: 1615 loss: 0.000277370476
Iter: 1616 loss: 0.000277216488
Iter: 1617 loss: 0.000277218554
Iter: 1618 loss: 0.000277144514
Iter: 1619 loss: 0.000276903564
Iter: 1620 loss: 0.000276840408
Iter: 1621 loss: 0.000276630191
Iter: 1622 loss: 0.000276663835
Iter: 1623 loss: 0.000276401144
Iter: 1624 loss: 0.000286840048
Iter: 1625 loss: 0.000276088103
Iter: 1626 loss: 0.000275983155
Iter: 1627 loss: 0.000277615385
Iter: 1628 loss: 0.00027598333
Iter: 1629 loss: 0.000275977072
Iter: 1630 loss: 0.000276037201
Iter: 1631 loss: 0.000275974744
Iter: 1632 loss: 0.000275956467
Iter: 1633 loss: 0.000275901
Iter: 1634 loss: 0.000275969651
Iter: 1635 loss: 0.000275856873
Iter: 1636 loss: 0.000275805854
Iter: 1637 loss: 0.000275750703
Iter: 1638 loss: 0.000275647268
Iter: 1639 loss: 0.000275360449
Iter: 1640 loss: 0.000276904932
Iter: 1641 loss: 0.0002752695
Iter: 1642 loss: 0.000274493388
Iter: 1643 loss: 0.000274917227
Iter: 1644 loss: 0.000273973332
Iter: 1645 loss: 0.000273623737
Iter: 1646 loss: 0.000273962651
Iter: 1647 loss: 0.000273425889
Iter: 1648 loss: 0.000273100479
Iter: 1649 loss: 0.000272548874
Iter: 1650 loss: 0.000272549107
Iter: 1651 loss: 0.000272421981
Iter: 1652 loss: 0.000273520593
Iter: 1653 loss: 0.000272414385
Iter: 1654 loss: 0.000272405276
Iter: 1655 loss: 0.000272412959
Iter: 1656 loss: 0.000272396544
Iter: 1657 loss: 0.000275362923
Iter: 1658 loss: 0.000272357371
Iter: 1659 loss: 0.000272336591
Iter: 1660 loss: 0.000272332807
Iter: 1661 loss: 0.000272298115
Iter: 1662 loss: 0.000272194855
Iter: 1663 loss: 0.000272746489
Iter: 1664 loss: 0.000272165285
Iter: 1665 loss: 0.000271897414
Iter: 1666 loss: 0.000273605634
Iter: 1667 loss: 0.000271868485
Iter: 1668 loss: 0.00027177218
Iter: 1669 loss: 0.00027178906
Iter: 1670 loss: 0.0002717
Iter: 1671 loss: 0.000271582627
Iter: 1672 loss: 0.000271241704
Iter: 1673 loss: 0.000272724661
Iter: 1674 loss: 0.000271109573
Iter: 1675 loss: 0.000270993711
Iter: 1676 loss: 0.000270908
Iter: 1677 loss: 0.000270868914
Iter: 1678 loss: 0.000270676974
Iter: 1679 loss: 0.000270227349
Iter: 1680 loss: 0.000275910192
Iter: 1681 loss: 0.000270192977
Iter: 1682 loss: 0.000270133372
Iter: 1683 loss: 0.000270509452
Iter: 1684 loss: 0.000270128308
Iter: 1685 loss: 0.000270188291
Iter: 1686 loss: 0.000270118093
Iter: 1687 loss: 0.000270098797
Iter: 1688 loss: 0.000270053541
Iter: 1689 loss: 0.000270450546
Iter: 1690 loss: 0.000270047021
Iter: 1691 loss: 0.000269919692
Iter: 1692 loss: 0.000270699966
Iter: 1693 loss: 0.000269905897
Iter: 1694 loss: 0.000269886863
Iter: 1695 loss: 0.000269896
Iter: 1696 loss: 0.000269874028
Iter: 1697 loss: 0.000269835145
Iter: 1698 loss: 0.000269748474
Iter: 1699 loss: 0.000271128636
Iter: 1700 loss: 0.000269745418
Iter: 1701 loss: 0.000269670039
Iter: 1702 loss: 0.0002696671
Iter: 1703 loss: 0.000269558281
Iter: 1704 loss: 0.000269807642
Iter: 1705 loss: 0.000269514654
Iter: 1706 loss: 0.000269202457
Iter: 1707 loss: 0.00027009187
Iter: 1708 loss: 0.000269100652
Iter: 1709 loss: 0.000269361073
Iter: 1710 loss: 0.000269029435
Iter: 1711 loss: 0.000268844655
Iter: 1712 loss: 0.000269800599
Iter: 1713 loss: 0.000268820877
Iter: 1714 loss: 0.000268806587
Iter: 1715 loss: 0.000268786738
Iter: 1716 loss: 0.000268785
Iter: 1717 loss: 0.000268726377
Iter: 1718 loss: 0.000268709351
Iter: 1719 loss: 0.000268675591
Iter: 1720 loss: 0.000268532429
Iter: 1721 loss: 0.00026852783
Iter: 1722 loss: 0.000268416537
Iter: 1723 loss: 0.000268288859
Iter: 1724 loss: 0.000268442527
Iter: 1725 loss: 0.000268220028
Iter: 1726 loss: 0.000268149306
Iter: 1727 loss: 0.000268113683
Iter: 1728 loss: 0.000268080446
Iter: 1729 loss: 0.000268003787
Iter: 1730 loss: 0.000268933945
Iter: 1731 loss: 0.000267994532
Iter: 1732 loss: 0.000267878757
Iter: 1733 loss: 0.00026777116
Iter: 1734 loss: 0.000267744123
Iter: 1735 loss: 0.000267718598
Iter: 1736 loss: 0.000267699186
Iter: 1737 loss: 0.000267690804
Iter: 1738 loss: 0.000267628289
Iter: 1739 loss: 0.000267489493
Iter: 1740 loss: 0.00026950438
Iter: 1741 loss: 0.000267483294
Iter: 1742 loss: 0.000266954361
Iter: 1743 loss: 0.000272158912
Iter: 1744 loss: 0.000266936317
Iter: 1745 loss: 0.00028844297
Iter: 1746 loss: 0.000266406074
Iter: 1747 loss: 0.000266212039
Iter: 1748 loss: 0.000266208313
Iter: 1749 loss: 0.000266010407
Iter: 1750 loss: 0.000265997893
Iter: 1751 loss: 0.000265896146
Iter: 1752 loss: 0.000266232819
Iter: 1753 loss: 0.000265870884
Iter: 1754 loss: 0.000266078918
Iter: 1755 loss: 0.00026575831
Iter: 1756 loss: 0.000265705981
Iter: 1757 loss: 0.000265679759
Iter: 1758 loss: 0.000265655312
Iter: 1759 loss: 0.00026560633
Iter: 1760 loss: 0.000265453767
Iter: 1761 loss: 0.000265695038
Iter: 1762 loss: 0.000265345647
Iter: 1763 loss: 0.000268501404
Iter: 1764 loss: 0.000265319424
Iter: 1765 loss: 0.000265289331
Iter: 1766 loss: 0.000265291659
Iter: 1767 loss: 0.000265263545
Iter: 1768 loss: 0.000266122777
Iter: 1769 loss: 0.000265044771
Iter: 1770 loss: 0.000264857692
Iter: 1771 loss: 0.000264616974
Iter: 1772 loss: 0.000264601433
Iter: 1773 loss: 0.00026432614
Iter: 1774 loss: 0.000263814174
Iter: 1775 loss: 0.000276553619
Iter: 1776 loss: 0.000263811904
Iter: 1777 loss: 0.000263812195
Iter: 1778 loss: 0.000263708731
Iter: 1779 loss: 0.000263678376
Iter: 1780 loss: 0.000263938447
Iter: 1781 loss: 0.000263678114
Iter: 1782 loss: 0.000274931896
Iter: 1783 loss: 0.000263624592
Iter: 1784 loss: 0.000263532827
Iter: 1785 loss: 0.000263527152
Iter: 1786 loss: 0.000263461581
Iter: 1787 loss: 0.000263411348
Iter: 1788 loss: 0.000263905706
Iter: 1789 loss: 0.000263411202
Iter: 1790 loss: 0.000263327733
Iter: 1791 loss: 0.00026324566
Iter: 1792 loss: 0.000263227761
Iter: 1793 loss: 0.000262862217
Iter: 1794 loss: 0.000262947258
Iter: 1795 loss: 0.000262595102
Iter: 1796 loss: 0.00026255555
Iter: 1797 loss: 0.000262831163
Iter: 1798 loss: 0.000262553105
Iter: 1799 loss: 0.000262543559
Iter: 1800 loss: 0.000262506946
Iter: 1801 loss: 0.000262391026
Iter: 1802 loss: 0.00026352331
Iter: 1803 loss: 0.000262375223
Iter: 1804 loss: 0.000263548834
Iter: 1805 loss: 0.00026219344
Iter: 1806 loss: 0.00026211908
Iter: 1807 loss: 0.000262340531
Iter: 1808 loss: 0.000262095127
Iter: 1809 loss: 0.000262107758
Iter: 1810 loss: 0.000262068585
Iter: 1811 loss: 0.000262005138
Iter: 1812 loss: 0.000262010959
Iter: 1813 loss: 0.000261955516
Iter: 1814 loss: 0.000261811598
Iter: 1815 loss: 0.000262485642
Iter: 1816 loss: 0.000261786132
Iter: 1817 loss: 0.000261695241
Iter: 1818 loss: 0.000262250076
Iter: 1819 loss: 0.000261684443
Iter: 1820 loss: 0.000261646346
Iter: 1821 loss: 0.000261584471
Iter: 1822 loss: 0.00026158354
Iter: 1823 loss: 0.000261490932
Iter: 1824 loss: 0.00026149
Iter: 1825 loss: 0.000261450303
Iter: 1826 loss: 0.000261452893
Iter: 1827 loss: 0.000261417154
Iter: 1828 loss: 0.000261403329
Iter: 1829 loss: 0.00026136823
Iter: 1830 loss: 0.000261633017
Iter: 1831 loss: 0.000261362293
Iter: 1832 loss: 0.000261314475
Iter: 1833 loss: 0.000261158479
Iter: 1834 loss: 0.000261396955
Iter: 1835 loss: 0.000261050911
Iter: 1836 loss: 0.000260761328
Iter: 1837 loss: 0.000260802102
Iter: 1838 loss: 0.000260537199
Iter: 1839 loss: 0.000260510365
Iter: 1840 loss: 0.000260410918
Iter: 1841 loss: 0.000260261353
Iter: 1842 loss: 0.000260245
Iter: 1843 loss: 0.000260078697
Iter: 1844 loss: 0.000260635687
Iter: 1845 loss: 0.000260033761
Iter: 1846 loss: 0.000260004395
Iter: 1847 loss: 0.000260014262
Iter: 1848 loss: 0.000259981258
Iter: 1849 loss: 0.000259946304
Iter: 1850 loss: 0.00025994255
Iter: 1851 loss: 0.000259917811
Iter: 1852 loss: 0.000259892666
Iter: 1853 loss: 0.000259839348
Iter: 1854 loss: 0.000260646
Iter: 1855 loss: 0.00025983568
Iter: 1856 loss: 0.000259760476
Iter: 1857 loss: 0.00025965739
Iter: 1858 loss: 0.000259651977
Iter: 1859 loss: 0.000259642664
Iter: 1860 loss: 0.000259639753
Iter: 1861 loss: 0.000259630382
Iter: 1862 loss: 0.000259621709
Iter: 1863 loss: 0.000259618042
Iter: 1864 loss: 0.000259575376
Iter: 1865 loss: 0.000259572145
Iter: 1866 loss: 0.000259540189
Iter: 1867 loss: 0.000259476306
Iter: 1868 loss: 0.000259294582
Iter: 1869 loss: 0.000260210887
Iter: 1870 loss: 0.000259234948
Iter: 1871 loss: 0.000260160508
Iter: 1872 loss: 0.0002591472
Iter: 1873 loss: 0.000259098742
Iter: 1874 loss: 0.00025934726
Iter: 1875 loss: 0.000259089982
Iter: 1876 loss: 0.000259076507
Iter: 1877 loss: 0.000259019434
Iter: 1878 loss: 0.000258851738
Iter: 1879 loss: 0.000260632893
Iter: 1880 loss: 0.000258834421
Iter: 1881 loss: 0.000258762302
Iter: 1882 loss: 0.000259191758
Iter: 1883 loss: 0.000258754677
Iter: 1884 loss: 0.00025874775
Iter: 1885 loss: 0.000258728833
Iter: 1886 loss: 0.000258729269
Iter: 1887 loss: 0.000258707092
Iter: 1888 loss: 0.000259449909
Iter: 1889 loss: 0.000258677057
Iter: 1890 loss: 0.000258572865
Iter: 1891 loss: 0.000258458953
Iter: 1892 loss: 0.000258440792
Iter: 1893 loss: 0.000267024036
Iter: 1894 loss: 0.00025842333
Iter: 1895 loss: 0.000258367596
Iter: 1896 loss: 0.000258370652
Iter: 1897 loss: 0.00025832234
Iter: 1898 loss: 0.000258286076
Iter: 1899 loss: 0.00025874353
Iter: 1900 loss: 0.000258286484
Iter: 1901 loss: 0.000258264889
Iter: 1902 loss: 0.000258207641
Iter: 1903 loss: 0.000258558372
Iter: 1904 loss: 0.000258193118
Iter: 1905 loss: 0.000258061424
Iter: 1906 loss: 0.000258118
Iter: 1907 loss: 0.000257971609
Iter: 1908 loss: 0.000257836597
Iter: 1909 loss: 0.000259482273
Iter: 1910 loss: 0.000257835985
Iter: 1911 loss: 0.000257657724
Iter: 1912 loss: 0.000257203
Iter: 1913 loss: 0.00026134061
Iter: 1914 loss: 0.000257133623
Iter: 1915 loss: 0.00028174519
Iter: 1916 loss: 0.000257106149
Iter: 1917 loss: 0.00025707364
Iter: 1918 loss: 0.000257115404
Iter: 1919 loss: 0.000257055101
Iter: 1920 loss: 0.00025704
Iter: 1921 loss: 0.000257039181
Iter: 1922 loss: 0.000257028441
Iter: 1923 loss: 0.000256978325
Iter: 1924 loss: 0.00025686319
Iter: 1925 loss: 0.000258323271
Iter: 1926 loss: 0.000256853062
Iter: 1927 loss: 0.000256692845
Iter: 1928 loss: 0.000258648157
Iter: 1929 loss: 0.000256690604
Iter: 1930 loss: 0.000256615982
Iter: 1931 loss: 0.000256372965
Iter: 1932 loss: 0.000256647647
Iter: 1933 loss: 0.000256187661
Iter: 1934 loss: 0.000256135972
Iter: 1935 loss: 0.000256666623
Iter: 1936 loss: 0.000256135187
Iter: 1937 loss: 0.000256131811
Iter: 1938 loss: 0.000256119
Iter: 1939 loss: 0.000256067142
Iter: 1940 loss: 0.000256057276
Iter: 1941 loss: 0.000256012077
Iter: 1942 loss: 0.000256581901
Iter: 1943 loss: 0.000255930965
Iter: 1944 loss: 0.000255839841
Iter: 1945 loss: 0.000256573025
Iter: 1946 loss: 0.000255833147
Iter: 1947 loss: 0.000255755207
Iter: 1948 loss: 0.000255850318
Iter: 1949 loss: 0.000255714054
Iter: 1950 loss: 0.000301179942
Iter: 1951 loss: 0.000255694933
Iter: 1952 loss: 0.000255628489
Iter: 1953 loss: 0.000255593099
Iter: 1954 loss: 0.00025556382
Iter: 1955 loss: 0.00025549822
Iter: 1956 loss: 0.000255269348
Iter: 1957 loss: 0.00025506469
Iter: 1958 loss: 0.000254958577
Iter: 1959 loss: 0.000254538609
Iter: 1960 loss: 0.000259070366
Iter: 1961 loss: 0.000254530401
Iter: 1962 loss: 0.000257368723
Iter: 1963 loss: 0.000254414772
Iter: 1964 loss: 0.000254375016
Iter: 1965 loss: 0.000254339509
Iter: 1966 loss: 0.000254326384
Iter: 1967 loss: 0.000254310697
Iter: 1968 loss: 0.000254265848
Iter: 1969 loss: 0.000254708342
Iter: 1970 loss: 0.000254258804
Iter: 1971 loss: 0.000254118058
Iter: 1972 loss: 0.000253790233
Iter: 1973 loss: 0.000257692707
Iter: 1974 loss: 0.000253760314
Iter: 1975 loss: 0.000253680104
Iter: 1976 loss: 0.000253973849
Iter: 1977 loss: 0.000253660255
Iter: 1978 loss: 0.000253632374
Iter: 1979 loss: 0.000253630744
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script122
+ '[' -r STOP.script122 ']'
+ exit 1
