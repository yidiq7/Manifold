+ RUN=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ LAYERS=300_300_300_1
+ case $RUN in
+ PSI='2 3'
+ OPTIONS='				 --optimizer adam 				 --n_pairs 20000 				 --batch_size 5000 				 --max_epochs 400 				 --loss_func weighted_MAPE 
'
++ pwd
+ OUT=/home/mrdouglas/Manifold/experiments.final/output130
++ pwd
+ OUT2=/home/mrdouglas/Manifold/experiments.final/output131
+ for fn in f1 f2
+ case $fn in
+ OPT=--phi
+ for layers in $LAYERS
+ for psi in $PSI
+ MODEL=
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi0
+ date
Sat Nov  7 18:03:09 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --function f1 --psi 2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd6ecd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd6f1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd764840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd71b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd7f8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd71b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd68a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd71b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd7f8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd5f69d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd7f8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd6b2d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd6b2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd539400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd6a5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd56dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7367013400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73670327b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7367013d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7367032d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd5a3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd5a39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd5a3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd50d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73bd516730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7366f41d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7366f41950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7366fc7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7366fc7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73406fed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73406feae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7340709b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f73406c8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f734069cd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f734064cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f734065dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 1.9974979988007315
test_loss: 2.007531785240608
train_loss: 2.0018289862747585
test_loss: 2.4756793186424724
train_loss: 1.998150927222231
test_loss: 2.0111145850696945
train_loss: 1.99829489121061
test_loss: 2.0088797262717923
train_loss: 2.0578059428321316
test_loss: 2.004517052629681
train_loss: 1.996466768402938
test_loss: 2.006300963254922
train_loss: 1.9981488646925376
test_loss: 2.34082996321337
train_loss: 1.998949478191596
test_loss: 2.0182192338574905
train_loss: 1.9979334295910423
test_loss: 2.023579362487206
train_loss: 1.9981655894428845
test_loss: 2.019931621895669
train_loss: 1.9986470550267652
test_loss: 2.0045038936704214
train_loss: 1.9995924218876362
test_loss: 2.0219482290031143
train_loss: 1.9976657231134625
test_loss: 2.0063134781462204
train_loss: 2.003914755161159
test_loss: 1.9990353364535518
train_loss: 2.0096466949189424
test_loss: 2.004636769505185
train_loss: 1.9968608798368248
test_loss: 1.999719942596084
train_loss: 1.9978164969612409
test_loss: 2.0061891242822405
train_loss: 1.9967162056737036
test_loss: 2.0084508263019965
train_loss: 1.9969597204996457
test_loss: 1.9976760863203507
train_loss: 1.9965949465457178
test_loss: 1.9988176035834329
train_loss: 1.9980730979929442
test_loss: 1.997836137146455
train_loss: 1.993851374245088
test_loss: 1.9978056226515761
train_loss: 1.9937788835568182
test_loss: 1.9974099338636429
train_loss: 2.0069194278800304
test_loss: 1.9981556686744053
train_loss: 1.9960359796944243
test_loss: 1.9986224357850888
train_loss: 1.9939650275590681
test_loss: 1.9971746167904811
train_loss: 1.9951591196722802
test_loss: 1.9977859214840954
train_loss: 2.036822758664882
test_loss: 1.996992763046855
train_loss: 1.9967648156561
test_loss: 1.998644969962062
train_loss: 1.997756414237773
test_loss: 1.997367004003668
train_loss: 1.9959867295158897
test_loss: 1.9971219490619914
train_loss: 1.9982457045592719
test_loss: 1.998009375898686
train_loss: 1.9951212817852297
test_loss: 1.9972939132618215
train_loss: 1.9937856524794684
test_loss: 1.996761633214377
train_loss: 1.9976514583029001
test_loss: 1.9973979904194366
train_loss: 1.9962785639342033
test_loss: 1.9979088916507013
train_loss: 1.99501842557522
test_loss: 1.9989806618958492
train_loss: 1.9953017408851075
test_loss: 1.9987490057648374
train_loss: 1.9953400873077094
test_loss: 1.9980818399134952
train_loss: 1.994766393391973
test_loss: 1.9977836147878059
train_loss: 1.9961174995590567
test_loss: 1.9976417782871143
train_loss: 1.9964593519711613
test_loss: 1.9972821454149194
train_loss: 1.9961898086518057
test_loss: 1.9963740258780296
train_loss: 1.9938584563036763
test_loss: 2.0001797832584813
train_loss: 1.9948687182975462
test_loss: 1.9973746439317193
train_loss: 1.993690628790728
test_loss: 1.9969968472326174
train_loss: 1.9941966663509851
test_loss: 1.997282409073292
train_loss: 1.9921309279942383
test_loss: 1.997227367735988
train_loss: 1.9948095438500766
test_loss: 1.9968312402369448
train_loss: 1.9967852109648128
test_loss: 1.9972161926041927
train_loss: 1.9950462973151277
test_loss: 1.9994546528069115
train_loss: 1.9943638673251063
test_loss: 1.99766200662837
train_loss: 1.998328061049658
test_loss: 1.9974156537193992
train_loss: 1.9939031335300217
test_loss: 1.9974454438022329
train_loss: 1.9981231308213727
test_loss: 1.998258699339581
train_loss: 2.0017009048900847
test_loss: 1.9993450553673815
train_loss: 1.9961643112996903
test_loss: 1.9986934946276793
train_loss: 1.9964127931045255
test_loss: 1.9960371260736407
train_loss: 1.9971766701809759
test_loss: 1.9995074733564566
train_loss: 1.9959726344489022
test_loss: 1.9985602347902909
train_loss: 1.99597982666357
test_loss: 1.9991919626392578
train_loss: 1.993812061880487
test_loss: 1.9966341375940169
train_loss: 1.99126268821177
test_loss: 1.9972761644796295
train_loss: 1.9950971857914024
test_loss: 1.9970408764256748
train_loss: 1.996637507259298
test_loss: 1.9973293996702446
train_loss: 1.9996196599302283
test_loss: 1.995499620496235
train_loss: 1.997917683642296
test_loss: 1.9960948294422756
train_loss: 1.99806135463247
test_loss: 1.9988184688406252
train_loss: 1.9930589275894695
test_loss: 1.9983728723732401
train_loss: 1.9930932772600958
test_loss: 1.9974284011790895
train_loss: 1.9996253583319668
test_loss: 1.9974979793993468
train_loss: 2.0019646615695614
test_loss: 1.9978303675754145
train_loss: 0.5539059868193597
test_loss: 0.5661814105079243
train_loss: 0.5698831617867819
test_loss: 0.5692174337214606
train_loss: 0.5666908595333652
test_loss: 0.5687887992725686
train_loss: 0.5694291437743446
test_loss: 0.568460977876806
train_loss: 0.5656185665317005
test_loss: 0.5680584048409538
train_loss: 0.5621452849552235
test_loss: 0.5676362217513636
train_loss: 0.560011469843799
test_loss: 0.5671938850605822
train_loss: 0.5779226128133653
test_loss: 0.5667262058059561
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi0/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b05cc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b05d1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b05ccd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b056a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b058ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b058cbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b054a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b054aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b05392f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b056a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b058ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b04a6ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b04a6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b0403400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b04a6c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b042c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b042ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b042c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b0340e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b036c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b02f21e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b02fe7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b02d4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b0299f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b0289730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b023b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b025df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b025dae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b02091e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b0209f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b0209488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b0209840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b0198a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b0158b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b0158840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f55b00bcd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.5441228983063372
Iter: 2 loss: 0.36585201455752847
Iter: 3 loss: 0.28944045974270105
Iter: 4 loss: 0.14167684209637582
Iter: 5 loss: 0.43520340050954887
Iter: 6 loss: 0.10606734286276925
Iter: 7 loss: 0.067368435077742511
Iter: 8 loss: 0.095050315067738089
Iter: 9 loss: 0.043821097632599768
Iter: 10 loss: 0.023311913813915015
Iter: 11 loss: 0.18563665644043764
Iter: 12 loss: 0.021325794930274734
Iter: 13 loss: 0.013612870761869039
Iter: 14 loss: 0.051054334896170811
Iter: 15 loss: 0.012376587254372982
Iter: 16 loss: 0.01024944223111059
Iter: 17 loss: 0.015706655295910817
Iter: 18 loss: 0.0096184049881375
Iter: 19 loss: 0.0086932621997204054
Iter: 20 loss: 0.0086912374892555641
Iter: 21 loss: 0.008406829194825589
Iter: 22 loss: 0.00858201896518136
Iter: 23 loss: 0.0082276727045970423
Iter: 24 loss: 0.007933129000873989
Iter: 25 loss: 0.009463704312284129
Iter: 26 loss: 0.0078930535328263149
Iter: 27 loss: 0.0078727727467968224
Iter: 28 loss: 0.0078426625966161127
Iter: 29 loss: 0.0078060681055023909
Iter: 30 loss: 0.0078810133618404284
Iter: 31 loss: 0.0077913349384293538
Iter: 32 loss: 0.0077722928104358921
Iter: 33 loss: 0.0077764059872688675
Iter: 34 loss: 0.0077582923548882169
Iter: 35 loss: 0.0077461998716020462
Iter: 36 loss: 0.0077524296255098929
Iter: 37 loss: 0.0077382195460331078
Iter: 38 loss: 0.0077287749104313457
Iter: 39 loss: 0.0077416178123141692
Iter: 40 loss: 0.0077240808480374488
Iter: 41 loss: 0.0077175725682408153
Iter: 42 loss: 0.0077796757423808458
Iter: 43 loss: 0.0077173298439710391
Iter: 44 loss: 0.007712762366585255
Iter: 45 loss: 0.00774695410286153
Iter: 46 loss: 0.0077124035208498039
Iter: 47 loss: 0.0077081905105753756
Iter: 48 loss: 0.0076985619194341387
Iter: 49 loss: 0.0078187102981927218
Iter: 50 loss: 0.0076979019964068789
Iter: 51 loss: 0.0076855277832594332
Iter: 52 loss: 0.0077165247629923973
Iter: 53 loss: 0.0076812667092591444
Iter: 54 loss: 0.007664261794289572
Iter: 55 loss: 0.0078061305906157365
Iter: 56 loss: 0.0076633536569108932
Iter: 57 loss: 0.0076513194715607455
Iter: 58 loss: 0.0076379920537817166
Iter: 59 loss: 0.0076362348414948
Iter: 60 loss: 0.0076113113025196013
Iter: 61 loss: 0.007552732144736154
Iter: 62 loss: 0.008466034627042025
Iter: 63 loss: 0.0075485426729200583
Iter: 64 loss: 0.0074560354071135804
Iter: 65 loss: 0.0076298740330737139
Iter: 66 loss: 0.0074088565877318937
Iter: 67 loss: 0.0072700086843050776
Iter: 68 loss: 0.0089764184091074978
Iter: 69 loss: 0.00726997224104154
Iter: 70 loss: 0.0071373506198947655
Iter: 71 loss: 0.0072135654944405613
Iter: 72 loss: 0.0070445786557675012
Iter: 73 loss: 0.0069234710335672831
Iter: 74 loss: 0.007889041524217013
Iter: 75 loss: 0.0069194144267420528
Iter: 76 loss: 0.0070037288047176786
Iter: 77 loss: 0.0068121531209924761
Iter: 78 loss: 0.0066963541118192256
Iter: 79 loss: 0.0066951053383786392
Iter: 80 loss: 0.0065756777140829016
Iter: 81 loss: 0.0064838287035079009
Iter: 82 loss: 0.00645782346273152
Iter: 83 loss: 0.0062973078055985725
Iter: 84 loss: 0.0077276330806498031
Iter: 85 loss: 0.006285915594383463
Iter: 86 loss: 0.00619570049679731
Iter: 87 loss: 0.0061956949289782293
Iter: 88 loss: 0.0060154863053926769
Iter: 89 loss: 0.00824909427163499
Iter: 90 loss: 0.0060114520030133956
Iter: 91 loss: 0.0058796308412702326
Iter: 92 loss: 0.0059996450574013975
Iter: 93 loss: 0.0058101612585393075
Iter: 94 loss: 0.0056969423408174341
Iter: 95 loss: 0.0061871939714208245
Iter: 96 loss: 0.0056834108120715886
Iter: 97 loss: 0.0055937350115745658
Iter: 98 loss: 0.0057454724735779292
Iter: 99 loss: 0.0055502243999302142
Iter: 100 loss: 0.0054782865283636763
Iter: 101 loss: 0.0054691387528299079
Iter: 102 loss: 0.0053731932473122428
Iter: 103 loss: 0.0057191468654446721
Iter: 104 loss: 0.0053368555150758255
Iter: 105 loss: 0.0052327862283402821
Iter: 106 loss: 0.0052920928461561469
Iter: 107 loss: 0.0051827297163048649
Iter: 108 loss: 0.005108526980291345
Iter: 109 loss: 0.0054507473727060914
Iter: 110 loss: 0.005094340917742874
Iter: 111 loss: 0.005062280495934727
Iter: 112 loss: 0.0052123327022495849
Iter: 113 loss: 0.0050560532644959525
Iter: 114 loss: 0.0049989058526791925
Iter: 115 loss: 0.005877096078581617
Iter: 116 loss: 0.0049970294668326195
Iter: 117 loss: 0.0049478561354409558
Iter: 118 loss: 0.0054897263539694617
Iter: 119 loss: 0.0049471520779374341
Iter: 120 loss: 0.0048888024850392911
Iter: 121 loss: 0.0050175194548198656
Iter: 122 loss: 0.0048669733459241731
Iter: 123 loss: 0.0048344167913852015
Iter: 124 loss: 0.0048297256183422958
Iter: 125 loss: 0.0048060311347741863
Iter: 126 loss: 0.0047193379975775489
Iter: 127 loss: 0.00527277513319196
Iter: 128 loss: 0.0047038610390481451
Iter: 129 loss: 0.0046308578639722708
Iter: 130 loss: 0.0048155261683692644
Iter: 131 loss: 0.0046009408712803911
Iter: 132 loss: 0.0045523864082268207
Iter: 133 loss: 0.0045458731429176662
Iter: 134 loss: 0.00447684552270506
Iter: 135 loss: 0.0052220006690687562
Iter: 136 loss: 0.0044740636010389967
Iter: 137 loss: 0.0044105563940327139
Iter: 138 loss: 0.0046954440770125786
Iter: 139 loss: 0.0043950732968309645
Iter: 140 loss: 0.0043490771607993086
Iter: 141 loss: 0.00454036302196833
Iter: 142 loss: 0.0043385792561207431
Iter: 143 loss: 0.0043018679774917846
Iter: 144 loss: 0.0044397132376656767
Iter: 145 loss: 0.0042928779355638283
Iter: 146 loss: 0.004267897000879128
Iter: 147 loss: 0.0043650735587907183
Iter: 148 loss: 0.0042612624228737213
Iter: 149 loss: 0.0042395358269957783
Iter: 150 loss: 0.0043908299457609224
Iter: 151 loss: 0.0042373799303625261
Iter: 152 loss: 0.0042118147182934745
Iter: 153 loss: 0.0043033090689754173
Iter: 154 loss: 0.0042045442170327669
Iter: 155 loss: 0.0041546321424329857
Iter: 156 loss: 0.004455538533148202
Iter: 157 loss: 0.0041429814164374908
Iter: 158 loss: 0.0040532840387107888
Iter: 159 loss: 0.00447223793693047
Iter: 160 loss: 0.0040345253594700819
Iter: 161 loss: 0.0039818982815854088
Iter: 162 loss: 0.0039731841514078084
Iter: 163 loss: 0.0039210468281023515
Iter: 164 loss: 0.0040532143479058737
Iter: 165 loss: 0.00390387764313586
Iter: 166 loss: 0.0038434515308843264
Iter: 167 loss: 0.0047885878710779654
Iter: 168 loss: 0.0038412428486373495
Iter: 169 loss: 0.0037839495861500391
Iter: 170 loss: 0.0040324652702318844
Iter: 171 loss: 0.003771466806281488
Iter: 172 loss: 0.0037204823636731409
Iter: 173 loss: 0.0044122457095206319
Iter: 174 loss: 0.0037193626191821382
Iter: 175 loss: 0.0036822663781645093
Iter: 176 loss: 0.0036818862830209726
Iter: 177 loss: 0.0036557532038811098
Iter: 178 loss: 0.003651909577115431
Iter: 179 loss: 0.0036300621406463959
Iter: 180 loss: 0.00364109518249384
Iter: 181 loss: 0.0036152039992847452
Iter: 182 loss: 0.0035815127334892167
Iter: 183 loss: 0.0036785235443270413
Iter: 184 loss: 0.0035721464758209924
Iter: 185 loss: 0.0035455577552153516
Iter: 186 loss: 0.00365251858564435
Iter: 187 loss: 0.0035390966167244643
Iter: 188 loss: 0.0035054203961494656
Iter: 189 loss: 0.0035018548248328743
Iter: 190 loss: 0.0034784837276687821
Iter: 191 loss: 0.00344238647871691
Iter: 192 loss: 0.0035518490196403984
Iter: 193 loss: 0.0034337534305627386
Iter: 194 loss: 0.0033964841954685068
Iter: 195 loss: 0.0036412777678473769
Iter: 196 loss: 0.0033904800436468903
Iter: 197 loss: 0.0033416494414282476
Iter: 198 loss: 0.0034875185746170417
Iter: 199 loss: 0.00332838619683335
Iter: 200 loss: 0.0032984364299633818
Iter: 201 loss: 0.00329687839368625
Iter: 202 loss: 0.0032500990150196672
Iter: 203 loss: 0.0035232444235724007
Iter: 204 loss: 0.0032452202478031166
Iter: 205 loss: 0.00321602786260733
Iter: 206 loss: 0.00326509043851759
Iter: 207 loss: 0.0032014939632308212
Iter: 208 loss: 0.003167579294850802
Iter: 209 loss: 0.0032164814672163387
Iter: 210 loss: 0.0031505126175694503
Iter: 211 loss: 0.0031239988842507024
Iter: 212 loss: 0.0031238907701217709
Iter: 213 loss: 0.0031088411894951494
Iter: 214 loss: 0.003225125492105485
Iter: 215 loss: 0.0031076824731442005
Iter: 216 loss: 0.0030835487397021686
Iter: 217 loss: 0.0032238200622083674
Iter: 218 loss: 0.0030788039534352055
Iter: 219 loss: 0.0030537599607840214
Iter: 220 loss: 0.0033423157398929726
Iter: 221 loss: 0.0030529679402356204
Iter: 222 loss: 0.0030370948151223886
Iter: 223 loss: 0.0030913721317225242
Iter: 224 loss: 0.0030330317080821969
Iter: 225 loss: 0.0030116465732275516
Iter: 226 loss: 0.0030008282840194717
Iter: 227 loss: 0.0029899989889593838
Iter: 228 loss: 0.0029664774234189608
Iter: 229 loss: 0.003194714243499196
Iter: 230 loss: 0.0029632339723180117
Iter: 231 loss: 0.002922837808765847
Iter: 232 loss: 0.0032943302504092192
Iter: 233 loss: 0.0029208891578790461
Iter: 234 loss: 0.0028611712950080016
Iter: 235 loss: 0.0051360925570122038
Iter: 236 loss: 0.0028595353392916662
Iter: 237 loss: 0.0027614147250981831
Iter: 238 loss: 0.0061309387568075791
Iter: 239 loss: 0.0027604500830970789
Iter: 240 loss: 0.0027212575679143444
Iter: 241 loss: 0.0027131589800928621
Iter: 242 loss: 0.0026980739024975264
Iter: 243 loss: 0.002685975619580934
Iter: 244 loss: 0.0026100773090826576
Iter: 245 loss: 0.013090919177771537
Iter: 246 loss: 0.0026100394343824568
Iter: 247 loss: 0.0025590898015399807
Iter: 248 loss: 0.0034995269799254704
Iter: 249 loss: 0.0025572497864237119
Iter: 250 loss: 0.0025197194393737025
Iter: 251 loss: 0.0025155677270506348
Iter: 252 loss: 0.0024571273221247348
Iter: 253 loss: 0.0050169505916442987
Iter: 254 loss: 0.0024561291281143642
Iter: 255 loss: 0.0024258062663333143
Iter: 256 loss: 0.0024202086819331142
Iter: 257 loss: 0.0023951393584162924
Iter: 258 loss: 0.0023931479206299516
Iter: 259 loss: 0.0023638545302280679
Iter: 260 loss: 0.0023832055493704617
Iter: 261 loss: 0.0023416031830975764
Iter: 262 loss: 0.0022976926627284707
Iter: 263 loss: 0.0025635854352414719
Iter: 264 loss: 0.0022912582206687216
Iter: 265 loss: 0.0022708121224236724
Iter: 266 loss: 0.0022708028799224489
Iter: 267 loss: 0.0022491147229130658
Iter: 268 loss: 0.0023632082727550238
Iter: 269 loss: 0.0022438806018565626
Iter: 270 loss: 0.0022418377981150966
Iter: 271 loss: 0.0022322639352246433
Iter: 272 loss: 0.0022197466735589329
Iter: 273 loss: 0.0024453358602259338
Iter: 274 loss: 0.0022193363253694195
Iter: 275 loss: 0.0022006458073625806
Iter: 276 loss: 0.0021878729588091688
Iter: 277 loss: 0.002179468190978349
Iter: 278 loss: 0.0021643514630205391
Iter: 279 loss: 0.0021613308709530479
Iter: 280 loss: 0.0021480576890994477
Iter: 281 loss: 0.0022819789107659857
Iter: 282 loss: 0.0021474817530931561
Iter: 283 loss: 0.0021421896475658144
Iter: 284 loss: 0.0021416139086325057
Iter: 285 loss: 0.0021374971418577052
Iter: 286 loss: 0.0021318962457774733
Iter: 287 loss: 0.0021316871402554858
Iter: 288 loss: 0.0021270913415554066
Iter: 289 loss: 0.0021412626504454611
Iter: 290 loss: 0.0021256309364268487
Iter: 291 loss: 0.0021203051186525234
Iter: 292 loss: 0.0021127276742376709
Iter: 293 loss: 0.0021124652322456082
Iter: 294 loss: 0.002099331649657859
Iter: 295 loss: 0.0022129990015786923
Iter: 296 loss: 0.0020982775661461876
Iter: 297 loss: 0.0020918737714498824
Iter: 298 loss: 0.0021643639087435169
Iter: 299 loss: 0.0020917564521237878
Iter: 300 loss: 0.0020863863737671504
Iter: 301 loss: 0.00213005705799352
Iter: 302 loss: 0.0020856909045768989
Iter: 303 loss: 0.0020771648272826957
Iter: 304 loss: 0.0020775619907240934
Iter: 305 loss: 0.0020705238981420611
Iter: 306 loss: 0.00205577666738768
Iter: 307 loss: 0.0021341709304192038
Iter: 308 loss: 0.0020520561415424796
Iter: 309 loss: 0.0020402691198462631
Iter: 310 loss: 0.0020982195327454069
Iter: 311 loss: 0.0020382368833502763
Iter: 312 loss: 0.0020297480640207432
Iter: 313 loss: 0.0020292448063173709
Iter: 314 loss: 0.0020179168913688639
Iter: 315 loss: 0.0020927759330431495
Iter: 316 loss: 0.0020166696080650987
Iter: 317 loss: 0.0020117828120455867
Iter: 318 loss: 0.0020063124930508763
Iter: 319 loss: 0.0020056114160511033
Iter: 320 loss: 0.0019997464056006591
Iter: 321 loss: 0.0020162799418214881
Iter: 322 loss: 0.0019974922409999527
Iter: 323 loss: 0.0019912720874670962
Iter: 324 loss: 0.0019912714904592745
Iter: 325 loss: 0.001985488881360388
Iter: 326 loss: 0.0019874885605846979
Iter: 327 loss: 0.0019812887834849287
Iter: 328 loss: 0.0019769786553250797
Iter: 329 loss: 0.0020378833681870992
Iter: 330 loss: 0.0019769606234389353
Iter: 331 loss: 0.0019735523795949466
Iter: 332 loss: 0.0019966757703328885
Iter: 333 loss: 0.0019732167976434
Iter: 334 loss: 0.0019715280300976606
Iter: 335 loss: 0.0019693643561857015
Iter: 336 loss: 0.0019692052775539433
Iter: 337 loss: 0.0019673957867188696
Iter: 338 loss: 0.0019689273564392935
Iter: 339 loss: 0.0019663237611447595
Iter: 340 loss: 0.0019636432432744975
Iter: 341 loss: 0.0020046765474367859
Iter: 342 loss: 0.001963639574680803
Iter: 343 loss: 0.0019615294003425235
Iter: 344 loss: 0.0019677929694993223
Iter: 345 loss: 0.0019608273897026786
Iter: 346 loss: 0.0019587200366944311
Iter: 347 loss: 0.0019663854092539729
Iter: 348 loss: 0.0019582042082297918
Iter: 349 loss: 0.001955748143856342
Iter: 350 loss: 0.0019573220936299034
Iter: 351 loss: 0.0019541613466971613
Iter: 352 loss: 0.0019528659113611934
Iter: 353 loss: 0.0019629270366189329
Iter: 354 loss: 0.0019527466683817889
Iter: 355 loss: 0.0019516763513724228
Iter: 356 loss: 0.0019604842857466583
Iter: 357 loss: 0.0019516134296723277
Iter: 358 loss: 0.0019510637035478344
Iter: 359 loss: 0.0019530508991082503
Iter: 360 loss: 0.0019509086409573361
Iter: 361 loss: 0.0019503695265057151
Iter: 362 loss: 0.001958258153215956
Iter: 363 loss: 0.0019503686513296153
Iter: 364 loss: 0.0019496497993689249
Iter: 365 loss: 0.0019485192653956413
Iter: 366 loss: 0.001948506690657796
Iter: 367 loss: 0.0019483231091363825
Iter: 368 loss: 0.0019480060020837186
Iter: 369 loss: 0.0019474092271110168
Iter: 370 loss: 0.0019476676712705603
Iter: 371 loss: 0.0019469998251539018
Iter: 372 loss: 0.0019461571060371053
Iter: 373 loss: 0.0019443461521309689
Iter: 374 loss: 0.0019724790819252726
Iter: 375 loss: 0.0019442820846009047
Iter: 376 loss: 0.0019428484842297226
Iter: 377 loss: 0.0019585204611847554
Iter: 378 loss: 0.0019428012797878969
Iter: 379 loss: 0.001941944649326618
Iter: 380 loss: 0.0019535113363159866
Iter: 381 loss: 0.0019419388832745356
Iter: 382 loss: 0.0019413209399087084
Iter: 383 loss: 0.0019412547569811174
Iter: 384 loss: 0.0019408066777360541
Iter: 385 loss: 0.0019396385297170593
Iter: 386 loss: 0.0019394802110688965
Iter: 387 loss: 0.0019386250872363573
Iter: 388 loss: 0.0019381046639810706
Iter: 389 loss: 0.0019377858612734116
Iter: 390 loss: 0.001937577781839082
Iter: 391 loss: 0.0019365819781202471
Iter: 392 loss: 0.0019384610990421309
Iter: 393 loss: 0.0019361264835309894
Iter: 394 loss: 0.0019356161071766432
Iter: 395 loss: 0.0019348546493788881
Iter: 396 loss: 0.0019348383272193214
Iter: 397 loss: 0.0019335040281192996
Iter: 398 loss: 0.0019369380669973814
Iter: 399 loss: 0.0019330413249557567
Iter: 400 loss: 0.0019320390847763916
Iter: 401 loss: 0.0019388963239667027
Iter: 402 loss: 0.0019319373957163248
Iter: 403 loss: 0.0019311825379366089
Iter: 404 loss: 0.0019329814568179609
Iter: 405 loss: 0.0019309128391911457
Iter: 406 loss: 0.0019295367529279278
Iter: 407 loss: 0.0019342540993701563
Iter: 408 loss: 0.0019291594309369733
Iter: 409 loss: 0.0019286581786331608
Iter: 410 loss: 0.0019305615574955264
Iter: 411 loss: 0.0019285307177823385
Iter: 412 loss: 0.0019279602104046805
Iter: 413 loss: 0.0019341789248799922
Iter: 414 loss: 0.0019279481498802788
Iter: 415 loss: 0.0019272450951198102
Iter: 416 loss: 0.001930927731613458
Iter: 417 loss: 0.0019271338241302486
Iter: 418 loss: 0.0019267547931469354
Iter: 419 loss: 0.001927350704856335
Iter: 420 loss: 0.0019265780572350756
Iter: 421 loss: 0.0019259238406862508
Iter: 422 loss: 0.0019378587825476545
Iter: 423 loss: 0.0019259237770531462
Iter: 424 loss: 0.0019251116262440069
Iter: 425 loss: 0.001926254252180994
Iter: 426 loss: 0.0019247108557563296
Iter: 427 loss: 0.0019245908210000053
Iter: 428 loss: 0.0019244581638227006
Iter: 429 loss: 0.0019242418063390376
Iter: 430 loss: 0.0019260814452363483
Iter: 431 loss: 0.0019242283539161356
Iter: 432 loss: 0.0019241199812579683
Iter: 433 loss: 0.001923942370994572
Iter: 434 loss: 0.0019239414795895419
Iter: 435 loss: 0.0019234769807068577
Iter: 436 loss: 0.0019237141839468807
Iter: 437 loss: 0.001923171788853489
Iter: 438 loss: 0.0019224223323210418
Iter: 439 loss: 0.0019224201870305549
Iter: 440 loss: 0.0019222210449328927
Iter: 441 loss: 0.0019222550216177522
Iter: 442 loss: 0.0019220712665548069
Iter: 443 loss: 0.0019216155524617432
Iter: 444 loss: 0.0019204630472598435
Iter: 445 loss: 0.00193034162434265
Iter: 446 loss: 0.0019202808334712084
Iter: 447 loss: 0.0019199028665404165
Iter: 448 loss: 0.0019194119498429757
Iter: 449 loss: 0.0019190366001489616
Iter: 450 loss: 0.0019195917025082168
Iter: 451 loss: 0.0019188559867575777
Iter: 452 loss: 0.0019183750180402877
Iter: 453 loss: 0.0019172801424842153
Iter: 454 loss: 0.0019310637080230827
Iter: 455 loss: 0.0019172074672761438
Iter: 456 loss: 0.001915835218424249
Iter: 457 loss: 0.0019168858410475312
Iter: 458 loss: 0.0019149742996270396
Iter: 459 loss: 0.0019156706476449871
Iter: 460 loss: 0.0019144434877734454
Iter: 461 loss: 0.0019140401398919484
Iter: 462 loss: 0.0019148346325722871
Iter: 463 loss: 0.0019138667840698669
Iter: 464 loss: 0.0019133552581488945
Iter: 465 loss: 0.001912391955569978
Iter: 466 loss: 0.0019343183037132256
Iter: 467 loss: 0.001912389439528214
Iter: 468 loss: 0.0019106271905772864
Iter: 469 loss: 0.001916721629205689
Iter: 470 loss: 0.0019101551685856645
Iter: 471 loss: 0.0019093546358607489
Iter: 472 loss: 0.0019091511849916561
Iter: 473 loss: 0.0019085088339499034
Iter: 474 loss: 0.0019087920542361783
Iter: 475 loss: 0.0019080760322046852
Iter: 476 loss: 0.0019071977559800915
Iter: 477 loss: 0.0019058549816258362
Iter: 478 loss: 0.001905830938312214
Iter: 479 loss: 0.0019053605824796115
Iter: 480 loss: 0.0019053371833173431
Iter: 481 loss: 0.0019049404607723534
Iter: 482 loss: 0.0019052417605905012
Iter: 483 loss: 0.0019046998947628625
Iter: 484 loss: 0.0019041043512764482
Iter: 485 loss: 0.0019028362233630307
Iter: 486 loss: 0.0019232476511670095
Iter: 487 loss: 0.0019027963913683813
Iter: 488 loss: 0.001901932290318245
Iter: 489 loss: 0.0019026611597249405
Iter: 490 loss: 0.0019014121126062118
Iter: 491 loss: 0.0019009063755359972
Iter: 492 loss: 0.0019007698668843922
Iter: 493 loss: 0.0019004308845546457
Iter: 494 loss: 0.001901376065256365
Iter: 495 loss: 0.0019003212342643054
Iter: 496 loss: 0.0018997810434299581
Iter: 497 loss: 0.0019012371169277154
Iter: 498 loss: 0.0018996144987333328
Iter: 499 loss: 0.0018993910817794179
Iter: 500 loss: 0.0019000352314081335
Iter: 501 loss: 0.0018993207064685268
Iter: 502 loss: 0.0018991448596593842
Iter: 503 loss: 0.0019001620613750729
Iter: 504 loss: 0.0018991219309976073
Iter: 505 loss: 0.0018989913477665062
Iter: 506 loss: 0.0018991790052802183
Iter: 507 loss: 0.0018989281823485757
Iter: 508 loss: 0.0018987667608076182
Iter: 509 loss: 0.0018986162943708754
Iter: 510 loss: 0.0018985779107807401
Iter: 511 loss: 0.0018981853414754839
Iter: 512 loss: 0.0018984042132577333
Iter: 513 loss: 0.0018979307342878362
Iter: 514 loss: 0.0018976425108354726
Iter: 515 loss: 0.0018974833914323834
Iter: 516 loss: 0.0018973576649878392
Iter: 517 loss: 0.0018968745152240832
Iter: 518 loss: 0.0018962653807
Iter: 519 loss: 0.0018962178278481906
Iter: 520 loss: 0.0018962520307507023
Iter: 521 loss: 0.0018960405938754418
Iter: 522 loss: 0.0018958449537880453
Iter: 523 loss: 0.0018954875912459046
Iter: 524 loss: 0.0019046114141763766
Iter: 525 loss: 0.0018954874614637717
Iter: 526 loss: 0.0018952761284748753
Iter: 527 loss: 0.0018952747352625218
Iter: 528 loss: 0.0018951081581387408
Iter: 529 loss: 0.0018956332253905802
Iter: 530 loss: 0.0018950596754230982
Iter: 531 loss: 0.0018948266514103417
Iter: 532 loss: 0.001894326499103141
Iter: 533 loss: 0.0019020481196385683
Iter: 534 loss: 0.0018943088319044563
Iter: 535 loss: 0.001896611613406787
Iter: 536 loss: 0.0018942054474029896
Iter: 537 loss: 0.0018941034943592348
Iter: 538 loss: 0.00189381438807585
Iter: 539 loss: 0.0018953000645737269
Iter: 540 loss: 0.0018937172296103781
Iter: 541 loss: 0.0018930613809306719
Iter: 542 loss: 0.0018929865368294335
Iter: 543 loss: 0.0018925200929836416
Iter: 544 loss: 0.0018922876615119616
Iter: 545 loss: 0.0018922604886080236
Iter: 546 loss: 0.0018921118291702521
Iter: 547 loss: 0.001891982293621952
Iter: 548 loss: 0.0018919432427271549
Iter: 549 loss: 0.0018916395449512953
Iter: 550 loss: 0.0018914405243369396
Iter: 551 loss: 0.0018913267146158366
Iter: 552 loss: 0.001890870554359496
Iter: 553 loss: 0.0018912426449174977
Iter: 554 loss: 0.0018905910800082336
Iter: 555 loss: 0.0018901974932294652
Iter: 556 loss: 0.001894440858103961
Iter: 557 loss: 0.0018901889883877259
Iter: 558 loss: 0.001889766968205396
Iter: 559 loss: 0.0018926746665124243
Iter: 560 loss: 0.001889737512611804
Iter: 561 loss: 0.0018894996910314207
Iter: 562 loss: 0.0018911460385923565
Iter: 563 loss: 0.0018894761587478666
Iter: 564 loss: 0.001889378789714166
Iter: 565 loss: 0.0018892740775772773
Iter: 566 loss: 0.0018892574220106904
Iter: 567 loss: 0.0018890609455115333
Iter: 568 loss: 0.0018899887572911235
Iter: 569 loss: 0.0018890251338678557
Iter: 570 loss: 0.0018888167589245679
Iter: 571 loss: 0.001890733097712391
Iter: 572 loss: 0.0018888084029315961
Iter: 573 loss: 0.001888759254591571
Iter: 574 loss: 0.0018886746815239188
Iter: 575 loss: 0.001888674609474342
Iter: 576 loss: 0.0018885992506723496
Iter: 577 loss: 0.0018885702850101049
Iter: 578 loss: 0.0018884039598002715
Iter: 579 loss: 0.0018882643593509944
Iter: 580 loss: 0.0018882172470072456
Iter: 581 loss: 0.0018879751609682249
Iter: 582 loss: 0.0018879142078141731
Iter: 583 loss: 0.0018877646642861223
Iter: 584 loss: 0.0018875195314506498
Iter: 585 loss: 0.0018881488582888398
Iter: 586 loss: 0.0018874333571268355
Iter: 587 loss: 0.0018872642194456878
Iter: 588 loss: 0.0018871036607033484
Iter: 589 loss: 0.0018870657602600483
Iter: 590 loss: 0.0018869080158656327
Iter: 591 loss: 0.0018869054825380979
Iter: 592 loss: 0.0018866513676765631
Iter: 593 loss: 0.0018881244081829006
Iter: 594 loss: 0.001886617871627017
Iter: 595 loss: 0.0018864641801945908
Iter: 596 loss: 0.0018863351907449605
Iter: 597 loss: 0.0018862917963308134
Iter: 598 loss: 0.0018858870108734106
Iter: 599 loss: 0.0018854518954689872
Iter: 600 loss: 0.0018853869052243889
Iter: 601 loss: 0.0018852072337375684
Iter: 602 loss: 0.0018851577809826155
Iter: 603 loss: 0.0018849709138427674
Iter: 604 loss: 0.0018845940041101919
Iter: 605 loss: 0.0018916517961644163
Iter: 606 loss: 0.0018845884973187315
Iter: 607 loss: 0.0018846016874617402
Iter: 608 loss: 0.00188434188193375
Iter: 609 loss: 0.0018840466312528893
Iter: 610 loss: 0.0018853699362608978
Iter: 611 loss: 0.0018839880830091727
Iter: 612 loss: 0.001883823315861167
Iter: 613 loss: 0.0018837640866840285
Iter: 614 loss: 0.001883672846914063
Iter: 615 loss: 0.0018833551801905445
Iter: 616 loss: 0.0018844922361036446
Iter: 617 loss: 0.0018832757384659991
Iter: 618 loss: 0.0018828112594760408
Iter: 619 loss: 0.0018832613497731631
Iter: 620 loss: 0.0018825597087815298
Iter: 621 loss: 0.0018820968330802571
Iter: 622 loss: 0.0018865284247654881
Iter: 623 loss: 0.0018820771387874071
Iter: 624 loss: 0.0018819716420976317
Iter: 625 loss: 0.0018818588578356017
Iter: 626 loss: 0.0018817591750921062
Iter: 627 loss: 0.0018818268537967361
Iter: 628 loss: 0.0018816960283769119
Iter: 629 loss: 0.0018815303710056361
Iter: 630 loss: 0.0018811039089184048
Iter: 631 loss: 0.0018845152964875431
Iter: 632 loss: 0.0018810278553188804
Iter: 633 loss: 0.0018806689260481397
Iter: 634 loss: 0.0018821399217905355
Iter: 635 loss: 0.0018805897378636484
Iter: 636 loss: 0.0018798850490810205
Iter: 637 loss: 0.0018791343257912871
Iter: 638 loss: 0.0018790162295788469
Iter: 639 loss: 0.0018786569549847955
Iter: 640 loss: 0.00188163318935356
Iter: 641 loss: 0.0018786327423296507
Iter: 642 loss: 0.0018783965177830153
Iter: 643 loss: 0.0018785033557996743
Iter: 644 loss: 0.0018782367394446805
Iter: 645 loss: 0.0018777636894301739
Iter: 646 loss: 0.0018799269919397385
Iter: 647 loss: 0.0018776693137175628
Iter: 648 loss: 0.0018773859398134407
Iter: 649 loss: 0.0018782119636199023
Iter: 650 loss: 0.0018772980435528804
Iter: 651 loss: 0.001876956042506237
Iter: 652 loss: 0.0018761407936586814
Iter: 653 loss: 0.001885737717102065
Iter: 654 loss: 0.0018760640584790495
Iter: 655 loss: 0.0018754257559388033
Iter: 656 loss: 0.0018798894653816868
Iter: 657 loss: 0.0018753743831100306
Iter: 658 loss: 0.0018752445099645596
Iter: 659 loss: 0.0018750198901923614
Iter: 660 loss: 0.0018747761929367554
Iter: 661 loss: 0.0018745487943363005
Iter: 662 loss: 0.0018744917414026186
Iter: 663 loss: 0.0018739134631166568
Iter: 664 loss: 0.0018744573081892848
Iter: 665 loss: 0.0018735555360427539
Iter: 666 loss: 0.001873326277757264
Iter: 667 loss: 0.0018733838495000811
Iter: 668 loss: 0.0018731588588048111
Iter: 669 loss: 0.0018728971796438502
Iter: 670 loss: 0.0018728957668767052
Iter: 671 loss: 0.0018727483898825954
Iter: 672 loss: 0.0018733125855376739
Iter: 673 loss: 0.0018727134215276724
Iter: 674 loss: 0.001872494808645288
Iter: 675 loss: 0.0018720242149625928
Iter: 676 loss: 0.001879299608626694
Iter: 677 loss: 0.00187200711157113
Iter: 678 loss: 0.0018718483183094749
Iter: 679 loss: 0.0018715951651856353
Iter: 680 loss: 0.001871325579993375
Iter: 681 loss: 0.0018733188146073721
Iter: 682 loss: 0.0018713044013403375
Iter: 683 loss: 0.0018712000587099084
Iter: 684 loss: 0.00187162000002674
Iter: 685 loss: 0.001871175898439173
Iter: 686 loss: 0.0018710383179705438
Iter: 687 loss: 0.001870732156165191
Iter: 688 loss: 0.0018748250500274679
Iter: 689 loss: 0.0018707156816379971
Iter: 690 loss: 0.0018712527558927086
Iter: 691 loss: 0.0018705455059246654
Iter: 692 loss: 0.00187036822563986
Iter: 693 loss: 0.0018703305700152623
Iter: 694 loss: 0.001870214911340146
Iter: 695 loss: 0.001869851069069665
Iter: 696 loss: 0.0018726060447739803
Iter: 697 loss: 0.0018698233487231786
Iter: 698 loss: 0.0018695783761707415
Iter: 699 loss: 0.0018700550455098188
Iter: 700 loss: 0.0018694756112355546
Iter: 701 loss: 0.0018692416465347634
Iter: 702 loss: 0.0018685534986805407
Iter: 703 loss: 0.0018711275992260997
Iter: 704 loss: 0.0018682525017518878
Iter: 705 loss: 0.0018684140328796158
Iter: 706 loss: 0.0018680421257983408
Iter: 707 loss: 0.0018678286592283438
Iter: 708 loss: 0.00186778901172608
Iter: 709 loss: 0.0018676443018462389
Iter: 710 loss: 0.00186744925088938
Iter: 711 loss: 0.0018686002902322323
Iter: 712 loss: 0.0018674217890786078
Iter: 713 loss: 0.0018673045963803934
Iter: 714 loss: 0.0018674462582799259
Iter: 715 loss: 0.0018672424126993761
Iter: 716 loss: 0.0018669494632703601
Iter: 717 loss: 0.001867011254669225
Iter: 718 loss: 0.0018667327512252802
Iter: 719 loss: 0.0018662113126388538
Iter: 720 loss: 0.0018687897246146192
Iter: 721 loss: 0.001866120290600428
Iter: 722 loss: 0.0018658348721346816
Iter: 723 loss: 0.0018658000114620855
Iter: 724 loss: 0.0018656146453180059
Iter: 725 loss: 0.0018681215894419808
Iter: 726 loss: 0.0018656136464724503
Iter: 727 loss: 0.0018654112127472512
Iter: 728 loss: 0.0018650718416801573
Iter: 729 loss: 0.0018650711149087182
Iter: 730 loss: 0.0018647044092993197
Iter: 731 loss: 0.0018685785758051959
Iter: 732 loss: 0.0018646949813193397
Iter: 733 loss: 0.0018645856229190612
Iter: 734 loss: 0.0018647884500118993
Iter: 735 loss: 0.0018645388406658746
Iter: 736 loss: 0.0018643918043179273
Iter: 737 loss: 0.0018642314989625233
Iter: 738 loss: 0.0018642073773259709
Iter: 739 loss: 0.0018639784610467208
Iter: 740 loss: 0.0018658546041718019
Iter: 741 loss: 0.0018639644401120181
Iter: 742 loss: 0.0018638900877619561
Iter: 743 loss: 0.0018650159287977395
Iter: 744 loss: 0.0018638900625669729
Iter: 745 loss: 0.0018638101363605639
Iter: 746 loss: 0.0018636302134873606
Iter: 747 loss: 0.0018660262738262935
Iter: 748 loss: 0.0018636196114330636
Iter: 749 loss: 0.0018634676338895955
Iter: 750 loss: 0.0018634675618298994
Iter: 751 loss: 0.00186340658419919
Iter: 752 loss: 0.0018634974367339773
Iter: 753 loss: 0.0018633774043922947
Iter: 754 loss: 0.0018633037544451526
Iter: 755 loss: 0.0018634096609719626
Iter: 756 loss: 0.0018632681934243058
Iter: 757 loss: 0.0018631473010575802
Iter: 758 loss: 0.0018628241383712618
Iter: 759 loss: 0.0018652239911610159
Iter: 760 loss: 0.0018627525280131044
Iter: 761 loss: 0.001863417507996022
Iter: 762 loss: 0.0018625947983734335
Iter: 763 loss: 0.0018624365966264877
Iter: 764 loss: 0.0018630360405452551
Iter: 765 loss: 0.0018623989390935958
Iter: 766 loss: 0.0018622599058780169
Iter: 767 loss: 0.0018625111618862356
Iter: 768 loss: 0.0018621997548148805
Iter: 769 loss: 0.0018619592259134061
Iter: 770 loss: 0.0018620976182616008
Iter: 771 loss: 0.0018618037211555517
Iter: 772 loss: 0.0018615771111567165
Iter: 773 loss: 0.0018617190880290337
Iter: 774 loss: 0.0018614318033696319
Iter: 775 loss: 0.001861217903212677
Iter: 776 loss: 0.0018635810091899849
Iter: 777 loss: 0.0018612134125469836
Iter: 778 loss: 0.001860929977381803
Iter: 779 loss: 0.0018612748224926525
Iter: 780 loss: 0.0018607816439456838
Iter: 781 loss: 0.0018606370614000596
Iter: 782 loss: 0.0018605324668535702
Iter: 783 loss: 0.0018604829401256818
Iter: 784 loss: 0.0018602997159598803
Iter: 785 loss: 0.0018623636634591946
Iter: 786 loss: 0.0018602962912171565
Iter: 787 loss: 0.0018601226228729326
Iter: 788 loss: 0.0018597685223391085
Iter: 789 loss: 0.0018662120807106614
Iter: 790 loss: 0.0018597624499001105
Iter: 791 loss: 0.0018593899400366435
Iter: 792 loss: 0.0018595139507006139
Iter: 793 loss: 0.001859126180428146
Iter: 794 loss: 0.0018584757637920442
Iter: 795 loss: 0.0018618331545053836
Iter: 796 loss: 0.0018583647678267678
Iter: 797 loss: 0.0018581683541460323
Iter: 798 loss: 0.0018581034715475949
Iter: 799 loss: 0.0018579359322278427
Iter: 800 loss: 0.0018577921126479227
Iter: 801 loss: 0.0018577464435283689
Iter: 802 loss: 0.0018573822220275995
Iter: 803 loss: 0.0018571665428237689
Iter: 804 loss: 0.0018570134677427653
Iter: 805 loss: 0.0018563955388997771
Iter: 806 loss: 0.0018585070635953336
Iter: 807 loss: 0.0018562305154552804
Iter: 808 loss: 0.0018558493155324831
Iter: 809 loss: 0.0018600388270353253
Iter: 810 loss: 0.0018558413612660093
Iter: 811 loss: 0.0018556060478550954
Iter: 812 loss: 0.0018560073225504447
Iter: 813 loss: 0.0018555003494675013
Iter: 814 loss: 0.0018552617570816421
Iter: 815 loss: 0.0018547674476423154
Iter: 816 loss: 0.00186425756101938
Iter: 817 loss: 0.001854757969079144
Iter: 818 loss: 0.0018538790655307173
Iter: 819 loss: 0.0018541507545956149
Iter: 820 loss: 0.0018532547886167239
Iter: 821 loss: 0.0018523389634238292
Iter: 822 loss: 0.0018664513029760103
Iter: 823 loss: 0.0018523388139231839
Iter: 824 loss: 0.0018520690939264272
Iter: 825 loss: 0.0018527815253918598
Iter: 826 loss: 0.001851973809155227
Iter: 827 loss: 0.001851526939855038
Iter: 828 loss: 0.0018520063453958719
Iter: 829 loss: 0.0018512799002091014
Iter: 830 loss: 0.0018509120736147277
Iter: 831 loss: 0.0018527918974725649
Iter: 832 loss: 0.0018508520413384774
Iter: 833 loss: 0.0018506140081430613
Iter: 834 loss: 0.00185210918578775
Iter: 835 loss: 0.0018505870699900243
Iter: 836 loss: 0.0018503024921289921
Iter: 837 loss: 0.001849599249258017
Iter: 838 loss: 0.0018566432790983857
Iter: 839 loss: 0.0018495094655121762
Iter: 840 loss: 0.0018487896753005184
Iter: 841 loss: 0.0018516178446166295
Iter: 842 loss: 0.0018486086508947815
Iter: 843 loss: 0.0018483598325104516
Iter: 844 loss: 0.0018507830370900796
Iter: 845 loss: 0.001848350761075121
Iter: 846 loss: 0.0018480624587760921
Iter: 847 loss: 0.0018477852853646627
Iter: 848 loss: 0.0018477207924359898
Iter: 849 loss: 0.0018470692260428578
Iter: 850 loss: 0.0018478435600646731
Iter: 851 loss: 0.0018467177612700903
Iter: 852 loss: 0.0018462790261049994
Iter: 853 loss: 0.0018531241851426441
Iter: 854 loss: 0.0018462789380968574
Iter: 855 loss: 0.0018459712900910574
Iter: 856 loss: 0.0018458235521356964
Iter: 857 loss: 0.0018456707960894971
Iter: 858 loss: 0.0018450986910121178
Iter: 859 loss: 0.001844588336878629
Iter: 860 loss: 0.0018444378599874142
Iter: 861 loss: 0.0018438824457862081
Iter: 862 loss: 0.0018466988897047917
Iter: 863 loss: 0.0018437874224158475
Iter: 864 loss: 0.0018435051487336049
Iter: 865 loss: 0.0018431708295782415
Iter: 866 loss: 0.0018431358775811671
Iter: 867 loss: 0.0018426937052081975
Iter: 868 loss: 0.0018439231305241741
Iter: 869 loss: 0.001842545812231574
Iter: 870 loss: 0.0018422017300478125
Iter: 871 loss: 0.0018425033645637257
Iter: 872 loss: 0.0018419984047794143
Iter: 873 loss: 0.0018412894732446838
Iter: 874 loss: 0.0018450366005002264
Iter: 875 loss: 0.0018411687626175483
Iter: 876 loss: 0.0018409140741274307
Iter: 877 loss: 0.0018404817911742899
Iter: 878 loss: 0.0018394915509377518
Iter: 879 loss: 0.0018449411314118334
Iter: 880 loss: 0.0018393474766995425
Iter: 881 loss: 0.0018388754511345889
Iter: 882 loss: 0.0018394282647526872
Iter: 883 loss: 0.0018386180048228951
Iter: 884 loss: 0.0018382665400608305
Iter: 885 loss: 0.0018426081424137443
Iter: 886 loss: 0.0018382631661149715
Iter: 887 loss: 0.0018380536960708744
Iter: 888 loss: 0.0018376880863037997
Iter: 889 loss: 0.0018376880586488762
Iter: 890 loss: 0.0018371467328095418
Iter: 891 loss: 0.0018406849478297938
Iter: 892 loss: 0.0018370846066060017
Iter: 893 loss: 0.0018368340867607929
Iter: 894 loss: 0.0018368322267187792
Iter: 895 loss: 0.0018366915224894646
Iter: 896 loss: 0.0018367990660417607
Iter: 897 loss: 0.0018366057980322963
Iter: 898 loss: 0.0018364216922984026
Iter: 899 loss: 0.0018360801225886028
Iter: 900 loss: 0.0018439540437325292
Iter: 901 loss: 0.0018360796290471507
Iter: 902 loss: 0.0018359700124785964
Iter: 903 loss: 0.0018359015141685719
Iter: 904 loss: 0.0018357870925678468
Iter: 905 loss: 0.0018355923981398375
Iter: 906 loss: 0.0018355921272680729
Iter: 907 loss: 0.0018353129919544539
Iter: 908 loss: 0.0018369294708312374
Iter: 909 loss: 0.0018352763848048063
Iter: 910 loss: 0.0018350931340266503
Iter: 911 loss: 0.0018371979670196298
Iter: 912 loss: 0.0018350897622717087
Iter: 913 loss: 0.001834950958534063
Iter: 914 loss: 0.0018345002259117257
Iter: 915 loss: 0.0018345802767349589
Iter: 916 loss: 0.0018340730449500616
Iter: 917 loss: 0.0018335408106590683
Iter: 918 loss: 0.0018415064992226247
Iter: 919 loss: 0.0018335286651640697
Iter: 920 loss: 0.0018333736627698135
Iter: 921 loss: 0.0018337030128935
Iter: 922 loss: 0.0018333127981949274
Iter: 923 loss: 0.0018330858225684846
Iter: 924 loss: 0.0018325140122509489
Iter: 925 loss: 0.0018385742837796135
Iter: 926 loss: 0.0018324357509238716
Iter: 927 loss: 0.0018318921032796742
Iter: 928 loss: 0.0018389438779769741
Iter: 929 loss: 0.0018318879182307548
Iter: 930 loss: 0.0018312543371678197
Iter: 931 loss: 0.0018324607706642912
Iter: 932 loss: 0.001830968391714565
Iter: 933 loss: 0.0018296168902883945
Iter: 934 loss: 0.001829285998163522
Iter: 935 loss: 0.0018284104259251612
Iter: 936 loss: 0.0018282128445457355
Iter: 937 loss: 0.0018277883231461809
Iter: 938 loss: 0.0018265638042673084
Iter: 939 loss: 0.001829618571964882
Iter: 940 loss: 0.0018261550373567385
Iter: 941 loss: 0.001825052401433633
Iter: 942 loss: 0.0018276564931383531
Iter: 943 loss: 0.0018246394573699126
Iter: 944 loss: 0.001823906500782646
Iter: 945 loss: 0.0018237939515219068
Iter: 946 loss: 0.0018232916878904155
Iter: 947 loss: 0.0018223186627114591
Iter: 948 loss: 0.0018230315723355749
Iter: 949 loss: 0.0018217172838042002
Iter: 950 loss: 0.0018209098376310363
Iter: 951 loss: 0.0018269271200013662
Iter: 952 loss: 0.001820841724351097
Iter: 953 loss: 0.0018198429396174306
Iter: 954 loss: 0.0018301257812945889
Iter: 955 loss: 0.001819796334560927
Iter: 956 loss: 0.0018193244035687598
Iter: 957 loss: 0.0018270298015071722
Iter: 958 loss: 0.001819324349112091
Iter: 959 loss: 0.0018189355809555387
Iter: 960 loss: 0.0018208556344388757
Iter: 961 loss: 0.0018188817088621596
Iter: 962 loss: 0.0018178473676806364
Iter: 963 loss: 0.001818281656565765
Iter: 964 loss: 0.00181712941812903
Iter: 965 loss: 0.0018154771086351137
Iter: 966 loss: 0.0018195145969208361
Iter: 967 loss: 0.0018148821999988199
Iter: 968 loss: 0.0018142532330379878
Iter: 969 loss: 0.0018140141547437184
Iter: 970 loss: 0.001813044098216873
Iter: 971 loss: 0.001818334881307141
Iter: 972 loss: 0.0018129042356109105
Iter: 973 loss: 0.0018122138532199656
Iter: 974 loss: 0.0018136698268097471
Iter: 975 loss: 0.0018119382992447641
Iter: 976 loss: 0.0018112371802158004
Iter: 977 loss: 0.0018122894901271217
Iter: 978 loss: 0.0018108462235503177
Iter: 979 loss: 0.0018101981350221682
Iter: 980 loss: 0.0018100348818932063
Iter: 981 loss: 0.0018087823118663401
Iter: 982 loss: 0.0018110037604358547
Iter: 983 loss: 0.0018082280245468366
Iter: 984 loss: 0.0018068611418713774
Iter: 985 loss: 0.0018095688057674928
Iter: 986 loss: 0.0018062301891037514
Iter: 987 loss: 0.0018054400391144716
Iter: 988 loss: 0.0018052083832169052
Iter: 989 loss: 0.0018044536608945209
Iter: 990 loss: 0.0018148969263944183
Iter: 991 loss: 0.0018044520001825714
Iter: 992 loss: 0.0018039445988245976
Iter: 993 loss: 0.0018031874057116786
Iter: 994 loss: 0.001803170128320862
Iter: 995 loss: 0.0018022788799797788
Iter: 996 loss: 0.0018082985647803903
Iter: 997 loss: 0.0018021893064545747
Iter: 998 loss: 0.0018015835974005321
Iter: 999 loss: 0.00180187799105541
Iter: 1000 loss: 0.0018011773640554539
Iter: 1001 loss: 0.001799840595770319
Iter: 1002 loss: 0.0017995262892118048
Iter: 1003 loss: 0.0017986773239671901
Iter: 1004 loss: 0.0017973791755480353
Iter: 1005 loss: 0.0017973779382425056
Iter: 1006 loss: 0.0017965633218332213
Iter: 1007 loss: 0.0018012136296233733
Iter: 1008 loss: 0.0017964527303269207
Iter: 1009 loss: 0.0017958330736920392
Iter: 1010 loss: 0.0017976067080215882
Iter: 1011 loss: 0.0017956317835451422
Iter: 1012 loss: 0.0017952847456354226
Iter: 1013 loss: 0.0017977757364498555
Iter: 1014 loss: 0.0017952536058693156
Iter: 1015 loss: 0.0017951304176081853
Iter: 1016 loss: 0.001795630474535858
Iter: 1017 loss: 0.0017951031883178549
Iter: 1018 loss: 0.0017949149292121678
Iter: 1019 loss: 0.0017945879292721244
Iter: 1020 loss: 0.0017945870320589946
Iter: 1021 loss: 0.0017939835527801577
Iter: 1022 loss: 0.0018001183884109497
Iter: 1023 loss: 0.0017939622896310575
Iter: 1024 loss: 0.0017934510512696168
Iter: 1025 loss: 0.001796379213811373
Iter: 1026 loss: 0.0017933843878794016
Iter: 1027 loss: 0.001793146622509117
Iter: 1028 loss: 0.001794286613731916
Iter: 1029 loss: 0.0017931016947977231
Iter: 1030 loss: 0.0017928398585968547
Iter: 1031 loss: 0.0017942262826076711
Iter: 1032 loss: 0.0017927988497120077
Iter: 1033 loss: 0.0017925853499350572
Iter: 1034 loss: 0.0017927447975977493
Iter: 1035 loss: 0.0017924552772499512
Iter: 1036 loss: 0.0017921974390221478
Iter: 1037 loss: 0.001792990797154705
Iter: 1038 loss: 0.0017921216198110361
Iter: 1039 loss: 0.0017918437457074506
Iter: 1040 loss: 0.0017928268220134957
Iter: 1041 loss: 0.0017917729751152876
Iter: 1042 loss: 0.0017914443261268704
Iter: 1043 loss: 0.0017909729660037441
Iter: 1044 loss: 0.0017909575013073196
Iter: 1045 loss: 0.001790468524569969
Iter: 1046 loss: 0.0017964890058254032
Iter: 1047 loss: 0.0017904638015266315
Iter: 1048 loss: 0.0017900312954424865
Iter: 1049 loss: 0.0017913178807077482
Iter: 1050 loss: 0.0017899005143411275
Iter: 1051 loss: 0.0017895114903407495
Iter: 1052 loss: 0.0017901645131159381
Iter: 1053 loss: 0.0017893250777668832
Iter: 1054 loss: 0.0017889577889009954
Iter: 1055 loss: 0.0017901592563647763
Iter: 1056 loss: 0.001788855280649385
Iter: 1057 loss: 0.0017885979174331835
Iter: 1058 loss: 0.0017886165503969136
Iter: 1059 loss: 0.0017883940625827103
Iter: 1060 loss: 0.0017879695730350852
Iter: 1061 loss: 0.0017901114168867788
Iter: 1062 loss: 0.0017878984548464795
Iter: 1063 loss: 0.0017874066339238492
Iter: 1064 loss: 0.0017872898763204939
Iter: 1065 loss: 0.0017869816179830475
Iter: 1066 loss: 0.0017861439570553939
Iter: 1067 loss: 0.0017881266085490095
Iter: 1068 loss: 0.0017858330612113213
Iter: 1069 loss: 0.0017853766762949614
Iter: 1070 loss: 0.0017866982152240159
Iter: 1071 loss: 0.001785233627062314
Iter: 1072 loss: 0.0017845565075811035
Iter: 1073 loss: 0.00178798116941609
Iter: 1074 loss: 0.0017844451925679132
Iter: 1075 loss: 0.0017840275784964761
Iter: 1076 loss: 0.0017852684875870463
Iter: 1077 loss: 0.001783903363730881
Iter: 1078 loss: 0.0017836041425375445
Iter: 1079 loss: 0.0017839101310902854
Iter: 1080 loss: 0.0017834407410403964
Iter: 1081 loss: 0.0017829125946240022
Iter: 1082 loss: 0.0017854349739154104
Iter: 1083 loss: 0.0017828143437278454
Iter: 1084 loss: 0.0017822500165611513
Iter: 1085 loss: 0.0017862431874530048
Iter: 1086 loss: 0.0017821977342133942
Iter: 1087 loss: 0.0017819481805095298
Iter: 1088 loss: 0.0017823359349328832
Iter: 1089 loss: 0.0017818302972416691
Iter: 1090 loss: 0.0017813991501458826
Iter: 1091 loss: 0.0017803826013214289
Iter: 1092 loss: 0.0017923340734749586
Iter: 1093 loss: 0.0017802933309368284
Iter: 1094 loss: 0.0017796768510614678
Iter: 1095 loss: 0.0017881546195649522
Iter: 1096 loss: 0.001779676043846588
Iter: 1097 loss: 0.0017790765113478981
Iter: 1098 loss: 0.0017790550962238031
Iter: 1099 loss: 0.0017785885780291968
Iter: 1100 loss: 0.0017778259567499879
Iter: 1101 loss: 0.0017782116958280803
Iter: 1102 loss: 0.0017772856578043652
Iter: 1103 loss: 0.0017764344292549629
Iter: 1104 loss: 0.0017790204911734538
Iter: 1105 loss: 0.0017761834672139567
Iter: 1106 loss: 0.001775479596232624
Iter: 1107 loss: 0.0017840941157453304
Iter: 1108 loss: 0.0017754736883187228
Iter: 1109 loss: 0.0017748294495893867
Iter: 1110 loss: 0.0017769962523304385
Iter: 1111 loss: 0.0017746483757028341
Iter: 1112 loss: 0.0017740471510762214
Iter: 1113 loss: 0.0017740296760147846
Iter: 1114 loss: 0.001773542250939052
Iter: 1115 loss: 0.0017728764571416297
Iter: 1116 loss: 0.001776227965504357
Iter: 1117 loss: 0.0017727731247839846
Iter: 1118 loss: 0.0017725665363382309
Iter: 1119 loss: 0.0017725630387473855
Iter: 1120 loss: 0.001772397263239409
Iter: 1121 loss: 0.0017721605929632395
Iter: 1122 loss: 0.0017721524534216508
Iter: 1123 loss: 0.0017718273724722819
Iter: 1124 loss: 0.0017729349018502222
Iter: 1125 loss: 0.0017717407976048896
Iter: 1126 loss: 0.0017713848611728552
Iter: 1127 loss: 0.0017720400857803367
Iter: 1128 loss: 0.0017712284339222286
Iter: 1129 loss: 0.0017709079303314091
Iter: 1130 loss: 0.0017723554724532898
Iter: 1131 loss: 0.0017708368611190268
Iter: 1132 loss: 0.0017709584725601264
Iter: 1133 loss: 0.0017706058506125452
Iter: 1134 loss: 0.0017703370918319692
Iter: 1135 loss: 0.0017704241873000115
Iter: 1136 loss: 0.0017701439012323065
Iter: 1137 loss: 0.0017698356173240764
Iter: 1138 loss: 0.0017701823882382058
Iter: 1139 loss: 0.00176966991640849
Iter: 1140 loss: 0.0017693294432939224
Iter: 1141 loss: 0.0017702642715807905
Iter: 1142 loss: 0.0017692179268397994
Iter: 1143 loss: 0.0017688950826326947
Iter: 1144 loss: 0.0017690515054961267
Iter: 1145 loss: 0.0017686723531494704
Iter: 1146 loss: 0.0017679366954220358
Iter: 1147 loss: 0.0017726568381721658
Iter: 1148 loss: 0.0017678539226044926
Iter: 1149 loss: 0.0017677805415179096
Iter: 1150 loss: 0.0017676211056069479
Iter: 1151 loss: 0.0017674030150062084
Iter: 1152 loss: 0.0017676865614714321
Iter: 1153 loss: 0.0017672910809937862
Iter: 1154 loss: 0.0017670950341309114
Iter: 1155 loss: 0.0017670095481676627
Iter: 1156 loss: 0.0017669054462310361
Iter: 1157 loss: 0.0017665180017861037
Iter: 1158 loss: 0.0017684285185407718
Iter: 1159 loss: 0.0017664529203431337
Iter: 1160 loss: 0.0017659612065555144
Iter: 1161 loss: 0.0017667341878278438
Iter: 1162 loss: 0.0017657322993844644
Iter: 1163 loss: 0.0017652669677544119
Iter: 1164 loss: 0.0017651432807100841
Iter: 1165 loss: 0.0017648527760376954
Iter: 1166 loss: 0.001764316781433687
Iter: 1167 loss: 0.0017668456304734547
Iter: 1168 loss: 0.00176421858560467
Iter: 1169 loss: 0.0017637236405136393
Iter: 1170 loss: 0.0017687986102139187
Iter: 1171 loss: 0.0017637079628121416
Iter: 1172 loss: 0.0017634976008238461
Iter: 1173 loss: 0.0017642081139125864
Iter: 1174 loss: 0.0017634421593406304
Iter: 1175 loss: 0.0017631530107528192
Iter: 1176 loss: 0.0017629850236812778
Iter: 1177 loss: 0.0017628614898494064
Iter: 1178 loss: 0.0017621948863276024
Iter: 1179 loss: 0.0017610714971520425
Iter: 1180 loss: 0.0017610686205945486
Iter: 1181 loss: 0.0017603978860021817
Iter: 1182 loss: 0.0017712946683373638
Iter: 1183 loss: 0.0017603978796587915
Iter: 1184 loss: 0.0017594432557616023
Iter: 1185 loss: 0.0017643923890404776
Iter: 1186 loss: 0.0017592758953282972
Iter: 1187 loss: 0.0017587589389703811
Iter: 1188 loss: 0.0017585191369731117
Iter: 1189 loss: 0.0017582640970791237
Iter: 1190 loss: 0.0017570395997895103
Iter: 1191 loss: 0.0017570192639665334
Iter: 1192 loss: 0.0017560442530727318
Iter: 1193 loss: 0.0017554255072811781
Iter: 1194 loss: 0.0017550033913024126
Iter: 1195 loss: 0.0017544427076592365
Iter: 1196 loss: 0.0017537859945175238
Iter: 1197 loss: 0.0017537176921916485
Iter: 1198 loss: 0.0017527358308299344
Iter: 1199 loss: 0.0017587335121666153
Iter: 1200 loss: 0.0017526185850021588
Iter: 1201 loss: 0.0017521312326394915
Iter: 1202 loss: 0.0017521215198464317
Iter: 1203 loss: 0.0017518732331765448
Iter: 1204 loss: 0.0017517250505949418
Iter: 1205 loss: 0.0017516188377893012
Iter: 1206 loss: 0.0017509947465532152
Iter: 1207 loss: 0.0017497260800477688
Iter: 1208 loss: 0.0017729363621679472
Iter: 1209 loss: 0.0017497052731451431
Iter: 1210 loss: 0.0017478086497683773
Iter: 1211 loss: 0.0017495970630396701
Iter: 1212 loss: 0.0017466168140231927
Iter: 1213 loss: 0.001745469065998346
Iter: 1214 loss: 0.0017511242989241434
Iter: 1215 loss: 0.0017452726064092639
Iter: 1216 loss: 0.00174924022118198
Iter: 1217 loss: 0.0017450489589839959
Iter: 1218 loss: 0.0017448464688114753
Iter: 1219 loss: 0.0017446664172158944
Iter: 1220 loss: 0.0017446143991835632
Iter: 1221 loss: 0.0017441475710416824
Iter: 1222 loss: 0.001743255879887546
Iter: 1223 loss: 0.001763052944008474
Iter: 1224 loss: 0.0017432523277419616
Iter: 1225 loss: 0.00174236782913287
Iter: 1226 loss: 0.0017423328325072672
Iter: 1227 loss: 0.001741188945339354
Iter: 1228 loss: 0.0017442872030085565
Iter: 1229 loss: 0.0017408035916350328
Iter: 1230 loss: 0.0017400224163604087
Iter: 1231 loss: 0.0017416064421575538
Iter: 1232 loss: 0.001739710242172975
Iter: 1233 loss: 0.0017393763360849111
Iter: 1234 loss: 0.0017393711117432761
Iter: 1235 loss: 0.0017391715700530309
Iter: 1236 loss: 0.0017420960178952559
Iter: 1237 loss: 0.0017391710455668913
Iter: 1238 loss: 0.0017389306573352273
Iter: 1239 loss: 0.0017382815608099499
Iter: 1240 loss: 0.0017424236661719309
Iter: 1241 loss: 0.0017381204749293308
Iter: 1242 loss: 0.0017373816971619914
Iter: 1243 loss: 0.0017380347031771349
Iter: 1244 loss: 0.001736948390695965
Iter: 1245 loss: 0.001736450530951005
Iter: 1246 loss: 0.0017360219865729977
Iter: 1247 loss: 0.001735325160980577
Iter: 1248 loss: 0.0017398798837207463
Iter: 1249 loss: 0.0017352469893728909
Iter: 1250 loss: 0.0017348632790431559
Iter: 1251 loss: 0.0017363951480403655
Iter: 1252 loss: 0.0017347706189879861
Iter: 1253 loss: 0.0017343793492585339
Iter: 1254 loss: 0.0017357992162807402
Iter: 1255 loss: 0.0017342791094938233
Iter: 1256 loss: 0.0017340466386728858
Iter: 1257 loss: 0.001734046175115187
Iter: 1258 loss: 0.0017338437392678439
Iter: 1259 loss: 0.0017343073096825212
Iter: 1260 loss: 0.0017337674526273186
Iter: 1261 loss: 0.0017335040937180013
Iter: 1262 loss: 0.0017347213488803095
Iter: 1263 loss: 0.0017334534828325988
Iter: 1264 loss: 0.0017332064413711245
Iter: 1265 loss: 0.0017334856143091724
Iter: 1266 loss: 0.0017330716277897205
Iter: 1267 loss: 0.0017327968810866136
Iter: 1268 loss: 0.0017328912843192891
Iter: 1269 loss: 0.0017326044392907208
Iter: 1270 loss: 0.0017321404584561761
Iter: 1271 loss: 0.0017360401695321356
Iter: 1272 loss: 0.0017321107918989221
Iter: 1273 loss: 0.0017315803246258483
Iter: 1274 loss: 0.0017322008345192819
Iter: 1275 loss: 0.0017312941317696947
Iter: 1276 loss: 0.0017309982642667279
Iter: 1277 loss: 0.001731136690947961
Iter: 1278 loss: 0.0017307982348771097
Iter: 1279 loss: 0.001730815891260945
Iter: 1280 loss: 0.0017306366297063567
Iter: 1281 loss: 0.0017305034748473413
Iter: 1282 loss: 0.0017304393626892392
Iter: 1283 loss: 0.0017303753719623714
Iter: 1284 loss: 0.0017299234794419926
Iter: 1285 loss: 0.0017300941819200491
Iter: 1286 loss: 0.0017296143036343104
Iter: 1287 loss: 0.001729089974925856
Iter: 1288 loss: 0.0017336562851685243
Iter: 1289 loss: 0.0017290616621416979
Iter: 1290 loss: 0.0017286338038845052
Iter: 1291 loss: 0.0017280366488955307
Iter: 1292 loss: 0.0017280107468831481
Iter: 1293 loss: 0.0017277033994739033
Iter: 1294 loss: 0.0017275836347596337
Iter: 1295 loss: 0.0017273386263894595
Iter: 1296 loss: 0.001728190157825812
Iter: 1297 loss: 0.0017272747412355439
Iter: 1298 loss: 0.001727120274685085
Iter: 1299 loss: 0.0017269936635535817
Iter: 1300 loss: 0.0017269471860862618
Iter: 1301 loss: 0.001726679561476499
Iter: 1302 loss: 0.0017266790231021424
Iter: 1303 loss: 0.001726292533806979
Iter: 1304 loss: 0.001727499171955406
Iter: 1305 loss: 0.0017261789427355217
Iter: 1306 loss: 0.0017259162195523204
Iter: 1307 loss: 0.0017252698434309466
Iter: 1308 loss: 0.001731840178505801
Iter: 1309 loss: 0.0017251899319057399
Iter: 1310 loss: 0.0017249458518758148
Iter: 1311 loss: 0.0017249061084230117
Iter: 1312 loss: 0.0017246200290023926
Iter: 1313 loss: 0.0017273361441792695
Iter: 1314 loss: 0.0017246090564813589
Iter: 1315 loss: 0.0017244984469828885
Iter: 1316 loss: 0.0017246406480239951
Iter: 1317 loss: 0.0017244417074836621
Iter: 1318 loss: 0.0017242730765144587
Iter: 1319 loss: 0.0017241855959285689
Iter: 1320 loss: 0.0017241080271852896
Iter: 1321 loss: 0.0017237903411006432
Iter: 1322 loss: 0.0017265088126286092
Iter: 1323 loss: 0.0017237730448703067
Iter: 1324 loss: 0.0017236563143939658
Iter: 1325 loss: 0.0017243028163935823
Iter: 1326 loss: 0.0017236394839938317
Iter: 1327 loss: 0.0017235133426875546
Iter: 1328 loss: 0.0017236419883893783
Iter: 1329 loss: 0.0017234419625397289
Iter: 1330 loss: 0.0017232524050505081
Iter: 1331 loss: 0.0017228597261191299
Iter: 1332 loss: 0.0017295802572605307
Iter: 1333 loss: 0.0017228509784961996
Iter: 1334 loss: 0.0017224394645584608
Iter: 1335 loss: 0.0017254479336386466
Iter: 1336 loss: 0.0017224023762044322
Iter: 1337 loss: 0.0017226304083628806
Iter: 1338 loss: 0.0017222807610261339
Iter: 1339 loss: 0.0017222161116601724
Iter: 1340 loss: 0.0017220075574236736
Iter: 1341 loss: 0.0017221520225403372
Iter: 1342 loss: 0.0017218285807620512
Iter: 1343 loss: 0.001721523425674323
Iter: 1344 loss: 0.0017220228864631981
Iter: 1345 loss: 0.0017213858529272819
Iter: 1346 loss: 0.0017213758321701523
Iter: 1347 loss: 0.0017213310105255427
Iter: 1348 loss: 0.0017212825185260491
Iter: 1349 loss: 0.0017212522413898941
Iter: 1350 loss: 0.0017212329647073585
Iter: 1351 loss: 0.0017211271247360911
Iter: 1352 loss: 0.0017209945265055627
Iter: 1353 loss: 0.001720983785152647
Iter: 1354 loss: 0.00172079096088919
Iter: 1355 loss: 0.0017210714808659215
Iter: 1356 loss: 0.0017206968821742935
Iter: 1357 loss: 0.0017203203604200228
Iter: 1358 loss: 0.001720165559303257
Iter: 1359 loss: 0.0017199655055946837
Iter: 1360 loss: 0.0017197115474724182
Iter: 1361 loss: 0.0017196948384523522
Iter: 1362 loss: 0.0017195176085719665
Iter: 1363 loss: 0.0017193829245957053
Iter: 1364 loss: 0.0017193240584920087
Iter: 1365 loss: 0.0017191552327299959
Iter: 1366 loss: 0.0017194534633607865
Iter: 1367 loss: 0.001719080657976481
Iter: 1368 loss: 0.0017189882571865714
Iter: 1369 loss: 0.0017189875518959974
Iter: 1370 loss: 0.0017189139352008203
Iter: 1371 loss: 0.0017187647945988531
Iter: 1372 loss: 0.0017202853331350896
Iter: 1373 loss: 0.0017187603347482839
Iter: 1374 loss: 0.001718607465144518
Iter: 1375 loss: 0.0017182500186334782
Iter: 1376 loss: 0.0017226674160830376
Iter: 1377 loss: 0.00171822125050536
Iter: 1378 loss: 0.0017177225847910826
Iter: 1379 loss: 0.0017174222403577345
Iter: 1380 loss: 0.0017172082302030134
Iter: 1381 loss: 0.0017170331025680807
Iter: 1382 loss: 0.0017168538957289855
Iter: 1383 loss: 0.0017164916923818306
Iter: 1384 loss: 0.0017196830525845128
Iter: 1385 loss: 0.0017164757636147995
Iter: 1386 loss: 0.0017162831778546515
Iter: 1387 loss: 0.0017158986174602257
Iter: 1388 loss: 0.0017234079819587319
Iter: 1389 loss: 0.0017158940752839429
Iter: 1390 loss: 0.0017155317487230826
Iter: 1391 loss: 0.0017180606853318225
Iter: 1392 loss: 0.0017154987107891625
Iter: 1393 loss: 0.0017152240765385524
Iter: 1394 loss: 0.001717630449857527
Iter: 1395 loss: 0.0017152101568776819
Iter: 1396 loss: 0.0017150345237754716
Iter: 1397 loss: 0.0017152160680975674
Iter: 1398 loss: 0.0017149366916611665
Iter: 1399 loss: 0.0017146666179948973
Iter: 1400 loss: 0.0017151043601497358
Iter: 1401 loss: 0.0017145400766607984
Iter: 1402 loss: 0.0017141200954452593
Iter: 1403 loss: 0.0017150969487595299
Iter: 1404 loss: 0.0017139630352999281
Iter: 1405 loss: 0.0017134950305081852
Iter: 1406 loss: 0.0017151150742852851
Iter: 1407 loss: 0.0017133679026711244
Iter: 1408 loss: 0.0017130776542969948
Iter: 1409 loss: 0.0017130772399657022
Iter: 1410 loss: 0.0017129648849132794
Iter: 1411 loss: 0.0017128025168505808
Iter: 1412 loss: 0.00171279767675293
Iter: 1413 loss: 0.0017126036159820636
Iter: 1414 loss: 0.0017151437192298381
Iter: 1415 loss: 0.0017126019698610671
Iter: 1416 loss: 0.0017125101223894263
Iter: 1417 loss: 0.0017123809753858822
Iter: 1418 loss: 0.0017123758643619736
Iter: 1419 loss: 0.001712152345232692
Iter: 1420 loss: 0.0017133938154445028
Iter: 1421 loss: 0.0017121210341907416
Iter: 1422 loss: 0.00171197434375572
Iter: 1423 loss: 0.0017118457450390982
Iter: 1424 loss: 0.0017118071941776935
Iter: 1425 loss: 0.0017116985392071331
Iter: 1426 loss: 0.0017116762081260965
Iter: 1427 loss: 0.0017115849701931191
Iter: 1428 loss: 0.001711801865995929
Iter: 1429 loss: 0.0017115516308238744
Iter: 1430 loss: 0.0017114648195734295
Iter: 1431 loss: 0.0017119675029889608
Iter: 1432 loss: 0.0017114532395509992
Iter: 1433 loss: 0.0017113568626830923
Iter: 1434 loss: 0.0017116421561318776
Iter: 1435 loss: 0.0017113268636977667
Iter: 1436 loss: 0.0017112536362546712
Iter: 1437 loss: 0.001711158836311798
Iter: 1438 loss: 0.0017111523590281554
Iter: 1439 loss: 0.0017110337211953884
Iter: 1440 loss: 0.00171230274121728
Iter: 1441 loss: 0.0017110308600209352
Iter: 1442 loss: 0.0017109306001908951
Iter: 1443 loss: 0.001711395535893057
Iter: 1444 loss: 0.001710911586262583
Iter: 1445 loss: 0.0017108819684009697
Iter: 1446 loss: 0.0017107940923151777
Iter: 1447 loss: 0.0017110871770660253
Iter: 1448 loss: 0.0017107529632159921
Iter: 1449 loss: 0.0017106058687205276
Iter: 1450 loss: 0.0017118649346621233
Iter: 1451 loss: 0.0017105972807477041
Iter: 1452 loss: 0.0017104596446410837
Iter: 1453 loss: 0.0017104584982091703
Iter: 1454 loss: 0.0017103990672447294
Iter: 1455 loss: 0.0017102677474901372
Iter: 1456 loss: 0.0017121625744659441
Iter: 1457 loss: 0.0017102614724859727
Iter: 1458 loss: 0.0017102088002405034
Iter: 1459 loss: 0.0017101878187000333
Iter: 1460 loss: 0.0017101201397116807
Iter: 1461 loss: 0.0017102112348639732
Iter: 1462 loss: 0.0017100861303032035
Iter: 1463 loss: 0.0017100072660913234
Iter: 1464 loss: 0.0017101427070781699
Iter: 1465 loss: 0.0017099721050124952
Iter: 1466 loss: 0.001709842200789945
Iter: 1467 loss: 0.0017114766829643996
Iter: 1468 loss: 0.0017098412360265494
Iter: 1469 loss: 0.0017097526257126589
Iter: 1470 loss: 0.0017094843976482403
Iter: 1471 loss: 0.0017102116239123958
Iter: 1472 loss: 0.0017093404374075298
Iter: 1473 loss: 0.0017095353535785125
Iter: 1474 loss: 0.0017092205827103411
Iter: 1475 loss: 0.0017091549142369783
Iter: 1476 loss: 0.0017090543183132317
Iter: 1477 loss: 0.0017090526765272873
Iter: 1478 loss: 0.0017088908453185825
Iter: 1479 loss: 0.0017086509172791805
Iter: 1480 loss: 0.0017086452057942752
Iter: 1481 loss: 0.0017083446493960572
Iter: 1482 loss: 0.0017093991004541887
Iter: 1483 loss: 0.0017082589334201589
Iter: 1484 loss: 0.0017080990354214811
Iter: 1485 loss: 0.0017080690977394064
Iter: 1486 loss: 0.0017079469754304787
Iter: 1487 loss: 0.0017078212977567502
Iter: 1488 loss: 0.0017077979019722211
Iter: 1489 loss: 0.0017076488306749257
Iter: 1490 loss: 0.0017076996577642296
Iter: 1491 loss: 0.0017075438321752846
Iter: 1492 loss: 0.0017073539479343122
Iter: 1493 loss: 0.0017100987540939732
Iter: 1494 loss: 0.0017073536207641996
Iter: 1495 loss: 0.00170725018100021
Iter: 1496 loss: 0.0017073108463239379
Iter: 1497 loss: 0.0017071834164227358
Iter: 1498 loss: 0.0017070625304018874
Iter: 1499 loss: 0.0017075966235741147
Iter: 1500 loss: 0.0017070381872724685
Iter: 1501 loss: 0.0017068728420601862
Iter: 1502 loss: 0.0017072354598814733
Iter: 1503 loss: 0.0017068098462178554
Iter: 1504 loss: 0.0017067544785904336
Iter: 1505 loss: 0.0017067544242122365
Iter: 1506 loss: 0.0017067224785191583
Iter: 1507 loss: 0.0017066777937277777
Iter: 1508 loss: 0.0017066760312900973
Iter: 1509 loss: 0.0017065351396339384
Iter: 1510 loss: 0.0017062801487112685
Iter: 1511 loss: 0.0017126956468332605
Iter: 1512 loss: 0.001706280104123715
Iter: 1513 loss: 0.001706279823696612
Iter: 1514 loss: 0.0017061129688734026
Iter: 1515 loss: 0.0017060125801917655
Iter: 1516 loss: 0.0017069572065787628
Iter: 1517 loss: 0.0017060086097362569
Iter: 1518 loss: 0.0017059252851695152
Iter: 1519 loss: 0.0017063281903484447
Iter: 1520 loss: 0.0017059102515866184
Iter: 1521 loss: 0.0017058632095964111
Iter: 1522 loss: 0.0017057949377057844
Iter: 1523 loss: 0.0017057929288866367
Iter: 1524 loss: 0.0017057368603787624
Iter: 1525 loss: 0.0017057341928106531
Iter: 1526 loss: 0.0017056672428763168
Iter: 1527 loss: 0.0017056452038937057
Iter: 1528 loss: 0.0017056063687178342
Iter: 1529 loss: 0.0017055061846505989
Iter: 1530 loss: 0.001705392516993069
Iter: 1531 loss: 0.0017053779498218938
Iter: 1532 loss: 0.0017056157770476278
Iter: 1533 loss: 0.0017053300581220676
Iter: 1534 loss: 0.0017053037209851338
Iter: 1535 loss: 0.0017053393866259545
Iter: 1536 loss: 0.0017052904555700699
Iter: 1537 loss: 0.0017052646695939083
Iter: 1538 loss: 0.0017052638683472151
Iter: 1539 loss: 0.0017052402100361341
Iter: 1540 loss: 0.0017052122122947699
Iter: 1541 loss: 0.0017052092366807087
Iter: 1542 loss: 0.0017051588082231289
Iter: 1543 loss: 0.0017051451706167674
Iter: 1544 loss: 0.0017051140704627894
Iter: 1545 loss: 0.001705013974807004
Iter: 1546 loss: 0.0017050471805132332
Iter: 1547 loss: 0.0017049428367964198
Iter: 1548 loss: 0.0017047638064005472
Iter: 1549 loss: 0.0017055016079161225
Iter: 1550 loss: 0.001704725235254439
Iter: 1551 loss: 0.0017046148830074785
Iter: 1552 loss: 0.0017046115459361253
Iter: 1553 loss: 0.0017045257212670449
Iter: 1554 loss: 0.0017044485456372766
Iter: 1555 loss: 0.0017046410004616188
Iter: 1556 loss: 0.0017044212678728741
Iter: 1557 loss: 0.0017043096126890529
Iter: 1558 loss: 0.0017049297365148996
Iter: 1559 loss: 0.0017042937870401231
Iter: 1560 loss: 0.0017041900750083168
Iter: 1561 loss: 0.0017039389352194539
Iter: 1562 loss: 0.0017065178187861891
Iter: 1563 loss: 0.001703910761003818
Iter: 1564 loss: 0.0017037434337338394
Iter: 1565 loss: 0.0017037332657316797
Iter: 1566 loss: 0.0017035500555888357
Iter: 1567 loss: 0.0017039887158148178
Iter: 1568 loss: 0.0017034833647057656
Iter: 1569 loss: 0.0017034516479846372
Iter: 1570 loss: 0.0017034515752129273
Iter: 1571 loss: 0.0017034235552026167
Iter: 1572 loss: 0.0017033964381577397
Iter: 1573 loss: 0.0017033902622327006
Iter: 1574 loss: 0.0017033191875207255
Iter: 1575 loss: 0.0017032483848812277
Iter: 1576 loss: 0.0017032337155529524
Iter: 1577 loss: 0.0017030603038295203
Iter: 1578 loss: 0.0017049258246992225
Iter: 1579 loss: 0.0017030563024105663
Iter: 1580 loss: 0.0017028442693962677
Iter: 1581 loss: 0.0017038829827625398
Iter: 1582 loss: 0.0017028068117059376
Iter: 1583 loss: 0.0017026398011943398
Iter: 1584 loss: 0.0017022796109648426
Iter: 1585 loss: 0.0017085113113652873
Iter: 1586 loss: 0.0017022677885851049
Iter: 1587 loss: 0.0017021158163261743
Iter: 1588 loss: 0.0017021112927598423
Iter: 1589 loss: 0.0017020434409709823
Iter: 1590 loss: 0.0017026327017212923
Iter: 1591 loss: 0.0017020398247157584
Iter: 1592 loss: 0.0017019759445253866
Iter: 1593 loss: 0.0017020024902700492
Iter: 1594 loss: 0.0017019320146700241
Iter: 1595 loss: 0.0017018568671649296
Iter: 1596 loss: 0.0017019883123544637
Iter: 1597 loss: 0.0017018238201012511
Iter: 1598 loss: 0.0017017767974941995
Iter: 1599 loss: 0.0017017698930626732
Iter: 1600 loss: 0.0017017443686089101
Iter: 1601 loss: 0.0017016785906001949
Iter: 1602 loss: 0.0017022385393992213
Iter: 1603 loss: 0.0017016671635345277
Iter: 1604 loss: 0.0017015961735076949
Iter: 1605 loss: 0.0017024887130470574
Iter: 1606 loss: 0.0017015955506516946
Iter: 1607 loss: 0.0017015599671427288
Iter: 1608 loss: 0.0017017467550468229
Iter: 1609 loss: 0.0017015544255270772
Iter: 1610 loss: 0.0017015078182324804
Iter: 1611 loss: 0.0017013715511862257
Iter: 1612 loss: 0.0017019167571664547
Iter: 1613 loss: 0.0017013147357903386
Iter: 1614 loss: 0.0017011921855006615
Iter: 1615 loss: 0.001701183842608612
Iter: 1616 loss: 0.0017010351466289254
Iter: 1617 loss: 0.0017016358809872313
Iter: 1618 loss: 0.0017010015828816476
Iter: 1619 loss: 0.0017009575842102272
Iter: 1620 loss: 0.001700964086083064
Iter: 1621 loss: 0.0017009241188250621
Iter: 1622 loss: 0.0017008583924969485
Iter: 1623 loss: 0.0017007556130015531
Iter: 1624 loss: 0.0017007543502941577
Iter: 1625 loss: 0.001700645162103161
Iter: 1626 loss: 0.00170063952443166
Iter: 1627 loss: 0.0017005545571316418
Iter: 1628 loss: 0.0017005105080145778
Iter: 1629 loss: 0.0017004709725749272
Iter: 1630 loss: 0.0017003758546736266
Iter: 1631 loss: 0.0017010395774550617
Iter: 1632 loss: 0.0017003670221707413
Iter: 1633 loss: 0.0017002512144185717
Iter: 1634 loss: 0.0017005296397259615
Iter: 1635 loss: 0.001700209499017221
Iter: 1636 loss: 0.0017001092529469609
Iter: 1637 loss: 0.0017003100781693493
Iter: 1638 loss: 0.0017000687818762882
Iter: 1639 loss: 0.0016999357635619069
Iter: 1640 loss: 0.0017003584167728548
Iter: 1641 loss: 0.001699898144066151
Iter: 1642 loss: 0.0016998118007133403
Iter: 1643 loss: 0.0016997470898390851
Iter: 1644 loss: 0.0016997181812909142
Iter: 1645 loss: 0.0016995804138652867
Iter: 1646 loss: 0.00170098388514374
Iter: 1647 loss: 0.0016995758272882724
Iter: 1648 loss: 0.0016994281653677017
Iter: 1649 loss: 0.0017004650266860229
Iter: 1650 loss: 0.0016994147855680206
Iter: 1651 loss: 0.0016993042085084514
Iter: 1652 loss: 0.0016990254706862973
Iter: 1653 loss: 0.001701676530925544
Iter: 1654 loss: 0.0016989844680054757
Iter: 1655 loss: 0.0016987130500689916
Iter: 1656 loss: 0.0016995360174007286
Iter: 1657 loss: 0.00169863269000329
Iter: 1658 loss: 0.0016985210750568246
Iter: 1659 loss: 0.0016984954111889544
Iter: 1660 loss: 0.0016983424182160554
Iter: 1661 loss: 0.0016980078046166483
Iter: 1662 loss: 0.0017042987694320202
Iter: 1663 loss: 0.0016979965213833926
Iter: 1664 loss: 0.0016975740444984395
Iter: 1665 loss: 0.0016987274507349007
Iter: 1666 loss: 0.0016974366520540716
Iter: 1667 loss: 0.0016972842826634726
Iter: 1668 loss: 0.0016972825186705714
Iter: 1669 loss: 0.0016971705722795703
Iter: 1670 loss: 0.0016988348844254313
Iter: 1671 loss: 0.0016971704273514327
Iter: 1672 loss: 0.0016971116031159372
Iter: 1673 loss: 0.0016970288066139954
Iter: 1674 loss: 0.0016970258272249747
Iter: 1675 loss: 0.0016969065579731726
Iter: 1676 loss: 0.0016968936305421423
Iter: 1677 loss: 0.0016968452202394727
Iter: 1678 loss: 0.0016968425636368289
Iter: 1679 loss: 0.0016968045597970096
Iter: 1680 loss: 0.0016967367471667275
Iter: 1681 loss: 0.0016984020057767858
Iter: 1682 loss: 0.0016967367459928481
Iter: 1683 loss: 0.0016966480985296042
Iter: 1684 loss: 0.0016966478219074717
Iter: 1685 loss: 0.0016965624938922127
Iter: 1686 loss: 0.0016967175419337257
Iter: 1687 loss: 0.0016965264604147448
Iter: 1688 loss: 0.001696442722758268
Iter: 1689 loss: 0.0016963690703082721
Iter: 1690 loss: 0.0016963471882521437
Iter: 1691 loss: 0.0016962228963985295
Iter: 1692 loss: 0.0016965076291619943
Iter: 1693 loss: 0.0016961765783589493
Iter: 1694 loss: 0.0016960726076571202
Iter: 1695 loss: 0.001695820163572923
Iter: 1696 loss: 0.0016985516741629186
Iter: 1697 loss: 0.001695792287944203
Iter: 1698 loss: 0.0016955702599078569
Iter: 1699 loss: 0.0016962574168642227
Iter: 1700 loss: 0.0016955054362606486
Iter: 1701 loss: 0.0016953051718521638
Iter: 1702 loss: 0.0016958344596228084
Iter: 1703 loss: 0.0016952381701428325
Iter: 1704 loss: 0.0016950550417321533
Iter: 1705 loss: 0.0016951688947120539
Iter: 1706 loss: 0.0016949370246760929
Iter: 1707 loss: 0.001694803864778589
Iter: 1708 loss: 0.0016950483099629349
Iter: 1709 loss: 0.0016947465332620227
Iter: 1710 loss: 0.0016946865463663119
Iter: 1711 loss: 0.0016946849033651968
Iter: 1712 loss: 0.0016945963165802672
Iter: 1713 loss: 0.0016943631239824779
Iter: 1714 loss: 0.0016963005621059173
Iter: 1715 loss: 0.0016943174088754919
Iter: 1716 loss: 0.0016940659168825588
Iter: 1717 loss: 0.0016940595286279109
Iter: 1718 loss: 0.0016938748569400664
Iter: 1719 loss: 0.0016938039845600236
Iter: 1720 loss: 0.0016937024184414066
Iter: 1721 loss: 0.0016933967374814281
Iter: 1722 loss: 0.0016951616723194203
Iter: 1723 loss: 0.0016933626246140839
Iter: 1724 loss: 0.0016931069277924648
Iter: 1725 loss: 0.0016951179388085805
Iter: 1726 loss: 0.0016930849284548894
Iter: 1727 loss: 0.0016929929531916256
Iter: 1728 loss: 0.0016929576705125287
Iter: 1729 loss: 0.0016929088308422716
Iter: 1730 loss: 0.0016927964534614042
Iter: 1731 loss: 0.0016933835261100261
Iter: 1732 loss: 0.0016927745487684419
Iter: 1733 loss: 0.0016927317630773511
Iter: 1734 loss: 0.0016927160810047827
Iter: 1735 loss: 0.0016926380118024705
Iter: 1736 loss: 0.0016926581800064695
Iter: 1737 loss: 0.0016925812852874307
Iter: 1738 loss: 0.0016924454775992818
Iter: 1739 loss: 0.0016924051724780615
Iter: 1740 loss: 0.0016923223012687988
Iter: 1741 loss: 0.0016921831905663291
Iter: 1742 loss: 0.0016927147851451417
Iter: 1743 loss: 0.0016921500716594313
Iter: 1744 loss: 0.0016921082345812168
Iter: 1745 loss: 0.0016920915344117516
Iter: 1746 loss: 0.0016920318229211063
Iter: 1747 loss: 0.0016920843846089232
Iter: 1748 loss: 0.00169199657882718
Iter: 1749 loss: 0.001691911406524881
Iter: 1750 loss: 0.0016919827587675829
Iter: 1751 loss: 0.0016918606487481545
Iter: 1752 loss: 0.0016917634865669655
Iter: 1753 loss: 0.0016922344612184004
Iter: 1754 loss: 0.001691746069209325
Iter: 1755 loss: 0.0016916291997633955
Iter: 1756 loss: 0.0016917069321193226
Iter: 1757 loss: 0.0016915522936013245
Iter: 1758 loss: 0.0016914821246891123
Iter: 1759 loss: 0.0016915799314029156
Iter: 1760 loss: 0.0016914490572369719
Iter: 1761 loss: 0.0016913849789926352
Iter: 1762 loss: 0.0016915207066394455
Iter: 1763 loss: 0.0016913567928434283
Iter: 1764 loss: 0.00169120089881008
Iter: 1765 loss: 0.001690898221955854
Iter: 1766 loss: 0.0016970067746448494
Iter: 1767 loss: 0.0016908961012141411
Iter: 1768 loss: 0.0016908658319991544
Iter: 1769 loss: 0.0016906965786955119
Iter: 1770 loss: 0.0016904858841444831
Iter: 1771 loss: 0.0016906621570923681
Iter: 1772 loss: 0.0016903610355785948
Iter: 1773 loss: 0.0016901885701629234
Iter: 1774 loss: 0.0016906811458418161
Iter: 1775 loss: 0.0016901338443259821
Iter: 1776 loss: 0.001690019635152512
Iter: 1777 loss: 0.0016905984201368391
Iter: 1778 loss: 0.0016900006841959723
Iter: 1779 loss: 0.0016898550863840161
Iter: 1780 loss: 0.0016903708213191278
Iter: 1781 loss: 0.0016898174920258354
Iter: 1782 loss: 0.0016896929845173119
Iter: 1783 loss: 0.0016896407769444357
Iter: 1784 loss: 0.0016895741493243338
Iter: 1785 loss: 0.0016892948190741553
Iter: 1786 loss: 0.0016902890394166609
Iter: 1787 loss: 0.0016892234367457074
Iter: 1788 loss: 0.0016889306992707818
Iter: 1789 loss: 0.0016889250301511845
Iter: 1790 loss: 0.0016886989047412734
Iter: 1791 loss: 0.0016881692057065492
Iter: 1792 loss: 0.0016992915243286735
Iter: 1793 loss: 0.0016881690612985658
Iter: 1794 loss: 0.0016878694568030167
Iter: 1795 loss: 0.0016878408183076585
Iter: 1796 loss: 0.001687654247119842
Iter: 1797 loss: 0.0016876626583852052
Iter: 1798 loss: 0.001687508180086237
Iter: 1799 loss: 0.0016874332205591225
Iter: 1800 loss: 0.0016873143562663221
Iter: 1801 loss: 0.0016870842475741731
Iter: 1802 loss: 0.0016902179389794751
Iter: 1803 loss: 0.0016870770478459416
Iter: 1804 loss: 0.0016869219257039179
Iter: 1805 loss: 0.0016868058053262718
Iter: 1806 loss: 0.0016867546930473371
Iter: 1807 loss: 0.001686478007051071
Iter: 1808 loss: 0.0016870076281515465
Iter: 1809 loss: 0.0016863525556935308
Iter: 1810 loss: 0.0016861345654617854
Iter: 1811 loss: 0.0016890066549802248
Iter: 1812 loss: 0.0016861333099865072
Iter: 1813 loss: 0.0016859753660478777
Iter: 1814 loss: 0.0016863869026425265
Iter: 1815 loss: 0.0016859233770612186
Iter: 1816 loss: 0.0016856845967505797
Iter: 1817 loss: 0.0016859124855213589
Iter: 1818 loss: 0.0016855382152329693
Iter: 1819 loss: 0.0016851164015045683
Iter: 1820 loss: 0.0016869320072012597
Iter: 1821 loss: 0.0016850290179903121
Iter: 1822 loss: 0.0016851442308976118
Iter: 1823 loss: 0.001684885982159099
Iter: 1824 loss: 0.0016847391529413521
Iter: 1825 loss: 0.0016846029837865161
Iter: 1826 loss: 0.0016845679663322476
Iter: 1827 loss: 0.0016843837514852848
Iter: 1828 loss: 0.0016862865872081436
Iter: 1829 loss: 0.0016843789118739651
Iter: 1830 loss: 0.0016842341900141248
Iter: 1831 loss: 0.0016847779855866832
Iter: 1832 loss: 0.0016842020457373454
Iter: 1833 loss: 0.0016840431227440715
Iter: 1834 loss: 0.0016841918318809231
Iter: 1835 loss: 0.0016839491990452856
Iter: 1836 loss: 0.0016838176830059239
Iter: 1837 loss: 0.0016843428899062894
Iter: 1838 loss: 0.0016837880390005912
Iter: 1839 loss: 0.0016837023591616184
Iter: 1840 loss: 0.0016837332399446969
Iter: 1841 loss: 0.0016836438659109088
Iter: 1842 loss: 0.0016834447301023626
Iter: 1843 loss: 0.00168430502581623
Iter: 1844 loss: 0.0016833991641756227
Iter: 1845 loss: 0.0016833352678018449
Iter: 1846 loss: 0.0016832825735868169
Iter: 1847 loss: 0.0016831504834546056
Iter: 1848 loss: 0.0016828094464845528
Iter: 1849 loss: 0.0016854897545823609
Iter: 1850 loss: 0.0016827470715027619
Iter: 1851 loss: 0.0016824795384108781
Iter: 1852 loss: 0.001683640512677294
Iter: 1853 loss: 0.0016824259450873515
Iter: 1854 loss: 0.0016822799098131667
Iter: 1855 loss: 0.0016822797155218896
Iter: 1856 loss: 0.0016821303907416322
Iter: 1857 loss: 0.0016821530199796469
Iter: 1858 loss: 0.0016820169511754289
Iter: 1859 loss: 0.0016817791902473589
Iter: 1860 loss: 0.0016829138546070508
Iter: 1861 loss: 0.0016817350306109237
Iter: 1862 loss: 0.0016816116252019522
Iter: 1863 loss: 0.0016816018373916504
Iter: 1864 loss: 0.001681550461893302
Iter: 1865 loss: 0.0016815921517976565
Iter: 1866 loss: 0.0016815197533157111
Iter: 1867 loss: 0.0016814293809774739
Iter: 1868 loss: 0.0016817473547303145
Iter: 1869 loss: 0.0016814055991185885
Iter: 1870 loss: 0.0016812966265316437
Iter: 1871 loss: 0.0016818342940515512
Iter: 1872 loss: 0.0016812759144213081
Iter: 1873 loss: 0.0016811422803857212
Iter: 1874 loss: 0.0016808242653038807
Iter: 1875 loss: 0.001684309426331701
Iter: 1876 loss: 0.001680793365327046
Iter: 1877 loss: 0.0016803726199395733
Iter: 1878 loss: 0.0016863159932563802
Iter: 1879 loss: 0.0016803711550615543
Iter: 1880 loss: 0.0016800256113899332
Iter: 1881 loss: 0.0016825822371456301
Iter: 1882 loss: 0.0016799966769394893
Iter: 1883 loss: 0.001679884668081914
Iter: 1884 loss: 0.0016801524272692771
Iter: 1885 loss: 0.0016798448687708142
Iter: 1886 loss: 0.0016797490853769515
Iter: 1887 loss: 0.0016798331593922927
Iter: 1888 loss: 0.0016796917645734597
Iter: 1889 loss: 0.001679527507356786
Iter: 1890 loss: 0.0016795877695785924
Iter: 1891 loss: 0.0016794120445574098
Iter: 1892 loss: 0.0016790970529607617
Iter: 1893 loss: 0.0016805201931259241
Iter: 1894 loss: 0.0016790392988408859
Iter: 1895 loss: 0.0016788191928849374
Iter: 1896 loss: 0.0016792369967428434
Iter: 1897 loss: 0.0016787259294125452
Iter: 1898 loss: 0.0016784801448261714
Iter: 1899 loss: 0.0016792824268533871
Iter: 1900 loss: 0.001678409776826421
Iter: 1901 loss: 0.001678180941744323
Iter: 1902 loss: 0.0016796468375706954
Iter: 1903 loss: 0.0016781556747150273
Iter: 1904 loss: 0.0016780246998820425
Iter: 1905 loss: 0.0016780182275687597
Iter: 1906 loss: 0.0016779174909757764
Iter: 1907 loss: 0.0016777489501685317
Iter: 1908 loss: 0.0016774783251185608
Iter: 1909 loss: 0.0016774761757854567
Iter: 1910 loss: 0.0016778676209555001
Iter: 1911 loss: 0.0016772825751297155
Iter: 1912 loss: 0.0016771173525450076
Iter: 1913 loss: 0.0016777768777463549
Iter: 1914 loss: 0.0016770819654191023
Iter: 1915 loss: 0.0016769370812568184
Iter: 1916 loss: 0.0016770519990032245
Iter: 1917 loss: 0.0016768486295606808
Iter: 1918 loss: 0.0016767173697542132
Iter: 1919 loss: 0.0016773703363817923
Iter: 1920 loss: 0.0016766957388298688
Iter: 1921 loss: 0.0016765867570654818
Iter: 1922 loss: 0.0016767784242193867
Iter: 1923 loss: 0.0016765386152381078
Iter: 1924 loss: 0.0016763958010985402
Iter: 1925 loss: 0.0016762585827382417
Iter: 1926 loss: 0.0016762264641698625
Iter: 1927 loss: 0.0016759318356765896
Iter: 1928 loss: 0.001677238906360615
Iter: 1929 loss: 0.0016758743985542072
Iter: 1930 loss: 0.0016756682595525764
Iter: 1931 loss: 0.0016767042331509658
Iter: 1932 loss: 0.0016756329484793767
Iter: 1933 loss: 0.0016754264800883825
Iter: 1934 loss: 0.0016762792598934921
Iter: 1935 loss: 0.0016753830447258122
Iter: 1936 loss: 0.0016752841278670006
Iter: 1937 loss: 0.0016758219722133325
Iter: 1938 loss: 0.0016752693448991015
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.4 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi0.4
+ date
Sat Nov  7 18:55:54 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.4/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0/300_300_300_1 --function f1 --psi 2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2ccf1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2db6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2db6ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2d1ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2d1a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2c0a488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2c7ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2d1a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2b79510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2b799d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2c36268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19c165ef28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19c16616a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2b31ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19c16358c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19c163fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19c1634488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19c1634d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19c15716a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19c1571c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c62f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c62fb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c59e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c5a7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c5a7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f19e2bea620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c545730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c532840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c532620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c532158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c585bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c4b9268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c4d51e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c4cdea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c43ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f199c42bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.07101616759899285
test_loss: 0.06950783122939055
train_loss: 0.03765207653293243
test_loss: 0.0392074529727409
train_loss: 0.03461723002511985
test_loss: 0.03480412946075321
train_loss: 0.034133347063277454
test_loss: 0.03427263965074122
train_loss: 0.03179442917917903
test_loss: 0.03306325360916395
train_loss: 0.031217517324574007
test_loss: 0.031835061938385575
train_loss: 0.029720351703345006
test_loss: 0.03149894497597341
train_loss: 0.02587632887793341
test_loss: 0.02800368599333502
train_loss: 0.022336224058233597
test_loss: 0.022960577317346867
train_loss: 0.017877454998921148
test_loss: 0.01804026924896949
train_loss: 0.013173094801045473
test_loss: 0.012562638201420772
train_loss: 0.01015876890846164
test_loss: 0.011522849427433795
train_loss: 0.008669527489715579
test_loss: 0.009281852381292812
train_loss: 0.008070629615121842
test_loss: 0.008429916379229794
train_loss: 0.00794035905269904
test_loss: 0.007700893493662668
train_loss: 0.0073861796057998865
test_loss: 0.007388094193339298
train_loss: 0.006546799761044538
test_loss: 0.006878689544652685
train_loss: 0.0069306862627935666
test_loss: 0.007021002492286459
train_loss: 0.007059692269861424
test_loss: 0.006918861332542128
train_loss: 0.006237063543950021
test_loss: 0.006282745301526609
train_loss: 0.005883100221789501
test_loss: 0.006303231673492928
train_loss: 0.005926552230950949
test_loss: 0.006057136371007001
train_loss: 0.00592911319893905
test_loss: 0.005829717229390544
train_loss: 0.005974497376167126
test_loss: 0.005700214809623191
train_loss: 0.005370740606088287
test_loss: 0.005337822734033543
train_loss: 0.005594836181825696
test_loss: 0.005256332604954663
train_loss: 0.005358776975495242
test_loss: 0.0056489447727091835
train_loss: 0.005493183154185957
test_loss: 0.0056500792779682245
train_loss: 0.005845836907773591
test_loss: 0.005767935868790525
train_loss: 0.005334150344172601
test_loss: 0.005386195462847333
train_loss: 0.005612860675708551
test_loss: 0.005523750242359889
train_loss: 0.0051625601374558966
test_loss: 0.005253185552198135
train_loss: 0.00555630917221063
test_loss: 0.005237766898861958
train_loss: 0.0051244983684665315
test_loss: 0.0052474459376879515
train_loss: 0.004917628330031674
test_loss: 0.005062080236089104
train_loss: 0.005175965423011791
test_loss: 0.00522288428766105
train_loss: 0.004685314079459426
test_loss: 0.005131704912505611
train_loss: 0.004885009501222444
test_loss: 0.005419883129320339
train_loss: 0.0052717140261378265
test_loss: 0.005384506577298548
train_loss: 0.00488082418067174
test_loss: 0.004977155142227209
train_loss: 0.004539286900732079
test_loss: 0.004949300952272043
train_loss: 0.004999923877474812
test_loss: 0.005135370607075712
train_loss: 0.004727373485066011
test_loss: 0.004842081351473956
train_loss: 0.004579230836006077
test_loss: 0.004872669094854092
train_loss: 0.005008710560571039
test_loss: 0.0050782771366635615
train_loss: 0.004726224361004544
test_loss: 0.004862327487234092
train_loss: 0.0045781383803259745
test_loss: 0.004640962641719927
train_loss: 0.004816867890056002
test_loss: 0.004797962550510272
train_loss: 0.0047423357900307685
test_loss: 0.005154576153372243
train_loss: 0.004637404369118219
test_loss: 0.004882683832470482
train_loss: 0.004454255583820281
test_loss: 0.004527672609577655
train_loss: 0.00428840586860017
test_loss: 0.004370890593682208
train_loss: 0.004598908006411865
test_loss: 0.004584906403566856
train_loss: 0.004601105334959768
test_loss: 0.004596112842136132
train_loss: 0.004412452136447074
test_loss: 0.004558649209364445
train_loss: 0.004815891389908646
test_loss: 0.00493240023557709
train_loss: 0.004416927315666888
test_loss: 0.004572049966562457
train_loss: 0.004623957885832053
test_loss: 0.004581559814463664
train_loss: 0.004442604462728212
test_loss: 0.00527372546534049
train_loss: 0.004305380188403529
test_loss: 0.004320986916202694
train_loss: 0.004604218966226443
test_loss: 0.004695455003009274
train_loss: 0.004919810862520227
test_loss: 0.005171725612870346
train_loss: 0.004543511037695538
test_loss: 0.00459328232679227
train_loss: 0.004461916045785148
test_loss: 0.004567364496781248
train_loss: 0.004883934275903312
test_loss: 0.0047244674756893494
train_loss: 0.0043045954186606275
test_loss: 0.004435831342832936
train_loss: 0.004124011226057708
test_loss: 0.004431882243299617
train_loss: 0.004691078815077042
test_loss: 0.004719334217829466
train_loss: 0.0046180072132822585
test_loss: 0.004619435618381725
train_loss: 0.004405543934343705
test_loss: 0.00457252902696525
train_loss: 0.004254395884662208
test_loss: 0.0046899273817586715
train_loss: 0.004542877357777482
test_loss: 0.00486580687418017
train_loss: 0.0042573793903924444
test_loss: 0.004338694811991896
train_loss: 0.004301522929562615
test_loss: 0.004358951518680819
train_loss: 0.004467579496213751
test_loss: 0.0045420454103843435
train_loss: 0.004418223641761404
test_loss: 0.004276431127505618
train_loss: 0.004298666952418883
test_loss: 0.004784379746016626
train_loss: 0.004196066692401967
test_loss: 0.004378935464595713
train_loss: 0.004129833501535822
test_loss: 0.004201209557967779
train_loss: 0.004071830046776277
test_loss: 0.004213502360364577
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi0.4/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi0.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d789378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d74b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d74b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d802b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d6bd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d69d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d67eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d6927b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d656510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d62d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d5b2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d5bcd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf9d5ac950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf7b13b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf7b0ef510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf7b0daea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf7b097510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf7b0aca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf7b04e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf7b03f158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf7b03f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf7b03f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf540799d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf540b6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf540b6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf54043620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf407df6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf407df488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf4079b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf407b6378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf40769a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf40731158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf407251e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf406d6620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf406d0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fdf406b68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.620522299610288e-05
Iter: 2 loss: 2.1376312656519174e-05
Iter: 3 loss: 2.1323398970205722e-05
Iter: 4 loss: 1.8432291927587524e-05
Iter: 5 loss: 2.0856868736924335e-05
Iter: 6 loss: 1.6710435851734398e-05
Iter: 7 loss: 1.5067113178031241e-05
Iter: 8 loss: 1.8497778681487873e-05
Iter: 9 loss: 1.441679576968334e-05
Iter: 10 loss: 1.3600017725250746e-05
Iter: 11 loss: 1.358372708537981e-05
Iter: 12 loss: 1.2938218171281799e-05
Iter: 13 loss: 1.3095474979424784e-05
Iter: 14 loss: 1.2465525165675148e-05
Iter: 15 loss: 1.190240370609288e-05
Iter: 16 loss: 1.1875438900051743e-05
Iter: 17 loss: 1.1443640357561389e-05
Iter: 18 loss: 1.1086618962540735e-05
Iter: 19 loss: 1.0987203260242511e-05
Iter: 20 loss: 1.0740824604631386e-05
Iter: 21 loss: 1.0234015156273769e-05
Iter: 22 loss: 1.9215222256643115e-05
Iter: 23 loss: 1.0224039715129001e-05
Iter: 24 loss: 9.612177950583382e-06
Iter: 25 loss: 9.0525464653070948e-06
Iter: 26 loss: 8.9028062636436316e-06
Iter: 27 loss: 8.43150709577532e-06
Iter: 28 loss: 8.4095062112781363e-06
Iter: 29 loss: 8.1628537828591333e-06
Iter: 30 loss: 8.150764824557885e-06
Iter: 31 loss: 8.0291270809642758e-06
Iter: 32 loss: 7.7469037440690013e-06
Iter: 33 loss: 1.1239175313128335e-05
Iter: 34 loss: 7.7253306295903815e-06
Iter: 35 loss: 7.6138339516091648e-06
Iter: 36 loss: 7.5880659882778345e-06
Iter: 37 loss: 7.4392816498822304e-06
Iter: 38 loss: 7.4111427183171218e-06
Iter: 39 loss: 7.3114108934233805e-06
Iter: 40 loss: 7.18216990281076e-06
Iter: 41 loss: 7.3971534318521332e-06
Iter: 42 loss: 7.1235295727946768e-06
Iter: 43 loss: 7.0065000814468907e-06
Iter: 44 loss: 8.54438219357334e-06
Iter: 45 loss: 7.0058035285989887e-06
Iter: 46 loss: 6.9401103689985046e-06
Iter: 47 loss: 6.8427534888826276e-06
Iter: 48 loss: 6.8404534657720943e-06
Iter: 49 loss: 6.7129500861320418e-06
Iter: 50 loss: 7.1881689628494791e-06
Iter: 51 loss: 6.6818511565032356e-06
Iter: 52 loss: 6.5868787848662645e-06
Iter: 53 loss: 8.10388669238732e-06
Iter: 54 loss: 6.5868786949891562e-06
Iter: 55 loss: 6.5245652376616185e-06
Iter: 56 loss: 6.4544000935749166e-06
Iter: 57 loss: 6.4450801776987055e-06
Iter: 58 loss: 6.3696309501540022e-06
Iter: 59 loss: 6.3575500822815104e-06
Iter: 60 loss: 6.305485106006412e-06
Iter: 61 loss: 6.222082147074266e-06
Iter: 62 loss: 6.7926269741494291e-06
Iter: 63 loss: 6.21400878842106e-06
Iter: 64 loss: 6.1593601669071778e-06
Iter: 65 loss: 6.1575272326465585e-06
Iter: 66 loss: 6.1212089518434711e-06
Iter: 67 loss: 6.046027253457872e-06
Iter: 68 loss: 7.3542028776731777e-06
Iter: 69 loss: 6.0444021589792426e-06
Iter: 70 loss: 5.9816754444016369e-06
Iter: 71 loss: 6.0940410062102542e-06
Iter: 72 loss: 5.9543263881886871e-06
Iter: 73 loss: 5.8906006649681839e-06
Iter: 74 loss: 6.1678560798161843e-06
Iter: 75 loss: 5.8776451659089465e-06
Iter: 76 loss: 5.8589070233741434e-06
Iter: 77 loss: 5.8448922346702887e-06
Iter: 78 loss: 5.8276145808509122e-06
Iter: 79 loss: 5.7815155129749874e-06
Iter: 80 loss: 6.103647770794526e-06
Iter: 81 loss: 5.7711081925493752e-06
Iter: 82 loss: 5.7289956769220247e-06
Iter: 83 loss: 5.9657302374225422e-06
Iter: 84 loss: 5.7231032317233075e-06
Iter: 85 loss: 5.7008481991064421e-06
Iter: 86 loss: 5.69823779007391e-06
Iter: 87 loss: 5.6818141114900547e-06
Iter: 88 loss: 5.6487842791439281e-06
Iter: 89 loss: 6.26807909890426e-06
Iter: 90 loss: 5.6483296116186963e-06
Iter: 91 loss: 5.6173110330799481e-06
Iter: 92 loss: 5.8344833746035853e-06
Iter: 93 loss: 5.6144622040745566e-06
Iter: 94 loss: 5.5947065156501613e-06
Iter: 95 loss: 5.8770189897280544e-06
Iter: 96 loss: 5.5946671447389077e-06
Iter: 97 loss: 5.5799654283753215e-06
Iter: 98 loss: 5.5628017004968231e-06
Iter: 99 loss: 5.5608434939383053e-06
Iter: 100 loss: 5.5378820682234013e-06
Iter: 101 loss: 5.5218789884247862e-06
Iter: 102 loss: 5.5136578487262459e-06
Iter: 103 loss: 5.4816759486379386e-06
Iter: 104 loss: 5.7070762201807141e-06
Iter: 105 loss: 5.4787762645003231e-06
Iter: 106 loss: 5.4637920163212287e-06
Iter: 107 loss: 5.4614747345447821e-06
Iter: 108 loss: 5.45195003688923e-06
Iter: 109 loss: 5.4276286454649118e-06
Iter: 110 loss: 5.636729428005295e-06
Iter: 111 loss: 5.4236172130023062e-06
Iter: 112 loss: 5.3997625617863951e-06
Iter: 113 loss: 5.4418880284135161e-06
Iter: 114 loss: 5.3892833834449894e-06
Iter: 115 loss: 5.3650212992222248e-06
Iter: 116 loss: 5.5167244083005142e-06
Iter: 117 loss: 5.36220818188393e-06
Iter: 118 loss: 5.3516474252761235e-06
Iter: 119 loss: 5.3488569154449543e-06
Iter: 120 loss: 5.34194355732787e-06
Iter: 121 loss: 5.325865542815131e-06
Iter: 122 loss: 5.5233728569703424e-06
Iter: 123 loss: 5.32461108790874e-06
Iter: 124 loss: 5.3085493124031752e-06
Iter: 125 loss: 5.36531264746811e-06
Iter: 126 loss: 5.3044063590219308e-06
Iter: 127 loss: 5.2887024762120936e-06
Iter: 128 loss: 5.51772711598489e-06
Iter: 129 loss: 5.28868112941576e-06
Iter: 130 loss: 5.28139451200487e-06
Iter: 131 loss: 5.2673694732628287e-06
Iter: 132 loss: 5.5602041152898279e-06
Iter: 133 loss: 5.267292690100198e-06
Iter: 134 loss: 5.2566287196711062e-06
Iter: 135 loss: 5.2565816413172422e-06
Iter: 136 loss: 5.2473012201677349e-06
Iter: 137 loss: 5.271900935182306e-06
Iter: 138 loss: 5.2441922286013043e-06
Iter: 139 loss: 5.23603130393571e-06
Iter: 140 loss: 5.2272447145886183e-06
Iter: 141 loss: 5.2258556218675914e-06
Iter: 142 loss: 5.211495448375131e-06
Iter: 143 loss: 5.220713237096258e-06
Iter: 144 loss: 5.2023739571275511e-06
Iter: 145 loss: 5.1971743479199805e-06
Iter: 146 loss: 5.1946942008514254e-06
Iter: 147 loss: 5.1869935140788042e-06
Iter: 148 loss: 5.1862134379682653e-06
Iter: 149 loss: 5.1805926845254672e-06
Iter: 150 loss: 5.1726637705763086e-06
Iter: 151 loss: 5.1605806153316192e-06
Iter: 152 loss: 5.16037173543285e-06
Iter: 153 loss: 5.146356126904465e-06
Iter: 154 loss: 5.1931767118139144e-06
Iter: 155 loss: 5.1425314611945169e-06
Iter: 156 loss: 5.13712372418978e-06
Iter: 157 loss: 5.1357355600659805e-06
Iter: 158 loss: 5.1285298602639969e-06
Iter: 159 loss: 5.1346714709749326e-06
Iter: 160 loss: 5.1242705059261927e-06
Iter: 161 loss: 5.1191131046222413e-06
Iter: 162 loss: 5.1079835938385407e-06
Iter: 163 loss: 5.2803884694515548e-06
Iter: 164 loss: 5.1075739446926e-06
Iter: 165 loss: 5.1062735459426839e-06
Iter: 166 loss: 5.1015066758277443e-06
Iter: 167 loss: 5.096084737439474e-06
Iter: 168 loss: 5.0875930589568036e-06
Iter: 169 loss: 5.0874920230287588e-06
Iter: 170 loss: 5.0808189876354425e-06
Iter: 171 loss: 5.106687481764036e-06
Iter: 172 loss: 5.079262146779612e-06
Iter: 173 loss: 5.0739008957585013e-06
Iter: 174 loss: 5.0739008907184e-06
Iter: 175 loss: 5.0702320503914387e-06
Iter: 176 loss: 5.0629026402245964e-06
Iter: 177 loss: 5.2025167988816062e-06
Iter: 178 loss: 5.0628128295417012e-06
Iter: 179 loss: 5.0551970822583176e-06
Iter: 180 loss: 5.0922158857610465e-06
Iter: 181 loss: 5.0538631141236963e-06
Iter: 182 loss: 5.0475057359867626e-06
Iter: 183 loss: 5.057498233581307e-06
Iter: 184 loss: 5.0445367326096083e-06
Iter: 185 loss: 5.040454136551847e-06
Iter: 186 loss: 5.0398463273101721e-06
Iter: 187 loss: 5.0373991391966436e-06
Iter: 188 loss: 5.0306320399990741e-06
Iter: 189 loss: 5.06931842893439e-06
Iter: 190 loss: 5.0286753155745706e-06
Iter: 191 loss: 5.0195604958378974e-06
Iter: 192 loss: 5.0412228500174105e-06
Iter: 193 loss: 5.0162443215162205e-06
Iter: 194 loss: 5.0160753386234072e-06
Iter: 195 loss: 5.0129818408793484e-06
Iter: 196 loss: 5.00957995470573e-06
Iter: 197 loss: 5.00447880576181e-06
Iter: 198 loss: 5.0043721762521394e-06
Iter: 199 loss: 4.9990686244416148e-06
Iter: 200 loss: 4.9984107974635866e-06
Iter: 201 loss: 4.9946218042033755e-06
Iter: 202 loss: 4.9925777275527852e-06
Iter: 203 loss: 4.9912154698874405e-06
Iter: 204 loss: 4.9878228652923875e-06
Iter: 205 loss: 4.9809438836906792e-06
Iter: 206 loss: 5.1071219134043481e-06
Iter: 207 loss: 4.9808351290128839e-06
Iter: 208 loss: 4.9762001639513346e-06
Iter: 209 loss: 5.0031224584416357e-06
Iter: 210 loss: 4.9755866001171548e-06
Iter: 211 loss: 4.9719273524674147e-06
Iter: 212 loss: 5.0268580424628037e-06
Iter: 213 loss: 4.9719250023272847e-06
Iter: 214 loss: 4.96978783272464e-06
Iter: 215 loss: 4.9640328353127229e-06
Iter: 216 loss: 5.0023069440045055e-06
Iter: 217 loss: 4.9626468205154083e-06
Iter: 218 loss: 4.9569590825284e-06
Iter: 219 loss: 5.0222674419613148e-06
Iter: 220 loss: 4.9568651614676012e-06
Iter: 221 loss: 4.9531124026539771e-06
Iter: 222 loss: 4.9728409597828373e-06
Iter: 223 loss: 4.9525275668105672e-06
Iter: 224 loss: 4.9488556600694489e-06
Iter: 225 loss: 4.9750211080214678e-06
Iter: 226 loss: 4.94853075086402e-06
Iter: 227 loss: 4.9458567696013042e-06
Iter: 228 loss: 4.9403131827189164e-06
Iter: 229 loss: 5.03630546039591e-06
Iter: 230 loss: 4.9401906710314828e-06
Iter: 231 loss: 4.9344578060358477e-06
Iter: 232 loss: 4.9517177099977363e-06
Iter: 233 loss: 4.93273278946284e-06
Iter: 234 loss: 4.93108474250782e-06
Iter: 235 loss: 4.9301848918921732e-06
Iter: 236 loss: 4.9276807372221566e-06
Iter: 237 loss: 4.9255329954607445e-06
Iter: 238 loss: 4.92484983224575e-06
Iter: 239 loss: 4.922416663772597e-06
Iter: 240 loss: 4.9179899381621804e-06
Iter: 241 loss: 5.0232155315681678e-06
Iter: 242 loss: 4.9179879904425324e-06
Iter: 243 loss: 4.9165667945533828e-06
Iter: 244 loss: 4.9148987592451894e-06
Iter: 245 loss: 4.9126664020552387e-06
Iter: 246 loss: 4.9080988823533309e-06
Iter: 247 loss: 4.9899568871849764e-06
Iter: 248 loss: 4.9080156415081441e-06
Iter: 249 loss: 4.9047708091650049e-06
Iter: 250 loss: 4.9269181436665705e-06
Iter: 251 loss: 4.904455407239728e-06
Iter: 252 loss: 4.90113502875405e-06
Iter: 253 loss: 4.9241813898413531e-06
Iter: 254 loss: 4.9008237310272537e-06
Iter: 255 loss: 4.8990592457297296e-06
Iter: 256 loss: 4.89562803926699e-06
Iter: 257 loss: 4.9655092902318479e-06
Iter: 258 loss: 4.8956038488009914e-06
Iter: 259 loss: 4.8917593320125791e-06
Iter: 260 loss: 4.8982608716204722e-06
Iter: 261 loss: 4.8900304353928134e-06
Iter: 262 loss: 4.8886760851567729e-06
Iter: 263 loss: 4.8878853833288151e-06
Iter: 264 loss: 4.8860415554954554e-06
Iter: 265 loss: 4.8864101644984674e-06
Iter: 266 loss: 4.8846716174189085e-06
Iter: 267 loss: 4.8825561478734425e-06
Iter: 268 loss: 4.8808001266069675e-06
Iter: 269 loss: 4.8801932084519169e-06
Iter: 270 loss: 4.8762410092977329e-06
Iter: 271 loss: 4.8836783697264817e-06
Iter: 272 loss: 4.8745668319878348e-06
Iter: 273 loss: 4.8746076569727721e-06
Iter: 274 loss: 4.8730168706165046e-06
Iter: 275 loss: 4.8718795749523248e-06
Iter: 276 loss: 4.8688859255225954e-06
Iter: 277 loss: 4.891255229253291e-06
Iter: 278 loss: 4.868271287924324e-06
Iter: 279 loss: 4.8652833548137981e-06
Iter: 280 loss: 4.8746419878983943e-06
Iter: 281 loss: 4.8644161403306309e-06
Iter: 282 loss: 4.86254513460872e-06
Iter: 283 loss: 4.8623217101712059e-06
Iter: 284 loss: 4.8610429141401451e-06
Iter: 285 loss: 4.8577781332452182e-06
Iter: 286 loss: 4.8858722393880248e-06
Iter: 287 loss: 4.857240442979396e-06
Iter: 288 loss: 4.8551825386591827e-06
Iter: 289 loss: 4.8551374158513937e-06
Iter: 290 loss: 4.8530352280666025e-06
Iter: 291 loss: 4.8586434388071883e-06
Iter: 292 loss: 4.85233471427233e-06
Iter: 293 loss: 4.8509007936698271e-06
Iter: 294 loss: 4.8474465159327762e-06
Iter: 295 loss: 4.8849348916878222e-06
Iter: 296 loss: 4.8470829947760529e-06
Iter: 297 loss: 4.843187191536727e-06
Iter: 298 loss: 4.8663499008342432e-06
Iter: 299 loss: 4.8426921882617987e-06
Iter: 300 loss: 4.8420237461790441e-06
Iter: 301 loss: 4.8412966552973034e-06
Iter: 302 loss: 4.8399317210272391e-06
Iter: 303 loss: 4.8373499863724261e-06
Iter: 304 loss: 4.8935117685433811e-06
Iter: 305 loss: 4.837341513336203e-06
Iter: 306 loss: 4.834768900390362e-06
Iter: 307 loss: 4.8402867727944821e-06
Iter: 308 loss: 4.8337642400472179e-06
Iter: 309 loss: 4.8313533002664381e-06
Iter: 310 loss: 4.8446856584084925e-06
Iter: 311 loss: 4.8310066461400677e-06
Iter: 312 loss: 4.8296017703694552e-06
Iter: 313 loss: 4.8295716051472733e-06
Iter: 314 loss: 4.8284687016469005e-06
Iter: 315 loss: 4.82612789876654e-06
Iter: 316 loss: 4.8641303253031331e-06
Iter: 317 loss: 4.8260572777104093e-06
Iter: 318 loss: 4.82372818320297e-06
Iter: 319 loss: 4.8294220807181979e-06
Iter: 320 loss: 4.8228980711514007e-06
Iter: 321 loss: 4.8218103269160531e-06
Iter: 322 loss: 4.8215466250206841e-06
Iter: 323 loss: 4.8204231791332035e-06
Iter: 324 loss: 4.8175586320635843e-06
Iter: 325 loss: 4.8423410024685266e-06
Iter: 326 loss: 4.8170911882320945e-06
Iter: 327 loss: 4.8152763347884329e-06
Iter: 328 loss: 4.8414211284850656e-06
Iter: 329 loss: 4.8152731983977919e-06
Iter: 330 loss: 4.8134767186087086e-06
Iter: 331 loss: 4.8219965200534552e-06
Iter: 332 loss: 4.8131511912284105e-06
Iter: 333 loss: 4.8120335484841843e-06
Iter: 334 loss: 4.809380600491169e-06
Iter: 335 loss: 4.8397468994145366e-06
Iter: 336 loss: 4.8091335406171943e-06
Iter: 337 loss: 4.8065236370821839e-06
Iter: 338 loss: 4.819483611284322e-06
Iter: 339 loss: 4.8060805431304319e-06
Iter: 340 loss: 4.804789572760528e-06
Iter: 341 loss: 4.8047277236609591e-06
Iter: 342 loss: 4.8032839529998187e-06
Iter: 343 loss: 4.80491233390382e-06
Iter: 344 loss: 4.8025055011938119e-06
Iter: 345 loss: 4.8013844705104044e-06
Iter: 346 loss: 4.7989421892533259e-06
Iter: 347 loss: 4.8357426730700812e-06
Iter: 348 loss: 4.7988420886752588e-06
Iter: 349 loss: 4.7968113083089929e-06
Iter: 350 loss: 4.7968039797371996e-06
Iter: 351 loss: 4.7956283151668255e-06
Iter: 352 loss: 4.7956266876266759e-06
Iter: 353 loss: 4.7947385006626325e-06
Iter: 354 loss: 4.7934193525845658e-06
Iter: 355 loss: 4.7933887414198923e-06
Iter: 356 loss: 4.7917205881419991e-06
Iter: 357 loss: 4.7924872674610408e-06
Iter: 358 loss: 4.7905913784204174e-06
Iter: 359 loss: 4.789878578577328e-06
Iter: 360 loss: 4.78958819413595e-06
Iter: 361 loss: 4.7886188657636522e-06
Iter: 362 loss: 4.7864131336155657e-06
Iter: 363 loss: 4.8155023377338475e-06
Iter: 364 loss: 4.7862721300316152e-06
Iter: 365 loss: 4.7848128222895e-06
Iter: 366 loss: 4.7961416225735007e-06
Iter: 367 loss: 4.7847076044186428e-06
Iter: 368 loss: 4.78352170829694e-06
Iter: 369 loss: 4.7978480941862035e-06
Iter: 370 loss: 4.7835075509255119e-06
Iter: 371 loss: 4.7827675747933748e-06
Iter: 372 loss: 4.780928240157107e-06
Iter: 373 loss: 4.79862693640473e-06
Iter: 374 loss: 4.7806803266498621e-06
Iter: 375 loss: 4.7786705472201122e-06
Iter: 376 loss: 4.7851781626234786e-06
Iter: 377 loss: 4.7781051997679183e-06
Iter: 378 loss: 4.77641466159382e-06
Iter: 379 loss: 4.7846546777819147e-06
Iter: 380 loss: 4.7761199024573422e-06
Iter: 381 loss: 4.7747743191578151e-06
Iter: 382 loss: 4.7747599691491747e-06
Iter: 383 loss: 4.7740990844635195e-06
Iter: 384 loss: 4.7723147010032559e-06
Iter: 385 loss: 4.7840128010589065e-06
Iter: 386 loss: 4.7718769431128315e-06
Iter: 387 loss: 4.7697200664668346e-06
Iter: 388 loss: 4.7805031145063244e-06
Iter: 389 loss: 4.7693574423110017e-06
Iter: 390 loss: 4.7690702636200434e-06
Iter: 391 loss: 4.7685987747031284e-06
Iter: 392 loss: 4.7679267820098811e-06
Iter: 393 loss: 4.7670738373838037e-06
Iter: 394 loss: 4.7670094063556368e-06
Iter: 395 loss: 4.7658497112721972e-06
Iter: 396 loss: 4.7656111651112327e-06
Iter: 397 loss: 4.764846326927503e-06
Iter: 398 loss: 4.763410862637193e-06
Iter: 399 loss: 4.7799045024026483e-06
Iter: 400 loss: 4.7633873083040062e-06
Iter: 401 loss: 4.7621091647952482e-06
Iter: 402 loss: 4.7672919482201033e-06
Iter: 403 loss: 4.7618261375572386e-06
Iter: 404 loss: 4.7610410002724137e-06
Iter: 405 loss: 4.7597325701071714e-06
Iter: 406 loss: 4.7597283502582164e-06
Iter: 407 loss: 4.7590764181957829e-06
Iter: 408 loss: 4.7589237551695516e-06
Iter: 409 loss: 4.7581029581208218e-06
Iter: 410 loss: 4.75681532891525e-06
Iter: 411 loss: 4.7568003938069191e-06
Iter: 412 loss: 4.7554716695017053e-06
Iter: 413 loss: 4.7541642804990457e-06
Iter: 414 loss: 4.7538831685689947e-06
Iter: 415 loss: 4.75203044812673e-06
Iter: 416 loss: 4.7628737521783786e-06
Iter: 417 loss: 4.7517884053585229e-06
Iter: 418 loss: 4.7515009693047393e-06
Iter: 419 loss: 4.7510082932813355e-06
Iter: 420 loss: 4.75040581952036e-06
Iter: 421 loss: 4.7489238020838674e-06
Iter: 422 loss: 4.7637831556228327e-06
Iter: 423 loss: 4.7487395203150981e-06
Iter: 424 loss: 4.7471258850560368e-06
Iter: 425 loss: 4.7497171810742221e-06
Iter: 426 loss: 4.7463803985020553e-06
Iter: 427 loss: 4.7453993893547524e-06
Iter: 428 loss: 4.7453771097877815e-06
Iter: 429 loss: 4.7443336179201742e-06
Iter: 430 loss: 4.74737105069297e-06
Iter: 431 loss: 4.7440102329754369e-06
Iter: 432 loss: 4.7433490048208116e-06
Iter: 433 loss: 4.7422815472225659e-06
Iter: 434 loss: 4.7422740493459872e-06
Iter: 435 loss: 4.7410822389417621e-06
Iter: 436 loss: 4.7532035487366614e-06
Iter: 437 loss: 4.7410465151455731e-06
Iter: 438 loss: 4.7399807069678872e-06
Iter: 439 loss: 4.7454944554415655e-06
Iter: 440 loss: 4.7398104518706946e-06
Iter: 441 loss: 4.7389536227617871e-06
Iter: 442 loss: 4.737977548610801e-06
Iter: 443 loss: 4.737853872177951e-06
Iter: 444 loss: 4.7369353248203355e-06
Iter: 445 loss: 4.7448046750120211e-06
Iter: 446 loss: 4.7368848563968218e-06
Iter: 447 loss: 4.7357289035529818e-06
Iter: 448 loss: 4.7363982353831407e-06
Iter: 449 loss: 4.7349784891980691e-06
Iter: 450 loss: 4.7339227401708749e-06
Iter: 451 loss: 4.7332218396882693e-06
Iter: 452 loss: 4.732824274954333e-06
Iter: 453 loss: 4.7313571609703963e-06
Iter: 454 loss: 4.7326479339309709e-06
Iter: 455 loss: 4.7304979611518023e-06
Iter: 456 loss: 4.7298649237438213e-06
Iter: 457 loss: 4.7297009445600809e-06
Iter: 458 loss: 4.7288157719130669e-06
Iter: 459 loss: 4.7296910445465606e-06
Iter: 460 loss: 4.7283160087980827e-06
Iter: 461 loss: 4.7274812911560167e-06
Iter: 462 loss: 4.7259640120442359e-06
Iter: 463 loss: 4.7621055738836115e-06
Iter: 464 loss: 4.7259633957196447e-06
Iter: 465 loss: 4.7245673542047579e-06
Iter: 466 loss: 4.7326907738182185e-06
Iter: 467 loss: 4.7243831186206651e-06
Iter: 468 loss: 4.7240762338391258e-06
Iter: 469 loss: 4.7237923105223168e-06
Iter: 470 loss: 4.7232656808696514e-06
Iter: 471 loss: 4.7220004396965595e-06
Iter: 472 loss: 4.7358667533770481e-06
Iter: 473 loss: 4.7218701709238093e-06
Iter: 474 loss: 4.7206906502012475e-06
Iter: 475 loss: 4.7262541086566783e-06
Iter: 476 loss: 4.7204753128936318e-06
Iter: 477 loss: 4.7195883899492425e-06
Iter: 478 loss: 4.733368230533688e-06
Iter: 479 loss: 4.7195882681199059e-06
Iter: 480 loss: 4.7188371235725127e-06
Iter: 481 loss: 4.71816383708307e-06
Iter: 482 loss: 4.7179734459293659e-06
Iter: 483 loss: 4.7170885591576343e-06
Iter: 484 loss: 4.7198954238555e-06
Iter: 485 loss: 4.7168347693691587e-06
Iter: 486 loss: 4.7160903483781559e-06
Iter: 487 loss: 4.7247371611353417e-06
Iter: 488 loss: 4.7160788886197961e-06
Iter: 489 loss: 4.7153373271382417e-06
Iter: 490 loss: 4.7137626955338166e-06
Iter: 491 loss: 4.7392933102475947e-06
Iter: 492 loss: 4.7137149112830935e-06
Iter: 493 loss: 4.7125919315347438e-06
Iter: 494 loss: 4.71440567523627e-06
Iter: 495 loss: 4.7120746309380465e-06
Iter: 496 loss: 4.710811747879317e-06
Iter: 497 loss: 4.7138042866288222e-06
Iter: 498 loss: 4.7103513035230825e-06
Iter: 499 loss: 4.7093511447690763e-06
Iter: 500 loss: 4.7232219021883954e-06
Iter: 501 loss: 4.7093479651884218e-06
Iter: 502 loss: 4.7081995387762014e-06
Iter: 503 loss: 4.7107431381626795e-06
Iter: 504 loss: 4.7077605883559747e-06
Iter: 505 loss: 4.7070079274730311e-06
Iter: 506 loss: 4.7062897902789929e-06
Iter: 507 loss: 4.70612005684307e-06
Iter: 508 loss: 4.7050808781762044e-06
Iter: 509 loss: 4.7055930267372594e-06
Iter: 510 loss: 4.7043856059223329e-06
Iter: 511 loss: 4.704016682159052e-06
Iter: 512 loss: 4.7037036454149165e-06
Iter: 513 loss: 4.7029886001944219e-06
Iter: 514 loss: 4.70215903954356e-06
Iter: 515 loss: 4.7020617896887492e-06
Iter: 516 loss: 4.7011829489658449e-06
Iter: 517 loss: 4.70188244346241e-06
Iter: 518 loss: 4.7006534350672573e-06
Iter: 519 loss: 4.6998683774128409e-06
Iter: 520 loss: 4.69986614752254e-06
Iter: 521 loss: 4.6992039682143178e-06
Iter: 522 loss: 4.700172133987404e-06
Iter: 523 loss: 4.6988836917069424e-06
Iter: 524 loss: 4.6982505914645844e-06
Iter: 525 loss: 4.6975441234184762e-06
Iter: 526 loss: 4.69744674905787e-06
Iter: 527 loss: 4.6968144265565448e-06
Iter: 528 loss: 4.6967834846178151e-06
Iter: 529 loss: 4.6960885417812241e-06
Iter: 530 loss: 4.695261771821506e-06
Iter: 531 loss: 4.6951751821002272e-06
Iter: 532 loss: 4.6944156720296788e-06
Iter: 533 loss: 4.6942422270077553e-06
Iter: 534 loss: 4.6937527501528536e-06
Iter: 535 loss: 4.6924721525633211e-06
Iter: 536 loss: 4.6932657938764762e-06
Iter: 537 loss: 4.691652473756432e-06
Iter: 538 loss: 4.6903346986851108e-06
Iter: 539 loss: 4.6978863474940676e-06
Iter: 540 loss: 4.6901561534545832e-06
Iter: 541 loss: 4.6899082478276585e-06
Iter: 542 loss: 4.6896079237191928e-06
Iter: 543 loss: 4.6891328536979816e-06
Iter: 544 loss: 4.6881859653598434e-06
Iter: 545 loss: 4.7063338344042277e-06
Iter: 546 loss: 4.6881748479260647e-06
Iter: 547 loss: 4.6872734288679036e-06
Iter: 548 loss: 4.6874748795878108e-06
Iter: 549 loss: 4.686609398964417e-06
Iter: 550 loss: 4.6858699174547249e-06
Iter: 551 loss: 4.6858323093901767e-06
Iter: 552 loss: 4.6850430710778763e-06
Iter: 553 loss: 4.6863882999154408e-06
Iter: 554 loss: 4.6846896189709941e-06
Iter: 555 loss: 4.6842141174144357e-06
Iter: 556 loss: 4.6834346515068369e-06
Iter: 557 loss: 4.6834307392669718e-06
Iter: 558 loss: 4.6824327693332143e-06
Iter: 559 loss: 4.6924758476536931e-06
Iter: 560 loss: 4.6824014933886065e-06
Iter: 561 loss: 4.6815568528380772e-06
Iter: 562 loss: 4.6857932696893715e-06
Iter: 563 loss: 4.6814155491017813e-06
Iter: 564 loss: 4.6808111549731034e-06
Iter: 565 loss: 4.6799671564096954e-06
Iter: 566 loss: 4.6799329773869826e-06
Iter: 567 loss: 4.6789490914813448e-06
Iter: 568 loss: 4.6802646245051911e-06
Iter: 569 loss: 4.6784536608144571e-06
Iter: 570 loss: 4.678160559171144e-06
Iter: 571 loss: 4.6778673708042423e-06
Iter: 572 loss: 4.6774189984005856e-06
Iter: 573 loss: 4.6763387252090479e-06
Iter: 574 loss: 4.68805224128873e-06
Iter: 575 loss: 4.6762248813474361e-06
Iter: 576 loss: 4.6751341144092654e-06
Iter: 577 loss: 4.67632589204584e-06
Iter: 578 loss: 4.6745391243318234e-06
Iter: 579 loss: 4.67351757833745e-06
Iter: 580 loss: 4.6786443252504043e-06
Iter: 581 loss: 4.6733468287482027e-06
Iter: 582 loss: 4.6724490562407551e-06
Iter: 583 loss: 4.677214564385742e-06
Iter: 584 loss: 4.6723112362299252e-06
Iter: 585 loss: 4.6711635040118348e-06
Iter: 586 loss: 4.6762329884323375e-06
Iter: 587 loss: 4.6709348314755534e-06
Iter: 588 loss: 4.670365843902872e-06
Iter: 589 loss: 4.6693910667960019e-06
Iter: 590 loss: 4.6693900145304748e-06
Iter: 591 loss: 4.6693296120896845e-06
Iter: 592 loss: 4.6688934370906521e-06
Iter: 593 loss: 4.6684977363176769e-06
Iter: 594 loss: 4.66753589146211e-06
Iter: 595 loss: 4.6776272259531728e-06
Iter: 596 loss: 4.6674270232799307e-06
Iter: 597 loss: 4.66651212811725e-06
Iter: 598 loss: 4.6704383810489005e-06
Iter: 599 loss: 4.6663229893983553e-06
Iter: 600 loss: 4.6653889889339916e-06
Iter: 601 loss: 4.6661784961556671e-06
Iter: 602 loss: 4.6648356375417557e-06
Iter: 603 loss: 4.6639354955052408e-06
Iter: 604 loss: 4.6703377539490039e-06
Iter: 605 loss: 4.6638555239111178e-06
Iter: 606 loss: 4.66321204870331e-06
Iter: 607 loss: 4.6632117173776366e-06
Iter: 608 loss: 4.6628009457362182e-06
Iter: 609 loss: 4.6616995885984165e-06
Iter: 610 loss: 4.6691934906712541e-06
Iter: 611 loss: 4.6614421897955609e-06
Iter: 612 loss: 4.6602357585432431e-06
Iter: 613 loss: 4.6687558511669368e-06
Iter: 614 loss: 4.6601268431855211e-06
Iter: 615 loss: 4.6593673064821053e-06
Iter: 616 loss: 4.6593491037920643e-06
Iter: 617 loss: 4.6589219128421636e-06
Iter: 618 loss: 4.6578550650157589e-06
Iter: 619 loss: 4.6679276675839906e-06
Iter: 620 loss: 4.6577060270011243e-06
Iter: 621 loss: 4.6565976424294679e-06
Iter: 622 loss: 4.6590132392499891e-06
Iter: 623 loss: 4.6561694214352542e-06
Iter: 624 loss: 4.6549725417119106e-06
Iter: 625 loss: 4.6591572411655437e-06
Iter: 626 loss: 4.654660466793891e-06
Iter: 627 loss: 4.6542379185432505e-06
Iter: 628 loss: 4.6539835923785361e-06
Iter: 629 loss: 4.6535797364054484e-06
Iter: 630 loss: 4.6526637617640759e-06
Iter: 631 loss: 4.6648682592603205e-06
Iter: 632 loss: 4.6526069980008414e-06
Iter: 633 loss: 4.6518316668969507e-06
Iter: 634 loss: 4.6571699815277086e-06
Iter: 635 loss: 4.6517576944705625e-06
Iter: 636 loss: 4.65085323764796e-06
Iter: 637 loss: 4.6542961478098814e-06
Iter: 638 loss: 4.6506378951592963e-06
Iter: 639 loss: 4.6500430799814762e-06
Iter: 640 loss: 4.6494782538535711e-06
Iter: 641 loss: 4.649342856664694e-06
Iter: 642 loss: 4.6483728034283174e-06
Iter: 643 loss: 4.6508261668053791e-06
Iter: 644 loss: 4.6480357102585741e-06
Iter: 645 loss: 4.647034130166534e-06
Iter: 646 loss: 4.6484611265101064e-06
Iter: 647 loss: 4.6465438993982719e-06
Iter: 648 loss: 4.6461734837522857e-06
Iter: 649 loss: 4.6459397506600092e-06
Iter: 650 loss: 4.6455219302007641e-06
Iter: 651 loss: 4.6443629123689448e-06
Iter: 652 loss: 4.6508620456932432e-06
Iter: 653 loss: 4.6440204898139081e-06
Iter: 654 loss: 4.6429358573281821e-06
Iter: 655 loss: 4.6493877371795716e-06
Iter: 656 loss: 4.6427981626168438e-06
Iter: 657 loss: 4.6418962545272625e-06
Iter: 658 loss: 4.6418361764040662e-06
Iter: 659 loss: 4.6411563777413192e-06
Iter: 660 loss: 4.64074290391329e-06
Iter: 661 loss: 4.640479338314644e-06
Iter: 662 loss: 4.6397903329681605e-06
Iter: 663 loss: 4.639313053263354e-06
Iter: 664 loss: 4.6390646996670276e-06
Iter: 665 loss: 4.638324335292843e-06
Iter: 666 loss: 4.6397073990911332e-06
Iter: 667 loss: 4.6380093680835628e-06
Iter: 668 loss: 4.6372016436446026e-06
Iter: 669 loss: 4.6482830522516628e-06
Iter: 670 loss: 4.6371986823438585e-06
Iter: 671 loss: 4.6368554621102253e-06
Iter: 672 loss: 4.6358627433685059e-06
Iter: 673 loss: 4.6400253939535377e-06
Iter: 674 loss: 4.635473051895377e-06
Iter: 675 loss: 4.6342776228929245e-06
Iter: 676 loss: 4.6488423118140149e-06
Iter: 677 loss: 4.6342642066675066e-06
Iter: 678 loss: 4.6333537032886323e-06
Iter: 679 loss: 4.6440545113700068e-06
Iter: 680 loss: 4.6333406753802971e-06
Iter: 681 loss: 4.6328186795870858e-06
Iter: 682 loss: 4.6322232884233624e-06
Iter: 683 loss: 4.6321482286024159e-06
Iter: 684 loss: 4.6312534621924e-06
Iter: 685 loss: 4.6320373769843928e-06
Iter: 686 loss: 4.6307288864214509e-06
Iter: 687 loss: 4.6297593085702525e-06
Iter: 688 loss: 4.6337135643900196e-06
Iter: 689 loss: 4.6295460911430985e-06
Iter: 690 loss: 4.6286910473176988e-06
Iter: 691 loss: 4.6421542075566046e-06
Iter: 692 loss: 4.6286910116746473e-06
Iter: 693 loss: 4.62817647729712e-06
Iter: 694 loss: 4.6270745925216142e-06
Iter: 695 loss: 4.6445130904932663e-06
Iter: 696 loss: 4.6270374999011233e-06
Iter: 697 loss: 4.62601879372679e-06
Iter: 698 loss: 4.6270482780576617e-06
Iter: 699 loss: 4.6254477688999906e-06
Iter: 700 loss: 4.6240736560898216e-06
Iter: 701 loss: 4.6251759999943591e-06
Iter: 702 loss: 4.6232474659386012e-06
Iter: 703 loss: 4.6235619932252614e-06
Iter: 704 loss: 4.6226579432738706e-06
Iter: 705 loss: 4.6220495965419714e-06
Iter: 706 loss: 4.6214059810283027e-06
Iter: 707 loss: 4.6212975150743361e-06
Iter: 708 loss: 4.6205957052401976e-06
Iter: 709 loss: 4.6203005162378081e-06
Iter: 710 loss: 4.6199348288710744e-06
Iter: 711 loss: 4.619377220624609e-06
Iter: 712 loss: 4.6193146508014189e-06
Iter: 713 loss: 4.6186507691995694e-06
Iter: 714 loss: 4.6175623415816331e-06
Iter: 715 loss: 4.617556899043375e-06
Iter: 716 loss: 4.6165813879427446e-06
Iter: 717 loss: 4.6197867808858543e-06
Iter: 718 loss: 4.6163107443771628e-06
Iter: 719 loss: 4.6157451116188461e-06
Iter: 720 loss: 4.6157219157930995e-06
Iter: 721 loss: 4.6152226817399577e-06
Iter: 722 loss: 4.6144413844009579e-06
Iter: 723 loss: 4.6144319933951889e-06
Iter: 724 loss: 4.6133519818399254e-06
Iter: 725 loss: 4.6139477974738019e-06
Iter: 726 loss: 4.6126439877623391e-06
Iter: 727 loss: 4.611537272949526e-06
Iter: 728 loss: 4.6160705682556859e-06
Iter: 729 loss: 4.6112952159879948e-06
Iter: 730 loss: 4.6107257461152945e-06
Iter: 731 loss: 4.61063225042513e-06
Iter: 732 loss: 4.6103031046297215e-06
Iter: 733 loss: 4.6093693158077849e-06
Iter: 734 loss: 4.6138875864621442e-06
Iter: 735 loss: 4.6090478159935016e-06
Iter: 736 loss: 4.607704728792502e-06
Iter: 737 loss: 4.6097847009889265e-06
Iter: 738 loss: 4.6070728416616763e-06
Iter: 739 loss: 4.6056210298588316e-06
Iter: 740 loss: 4.6119342434139828e-06
Iter: 741 loss: 4.6053259334832151e-06
Iter: 742 loss: 4.6054652152230754e-06
Iter: 743 loss: 4.6047923340210588e-06
Iter: 744 loss: 4.6044346417671545e-06
Iter: 745 loss: 4.6034764393118951e-06
Iter: 746 loss: 4.6100287881486481e-06
Iter: 747 loss: 4.6032538951062067e-06
Iter: 748 loss: 4.602000252583896e-06
Iter: 749 loss: 4.6018702158607278e-06
Iter: 750 loss: 4.6009573445590637e-06
Iter: 751 loss: 4.6003825552408076e-06
Iter: 752 loss: 4.6001616447541039e-06
Iter: 753 loss: 4.5994627423471814e-06
Iter: 754 loss: 4.602450951256546e-06
Iter: 755 loss: 4.5993176150832636e-06
Iter: 756 loss: 4.5987749887219256e-06
Iter: 757 loss: 4.5978765855062764e-06
Iter: 758 loss: 4.5978730986085348e-06
Iter: 759 loss: 4.5967204909396195e-06
Iter: 760 loss: 4.6019343603559072e-06
Iter: 761 loss: 4.5964976751120812e-06
Iter: 762 loss: 4.5957783652688951e-06
Iter: 763 loss: 4.5957725283322033e-06
Iter: 764 loss: 4.5951431357207358e-06
Iter: 765 loss: 4.5938482641331609e-06
Iter: 766 loss: 4.6167092034942876e-06
Iter: 767 loss: 4.5938226140437408e-06
Iter: 768 loss: 4.5927877241025615e-06
Iter: 769 loss: 4.5973385648388774e-06
Iter: 770 loss: 4.5925803382029232e-06
Iter: 771 loss: 4.5920277416625957e-06
Iter: 772 loss: 4.5920067885033966e-06
Iter: 773 loss: 4.5913860586453491e-06
Iter: 774 loss: 4.5899898401947952e-06
Iter: 775 loss: 4.6090904263536992e-06
Iter: 776 loss: 4.5899100201594034e-06
Iter: 777 loss: 4.5887952488585616e-06
Iter: 778 loss: 4.5901498584091861e-06
Iter: 779 loss: 4.5882112087924378e-06
Iter: 780 loss: 4.5873273650706292e-06
Iter: 781 loss: 4.6006521541167158e-06
Iter: 782 loss: 4.5873268709699226e-06
Iter: 783 loss: 4.5863070833568982e-06
Iter: 784 loss: 4.5889559653321062e-06
Iter: 785 loss: 4.58596001289978e-06
Iter: 786 loss: 4.5853986916344562e-06
Iter: 787 loss: 4.5840513801407245e-06
Iter: 788 loss: 4.5988578579957872e-06
Iter: 789 loss: 4.5839136769537416e-06
Iter: 790 loss: 4.5824806468095247e-06
Iter: 791 loss: 4.5938656127177041e-06
Iter: 792 loss: 4.582383191694407e-06
Iter: 793 loss: 4.5819438553604452e-06
Iter: 794 loss: 4.5818050269065871e-06
Iter: 795 loss: 4.5812817658667632e-06
Iter: 796 loss: 4.5803161022260411e-06
Iter: 797 loss: 4.6025294199515692e-06
Iter: 798 loss: 4.5803149714846547e-06
Iter: 799 loss: 4.5791661706204694e-06
Iter: 800 loss: 4.5819565112672573e-06
Iter: 801 loss: 4.5787544949714633e-06
Iter: 802 loss: 4.5778499351933116e-06
Iter: 803 loss: 4.5839531109480789e-06
Iter: 804 loss: 4.577759882773881e-06
Iter: 805 loss: 4.5768289379038682e-06
Iter: 806 loss: 4.581554695140427e-06
Iter: 807 loss: 4.5766759873162668e-06
Iter: 808 loss: 4.5760323517525473e-06
Iter: 809 loss: 4.5748740475935918e-06
Iter: 810 loss: 4.6030862654850709e-06
Iter: 811 loss: 4.5748739072608052e-06
Iter: 812 loss: 4.57376166904913e-06
Iter: 813 loss: 4.5788432819490046e-06
Iter: 814 loss: 4.5735496220469105e-06
Iter: 815 loss: 4.572584822065298e-06
Iter: 816 loss: 4.5725842470981357e-06
Iter: 817 loss: 4.57207671877181e-06
Iter: 818 loss: 4.5708870145319624e-06
Iter: 819 loss: 4.58510876964894e-06
Iter: 820 loss: 4.5707874821073382e-06
Iter: 821 loss: 4.5695954275101229e-06
Iter: 822 loss: 4.5706557845062045e-06
Iter: 823 loss: 4.5688996093789458e-06
Iter: 824 loss: 4.5678857149440423e-06
Iter: 825 loss: 4.5678807047264123e-06
Iter: 826 loss: 4.5668370019574563e-06
Iter: 827 loss: 4.5718358891612167e-06
Iter: 828 loss: 4.56665050172978e-06
Iter: 829 loss: 4.56605051285685e-06
Iter: 830 loss: 4.5649499503985513e-06
Iter: 831 loss: 4.5906482710198528e-06
Iter: 832 loss: 4.5649490573269459e-06
Iter: 833 loss: 4.5637896590826366e-06
Iter: 834 loss: 4.5676097344718482e-06
Iter: 835 loss: 4.5634691262167426e-06
Iter: 836 loss: 4.562423546980307e-06
Iter: 837 loss: 4.56241972285929e-06
Iter: 838 loss: 4.5618665881394138e-06
Iter: 839 loss: 4.5608849093166591e-06
Iter: 840 loss: 4.5608849066806e-06
Iter: 841 loss: 4.5596305217633965e-06
Iter: 842 loss: 4.5631023188498448e-06
Iter: 843 loss: 4.5592251647269124e-06
Iter: 844 loss: 4.5585048210608831e-06
Iter: 845 loss: 4.5584914157010472e-06
Iter: 846 loss: 4.557807253374041e-06
Iter: 847 loss: 4.5578845886415728e-06
Iter: 848 loss: 4.5572824180524389e-06
Iter: 849 loss: 4.5565984432773966e-06
Iter: 850 loss: 4.55534329599977e-06
Iter: 851 loss: 4.5845998862432568e-06
Iter: 852 loss: 4.5553422444998068e-06
Iter: 853 loss: 4.5546609647581547e-06
Iter: 854 loss: 4.5545107365648873e-06
Iter: 855 loss: 4.5535946383726379e-06
Iter: 856 loss: 4.5534435860724e-06
Iter: 857 loss: 4.5528143379868134e-06
Iter: 858 loss: 4.5518915145280225e-06
Iter: 859 loss: 4.5509351032465093e-06
Iter: 860 loss: 4.5507618080945145e-06
Iter: 861 loss: 4.5493171227806669e-06
Iter: 862 loss: 4.5528435879233562e-06
Iter: 863 loss: 4.5488017335902776e-06
Iter: 864 loss: 4.5484565607585558e-06
Iter: 865 loss: 4.548061499410295e-06
Iter: 866 loss: 4.5473632210492789e-06
Iter: 867 loss: 4.546134298307634e-06
Iter: 868 loss: 4.5461341977949885e-06
Iter: 869 loss: 4.5452263133695208e-06
Iter: 870 loss: 4.5440812332522052e-06
Iter: 871 loss: 4.5439917348482509e-06
Iter: 872 loss: 4.5425231383076661e-06
Iter: 873 loss: 4.5613864071099148e-06
Iter: 874 loss: 4.5425122520913035e-06
Iter: 875 loss: 4.5415731176899732e-06
Iter: 876 loss: 4.5415722194966044e-06
Iter: 877 loss: 4.5409471336589923e-06
Iter: 878 loss: 4.5398894275277763e-06
Iter: 879 loss: 4.5398874248663187e-06
Iter: 880 loss: 4.5388539998696847e-06
Iter: 881 loss: 4.5449987057353554e-06
Iter: 882 loss: 4.5387225764537606e-06
Iter: 883 loss: 4.5380428122527016e-06
Iter: 884 loss: 4.5468361953226945e-06
Iter: 885 loss: 4.53803809512836e-06
Iter: 886 loss: 4.5373593711195012e-06
Iter: 887 loss: 4.5361903778511284e-06
Iter: 888 loss: 4.5361894576672333e-06
Iter: 889 loss: 4.5349184693392523e-06
Iter: 890 loss: 4.535597031677895e-06
Iter: 891 loss: 4.534080020472964e-06
Iter: 892 loss: 4.5341929503427076e-06
Iter: 893 loss: 4.533550326143875e-06
Iter: 894 loss: 4.533071670837762e-06
Iter: 895 loss: 4.5318606297530131e-06
Iter: 896 loss: 4.5426926291644921e-06
Iter: 897 loss: 4.5316740358455753e-06
Iter: 898 loss: 4.530339083387776e-06
Iter: 899 loss: 4.5309218628878659e-06
Iter: 900 loss: 4.5294280177274875e-06
Iter: 901 loss: 4.5282109419799255e-06
Iter: 902 loss: 4.5282100233171128e-06
Iter: 903 loss: 4.5270335910672049e-06
Iter: 904 loss: 4.5342359203721028e-06
Iter: 905 loss: 4.5268918188364615e-06
Iter: 906 loss: 4.5264270817544682e-06
Iter: 907 loss: 4.525318908065164e-06
Iter: 908 loss: 4.5377938069329362e-06
Iter: 909 loss: 4.5252116656747758e-06
Iter: 910 loss: 4.5237608961413794e-06
Iter: 911 loss: 4.5273679603879664e-06
Iter: 912 loss: 4.52325024237843e-06
Iter: 913 loss: 4.5225954091009514e-06
Iter: 914 loss: 4.5223396240255368e-06
Iter: 915 loss: 4.5217680575088586e-06
Iter: 916 loss: 4.5207376037757906e-06
Iter: 917 loss: 4.5457049782228777e-06
Iter: 918 loss: 4.52073743633944e-06
Iter: 919 loss: 4.5196985725536008e-06
Iter: 920 loss: 4.5219968471936234e-06
Iter: 921 loss: 4.5193009285596745e-06
Iter: 922 loss: 4.5183018488190312e-06
Iter: 923 loss: 4.5332056013439195e-06
Iter: 924 loss: 4.5183010710369423e-06
Iter: 925 loss: 4.5175101276525418e-06
Iter: 926 loss: 4.5177220144038352e-06
Iter: 927 loss: 4.5169369361930321e-06
Iter: 928 loss: 4.5160729896722892e-06
Iter: 929 loss: 4.5149850860995953e-06
Iter: 930 loss: 4.51489916634079e-06
Iter: 931 loss: 4.5149459817723251e-06
Iter: 932 loss: 4.5143489979275518e-06
Iter: 933 loss: 4.5138264362960114e-06
Iter: 934 loss: 4.5125956300628162e-06
Iter: 935 loss: 4.52706423542982e-06
Iter: 936 loss: 4.5124883010275983e-06
Iter: 937 loss: 4.5111033230404782e-06
Iter: 938 loss: 4.51235452344445e-06
Iter: 939 loss: 4.5102986205518406e-06
Iter: 940 loss: 4.5102133309920326e-06
Iter: 941 loss: 4.5096372934026833e-06
Iter: 942 loss: 4.5090363834521376e-06
Iter: 943 loss: 4.5082284704754485e-06
Iter: 944 loss: 4.5081854562280417e-06
Iter: 945 loss: 4.5073690159191648e-06
Iter: 946 loss: 4.5065840527801982e-06
Iter: 947 loss: 4.5064027401418314e-06
Iter: 948 loss: 4.5058836195085767e-06
Iter: 949 loss: 4.5056018082946873e-06
Iter: 950 loss: 4.5049508284553607e-06
Iter: 951 loss: 4.5035459202058975e-06
Iter: 952 loss: 4.5253055191724905e-06
Iter: 953 loss: 4.5034941638953556e-06
Iter: 954 loss: 4.5022510890483944e-06
Iter: 955 loss: 4.5073790901548338e-06
Iter: 956 loss: 4.5019815225513633e-06
Iter: 957 loss: 4.5010291448748821e-06
Iter: 958 loss: 4.5010257074005805e-06
Iter: 959 loss: 4.5004948261845964e-06
Iter: 960 loss: 4.4999239706515227e-06
Iter: 961 loss: 4.4998332018568163e-06
Iter: 962 loss: 4.4988406632218767e-06
Iter: 963 loss: 4.49934203429379e-06
Iter: 964 loss: 4.49817973909894e-06
Iter: 965 loss: 4.4976243917320194e-06
Iter: 966 loss: 4.4975208395918164e-06
Iter: 967 loss: 4.4969298442751429e-06
Iter: 968 loss: 4.4959715208395543e-06
Iter: 969 loss: 4.4959653667010406e-06
Iter: 970 loss: 4.49493564971667e-06
Iter: 971 loss: 4.4945553938200113e-06
Iter: 972 loss: 4.4939857610746787e-06
Iter: 973 loss: 4.4931562886203113e-06
Iter: 974 loss: 4.4930885636305393e-06
Iter: 975 loss: 4.4920777592234126e-06
Iter: 976 loss: 4.4921001869470161e-06
Iter: 977 loss: 4.491275841797288e-06
Iter: 978 loss: 4.4904230273847037e-06
Iter: 979 loss: 4.4891915179395219e-06
Iter: 980 loss: 4.4891538831934723e-06
Iter: 981 loss: 4.4879512858137643e-06
Iter: 982 loss: 4.4991421158366164e-06
Iter: 983 loss: 4.4879006437385168e-06
Iter: 984 loss: 4.4869344355801547e-06
Iter: 985 loss: 4.500637422024399e-06
Iter: 986 loss: 4.4869322456500132e-06
Iter: 987 loss: 4.4864111833891578e-06
Iter: 988 loss: 4.4850483690615055e-06
Iter: 989 loss: 4.4955483799909716e-06
Iter: 990 loss: 4.4847809840415142e-06
Iter: 991 loss: 4.4843040833701738e-06
Iter: 992 loss: 4.4840613083335876e-06
Iter: 993 loss: 4.4833469035966461e-06
Iter: 994 loss: 4.4840524221122289e-06
Iter: 995 loss: 4.4829434276806358e-06
Iter: 996 loss: 4.4822600368902584e-06
Iter: 997 loss: 4.4810452997883108e-06
Iter: 998 loss: 4.4810452996474661e-06
Iter: 999 loss: 4.4804067668413448e-06
Iter: 1000 loss: 4.4802433628892731e-06
Iter: 1001 loss: 4.4794964206327338e-06
Iter: 1002 loss: 4.4795733066580758e-06
Iter: 1003 loss: 4.4789214521650451e-06
Iter: 1004 loss: 4.4780065723151173e-06
Iter: 1005 loss: 4.4780092029354093e-06
Iter: 1006 loss: 4.4772751359082654e-06
Iter: 1007 loss: 4.4762923486613629e-06
Iter: 1008 loss: 4.4814426030190934e-06
Iter: 1009 loss: 4.47613849899755e-06
Iter: 1010 loss: 4.4753616632434719e-06
Iter: 1011 loss: 4.4875738526905813e-06
Iter: 1012 loss: 4.4753616253115417e-06
Iter: 1013 loss: 4.4748986469384952e-06
Iter: 1014 loss: 4.4735224442711862e-06
Iter: 1015 loss: 4.4780617557358749e-06
Iter: 1016 loss: 4.47287138924516e-06
Iter: 1017 loss: 4.4711399119211863e-06
Iter: 1018 loss: 4.4817737919424795e-06
Iter: 1019 loss: 4.4709324990971539e-06
Iter: 1020 loss: 4.4700402410788894e-06
Iter: 1021 loss: 4.4700277681936075e-06
Iter: 1022 loss: 4.469046033000146e-06
Iter: 1023 loss: 4.4697855534580851e-06
Iter: 1024 loss: 4.4684458898699835e-06
Iter: 1025 loss: 4.46773603301341e-06
Iter: 1026 loss: 4.4671602291187708e-06
Iter: 1027 loss: 4.466949627031154e-06
Iter: 1028 loss: 4.4663613787735389e-06
Iter: 1029 loss: 4.4662672298890766e-06
Iter: 1030 loss: 4.4655978319962763e-06
Iter: 1031 loss: 4.4643669148722779e-06
Iter: 1032 loss: 4.4929203936480933e-06
Iter: 1033 loss: 4.4643657426995345e-06
Iter: 1034 loss: 4.4634140135348312e-06
Iter: 1035 loss: 4.46560576532632e-06
Iter: 1036 loss: 4.4630598182459769e-06
Iter: 1037 loss: 4.461931169962821e-06
Iter: 1038 loss: 4.47513003694274e-06
Iter: 1039 loss: 4.4619145275611046e-06
Iter: 1040 loss: 4.4612687347546643e-06
Iter: 1041 loss: 4.459821547117134e-06
Iter: 1042 loss: 4.4798270245582714e-06
Iter: 1043 loss: 4.4597417341573692e-06
Iter: 1044 loss: 4.4584517219741508e-06
Iter: 1045 loss: 4.4697854441894153e-06
Iter: 1046 loss: 4.4583858562755829e-06
Iter: 1047 loss: 4.4577795805596956e-06
Iter: 1048 loss: 4.4577423541754949e-06
Iter: 1049 loss: 4.4572397657327517e-06
Iter: 1050 loss: 4.4564177293801633e-06
Iter: 1051 loss: 4.4564133958478959e-06
Iter: 1052 loss: 4.4553824335031637e-06
Iter: 1053 loss: 4.4557599319567993e-06
Iter: 1054 loss: 4.4546606286911151e-06
Iter: 1055 loss: 4.4533262183830362e-06
Iter: 1056 loss: 4.4551122483036032e-06
Iter: 1057 loss: 4.4526547488562515e-06
Iter: 1058 loss: 4.4521773094320606e-06
Iter: 1059 loss: 4.4518134625693436e-06
Iter: 1060 loss: 4.4513201409710384e-06
Iter: 1061 loss: 4.4500644219658636e-06
Iter: 1062 loss: 4.4610141487634285e-06
Iter: 1063 loss: 4.44986212162631e-06
Iter: 1064 loss: 4.4486867560032993e-06
Iter: 1065 loss: 4.4582006750651038e-06
Iter: 1066 loss: 4.4486109105875453e-06
Iter: 1067 loss: 4.44728056665674e-06
Iter: 1068 loss: 4.4533740205487611e-06
Iter: 1069 loss: 4.4470277725695935e-06
Iter: 1070 loss: 4.4464469280127549e-06
Iter: 1071 loss: 4.4454521429154684e-06
Iter: 1072 loss: 4.4454510533204947e-06
Iter: 1073 loss: 4.4451211167871146e-06
Iter: 1074 loss: 4.4448735628842694e-06
Iter: 1075 loss: 4.4443253355397904e-06
Iter: 1076 loss: 4.4431952699014359e-06
Iter: 1077 loss: 4.4630518606113146e-06
Iter: 1078 loss: 4.4431722509178356e-06
Iter: 1079 loss: 4.4420400595602807e-06
Iter: 1080 loss: 4.4433027740542181e-06
Iter: 1081 loss: 4.4414272386462051e-06
Iter: 1082 loss: 4.4408218390375888e-06
Iter: 1083 loss: 4.4405934881801025e-06
Iter: 1084 loss: 4.4400068571248684e-06
Iter: 1085 loss: 4.4388077864599565e-06
Iter: 1086 loss: 4.4604015368524228e-06
Iter: 1087 loss: 4.4387863316738505e-06
Iter: 1088 loss: 4.4377867110524532e-06
Iter: 1089 loss: 4.4421223768226012e-06
Iter: 1090 loss: 4.4375832271823957e-06
Iter: 1091 loss: 4.43655883903488e-06
Iter: 1092 loss: 4.43677955907727e-06
Iter: 1093 loss: 4.435801686805444e-06
Iter: 1094 loss: 4.4349883672761867e-06
Iter: 1095 loss: 4.434885978487876e-06
Iter: 1096 loss: 4.4342028725234139e-06
Iter: 1097 loss: 4.4329384819991284e-06
Iter: 1098 loss: 4.4618187874032096e-06
Iter: 1099 loss: 4.4329367507350807e-06
Iter: 1100 loss: 4.4320077685334363e-06
Iter: 1101 loss: 4.437094717864589e-06
Iter: 1102 loss: 4.4318719658766576e-06
Iter: 1103 loss: 4.430961920136282e-06
Iter: 1104 loss: 4.4402566968283136e-06
Iter: 1105 loss: 4.430935139800025e-06
Iter: 1106 loss: 4.43043868597003e-06
Iter: 1107 loss: 4.4293088296641694e-06
Iter: 1108 loss: 4.4442053467467344e-06
Iter: 1109 loss: 4.4292365141986148e-06
Iter: 1110 loss: 4.4283277004475889e-06
Iter: 1111 loss: 4.42832732238419e-06
Iter: 1112 loss: 4.4273034830768945e-06
Iter: 1113 loss: 4.428501560691559e-06
Iter: 1114 loss: 4.426758985373303e-06
Iter: 1115 loss: 4.4260246229436746e-06
Iter: 1116 loss: 4.4250070334963321e-06
Iter: 1117 loss: 4.4249633319358605e-06
Iter: 1118 loss: 4.4235966305798289e-06
Iter: 1119 loss: 4.4258252687177306e-06
Iter: 1120 loss: 4.4229701219998252e-06
Iter: 1121 loss: 4.422762717506333e-06
Iter: 1122 loss: 4.4222699608359337e-06
Iter: 1123 loss: 4.4216106168421808e-06
Iter: 1124 loss: 4.4207194842349346e-06
Iter: 1125 loss: 4.4206736975531172e-06
Iter: 1126 loss: 4.419837573387157e-06
Iter: 1127 loss: 4.419507331643182e-06
Iter: 1128 loss: 4.4190582695527482e-06
Iter: 1129 loss: 4.4180171070281344e-06
Iter: 1130 loss: 4.42685112229229e-06
Iter: 1131 loss: 4.4179582807113139e-06
Iter: 1132 loss: 4.4168743547036768e-06
Iter: 1133 loss: 4.4234866406522695e-06
Iter: 1134 loss: 4.4167429507487769e-06
Iter: 1135 loss: 4.4160585906394471e-06
Iter: 1136 loss: 4.4151545010900012e-06
Iter: 1137 loss: 4.4151004539655169e-06
Iter: 1138 loss: 4.4141051451254593e-06
Iter: 1139 loss: 4.4243341753421447e-06
Iter: 1140 loss: 4.414076646629071e-06
Iter: 1141 loss: 4.41332582696131e-06
Iter: 1142 loss: 4.419092986117516e-06
Iter: 1143 loss: 4.4132702869577729e-06
Iter: 1144 loss: 4.4126796474832641e-06
Iter: 1145 loss: 4.4113396672025667e-06
Iter: 1146 loss: 4.4292314618590756e-06
Iter: 1147 loss: 4.4112565973258379e-06
Iter: 1148 loss: 4.4099925144370562e-06
Iter: 1149 loss: 4.418870414417604e-06
Iter: 1150 loss: 4.4098771004820637e-06
Iter: 1151 loss: 4.4087700632673062e-06
Iter: 1152 loss: 4.4220571030289138e-06
Iter: 1153 loss: 4.4087562569701218e-06
Iter: 1154 loss: 4.4082703510111592e-06
Iter: 1155 loss: 4.4073403930209061e-06
Iter: 1156 loss: 4.4270288049008814e-06
Iter: 1157 loss: 4.4073360311521992e-06
Iter: 1158 loss: 4.4063338899406217e-06
Iter: 1159 loss: 4.4071992431838966e-06
Iter: 1160 loss: 4.4057438263393142e-06
Iter: 1161 loss: 4.40438487471597e-06
Iter: 1162 loss: 4.4074418664876178e-06
Iter: 1163 loss: 4.4038708013479592e-06
Iter: 1164 loss: 4.4036958358224013e-06
Iter: 1165 loss: 4.403137714035298e-06
Iter: 1166 loss: 4.4027464701776436e-06
Iter: 1167 loss: 4.4017284872410565e-06
Iter: 1168 loss: 4.4097612711523155e-06
Iter: 1169 loss: 4.40153598426072e-06
Iter: 1170 loss: 4.4002788280334162e-06
Iter: 1171 loss: 4.4010460902062567e-06
Iter: 1172 loss: 4.3994713879220992e-06
Iter: 1173 loss: 4.3980279469409719e-06
Iter: 1174 loss: 4.4051585492552128e-06
Iter: 1175 loss: 4.397781213867886e-06
Iter: 1176 loss: 4.3968800280083866e-06
Iter: 1177 loss: 4.3968788523977626e-06
Iter: 1178 loss: 4.3959668927923592e-06
Iter: 1179 loss: 4.3978341704176488e-06
Iter: 1180 loss: 4.3955997896530682e-06
Iter: 1181 loss: 4.3949560254873289e-06
Iter: 1182 loss: 4.39495611742807e-06
Iter: 1183 loss: 4.3944411388785262e-06
Iter: 1184 loss: 4.3935460476404418e-06
Iter: 1185 loss: 4.3964474495123885e-06
Iter: 1186 loss: 4.3932943857723613e-06
Iter: 1187 loss: 4.3923344090020846e-06
Iter: 1188 loss: 4.4005980514185695e-06
Iter: 1189 loss: 4.3922823809211927e-06
Iter: 1190 loss: 4.391713797707335e-06
Iter: 1191 loss: 4.3906278587486484e-06
Iter: 1192 loss: 4.4137033139956163e-06
Iter: 1193 loss: 4.3906230416154956e-06
Iter: 1194 loss: 4.3898587543657067e-06
Iter: 1195 loss: 4.3898580265770362e-06
Iter: 1196 loss: 4.3889810776635678e-06
Iter: 1197 loss: 4.3894282086009043e-06
Iter: 1198 loss: 4.3883977802099737e-06
Iter: 1199 loss: 4.3876465210070022e-06
Iter: 1200 loss: 4.3866204516678552e-06
Iter: 1201 loss: 4.3865714575149145e-06
Iter: 1202 loss: 4.3855889196317681e-06
Iter: 1203 loss: 4.4005123225739259e-06
Iter: 1204 loss: 4.3855884975607638e-06
Iter: 1205 loss: 4.3848018645980196e-06
Iter: 1206 loss: 4.3929197704695411e-06
Iter: 1207 loss: 4.3847796959502114e-06
Iter: 1208 loss: 4.3843710628640288e-06
Iter: 1209 loss: 4.3832395931180533e-06
Iter: 1210 loss: 4.3896516275031796e-06
Iter: 1211 loss: 4.3829093646876617e-06
Iter: 1212 loss: 4.3815391311197129e-06
Iter: 1213 loss: 4.3874275623252551e-06
Iter: 1214 loss: 4.3812562536097243e-06
Iter: 1215 loss: 4.3806752443048216e-06
Iter: 1216 loss: 4.3805052231408334e-06
Iter: 1217 loss: 4.3798793515260553e-06
Iter: 1218 loss: 4.3791216637750924e-06
Iter: 1219 loss: 4.3790486201930055e-06
Iter: 1220 loss: 4.378228741452921e-06
Iter: 1221 loss: 4.3794916133495088e-06
Iter: 1222 loss: 4.3778417347732968e-06
Iter: 1223 loss: 4.3770563895031529e-06
Iter: 1224 loss: 4.3885857469944594e-06
Iter: 1225 loss: 4.3770554813669954e-06
Iter: 1226 loss: 4.3763543188664984e-06
Iter: 1227 loss: 4.3761800398327247e-06
Iter: 1228 loss: 4.375737595462499e-06
Iter: 1229 loss: 4.3749016569164569e-06
Iter: 1230 loss: 4.3752791612758113e-06
Iter: 1231 loss: 4.3743340653716552e-06
Iter: 1232 loss: 4.3738347786615252e-06
Iter: 1233 loss: 4.3737724568730356e-06
Iter: 1234 loss: 4.3732355319656651e-06
Iter: 1235 loss: 4.3722471036415035e-06
Iter: 1236 loss: 4.395117063196816e-06
Iter: 1237 loss: 4.372246097782484e-06
Iter: 1238 loss: 4.3712964627896516e-06
Iter: 1239 loss: 4.3709934345681176e-06
Iter: 1240 loss: 4.37043756925213e-06
Iter: 1241 loss: 4.3695596629077085e-06
Iter: 1242 loss: 4.3695316289077231e-06
Iter: 1243 loss: 4.3685416039763e-06
Iter: 1244 loss: 4.3703497926263865e-06
Iter: 1245 loss: 4.3681150241274853e-06
Iter: 1246 loss: 4.3675161698928445e-06
Iter: 1247 loss: 4.3663982659364824e-06
Iter: 1248 loss: 4.3914883412248214e-06
Iter: 1249 loss: 4.3663960230278063e-06
Iter: 1250 loss: 4.365128906356248e-06
Iter: 1251 loss: 4.372271592893474e-06
Iter: 1252 loss: 4.3649524360721142e-06
Iter: 1253 loss: 4.36412825067855e-06
Iter: 1254 loss: 4.36409658556522e-06
Iter: 1255 loss: 4.3636041707165252e-06
Iter: 1256 loss: 4.3624042279559979e-06
Iter: 1257 loss: 4.3748829240806423e-06
Iter: 1258 loss: 4.3622657141973795e-06
Iter: 1259 loss: 4.36132261769767e-06
Iter: 1260 loss: 4.3709753537625058e-06
Iter: 1261 loss: 4.3612951077651793e-06
Iter: 1262 loss: 4.3603543334328762e-06
Iter: 1263 loss: 4.366170458541055e-06
Iter: 1264 loss: 4.360243096613898e-06
Iter: 1265 loss: 4.3596815779936242e-06
Iter: 1266 loss: 4.3586842052548236e-06
Iter: 1267 loss: 4.3586842043438125e-06
Iter: 1268 loss: 4.3577552898561969e-06
Iter: 1269 loss: 4.37165394644293e-06
Iter: 1270 loss: 4.3577546373607536e-06
Iter: 1271 loss: 4.3569280405226022e-06
Iter: 1272 loss: 4.3596192865043916e-06
Iter: 1273 loss: 4.3566968378018357e-06
Iter: 1274 loss: 4.3561133164196143e-06
Iter: 1275 loss: 4.355627539202398e-06
Iter: 1276 loss: 4.3554607719038749e-06
Iter: 1277 loss: 4.3544109256924815e-06
Iter: 1278 loss: 4.3544597129610816e-06
Iter: 1279 loss: 4.353585734579752e-06
Iter: 1280 loss: 4.3532789747199675e-06
Iter: 1281 loss: 4.3529534892950968e-06
Iter: 1282 loss: 4.3523319529888683e-06
Iter: 1283 loss: 4.3515066583444595e-06
Iter: 1284 loss: 4.3514589729192641e-06
Iter: 1285 loss: 4.3506706434136147e-06
Iter: 1286 loss: 4.3503989345161544e-06
Iter: 1287 loss: 4.3499504885406322e-06
Iter: 1288 loss: 4.3488859522952357e-06
Iter: 1289 loss: 4.3510109034566708e-06
Iter: 1290 loss: 4.348450663645541e-06
Iter: 1291 loss: 4.3478723729642518e-06
Iter: 1292 loss: 4.3477725367260263e-06
Iter: 1293 loss: 4.347057164775437e-06
Iter: 1294 loss: 4.3468438634044879e-06
Iter: 1295 loss: 4.3464155517734222e-06
Iter: 1296 loss: 4.3457508818386253e-06
Iter: 1297 loss: 4.3453320293304942e-06
Iter: 1298 loss: 4.3450689595176632e-06
Iter: 1299 loss: 4.3444598301454786e-06
Iter: 1300 loss: 4.3444179166774346e-06
Iter: 1301 loss: 4.3437103751183352e-06
Iter: 1302 loss: 4.3426349777040672e-06
Iter: 1303 loss: 4.3426158046582089e-06
Iter: 1304 loss: 4.3416618102539054e-06
Iter: 1305 loss: 4.34446619274578e-06
Iter: 1306 loss: 4.341368462286086e-06
Iter: 1307 loss: 4.3406085181681811e-06
Iter: 1308 loss: 4.3521834039043632e-06
Iter: 1309 loss: 4.3406082291241849e-06
Iter: 1310 loss: 4.3399527066501504e-06
Iter: 1311 loss: 4.3402882988943273e-06
Iter: 1312 loss: 4.3395171643268028e-06
Iter: 1313 loss: 4.3388844904977279e-06
Iter: 1314 loss: 4.3383351869103207e-06
Iter: 1315 loss: 4.3381658757130571e-06
Iter: 1316 loss: 4.3369566672968331e-06
Iter: 1317 loss: 4.3377576243227566e-06
Iter: 1318 loss: 4.3361942241293846e-06
Iter: 1319 loss: 4.3361769379347057e-06
Iter: 1320 loss: 4.3356113977910756e-06
Iter: 1321 loss: 4.3350822009911226e-06
Iter: 1322 loss: 4.3342614994734283e-06
Iter: 1323 loss: 4.3342502300190373e-06
Iter: 1324 loss: 4.3335048395653195e-06
Iter: 1325 loss: 4.3328873309471514e-06
Iter: 1326 loss: 4.3326728882735154e-06
Iter: 1327 loss: 4.331430959817086e-06
Iter: 1328 loss: 4.33557498551544e-06
Iter: 1329 loss: 4.3310914653824428e-06
Iter: 1330 loss: 4.3306256181014712e-06
Iter: 1331 loss: 4.3303709402677e-06
Iter: 1332 loss: 4.3299530404379431e-06
Iter: 1333 loss: 4.3290316556460385e-06
Iter: 1334 loss: 4.3424192014090964e-06
Iter: 1335 loss: 4.3289886477515533e-06
Iter: 1336 loss: 4.32808978563497e-06
Iter: 1337 loss: 4.3318487235906865e-06
Iter: 1338 loss: 4.3278979676383363e-06
Iter: 1339 loss: 4.3270651917032517e-06
Iter: 1340 loss: 4.3382944315764824e-06
Iter: 1341 loss: 4.327061410596e-06
Iter: 1342 loss: 4.3265707633999327e-06
Iter: 1343 loss: 4.3253691484163434e-06
Iter: 1344 loss: 4.3376046971499038e-06
Iter: 1345 loss: 4.3252246882751141e-06
Iter: 1346 loss: 4.324214540392636e-06
Iter: 1347 loss: 4.3242141928977726e-06
Iter: 1348 loss: 4.3235507517521121e-06
Iter: 1349 loss: 4.3304192216774115e-06
Iter: 1350 loss: 4.3235323137757771e-06
Iter: 1351 loss: 4.3230186803090382e-06
Iter: 1352 loss: 4.3221086359289027e-06
Iter: 1353 loss: 4.3221086282548833e-06
Iter: 1354 loss: 4.3209766223583152e-06
Iter: 1355 loss: 4.3214172635796487e-06
Iter: 1356 loss: 4.3201907875534237e-06
Iter: 1357 loss: 4.3196008394267042e-06
Iter: 1358 loss: 4.31949623088846e-06
Iter: 1359 loss: 4.318737199132945e-06
Iter: 1360 loss: 4.3188479592142935e-06
Iter: 1361 loss: 4.3181619878077534e-06
Iter: 1362 loss: 4.3174590471730005e-06
Iter: 1363 loss: 4.31677589554499e-06
Iter: 1364 loss: 4.3166232178003921e-06
Iter: 1365 loss: 4.3155355268840109e-06
Iter: 1366 loss: 4.3161044004186866e-06
Iter: 1367 loss: 4.3148155499787848e-06
Iter: 1368 loss: 4.3136551689821583e-06
Iter: 1369 loss: 4.3266445188563089e-06
Iter: 1370 loss: 4.3136330899227252e-06
Iter: 1371 loss: 4.3128392155194371e-06
Iter: 1372 loss: 4.3128372222020565e-06
Iter: 1373 loss: 4.3124233427755643e-06
Iter: 1374 loss: 4.3112927640059107e-06
Iter: 1375 loss: 4.3182302793304384e-06
Iter: 1376 loss: 4.310991996552435e-06
Iter: 1377 loss: 4.310266892789476e-06
Iter: 1378 loss: 4.3101714406672176e-06
Iter: 1379 loss: 4.3093652615778551e-06
Iter: 1380 loss: 4.3104591110079436e-06
Iter: 1381 loss: 4.3089617274974628e-06
Iter: 1382 loss: 4.3082886601277739e-06
Iter: 1383 loss: 4.3075644530180729e-06
Iter: 1384 loss: 4.3074497560324618e-06
Iter: 1385 loss: 4.3066679576533109e-06
Iter: 1386 loss: 4.3066679225583812e-06
Iter: 1387 loss: 4.3060043389771374e-06
Iter: 1388 loss: 4.30903196590865e-06
Iter: 1389 loss: 4.3058775664498032e-06
Iter: 1390 loss: 4.3052499592331453e-06
Iter: 1391 loss: 4.3041261251579486e-06
Iter: 1392 loss: 4.3317530956159721e-06
Iter: 1393 loss: 4.3041260759523579e-06
Iter: 1394 loss: 4.3029816657043985e-06
Iter: 1395 loss: 4.3059337166229085e-06
Iter: 1396 loss: 4.3025899380370516e-06
Iter: 1397 loss: 4.301673548054316e-06
Iter: 1398 loss: 4.3038947940389e-06
Iter: 1399 loss: 4.3013449189878991e-06
Iter: 1400 loss: 4.3008678953294677e-06
Iter: 1401 loss: 4.3007631076654143e-06
Iter: 1402 loss: 4.3001625022746619e-06
Iter: 1403 loss: 4.2991211180355659e-06
Iter: 1404 loss: 4.2991206014840274e-06
Iter: 1405 loss: 4.2981607694651287e-06
Iter: 1406 loss: 4.2979328731081806e-06
Iter: 1407 loss: 4.29732015413116e-06
Iter: 1408 loss: 4.2969718367484675e-06
Iter: 1409 loss: 4.2967044586332907e-06
Iter: 1410 loss: 4.2961016805449509e-06
Iter: 1411 loss: 4.2958356728237906e-06
Iter: 1412 loss: 4.2955293266180093e-06
Iter: 1413 loss: 4.2948447804076195e-06
Iter: 1414 loss: 4.2938490951265706e-06
Iter: 1415 loss: 4.2938206546420832e-06
Iter: 1416 loss: 4.2935086724320662e-06
Iter: 1417 loss: 4.2930996797672423e-06
Iter: 1418 loss: 4.2926886040973573e-06
Iter: 1419 loss: 4.2917510916189014e-06
Iter: 1420 loss: 4.3040491264536542e-06
Iter: 1421 loss: 4.2916899634122213e-06
Iter: 1422 loss: 4.2907122960617521e-06
Iter: 1423 loss: 4.294151980534262e-06
Iter: 1424 loss: 4.2904592601462477e-06
Iter: 1425 loss: 4.2896042209717815e-06
Iter: 1426 loss: 4.2955528002044364e-06
Iter: 1427 loss: 4.2895243723054181e-06
Iter: 1428 loss: 4.2887055729558686e-06
Iter: 1429 loss: 4.2936475344177394e-06
Iter: 1430 loss: 4.2886042555300908e-06
Iter: 1431 loss: 4.2879694738575531e-06
Iter: 1432 loss: 4.287021482949604e-06
Iter: 1433 loss: 4.2870007875983189e-06
Iter: 1434 loss: 4.2860611928780769e-06
Iter: 1435 loss: 4.2881474305793485e-06
Iter: 1436 loss: 4.2857026410416775e-06
Iter: 1437 loss: 4.2847188909094071e-06
Iter: 1438 loss: 4.2857824651699927e-06
Iter: 1439 loss: 4.28418026592573e-06
Iter: 1440 loss: 4.2834375087290849e-06
Iter: 1441 loss: 4.2833018329815562e-06
Iter: 1442 loss: 4.2829253623885293e-06
Iter: 1443 loss: 4.2819533505829224e-06
Iter: 1444 loss: 4.28990744265518e-06
Iter: 1445 loss: 4.2817795547507164e-06
Iter: 1446 loss: 4.2807688960382456e-06
Iter: 1447 loss: 4.2886360072801762e-06
Iter: 1448 loss: 4.280696548536731e-06
Iter: 1449 loss: 4.2798643018382859e-06
Iter: 1450 loss: 4.2899156614608191e-06
Iter: 1451 loss: 4.2798543490577479e-06
Iter: 1452 loss: 4.2793709174427843e-06
Iter: 1453 loss: 4.2781438298177377e-06
Iter: 1454 loss: 4.2889681595463906e-06
Iter: 1455 loss: 4.2779501574072847e-06
Iter: 1456 loss: 4.2771665935409666e-06
Iter: 1457 loss: 4.2771234425506165e-06
Iter: 1458 loss: 4.276311407534895e-06
Iter: 1459 loss: 4.27858501969424e-06
Iter: 1460 loss: 4.276051401657294e-06
Iter: 1461 loss: 4.2755935231160762e-06
Iter: 1462 loss: 4.274906894269925e-06
Iter: 1463 loss: 4.2748925189354061e-06
Iter: 1464 loss: 4.2738874580857281e-06
Iter: 1465 loss: 4.2774010086095183e-06
Iter: 1466 loss: 4.273625749418005e-06
Iter: 1467 loss: 4.2728207865836919e-06
Iter: 1468 loss: 4.2846731570601569e-06
Iter: 1469 loss: 4.2728199116392074e-06
Iter: 1470 loss: 4.2721038349238735e-06
Iter: 1471 loss: 4.2722432666636837e-06
Iter: 1472 loss: 4.2715706065821405e-06
Iter: 1473 loss: 4.2709234127631969e-06
Iter: 1474 loss: 4.27003696612301e-06
Iter: 1475 loss: 4.2699954959100243e-06
Iter: 1476 loss: 4.2686612660324653e-06
Iter: 1477 loss: 4.2725645959533957e-06
Iter: 1478 loss: 4.26824957248754e-06
Iter: 1479 loss: 4.2682626320303952e-06
Iter: 1480 loss: 4.2677128305238026e-06
Iter: 1481 loss: 4.267301397090467e-06
Iter: 1482 loss: 4.2663701510342195e-06
Iter: 1483 loss: 4.278864440843602e-06
Iter: 1484 loss: 4.26631358488919e-06
Iter: 1485 loss: 4.26545851788003e-06
Iter: 1486 loss: 4.2665821622990283e-06
Iter: 1487 loss: 4.2650247597706996e-06
Iter: 1488 loss: 4.2647506027461314e-06
Iter: 1489 loss: 4.2645476348831981e-06
Iter: 1490 loss: 4.2640959657288346e-06
Iter: 1491 loss: 4.2629028483380454e-06
Iter: 1492 loss: 4.2716648090134075e-06
Iter: 1493 loss: 4.2626516852645325e-06
Iter: 1494 loss: 4.2615196139830043e-06
Iter: 1495 loss: 4.2700602901698027e-06
Iter: 1496 loss: 4.2614319473330977e-06
Iter: 1497 loss: 4.2608565425438437e-06
Iter: 1498 loss: 4.2608221709458382e-06
Iter: 1499 loss: 4.2604138112860254e-06
Iter: 1500 loss: 4.2594288550919628e-06
Iter: 1501 loss: 4.2700516546449539e-06
Iter: 1502 loss: 4.2593240378821872e-06
Iter: 1503 loss: 4.2583056148641809e-06
Iter: 1504 loss: 4.2621877385839647e-06
Iter: 1505 loss: 4.2580631513557324e-06
Iter: 1506 loss: 4.2571732330500786e-06
Iter: 1507 loss: 4.26525985257881e-06
Iter: 1508 loss: 4.2571326801517775e-06
Iter: 1509 loss: 4.256345645916192e-06
Iter: 1510 loss: 4.2606082582203239e-06
Iter: 1511 loss: 4.2562286844845472e-06
Iter: 1512 loss: 4.2556735479497539e-06
Iter: 1513 loss: 4.2549220885826504e-06
Iter: 1514 loss: 4.2548838537074964e-06
Iter: 1515 loss: 4.25404331555227e-06
Iter: 1516 loss: 4.2535851027375986e-06
Iter: 1517 loss: 4.25321085212188e-06
Iter: 1518 loss: 4.2518630359692316e-06
Iter: 1519 loss: 4.2610360339761437e-06
Iter: 1520 loss: 4.2517313868905311e-06
Iter: 1521 loss: 4.2510301333721138e-06
Iter: 1522 loss: 4.2509387374507626e-06
Iter: 1523 loss: 4.2505123628188767e-06
Iter: 1524 loss: 4.24947932579227e-06
Iter: 1525 loss: 4.2604560983536887e-06
Iter: 1526 loss: 4.2493654462297684e-06
Iter: 1527 loss: 4.2482146159656925e-06
Iter: 1528 loss: 4.2491677069388186e-06
Iter: 1529 loss: 4.2475288155181593e-06
Iter: 1530 loss: 4.2467489005140519e-06
Iter: 1531 loss: 4.2466977054180505e-06
Iter: 1532 loss: 4.2457672687029818e-06
Iter: 1533 loss: 4.2462622046622908e-06
Iter: 1534 loss: 4.2451533153784685e-06
Iter: 1535 loss: 4.2444286259343285e-06
Iter: 1536 loss: 4.2436764925654212e-06
Iter: 1537 loss: 4.2435407997919373e-06
Iter: 1538 loss: 4.2428083424567679e-06
Iter: 1539 loss: 4.2428036665677431e-06
Iter: 1540 loss: 4.2419979713809774e-06
Iter: 1541 loss: 4.2430567995830608e-06
Iter: 1542 loss: 4.2415893235150931e-06
Iter: 1543 loss: 4.24080643905138e-06
Iter: 1544 loss: 4.240940313526162e-06
Iter: 1545 loss: 4.2402183685388538e-06
Iter: 1546 loss: 4.2393925515277949e-06
Iter: 1547 loss: 4.2450510990504815e-06
Iter: 1548 loss: 4.23931297673977e-06
Iter: 1549 loss: 4.2384739119781037e-06
Iter: 1550 loss: 4.242059038179152e-06
Iter: 1551 loss: 4.2382996449735023e-06
Iter: 1552 loss: 4.23768356650281e-06
Iter: 1553 loss: 4.2368403065674106e-06
Iter: 1554 loss: 4.2368006085652864e-06
Iter: 1555 loss: 4.235764419507698e-06
Iter: 1556 loss: 4.2358360398679e-06
Iter: 1557 loss: 4.2349566305925213e-06
Iter: 1558 loss: 4.2346243837372737e-06
Iter: 1559 loss: 4.2342570669254286e-06
Iter: 1560 loss: 4.2335476451629608e-06
Iter: 1561 loss: 4.23272664227828e-06
Iter: 1562 loss: 4.2326293786907575e-06
Iter: 1563 loss: 4.2319077348189246e-06
Iter: 1564 loss: 4.2310371423166066e-06
Iter: 1565 loss: 4.2309517741352673e-06
Iter: 1566 loss: 4.2294835587955306e-06
Iter: 1567 loss: 4.2345350124104267e-06
Iter: 1568 loss: 4.2290945321117278e-06
Iter: 1569 loss: 4.228735469922992e-06
Iter: 1570 loss: 4.2284491178586011e-06
Iter: 1571 loss: 4.227812373285248e-06
Iter: 1572 loss: 4.22667247980022e-06
Iter: 1573 loss: 4.2547250105216478e-06
Iter: 1574 loss: 4.2266724340759046e-06
Iter: 1575 loss: 4.2256943910589829e-06
Iter: 1576 loss: 4.2272567098824895e-06
Iter: 1577 loss: 4.2252411952400418e-06
Iter: 1578 loss: 4.2244669575488717e-06
Iter: 1579 loss: 4.2340333722762344e-06
Iter: 1580 loss: 4.2244591287312807e-06
Iter: 1581 loss: 4.2235560187986394e-06
Iter: 1582 loss: 4.22411029783142e-06
Iter: 1583 loss: 4.2229768228174834e-06
Iter: 1584 loss: 4.2220266360075454e-06
Iter: 1585 loss: 4.2232990902015733e-06
Iter: 1586 loss: 4.2215481885459611e-06
Iter: 1587 loss: 4.2207977599958561e-06
Iter: 1588 loss: 4.2251971805688895e-06
Iter: 1589 loss: 4.2207000459013864e-06
Iter: 1590 loss: 4.2199268190607535e-06
Iter: 1591 loss: 4.2240086262127563e-06
Iter: 1592 loss: 4.2198071550687186e-06
Iter: 1593 loss: 4.2192755381730823e-06
Iter: 1594 loss: 4.2182364714006792e-06
Iter: 1595 loss: 4.2391647270076551e-06
Iter: 1596 loss: 4.2182282690796343e-06
Iter: 1597 loss: 4.2169770461149309e-06
Iter: 1598 loss: 4.217968313565158e-06
Iter: 1599 loss: 4.2162224046351781e-06
Iter: 1600 loss: 4.2149571655451e-06
Iter: 1601 loss: 4.2249110406931561e-06
Iter: 1602 loss: 4.2148691089830718e-06
Iter: 1603 loss: 4.2144668889743829e-06
Iter: 1604 loss: 4.2142829368590138e-06
Iter: 1605 loss: 4.2139240226676649e-06
Iter: 1606 loss: 4.2128138386482938e-06
Iter: 1607 loss: 4.2150754918221029e-06
Iter: 1608 loss: 4.2121205331842268e-06
Iter: 1609 loss: 4.2105531185957542e-06
Iter: 1610 loss: 4.220713400370164e-06
Iter: 1611 loss: 4.2103838689073882e-06
Iter: 1612 loss: 4.20938258645515e-06
Iter: 1613 loss: 4.2161247998646674e-06
Iter: 1614 loss: 4.2092824521726193e-06
Iter: 1615 loss: 4.2083615722642821e-06
Iter: 1616 loss: 4.2185757481418012e-06
Iter: 1617 loss: 4.2083432063680454e-06
Iter: 1618 loss: 4.2079057542101492e-06
Iter: 1619 loss: 4.2068039619594219e-06
Iter: 1620 loss: 4.2168537444761784e-06
Iter: 1621 loss: 4.20663989413518e-06
Iter: 1622 loss: 4.205779355150036e-06
Iter: 1623 loss: 4.2057575425252756e-06
Iter: 1624 loss: 4.20488993005559e-06
Iter: 1625 loss: 4.206674402825675e-06
Iter: 1626 loss: 4.2045418695116565e-06
Iter: 1627 loss: 4.2039268634770907e-06
Iter: 1628 loss: 4.2041598470716819e-06
Iter: 1629 loss: 4.20349834993592e-06
Iter: 1630 loss: 4.2026985959918558e-06
Iter: 1631 loss: 4.2032083683084967e-06
Iter: 1632 loss: 4.2021899256049924e-06
Iter: 1633 loss: 4.2015219221451417e-06
Iter: 1634 loss: 4.2014856231702142e-06
Iter: 1635 loss: 4.2008830602977963e-06
Iter: 1636 loss: 4.1997570065147691e-06
Iter: 1637 loss: 4.224986961584605e-06
Iter: 1638 loss: 4.1997546498324819e-06
Iter: 1639 loss: 4.1986911060084363e-06
Iter: 1640 loss: 4.1994752729466036e-06
Iter: 1641 loss: 4.1980374597647157e-06
Iter: 1642 loss: 4.1971707885759144e-06
Iter: 1643 loss: 4.1971633435208066e-06
Iter: 1644 loss: 4.1961794960558372e-06
Iter: 1645 loss: 4.1971715088679418e-06
Iter: 1646 loss: 4.1956276188019786e-06
Iter: 1647 loss: 4.1948627920410209e-06
Iter: 1648 loss: 4.19357217414798e-06
Iter: 1649 loss: 4.1935694407033661e-06
Iter: 1650 loss: 4.1928208726042956e-06
Iter: 1651 loss: 4.192778952492657e-06
Iter: 1652 loss: 4.1919863686467608e-06
Iter: 1653 loss: 4.1947536539561244e-06
Iter: 1654 loss: 4.1917794304898141e-06
Iter: 1655 loss: 4.1913191516007841e-06
Iter: 1656 loss: 4.1902990666390746e-06
Iter: 1657 loss: 4.2048969303336741e-06
Iter: 1658 loss: 4.1902488221939193e-06
Iter: 1659 loss: 4.1894375444177506e-06
Iter: 1660 loss: 4.1894053554492969e-06
Iter: 1661 loss: 4.18853258296846e-06
Iter: 1662 loss: 4.1885949550611757e-06
Iter: 1663 loss: 4.1878527202858065e-06
Iter: 1664 loss: 4.1870287171165269e-06
Iter: 1665 loss: 4.1872793360626772e-06
Iter: 1666 loss: 4.1864393751843936e-06
Iter: 1667 loss: 4.1855442552973358e-06
Iter: 1668 loss: 4.1939560643348354e-06
Iter: 1669 loss: 4.1855078020200613e-06
Iter: 1670 loss: 4.18468433889695e-06
Iter: 1671 loss: 4.1884007884199491e-06
Iter: 1672 loss: 4.1845246552074889e-06
Iter: 1673 loss: 4.1837960342674289e-06
Iter: 1674 loss: 4.182550043828154e-06
Iter: 1675 loss: 4.1825485822378854e-06
Iter: 1676 loss: 4.1812948901531571e-06
Iter: 1677 loss: 4.1832910157445245e-06
Iter: 1678 loss: 4.1807130958827026e-06
Iter: 1679 loss: 4.1807993776337183e-06
Iter: 1680 loss: 4.1802118868155279e-06
Iter: 1681 loss: 4.1797350105210046e-06
Iter: 1682 loss: 4.1787048340848734e-06
Iter: 1683 loss: 4.1946045820897977e-06
Iter: 1684 loss: 4.1786664278588522e-06
Iter: 1685 loss: 4.1774189208805128e-06
Iter: 1686 loss: 4.1768035490880775e-06
Iter: 1687 loss: 4.1762092027791238e-06
Iter: 1688 loss: 4.1751908673971156e-06
Iter: 1689 loss: 4.1751788427734736e-06
Iter: 1690 loss: 4.1742362659639925e-06
Iter: 1691 loss: 4.180462536444332e-06
Iter: 1692 loss: 4.1741382440805823e-06
Iter: 1693 loss: 4.1736420812067834e-06
Iter: 1694 loss: 4.1725856768672285e-06
Iter: 1695 loss: 4.1895886709957748e-06
Iter: 1696 loss: 4.1725525388111764e-06
Iter: 1697 loss: 4.1718532752982537e-06
Iter: 1698 loss: 4.171839644090241e-06
Iter: 1699 loss: 4.1709684344405224e-06
Iter: 1700 loss: 4.17032155338759e-06
Iter: 1701 loss: 4.1700313668085094e-06
Iter: 1702 loss: 4.1691179442360159e-06
Iter: 1703 loss: 4.1703882762867793e-06
Iter: 1704 loss: 4.1686657218286405e-06
Iter: 1705 loss: 4.1678278897112849e-06
Iter: 1706 loss: 4.1758153235738332e-06
Iter: 1707 loss: 4.1677955614285593e-06
Iter: 1708 loss: 4.1668796204378619e-06
Iter: 1709 loss: 4.16828092635395e-06
Iter: 1710 loss: 4.16644634300669e-06
Iter: 1711 loss: 4.1655825575156e-06
Iter: 1712 loss: 4.165875042885147e-06
Iter: 1713 loss: 4.1649718220986078e-06
Iter: 1714 loss: 4.1640552034257178e-06
Iter: 1715 loss: 4.1635685961182918e-06
Iter: 1716 loss: 4.1631527719532849e-06
Iter: 1717 loss: 4.1617301066211378e-06
Iter: 1718 loss: 4.1676548684024161e-06
Iter: 1719 loss: 4.1614249920347213e-06
Iter: 1720 loss: 4.1609273798125906e-06
Iter: 1721 loss: 4.1607954581586767e-06
Iter: 1722 loss: 4.1600597593432114e-06
Iter: 1723 loss: 4.1586552858905958e-06
Iter: 1724 loss: 4.1885044397005211e-06
Iter: 1725 loss: 4.1586491226195728e-06
Iter: 1726 loss: 4.1575491850562688e-06
Iter: 1727 loss: 4.159408810671703e-06
Iter: 1728 loss: 4.1570543737336084e-06
Iter: 1729 loss: 4.1562101635653875e-06
Iter: 1730 loss: 4.1664048228873064e-06
Iter: 1731 loss: 4.1562000549159947e-06
Iter: 1732 loss: 4.15527791116369e-06
Iter: 1733 loss: 4.1575760828770569e-06
Iter: 1734 loss: 4.1549539770443221e-06
Iter: 1735 loss: 4.154303326221033e-06
Iter: 1736 loss: 4.1532440706676825e-06
Iter: 1737 loss: 4.1532378342996169e-06
Iter: 1738 loss: 4.1528229075184389e-06
Iter: 1739 loss: 4.1526596273848437e-06
Iter: 1740 loss: 4.1520440771594488e-06
Iter: 1741 loss: 4.1513191777236676e-06
Iter: 1742 loss: 4.1512396679777015e-06
Iter: 1743 loss: 4.1504654315643086e-06
Iter: 1744 loss: 4.1512546182787991e-06
Iter: 1745 loss: 4.15003256534068e-06
Iter: 1746 loss: 4.1491950973967861e-06
Iter: 1747 loss: 4.1581273098126171e-06
Iter: 1748 loss: 4.1491748013215722e-06
Iter: 1749 loss: 4.1483152638763417e-06
Iter: 1750 loss: 4.1493918289303387e-06
Iter: 1751 loss: 4.1478706413493268e-06
Iter: 1752 loss: 4.1469776055457046e-06
Iter: 1753 loss: 4.1463520730392125e-06
Iter: 1754 loss: 4.1460337398170313e-06
Iter: 1755 loss: 4.1448543869734927e-06
Iter: 1756 loss: 4.1463096619777824e-06
Iter: 1757 loss: 4.1442405705949081e-06
Iter: 1758 loss: 4.14350776700058e-06
Iter: 1759 loss: 4.14345280086012e-06
Iter: 1760 loss: 4.1425631561235124e-06
Iter: 1761 loss: 4.1424174651513856e-06
Iter: 1762 loss: 4.14180571228088e-06
Iter: 1763 loss: 4.141022083267263e-06
Iter: 1764 loss: 4.1402657167320166e-06
Iter: 1765 loss: 4.1400931373648776e-06
Iter: 1766 loss: 4.1389456719702462e-06
Iter: 1767 loss: 4.1436664116810086e-06
Iter: 1768 loss: 4.1386958949712237e-06
Iter: 1769 loss: 4.13823791795254e-06
Iter: 1770 loss: 4.1380705011671686e-06
Iter: 1771 loss: 4.13761252124226e-06
Iter: 1772 loss: 4.1363670744100577e-06
Iter: 1773 loss: 4.14421374367156e-06
Iter: 1774 loss: 4.1360459600626573e-06
Iter: 1775 loss: 4.1347968747245893e-06
Iter: 1776 loss: 4.1458296461793858e-06
Iter: 1777 loss: 4.1347344001312027e-06
Iter: 1778 loss: 4.1339923644923764e-06
Iter: 1779 loss: 4.1339827223916413e-06
Iter: 1780 loss: 4.1335061093204482e-06
Iter: 1781 loss: 4.1322258540452256e-06
Iter: 1782 loss: 4.1408393201341443e-06
Iter: 1783 loss: 4.1319225724678539e-06
Iter: 1784 loss: 4.1308665624742361e-06
Iter: 1785 loss: 4.13086450009229e-06
Iter: 1786 loss: 4.1299999860472238e-06
Iter: 1787 loss: 4.1362438462590882e-06
Iter: 1788 loss: 4.1299257821353226e-06
Iter: 1789 loss: 4.129327445669521e-06
Iter: 1790 loss: 4.1289914579074912e-06
Iter: 1791 loss: 4.1287309256219215e-06
Iter: 1792 loss: 4.1279617203057553e-06
Iter: 1793 loss: 4.1280534557898647e-06
Iter: 1794 loss: 4.1273729205929846e-06
Iter: 1795 loss: 4.1261679457087731e-06
Iter: 1796 loss: 4.12608108773463e-06
Iter: 1797 loss: 4.1251776253952107e-06
Iter: 1798 loss: 4.12347243262531e-06
Iter: 1799 loss: 4.1321405553998271e-06
Iter: 1800 loss: 4.1231927790710906e-06
Iter: 1801 loss: 4.1229870289289877e-06
Iter: 1802 loss: 4.122613307769334e-06
Iter: 1803 loss: 4.12201829326892e-06
Iter: 1804 loss: 4.1208615492152217e-06
Iter: 1805 loss: 4.1444243779138955e-06
Iter: 1806 loss: 4.1208534376246912e-06
Iter: 1807 loss: 4.1197818095741447e-06
Iter: 1808 loss: 4.1198730521560627e-06
Iter: 1809 loss: 4.1189511527529815e-06
Iter: 1810 loss: 4.11863778961737e-06
Iter: 1811 loss: 4.11821767755217e-06
Iter: 1812 loss: 4.1176819052509474e-06
Iter: 1813 loss: 4.1164332267702258e-06
Iter: 1814 loss: 4.1316405302868153e-06
Iter: 1815 loss: 4.1163338451139816e-06
Iter: 1816 loss: 4.1152877284980577e-06
Iter: 1817 loss: 4.1223451999923877e-06
Iter: 1818 loss: 4.11518349379104e-06
Iter: 1819 loss: 4.1143760333753056e-06
Iter: 1820 loss: 4.1257295592788055e-06
Iter: 1821 loss: 4.1143739242672141e-06
Iter: 1822 loss: 4.1138364817170151e-06
Iter: 1823 loss: 4.1126188752852e-06
Iter: 1824 loss: 4.1288677329253291e-06
Iter: 1825 loss: 4.1125441062564389e-06
Iter: 1826 loss: 4.1113062844042688e-06
Iter: 1827 loss: 4.1200320249556425e-06
Iter: 1828 loss: 4.1111936247208e-06
Iter: 1829 loss: 4.1103894761610625e-06
Iter: 1830 loss: 4.1218460877429818e-06
Iter: 1831 loss: 4.1103877912386853e-06
Iter: 1832 loss: 4.1096793307325014e-06
Iter: 1833 loss: 4.1094639647311905e-06
Iter: 1834 loss: 4.1090425277759124e-06
Iter: 1835 loss: 4.1082105203338715e-06
Iter: 1836 loss: 4.1071731100171849e-06
Iter: 1837 loss: 4.1070867903211404e-06
Iter: 1838 loss: 4.10565657490672e-06
Iter: 1839 loss: 4.1102507582747391e-06
Iter: 1840 loss: 4.1052515471944838e-06
Iter: 1841 loss: 4.1050876053469452e-06
Iter: 1842 loss: 4.1045901877385862e-06
Iter: 1843 loss: 4.1041080858304877e-06
Iter: 1844 loss: 4.1031206330852322e-06
Iter: 1845 loss: 4.1207718636815647e-06
Iter: 1846 loss: 4.1031023483121911e-06
Iter: 1847 loss: 4.1021804577492828e-06
Iter: 1848 loss: 4.1028685738333927e-06
Iter: 1849 loss: 4.1016155843879991e-06
Iter: 1850 loss: 4.101053660181695e-06
Iter: 1851 loss: 4.100903766424069e-06
Iter: 1852 loss: 4.1003465864335671e-06
Iter: 1853 loss: 4.0989198156465913e-06
Iter: 1854 loss: 4.111043781897287e-06
Iter: 1855 loss: 4.098679665176335e-06
Iter: 1856 loss: 4.0974937318132765e-06
Iter: 1857 loss: 4.1056700581602382e-06
Iter: 1858 loss: 4.0973810653917766e-06
Iter: 1859 loss: 4.0962493980746563e-06
Iter: 1860 loss: 4.1078736662837426e-06
Iter: 1861 loss: 4.0962168677062571e-06
Iter: 1862 loss: 4.0955894708773249e-06
Iter: 1863 loss: 4.0942434913138494e-06
Iter: 1864 loss: 4.1154630957838886e-06
Iter: 1865 loss: 4.0941972661229872e-06
Iter: 1866 loss: 4.0929710872957859e-06
Iter: 1867 loss: 4.1039349662135741e-06
Iter: 1868 loss: 4.0929121675501682e-06
Iter: 1869 loss: 4.0919643343448605e-06
Iter: 1870 loss: 4.1036998823903589e-06
Iter: 1871 loss: 4.0919549074123771e-06
Iter: 1872 loss: 4.0913608031660292e-06
Iter: 1873 loss: 4.0907173132031304e-06
Iter: 1874 loss: 4.0906176829489944e-06
Iter: 1875 loss: 4.0897291366315628e-06
Iter: 1876 loss: 4.0909139873808524e-06
Iter: 1877 loss: 4.0892813545681269e-06
Iter: 1878 loss: 4.0880987863836419e-06
Iter: 1879 loss: 4.0876208765195213e-06
Iter: 1880 loss: 4.0869924288628539e-06
Iter: 1881 loss: 4.08584434989997e-06
Iter: 1882 loss: 4.0858442208348795e-06
Iter: 1883 loss: 4.0849134809810482e-06
Iter: 1884 loss: 4.095204382018092e-06
Iter: 1885 loss: 4.0848946297742639e-06
Iter: 1886 loss: 4.0844159859793896e-06
Iter: 1887 loss: 4.0832305578473988e-06
Iter: 1888 loss: 4.0948094745514223e-06
Iter: 1889 loss: 4.0830752283840416e-06
Iter: 1890 loss: 4.0818705706906228e-06
Iter: 1891 loss: 4.0850155289186363e-06
Iter: 1892 loss: 4.0814624163399768e-06
Iter: 1893 loss: 4.0806197006836981e-06
Iter: 1894 loss: 4.080527131332255e-06
Iter: 1895 loss: 4.0798576032295571e-06
Iter: 1896 loss: 4.0787421212581666e-06
Iter: 1897 loss: 4.0787384940642e-06
Iter: 1898 loss: 4.0778063025391741e-06
Iter: 1899 loss: 4.077778873246961e-06
Iter: 1900 loss: 4.0770523153020962e-06
Iter: 1901 loss: 4.0765161261056327e-06
Iter: 1902 loss: 4.076274133526378e-06
Iter: 1903 loss: 4.0756358756021822e-06
Iter: 1904 loss: 4.0745136797206885e-06
Iter: 1905 loss: 4.074513565613144e-06
Iter: 1906 loss: 4.0735483762525354e-06
Iter: 1907 loss: 4.0780743914468064e-06
Iter: 1908 loss: 4.0733709383988137e-06
Iter: 1909 loss: 4.07249288023839e-06
Iter: 1910 loss: 4.0818550695097734e-06
Iter: 1911 loss: 4.07247155247626e-06
Iter: 1912 loss: 4.0718490362511276e-06
Iter: 1913 loss: 4.071447310248082e-06
Iter: 1914 loss: 4.0712062505734766e-06
Iter: 1915 loss: 4.0703081621172145e-06
Iter: 1916 loss: 4.0696362166465623e-06
Iter: 1917 loss: 4.0693399486431587e-06
Iter: 1918 loss: 4.067715574873695e-06
Iter: 1919 loss: 4.0719794805369615e-06
Iter: 1920 loss: 4.0671670618344025e-06
Iter: 1921 loss: 4.0671875222862946e-06
Iter: 1922 loss: 4.0666459810590145e-06
Iter: 1923 loss: 4.0661332203823478e-06
Iter: 1924 loss: 4.065080815910844e-06
Iter: 1925 loss: 4.0837735011213943e-06
Iter: 1926 loss: 4.0650606900209943e-06
Iter: 1927 loss: 4.0640115710049e-06
Iter: 1928 loss: 4.0638895682724782e-06
Iter: 1929 loss: 4.0631344722630704e-06
Iter: 1930 loss: 4.062168403138847e-06
Iter: 1931 loss: 4.0621663680903469e-06
Iter: 1932 loss: 4.0612003566450217e-06
Iter: 1933 loss: 4.0651940943756265e-06
Iter: 1934 loss: 4.0609913459344925e-06
Iter: 1935 loss: 4.0604646250903346e-06
Iter: 1936 loss: 4.0594330004719268e-06
Iter: 1937 loss: 4.0800788481492468e-06
Iter: 1938 loss: 4.0594244588352567e-06
Iter: 1939 loss: 4.0584530551185336e-06
Iter: 1940 loss: 4.0683382602269328e-06
Iter: 1941 loss: 4.0584239922658953e-06
Iter: 1942 loss: 4.0573551460688079e-06
Iter: 1943 loss: 4.0613502489385165e-06
Iter: 1944 loss: 4.0570956663334521e-06
Iter: 1945 loss: 4.0564114487131372e-06
Iter: 1946 loss: 4.0552942600096933e-06
Iter: 1947 loss: 4.0552880769838223e-06
Iter: 1948 loss: 4.054716068128332e-06
Iter: 1949 loss: 4.0545670420796886e-06
Iter: 1950 loss: 4.053910010166408e-06
Iter: 1951 loss: 4.0534773136813978e-06
Iter: 1952 loss: 4.0532280031782306e-06
Iter: 1953 loss: 4.0522119955474852e-06
Iter: 1954 loss: 4.052449095626941e-06
Iter: 1955 loss: 4.05146582740343e-06
Iter: 1956 loss: 4.0502761118470544e-06
Iter: 1957 loss: 4.0517145290481625e-06
Iter: 1958 loss: 4.0496516949852025e-06
Iter: 1959 loss: 4.0486806537941568e-06
Iter: 1960 loss: 4.0597161535392448e-06
Iter: 1961 loss: 4.0486636589392265e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.4/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.8 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi0.8
+ date
Sat Nov  7 20:00:11 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.8/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.4/300_300_300_1 --function f1 --psi 2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa357c5f2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa357c422f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa357c42bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa357ca0598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3356e0d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3356e0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3356b1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3355e1840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3355e1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa335671378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa335671400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa31065dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa31066e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3106abd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa335639840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa335642e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa335624488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa335624620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3105f1c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3105ea730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3105101e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa310510e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa310491950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3104af6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3104af400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa31057b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa31062d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa31062d158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa310613730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa310613950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3103e1730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa310355048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3103832f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa310374ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3103b98c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa3103a8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.005291092309874897
test_loss: 0.00540867651523252
train_loss: 0.004522086229686513
test_loss: 0.00507727168115315
train_loss: 0.005156019301879282
test_loss: 0.004945650679308092
train_loss: 0.004671700937253409
test_loss: 0.004731536861926327
train_loss: 0.004456449730720728
test_loss: 0.004898575255494434
train_loss: 0.004790469548912615
test_loss: 0.005072434504466882
train_loss: 0.0049075505489083425
test_loss: 0.0050375001754714415
train_loss: 0.004657039236321277
test_loss: 0.004607280969686196
train_loss: 0.004778034053796582
test_loss: 0.005042706808436893
train_loss: 0.004199762923233677
test_loss: 0.004489881961518915
train_loss: 0.004961619674002408
test_loss: 0.0048747823174650545
train_loss: 0.004459590613499118
test_loss: 0.0047708583045117416
train_loss: 0.004568460148179854
test_loss: 0.004783726063717899
train_loss: 0.004373076762461647
test_loss: 0.004599580625080962
train_loss: 0.004363144313949112
test_loss: 0.004719985434230223
train_loss: 0.004955983773185462
test_loss: 0.00485602797035527
train_loss: 0.00455000814138852
test_loss: 0.004816948468834909
train_loss: 0.004959142357607211
test_loss: 0.0047252367052737845
train_loss: 0.004713233092485402
test_loss: 0.004717917571474044
train_loss: 0.005050695883765773
test_loss: 0.00507116953168224
train_loss: 0.004551798008477156
test_loss: 0.004602583471237397
train_loss: 0.004184355837899786
test_loss: 0.004335842470794009
train_loss: 0.004130114879301486
test_loss: 0.004684781422477207
train_loss: 0.004492283004790565
test_loss: 0.0044106385776007465
train_loss: 0.004029780180286446
test_loss: 0.004415449062525471
train_loss: 0.004764175325533582
test_loss: 0.004451033535741133
train_loss: 0.004100857467476571
test_loss: 0.004512813563413889
train_loss: 0.004358354830742385
test_loss: 0.004563520218568716
train_loss: 0.004143529090370801
test_loss: 0.0046598341047776725
train_loss: 0.004160391518528591
test_loss: 0.004534827956491201
train_loss: 0.004032736356506425
test_loss: 0.004559297658831772
train_loss: 0.0039870826885528565
test_loss: 0.004183425267116449
train_loss: 0.004157965437843855
test_loss: 0.004309672176332487
train_loss: 0.00407430282735385
test_loss: 0.004553786687218872
train_loss: 0.004230012280149923
test_loss: 0.004501907592516084
train_loss: 0.004034435211332285
test_loss: 0.004395529049816002
train_loss: 0.004470782496670297
test_loss: 0.0050026069393284845
train_loss: 0.004137388467112264
test_loss: 0.004500434449604608
train_loss: 0.0038310937815080213
test_loss: 0.004147963129166164
train_loss: 0.004166645782639451
test_loss: 0.004434479002887313
train_loss: 0.0038628847198016137
test_loss: 0.004243450263233323
train_loss: 0.0038939931294692605
test_loss: 0.004126499898666045
train_loss: 0.0037150118357067922
test_loss: 0.00425227864010057
train_loss: 0.004151141314536029
test_loss: 0.004557640123541958
train_loss: 0.004236025584867667
test_loss: 0.004184994967572229
train_loss: 0.003970446951443137
test_loss: 0.0043646461999212
train_loss: 0.0038095132648669414
test_loss: 0.004106874899116078
train_loss: 0.0039176191320992775
test_loss: 0.00415399112846192
train_loss: 0.0037729962953917725
test_loss: 0.004130766764793438
train_loss: 0.0037760359427858894
test_loss: 0.004385425740292172
train_loss: 0.003912249142128088
test_loss: 0.004083528807025743
train_loss: 0.0040866186725016474
test_loss: 0.004373617860411664
train_loss: 0.003841949350122431
test_loss: 0.00424251001251395
train_loss: 0.003882098991746556
test_loss: 0.004020349337704669
train_loss: 0.0037634536266462834
test_loss: 0.004286281820638377
train_loss: 0.0038944537173452682
test_loss: 0.00419611407321176
train_loss: 0.0038142439296209195
test_loss: 0.004234689384678515
train_loss: 0.0041581798218914045
test_loss: 0.004096651371811932
train_loss: 0.0035489419670162023
test_loss: 0.004033773085763521
train_loss: 0.00347009700441613
test_loss: 0.004167973933849452
train_loss: 0.00414440006804822
test_loss: 0.004168428324774069
train_loss: 0.003799079172498525
test_loss: 0.004073463996948725
train_loss: 0.00394781834960267
test_loss: 0.004205769536421256
train_loss: 0.003459778292996634
test_loss: 0.004107737739620376
train_loss: 0.0036183900965908854
test_loss: 0.003929405852849535
train_loss: 0.003630572316854642
test_loss: 0.003938944551036016
train_loss: 0.0036227424563064657
test_loss: 0.00389375990940273
train_loss: 0.0036101295586461906
test_loss: 0.004090087156137918
train_loss: 0.0036886948690246497
test_loss: 0.003970587599312189
train_loss: 0.003672193421348704
test_loss: 0.004033178577693929
train_loss: 0.0034347372764605825
test_loss: 0.004036584606187247
train_loss: 0.003399789479050987
test_loss: 0.0038496792851753733
train_loss: 0.003339455255415515
test_loss: 0.0038153393551401157
train_loss: 0.004071080836801841
test_loss: 0.004112988104547603
train_loss: 0.0035803063806164
test_loss: 0.003918377121795146
train_loss: 0.0032868827615740023
test_loss: 0.003858914679987561
train_loss: 0.0036559560157025613
test_loss: 0.0038614731302049027
train_loss: 0.0036600179636063304
test_loss: 0.003960594344041016
train_loss: 0.0033849651507241087
test_loss: 0.0038229828369292693
train_loss: 0.004199707025809615
test_loss: 0.004339475147846252
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi0.8/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi0.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58dcf2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58edcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58edc730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58e14510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58da3a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58da3c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58d66ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58e2a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58cff378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58cff158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58c8e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58c8cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58c8ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58c75ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58c09400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58c18e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58bee488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58be4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58ba0620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58ba0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58b4a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58b4a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58abf7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58aeb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58aeb0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58aaca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58a5b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58a23730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58a23400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58a19378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f58a190d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f508551e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f50826378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f507e7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f5080c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6f507bc598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.5872647635479349e-05
Iter: 2 loss: 2.1025387688110957e-05
Iter: 3 loss: 1.9896717133091816e-05
Iter: 4 loss: 1.7494502340569118e-05
Iter: 5 loss: 1.7307269275337338e-05
Iter: 6 loss: 1.5517567194963777e-05
Iter: 7 loss: 1.3838154025410426e-05
Iter: 8 loss: 1.9222228948238697e-05
Iter: 9 loss: 1.3360186445431906e-05
Iter: 10 loss: 1.209919300570619e-05
Iter: 11 loss: 3.0916265425044262e-05
Iter: 12 loss: 1.2098168371108444e-05
Iter: 13 loss: 1.15968501352088e-05
Iter: 14 loss: 1.1195215851208967e-05
Iter: 15 loss: 1.1044182261577902e-05
Iter: 16 loss: 1.0402571157240044e-05
Iter: 17 loss: 1.5945978043145845e-05
Iter: 18 loss: 1.0368151586291902e-05
Iter: 19 loss: 9.7289711558285075e-06
Iter: 20 loss: 1.0602907502385726e-05
Iter: 21 loss: 9.4099814735155116e-06
Iter: 22 loss: 8.9321523719334929e-06
Iter: 23 loss: 9.1253529528783278e-06
Iter: 24 loss: 8.6026837031875065e-06
Iter: 25 loss: 8.187890503452429e-06
Iter: 26 loss: 8.51312047523127e-06
Iter: 27 loss: 7.9368769220201417e-06
Iter: 28 loss: 7.6784345179895113e-06
Iter: 29 loss: 7.6115936947559078e-06
Iter: 30 loss: 7.36810174813562e-06
Iter: 31 loss: 6.8366938384115742e-06
Iter: 32 loss: 1.4802013145463416e-05
Iter: 33 loss: 6.8144839888291289e-06
Iter: 34 loss: 6.3803341872974366e-06
Iter: 35 loss: 7.85880959661292e-06
Iter: 36 loss: 6.263914372708352e-06
Iter: 37 loss: 5.96288598433093e-06
Iter: 38 loss: 1.0147159984715122e-05
Iter: 39 loss: 5.96196239549297e-06
Iter: 40 loss: 5.7328335097774254e-06
Iter: 41 loss: 7.9168488767576235e-06
Iter: 42 loss: 5.7239471640019075e-06
Iter: 43 loss: 5.62246088028134e-06
Iter: 44 loss: 5.4199908818539e-06
Iter: 45 loss: 9.2934048283627749e-06
Iter: 46 loss: 5.417573889497842e-06
Iter: 47 loss: 5.3359608706875535e-06
Iter: 48 loss: 5.2928833682206737e-06
Iter: 49 loss: 5.2080542656875415e-06
Iter: 50 loss: 5.0332940240183663e-06
Iter: 51 loss: 8.1084581912082083e-06
Iter: 52 loss: 5.0297632769211982e-06
Iter: 53 loss: 4.8917213543278368e-06
Iter: 54 loss: 6.5890825039480153e-06
Iter: 55 loss: 4.8902543680151941e-06
Iter: 56 loss: 4.7839799099851113e-06
Iter: 57 loss: 5.4518061673377316e-06
Iter: 58 loss: 4.771775868653867e-06
Iter: 59 loss: 4.7065205356396558e-06
Iter: 60 loss: 4.6545001086428743e-06
Iter: 61 loss: 4.6346843006094022e-06
Iter: 62 loss: 4.5354224085289329e-06
Iter: 63 loss: 4.67935548690479e-06
Iter: 64 loss: 4.48718872215381e-06
Iter: 65 loss: 4.4078393973608107e-06
Iter: 66 loss: 5.1793756503487729e-06
Iter: 67 loss: 4.4049924984837116e-06
Iter: 68 loss: 4.3293525628430189e-06
Iter: 69 loss: 4.7125608173718628e-06
Iter: 70 loss: 4.3168909040159269e-06
Iter: 71 loss: 4.2672512896908515e-06
Iter: 72 loss: 4.2061224827962739e-06
Iter: 73 loss: 4.2007040834054382e-06
Iter: 74 loss: 4.1245564940177828e-06
Iter: 75 loss: 4.1720605899510845e-06
Iter: 76 loss: 4.0758831637706113e-06
Iter: 77 loss: 4.0073327078833494e-06
Iter: 78 loss: 4.780147315412965e-06
Iter: 79 loss: 4.00608142429796e-06
Iter: 80 loss: 3.9404565917236715e-06
Iter: 81 loss: 4.5171615665998927e-06
Iter: 82 loss: 3.9371178693877051e-06
Iter: 83 loss: 3.9088943890234804e-06
Iter: 84 loss: 3.8726830650071033e-06
Iter: 85 loss: 3.8701087109235557e-06
Iter: 86 loss: 3.8393208900214841e-06
Iter: 87 loss: 3.8387354803614842e-06
Iter: 88 loss: 3.808504010630555e-06
Iter: 89 loss: 3.7849383488099237e-06
Iter: 90 loss: 3.775465567057e-06
Iter: 91 loss: 3.7416391343766423e-06
Iter: 92 loss: 3.7901043863868128e-06
Iter: 93 loss: 3.7251235849897048e-06
Iter: 94 loss: 3.6916698228128515e-06
Iter: 95 loss: 3.6916426903472623e-06
Iter: 96 loss: 3.6720291184627507e-06
Iter: 97 loss: 3.6295174440542693e-06
Iter: 98 loss: 4.2805079089183e-06
Iter: 99 loss: 3.627874022543212e-06
Iter: 100 loss: 3.5928199009970919e-06
Iter: 101 loss: 3.8060738226350113e-06
Iter: 102 loss: 3.5885493000549451e-06
Iter: 103 loss: 3.5582695460353803e-06
Iter: 104 loss: 3.7678899344223011e-06
Iter: 105 loss: 3.5554120795444848e-06
Iter: 106 loss: 3.5267090371191158e-06
Iter: 107 loss: 3.6466702076779747e-06
Iter: 108 loss: 3.5205760293950722e-06
Iter: 109 loss: 3.5004338712590241e-06
Iter: 110 loss: 3.475919276103829e-06
Iter: 111 loss: 3.4736158758552828e-06
Iter: 112 loss: 3.4424511513941257e-06
Iter: 113 loss: 3.5561446179777794e-06
Iter: 114 loss: 3.4346764058320034e-06
Iter: 115 loss: 3.4163675569442906e-06
Iter: 116 loss: 3.4162178289135639e-06
Iter: 117 loss: 3.3975345089115552e-06
Iter: 118 loss: 3.4191791250008947e-06
Iter: 119 loss: 3.3875626521463919e-06
Iter: 120 loss: 3.3750774623762498e-06
Iter: 121 loss: 3.35643940387198e-06
Iter: 122 loss: 3.3560300985508974e-06
Iter: 123 loss: 3.340823016802625e-06
Iter: 124 loss: 3.3384855690220474e-06
Iter: 125 loss: 3.3299642531625e-06
Iter: 126 loss: 3.3116240689960614e-06
Iter: 127 loss: 3.5977741697593535e-06
Iter: 128 loss: 3.3109691349971439e-06
Iter: 129 loss: 3.2946592409835064e-06
Iter: 130 loss: 3.5101550110913545e-06
Iter: 131 loss: 3.2945672237179476e-06
Iter: 132 loss: 3.2776597352791568e-06
Iter: 133 loss: 3.2928287742810426e-06
Iter: 134 loss: 3.2678162203443234e-06
Iter: 135 loss: 3.2541097233191977e-06
Iter: 136 loss: 3.2460093477815277e-06
Iter: 137 loss: 3.2402758084330575e-06
Iter: 138 loss: 3.2213532196172609e-06
Iter: 139 loss: 3.2619006635557921e-06
Iter: 140 loss: 3.2139583395653727e-06
Iter: 141 loss: 3.2083293903090353e-06
Iter: 142 loss: 3.2040714432181671e-06
Iter: 143 loss: 3.196472682121274e-06
Iter: 144 loss: 3.1835144059990179e-06
Iter: 145 loss: 3.1834968326653618e-06
Iter: 146 loss: 3.1695634521319348e-06
Iter: 147 loss: 3.2223401062957247e-06
Iter: 148 loss: 3.166225627297739e-06
Iter: 149 loss: 3.1530808301276742e-06
Iter: 150 loss: 3.16478224464471e-06
Iter: 151 loss: 3.1454096150327237e-06
Iter: 152 loss: 3.132263391169412e-06
Iter: 153 loss: 3.13194948623111e-06
Iter: 154 loss: 3.1267923957003043e-06
Iter: 155 loss: 3.1140858190641508e-06
Iter: 156 loss: 3.240699691915898e-06
Iter: 157 loss: 3.1124857311597761e-06
Iter: 158 loss: 3.10475761350472e-06
Iter: 159 loss: 3.1035520774423972e-06
Iter: 160 loss: 3.0951533009605214e-06
Iter: 161 loss: 3.0866213968078114e-06
Iter: 162 loss: 3.0849669670415404e-06
Iter: 163 loss: 3.0739940119720822e-06
Iter: 164 loss: 3.0962461401926379e-06
Iter: 165 loss: 3.0695513888467175e-06
Iter: 166 loss: 3.0622447128428453e-06
Iter: 167 loss: 3.0618628646781848e-06
Iter: 168 loss: 3.0570715771947825e-06
Iter: 169 loss: 3.0450653109484736e-06
Iter: 170 loss: 3.1568406460402767e-06
Iter: 171 loss: 3.0433441515212386e-06
Iter: 172 loss: 3.0323629727653556e-06
Iter: 173 loss: 3.0846136535719348e-06
Iter: 174 loss: 3.03038185693577e-06
Iter: 175 loss: 3.0212114204514735e-06
Iter: 176 loss: 3.0816298355886737e-06
Iter: 177 loss: 3.020252560666714e-06
Iter: 178 loss: 3.0103711150299023e-06
Iter: 179 loss: 3.0610102550993091e-06
Iter: 180 loss: 3.008770274158611e-06
Iter: 181 loss: 3.0026084751638824e-06
Iter: 182 loss: 2.9957431567491179e-06
Iter: 183 loss: 2.9947909253834142e-06
Iter: 184 loss: 2.9849265876680904e-06
Iter: 185 loss: 3.0209151060175731e-06
Iter: 186 loss: 2.9824674386145208e-06
Iter: 187 loss: 2.9743788741211173e-06
Iter: 188 loss: 2.9743522176538976e-06
Iter: 189 loss: 2.9696232222187523e-06
Iter: 190 loss: 2.9675668156582193e-06
Iter: 191 loss: 2.9651444071524698e-06
Iter: 192 loss: 2.9593317237726869e-06
Iter: 193 loss: 2.9648411705534336e-06
Iter: 194 loss: 2.9560045234184382e-06
Iter: 195 loss: 2.948659406760983e-06
Iter: 196 loss: 3.0363927285699779e-06
Iter: 197 loss: 2.948564784065951e-06
Iter: 198 loss: 2.9451695442453561e-06
Iter: 199 loss: 2.9373469742002803e-06
Iter: 200 loss: 3.0364631386752923e-06
Iter: 201 loss: 2.9367858527867758e-06
Iter: 202 loss: 2.9338805274822478e-06
Iter: 203 loss: 2.9322269991194116e-06
Iter: 204 loss: 2.9281450855453367e-06
Iter: 205 loss: 2.9201491362415566e-06
Iter: 206 loss: 3.0801507002386249e-06
Iter: 207 loss: 2.9200827472870633e-06
Iter: 208 loss: 2.9124802358364408e-06
Iter: 209 loss: 2.9362710668155911e-06
Iter: 210 loss: 2.9102719540727843e-06
Iter: 211 loss: 2.90368694667045e-06
Iter: 212 loss: 2.90922329638499e-06
Iter: 213 loss: 2.8997790828940254e-06
Iter: 214 loss: 2.8964742491956309e-06
Iter: 215 loss: 2.8949929171547663e-06
Iter: 216 loss: 2.892102660435295e-06
Iter: 217 loss: 2.8857990416597169e-06
Iter: 218 loss: 2.9804555552979374e-06
Iter: 219 loss: 2.8855374794488875e-06
Iter: 220 loss: 2.879093713594959e-06
Iter: 221 loss: 2.8976853352956684e-06
Iter: 222 loss: 2.8770814560477007e-06
Iter: 223 loss: 2.8747655029907943e-06
Iter: 224 loss: 2.8736204720918315e-06
Iter: 225 loss: 2.8710706918449061e-06
Iter: 226 loss: 2.8665604473687668e-06
Iter: 227 loss: 2.866560354295245e-06
Iter: 228 loss: 2.8616646179494764e-06
Iter: 229 loss: 2.8800274909915748e-06
Iter: 230 loss: 2.8604802714411861e-06
Iter: 231 loss: 2.8568799256615827e-06
Iter: 232 loss: 2.9128885808947575e-06
Iter: 233 loss: 2.8568794780257164e-06
Iter: 234 loss: 2.8541503088323855e-06
Iter: 235 loss: 2.8481755738884113e-06
Iter: 236 loss: 2.9369188960003987e-06
Iter: 237 loss: 2.84791727779e-06
Iter: 238 loss: 2.8432310884332737e-06
Iter: 239 loss: 2.9110848664153e-06
Iter: 240 loss: 2.8432237712652374e-06
Iter: 241 loss: 2.8396316009436863e-06
Iter: 242 loss: 2.8629261968037016e-06
Iter: 243 loss: 2.8392439584172436e-06
Iter: 244 loss: 2.8369066272992562e-06
Iter: 245 loss: 2.8311289704312445e-06
Iter: 246 loss: 2.8879665113004404e-06
Iter: 247 loss: 2.8303828375792756e-06
Iter: 248 loss: 2.8236324241052712e-06
Iter: 249 loss: 2.8501298650070654e-06
Iter: 250 loss: 2.8220800003695502e-06
Iter: 251 loss: 2.8194619605291994e-06
Iter: 252 loss: 2.8190064711824008e-06
Iter: 253 loss: 2.8158623120308304e-06
Iter: 254 loss: 2.8148893832017629e-06
Iter: 255 loss: 2.8130295802049694e-06
Iter: 256 loss: 2.8091434352042514e-06
Iter: 257 loss: 2.8090665316634671e-06
Iter: 258 loss: 2.8060112556206466e-06
Iter: 259 loss: 2.8028436378264736e-06
Iter: 260 loss: 2.8510518205260395e-06
Iter: 261 loss: 2.8028423795042245e-06
Iter: 262 loss: 2.799642489955275e-06
Iter: 263 loss: 2.8106561994893213e-06
Iter: 264 loss: 2.7987947147037652e-06
Iter: 265 loss: 2.7966670769104e-06
Iter: 266 loss: 2.7921806913316478e-06
Iter: 267 loss: 2.8663323501513405e-06
Iter: 268 loss: 2.7920560274061237e-06
Iter: 269 loss: 2.791469394560012e-06
Iter: 270 loss: 2.7897774529650466e-06
Iter: 271 loss: 2.7879405842663129e-06
Iter: 272 loss: 2.7844222334004692e-06
Iter: 273 loss: 2.8587488281625837e-06
Iter: 274 loss: 2.7844053448892231e-06
Iter: 275 loss: 2.7805041541433073e-06
Iter: 276 loss: 2.7979447075272526e-06
Iter: 277 loss: 2.7797389548547743e-06
Iter: 278 loss: 2.777250732854232e-06
Iter: 279 loss: 2.8142621943222488e-06
Iter: 280 loss: 2.7772486463231574e-06
Iter: 281 loss: 2.7751544265104582e-06
Iter: 282 loss: 2.77200970836029e-06
Iter: 283 loss: 2.7719449612279722e-06
Iter: 284 loss: 2.7689098781535094e-06
Iter: 285 loss: 2.7685419915358511e-06
Iter: 286 loss: 2.7663678619218544e-06
Iter: 287 loss: 2.7626107397942837e-06
Iter: 288 loss: 2.8007244105563875e-06
Iter: 289 loss: 2.7624968346241874e-06
Iter: 290 loss: 2.7605148524083518e-06
Iter: 291 loss: 2.7604954715605404e-06
Iter: 292 loss: 2.7588093879853662e-06
Iter: 293 loss: 2.7548586098256561e-06
Iter: 294 loss: 2.8021527441865092e-06
Iter: 295 loss: 2.7545292451745406e-06
Iter: 296 loss: 2.7508138631570844e-06
Iter: 297 loss: 2.76391781136717e-06
Iter: 298 loss: 2.74985395057076e-06
Iter: 299 loss: 2.7485966388554263e-06
Iter: 300 loss: 2.7478738435151918e-06
Iter: 301 loss: 2.7465238345822578e-06
Iter: 302 loss: 2.7432511832440464e-06
Iter: 303 loss: 2.7779465481114526e-06
Iter: 304 loss: 2.7428887696847015e-06
Iter: 305 loss: 2.7402409031018208e-06
Iter: 306 loss: 2.766391963861106e-06
Iter: 307 loss: 2.7401514511149278e-06
Iter: 308 loss: 2.7376292189983441e-06
Iter: 309 loss: 2.7521119471495264e-06
Iter: 310 loss: 2.7372886456760037e-06
Iter: 311 loss: 2.7355138256428505e-06
Iter: 312 loss: 2.7326503081074146e-06
Iter: 313 loss: 2.7326299670628734e-06
Iter: 314 loss: 2.7305007848884789e-06
Iter: 315 loss: 2.7304821345134605e-06
Iter: 316 loss: 2.7282413880316614e-06
Iter: 317 loss: 2.7277172418360824e-06
Iter: 318 loss: 2.7262813675467864e-06
Iter: 319 loss: 2.7238648975099908e-06
Iter: 320 loss: 2.7293280445818154e-06
Iter: 321 loss: 2.7229541610195017e-06
Iter: 322 loss: 2.7205605344302466e-06
Iter: 323 loss: 2.7192216384655904e-06
Iter: 324 loss: 2.7181758775411072e-06
Iter: 325 loss: 2.7159970876775612e-06
Iter: 326 loss: 2.7159839992590426e-06
Iter: 327 loss: 2.7136955665823125e-06
Iter: 328 loss: 2.7197504812116515e-06
Iter: 329 loss: 2.7129278956322211e-06
Iter: 330 loss: 2.7113994632011562e-06
Iter: 331 loss: 2.7095308229988529e-06
Iter: 332 loss: 2.7093590601523084e-06
Iter: 333 loss: 2.7067803300532975e-06
Iter: 334 loss: 2.7259147444194462e-06
Iter: 335 loss: 2.7065725276338462e-06
Iter: 336 loss: 2.7044173629975316e-06
Iter: 337 loss: 2.7286903037946523e-06
Iter: 338 loss: 2.7043777478433066e-06
Iter: 339 loss: 2.70331876609946e-06
Iter: 340 loss: 2.7005035359795144e-06
Iter: 341 loss: 2.7205277233534109e-06
Iter: 342 loss: 2.6998837437526495e-06
Iter: 343 loss: 2.6993133956853887e-06
Iter: 344 loss: 2.6984379154715753e-06
Iter: 345 loss: 2.6970389008713215e-06
Iter: 346 loss: 2.6963618728268222e-06
Iter: 347 loss: 2.6956874736342024e-06
Iter: 348 loss: 2.6941054444572462e-06
Iter: 349 loss: 2.6931446738991295e-06
Iter: 350 loss: 2.692497812154067e-06
Iter: 351 loss: 2.6911883119591829e-06
Iter: 352 loss: 2.6909270902358997e-06
Iter: 353 loss: 2.6899456643285555e-06
Iter: 354 loss: 2.6880731140530757e-06
Iter: 355 loss: 2.7279852654133751e-06
Iter: 356 loss: 2.6880650600739367e-06
Iter: 357 loss: 2.6855819481528504e-06
Iter: 358 loss: 2.6867414362134887e-06
Iter: 359 loss: 2.6839054158335843e-06
Iter: 360 loss: 2.6818047047350815e-06
Iter: 361 loss: 2.7082540006603604e-06
Iter: 362 loss: 2.681786309428307e-06
Iter: 363 loss: 2.6805367625844927e-06
Iter: 364 loss: 2.6911982897570736e-06
Iter: 365 loss: 2.6804672550687404e-06
Iter: 366 loss: 2.6790956949731944e-06
Iter: 367 loss: 2.6786351622962649e-06
Iter: 368 loss: 2.6778470831512824e-06
Iter: 369 loss: 2.6761508204385748e-06
Iter: 370 loss: 2.6749570189964554e-06
Iter: 371 loss: 2.6743560090194845e-06
Iter: 372 loss: 2.6735735581282676e-06
Iter: 373 loss: 2.673211182659639e-06
Iter: 374 loss: 2.6719312936433407e-06
Iter: 375 loss: 2.6704497304963161e-06
Iter: 376 loss: 2.6702743508215805e-06
Iter: 377 loss: 2.6686696838850713e-06
Iter: 378 loss: 2.6691709975991508e-06
Iter: 379 loss: 2.6675249591101336e-06
Iter: 380 loss: 2.6665628393738727e-06
Iter: 381 loss: 2.666350536314827e-06
Iter: 382 loss: 2.6653060048529904e-06
Iter: 383 loss: 2.6633062688259815e-06
Iter: 384 loss: 2.7055989405257224e-06
Iter: 385 loss: 2.66329679760317e-06
Iter: 386 loss: 2.6614341987904565e-06
Iter: 387 loss: 2.668290697124034e-06
Iter: 388 loss: 2.6609742138281881e-06
Iter: 389 loss: 2.6596012053788549e-06
Iter: 390 loss: 2.6595915878043111e-06
Iter: 391 loss: 2.6586955089660265e-06
Iter: 392 loss: 2.65650896319597e-06
Iter: 393 loss: 2.6791235148251794e-06
Iter: 394 loss: 2.6562538009738863e-06
Iter: 395 loss: 2.6543244438641108e-06
Iter: 396 loss: 2.663738977195036e-06
Iter: 397 loss: 2.6539885960520206e-06
Iter: 398 loss: 2.652231445088566e-06
Iter: 399 loss: 2.6621068894533788e-06
Iter: 400 loss: 2.6519854413052641e-06
Iter: 401 loss: 2.6507161264811767e-06
Iter: 402 loss: 2.6674230375902876e-06
Iter: 403 loss: 2.6507086806421238e-06
Iter: 404 loss: 2.6496286892131713e-06
Iter: 405 loss: 2.647882186019935e-06
Iter: 406 loss: 2.6478703428472111e-06
Iter: 407 loss: 2.6460284625965059e-06
Iter: 408 loss: 2.6523433530691493e-06
Iter: 409 loss: 2.6455386484969779e-06
Iter: 410 loss: 2.6443737907335259e-06
Iter: 411 loss: 2.6443685023587755e-06
Iter: 412 loss: 2.6431359516372388e-06
Iter: 413 loss: 2.6419515764915925e-06
Iter: 414 loss: 2.6416775535088286e-06
Iter: 415 loss: 2.6403507215783952e-06
Iter: 416 loss: 2.640607233032372e-06
Iter: 417 loss: 2.6393625070105317e-06
Iter: 418 loss: 2.6387026934888085e-06
Iter: 419 loss: 2.6384318807078346e-06
Iter: 420 loss: 2.63762080658049e-06
Iter: 421 loss: 2.6357059998459605e-06
Iter: 422 loss: 2.6580426920609907e-06
Iter: 423 loss: 2.6355357031992637e-06
Iter: 424 loss: 2.6340781205274141e-06
Iter: 425 loss: 2.6462601007180011e-06
Iter: 426 loss: 2.633992002747541e-06
Iter: 427 loss: 2.6326216095707409e-06
Iter: 428 loss: 2.6426622018721938e-06
Iter: 429 loss: 2.6325078557782166e-06
Iter: 430 loss: 2.6316525840924609e-06
Iter: 431 loss: 2.6304184521207961e-06
Iter: 432 loss: 2.6303804846704463e-06
Iter: 433 loss: 2.6287415417543245e-06
Iter: 434 loss: 2.6294547441734615e-06
Iter: 435 loss: 2.6276224558960667e-06
Iter: 436 loss: 2.6261552996183918e-06
Iter: 437 loss: 2.6435409354865562e-06
Iter: 438 loss: 2.6261353687377374e-06
Iter: 439 loss: 2.62450969606335e-06
Iter: 440 loss: 2.629933030359857e-06
Iter: 441 loss: 2.6240654459668029e-06
Iter: 442 loss: 2.6230311423183986e-06
Iter: 443 loss: 2.623199722345394e-06
Iter: 444 loss: 2.6222522243902084e-06
Iter: 445 loss: 2.6209396075071305e-06
Iter: 446 loss: 2.6250815805536908e-06
Iter: 447 loss: 2.6205613035362369e-06
Iter: 448 loss: 2.6197640713106685e-06
Iter: 449 loss: 2.619719178902553e-06
Iter: 450 loss: 2.6191602044405142e-06
Iter: 451 loss: 2.6177481388692933e-06
Iter: 452 loss: 2.6304597796482787e-06
Iter: 453 loss: 2.6175330584028e-06
Iter: 454 loss: 2.6159370999845541e-06
Iter: 455 loss: 2.6244447069684949e-06
Iter: 456 loss: 2.6156936792503379e-06
Iter: 457 loss: 2.614665223588584e-06
Iter: 458 loss: 2.61465019210494e-06
Iter: 459 loss: 2.6139105525769864e-06
Iter: 460 loss: 2.6121355247020316e-06
Iter: 461 loss: 2.6316672214433219e-06
Iter: 462 loss: 2.6119544255910853e-06
Iter: 463 loss: 2.6109609638333686e-06
Iter: 464 loss: 2.6109540985665022e-06
Iter: 465 loss: 2.6097840133145375e-06
Iter: 466 loss: 2.6102759164208031e-06
Iter: 467 loss: 2.608980856041967e-06
Iter: 468 loss: 2.6078154275128458e-06
Iter: 469 loss: 2.6062856724916387e-06
Iter: 470 loss: 2.6061906573288932e-06
Iter: 471 loss: 2.6045531210257575e-06
Iter: 472 loss: 2.6161635867336558e-06
Iter: 473 loss: 2.6044066014535418e-06
Iter: 474 loss: 2.603475462651801e-06
Iter: 475 loss: 2.6034719244565918e-06
Iter: 476 loss: 2.6025787175268478e-06
Iter: 477 loss: 2.6029211254555474e-06
Iter: 478 loss: 2.60195750679516e-06
Iter: 479 loss: 2.60104780248668e-06
Iter: 480 loss: 2.6001245965206713e-06
Iter: 481 loss: 2.599944992938355e-06
Iter: 482 loss: 2.5991658369086086e-06
Iter: 483 loss: 2.5990563728389192e-06
Iter: 484 loss: 2.5981427771913865e-06
Iter: 485 loss: 2.5975727262789418e-06
Iter: 486 loss: 2.5972079798165649e-06
Iter: 487 loss: 2.5959776640929704e-06
Iter: 488 loss: 2.5975797911450133e-06
Iter: 489 loss: 2.5953509994680166e-06
Iter: 490 loss: 2.5944716903781052e-06
Iter: 491 loss: 2.6018918325023821e-06
Iter: 492 loss: 2.5944211672858422e-06
Iter: 493 loss: 2.593477540790458e-06
Iter: 494 loss: 2.596165452821801e-06
Iter: 495 loss: 2.5931796348509162e-06
Iter: 496 loss: 2.5924845929600159e-06
Iter: 497 loss: 2.59098537948716e-06
Iter: 498 loss: 2.6142342226110883e-06
Iter: 499 loss: 2.5909304681632415e-06
Iter: 500 loss: 2.5903174799003215e-06
Iter: 501 loss: 2.590021186767523e-06
Iter: 502 loss: 2.5891987321483169e-06
Iter: 503 loss: 2.5885889601021478e-06
Iter: 504 loss: 2.5883144821812846e-06
Iter: 505 loss: 2.5873234559319203e-06
Iter: 506 loss: 2.5868320773931526e-06
Iter: 507 loss: 2.5863614870553791e-06
Iter: 508 loss: 2.5850022062820607e-06
Iter: 509 loss: 2.5922115662624758e-06
Iter: 510 loss: 2.5847932689429102e-06
Iter: 511 loss: 2.5838661137726026e-06
Iter: 512 loss: 2.5890778124981442e-06
Iter: 513 loss: 2.5837363599233453e-06
Iter: 514 loss: 2.5823748731315525e-06
Iter: 515 loss: 2.58308636733371e-06
Iter: 516 loss: 2.5814734800671266e-06
Iter: 517 loss: 2.5805476746085229e-06
Iter: 518 loss: 2.5822250017803185e-06
Iter: 519 loss: 2.5801468186419242e-06
Iter: 520 loss: 2.5794180078251939e-06
Iter: 521 loss: 2.5867652975511222e-06
Iter: 522 loss: 2.5793953457592937e-06
Iter: 523 loss: 2.5785110995734376e-06
Iter: 524 loss: 2.5784265127322945e-06
Iter: 525 loss: 2.5777776601291234e-06
Iter: 526 loss: 2.5769729519190078e-06
Iter: 527 loss: 2.5773913615252553e-06
Iter: 528 loss: 2.5764396935594359e-06
Iter: 529 loss: 2.5757129371374569e-06
Iter: 530 loss: 2.5851463192414561e-06
Iter: 531 loss: 2.5757080563295849e-06
Iter: 532 loss: 2.5749346115028195e-06
Iter: 533 loss: 2.5751964539061178e-06
Iter: 534 loss: 2.5743880079206466e-06
Iter: 535 loss: 2.5733609967570105e-06
Iter: 536 loss: 2.5730860837909081e-06
Iter: 537 loss: 2.5724507580106254e-06
Iter: 538 loss: 2.5716971198231717e-06
Iter: 539 loss: 2.5832589961006218e-06
Iter: 540 loss: 2.5716969090182358e-06
Iter: 541 loss: 2.5708837847073147e-06
Iter: 542 loss: 2.5713333267484906e-06
Iter: 543 loss: 2.5703510710110488e-06
Iter: 544 loss: 2.5695636240093886e-06
Iter: 545 loss: 2.5682139591859226e-06
Iter: 546 loss: 2.5682125403302416e-06
Iter: 547 loss: 2.5668805528840412e-06
Iter: 548 loss: 2.5742054932496115e-06
Iter: 549 loss: 2.5666872641254569e-06
Iter: 550 loss: 2.5658582170416209e-06
Iter: 551 loss: 2.565850151432891e-06
Iter: 552 loss: 2.5650642577978932e-06
Iter: 553 loss: 2.5662393671776155e-06
Iter: 554 loss: 2.5646881393174456e-06
Iter: 555 loss: 2.563968223803304e-06
Iter: 556 loss: 2.5628342476904459e-06
Iter: 557 loss: 2.562821908248304e-06
Iter: 558 loss: 2.5626155508672136e-06
Iter: 559 loss: 2.5622589270950276e-06
Iter: 560 loss: 2.5616414297235919e-06
Iter: 561 loss: 2.5603259833052895e-06
Iter: 562 loss: 2.5814551663813859e-06
Iter: 563 loss: 2.5602844257334132e-06
Iter: 564 loss: 2.5592421414873873e-06
Iter: 565 loss: 2.563472868706666e-06
Iter: 566 loss: 2.5590116193424015e-06
Iter: 567 loss: 2.558386881328626e-06
Iter: 568 loss: 2.5583761714706454e-06
Iter: 569 loss: 2.5577959004333491e-06
Iter: 570 loss: 2.5572292540380232e-06
Iter: 571 loss: 2.5571044987663607e-06
Iter: 572 loss: 2.5562389738587906e-06
Iter: 573 loss: 2.5570804524630647e-06
Iter: 574 loss: 2.5557475439694624e-06
Iter: 575 loss: 2.5550388219652883e-06
Iter: 576 loss: 2.55503841745722e-06
Iter: 577 loss: 2.5543822223718785e-06
Iter: 578 loss: 2.5545025322172243e-06
Iter: 579 loss: 2.5538917224771824e-06
Iter: 580 loss: 2.5532223554579982e-06
Iter: 581 loss: 2.5528419445174304e-06
Iter: 582 loss: 2.5525530233215603e-06
Iter: 583 loss: 2.5514466508796986e-06
Iter: 584 loss: 2.5517495012008721e-06
Iter: 585 loss: 2.5506463525079388e-06
Iter: 586 loss: 2.5497931810084206e-06
Iter: 587 loss: 2.549786210855025e-06
Iter: 588 loss: 2.5487620935345342e-06
Iter: 589 loss: 2.549994951359248e-06
Iter: 590 loss: 2.5482235232933312e-06
Iter: 591 loss: 2.5474447140888044e-06
Iter: 592 loss: 2.5476433258338865e-06
Iter: 593 loss: 2.5468775213397875e-06
Iter: 594 loss: 2.5460722667269462e-06
Iter: 595 loss: 2.5545719131415165e-06
Iter: 596 loss: 2.5460517672900962e-06
Iter: 597 loss: 2.5451485727031966e-06
Iter: 598 loss: 2.5461512437120859e-06
Iter: 599 loss: 2.5446587124237025e-06
Iter: 600 loss: 2.5439651014879723e-06
Iter: 601 loss: 2.5427268451931918e-06
Iter: 602 loss: 2.5733890687686175e-06
Iter: 603 loss: 2.5427268271914737e-06
Iter: 604 loss: 2.5423461165358917e-06
Iter: 605 loss: 2.5420564231395222e-06
Iter: 606 loss: 2.5413754416308884e-06
Iter: 607 loss: 2.5407917809851655e-06
Iter: 608 loss: 2.5406058096863712e-06
Iter: 609 loss: 2.5399106348596032e-06
Iter: 610 loss: 2.541236238484778e-06
Iter: 611 loss: 2.539618451593247e-06
Iter: 612 loss: 2.5389196764241549e-06
Iter: 613 loss: 2.5455171902644533e-06
Iter: 614 loss: 2.5388917273181876e-06
Iter: 615 loss: 2.5382038911541587e-06
Iter: 616 loss: 2.5385180232832344e-06
Iter: 617 loss: 2.5377377715351116e-06
Iter: 618 loss: 2.5370317898732774e-06
Iter: 619 loss: 2.5368345156608364e-06
Iter: 620 loss: 2.5364032185452813e-06
Iter: 621 loss: 2.5353725031882194e-06
Iter: 622 loss: 2.5365003962676149e-06
Iter: 623 loss: 2.5348105491888218e-06
Iter: 624 loss: 2.5338253798960991e-06
Iter: 625 loss: 2.5446706858980136e-06
Iter: 626 loss: 2.5338049386906386e-06
Iter: 627 loss: 2.5330098625474261e-06
Iter: 628 loss: 2.5399165457005121e-06
Iter: 629 loss: 2.5329679654250283e-06
Iter: 630 loss: 2.5324349545336435e-06
Iter: 631 loss: 2.5310472679701136e-06
Iter: 632 loss: 2.5419734527820073e-06
Iter: 633 loss: 2.5307837934378385e-06
Iter: 634 loss: 2.5308471961994221e-06
Iter: 635 loss: 2.5301992881439718e-06
Iter: 636 loss: 2.5296023971400506e-06
Iter: 637 loss: 2.5290779338747275e-06
Iter: 638 loss: 2.5289214061264146e-06
Iter: 639 loss: 2.5283118596770461e-06
Iter: 640 loss: 2.528341371803995e-06
Iter: 641 loss: 2.5278329631560277e-06
Iter: 642 loss: 2.5272555717216486e-06
Iter: 643 loss: 2.5272499615136094e-06
Iter: 644 loss: 2.5266095744666231e-06
Iter: 645 loss: 2.5259549984867645e-06
Iter: 646 loss: 2.5258306609082585e-06
Iter: 647 loss: 2.5250057973329129e-06
Iter: 648 loss: 2.525301970689971e-06
Iter: 649 loss: 2.5244271213481445e-06
Iter: 650 loss: 2.5236790564674037e-06
Iter: 651 loss: 2.5236646781840641e-06
Iter: 652 loss: 2.5229330978952657e-06
Iter: 653 loss: 2.522026756170909e-06
Iter: 654 loss: 2.5219488691430191e-06
Iter: 655 loss: 2.5211106050266126e-06
Iter: 656 loss: 2.5233999081404973e-06
Iter: 657 loss: 2.520836446204472e-06
Iter: 658 loss: 2.5198648466131968e-06
Iter: 659 loss: 2.5208712960813939e-06
Iter: 660 loss: 2.5193248744662353e-06
Iter: 661 loss: 2.5185507901670352e-06
Iter: 662 loss: 2.5265749596086677e-06
Iter: 663 loss: 2.5185294063724076e-06
Iter: 664 loss: 2.5177647865896838e-06
Iter: 665 loss: 2.5213129193440159e-06
Iter: 666 loss: 2.5176220423356719e-06
Iter: 667 loss: 2.5171209224411723e-06
Iter: 668 loss: 2.5162529628608957e-06
Iter: 669 loss: 2.5162524950514082e-06
Iter: 670 loss: 2.5156891730682644e-06
Iter: 671 loss: 2.5156296008256025e-06
Iter: 672 loss: 2.5150335178567027e-06
Iter: 673 loss: 2.5152706951316022e-06
Iter: 674 loss: 2.5146210883431871e-06
Iter: 675 loss: 2.5140360518895705e-06
Iter: 676 loss: 2.51260849861809e-06
Iter: 677 loss: 2.5273722570321375e-06
Iter: 678 loss: 2.5124419090953151e-06
Iter: 679 loss: 2.5131714412619424e-06
Iter: 680 loss: 2.5119001605648797e-06
Iter: 681 loss: 2.5113981274669547e-06
Iter: 682 loss: 2.5105462362950842e-06
Iter: 683 loss: 2.5105447919410285e-06
Iter: 684 loss: 2.5096720043308468e-06
Iter: 685 loss: 2.50994274916984e-06
Iter: 686 loss: 2.5090488991637455e-06
Iter: 687 loss: 2.5085469195030972e-06
Iter: 688 loss: 2.5083854714372764e-06
Iter: 689 loss: 2.5079355078769903e-06
Iter: 690 loss: 2.5073708336732183e-06
Iter: 691 loss: 2.5073254601528434e-06
Iter: 692 loss: 2.5065590264090992e-06
Iter: 693 loss: 2.5063587783350791e-06
Iter: 694 loss: 2.5058814632372745e-06
Iter: 695 loss: 2.5049344353158675e-06
Iter: 696 loss: 2.5108837867130165e-06
Iter: 697 loss: 2.504825635357711e-06
Iter: 698 loss: 2.5041304375915327e-06
Iter: 699 loss: 2.51364009638977e-06
Iter: 700 loss: 2.5041277919597375e-06
Iter: 701 loss: 2.5035635519094018e-06
Iter: 702 loss: 2.5039858334717277e-06
Iter: 703 loss: 2.5032180616733851e-06
Iter: 704 loss: 2.5025719348598642e-06
Iter: 705 loss: 2.5017467048560866e-06
Iter: 706 loss: 2.50168649010465e-06
Iter: 707 loss: 2.501597386244845e-06
Iter: 708 loss: 2.5011538633779917e-06
Iter: 709 loss: 2.5008093549966419e-06
Iter: 710 loss: 2.4998619603061898e-06
Iter: 711 loss: 2.5054600761613454e-06
Iter: 712 loss: 2.49959819889057e-06
Iter: 713 loss: 2.4985730559857074e-06
Iter: 714 loss: 2.5038890222985334e-06
Iter: 715 loss: 2.4984098706273551e-06
Iter: 716 loss: 2.4979202769872259e-06
Iter: 717 loss: 2.497859409810181e-06
Iter: 718 loss: 2.4974038596003055e-06
Iter: 719 loss: 2.4966147887621414e-06
Iter: 720 loss: 2.4966143640044863e-06
Iter: 721 loss: 2.4958279128783862e-06
Iter: 722 loss: 2.496253885222837e-06
Iter: 723 loss: 2.4953106877985953e-06
Iter: 724 loss: 2.4947381984474264e-06
Iter: 725 loss: 2.4946361859999809e-06
Iter: 726 loss: 2.4942396465447157e-06
Iter: 727 loss: 2.4933295544735005e-06
Iter: 728 loss: 2.5050032828815521e-06
Iter: 729 loss: 2.4932665292195818e-06
Iter: 730 loss: 2.4922482031820885e-06
Iter: 731 loss: 2.4938274180613983e-06
Iter: 732 loss: 2.4917693352179122e-06
Iter: 733 loss: 2.4907726589254484e-06
Iter: 734 loss: 2.5017361174835306e-06
Iter: 735 loss: 2.4907519138014786e-06
Iter: 736 loss: 2.4899820462809784e-06
Iter: 737 loss: 2.4968513673768611e-06
Iter: 738 loss: 2.489944749094025e-06
Iter: 739 loss: 2.4894863378298644e-06
Iter: 740 loss: 2.4889077090668808e-06
Iter: 741 loss: 2.4888626405398709e-06
Iter: 742 loss: 2.4882292547429764e-06
Iter: 743 loss: 2.4961666560622236e-06
Iter: 744 loss: 2.4882234923860859e-06
Iter: 745 loss: 2.4875807616141638e-06
Iter: 746 loss: 2.4882918292507072e-06
Iter: 747 loss: 2.4872317894281181e-06
Iter: 748 loss: 2.4865616605610182e-06
Iter: 749 loss: 2.4858764440327703e-06
Iter: 750 loss: 2.4857464085210104e-06
Iter: 751 loss: 2.4848164867816592e-06
Iter: 752 loss: 2.4874230695842704e-06
Iter: 753 loss: 2.4845189569346079e-06
Iter: 754 loss: 2.4837985067721105e-06
Iter: 755 loss: 2.4837659949887085e-06
Iter: 756 loss: 2.4834111653435841e-06
Iter: 757 loss: 2.4825176925941383e-06
Iter: 758 loss: 2.4906710220983192e-06
Iter: 759 loss: 2.4823848527124791e-06
Iter: 760 loss: 2.4814396776308103e-06
Iter: 761 loss: 2.4886539073389218e-06
Iter: 762 loss: 2.4813685413592569e-06
Iter: 763 loss: 2.4803645828940044e-06
Iter: 764 loss: 2.4861488933046189e-06
Iter: 765 loss: 2.4802298471733584e-06
Iter: 766 loss: 2.4798058708830575e-06
Iter: 767 loss: 2.4789629793563035e-06
Iter: 768 loss: 2.4952227180365638e-06
Iter: 769 loss: 2.4789535462012467e-06
Iter: 770 loss: 2.4779441477101573e-06
Iter: 771 loss: 2.4820427620916592e-06
Iter: 772 loss: 2.4777209767310914e-06
Iter: 773 loss: 2.4770253805197694e-06
Iter: 774 loss: 2.4770253727038546e-06
Iter: 775 loss: 2.4762997651810112e-06
Iter: 776 loss: 2.4763467216259111e-06
Iter: 777 loss: 2.4757331186174234e-06
Iter: 778 loss: 2.4750410130165511e-06
Iter: 779 loss: 2.4752887610465038e-06
Iter: 780 loss: 2.4745553164362746e-06
Iter: 781 loss: 2.473764537148896e-06
Iter: 782 loss: 2.4737596149292762e-06
Iter: 783 loss: 2.4733488882982051e-06
Iter: 784 loss: 2.4730196750819976e-06
Iter: 785 loss: 2.4728957441955014e-06
Iter: 786 loss: 2.4722387592339932e-06
Iter: 787 loss: 2.4715788739854188e-06
Iter: 788 loss: 2.4714460986446216e-06
Iter: 789 loss: 2.4709077473448548e-06
Iter: 790 loss: 2.4708130367713107e-06
Iter: 791 loss: 2.470194122184658e-06
Iter: 792 loss: 2.4701060820276007e-06
Iter: 793 loss: 2.46967155054637e-06
Iter: 794 loss: 2.4689452169022374e-06
Iter: 795 loss: 2.4687321960539849e-06
Iter: 796 loss: 2.4682949791984186e-06
Iter: 797 loss: 2.4676724171222611e-06
Iter: 798 loss: 2.4676707230261241e-06
Iter: 799 loss: 2.4669837777481124e-06
Iter: 800 loss: 2.4669038960968948e-06
Iter: 801 loss: 2.4664094798567827e-06
Iter: 802 loss: 2.4657563097846409e-06
Iter: 803 loss: 2.4648723406046773e-06
Iter: 804 loss: 2.4648273209817786e-06
Iter: 805 loss: 2.4636593148163714e-06
Iter: 806 loss: 2.4708933827297039e-06
Iter: 807 loss: 2.4635215332739265e-06
Iter: 808 loss: 2.4627108159673359e-06
Iter: 809 loss: 2.4626982318737147e-06
Iter: 810 loss: 2.4622755854631144e-06
Iter: 811 loss: 2.4616422907525411e-06
Iter: 812 loss: 2.461628936558964e-06
Iter: 813 loss: 2.4610383055374926e-06
Iter: 814 loss: 2.469970582728322e-06
Iter: 815 loss: 2.4610380086296721e-06
Iter: 816 loss: 2.4604206873832904e-06
Iter: 817 loss: 2.4602768871473453e-06
Iter: 818 loss: 2.459880896720643e-06
Iter: 819 loss: 2.4592910186853417e-06
Iter: 820 loss: 2.4586688546435567e-06
Iter: 821 loss: 2.4585628687642728e-06
Iter: 822 loss: 2.4577649629659731e-06
Iter: 823 loss: 2.4688277233681449e-06
Iter: 824 loss: 2.4577624055200205e-06
Iter: 825 loss: 2.457016940168446e-06
Iter: 826 loss: 2.4602453588975807e-06
Iter: 827 loss: 2.4568646145816553e-06
Iter: 828 loss: 2.4562593955081781e-06
Iter: 829 loss: 2.4559910674202136e-06
Iter: 830 loss: 2.4556843083047479e-06
Iter: 831 loss: 2.4548954307264287e-06
Iter: 832 loss: 2.4546861241821854e-06
Iter: 833 loss: 2.4541968801901969e-06
Iter: 834 loss: 2.4532670930695328e-06
Iter: 835 loss: 2.4532337850806885e-06
Iter: 836 loss: 2.4527528139228003e-06
Iter: 837 loss: 2.4519205519103786e-06
Iter: 838 loss: 2.4519200704103175e-06
Iter: 839 loss: 2.4509826633537856e-06
Iter: 840 loss: 2.4518210350335875e-06
Iter: 841 loss: 2.450436346493377e-06
Iter: 842 loss: 2.44970818018379e-06
Iter: 843 loss: 2.4497081273814333e-06
Iter: 844 loss: 2.4490407702519229e-06
Iter: 845 loss: 2.4524628865378164e-06
Iter: 846 loss: 2.448932730314848e-06
Iter: 847 loss: 2.4484157146171645e-06
Iter: 848 loss: 2.4473213100367137e-06
Iter: 849 loss: 2.4652166277696438e-06
Iter: 850 loss: 2.4472893657072476e-06
Iter: 851 loss: 2.4471692044129122e-06
Iter: 852 loss: 2.4466820174836695e-06
Iter: 853 loss: 2.4463628648725913e-06
Iter: 854 loss: 2.4454705523685856e-06
Iter: 855 loss: 2.450231402938228e-06
Iter: 856 loss: 2.4451923684761894e-06
Iter: 857 loss: 2.4442440684247172e-06
Iter: 858 loss: 2.4514742164041353e-06
Iter: 859 loss: 2.4441724942340147e-06
Iter: 860 loss: 2.4436728909208617e-06
Iter: 861 loss: 2.4436582423112922e-06
Iter: 862 loss: 2.4431961356239455e-06
Iter: 863 loss: 2.4423189707652264e-06
Iter: 864 loss: 2.4612375179779634e-06
Iter: 865 loss: 2.4423157404491539e-06
Iter: 866 loss: 2.441342953794337e-06
Iter: 867 loss: 2.4432756341965136e-06
Iter: 868 loss: 2.440943967755673e-06
Iter: 869 loss: 2.4403095786630883e-06
Iter: 870 loss: 2.4402921068995525e-06
Iter: 871 loss: 2.4397570697126935e-06
Iter: 872 loss: 2.4400123515318291e-06
Iter: 873 loss: 2.4393971591635162e-06
Iter: 874 loss: 2.438770673679481e-06
Iter: 875 loss: 2.4379967655382465e-06
Iter: 876 loss: 2.4379292231141417e-06
Iter: 877 loss: 2.4367939337245961e-06
Iter: 878 loss: 2.4393058072623283e-06
Iter: 879 loss: 2.4363597578431752e-06
Iter: 880 loss: 2.4356241393981275e-06
Iter: 881 loss: 2.4356086322835663e-06
Iter: 882 loss: 2.4347613716006257e-06
Iter: 883 loss: 2.4343505626835212e-06
Iter: 884 loss: 2.4339426116859614e-06
Iter: 885 loss: 2.4331467509019825e-06
Iter: 886 loss: 2.437342464369105e-06
Iter: 887 loss: 2.4330232581998207e-06
Iter: 888 loss: 2.4322504287656963e-06
Iter: 889 loss: 2.4375166838029572e-06
Iter: 890 loss: 2.432175053695582e-06
Iter: 891 loss: 2.4317030325354433e-06
Iter: 892 loss: 2.4307660768224638e-06
Iter: 893 loss: 2.4489161817938195e-06
Iter: 894 loss: 2.4307559006764018e-06
Iter: 895 loss: 2.4297099568414276e-06
Iter: 896 loss: 2.4309109014442937e-06
Iter: 897 loss: 2.4291498545753653e-06
Iter: 898 loss: 2.428446470091569e-06
Iter: 899 loss: 2.4282828843469666e-06
Iter: 900 loss: 2.4278745615937807e-06
Iter: 901 loss: 2.4268617217304337e-06
Iter: 902 loss: 2.4366900921711137e-06
Iter: 903 loss: 2.4267273815976183e-06
Iter: 904 loss: 2.4259030147939113e-06
Iter: 905 loss: 2.4348610321620381e-06
Iter: 906 loss: 2.4258847481723569e-06
Iter: 907 loss: 2.4251645376057917e-06
Iter: 908 loss: 2.4310248354045917e-06
Iter: 909 loss: 2.4251186863865851e-06
Iter: 910 loss: 2.4246908415036068e-06
Iter: 911 loss: 2.4240780109407214e-06
Iter: 912 loss: 2.4240578694886891e-06
Iter: 913 loss: 2.4230769432541442e-06
Iter: 914 loss: 2.4234337384323207e-06
Iter: 915 loss: 2.4223900988887004e-06
Iter: 916 loss: 2.4212880469357982e-06
Iter: 917 loss: 2.4284204243900653e-06
Iter: 918 loss: 2.4211685292440178e-06
Iter: 919 loss: 2.4204965284860526e-06
Iter: 920 loss: 2.4204877208484414e-06
Iter: 921 loss: 2.4200384719594296e-06
Iter: 922 loss: 2.419557624047774e-06
Iter: 923 loss: 2.4194799271471474e-06
Iter: 924 loss: 2.41888492541173e-06
Iter: 925 loss: 2.4249267607763167e-06
Iter: 926 loss: 2.4188669740628948e-06
Iter: 927 loss: 2.4181816925100384e-06
Iter: 928 loss: 2.4173448091042976e-06
Iter: 929 loss: 2.4172674322881082e-06
Iter: 930 loss: 2.4165309293040788e-06
Iter: 931 loss: 2.4157641555693344e-06
Iter: 932 loss: 2.415627407885585e-06
Iter: 933 loss: 2.4155974347252008e-06
Iter: 934 loss: 2.415082255733221e-06
Iter: 935 loss: 2.41460548914536e-06
Iter: 936 loss: 2.4142164435290358e-06
Iter: 937 loss: 2.41407620515601e-06
Iter: 938 loss: 2.4135227328118859e-06
Iter: 939 loss: 2.4133292561795761e-06
Iter: 940 loss: 2.4130161167909213e-06
Iter: 941 loss: 2.4123978611928319e-06
Iter: 942 loss: 2.4123913382023923e-06
Iter: 943 loss: 2.411762864912849e-06
Iter: 944 loss: 2.4111395785241557e-06
Iter: 945 loss: 2.4110088627924807e-06
Iter: 946 loss: 2.4103302251633557e-06
Iter: 947 loss: 2.4103675622229548e-06
Iter: 948 loss: 2.4097984099905926e-06
Iter: 949 loss: 2.408789580081167e-06
Iter: 950 loss: 2.4134132719140832e-06
Iter: 951 loss: 2.4085979525219789e-06
Iter: 952 loss: 2.4079022345650634e-06
Iter: 953 loss: 2.4078980022663161e-06
Iter: 954 loss: 2.4073964205566821e-06
Iter: 955 loss: 2.4072884027043692e-06
Iter: 956 loss: 2.40696085678948e-06
Iter: 957 loss: 2.4063454817067514e-06
Iter: 958 loss: 2.4087569002954845e-06
Iter: 959 loss: 2.406203701673354e-06
Iter: 960 loss: 2.4053701655409355e-06
Iter: 961 loss: 2.4061067040065174e-06
Iter: 962 loss: 2.4048825880096308e-06
Iter: 963 loss: 2.4043149244254523e-06
Iter: 964 loss: 2.4041935991604161e-06
Iter: 965 loss: 2.4038222802318593e-06
Iter: 966 loss: 2.4030235717679516e-06
Iter: 967 loss: 2.4064031383685742e-06
Iter: 968 loss: 2.4028555268068236e-06
Iter: 969 loss: 2.4021243019737383e-06
Iter: 970 loss: 2.4115540617472445e-06
Iter: 971 loss: 2.4021190911443858e-06
Iter: 972 loss: 2.401730535360624e-06
Iter: 973 loss: 2.4007486584536563e-06
Iter: 974 loss: 2.4095754917142914e-06
Iter: 975 loss: 2.4005987396601403e-06
Iter: 976 loss: 2.3998258993963885e-06
Iter: 977 loss: 2.3998258979593636e-06
Iter: 978 loss: 2.3991310974446975e-06
Iter: 979 loss: 2.4035003129828524e-06
Iter: 980 loss: 2.399051453888599e-06
Iter: 981 loss: 2.3986662797433965e-06
Iter: 982 loss: 2.397754749857428e-06
Iter: 983 loss: 2.4082966620260783e-06
Iter: 984 loss: 2.3976719871660859e-06
Iter: 985 loss: 2.3967108830344756e-06
Iter: 986 loss: 2.4004327209656718e-06
Iter: 987 loss: 2.3964863626296057e-06
Iter: 988 loss: 2.395910536096787e-06
Iter: 989 loss: 2.3958530371532113e-06
Iter: 990 loss: 2.3953117210101185e-06
Iter: 991 loss: 2.3948709418567781e-06
Iter: 992 loss: 2.394711250380093e-06
Iter: 993 loss: 2.394036839316987e-06
Iter: 994 loss: 2.3989176768016428e-06
Iter: 995 loss: 2.3939792184336471e-06
Iter: 996 loss: 2.3933103079130882e-06
Iter: 997 loss: 2.3949409714195403e-06
Iter: 998 loss: 2.3930713627928517e-06
Iter: 999 loss: 2.3925762875260848e-06
Iter: 1000 loss: 2.3920374111291115e-06
Iter: 1001 loss: 2.3919556361880631e-06
Iter: 1002 loss: 2.3911238948345853e-06
Iter: 1003 loss: 2.3947237764446656e-06
Iter: 1004 loss: 2.3909537490589784e-06
Iter: 1005 loss: 2.3903991864683442e-06
Iter: 1006 loss: 2.3903949379005754e-06
Iter: 1007 loss: 2.3899182404500429e-06
Iter: 1008 loss: 2.3887772270659572e-06
Iter: 1009 loss: 2.4014503117304021e-06
Iter: 1010 loss: 2.3886632883373596e-06
Iter: 1011 loss: 2.387896854570892e-06
Iter: 1012 loss: 2.3927794963906833e-06
Iter: 1013 loss: 2.3878112018721688e-06
Iter: 1014 loss: 2.38718776718684e-06
Iter: 1015 loss: 2.3963389961874355e-06
Iter: 1016 loss: 2.3871870370525144e-06
Iter: 1017 loss: 2.3867172839390263e-06
Iter: 1018 loss: 2.3857919923158652e-06
Iter: 1019 loss: 2.404051518957304e-06
Iter: 1020 loss: 2.38578335679939e-06
Iter: 1021 loss: 2.3849155300877179e-06
Iter: 1022 loss: 2.3870716012081458e-06
Iter: 1023 loss: 2.3846099067053296e-06
Iter: 1024 loss: 2.3836310597974091e-06
Iter: 1025 loss: 2.3860569947400988e-06
Iter: 1026 loss: 2.3832857785849779e-06
Iter: 1027 loss: 2.3826699977447177e-06
Iter: 1028 loss: 2.3825840863723484e-06
Iter: 1029 loss: 2.3822881561263255e-06
Iter: 1030 loss: 2.3816468912258543e-06
Iter: 1031 loss: 2.3914615921797924e-06
Iter: 1032 loss: 2.381622139896463e-06
Iter: 1033 loss: 2.3808071857621338e-06
Iter: 1034 loss: 2.3905818242725961e-06
Iter: 1035 loss: 2.3807969576929674e-06
Iter: 1036 loss: 2.3803247306681959e-06
Iter: 1037 loss: 2.3794925491263297e-06
Iter: 1038 loss: 2.3794924982848834e-06
Iter: 1039 loss: 2.3786058276669237e-06
Iter: 1040 loss: 2.3818577046277902e-06
Iter: 1041 loss: 2.3783858670295255e-06
Iter: 1042 loss: 2.3777840990235623e-06
Iter: 1043 loss: 2.37777997114438e-06
Iter: 1044 loss: 2.3772096626919213e-06
Iter: 1045 loss: 2.376548535502075e-06
Iter: 1046 loss: 2.3764707970369229e-06
Iter: 1047 loss: 2.3757489657041292e-06
Iter: 1048 loss: 2.3779804265709179e-06
Iter: 1049 loss: 2.3755368839497552e-06
Iter: 1050 loss: 2.374891701331822e-06
Iter: 1051 loss: 2.37685750256564e-06
Iter: 1052 loss: 2.3746996758529258e-06
Iter: 1053 loss: 2.3738057747851238e-06
Iter: 1054 loss: 2.3769915618424459e-06
Iter: 1055 loss: 2.3735773914731889e-06
Iter: 1056 loss: 2.3729967548046291e-06
Iter: 1057 loss: 2.3721956765633717e-06
Iter: 1058 loss: 2.3721600927700368e-06
Iter: 1059 loss: 2.3711314987585228e-06
Iter: 1060 loss: 2.3721063601444173e-06
Iter: 1061 loss: 2.370542802947882e-06
Iter: 1062 loss: 2.3704100023558528e-06
Iter: 1063 loss: 2.3700051579626124e-06
Iter: 1064 loss: 2.3695309798830049e-06
Iter: 1065 loss: 2.3692211601384219e-06
Iter: 1066 loss: 2.3690397828405995e-06
Iter: 1067 loss: 2.36848173385355e-06
Iter: 1068 loss: 2.3711999853463235e-06
Iter: 1069 loss: 2.368384340026776e-06
Iter: 1070 loss: 2.3677378226267891e-06
Iter: 1071 loss: 2.3691574927468379e-06
Iter: 1072 loss: 2.3674892872205657e-06
Iter: 1073 loss: 2.3670071663048644e-06
Iter: 1074 loss: 2.3659619732514823e-06
Iter: 1075 loss: 2.3819387226505743e-06
Iter: 1076 loss: 2.3659214293766084e-06
Iter: 1077 loss: 2.3651680368351007e-06
Iter: 1078 loss: 2.36515741746184e-06
Iter: 1079 loss: 2.3643961466618122e-06
Iter: 1080 loss: 2.3677566091551773e-06
Iter: 1081 loss: 2.3642443235842877e-06
Iter: 1082 loss: 2.36382479950242e-06
Iter: 1083 loss: 2.3630794452588344e-06
Iter: 1084 loss: 2.3630794449708974e-06
Iter: 1085 loss: 2.3622768699116409e-06
Iter: 1086 loss: 2.3686046248294997e-06
Iter: 1087 loss: 2.3622213123196214e-06
Iter: 1088 loss: 2.3615440081722617e-06
Iter: 1089 loss: 2.3670036511789291e-06
Iter: 1090 loss: 2.3614997960672627e-06
Iter: 1091 loss: 2.3609358680165665e-06
Iter: 1092 loss: 2.360754157778236e-06
Iter: 1093 loss: 2.3604251554882427e-06
Iter: 1094 loss: 2.3597616919138942e-06
Iter: 1095 loss: 2.3589321464028619e-06
Iter: 1096 loss: 2.3588642118624416e-06
Iter: 1097 loss: 2.3580336168770553e-06
Iter: 1098 loss: 2.3580318244266504e-06
Iter: 1099 loss: 2.3574384640013627e-06
Iter: 1100 loss: 2.364862376296721e-06
Iter: 1101 loss: 2.3574329941687503e-06
Iter: 1102 loss: 2.3570738956670339e-06
Iter: 1103 loss: 2.3564597069370911e-06
Iter: 1104 loss: 2.3564589854306815e-06
Iter: 1105 loss: 2.3558863569150868e-06
Iter: 1106 loss: 2.3558824662902044e-06
Iter: 1107 loss: 2.35541897351767e-06
Iter: 1108 loss: 2.3543735177109675e-06
Iter: 1109 loss: 2.3685554641503948e-06
Iter: 1110 loss: 2.3543121108664653e-06
Iter: 1111 loss: 2.3534218567479016e-06
Iter: 1112 loss: 2.3557476787134369e-06
Iter: 1113 loss: 2.3531202802890265e-06
Iter: 1114 loss: 2.3526741456676721e-06
Iter: 1115 loss: 2.3525636861687975e-06
Iter: 1116 loss: 2.3521114889496638e-06
Iter: 1117 loss: 2.35154084240882e-06
Iter: 1118 loss: 2.3514963492444173e-06
Iter: 1119 loss: 2.3508673803474864e-06
Iter: 1120 loss: 2.350942797603203e-06
Iter: 1121 loss: 2.3503861596649597e-06
Iter: 1122 loss: 2.3497046372249714e-06
Iter: 1123 loss: 2.3590288323164292e-06
Iter: 1124 loss: 2.34970204847328e-06
Iter: 1125 loss: 2.3489460746640261e-06
Iter: 1126 loss: 2.3493508435530867e-06
Iter: 1127 loss: 2.3484477104340577e-06
Iter: 1128 loss: 2.3478614003120636e-06
Iter: 1129 loss: 2.3479854264420619e-06
Iter: 1130 loss: 2.3474276874524934e-06
Iter: 1131 loss: 2.3467222717310033e-06
Iter: 1132 loss: 2.3501486311562082e-06
Iter: 1133 loss: 2.3465985902417803e-06
Iter: 1134 loss: 2.3460160072932693e-06
Iter: 1135 loss: 2.3497724313366562e-06
Iter: 1136 loss: 2.3459524060610203e-06
Iter: 1137 loss: 2.3452405928291016e-06
Iter: 1138 loss: 2.346044777955458e-06
Iter: 1139 loss: 2.3448571131878659e-06
Iter: 1140 loss: 2.3444425745102988e-06
Iter: 1141 loss: 2.3454260099896803e-06
Iter: 1142 loss: 2.3442915580712404e-06
Iter: 1143 loss: 2.3436033753505767e-06
Iter: 1144 loss: 2.3436612022677969e-06
Iter: 1145 loss: 2.3430697532030824e-06
Iter: 1146 loss: 2.3424025672195496e-06
Iter: 1147 loss: 2.3431438486313727e-06
Iter: 1148 loss: 2.3420408758006908e-06
Iter: 1149 loss: 2.3413761400230194e-06
Iter: 1150 loss: 2.3425582167026446e-06
Iter: 1151 loss: 2.3410852668744582e-06
Iter: 1152 loss: 2.3403597639239396e-06
Iter: 1153 loss: 2.3519227809746642e-06
Iter: 1154 loss: 2.3403597621943297e-06
Iter: 1155 loss: 2.3400006079001438e-06
Iter: 1156 loss: 2.3391178273452427e-06
Iter: 1157 loss: 2.3479959491750447e-06
Iter: 1158 loss: 2.339008732249468e-06
Iter: 1159 loss: 2.3381050659998552e-06
Iter: 1160 loss: 2.342358601655614e-06
Iter: 1161 loss: 2.3379396382677616e-06
Iter: 1162 loss: 2.3373368633611524e-06
Iter: 1163 loss: 2.3373097248975658e-06
Iter: 1164 loss: 2.3369141526330721e-06
Iter: 1165 loss: 2.3360360851159288e-06
Iter: 1166 loss: 2.3485438409761973e-06
Iter: 1167 loss: 2.3359921308954106e-06
Iter: 1168 loss: 2.3350978138841782e-06
Iter: 1169 loss: 2.3381641785932807e-06
Iter: 1170 loss: 2.3348598792947989e-06
Iter: 1171 loss: 2.334180450612396e-06
Iter: 1172 loss: 2.3414313891786363e-06
Iter: 1173 loss: 2.3341640381606091e-06
Iter: 1174 loss: 2.3335732877717048e-06
Iter: 1175 loss: 2.3376008343416064e-06
Iter: 1176 loss: 2.333515753193694e-06
Iter: 1177 loss: 2.3331431937283145e-06
Iter: 1178 loss: 2.33286526212389e-06
Iter: 1179 loss: 2.3327418339582316e-06
Iter: 1180 loss: 2.3322477381697827e-06
Iter: 1181 loss: 2.33797564543317e-06
Iter: 1182 loss: 2.3322400407411952e-06
Iter: 1183 loss: 2.331806549957875e-06
Iter: 1184 loss: 2.3314681191948122e-06
Iter: 1185 loss: 2.331332608092367e-06
Iter: 1186 loss: 2.330722159038662e-06
Iter: 1187 loss: 2.3303164685453558e-06
Iter: 1188 loss: 2.3300867955802586e-06
Iter: 1189 loss: 2.3296199903680866e-06
Iter: 1190 loss: 2.3295787716729354e-06
Iter: 1191 loss: 2.3289956002571103e-06
Iter: 1192 loss: 2.3288336131766546e-06
Iter: 1193 loss: 2.3284766790074061e-06
Iter: 1194 loss: 2.3278344943922385e-06
Iter: 1195 loss: 2.3274871194273609e-06
Iter: 1196 loss: 2.3271995656565686e-06
Iter: 1197 loss: 2.3264101651160077e-06
Iter: 1198 loss: 2.3293355926364345e-06
Iter: 1199 loss: 2.3262166363951015e-06
Iter: 1200 loss: 2.3255178558973214e-06
Iter: 1201 loss: 2.3334405054830962e-06
Iter: 1202 loss: 2.3255054817227036e-06
Iter: 1203 loss: 2.3247700403448351e-06
Iter: 1204 loss: 2.3253663323923885e-06
Iter: 1205 loss: 2.3243291116150545e-06
Iter: 1206 loss: 2.3238531673004425e-06
Iter: 1207 loss: 2.3229808223294803e-06
Iter: 1208 loss: 2.3433624548890956e-06
Iter: 1209 loss: 2.3229801451518174e-06
Iter: 1210 loss: 2.322222416734339e-06
Iter: 1211 loss: 2.3222216627114861e-06
Iter: 1212 loss: 2.3216190989046595e-06
Iter: 1213 loss: 2.3280836376735729e-06
Iter: 1214 loss: 2.3216048891503392e-06
Iter: 1215 loss: 2.32124095209447e-06
Iter: 1216 loss: 2.320652043283271e-06
Iter: 1217 loss: 2.3206481053653632e-06
Iter: 1218 loss: 2.3200936074059814e-06
Iter: 1219 loss: 2.3200934611254946e-06
Iter: 1220 loss: 2.3195983762612087e-06
Iter: 1221 loss: 2.319071606151724e-06
Iter: 1222 loss: 2.3189845855636755e-06
Iter: 1223 loss: 2.3183857126937619e-06
Iter: 1224 loss: 2.3195672084466094e-06
Iter: 1225 loss: 2.3181391895242406e-06
Iter: 1226 loss: 2.3174114803162382e-06
Iter: 1227 loss: 2.3199208441715282e-06
Iter: 1228 loss: 2.3172190093250305e-06
Iter: 1229 loss: 2.31651275134724e-06
Iter: 1230 loss: 2.3243232706442929e-06
Iter: 1231 loss: 2.3164984304169141e-06
Iter: 1232 loss: 2.3161674545293768e-06
Iter: 1233 loss: 2.3154256459451135e-06
Iter: 1234 loss: 2.3256841366371658e-06
Iter: 1235 loss: 2.3153847025278803e-06
Iter: 1236 loss: 2.314504136810969e-06
Iter: 1237 loss: 2.315266329680955e-06
Iter: 1238 loss: 2.3139859731150724e-06
Iter: 1239 loss: 2.3136983883384778e-06
Iter: 1240 loss: 2.313481065437278e-06
Iter: 1241 loss: 2.3129704228944145e-06
Iter: 1242 loss: 2.3125504991351198e-06
Iter: 1243 loss: 2.3124019785243569e-06
Iter: 1244 loss: 2.3117684881774852e-06
Iter: 1245 loss: 2.3118932856167132e-06
Iter: 1246 loss: 2.3112973064452267e-06
Iter: 1247 loss: 2.3104656475133297e-06
Iter: 1248 loss: 2.3122417037261959e-06
Iter: 1249 loss: 2.3101399827869428e-06
Iter: 1250 loss: 2.3100188803098194e-06
Iter: 1251 loss: 2.3097187499144917e-06
Iter: 1252 loss: 2.3094677261236512e-06
Iter: 1253 loss: 2.3088201331018895e-06
Iter: 1254 loss: 2.314139972900738e-06
Iter: 1255 loss: 2.3087050333544057e-06
Iter: 1256 loss: 2.3082582656415386e-06
Iter: 1257 loss: 2.3082259217689739e-06
Iter: 1258 loss: 2.3077923329780366e-06
Iter: 1259 loss: 2.3074396031690834e-06
Iter: 1260 loss: 2.3073115181065961e-06
Iter: 1261 loss: 2.3067335442525048e-06
Iter: 1262 loss: 2.30643747329035e-06
Iter: 1263 loss: 2.3061686771923504e-06
Iter: 1264 loss: 2.3053901701039112e-06
Iter: 1265 loss: 2.3137373467039226e-06
Iter: 1266 loss: 2.3053717958381686e-06
Iter: 1267 loss: 2.304717966023015e-06
Iter: 1268 loss: 2.3096885306007469e-06
Iter: 1269 loss: 2.3046682973482189e-06
Iter: 1270 loss: 2.3042713500762852e-06
Iter: 1271 loss: 2.3037085311386858e-06
Iter: 1272 loss: 2.303688351453107e-06
Iter: 1273 loss: 2.3029297951693805e-06
Iter: 1274 loss: 2.3040755844666013e-06
Iter: 1275 loss: 2.3025686236118547e-06
Iter: 1276 loss: 2.3018020570045404e-06
Iter: 1277 loss: 2.3053598131991118e-06
Iter: 1278 loss: 2.3016589588802459e-06
Iter: 1279 loss: 2.3010684328823243e-06
Iter: 1280 loss: 2.3010677286830022e-06
Iter: 1281 loss: 2.3006845205115092e-06
Iter: 1282 loss: 2.2996132767930688e-06
Iter: 1283 loss: 2.3053293170867151e-06
Iter: 1284 loss: 2.2992795419098374e-06
Iter: 1285 loss: 2.298395523030211e-06
Iter: 1286 loss: 2.3091225630687281e-06
Iter: 1287 loss: 2.2983852841247297e-06
Iter: 1288 loss: 2.2978846758901965e-06
Iter: 1289 loss: 2.2978776262211209e-06
Iter: 1290 loss: 2.2974042989527227e-06
Iter: 1291 loss: 2.2969376657973322e-06
Iter: 1292 loss: 2.2968379402629171e-06
Iter: 1293 loss: 2.296366810982312e-06
Iter: 1294 loss: 2.2981804441942552e-06
Iter: 1295 loss: 2.2962560488512831e-06
Iter: 1296 loss: 2.2956417848136734e-06
Iter: 1297 loss: 2.2974864046703654e-06
Iter: 1298 loss: 2.2954565651739892e-06
Iter: 1299 loss: 2.2949719520653125e-06
Iter: 1300 loss: 2.2940058620981549e-06
Iter: 1301 loss: 2.3125049210031652e-06
Iter: 1302 loss: 2.293994471806247e-06
Iter: 1303 loss: 2.2931824489166017e-06
Iter: 1304 loss: 2.3032212661111427e-06
Iter: 1305 loss: 2.2931742152043832e-06
Iter: 1306 loss: 2.2924222550412168e-06
Iter: 1307 loss: 2.29825928258583e-06
Iter: 1308 loss: 2.292368034918344e-06
Iter: 1309 loss: 2.2920285570211021e-06
Iter: 1310 loss: 2.2912776198424452e-06
Iter: 1311 loss: 2.3020774962686344e-06
Iter: 1312 loss: 2.2912413306876433e-06
Iter: 1313 loss: 2.2902532261387604e-06
Iter: 1314 loss: 2.2948238224577068e-06
Iter: 1315 loss: 2.2900677199759973e-06
Iter: 1316 loss: 2.289453392999245e-06
Iter: 1317 loss: 2.2934107618643627e-06
Iter: 1318 loss: 2.2893862232330536e-06
Iter: 1319 loss: 2.2888376368379817e-06
Iter: 1320 loss: 2.2934352303241341e-06
Iter: 1321 loss: 2.2888054968792977e-06
Iter: 1322 loss: 2.2883744571504686e-06
Iter: 1323 loss: 2.28770748687861e-06
Iter: 1324 loss: 2.2876980287350317e-06
Iter: 1325 loss: 2.2869396576895334e-06
Iter: 1326 loss: 2.28772665653678e-06
Iter: 1327 loss: 2.2865184289997644e-06
Iter: 1328 loss: 2.2862061999587726e-06
Iter: 1329 loss: 2.2861013792459085e-06
Iter: 1330 loss: 2.2856261856267398e-06
Iter: 1331 loss: 2.2849344166092583e-06
Iter: 1332 loss: 2.2849148077155645e-06
Iter: 1333 loss: 2.2843500033906873e-06
Iter: 1334 loss: 2.2860500388643957e-06
Iter: 1335 loss: 2.2841800610210332e-06
Iter: 1336 loss: 2.2834945300932538e-06
Iter: 1337 loss: 2.288145748906466e-06
Iter: 1338 loss: 2.2834270685585559e-06
Iter: 1339 loss: 2.2830010479604169e-06
Iter: 1340 loss: 2.2822656258208481e-06
Iter: 1341 loss: 2.2822651211054486e-06
Iter: 1342 loss: 2.2814738387228065e-06
Iter: 1343 loss: 2.2854626909101008e-06
Iter: 1344 loss: 2.2813424710199922e-06
Iter: 1345 loss: 2.2807276932931357e-06
Iter: 1346 loss: 2.2807269738470841e-06
Iter: 1347 loss: 2.2803279468980755e-06
Iter: 1348 loss: 2.2795679246480809e-06
Iter: 1349 loss: 2.295833369718089e-06
Iter: 1350 loss: 2.279564818759738e-06
Iter: 1351 loss: 2.2788699154607695e-06
Iter: 1352 loss: 2.2788449880956807e-06
Iter: 1353 loss: 2.2783064834751005e-06
Iter: 1354 loss: 2.2774551023979164e-06
Iter: 1355 loss: 2.2886936332680392e-06
Iter: 1356 loss: 2.2774502323694767e-06
Iter: 1357 loss: 2.2767851988198532e-06
Iter: 1358 loss: 2.2828724411575482e-06
Iter: 1359 loss: 2.2767555629605238e-06
Iter: 1360 loss: 2.2763187293556371e-06
Iter: 1361 loss: 2.2760196815994414e-06
Iter: 1362 loss: 2.2758602873518445e-06
Iter: 1363 loss: 2.2752480420826114e-06
Iter: 1364 loss: 2.2755999736604643e-06
Iter: 1365 loss: 2.2748499071636143e-06
Iter: 1366 loss: 2.2742594463271373e-06
Iter: 1367 loss: 2.2824299011403384e-06
Iter: 1368 loss: 2.2742575125624947e-06
Iter: 1369 loss: 2.2736799112289484e-06
Iter: 1370 loss: 2.2755302168481171e-06
Iter: 1371 loss: 2.2735158030746856e-06
Iter: 1372 loss: 2.2731310085096946e-06
Iter: 1373 loss: 2.2724352097413429e-06
Iter: 1374 loss: 2.2891981271026266e-06
Iter: 1375 loss: 2.2724350449360943e-06
Iter: 1376 loss: 2.2718846807141438e-06
Iter: 1377 loss: 2.2718571431264572e-06
Iter: 1378 loss: 2.2713673216188197e-06
Iter: 1379 loss: 2.271118151400581e-06
Iter: 1380 loss: 2.2708893747537275e-06
Iter: 1381 loss: 2.2704024762135133e-06
Iter: 1382 loss: 2.269667832717286e-06
Iter: 1383 loss: 2.2696535097941339e-06
Iter: 1384 loss: 2.2696606930544897e-06
Iter: 1385 loss: 2.2691787002847712e-06
Iter: 1386 loss: 2.268924449807006e-06
Iter: 1387 loss: 2.2683029787656131e-06
Iter: 1388 loss: 2.274688478386266e-06
Iter: 1389 loss: 2.2682294602228796e-06
Iter: 1390 loss: 2.2673910597039707e-06
Iter: 1391 loss: 2.2674935012402422e-06
Iter: 1392 loss: 2.2667500577644994e-06
Iter: 1393 loss: 2.266166463573111e-06
Iter: 1394 loss: 2.2661058073548079e-06
Iter: 1395 loss: 2.2655377692552414e-06
Iter: 1396 loss: 2.2675221597689894e-06
Iter: 1397 loss: 2.2653895583574436e-06
Iter: 1398 loss: 2.2650185536954528e-06
Iter: 1399 loss: 2.2642376757454159e-06
Iter: 1400 loss: 2.2772077833168064e-06
Iter: 1401 loss: 2.2642164819833284e-06
Iter: 1402 loss: 2.2634391012777917e-06
Iter: 1403 loss: 2.2689275211477757e-06
Iter: 1404 loss: 2.2633688532748069e-06
Iter: 1405 loss: 2.2625717307480359e-06
Iter: 1406 loss: 2.2701051155034358e-06
Iter: 1407 loss: 2.2625399618956327e-06
Iter: 1408 loss: 2.2621610083020364e-06
Iter: 1409 loss: 2.2614550014578823e-06
Iter: 1410 loss: 2.2773568243729605e-06
Iter: 1411 loss: 2.2614536949545526e-06
Iter: 1412 loss: 2.2608890706931628e-06
Iter: 1413 loss: 2.2608878158711655e-06
Iter: 1414 loss: 2.2603467724058305e-06
Iter: 1415 loss: 2.2608304043700875e-06
Iter: 1416 loss: 2.2600313994630251e-06
Iter: 1417 loss: 2.2595167961374176e-06
Iter: 1418 loss: 2.2588945114123841e-06
Iter: 1419 loss: 2.2588342025507282e-06
Iter: 1420 loss: 2.258354447466214e-06
Iter: 1421 loss: 2.258306860694791e-06
Iter: 1422 loss: 2.2577823672895208e-06
Iter: 1423 loss: 2.2578001378595934e-06
Iter: 1424 loss: 2.2573680039761e-06
Iter: 1425 loss: 2.2567112793521174e-06
Iter: 1426 loss: 2.2573312049122863e-06
Iter: 1427 loss: 2.2563349917533976e-06
Iter: 1428 loss: 2.2557889753750806e-06
Iter: 1429 loss: 2.2552846220282936e-06
Iter: 1430 loss: 2.2551535022939062e-06
Iter: 1431 loss: 2.25490015574321e-06
Iter: 1432 loss: 2.2546923770728554e-06
Iter: 1433 loss: 2.2542404613750671e-06
Iter: 1434 loss: 2.2538753751438388e-06
Iter: 1435 loss: 2.2537405305985495e-06
Iter: 1436 loss: 2.2530970014905719e-06
Iter: 1437 loss: 2.2526592114420885e-06
Iter: 1438 loss: 2.2524227994144626e-06
Iter: 1439 loss: 2.2519035734297478e-06
Iter: 1440 loss: 2.2518547292951287e-06
Iter: 1441 loss: 2.2513247309962142e-06
Iter: 1442 loss: 2.2526141090010381e-06
Iter: 1443 loss: 2.2511351650503882e-06
Iter: 1444 loss: 2.2507768662925434e-06
Iter: 1445 loss: 2.2499910156922175e-06
Iter: 1446 loss: 2.261597300304314e-06
Iter: 1447 loss: 2.2499563477838331e-06
Iter: 1448 loss: 2.2495641486821478e-06
Iter: 1449 loss: 2.2494414863871927e-06
Iter: 1450 loss: 2.2489428042492933e-06
Iter: 1451 loss: 2.2481885024893491e-06
Iter: 1452 loss: 2.2481742220545123e-06
Iter: 1453 loss: 2.2475401461202471e-06
Iter: 1454 loss: 2.2489474978916534e-06
Iter: 1455 loss: 2.2472981802730896e-06
Iter: 1456 loss: 2.2468880738309938e-06
Iter: 1457 loss: 2.2468473393857923e-06
Iter: 1458 loss: 2.2465279191099395e-06
Iter: 1459 loss: 2.2459553110088294e-06
Iter: 1460 loss: 2.2600165313574605e-06
Iter: 1461 loss: 2.2459552787388308e-06
Iter: 1462 loss: 2.2453300396588059e-06
Iter: 1463 loss: 2.2448357445112747e-06
Iter: 1464 loss: 2.2446435426554564e-06
Iter: 1465 loss: 2.2437804337173065e-06
Iter: 1466 loss: 2.2542913697720325e-06
Iter: 1467 loss: 2.2437706567418296e-06
Iter: 1468 loss: 2.2432333025408151e-06
Iter: 1469 loss: 2.2516595076867156e-06
Iter: 1470 loss: 2.2432332678705919e-06
Iter: 1471 loss: 2.24283362416017e-06
Iter: 1472 loss: 2.2422411544271472e-06
Iter: 1473 loss: 2.2422271569733155e-06
Iter: 1474 loss: 2.2414468076596505e-06
Iter: 1475 loss: 2.2431009944420586e-06
Iter: 1476 loss: 2.2411395828474372e-06
Iter: 1477 loss: 2.2408435096129994e-06
Iter: 1478 loss: 2.2407746538589385e-06
Iter: 1479 loss: 2.2404215099129471e-06
Iter: 1480 loss: 2.2398232263562768e-06
Iter: 1481 loss: 2.23982214361493e-06
Iter: 1482 loss: 2.2392360063981356e-06
Iter: 1483 loss: 2.2391060327206338e-06
Iter: 1484 loss: 2.2387257083937733e-06
Iter: 1485 loss: 2.2381937637277558e-06
Iter: 1486 loss: 2.2381395106163649e-06
Iter: 1487 loss: 2.2376849657999864e-06
Iter: 1488 loss: 2.2372822729169757e-06
Iter: 1489 loss: 2.2371647214628414e-06
Iter: 1490 loss: 2.2365307990759078e-06
Iter: 1491 loss: 2.2364535665152281e-06
Iter: 1492 loss: 2.235999743044946e-06
Iter: 1493 loss: 2.2358235262014353e-06
Iter: 1494 loss: 2.2355444544059026e-06
Iter: 1495 loss: 2.2353299175710637e-06
Iter: 1496 loss: 2.2346961011464462e-06
Iter: 1497 loss: 2.2369107665160269e-06
Iter: 1498 loss: 2.2344085225618106e-06
Iter: 1499 loss: 2.2335442585286656e-06
Iter: 1500 loss: 2.2366507349186326e-06
Iter: 1501 loss: 2.2333253724148306e-06
Iter: 1502 loss: 2.2329800972202359e-06
Iter: 1503 loss: 2.232874724706935e-06
Iter: 1504 loss: 2.2324513157263493e-06
Iter: 1505 loss: 2.2319884574616024e-06
Iter: 1506 loss: 2.2319193209779211e-06
Iter: 1507 loss: 2.2312670821257259e-06
Iter: 1508 loss: 2.2313795591526468e-06
Iter: 1509 loss: 2.230777609033575e-06
Iter: 1510 loss: 2.2304670152322983e-06
Iter: 1511 loss: 2.2303541926262522e-06
Iter: 1512 loss: 2.2299448552034059e-06
Iter: 1513 loss: 2.2294019313564913e-06
Iter: 1514 loss: 2.2293703214266937e-06
Iter: 1515 loss: 2.228768760277842e-06
Iter: 1516 loss: 2.2294865805810752e-06
Iter: 1517 loss: 2.228451417776452e-06
Iter: 1518 loss: 2.227799627365167e-06
Iter: 1519 loss: 2.2324335010951636e-06
Iter: 1520 loss: 2.2277416339529125e-06
Iter: 1521 loss: 2.22719801726354e-06
Iter: 1522 loss: 2.2310573602415357e-06
Iter: 1523 loss: 2.2271494931543824e-06
Iter: 1524 loss: 2.2268102539656403e-06
Iter: 1525 loss: 2.2261052731385074e-06
Iter: 1526 loss: 2.2382269000451166e-06
Iter: 1527 loss: 2.2260891497761076e-06
Iter: 1528 loss: 2.2258306286864627e-06
Iter: 1529 loss: 2.2257272711389118e-06
Iter: 1530 loss: 2.2253128576944036e-06
Iter: 1531 loss: 2.2245025056475547e-06
Iter: 1532 loss: 2.2407909723055028e-06
Iter: 1533 loss: 2.2244960333648314e-06
Iter: 1534 loss: 2.2238275660456848e-06
Iter: 1535 loss: 2.2243060263166409e-06
Iter: 1536 loss: 2.2234136369843693e-06
Iter: 1537 loss: 2.222460258754603e-06
Iter: 1538 loss: 2.2250225966617052e-06
Iter: 1539 loss: 2.2221444309969661e-06
Iter: 1540 loss: 2.2215524782820919e-06
Iter: 1541 loss: 2.2293284484151811e-06
Iter: 1542 loss: 2.2215489398577145e-06
Iter: 1543 loss: 2.2208930640339555e-06
Iter: 1544 loss: 2.2224320285877212e-06
Iter: 1545 loss: 2.2206522405755167e-06
Iter: 1546 loss: 2.2202144136400592e-06
Iter: 1547 loss: 2.21977723933216e-06
Iter: 1548 loss: 2.219687546471996e-06
Iter: 1549 loss: 2.2193534782527986e-06
Iter: 1550 loss: 2.2193022237193569e-06
Iter: 1551 loss: 2.2188744720362e-06
Iter: 1552 loss: 2.218107744860539e-06
Iter: 1553 loss: 2.236933999534811e-06
Iter: 1554 loss: 2.2181077024322292e-06
Iter: 1555 loss: 2.2173968737307079e-06
Iter: 1556 loss: 2.2188348127888904e-06
Iter: 1557 loss: 2.2171086156123086e-06
Iter: 1558 loss: 2.2164218889659677e-06
Iter: 1559 loss: 2.2179358048085449e-06
Iter: 1560 loss: 2.2161586545574195e-06
Iter: 1561 loss: 2.2157355355956734e-06
Iter: 1562 loss: 2.2156884854084641e-06
Iter: 1563 loss: 2.2153681336877254e-06
Iter: 1564 loss: 2.2149542763514642e-06
Iter: 1565 loss: 2.21492598445844e-06
Iter: 1566 loss: 2.2143669547750094e-06
Iter: 1567 loss: 2.21576887142826e-06
Iter: 1568 loss: 2.2141715278143426e-06
Iter: 1569 loss: 2.2137612596720745e-06
Iter: 1570 loss: 2.2137599754922677e-06
Iter: 1571 loss: 2.2133917266108917e-06
Iter: 1572 loss: 2.2126808163526735e-06
Iter: 1573 loss: 2.2274295539301404e-06
Iter: 1574 loss: 2.212676621719185e-06
Iter: 1575 loss: 2.2120064648061906e-06
Iter: 1576 loss: 2.2120350681943389e-06
Iter: 1577 loss: 2.2114788771475509e-06
Iter: 1578 loss: 2.2106068107060617e-06
Iter: 1579 loss: 2.2163714754470352e-06
Iter: 1580 loss: 2.2105162032379687e-06
Iter: 1581 loss: 2.2100755619373728e-06
Iter: 1582 loss: 2.2100430779162816e-06
Iter: 1583 loss: 2.2096630748659763e-06
Iter: 1584 loss: 2.20903304789639e-06
Iter: 1585 loss: 2.209030688461654e-06
Iter: 1586 loss: 2.208385092348629e-06
Iter: 1587 loss: 2.2093763115489668e-06
Iter: 1588 loss: 2.2080800890493479e-06
Iter: 1589 loss: 2.2075688896286496e-06
Iter: 1590 loss: 2.2075318363325836e-06
Iter: 1591 loss: 2.2071970937508508e-06
Iter: 1592 loss: 2.206363993058803e-06
Iter: 1593 loss: 2.2143403483914185e-06
Iter: 1594 loss: 2.2062506247430969e-06
Iter: 1595 loss: 2.2054764331487803e-06
Iter: 1596 loss: 2.2071894977065349e-06
Iter: 1597 loss: 2.2051802892009616e-06
Iter: 1598 loss: 2.2050077012328315e-06
Iter: 1599 loss: 2.2047633766100697e-06
Iter: 1600 loss: 2.2045053095448e-06
Iter: 1601 loss: 2.2040192447482307e-06
Iter: 1602 loss: 2.2146945055500793e-06
Iter: 1603 loss: 2.2040178639701006e-06
Iter: 1604 loss: 2.2034414314178257e-06
Iter: 1605 loss: 2.2047880751808675e-06
Iter: 1606 loss: 2.2032290717823129e-06
Iter: 1607 loss: 2.2025896623067342e-06
Iter: 1608 loss: 2.2096915727256143e-06
Iter: 1609 loss: 2.2025769758492792e-06
Iter: 1610 loss: 2.2021516981673603e-06
Iter: 1611 loss: 2.2017337562038297e-06
Iter: 1612 loss: 2.20164359023371e-06
Iter: 1613 loss: 2.2010334037083096e-06
Iter: 1614 loss: 2.2015140496049218e-06
Iter: 1615 loss: 2.2006646531060226e-06
Iter: 1616 loss: 2.1998774834592071e-06
Iter: 1617 loss: 2.2015198990084719e-06
Iter: 1618 loss: 2.1995645950123253e-06
Iter: 1619 loss: 2.1988660674942023e-06
Iter: 1620 loss: 2.202084379729955e-06
Iter: 1621 loss: 2.1987343855536329e-06
Iter: 1622 loss: 2.1981691353941489e-06
Iter: 1623 loss: 2.1981675526213148e-06
Iter: 1624 loss: 2.1978462338564044e-06
Iter: 1625 loss: 2.19700843552274e-06
Iter: 1626 loss: 2.2035617403560589e-06
Iter: 1627 loss: 2.19684768862005e-06
Iter: 1628 loss: 2.1967975374801556e-06
Iter: 1629 loss: 2.1964984951431484e-06
Iter: 1630 loss: 2.1961177841320662e-06
Iter: 1631 loss: 2.1955992267423995e-06
Iter: 1632 loss: 2.1955739699434224e-06
Iter: 1633 loss: 2.1948960992716361e-06
Iter: 1634 loss: 2.1947936012008181e-06
Iter: 1635 loss: 2.1943218236557187e-06
Iter: 1636 loss: 2.1935912031118156e-06
Iter: 1637 loss: 2.1968772456273554e-06
Iter: 1638 loss: 2.1934489964800359e-06
Iter: 1639 loss: 2.1927219804989362e-06
Iter: 1640 loss: 2.2016375339895843e-06
Iter: 1641 loss: 2.1927141698222943e-06
Iter: 1642 loss: 2.1924002451738953e-06
Iter: 1643 loss: 2.1916486169756376e-06
Iter: 1644 loss: 2.1999886258309764e-06
Iter: 1645 loss: 2.1915733801481553e-06
Iter: 1646 loss: 2.1914416282437737e-06
Iter: 1647 loss: 2.1911746767903947e-06
Iter: 1648 loss: 2.1908232722630012e-06
Iter: 1649 loss: 2.190230054523273e-06
Iter: 1650 loss: 2.1902288152256964e-06
Iter: 1651 loss: 2.1895625278502024e-06
Iter: 1652 loss: 2.1902004831535831e-06
Iter: 1653 loss: 2.1891824493220935e-06
Iter: 1654 loss: 2.1884958436899614e-06
Iter: 1655 loss: 2.1906367644462625e-06
Iter: 1656 loss: 2.1882956644943727e-06
Iter: 1657 loss: 2.187642331242752e-06
Iter: 1658 loss: 2.1908644872307962e-06
Iter: 1659 loss: 2.1875303811003105e-06
Iter: 1660 loss: 2.186961101406025e-06
Iter: 1661 loss: 2.1931263733014583e-06
Iter: 1662 loss: 2.1869482769174612e-06
Iter: 1663 loss: 2.1864964117470042e-06
Iter: 1664 loss: 2.1861834084874047e-06
Iter: 1665 loss: 2.1860204945749434e-06
Iter: 1666 loss: 2.1854574209572389e-06
Iter: 1667 loss: 2.1862793817485376e-06
Iter: 1668 loss: 2.1851848663360084e-06
Iter: 1669 loss: 2.1844868427783635e-06
Iter: 1670 loss: 2.193072908393184e-06
Iter: 1671 loss: 2.1844795368704419e-06
Iter: 1672 loss: 2.1841735335021963e-06
Iter: 1673 loss: 2.1834050384476531e-06
Iter: 1674 loss: 2.19049726207071e-06
Iter: 1675 loss: 2.1832930412421934e-06
Iter: 1676 loss: 2.18250218002333e-06
Iter: 1677 loss: 2.1882968259167773e-06
Iter: 1678 loss: 2.1824365203829334e-06
Iter: 1679 loss: 2.1819402531280591e-06
Iter: 1680 loss: 2.1819383133959297e-06
Iter: 1681 loss: 2.1815016623693204e-06
Iter: 1682 loss: 2.1808034973432544e-06
Iter: 1683 loss: 2.180797607290378e-06
Iter: 1684 loss: 2.1802742230072941e-06
Iter: 1685 loss: 2.1846520942421461e-06
Iter: 1686 loss: 2.1802433649686195e-06
Iter: 1687 loss: 2.179617125305092e-06
Iter: 1688 loss: 2.1807340156859522e-06
Iter: 1689 loss: 2.1793436043761511e-06
Iter: 1690 loss: 2.1789124500333194e-06
Iter: 1691 loss: 2.1781852198564532e-06
Iter: 1692 loss: 2.178183653869141e-06
Iter: 1693 loss: 2.1773718673297603e-06
Iter: 1694 loss: 2.1815466512242491e-06
Iter: 1695 loss: 2.177241016318225e-06
Iter: 1696 loss: 2.17648348378884e-06
Iter: 1697 loss: 2.1770342165046462e-06
Iter: 1698 loss: 2.1760162122389022e-06
Iter: 1699 loss: 2.175697276080389e-06
Iter: 1700 loss: 2.1754814915902812e-06
Iter: 1701 loss: 2.175145651254515e-06
Iter: 1702 loss: 2.1744324614700751e-06
Iter: 1703 loss: 2.1860000906767683e-06
Iter: 1704 loss: 2.1744108077042226e-06
Iter: 1705 loss: 2.1737963158601663e-06
Iter: 1706 loss: 2.1793007936984666e-06
Iter: 1707 loss: 2.173766942756004e-06
Iter: 1708 loss: 2.1730643167623578e-06
Iter: 1709 loss: 2.1750095522764117e-06
Iter: 1710 loss: 2.172837139574525e-06
Iter: 1711 loss: 2.1724619746070697e-06
Iter: 1712 loss: 2.1720084096180461e-06
Iter: 1713 loss: 2.1719643498607581e-06
Iter: 1714 loss: 2.1711308412439495e-06
Iter: 1715 loss: 2.1724473568187141e-06
Iter: 1716 loss: 2.1707426602601873e-06
Iter: 1717 loss: 2.1702473861725056e-06
Iter: 1718 loss: 2.1702431791282984e-06
Iter: 1719 loss: 2.1697499144580129e-06
Iter: 1720 loss: 2.1703743027213787e-06
Iter: 1721 loss: 2.1694956855281941e-06
Iter: 1722 loss: 2.1690279755960832e-06
Iter: 1723 loss: 2.1688128588451027e-06
Iter: 1724 loss: 2.1685805534706597e-06
Iter: 1725 loss: 2.1680948821665524e-06
Iter: 1726 loss: 2.1680858225799707e-06
Iter: 1727 loss: 2.1676432573319038e-06
Iter: 1728 loss: 2.1669013748687128e-06
Iter: 1729 loss: 2.1668993885634902e-06
Iter: 1730 loss: 2.16617276699211e-06
Iter: 1731 loss: 2.1656244200890208e-06
Iter: 1732 loss: 2.1653871339873116e-06
Iter: 1733 loss: 2.1645358391660594e-06
Iter: 1734 loss: 2.1776670626880835e-06
Iter: 1735 loss: 2.1645356561793316e-06
Iter: 1736 loss: 2.1639725550893565e-06
Iter: 1737 loss: 2.1728860884990476e-06
Iter: 1738 loss: 2.1639725444153514e-06
Iter: 1739 loss: 2.1636446786431259e-06
Iter: 1740 loss: 2.1629327498144554e-06
Iter: 1741 loss: 2.173756575362065e-06
Iter: 1742 loss: 2.16290460891999e-06
Iter: 1743 loss: 2.1624903162844409e-06
Iter: 1744 loss: 2.1624363098792568e-06
Iter: 1745 loss: 2.1619647885830674e-06
Iter: 1746 loss: 2.1613927505842358e-06
Iter: 1747 loss: 2.1613381522771596e-06
Iter: 1748 loss: 2.1606854405269511e-06
Iter: 1749 loss: 2.1609111370042961e-06
Iter: 1750 loss: 2.1602253368489766e-06
Iter: 1751 loss: 2.1594935354832727e-06
Iter: 1752 loss: 2.1604402253720603e-06
Iter: 1753 loss: 2.159119803738402e-06
Iter: 1754 loss: 2.15897153710369e-06
Iter: 1755 loss: 2.1586846660385746e-06
Iter: 1756 loss: 2.1584357755360035e-06
Iter: 1757 loss: 2.1578328024774332e-06
Iter: 1758 loss: 2.1642374233722298e-06
Iter: 1759 loss: 2.1577663488182764e-06
Iter: 1760 loss: 2.1571714140408017e-06
Iter: 1761 loss: 2.1652753757921545e-06
Iter: 1762 loss: 2.1571690170379827e-06
Iter: 1763 loss: 2.1565968272648268e-06
Iter: 1764 loss: 2.1575782273885122e-06
Iter: 1765 loss: 2.1563414393348585e-06
Iter: 1766 loss: 2.1559104369006122e-06
Iter: 1767 loss: 2.1555218798010105e-06
Iter: 1768 loss: 2.1554137680479612e-06
Iter: 1769 loss: 2.1546751921438928e-06
Iter: 1770 loss: 2.1566548551605274e-06
Iter: 1771 loss: 2.1544298370743549e-06
Iter: 1772 loss: 2.1537036544877429e-06
Iter: 1773 loss: 2.1566326667389426e-06
Iter: 1774 loss: 2.1535418819814439e-06
Iter: 1775 loss: 2.1530519662000694e-06
Iter: 1776 loss: 2.153043946388005e-06
Iter: 1777 loss: 2.1527068943624432e-06
Iter: 1778 loss: 2.1519620696936042e-06
Iter: 1779 loss: 2.1627252129930162e-06
Iter: 1780 loss: 2.1519265030263129e-06
Iter: 1781 loss: 2.151694113990061e-06
Iter: 1782 loss: 2.1515477484293077e-06
Iter: 1783 loss: 2.1511858974571816e-06
Iter: 1784 loss: 2.1503925851320409e-06
Iter: 1785 loss: 2.1621259592271682e-06
Iter: 1786 loss: 2.1503577507157742e-06
Iter: 1787 loss: 2.1496900266145215e-06
Iter: 1788 loss: 2.150171107841078e-06
Iter: 1789 loss: 2.1492772394054e-06
Iter: 1790 loss: 2.1484576310938631e-06
Iter: 1791 loss: 2.1522925391859905e-06
Iter: 1792 loss: 2.14830631161896e-06
Iter: 1793 loss: 2.1475366109089653e-06
Iter: 1794 loss: 2.1481783122647881e-06
Iter: 1795 loss: 2.1470787454210957e-06
Iter: 1796 loss: 2.1462822845843945e-06
Iter: 1797 loss: 2.14947176508739e-06
Iter: 1798 loss: 2.1461032982680135e-06
Iter: 1799 loss: 2.1455882795922727e-06
Iter: 1800 loss: 2.1455398453847792e-06
Iter: 1801 loss: 2.1452100163267257e-06
Iter: 1802 loss: 2.1445217818056347e-06
Iter: 1803 loss: 2.1562230668183082e-06
Iter: 1804 loss: 2.1445051259374268e-06
Iter: 1805 loss: 2.1437885698406629e-06
Iter: 1806 loss: 2.1477624226097052e-06
Iter: 1807 loss: 2.1436859914847705e-06
Iter: 1808 loss: 2.1432095580999832e-06
Iter: 1809 loss: 2.1432063917349748e-06
Iter: 1810 loss: 2.1428873764785721e-06
Iter: 1811 loss: 2.1423035994641341e-06
Iter: 1812 loss: 2.1559874254884778e-06
Iter: 1813 loss: 2.1423031922643153e-06
Iter: 1814 loss: 2.1416325927453543e-06
Iter: 1815 loss: 2.1428075656959251e-06
Iter: 1816 loss: 2.1413365811889559e-06
Iter: 1817 loss: 2.1406404201362664e-06
Iter: 1818 loss: 2.1446843190639327e-06
Iter: 1819 loss: 2.1405483384461741e-06
Iter: 1820 loss: 2.1399364241939558e-06
Iter: 1821 loss: 2.1465798837406171e-06
Iter: 1822 loss: 2.1399228053751018e-06
Iter: 1823 loss: 2.1395364265277331e-06
Iter: 1824 loss: 2.1386552521024074e-06
Iter: 1825 loss: 2.1501961977788327e-06
Iter: 1826 loss: 2.13859773700542e-06
Iter: 1827 loss: 2.1383050423883746e-06
Iter: 1828 loss: 2.1381704726409326e-06
Iter: 1829 loss: 2.1377354397235231e-06
Iter: 1830 loss: 2.1370036385081371e-06
Iter: 1831 loss: 2.1370019029974486e-06
Iter: 1832 loss: 2.1362971549660833e-06
Iter: 1833 loss: 2.1373296507811825e-06
Iter: 1834 loss: 2.1359565724629962e-06
Iter: 1835 loss: 2.1350801571692164e-06
Iter: 1836 loss: 2.1354639345558573e-06
Iter: 1837 loss: 2.134482339110011e-06
Iter: 1838 loss: 2.1336109684327434e-06
Iter: 1839 loss: 2.1381426006460941e-06
Iter: 1840 loss: 2.1334728966902555e-06
Iter: 1841 loss: 2.1328586917641268e-06
Iter: 1842 loss: 2.1328337681883156e-06
Iter: 1843 loss: 2.132525083524556e-06
Iter: 1844 loss: 2.1319924681993033e-06
Iter: 1845 loss: 2.1319920918831389e-06
Iter: 1846 loss: 2.131414335022562e-06
Iter: 1847 loss: 2.133069305910767e-06
Iter: 1848 loss: 2.131232798525643e-06
Iter: 1849 loss: 2.1305161507195673e-06
Iter: 1850 loss: 2.136529177363096e-06
Iter: 1851 loss: 2.1304743090063956e-06
Iter: 1852 loss: 2.1299988602918584e-06
Iter: 1853 loss: 2.1293397447104873e-06
Iter: 1854 loss: 2.1293114936208558e-06
Iter: 1855 loss: 2.1286509618071245e-06
Iter: 1856 loss: 2.1301743796171489e-06
Iter: 1857 loss: 2.1284055832341075e-06
Iter: 1858 loss: 2.1277887508759357e-06
Iter: 1859 loss: 2.1355503546211433e-06
Iter: 1860 loss: 2.1277833043941318e-06
Iter: 1861 loss: 2.1272322145915533e-06
Iter: 1862 loss: 2.129161445958752e-06
Iter: 1863 loss: 2.1270887008652514e-06
Iter: 1864 loss: 2.1266855421124425e-06
Iter: 1865 loss: 2.126171694520238e-06
Iter: 1866 loss: 2.1261337744655541e-06
Iter: 1867 loss: 2.1256923086971067e-06
Iter: 1868 loss: 2.125678774362318e-06
Iter: 1869 loss: 2.1252236863031651e-06
Iter: 1870 loss: 2.1243445928376251e-06
Iter: 1871 loss: 2.142550281090206e-06
Iter: 1872 loss: 2.12433932175179e-06
Iter: 1873 loss: 2.123676774860881e-06
Iter: 1874 loss: 2.1235992635123917e-06
Iter: 1875 loss: 2.1231227424255051e-06
Iter: 1876 loss: 2.1221715752340391e-06
Iter: 1877 loss: 2.13001464289728e-06
Iter: 1878 loss: 2.1221132066454618e-06
Iter: 1879 loss: 2.1216487209322325e-06
Iter: 1880 loss: 2.1216486750742753e-06
Iter: 1881 loss: 2.1211226517935039e-06
Iter: 1882 loss: 2.1205563762819898e-06
Iter: 1883 loss: 2.1204667809980946e-06
Iter: 1884 loss: 2.1198031500607765e-06
Iter: 1885 loss: 2.1204330597401135e-06
Iter: 1886 loss: 2.1194234556349194e-06
Iter: 1887 loss: 2.1192071075609633e-06
Iter: 1888 loss: 2.1190616997621474e-06
Iter: 1889 loss: 2.1187429014116455e-06
Iter: 1890 loss: 2.1180618041287143e-06
Iter: 1891 loss: 2.1289115740758674e-06
Iter: 1892 loss: 2.1180395149999011e-06
Iter: 1893 loss: 2.1173072537254964e-06
Iter: 1894 loss: 2.1178618037492723e-06
Iter: 1895 loss: 2.1168602058298729e-06
Iter: 1896 loss: 2.1160968420713195e-06
Iter: 1897 loss: 2.119811853660488e-06
Iter: 1898 loss: 2.1159634587667846e-06
Iter: 1899 loss: 2.1156558436905842e-06
Iter: 1900 loss: 2.1155854779960568e-06
Iter: 1901 loss: 2.1152153165175132e-06
Iter: 1902 loss: 2.1144107888762698e-06
Iter: 1903 loss: 2.1266063718380578e-06
Iter: 1904 loss: 2.1143786442143689e-06
Iter: 1905 loss: 2.1138086618380755e-06
Iter: 1906 loss: 2.1172233667508652e-06
Iter: 1907 loss: 2.1137371630875027e-06
Iter: 1908 loss: 2.1130537876875539e-06
Iter: 1909 loss: 2.1156410433004676e-06
Iter: 1910 loss: 2.1128900663854609e-06
Iter: 1911 loss: 2.1123925616436103e-06
Iter: 1912 loss: 2.1123559824955241e-06
Iter: 1913 loss: 2.1119834507193128e-06
Iter: 1914 loss: 2.1113050262433206e-06
Iter: 1915 loss: 2.1118805593670723e-06
Iter: 1916 loss: 2.1109033898224337e-06
Iter: 1917 loss: 2.1101954335967876e-06
Iter: 1918 loss: 2.1125215640225514e-06
Iter: 1919 loss: 2.1099990770362861e-06
Iter: 1920 loss: 2.109545121081745e-06
Iter: 1921 loss: 2.109511617853907e-06
Iter: 1922 loss: 2.10921022614593e-06
Iter: 1923 loss: 2.1085458735034773e-06
Iter: 1924 loss: 2.1182141219682342e-06
Iter: 1925 loss: 2.1085149631384505e-06
Iter: 1926 loss: 2.1079505353322104e-06
Iter: 1927 loss: 2.1141839019685419e-06
Iter: 1928 loss: 2.1079390225730564e-06
Iter: 1929 loss: 2.1073131767121456e-06
Iter: 1930 loss: 2.1087107368542537e-06
Iter: 1931 loss: 2.107075266928933e-06
Iter: 1932 loss: 2.1066227674692372e-06
Iter: 1933 loss: 2.1063323166416033e-06
Iter: 1934 loss: 2.1061562789798332e-06
Iter: 1935 loss: 2.1055021707337165e-06
Iter: 1936 loss: 2.1053553364856649e-06
Iter: 1937 loss: 2.1049320796893111e-06
Iter: 1938 loss: 2.1051860291629342e-06
Iter: 1939 loss: 2.1045499430508494e-06
Iter: 1940 loss: 2.10430503261382e-06
Iter: 1941 loss: 2.1036025956049228e-06
Iter: 1942 loss: 2.106744398060432e-06
Iter: 1943 loss: 2.1033421607649337e-06
Iter: 1944 loss: 2.1026868167726823e-06
Iter: 1945 loss: 2.1124385788777737e-06
Iter: 1946 loss: 2.10268627341844e-06
Iter: 1947 loss: 2.1021382654919245e-06
Iter: 1948 loss: 2.1058391083291246e-06
Iter: 1949 loss: 2.1020837974386476e-06
Iter: 1950 loss: 2.1017372631715495e-06
Iter: 1951 loss: 2.1011531565118411e-06
Iter: 1952 loss: 2.1011518679114573e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.8/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.2 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi1.2
+ date
Sat Nov  7 21:05:09 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi0.8/300_300_300_1 --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae6449f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae644e46a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae644b5c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae64411ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae64414840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae64414ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae643738c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae6428d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae642bf488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae642bf730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae64306158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae40daa730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae40daa378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae40d699d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae6438c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae643b9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae643b8400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae40dbcae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae40d148c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae40d14840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c539598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c5399d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c58f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c575730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c575400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c4a77b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c45a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c46d2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c3f3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c46d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae40ce1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c3b8378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c3bd158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c3bbb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c37db70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fae1c35e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0047297080142776156
test_loss: 0.004916886986558102
train_loss: 0.00453305171846811
test_loss: 0.004941238177159363
train_loss: 0.0040299896302288364
test_loss: 0.004614972818004953
train_loss: 0.00429006591052585
test_loss: 0.0047484321372551755
train_loss: 0.004072427549647513
test_loss: 0.004326219139392394
train_loss: 0.004069725730089133
test_loss: 0.004520155515589783
train_loss: 0.0041358118262966774
test_loss: 0.004569772029502644
train_loss: 0.004117592659209549
test_loss: 0.004516746259812127
train_loss: 0.0038914783770082235
test_loss: 0.0044188386960074925
train_loss: 0.004168592106876583
test_loss: 0.00473767115366711
train_loss: 0.00398721309217191
test_loss: 0.00430015480037913
train_loss: 0.003909201569554524
test_loss: 0.00435934001242929
train_loss: 0.004435210204328224
test_loss: 0.004467372875463198
train_loss: 0.003926755739738993
test_loss: 0.00445076776173855
train_loss: 0.0037534499126527197
test_loss: 0.004157329776780464
train_loss: 0.0040489545663516535
test_loss: 0.00445964219871211
train_loss: 0.0039964459546333745
test_loss: 0.004299146497179319
train_loss: 0.0038015280005786987
test_loss: 0.004226835676342498
train_loss: 0.00367522162642594
test_loss: 0.004284521812671743
train_loss: 0.0039042475263514893
test_loss: 0.0041479019534264175
train_loss: 0.0034695243150672382
test_loss: 0.004048414868883355
train_loss: 0.003618232828550107
test_loss: 0.0042954131887191
train_loss: 0.003906234456843977
test_loss: 0.004296190561958796
train_loss: 0.003891538741236544
test_loss: 0.004366862892674895
train_loss: 0.003781422607999611
test_loss: 0.004337652257628655
train_loss: 0.0037172699457185257
test_loss: 0.004118466126472614
train_loss: 0.004289839516293603
test_loss: 0.004264763082373601
train_loss: 0.0035090044314444835
test_loss: 0.003976370090189371
train_loss: 0.0037090646766002
test_loss: 0.004451477144486384
train_loss: 0.003947344414353004
test_loss: 0.004175770115932413
train_loss: 0.004166888413532625
test_loss: 0.004126136855240612
train_loss: 0.003554781186535135
test_loss: 0.0041489525857005445
train_loss: 0.0036037506692519422
test_loss: 0.004388211782146988
train_loss: 0.0036489017993381143
test_loss: 0.004540411959857766
train_loss: 0.00387236204654166
test_loss: 0.004083019089978341
train_loss: 0.0035279708159661107
test_loss: 0.0041828827605201796
train_loss: 0.0035441626344145613
test_loss: 0.003968046543985262
train_loss: 0.00360851776439526
test_loss: 0.0040277611749242334
train_loss: 0.0034978751033908516
test_loss: 0.004207988066167582
train_loss: 0.0034815282257824763
test_loss: 0.003937584615064182
train_loss: 0.0035817780350960483
test_loss: 0.004047051416602712
train_loss: 0.0036593154983877003
test_loss: 0.004035683328141125
train_loss: 0.003600148203176657
test_loss: 0.004167449184105925
train_loss: 0.003673181924590165
test_loss: 0.004162368975184991
train_loss: 0.0038304927283768853
test_loss: 0.004168936297314902
train_loss: 0.0036927864585343632
test_loss: 0.004193115238140384
train_loss: 0.0034975240557945874
test_loss: 0.004093776846934748
train_loss: 0.0035354065497471276
test_loss: 0.003909461129667484
train_loss: 0.003533540730655455
test_loss: 0.004042826055665904
train_loss: 0.0033967533912757242
test_loss: 0.003918865392637646
train_loss: 0.0036874379410347293
test_loss: 0.004147461936190848
train_loss: 0.0036213706844467498
test_loss: 0.004354706980402139
train_loss: 0.003490289536812208
test_loss: 0.004088212580546725
train_loss: 0.003717415916868182
test_loss: 0.003945820782424268
train_loss: 0.0035778033354795734
test_loss: 0.004075412240055225
train_loss: 0.0038640408737694665
test_loss: 0.003970767899622291
train_loss: 0.003454838094325697
test_loss: 0.004125265419994741
train_loss: 0.003592487007364369
test_loss: 0.003974513459601799
train_loss: 0.0034172227737747627
test_loss: 0.003902502983184109
train_loss: 0.003497539460226057
test_loss: 0.004249790373454845
train_loss: 0.0036141263054940073
test_loss: 0.004018579659988921
train_loss: 0.0032780862728159638
test_loss: 0.0038369530029351362
train_loss: 0.003549256497291285
test_loss: 0.0038651780020926866
train_loss: 0.0035007300288112404
test_loss: 0.0039618648217410145
train_loss: 0.003494353068932349
test_loss: 0.003955234791691609
train_loss: 0.0036181146173170607
test_loss: 0.003934064295428954
train_loss: 0.0033899869600460715
test_loss: 0.004271543552417017
train_loss: 0.003440839828722087
test_loss: 0.0040356633773758616
train_loss: 0.0034525712822785853
test_loss: 0.0038900826373447902
train_loss: 0.0031980217619425808
test_loss: 0.0038010987187999173
train_loss: 0.0035831794964643632
test_loss: 0.004073149436075074
train_loss: 0.003819304836694274
test_loss: 0.004178909865790549
train_loss: 0.0033245707597312837
test_loss: 0.003915988108300883
train_loss: 0.0032976579192690167
test_loss: 0.0036942919516819266
train_loss: 0.0038786461778371225
test_loss: 0.003962774211219054
train_loss: 0.003386401069870565
test_loss: 0.003951809518078202
train_loss: 0.0035340251260658462
test_loss: 0.004130159276723084
train_loss: 0.003361186520206747
test_loss: 0.004151026613551384
train_loss: 0.003425591894297213
test_loss: 0.004215225873926264
train_loss: 0.0035044924669721134
test_loss: 0.004123084210208939
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi1.2/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff28900d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff28903d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff28903dea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2d35856a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2d356f620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff288fc8b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2640996a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff264021840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff26402a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff264021488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff26402af28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2507b16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2507b1d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff250742b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff25070ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff25070e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff25072f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff250693b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2506a16a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2506a1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff250679620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff250679d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2505c0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff25057e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff25057e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2505b7620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff250546620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2505022f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2505029d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2504fd730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2504fd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2504937b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff25048d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff25044cb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff250464598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff250424268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.182025926401133e-05
Iter: 2 loss: 2.0331040736745692e-05
Iter: 3 loss: 1.692864098686086e-05
Iter: 4 loss: 1.5511356929178075e-05
Iter: 5 loss: 1.3882163861020776e-05
Iter: 6 loss: 1.3682606464681965e-05
Iter: 7 loss: 1.2198775351419087e-05
Iter: 8 loss: 2.7217151193335992e-05
Iter: 9 loss: 1.2153733426495748e-05
Iter: 10 loss: 1.1105252780690771e-05
Iter: 11 loss: 1.2663924190283493e-05
Iter: 12 loss: 1.0602538255554234e-05
Iter: 13 loss: 9.8948395351850679e-06
Iter: 14 loss: 1.2936611131221925e-05
Iter: 15 loss: 9.7485673274583768e-06
Iter: 16 loss: 9.0940677581420364e-06
Iter: 17 loss: 1.3200887044189372e-05
Iter: 18 loss: 9.01845424454842e-06
Iter: 19 loss: 8.6241999055568169e-06
Iter: 20 loss: 8.1161242997489349e-06
Iter: 21 loss: 8.08103815189009e-06
Iter: 22 loss: 7.5503656865884787e-06
Iter: 23 loss: 1.4837728393323029e-05
Iter: 24 loss: 7.5483889621207793e-06
Iter: 25 loss: 7.174777472510793e-06
Iter: 26 loss: 9.0798341232834029e-06
Iter: 27 loss: 7.1135983135094896e-06
Iter: 28 loss: 6.8696275854942563e-06
Iter: 29 loss: 6.5312161772995845e-06
Iter: 30 loss: 6.5167796729677893e-06
Iter: 31 loss: 6.1225038392418951e-06
Iter: 32 loss: 6.4827528152362769e-06
Iter: 33 loss: 5.8940819948327661e-06
Iter: 34 loss: 5.5173616876618456e-06
Iter: 35 loss: 8.8343090724494985e-06
Iter: 36 loss: 5.4983616055707013e-06
Iter: 37 loss: 5.1986971814330207e-06
Iter: 38 loss: 6.8060570942423627e-06
Iter: 39 loss: 5.1534775283414793e-06
Iter: 40 loss: 4.933544185059543e-06
Iter: 41 loss: 4.9325119553626352e-06
Iter: 42 loss: 4.8454866206947548e-06
Iter: 43 loss: 4.6441181415287626e-06
Iter: 44 loss: 7.1557415338031826e-06
Iter: 45 loss: 4.6290842119920406e-06
Iter: 46 loss: 4.4868382042812751e-06
Iter: 47 loss: 4.4734350873259274e-06
Iter: 48 loss: 4.3752425142624514e-06
Iter: 49 loss: 4.2421404014837176e-06
Iter: 50 loss: 4.2354409869705045e-06
Iter: 51 loss: 4.1223810696181015e-06
Iter: 52 loss: 4.1197188311421312e-06
Iter: 53 loss: 4.0402124593135431e-06
Iter: 54 loss: 4.0235046329276658e-06
Iter: 55 loss: 3.9713229423382965e-06
Iter: 56 loss: 3.879146709901978e-06
Iter: 57 loss: 3.8900016940993828e-06
Iter: 58 loss: 3.8085506464113617e-06
Iter: 59 loss: 3.7535968499632919e-06
Iter: 60 loss: 3.7412772472017368e-06
Iter: 61 loss: 3.6958079465782449e-06
Iter: 62 loss: 3.6342594238496672e-06
Iter: 63 loss: 3.6311344417242908e-06
Iter: 64 loss: 3.552762801593622e-06
Iter: 65 loss: 3.7039041661832572e-06
Iter: 66 loss: 3.5200170087122658e-06
Iter: 67 loss: 3.4306137715626678e-06
Iter: 68 loss: 3.5255758656491667e-06
Iter: 69 loss: 3.3813558635151206e-06
Iter: 70 loss: 3.2936544542103387e-06
Iter: 71 loss: 3.5710878830311333e-06
Iter: 72 loss: 3.2684171449171548e-06
Iter: 73 loss: 3.2574481241883725e-06
Iter: 74 loss: 3.230364007158244e-06
Iter: 75 loss: 3.2001611988981636e-06
Iter: 76 loss: 3.1704878154571876e-06
Iter: 77 loss: 3.16407708411336e-06
Iter: 78 loss: 3.1179814154170888e-06
Iter: 79 loss: 3.1046583850072309e-06
Iter: 80 loss: 3.0767811420831874e-06
Iter: 81 loss: 3.0462462093687642e-06
Iter: 82 loss: 3.0383170668491938e-06
Iter: 83 loss: 3.0201261328471016e-06
Iter: 84 loss: 2.9811132297975982e-06
Iter: 85 loss: 3.5966185069519909e-06
Iter: 86 loss: 2.9797783826502608e-06
Iter: 87 loss: 2.9431570915297636e-06
Iter: 88 loss: 2.9426313445477806e-06
Iter: 89 loss: 2.9210541823361669e-06
Iter: 90 loss: 2.873507807379119e-06
Iter: 91 loss: 3.5653818394052939e-06
Iter: 92 loss: 2.8713013207640517e-06
Iter: 93 loss: 2.8376764702582539e-06
Iter: 94 loss: 3.2989442769953814e-06
Iter: 95 loss: 2.8375521815666149e-06
Iter: 96 loss: 2.8072661725565073e-06
Iter: 97 loss: 2.9477270751293096e-06
Iter: 98 loss: 2.8016083379797584e-06
Iter: 99 loss: 2.7772296207091391e-06
Iter: 100 loss: 2.7681494549860991e-06
Iter: 101 loss: 2.7547123225568772e-06
Iter: 102 loss: 2.7234716286420572e-06
Iter: 103 loss: 2.7455027720869176e-06
Iter: 104 loss: 2.7040533475475673e-06
Iter: 105 loss: 2.6740735534903428e-06
Iter: 106 loss: 2.947530972231142e-06
Iter: 107 loss: 2.6727262940670915e-06
Iter: 108 loss: 2.647096943308805e-06
Iter: 109 loss: 2.6312866956884796e-06
Iter: 110 loss: 2.6209473399953616e-06
Iter: 111 loss: 2.6317109493622573e-06
Iter: 112 loss: 2.6103816837670969e-06
Iter: 113 loss: 2.5995126201952821e-06
Iter: 114 loss: 2.5777140659394032e-06
Iter: 115 loss: 2.9890312858374415e-06
Iter: 116 loss: 2.5774278256423658e-06
Iter: 117 loss: 2.5527241320704651e-06
Iter: 118 loss: 2.5926055264576924e-06
Iter: 119 loss: 2.5413389171357169e-06
Iter: 120 loss: 2.5266430496967793e-06
Iter: 121 loss: 2.5266411403424823e-06
Iter: 122 loss: 2.5109966056722507e-06
Iter: 123 loss: 2.5179224943022795e-06
Iter: 124 loss: 2.5003433110333685e-06
Iter: 125 loss: 2.4868661920415318e-06
Iter: 126 loss: 2.4815275497711225e-06
Iter: 127 loss: 2.4742989207255621e-06
Iter: 128 loss: 2.46208042210831e-06
Iter: 129 loss: 2.4609182784947367e-06
Iter: 130 loss: 2.4532377062135822e-06
Iter: 131 loss: 2.43384120614847e-06
Iter: 132 loss: 2.6087583687409735e-06
Iter: 133 loss: 2.4308942612487763e-06
Iter: 134 loss: 2.4202699940884045e-06
Iter: 135 loss: 2.4191058750124028e-06
Iter: 136 loss: 2.4087595516045554e-06
Iter: 137 loss: 2.4283797765633381e-06
Iter: 138 loss: 2.4043970796404868e-06
Iter: 139 loss: 2.3914439258839239e-06
Iter: 140 loss: 2.3760401303332036e-06
Iter: 141 loss: 2.3744230756661614e-06
Iter: 142 loss: 2.3600667916621748e-06
Iter: 143 loss: 2.3901903602288482e-06
Iter: 144 loss: 2.3543807291256808e-06
Iter: 145 loss: 2.3376625308677767e-06
Iter: 146 loss: 2.4534552720121471e-06
Iter: 147 loss: 2.3360861279476449e-06
Iter: 148 loss: 2.3306729148108411e-06
Iter: 149 loss: 2.3298805870213274e-06
Iter: 150 loss: 2.3237787937926155e-06
Iter: 151 loss: 2.3122313975486765e-06
Iter: 152 loss: 2.5630382868484594e-06
Iter: 153 loss: 2.3121928027530818e-06
Iter: 154 loss: 2.3008097288796532e-06
Iter: 155 loss: 2.3095012061322794e-06
Iter: 156 loss: 2.2938744023240781e-06
Iter: 157 loss: 2.2815433489963735e-06
Iter: 158 loss: 2.3712542124771048e-06
Iter: 159 loss: 2.280503026170779e-06
Iter: 160 loss: 2.271848050851014e-06
Iter: 161 loss: 2.3978726726943378e-06
Iter: 162 loss: 2.2718359906760139e-06
Iter: 163 loss: 2.26618986501644e-06
Iter: 164 loss: 2.2530344920154309e-06
Iter: 165 loss: 2.4135302090055634e-06
Iter: 166 loss: 2.2519905578696726e-06
Iter: 167 loss: 2.2404315603287207e-06
Iter: 168 loss: 2.3054686186576077e-06
Iter: 169 loss: 2.2388162569374988e-06
Iter: 170 loss: 2.2286471860949257e-06
Iter: 171 loss: 2.3656262771474297e-06
Iter: 172 loss: 2.2286006245224621e-06
Iter: 173 loss: 2.2239667282218826e-06
Iter: 174 loss: 2.2138348273203378e-06
Iter: 175 loss: 2.3648902446600942e-06
Iter: 176 loss: 2.2134027570088367e-06
Iter: 177 loss: 2.2036498131275874e-06
Iter: 178 loss: 2.3301286821776228e-06
Iter: 179 loss: 2.2035836223465454e-06
Iter: 180 loss: 2.1969889491153955e-06
Iter: 181 loss: 2.2563700844229934e-06
Iter: 182 loss: 2.1966787527139447e-06
Iter: 183 loss: 2.19214960175965e-06
Iter: 184 loss: 2.1829487643859473e-06
Iter: 185 loss: 2.3509131521310581e-06
Iter: 186 loss: 2.1827988230112488e-06
Iter: 187 loss: 2.1724144416932744e-06
Iter: 188 loss: 2.2076955442851892e-06
Iter: 189 loss: 2.1696265302038349e-06
Iter: 190 loss: 2.1611639833452439e-06
Iter: 191 loss: 2.2139220208424517e-06
Iter: 192 loss: 2.1601784957782976e-06
Iter: 193 loss: 2.1527609517204103e-06
Iter: 194 loss: 2.15890080605593e-06
Iter: 195 loss: 2.1483394655467146e-06
Iter: 196 loss: 2.1457394314212804e-06
Iter: 197 loss: 2.1448322340566195e-06
Iter: 198 loss: 2.1408812347292031e-06
Iter: 199 loss: 2.1367546231810286e-06
Iter: 200 loss: 2.1360267776764795e-06
Iter: 201 loss: 2.130861648877073e-06
Iter: 202 loss: 2.1633052292630447e-06
Iter: 203 loss: 2.1302683687138012e-06
Iter: 204 loss: 2.1258973040681072e-06
Iter: 205 loss: 2.1508242824635689e-06
Iter: 206 loss: 2.1253001975178154e-06
Iter: 207 loss: 2.1220465560737855e-06
Iter: 208 loss: 2.11368702269455e-06
Iter: 209 loss: 2.1835961433448866e-06
Iter: 210 loss: 2.1122443616902626e-06
Iter: 211 loss: 2.1094115666807387e-06
Iter: 212 loss: 2.1079759573484811e-06
Iter: 213 loss: 2.103623833585487e-06
Iter: 214 loss: 2.1018426712737189e-06
Iter: 215 loss: 2.0995443503695318e-06
Iter: 216 loss: 2.094753349784593e-06
Iter: 217 loss: 2.0924470407562785e-06
Iter: 218 loss: 2.0901299887908349e-06
Iter: 219 loss: 2.0843648252344042e-06
Iter: 220 loss: 2.0843376696856789e-06
Iter: 221 loss: 2.079867156145572e-06
Iter: 222 loss: 2.0886809102812171e-06
Iter: 223 loss: 2.0780255426487052e-06
Iter: 224 loss: 2.0742827289756369e-06
Iter: 225 loss: 2.0681894924941628e-06
Iter: 226 loss: 2.068153625295519e-06
Iter: 227 loss: 2.0608699006001073e-06
Iter: 228 loss: 2.0892894095244549e-06
Iter: 229 loss: 2.0591835823455654e-06
Iter: 230 loss: 2.053317806157961e-06
Iter: 231 loss: 2.1059518383763937e-06
Iter: 232 loss: 2.0530385535146577e-06
Iter: 233 loss: 2.0486141174735969e-06
Iter: 234 loss: 2.0461624757430507e-06
Iter: 235 loss: 2.0442159171837175e-06
Iter: 236 loss: 2.0423266057344248e-06
Iter: 237 loss: 2.0408007392412776e-06
Iter: 238 loss: 2.0379281986113414e-06
Iter: 239 loss: 2.0350682634257026e-06
Iter: 240 loss: 2.0344759553100559e-06
Iter: 241 loss: 2.0309333034601732e-06
Iter: 242 loss: 2.0507611418330559e-06
Iter: 243 loss: 2.0304339765591159e-06
Iter: 244 loss: 2.0274360483213717e-06
Iter: 245 loss: 2.0559719773255323e-06
Iter: 246 loss: 2.0273195749710683e-06
Iter: 247 loss: 2.0255405302046336e-06
Iter: 248 loss: 2.0212971055265428e-06
Iter: 249 loss: 2.0690248234963532e-06
Iter: 250 loss: 2.0208854853576361e-06
Iter: 251 loss: 2.01911364978747e-06
Iter: 252 loss: 2.0182546304563311e-06
Iter: 253 loss: 2.0165900418737357e-06
Iter: 254 loss: 2.0123299979620906e-06
Iter: 255 loss: 2.0485968575815026e-06
Iter: 256 loss: 2.0116157500695059e-06
Iter: 257 loss: 2.0076739248174538e-06
Iter: 258 loss: 2.0504599023001052e-06
Iter: 259 loss: 2.0075861374522623e-06
Iter: 260 loss: 2.0040458959921093e-06
Iter: 261 loss: 2.0230075009947754e-06
Iter: 262 loss: 2.0035099674700353e-06
Iter: 263 loss: 2.0002952439098285e-06
Iter: 264 loss: 1.9977690689268529e-06
Iter: 265 loss: 1.9967725703628328e-06
Iter: 266 loss: 1.9929828723409078e-06
Iter: 267 loss: 1.9977951840483064e-06
Iter: 268 loss: 1.9910323383692596e-06
Iter: 269 loss: 1.9869885372734295e-06
Iter: 270 loss: 1.999223926397095e-06
Iter: 271 loss: 1.9857772853632587e-06
Iter: 272 loss: 1.9819071653701268e-06
Iter: 273 loss: 2.0068833637454645e-06
Iter: 274 loss: 1.9814855943823596e-06
Iter: 275 loss: 1.97810796145506e-06
Iter: 276 loss: 1.976071599033958e-06
Iter: 277 loss: 1.9746820091071117e-06
Iter: 278 loss: 1.9746941068972763e-06
Iter: 279 loss: 1.972490727838705e-06
Iter: 280 loss: 1.971028468366887e-06
Iter: 281 loss: 1.9708035174871272e-06
Iter: 282 loss: 1.969788378880258e-06
Iter: 283 loss: 1.9677705585620747e-06
Iter: 284 loss: 1.9682606851797555e-06
Iter: 285 loss: 1.9662945543652318e-06
Iter: 286 loss: 1.9632608436983107e-06
Iter: 287 loss: 1.9930636935670792e-06
Iter: 288 loss: 1.9631562060039746e-06
Iter: 289 loss: 1.962040716528141e-06
Iter: 290 loss: 1.9610012088049749e-06
Iter: 291 loss: 1.9607378032134368e-06
Iter: 292 loss: 1.9582102159781592e-06
Iter: 293 loss: 1.966681811465456e-06
Iter: 294 loss: 1.9575227032043181e-06
Iter: 295 loss: 1.9554265742260502e-06
Iter: 296 loss: 1.9515283798314505e-06
Iter: 297 loss: 2.0397100680851071e-06
Iter: 298 loss: 1.951521723379976e-06
Iter: 299 loss: 1.9502336422302865e-06
Iter: 300 loss: 1.9496338931293677e-06
Iter: 301 loss: 1.9478563444839129e-06
Iter: 302 loss: 1.9477173479847562e-06
Iter: 303 loss: 1.9463917210448479e-06
Iter: 304 loss: 1.9442495692477831e-06
Iter: 305 loss: 1.944253879348008e-06
Iter: 306 loss: 1.9425371385380166e-06
Iter: 307 loss: 1.9392005006514028e-06
Iter: 308 loss: 1.938800126084684e-06
Iter: 309 loss: 1.9364071282441244e-06
Iter: 310 loss: 1.9326612895506746e-06
Iter: 311 loss: 1.9581586680707109e-06
Iter: 312 loss: 1.9322951660821594e-06
Iter: 313 loss: 1.9287723068091835e-06
Iter: 314 loss: 1.9428632114006975e-06
Iter: 315 loss: 1.9279795562801349e-06
Iter: 316 loss: 1.9257723722813356e-06
Iter: 317 loss: 1.9307481711999984e-06
Iter: 318 loss: 1.92493894334956e-06
Iter: 319 loss: 1.9229087597312723e-06
Iter: 320 loss: 1.9229071501638616e-06
Iter: 321 loss: 1.9211418660571956e-06
Iter: 322 loss: 1.9202914339841386e-06
Iter: 323 loss: 1.9194381116560072e-06
Iter: 324 loss: 1.9176309262868331e-06
Iter: 325 loss: 1.932342313134786e-06
Iter: 326 loss: 1.9175159725959408e-06
Iter: 327 loss: 1.9158244254763192e-06
Iter: 328 loss: 1.9209975921761715e-06
Iter: 329 loss: 1.9153226764616323e-06
Iter: 330 loss: 1.9140522565765394e-06
Iter: 331 loss: 1.9145266943203975e-06
Iter: 332 loss: 1.9131655880249975e-06
Iter: 333 loss: 1.9109394077101618e-06
Iter: 334 loss: 1.9165133376566718e-06
Iter: 335 loss: 1.9101600668746133e-06
Iter: 336 loss: 1.9089685771350273e-06
Iter: 337 loss: 1.9065339740912802e-06
Iter: 338 loss: 1.9503073549250675e-06
Iter: 339 loss: 1.9064904892775384e-06
Iter: 340 loss: 1.9055923951861444e-06
Iter: 341 loss: 1.9048735235367696e-06
Iter: 342 loss: 1.9039167831588404e-06
Iter: 343 loss: 1.9019714134245919e-06
Iter: 344 loss: 1.9374065781347369e-06
Iter: 345 loss: 1.9019392504774285e-06
Iter: 346 loss: 1.8995678762492605e-06
Iter: 347 loss: 1.9065796764520137e-06
Iter: 348 loss: 1.8988429695737283e-06
Iter: 349 loss: 1.8968996897385035e-06
Iter: 350 loss: 1.8973419190994041e-06
Iter: 351 loss: 1.8954703236118957e-06
Iter: 352 loss: 1.8927292262275008e-06
Iter: 353 loss: 1.9059008784272964e-06
Iter: 354 loss: 1.8922415399985607e-06
Iter: 355 loss: 1.8898597827653e-06
Iter: 356 loss: 1.9024297970611162e-06
Iter: 357 loss: 1.8894908457222466e-06
Iter: 358 loss: 1.8880921770345362e-06
Iter: 359 loss: 1.9002980966347808e-06
Iter: 360 loss: 1.8880195036004562e-06
Iter: 361 loss: 1.8864410338625012e-06
Iter: 362 loss: 1.8892075756363438e-06
Iter: 363 loss: 1.8857448426585997e-06
Iter: 364 loss: 1.884423998512288e-06
Iter: 365 loss: 1.885802513724266e-06
Iter: 366 loss: 1.88369173157208e-06
Iter: 367 loss: 1.8822862712235608e-06
Iter: 368 loss: 1.8980410397482452e-06
Iter: 369 loss: 1.8822597686539769e-06
Iter: 370 loss: 1.8811828861905554e-06
Iter: 371 loss: 1.8807873619319188e-06
Iter: 372 loss: 1.8801901981951064e-06
Iter: 373 loss: 1.8790634986098513e-06
Iter: 374 loss: 1.8948778030338744e-06
Iter: 375 loss: 1.8790604841616608e-06
Iter: 376 loss: 1.8782044720758317e-06
Iter: 377 loss: 1.8757745635767026e-06
Iter: 378 loss: 1.8874888756273585e-06
Iter: 379 loss: 1.8749347848804577e-06
Iter: 380 loss: 1.8750275241575784e-06
Iter: 381 loss: 1.8740567306005677e-06
Iter: 382 loss: 1.8731284978195923e-06
Iter: 383 loss: 1.8715376129689547e-06
Iter: 384 loss: 1.8715359334045061e-06
Iter: 385 loss: 1.8697141973325238e-06
Iter: 386 loss: 1.8748019909924738e-06
Iter: 387 loss: 1.869129434244069e-06
Iter: 388 loss: 1.8675781023771676e-06
Iter: 389 loss: 1.868629125491694e-06
Iter: 390 loss: 1.8666048005150537e-06
Iter: 391 loss: 1.8644430053440939e-06
Iter: 392 loss: 1.8675559678697743e-06
Iter: 393 loss: 1.8633899667586303e-06
Iter: 394 loss: 1.8616209705789959e-06
Iter: 395 loss: 1.8791867295667902e-06
Iter: 396 loss: 1.8615625024679453e-06
Iter: 397 loss: 1.8601185763712745e-06
Iter: 398 loss: 1.8657394413695649e-06
Iter: 399 loss: 1.8597833880039665e-06
Iter: 400 loss: 1.8585791556796228e-06
Iter: 401 loss: 1.8760553262174694e-06
Iter: 402 loss: 1.8585773559757629e-06
Iter: 403 loss: 1.8578478550530904e-06
Iter: 404 loss: 1.8567253209282512e-06
Iter: 405 loss: 1.8567081811829793e-06
Iter: 406 loss: 1.8558848981031586e-06
Iter: 407 loss: 1.8558157315574829e-06
Iter: 408 loss: 1.8551552088698859e-06
Iter: 409 loss: 1.8539101365983903e-06
Iter: 410 loss: 1.8812160021077353e-06
Iter: 411 loss: 1.8539065038214197e-06
Iter: 412 loss: 1.8524661793525049e-06
Iter: 413 loss: 1.8585584407592167e-06
Iter: 414 loss: 1.8521630975906061e-06
Iter: 415 loss: 1.8509304018329231e-06
Iter: 416 loss: 1.8621792891213287e-06
Iter: 417 loss: 1.8508749248045386e-06
Iter: 418 loss: 1.8501905199340648e-06
Iter: 419 loss: 1.8486230401318891e-06
Iter: 420 loss: 1.8688694377636968e-06
Iter: 421 loss: 1.848516584022801e-06
Iter: 422 loss: 1.8474542962273414e-06
Iter: 423 loss: 1.8474448911801237e-06
Iter: 424 loss: 1.8462147717761065e-06
Iter: 425 loss: 1.8466614140774036e-06
Iter: 426 loss: 1.8453530365274453e-06
Iter: 427 loss: 1.8440620350728252e-06
Iter: 428 loss: 1.8460154805747153e-06
Iter: 429 loss: 1.8434477209573771e-06
Iter: 430 loss: 1.8421536695756632e-06
Iter: 431 loss: 1.8415364889775187e-06
Iter: 432 loss: 1.8409071805759835e-06
Iter: 433 loss: 1.8389422184509603e-06
Iter: 434 loss: 1.846969992407389e-06
Iter: 435 loss: 1.8385110265023579e-06
Iter: 436 loss: 1.8372086836813078e-06
Iter: 437 loss: 1.8458012620240425e-06
Iter: 438 loss: 1.837072877516682e-06
Iter: 439 loss: 1.8358567127973898e-06
Iter: 440 loss: 1.8413398972876493e-06
Iter: 441 loss: 1.8356208111768842e-06
Iter: 442 loss: 1.8344341921226822e-06
Iter: 443 loss: 1.8468654115056889e-06
Iter: 444 loss: 1.8344029246687408e-06
Iter: 445 loss: 1.8338203002224684e-06
Iter: 446 loss: 1.8325529683262954e-06
Iter: 447 loss: 1.8517318914568614e-06
Iter: 448 loss: 1.8325019047842373e-06
Iter: 449 loss: 1.831311710119664e-06
Iter: 450 loss: 1.8313113138623249e-06
Iter: 451 loss: 1.830409438807817e-06
Iter: 452 loss: 1.8324706484667089e-06
Iter: 453 loss: 1.8300720890445092e-06
Iter: 454 loss: 1.8292937255787577e-06
Iter: 455 loss: 1.8290691683405974e-06
Iter: 456 loss: 1.8285982112204752e-06
Iter: 457 loss: 1.8276362968943163e-06
Iter: 458 loss: 1.8276361401103962e-06
Iter: 459 loss: 1.8271874203342273e-06
Iter: 460 loss: 1.8258906582324347e-06
Iter: 461 loss: 1.8313620754265128e-06
Iter: 462 loss: 1.8253844606632483e-06
Iter: 463 loss: 1.8244423878738453e-06
Iter: 464 loss: 1.8243033424831723e-06
Iter: 465 loss: 1.8233017234493912e-06
Iter: 466 loss: 1.8247980428859734e-06
Iter: 467 loss: 1.822822187343121e-06
Iter: 468 loss: 1.8219454890010775e-06
Iter: 469 loss: 1.8213077502922969e-06
Iter: 470 loss: 1.8210085096571467e-06
Iter: 471 loss: 1.8194624697104394e-06
Iter: 472 loss: 1.8224803386945771e-06
Iter: 473 loss: 1.8188217328185065e-06
Iter: 474 loss: 1.817589472893201e-06
Iter: 475 loss: 1.8181490178598958e-06
Iter: 476 loss: 1.8167537276509201e-06
Iter: 477 loss: 1.8156865229363656e-06
Iter: 478 loss: 1.8156347220167344e-06
Iter: 479 loss: 1.8150281324935608e-06
Iter: 480 loss: 1.8150268612843482e-06
Iter: 481 loss: 1.814634137757293e-06
Iter: 482 loss: 1.8135701118413193e-06
Iter: 483 loss: 1.8204153109834521e-06
Iter: 484 loss: 1.8133027527395211e-06
Iter: 485 loss: 1.8121556819381761e-06
Iter: 486 loss: 1.8195668270457393e-06
Iter: 487 loss: 1.8120309470543493e-06
Iter: 488 loss: 1.8110581682141496e-06
Iter: 489 loss: 1.8206373568444596e-06
Iter: 490 loss: 1.8110249235042777e-06
Iter: 491 loss: 1.8101486917568387e-06
Iter: 492 loss: 1.8105188751687669e-06
Iter: 493 loss: 1.8095477174685881e-06
Iter: 494 loss: 1.8087333397170442e-06
Iter: 495 loss: 1.808015828182425e-06
Iter: 496 loss: 1.8078032325121771e-06
Iter: 497 loss: 1.8069124337194498e-06
Iter: 498 loss: 1.8069095689293364e-06
Iter: 499 loss: 1.8059562797439237e-06
Iter: 500 loss: 1.8060643886767272e-06
Iter: 501 loss: 1.805225150797734e-06
Iter: 502 loss: 1.8044505262055839e-06
Iter: 503 loss: 1.8043530336475578e-06
Iter: 504 loss: 1.8038005840093237e-06
Iter: 505 loss: 1.8027076099599061e-06
Iter: 506 loss: 1.8110986260043383e-06
Iter: 507 loss: 1.8026265580348756e-06
Iter: 508 loss: 1.8016690155921467e-06
Iter: 509 loss: 1.8081378575388187e-06
Iter: 510 loss: 1.8015739272581716e-06
Iter: 511 loss: 1.8008827376296277e-06
Iter: 512 loss: 1.7995945199188743e-06
Iter: 513 loss: 1.8285875662172455e-06
Iter: 514 loss: 1.7995920965603379e-06
Iter: 515 loss: 1.7984967831907062e-06
Iter: 516 loss: 1.8026772276077207e-06
Iter: 517 loss: 1.7982368152960228e-06
Iter: 518 loss: 1.7975062789746506e-06
Iter: 519 loss: 1.797442964521156e-06
Iter: 520 loss: 1.7968422348914569e-06
Iter: 521 loss: 1.7959732457067288e-06
Iter: 522 loss: 1.795947115334186e-06
Iter: 523 loss: 1.7951199509178976e-06
Iter: 524 loss: 1.7962452407487689e-06
Iter: 525 loss: 1.7947065498486769e-06
Iter: 526 loss: 1.7937199650551651e-06
Iter: 527 loss: 1.7945750380235039e-06
Iter: 528 loss: 1.7931396625317507e-06
Iter: 529 loss: 1.7923442137605485e-06
Iter: 530 loss: 1.7922723982261155e-06
Iter: 531 loss: 1.7916955332427346e-06
Iter: 532 loss: 1.7913640729727445e-06
Iter: 533 loss: 1.7911172183915178e-06
Iter: 534 loss: 1.7901617772653347e-06
Iter: 535 loss: 1.7887652206321788e-06
Iter: 536 loss: 1.788727149725934e-06
Iter: 537 loss: 1.7882603391564253e-06
Iter: 538 loss: 1.7880778856716069e-06
Iter: 539 loss: 1.7873868543126843e-06
Iter: 540 loss: 1.7874389664721367e-06
Iter: 541 loss: 1.7868493497667274e-06
Iter: 542 loss: 1.7861758110376125e-06
Iter: 543 loss: 1.7866814378098089e-06
Iter: 544 loss: 1.7857637051557616e-06
Iter: 545 loss: 1.7849203759701333e-06
Iter: 546 loss: 1.7865629547980226e-06
Iter: 547 loss: 1.7845703889082279e-06
Iter: 548 loss: 1.7838778964803859e-06
Iter: 549 loss: 1.7838718555089407e-06
Iter: 550 loss: 1.7832878078531441e-06
Iter: 551 loss: 1.7824169743305737e-06
Iter: 552 loss: 1.7823975930789697e-06
Iter: 553 loss: 1.7817758620674648e-06
Iter: 554 loss: 1.789240994650365e-06
Iter: 555 loss: 1.7817681223048698e-06
Iter: 556 loss: 1.7809907835536882e-06
Iter: 557 loss: 1.7811063897649893e-06
Iter: 558 loss: 1.780402286707214e-06
Iter: 559 loss: 1.7796990134055327e-06
Iter: 560 loss: 1.7796838042725322e-06
Iter: 561 loss: 1.7791318013188205e-06
Iter: 562 loss: 1.7780667259051113e-06
Iter: 563 loss: 1.77748077979362e-06
Iter: 564 loss: 1.7770097112154452e-06
Iter: 565 loss: 1.7758639388288718e-06
Iter: 566 loss: 1.7915950389394869e-06
Iter: 567 loss: 1.775859768205951e-06
Iter: 568 loss: 1.7750342189182521e-06
Iter: 569 loss: 1.7852668429671647e-06
Iter: 570 loss: 1.7750260360560028e-06
Iter: 571 loss: 1.7744554846356663e-06
Iter: 572 loss: 1.7742989218186257e-06
Iter: 573 loss: 1.7739484813782774e-06
Iter: 574 loss: 1.7731699225052689e-06
Iter: 575 loss: 1.7722584832168258e-06
Iter: 576 loss: 1.7721557798489745e-06
Iter: 577 loss: 1.7712619344364306e-06
Iter: 578 loss: 1.7814889326693056e-06
Iter: 579 loss: 1.771246896527131e-06
Iter: 580 loss: 1.7702840781143243e-06
Iter: 581 loss: 1.7736843869823881e-06
Iter: 582 loss: 1.7700356598687105e-06
Iter: 583 loss: 1.7692586656922913e-06
Iter: 584 loss: 1.7696483969194108e-06
Iter: 585 loss: 1.768740415937709e-06
Iter: 586 loss: 1.7679417753260425e-06
Iter: 587 loss: 1.7684991172733386e-06
Iter: 588 loss: 1.7674441859633495e-06
Iter: 589 loss: 1.7671232706862241e-06
Iter: 590 loss: 1.7670248195892551e-06
Iter: 591 loss: 1.7665270154803338e-06
Iter: 592 loss: 1.7658099415367538e-06
Iter: 593 loss: 1.7657875304396912e-06
Iter: 594 loss: 1.7651737704001129e-06
Iter: 595 loss: 1.7719096683083999e-06
Iter: 596 loss: 1.7651608343501982e-06
Iter: 597 loss: 1.7645484943304535e-06
Iter: 598 loss: 1.7640476717775369e-06
Iter: 599 loss: 1.7638681452350615e-06
Iter: 600 loss: 1.7630596864024175e-06
Iter: 601 loss: 1.7635813576560443e-06
Iter: 602 loss: 1.7625467880023403e-06
Iter: 603 loss: 1.7615255365903111e-06
Iter: 604 loss: 1.7633469497313213e-06
Iter: 605 loss: 1.7610793822336362e-06
Iter: 606 loss: 1.7602613756191502e-06
Iter: 607 loss: 1.7669478912242102e-06
Iter: 608 loss: 1.7602099509233171e-06
Iter: 609 loss: 1.759391625742781e-06
Iter: 610 loss: 1.7648433211505313e-06
Iter: 611 loss: 1.759307980406123e-06
Iter: 612 loss: 1.7587238806047074e-06
Iter: 613 loss: 1.7586613805968998e-06
Iter: 614 loss: 1.7582373031538481e-06
Iter: 615 loss: 1.7575007435888437e-06
Iter: 616 loss: 1.7569975294696776e-06
Iter: 617 loss: 1.7567281647601656e-06
Iter: 618 loss: 1.7557401614980679e-06
Iter: 619 loss: 1.7613901798182082e-06
Iter: 620 loss: 1.7556058342692779e-06
Iter: 621 loss: 1.7549372079113745e-06
Iter: 622 loss: 1.7549371705694381e-06
Iter: 623 loss: 1.7543179294076511e-06
Iter: 624 loss: 1.7537237121546901e-06
Iter: 625 loss: 1.7535856741287574e-06
Iter: 626 loss: 1.7526853359079327e-06
Iter: 627 loss: 1.7563120540440984e-06
Iter: 628 loss: 1.7524843909786535e-06
Iter: 629 loss: 1.7521296239771496e-06
Iter: 630 loss: 1.7520446090055339e-06
Iter: 631 loss: 1.7517838220872942e-06
Iter: 632 loss: 1.7509594047427761e-06
Iter: 633 loss: 1.7520868308044911e-06
Iter: 634 loss: 1.7503568301805782e-06
Iter: 635 loss: 1.7495054527581474e-06
Iter: 636 loss: 1.761967856174328e-06
Iter: 637 loss: 1.7495043940878e-06
Iter: 638 loss: 1.7488620891658631e-06
Iter: 639 loss: 1.7538157871740208e-06
Iter: 640 loss: 1.7488150037931036e-06
Iter: 641 loss: 1.7481479415833651e-06
Iter: 642 loss: 1.7486977749536368e-06
Iter: 643 loss: 1.7477498841964707e-06
Iter: 644 loss: 1.7471564112021139e-06
Iter: 645 loss: 1.7463854796172717e-06
Iter: 646 loss: 1.7463344900503564e-06
Iter: 647 loss: 1.746095667649061e-06
Iter: 648 loss: 1.7459210940421163e-06
Iter: 649 loss: 1.7454507644864148e-06
Iter: 650 loss: 1.7449873794141782e-06
Iter: 651 loss: 1.7448881537948343e-06
Iter: 652 loss: 1.7440351673608652e-06
Iter: 653 loss: 1.7446745746619235e-06
Iter: 654 loss: 1.7435131345211532e-06
Iter: 655 loss: 1.7426196759314779e-06
Iter: 656 loss: 1.7436594200952795e-06
Iter: 657 loss: 1.7421435768472755e-06
Iter: 658 loss: 1.74113942609307e-06
Iter: 659 loss: 1.7424062112711238e-06
Iter: 660 loss: 1.7406212541608432e-06
Iter: 661 loss: 1.7403770964730995e-06
Iter: 662 loss: 1.7401340837405475e-06
Iter: 663 loss: 1.7396987946983532e-06
Iter: 664 loss: 1.7405972733934808e-06
Iter: 665 loss: 1.7395245406788991e-06
Iter: 666 loss: 1.7390875295046758e-06
Iter: 667 loss: 1.7383031480114114e-06
Iter: 668 loss: 1.757510073020298e-06
Iter: 669 loss: 1.7383030893935646e-06
Iter: 670 loss: 1.7375193647466572e-06
Iter: 671 loss: 1.7375125359945197e-06
Iter: 672 loss: 1.7372026785144495e-06
Iter: 673 loss: 1.7363821823522983e-06
Iter: 674 loss: 1.7423339627772342e-06
Iter: 675 loss: 1.7362064636179675e-06
Iter: 676 loss: 1.7352181647365398e-06
Iter: 677 loss: 1.744021858317152e-06
Iter: 678 loss: 1.7351700278605729e-06
Iter: 679 loss: 1.7345167868690208e-06
Iter: 680 loss: 1.7366863577974855e-06
Iter: 681 loss: 1.7343374984389724e-06
Iter: 682 loss: 1.7336191681672715e-06
Iter: 683 loss: 1.7399055987040307e-06
Iter: 684 loss: 1.7335821655585666e-06
Iter: 685 loss: 1.7330910671913586e-06
Iter: 686 loss: 1.7320912818980221e-06
Iter: 687 loss: 1.7502476100746663e-06
Iter: 688 loss: 1.7320744321563272e-06
Iter: 689 loss: 1.7318530584644706e-06
Iter: 690 loss: 1.7315390519775073e-06
Iter: 691 loss: 1.7311729814233953e-06
Iter: 692 loss: 1.730547276863964e-06
Iter: 693 loss: 1.7305465172601626e-06
Iter: 694 loss: 1.7299160674191353e-06
Iter: 695 loss: 1.7319952039061439e-06
Iter: 696 loss: 1.7297418196161029e-06
Iter: 697 loss: 1.729039942491627e-06
Iter: 698 loss: 1.7282989576584097e-06
Iter: 699 loss: 1.7281731222382157e-06
Iter: 700 loss: 1.7285548563579002e-06
Iter: 701 loss: 1.7277579571768156e-06
Iter: 702 loss: 1.7274193242862437e-06
Iter: 703 loss: 1.7269108975561137e-06
Iter: 704 loss: 1.7269004130284676e-06
Iter: 705 loss: 1.7262699307864865e-06
Iter: 706 loss: 1.7264774725103275e-06
Iter: 707 loss: 1.725822876544959e-06
Iter: 708 loss: 1.7252980687293237e-06
Iter: 709 loss: 1.7252941191790557e-06
Iter: 710 loss: 1.7247821027434549e-06
Iter: 711 loss: 1.7239400414961436e-06
Iter: 712 loss: 1.7239361378792495e-06
Iter: 713 loss: 1.7231224528165628e-06
Iter: 714 loss: 1.7239675929841834e-06
Iter: 715 loss: 1.7226706111919076e-06
Iter: 716 loss: 1.7217443895738053e-06
Iter: 717 loss: 1.7264630622887984e-06
Iter: 718 loss: 1.72159297966556e-06
Iter: 719 loss: 1.7208092741235453e-06
Iter: 720 loss: 1.7325789045434043e-06
Iter: 721 loss: 1.7208087781257325e-06
Iter: 722 loss: 1.720392982389491e-06
Iter: 723 loss: 1.7203758844206097e-06
Iter: 724 loss: 1.7200551581645504e-06
Iter: 725 loss: 1.7196074099986e-06
Iter: 726 loss: 1.7228786076628546e-06
Iter: 727 loss: 1.719569990990784e-06
Iter: 728 loss: 1.7190067966706062e-06
Iter: 729 loss: 1.7180746930177488e-06
Iter: 730 loss: 1.7180710291516866e-06
Iter: 731 loss: 1.7173312397725944e-06
Iter: 732 loss: 1.7174518536533269e-06
Iter: 733 loss: 1.7167740848547016e-06
Iter: 734 loss: 1.7158043026306284e-06
Iter: 735 loss: 1.7246624170477688e-06
Iter: 736 loss: 1.7157608409560003e-06
Iter: 737 loss: 1.7153629995494087e-06
Iter: 738 loss: 1.7153160992420091e-06
Iter: 739 loss: 1.7149911558146021e-06
Iter: 740 loss: 1.7145194281364185e-06
Iter: 741 loss: 1.7145057022413591e-06
Iter: 742 loss: 1.7140019550498369e-06
Iter: 743 loss: 1.7137717493054669e-06
Iter: 744 loss: 1.7135206288073256e-06
Iter: 745 loss: 1.7127438143097265e-06
Iter: 746 loss: 1.7187081119086985e-06
Iter: 747 loss: 1.7126862195889018e-06
Iter: 748 loss: 1.7120260994960795e-06
Iter: 749 loss: 1.7159656646530951e-06
Iter: 750 loss: 1.7119427812635038e-06
Iter: 751 loss: 1.7113404270054998e-06
Iter: 752 loss: 1.7105284252839918e-06
Iter: 753 loss: 1.7104859590997529e-06
Iter: 754 loss: 1.7097803038829242e-06
Iter: 755 loss: 1.7107822702572001e-06
Iter: 756 loss: 1.709434339457603e-06
Iter: 757 loss: 1.7091549838816795e-06
Iter: 758 loss: 1.708988123011633e-06
Iter: 759 loss: 1.7086420267591442e-06
Iter: 760 loss: 1.708246191586893e-06
Iter: 761 loss: 1.7081968667520084e-06
Iter: 762 loss: 1.7075942758054208e-06
Iter: 763 loss: 1.7091705826788536e-06
Iter: 764 loss: 1.7073903138267934e-06
Iter: 765 loss: 1.7066637988144347e-06
Iter: 766 loss: 1.7106548204986767e-06
Iter: 767 loss: 1.7065581867167187e-06
Iter: 768 loss: 1.7061457866323352e-06
Iter: 769 loss: 1.7052232202372062e-06
Iter: 770 loss: 1.718058916239705e-06
Iter: 771 loss: 1.7051732562381354e-06
Iter: 772 loss: 1.7043725611341137e-06
Iter: 773 loss: 1.7130571942499201e-06
Iter: 774 loss: 1.7043546400907847e-06
Iter: 775 loss: 1.7036743667903777e-06
Iter: 776 loss: 1.7120670022624871e-06
Iter: 777 loss: 1.703667393301331e-06
Iter: 778 loss: 1.7032820910021932e-06
Iter: 779 loss: 1.7026498761840333e-06
Iter: 780 loss: 1.7026467737731909e-06
Iter: 781 loss: 1.7019260543851641e-06
Iter: 782 loss: 1.7026296799974607e-06
Iter: 783 loss: 1.7015174577115141e-06
Iter: 784 loss: 1.7007548350338318e-06
Iter: 785 loss: 1.7037895345509607e-06
Iter: 786 loss: 1.7005821846718013e-06
Iter: 787 loss: 1.6998417643132956e-06
Iter: 788 loss: 1.708738515334565e-06
Iter: 789 loss: 1.6998325886559434e-06
Iter: 790 loss: 1.6994470070707346e-06
Iter: 791 loss: 1.6995136131813562e-06
Iter: 792 loss: 1.6991576722742807e-06
Iter: 793 loss: 1.6985684644535795e-06
Iter: 794 loss: 1.6978811558606075e-06
Iter: 795 loss: 1.6978024586795486e-06
Iter: 796 loss: 1.6972715571983437e-06
Iter: 797 loss: 1.6972175784784983e-06
Iter: 798 loss: 1.6966181659172381e-06
Iter: 799 loss: 1.6968825492742518e-06
Iter: 800 loss: 1.6962097454934617e-06
Iter: 801 loss: 1.6956926022306661e-06
Iter: 802 loss: 1.6961101002481018e-06
Iter: 803 loss: 1.6953822195075659e-06
Iter: 804 loss: 1.6946679107383407e-06
Iter: 805 loss: 1.6999641109856883e-06
Iter: 806 loss: 1.6946102456394733e-06
Iter: 807 loss: 1.6940672961045694e-06
Iter: 808 loss: 1.6939358888966184e-06
Iter: 809 loss: 1.6935908658125504e-06
Iter: 810 loss: 1.6931004830978724e-06
Iter: 811 loss: 1.6935768560598137e-06
Iter: 812 loss: 1.6928220203249608e-06
Iter: 813 loss: 1.6920246461017357e-06
Iter: 814 loss: 1.698542282478346e-06
Iter: 815 loss: 1.691974470092399e-06
Iter: 816 loss: 1.6916277333134527e-06
Iter: 817 loss: 1.6910142971391339e-06
Iter: 818 loss: 1.6910142854823739e-06
Iter: 819 loss: 1.6903794638990214e-06
Iter: 820 loss: 1.6906060481419296e-06
Iter: 821 loss: 1.6899337862808955e-06
Iter: 822 loss: 1.6888870070944292e-06
Iter: 823 loss: 1.6929442537920337e-06
Iter: 824 loss: 1.6886427572603542e-06
Iter: 825 loss: 1.6880242424203463e-06
Iter: 826 loss: 1.6953415418009551e-06
Iter: 827 loss: 1.6880157554128723e-06
Iter: 828 loss: 1.6873694039801241e-06
Iter: 829 loss: 1.6884930624247185e-06
Iter: 830 loss: 1.6870830390673348e-06
Iter: 831 loss: 1.6866037878757801e-06
Iter: 832 loss: 1.6864473291816637e-06
Iter: 833 loss: 1.6861690905985964e-06
Iter: 834 loss: 1.6853997492883332e-06
Iter: 835 loss: 1.6900619683671513e-06
Iter: 836 loss: 1.6853053231714263e-06
Iter: 837 loss: 1.6846752093055554e-06
Iter: 838 loss: 1.6904520437534823e-06
Iter: 839 loss: 1.6846473035607277e-06
Iter: 840 loss: 1.684321574230101e-06
Iter: 841 loss: 1.6836180669380297e-06
Iter: 842 loss: 1.6944902670690211e-06
Iter: 843 loss: 1.6835919216036695e-06
Iter: 844 loss: 1.6829638858088072e-06
Iter: 845 loss: 1.6829618375936297e-06
Iter: 846 loss: 1.6824292719003517e-06
Iter: 847 loss: 1.6832675835265007e-06
Iter: 848 loss: 1.6821807583069836e-06
Iter: 849 loss: 1.6817124989089281e-06
Iter: 850 loss: 1.6822726090907475e-06
Iter: 851 loss: 1.6814656111677775e-06
Iter: 852 loss: 1.6809589378808737e-06
Iter: 853 loss: 1.6879702149847664e-06
Iter: 854 loss: 1.6809572804693131e-06
Iter: 855 loss: 1.6806359027164813e-06
Iter: 856 loss: 1.6797559460200146e-06
Iter: 857 loss: 1.6850897559712374e-06
Iter: 858 loss: 1.6795181632276608e-06
Iter: 859 loss: 1.6785478697093412e-06
Iter: 860 loss: 1.6803355744739045e-06
Iter: 861 loss: 1.6781317570881712e-06
Iter: 862 loss: 1.67738972531508e-06
Iter: 863 loss: 1.6890687202237925e-06
Iter: 864 loss: 1.6773896930442109e-06
Iter: 865 loss: 1.6768227469370614e-06
Iter: 866 loss: 1.6793417338249792e-06
Iter: 867 loss: 1.6767106412732693e-06
Iter: 868 loss: 1.6760077566621311e-06
Iter: 869 loss: 1.677026516127762e-06
Iter: 870 loss: 1.6756663985319022e-06
Iter: 871 loss: 1.6752212901941489e-06
Iter: 872 loss: 1.6747326600596745e-06
Iter: 873 loss: 1.6746608523607506e-06
Iter: 874 loss: 1.6741142669975745e-06
Iter: 875 loss: 1.6740675349922444e-06
Iter: 876 loss: 1.6736849062594393e-06
Iter: 877 loss: 1.673460970845417e-06
Iter: 878 loss: 1.6732996383227614e-06
Iter: 879 loss: 1.6727936771955326e-06
Iter: 880 loss: 1.6731820607945885e-06
Iter: 881 loss: 1.6724858656669452e-06
Iter: 882 loss: 1.671849909704091e-06
Iter: 883 loss: 1.6778315914547705e-06
Iter: 884 loss: 1.6718241132275856e-06
Iter: 885 loss: 1.6712341288176478e-06
Iter: 886 loss: 1.6708002769213854e-06
Iter: 887 loss: 1.6706014970969882e-06
Iter: 888 loss: 1.6702371587140109e-06
Iter: 889 loss: 1.6701916699954635e-06
Iter: 890 loss: 1.6698209499141872e-06
Iter: 891 loss: 1.669346068208875e-06
Iter: 892 loss: 1.669311992809144e-06
Iter: 893 loss: 1.6686417749638684e-06
Iter: 894 loss: 1.6684140236587372e-06
Iter: 895 loss: 1.6680306876717151e-06
Iter: 896 loss: 1.6672656240578978e-06
Iter: 897 loss: 1.6677839068792916e-06
Iter: 898 loss: 1.6667855907861883e-06
Iter: 899 loss: 1.6660202208159772e-06
Iter: 900 loss: 1.6738932532592064e-06
Iter: 901 loss: 1.6659983673520586e-06
Iter: 902 loss: 1.6652950146045129e-06
Iter: 903 loss: 1.6701803832357782e-06
Iter: 904 loss: 1.665229176746248e-06
Iter: 905 loss: 1.6646336816477115e-06
Iter: 906 loss: 1.6649502914098584e-06
Iter: 907 loss: 1.664240640590482e-06
Iter: 908 loss: 1.6637122204467327e-06
Iter: 909 loss: 1.6641020648263685e-06
Iter: 910 loss: 1.6633874712923987e-06
Iter: 911 loss: 1.66263038179366e-06
Iter: 912 loss: 1.6707734474512495e-06
Iter: 913 loss: 1.6626127337692371e-06
Iter: 914 loss: 1.662261283360771e-06
Iter: 915 loss: 1.6615263661810415e-06
Iter: 916 loss: 1.673951417041871e-06
Iter: 917 loss: 1.6615080634861353e-06
Iter: 918 loss: 1.6609033075581298e-06
Iter: 919 loss: 1.6609021320832883e-06
Iter: 920 loss: 1.6602985502361376e-06
Iter: 921 loss: 1.6607824096873303e-06
Iter: 922 loss: 1.65993554781494e-06
Iter: 923 loss: 1.6594138988220827e-06
Iter: 924 loss: 1.662517880627323e-06
Iter: 925 loss: 1.6593477380453024e-06
Iter: 926 loss: 1.6588455631116024e-06
Iter: 927 loss: 1.6607531774321309e-06
Iter: 928 loss: 1.6587257308143286e-06
Iter: 929 loss: 1.6582612815164736e-06
Iter: 930 loss: 1.6573201989161716e-06
Iter: 931 loss: 1.6746288628202869e-06
Iter: 932 loss: 1.6573055070564581e-06
Iter: 933 loss: 1.6564969850025213e-06
Iter: 934 loss: 1.6598751228781794e-06
Iter: 935 loss: 1.6563243291697303e-06
Iter: 936 loss: 1.6554215349629168e-06
Iter: 937 loss: 1.6564853278975e-06
Iter: 938 loss: 1.6549427262938398e-06
Iter: 939 loss: 1.6543626508721695e-06
Iter: 940 loss: 1.6603137825662104e-06
Iter: 941 loss: 1.6543458895517618e-06
Iter: 942 loss: 1.6537532805135413e-06
Iter: 943 loss: 1.6565014883902588e-06
Iter: 944 loss: 1.6536425664744716e-06
Iter: 945 loss: 1.6531307577689913e-06
Iter: 946 loss: 1.6523242032233216e-06
Iter: 947 loss: 1.6523154884355794e-06
Iter: 948 loss: 1.651992984068764e-06
Iter: 949 loss: 1.6518538595917404e-06
Iter: 950 loss: 1.6514687513147848e-06
Iter: 951 loss: 1.6514841971920306e-06
Iter: 952 loss: 1.6511652497901492e-06
Iter: 953 loss: 1.6506479983122659e-06
Iter: 954 loss: 1.6499324643237895e-06
Iter: 955 loss: 1.6499013218925956e-06
Iter: 956 loss: 1.6498443877309037e-06
Iter: 957 loss: 1.6495746591342538e-06
Iter: 958 loss: 1.6492482451519509e-06
Iter: 959 loss: 1.6484273421947507e-06
Iter: 960 loss: 1.6559619338117326e-06
Iter: 961 loss: 1.6483064619650342e-06
Iter: 962 loss: 1.6479039087958062e-06
Iter: 963 loss: 1.6477393603106806e-06
Iter: 964 loss: 1.6473874523346025e-06
Iter: 965 loss: 1.6466984197780054e-06
Iter: 966 loss: 1.660497846701082e-06
Iter: 967 loss: 1.6466927515587143e-06
Iter: 968 loss: 1.6459372757262695e-06
Iter: 969 loss: 1.6476322402556631e-06
Iter: 970 loss: 1.6456509881372958e-06
Iter: 971 loss: 1.644990296887103e-06
Iter: 972 loss: 1.6460982857240921e-06
Iter: 973 loss: 1.6446918980751337e-06
Iter: 974 loss: 1.6439079560017914e-06
Iter: 975 loss: 1.646470143430845e-06
Iter: 976 loss: 1.6436893479655007e-06
Iter: 977 loss: 1.6429127077405804e-06
Iter: 978 loss: 1.6447497002992089e-06
Iter: 979 loss: 1.6426291831701654e-06
Iter: 980 loss: 1.641987342903477e-06
Iter: 981 loss: 1.6419815891554051e-06
Iter: 982 loss: 1.6416082392719516e-06
Iter: 983 loss: 1.64076213985561e-06
Iter: 984 loss: 1.652071344155525e-06
Iter: 985 loss: 1.640710134556079e-06
Iter: 986 loss: 1.6406403077537541e-06
Iter: 987 loss: 1.6403199984073378e-06
Iter: 988 loss: 1.6399874212814246e-06
Iter: 989 loss: 1.6392652222359579e-06
Iter: 990 loss: 1.6502477163686522e-06
Iter: 991 loss: 1.6392366679523709e-06
Iter: 992 loss: 1.638601661865665e-06
Iter: 993 loss: 1.6428003433759781e-06
Iter: 994 loss: 1.6385357356052518e-06
Iter: 995 loss: 1.6380914068690153e-06
Iter: 996 loss: 1.6428891265266424e-06
Iter: 997 loss: 1.6380812537090683e-06
Iter: 998 loss: 1.6376318403468304e-06
Iter: 999 loss: 1.6371036732833783e-06
Iter: 1000 loss: 1.6370451735047914e-06
Iter: 1001 loss: 1.6366649341060261e-06
Iter: 1002 loss: 1.6366140071232063e-06
Iter: 1003 loss: 1.6363706749480264e-06
Iter: 1004 loss: 1.635669255400492e-06
Iter: 1005 loss: 1.6386880198369987e-06
Iter: 1006 loss: 1.6354001505714539e-06
Iter: 1007 loss: 1.6345201157865681e-06
Iter: 1008 loss: 1.6394093716424225e-06
Iter: 1009 loss: 1.6343945109361571e-06
Iter: 1010 loss: 1.6336308557790244e-06
Iter: 1011 loss: 1.6355324686474721e-06
Iter: 1012 loss: 1.63336245964845e-06
Iter: 1013 loss: 1.6325705811139568e-06
Iter: 1014 loss: 1.6336168176260789e-06
Iter: 1015 loss: 1.6321696633833391e-06
Iter: 1016 loss: 1.6314778311413674e-06
Iter: 1017 loss: 1.6314758427385071e-06
Iter: 1018 loss: 1.6309745883112169e-06
Iter: 1019 loss: 1.6329479169527173e-06
Iter: 1020 loss: 1.6308597312903918e-06
Iter: 1021 loss: 1.6303940110146686e-06
Iter: 1022 loss: 1.6297931743519867e-06
Iter: 1023 loss: 1.629751776829078e-06
Iter: 1024 loss: 1.6293719223655612e-06
Iter: 1025 loss: 1.6292920313950475e-06
Iter: 1026 loss: 1.6289268001260599e-06
Iter: 1027 loss: 1.6283319151120588e-06
Iter: 1028 loss: 1.6283284542052543e-06
Iter: 1029 loss: 1.6278096303772948e-06
Iter: 1030 loss: 1.6285067790001782e-06
Iter: 1031 loss: 1.6275489069994695e-06
Iter: 1032 loss: 1.6269003238110227e-06
Iter: 1033 loss: 1.6353183942693415e-06
Iter: 1034 loss: 1.6268959703019569e-06
Iter: 1035 loss: 1.6264724221063849e-06
Iter: 1036 loss: 1.6266040055885848e-06
Iter: 1037 loss: 1.6261700876910152e-06
Iter: 1038 loss: 1.6256373696765693e-06
Iter: 1039 loss: 1.6292668373369684e-06
Iter: 1040 loss: 1.6255854035891189e-06
Iter: 1041 loss: 1.6251468549903876e-06
Iter: 1042 loss: 1.6242540494890107e-06
Iter: 1043 loss: 1.6404698929194185e-06
Iter: 1044 loss: 1.6242390052301371e-06
Iter: 1045 loss: 1.6233262573410962e-06
Iter: 1046 loss: 1.624030077714072e-06
Iter: 1047 loss: 1.6227716535146307e-06
Iter: 1048 loss: 1.6219346692888827e-06
Iter: 1049 loss: 1.6290538650198009e-06
Iter: 1050 loss: 1.6218876865736053e-06
Iter: 1051 loss: 1.6211994398264988e-06
Iter: 1052 loss: 1.6223018294649704e-06
Iter: 1053 loss: 1.6208810549559473e-06
Iter: 1054 loss: 1.6199813209549214e-06
Iter: 1055 loss: 1.6210745485621212e-06
Iter: 1056 loss: 1.6195099155863977e-06
Iter: 1057 loss: 1.6188834453599859e-06
Iter: 1058 loss: 1.6281603366038443e-06
Iter: 1059 loss: 1.6188828505637984e-06
Iter: 1060 loss: 1.6182353854684359e-06
Iter: 1061 loss: 1.6206385455546812e-06
Iter: 1062 loss: 1.6180769063417199e-06
Iter: 1063 loss: 1.6175768871241129e-06
Iter: 1064 loss: 1.6182981039420152e-06
Iter: 1065 loss: 1.6173335370258423e-06
Iter: 1066 loss: 1.6168591991725932e-06
Iter: 1067 loss: 1.6193114728966371e-06
Iter: 1068 loss: 1.616783359788359e-06
Iter: 1069 loss: 1.616235795868144e-06
Iter: 1070 loss: 1.6160637619091948e-06
Iter: 1071 loss: 1.6157415240653582e-06
Iter: 1072 loss: 1.6153271034165852e-06
Iter: 1073 loss: 1.6192925199035758e-06
Iter: 1074 loss: 1.615311308088657e-06
Iter: 1075 loss: 1.6147985305680664e-06
Iter: 1076 loss: 1.6147129297278079e-06
Iter: 1077 loss: 1.6143614883389506e-06
Iter: 1078 loss: 1.6137527585661742e-06
Iter: 1079 loss: 1.6144337479916048e-06
Iter: 1080 loss: 1.613423540306797e-06
Iter: 1081 loss: 1.6130044379299689e-06
Iter: 1082 loss: 1.6185390805446356e-06
Iter: 1083 loss: 1.6130020607912939e-06
Iter: 1084 loss: 1.612546087364584e-06
Iter: 1085 loss: 1.611657722705857e-06
Iter: 1086 loss: 1.6296679764784852e-06
Iter: 1087 loss: 1.6116511806230007e-06
Iter: 1088 loss: 1.6108909685242644e-06
Iter: 1089 loss: 1.6128628021785565e-06
Iter: 1090 loss: 1.6106319388872673e-06
Iter: 1091 loss: 1.6099276468554602e-06
Iter: 1092 loss: 1.6108446785235438e-06
Iter: 1093 loss: 1.6095689330827806e-06
Iter: 1094 loss: 1.6087085810153997e-06
Iter: 1095 loss: 1.6136532390583112e-06
Iter: 1096 loss: 1.6085926124557764e-06
Iter: 1097 loss: 1.6078766151001249e-06
Iter: 1098 loss: 1.6076678838942156e-06
Iter: 1099 loss: 1.6072360573172982e-06
Iter: 1100 loss: 1.6071505741061967e-06
Iter: 1101 loss: 1.6067090357949087e-06
Iter: 1102 loss: 1.6064120923280328e-06
Iter: 1103 loss: 1.6057231806802305e-06
Iter: 1104 loss: 1.6142495335496716e-06
Iter: 1105 loss: 1.6056705553975364e-06
Iter: 1106 loss: 1.6050788339587188e-06
Iter: 1107 loss: 1.6050788337039643e-06
Iter: 1108 loss: 1.6046578602329835e-06
Iter: 1109 loss: 1.6065515952643648e-06
Iter: 1110 loss: 1.604575939931734e-06
Iter: 1111 loss: 1.6042193543572623e-06
Iter: 1112 loss: 1.6042969882242925e-06
Iter: 1113 loss: 1.6039561363574379e-06
Iter: 1114 loss: 1.6032823550873579e-06
Iter: 1115 loss: 1.6047411889418614e-06
Iter: 1116 loss: 1.6030208533130544e-06
Iter: 1117 loss: 1.6026637927657829e-06
Iter: 1118 loss: 1.6023371946620341e-06
Iter: 1119 loss: 1.6022498922572579e-06
Iter: 1120 loss: 1.6015917579560809e-06
Iter: 1121 loss: 1.6050796046392861e-06
Iter: 1122 loss: 1.6014904821073346e-06
Iter: 1123 loss: 1.600926847402673e-06
Iter: 1124 loss: 1.6051908871177278e-06
Iter: 1125 loss: 1.6008835183553815e-06
Iter: 1126 loss: 1.6004732641548107e-06
Iter: 1127 loss: 1.599740026134814e-06
Iter: 1128 loss: 1.6178739290204133e-06
Iter: 1129 loss: 1.5997400096264e-06
Iter: 1130 loss: 1.5989125871484648e-06
Iter: 1131 loss: 1.5997874556277425e-06
Iter: 1132 loss: 1.598455985711259e-06
Iter: 1133 loss: 1.5975421388268167e-06
Iter: 1134 loss: 1.6025465718526161e-06
Iter: 1135 loss: 1.597408622540963e-06
Iter: 1136 loss: 1.5965216308560773e-06
Iter: 1137 loss: 1.598273842131414e-06
Iter: 1138 loss: 1.5961566790859465e-06
Iter: 1139 loss: 1.5961640655866978e-06
Iter: 1140 loss: 1.595837178656261e-06
Iter: 1141 loss: 1.5955536859897085e-06
Iter: 1142 loss: 1.5946904981145402e-06
Iter: 1143 loss: 1.5968726822178735e-06
Iter: 1144 loss: 1.5942080157594778e-06
Iter: 1145 loss: 1.5941306011559435e-06
Iter: 1146 loss: 1.5937814712307878e-06
Iter: 1147 loss: 1.5933420150686413e-06
Iter: 1148 loss: 1.5933037929310745e-06
Iter: 1149 loss: 1.592978743537754e-06
Iter: 1150 loss: 1.5924726584988985e-06
Iter: 1151 loss: 1.5953955533918208e-06
Iter: 1152 loss: 1.5924049912794908e-06
Iter: 1153 loss: 1.5918971177040948e-06
Iter: 1154 loss: 1.5919819398792282e-06
Iter: 1155 loss: 1.5915152093154541e-06
Iter: 1156 loss: 1.5910050968610022e-06
Iter: 1157 loss: 1.5910311590766689e-06
Iter: 1158 loss: 1.5906047066986978e-06
Iter: 1159 loss: 1.5900692608436322e-06
Iter: 1160 loss: 1.5944481497142643e-06
Iter: 1161 loss: 1.5900356390806378e-06
Iter: 1162 loss: 1.5894547506819824e-06
Iter: 1163 loss: 1.5912412089532239e-06
Iter: 1164 loss: 1.5892833325927967e-06
Iter: 1165 loss: 1.5888805752387272e-06
Iter: 1166 loss: 1.5886339379193642e-06
Iter: 1167 loss: 1.5884704250145523e-06
Iter: 1168 loss: 1.5877845727596448e-06
Iter: 1169 loss: 1.5892104397460057e-06
Iter: 1170 loss: 1.5875113197951714e-06
Iter: 1171 loss: 1.5867344711509317e-06
Iter: 1172 loss: 1.5866366338862069e-06
Iter: 1173 loss: 1.5860825806384556e-06
Iter: 1174 loss: 1.5852868747073711e-06
Iter: 1175 loss: 1.5941445669458808e-06
Iter: 1176 loss: 1.58527131028033e-06
Iter: 1177 loss: 1.5847343354857916e-06
Iter: 1178 loss: 1.5919849191148913e-06
Iter: 1179 loss: 1.5847319456141158e-06
Iter: 1180 loss: 1.5842309941247752e-06
Iter: 1181 loss: 1.5839425247990702e-06
Iter: 1182 loss: 1.5837285137648279e-06
Iter: 1183 loss: 1.583253402677747e-06
Iter: 1184 loss: 1.5844422397193241e-06
Iter: 1185 loss: 1.5830870256978376e-06
Iter: 1186 loss: 1.5824824912466789e-06
Iter: 1187 loss: 1.5872513699382869e-06
Iter: 1188 loss: 1.5824406498265653e-06
Iter: 1189 loss: 1.5821114437392423e-06
Iter: 1190 loss: 1.5813900054615286e-06
Iter: 1191 loss: 1.5920705215081381e-06
Iter: 1192 loss: 1.5813584628046543e-06
Iter: 1193 loss: 1.5806167534035988e-06
Iter: 1194 loss: 1.5806131707458179e-06
Iter: 1195 loss: 1.5803066732007787e-06
Iter: 1196 loss: 1.5797320181329611e-06
Iter: 1197 loss: 1.592492033361087e-06
Iter: 1198 loss: 1.5797306408702769e-06
Iter: 1199 loss: 1.5789984333387117e-06
Iter: 1200 loss: 1.5818100818125435e-06
Iter: 1201 loss: 1.5788258149714192e-06
Iter: 1202 loss: 1.5782311655184069e-06
Iter: 1203 loss: 1.5864500496501828e-06
Iter: 1204 loss: 1.5782291846096911e-06
Iter: 1205 loss: 1.5777701895154974e-06
Iter: 1206 loss: 1.5771189505054235e-06
Iter: 1207 loss: 1.5770957524464033e-06
Iter: 1208 loss: 1.5764023532884035e-06
Iter: 1209 loss: 1.5764267716936004e-06
Iter: 1210 loss: 1.5758548994912037e-06
Iter: 1211 loss: 1.5750128716590344e-06
Iter: 1212 loss: 1.5823090504160169e-06
Iter: 1213 loss: 1.5749681249550376e-06
Iter: 1214 loss: 1.5742709819076004e-06
Iter: 1215 loss: 1.5751579520470511e-06
Iter: 1216 loss: 1.5739125126590653e-06
Iter: 1217 loss: 1.5738053774594067e-06
Iter: 1218 loss: 1.5735773657431791e-06
Iter: 1219 loss: 1.5733137569793147e-06
Iter: 1220 loss: 1.5726887225289697e-06
Iter: 1221 loss: 1.5798716542661119e-06
Iter: 1222 loss: 1.5726310602930709e-06
Iter: 1223 loss: 1.5719805791541582e-06
Iter: 1224 loss: 1.5795320499264393e-06
Iter: 1225 loss: 1.5719705319251574e-06
Iter: 1226 loss: 1.5715146192725967e-06
Iter: 1227 loss: 1.57530462311676e-06
Iter: 1228 loss: 1.5714872890747516e-06
Iter: 1229 loss: 1.5712384087372177e-06
Iter: 1230 loss: 1.5706382010526251e-06
Iter: 1231 loss: 1.5771298187430074e-06
Iter: 1232 loss: 1.5705744911987763e-06
Iter: 1233 loss: 1.5699912673261754e-06
Iter: 1234 loss: 1.5774958056645707e-06
Iter: 1235 loss: 1.5699870239744863e-06
Iter: 1236 loss: 1.5693697945297078e-06
Iter: 1237 loss: 1.5697661276469179e-06
Iter: 1238 loss: 1.5689777869219458e-06
Iter: 1239 loss: 1.5685092072962745e-06
Iter: 1240 loss: 1.5679735010625534e-06
Iter: 1241 loss: 1.5679066339015328e-06
Iter: 1242 loss: 1.5670943002262665e-06
Iter: 1243 loss: 1.5788105409550865e-06
Iter: 1244 loss: 1.567092928633681e-06
Iter: 1245 loss: 1.5664524919369812e-06
Iter: 1246 loss: 1.5679900309945667e-06
Iter: 1247 loss: 1.5662211740297345e-06
Iter: 1248 loss: 1.5657972258242896e-06
Iter: 1249 loss: 1.5650667174638369e-06
Iter: 1250 loss: 1.5650661544275927e-06
Iter: 1251 loss: 1.5642443144709455e-06
Iter: 1252 loss: 1.5646387674783781e-06
Iter: 1253 loss: 1.5636919330892144e-06
Iter: 1254 loss: 1.5627255876337323e-06
Iter: 1255 loss: 1.5763646787129784e-06
Iter: 1256 loss: 1.5627232238554105e-06
Iter: 1257 loss: 1.5621691626443496e-06
Iter: 1258 loss: 1.5683578062679095e-06
Iter: 1259 loss: 1.562158513134386e-06
Iter: 1260 loss: 1.5615868973199618e-06
Iter: 1261 loss: 1.5624556930650663e-06
Iter: 1262 loss: 1.5613154396954869e-06
Iter: 1263 loss: 1.5609468954726255e-06
Iter: 1264 loss: 1.5619690200897939e-06
Iter: 1265 loss: 1.5608279315684742e-06
Iter: 1266 loss: 1.5603521633123459e-06
Iter: 1267 loss: 1.5621278827120002e-06
Iter: 1268 loss: 1.5602364036114859e-06
Iter: 1269 loss: 1.5598951796812132e-06
Iter: 1270 loss: 1.5593078788962606e-06
Iter: 1271 loss: 1.5593073907653711e-06
Iter: 1272 loss: 1.5584604536559542e-06
Iter: 1273 loss: 1.5600798762146063e-06
Iter: 1274 loss: 1.5581051577133042e-06
Iter: 1275 loss: 1.5574965260433734e-06
Iter: 1276 loss: 1.5637087487855811e-06
Iter: 1277 loss: 1.5574785528370753e-06
Iter: 1278 loss: 1.5568253438893361e-06
Iter: 1279 loss: 1.5582177553476942e-06
Iter: 1280 loss: 1.5565691971399934e-06
Iter: 1281 loss: 1.5561964239899052e-06
Iter: 1282 loss: 1.5560330799457977e-06
Iter: 1283 loss: 1.5558429241349551e-06
Iter: 1284 loss: 1.5552278347369563e-06
Iter: 1285 loss: 1.55912878896732e-06
Iter: 1286 loss: 1.5551584986755934e-06
Iter: 1287 loss: 1.5545383844897614e-06
Iter: 1288 loss: 1.5565919398523232e-06
Iter: 1289 loss: 1.5543676901870873e-06
Iter: 1290 loss: 1.553970731736668e-06
Iter: 1291 loss: 1.5532933865431545e-06
Iter: 1292 loss: 1.5532924917277216e-06
Iter: 1293 loss: 1.5525154908526759e-06
Iter: 1294 loss: 1.5560192438736461e-06
Iter: 1295 loss: 1.5523648148482698e-06
Iter: 1296 loss: 1.5517788234065193e-06
Iter: 1297 loss: 1.5555623603441527e-06
Iter: 1298 loss: 1.5517150158891375e-06
Iter: 1299 loss: 1.551273659761379e-06
Iter: 1300 loss: 1.5582440970083441e-06
Iter: 1301 loss: 1.5512736480908284e-06
Iter: 1302 loss: 1.5508954906732288e-06
Iter: 1303 loss: 1.5502840595254367e-06
Iter: 1304 loss: 1.5502798959091154e-06
Iter: 1305 loss: 1.5499637746700154e-06
Iter: 1306 loss: 1.5499330625405159e-06
Iter: 1307 loss: 1.5495895782680483e-06
Iter: 1308 loss: 1.5487327036712832e-06
Iter: 1309 loss: 1.5568621795134817e-06
Iter: 1310 loss: 1.5486140092778939e-06
Iter: 1311 loss: 1.5478388788248511e-06
Iter: 1312 loss: 1.5499766287703302e-06
Iter: 1313 loss: 1.5475875706346964e-06
Iter: 1314 loss: 1.5468057556974852e-06
Iter: 1315 loss: 1.5511182887979596e-06
Iter: 1316 loss: 1.5466927953974776e-06
Iter: 1317 loss: 1.5461036472725452e-06
Iter: 1318 loss: 1.5468351514977804e-06
Iter: 1319 loss: 1.5457976725500639e-06
Iter: 1320 loss: 1.545239320013213e-06
Iter: 1321 loss: 1.5539731999279548e-06
Iter: 1322 loss: 1.5452392743755257e-06
Iter: 1323 loss: 1.5447579899574562e-06
Iter: 1324 loss: 1.5451594867155243e-06
Iter: 1325 loss: 1.5444717361842773e-06
Iter: 1326 loss: 1.5439832783990034e-06
Iter: 1327 loss: 1.5440443501660378e-06
Iter: 1328 loss: 1.5436102732178512e-06
Iter: 1329 loss: 1.5429919089658183e-06
Iter: 1330 loss: 1.5439837317653162e-06
Iter: 1331 loss: 1.5427060410027798e-06
Iter: 1332 loss: 1.5421563258316005e-06
Iter: 1333 loss: 1.5421412850687651e-06
Iter: 1334 loss: 1.5418521900843378e-06
Iter: 1335 loss: 1.5410463594392754e-06
Iter: 1336 loss: 1.5454293187303989e-06
Iter: 1337 loss: 1.5408002622580982e-06
Iter: 1338 loss: 1.5406684626222411e-06
Iter: 1339 loss: 1.5403634412114547e-06
Iter: 1340 loss: 1.5399592762893432e-06
Iter: 1341 loss: 1.5406458492508546e-06
Iter: 1342 loss: 1.539777977140039e-06
Iter: 1343 loss: 1.53935843970901e-06
Iter: 1344 loss: 1.5385539726763832e-06
Iter: 1345 loss: 1.5555029959484149e-06
Iter: 1346 loss: 1.5385499917099872e-06
Iter: 1347 loss: 1.5385993673458513e-06
Iter: 1348 loss: 1.5382162359987129e-06
Iter: 1349 loss: 1.5379601751029435e-06
Iter: 1350 loss: 1.5372088799249587e-06
Iter: 1351 loss: 1.5400035032967101e-06
Iter: 1352 loss: 1.5368837145122e-06
Iter: 1353 loss: 1.5361571211332671e-06
Iter: 1354 loss: 1.5432856284044651e-06
Iter: 1355 loss: 1.5361319141673105e-06
Iter: 1356 loss: 1.5356095538616354e-06
Iter: 1357 loss: 1.5367563364026192e-06
Iter: 1358 loss: 1.5354087322353882e-06
Iter: 1359 loss: 1.5348993574454595e-06
Iter: 1360 loss: 1.5405167343404387e-06
Iter: 1361 loss: 1.534888883190948e-06
Iter: 1362 loss: 1.5343446903276107e-06
Iter: 1363 loss: 1.5338435381204523e-06
Iter: 1364 loss: 1.5337121116910453e-06
Iter: 1365 loss: 1.5331243204143193e-06
Iter: 1366 loss: 1.5341624019438524e-06
Iter: 1367 loss: 1.5328662206345095e-06
Iter: 1368 loss: 1.5321619049842284e-06
Iter: 1369 loss: 1.5351092186957627e-06
Iter: 1370 loss: 1.5320116652232973e-06
Iter: 1371 loss: 1.5314921087432128e-06
Iter: 1372 loss: 1.5374767719073267e-06
Iter: 1373 loss: 1.5314837054931593e-06
Iter: 1374 loss: 1.5309927920748131e-06
Iter: 1375 loss: 1.530613526477357e-06
Iter: 1376 loss: 1.5304579226714968e-06
Iter: 1377 loss: 1.5300251317697424e-06
Iter: 1378 loss: 1.5329065664357386e-06
Iter: 1379 loss: 1.529980842684267e-06
Iter: 1380 loss: 1.5294152008164237e-06
Iter: 1381 loss: 1.5303157454439243e-06
Iter: 1382 loss: 1.5291526687958984e-06
Iter: 1383 loss: 1.5287565479821996e-06
Iter: 1384 loss: 1.5283743086302196e-06
Iter: 1385 loss: 1.5282870032274436e-06
Iter: 1386 loss: 1.5277809702400036e-06
Iter: 1387 loss: 1.5277809331684492e-06
Iter: 1388 loss: 1.5273303213085309e-06
Iter: 1389 loss: 1.5279683671999791e-06
Iter: 1390 loss: 1.527109114056745e-06
Iter: 1391 loss: 1.5267757406016115e-06
Iter: 1392 loss: 1.5259785289201826e-06
Iter: 1393 loss: 1.5348642027506684e-06
Iter: 1394 loss: 1.5258995406936011e-06
Iter: 1395 loss: 1.5250161008327949e-06
Iter: 1396 loss: 1.5305209259721793e-06
Iter: 1397 loss: 1.5249130938904344e-06
Iter: 1398 loss: 1.5242482634744882e-06
Iter: 1399 loss: 1.5242454128709169e-06
Iter: 1400 loss: 1.5237786235966822e-06
Iter: 1401 loss: 1.5237006996803936e-06
Iter: 1402 loss: 1.5233807596514288e-06
Iter: 1403 loss: 1.5228312524784882e-06
Iter: 1404 loss: 1.5228082872860269e-06
Iter: 1405 loss: 1.5223846807219227e-06
Iter: 1406 loss: 1.5218777884761313e-06
Iter: 1407 loss: 1.5282346487661374e-06
Iter: 1408 loss: 1.5218732077710034e-06
Iter: 1409 loss: 1.5213252275063939e-06
Iter: 1410 loss: 1.52267702718756e-06
Iter: 1411 loss: 1.5211312160415894e-06
Iter: 1412 loss: 1.5207412938446491e-06
Iter: 1413 loss: 1.5207299153514889e-06
Iter: 1414 loss: 1.5204259233060197e-06
Iter: 1415 loss: 1.5199109330254364e-06
Iter: 1416 loss: 1.5199108437722566e-06
Iter: 1417 loss: 1.519698689183478e-06
Iter: 1418 loss: 1.5192106546981645e-06
Iter: 1419 loss: 1.5254236189099816e-06
Iter: 1420 loss: 1.5191761350306831e-06
Iter: 1421 loss: 1.5184592929759553e-06
Iter: 1422 loss: 1.5203349766810987e-06
Iter: 1423 loss: 1.5182167119503772e-06
Iter: 1424 loss: 1.517537257816551e-06
Iter: 1425 loss: 1.5276778190237922e-06
Iter: 1426 loss: 1.5175367377972683e-06
Iter: 1427 loss: 1.5173145809318826e-06
Iter: 1428 loss: 1.5166701128461436e-06
Iter: 1429 loss: 1.5193066310635586e-06
Iter: 1430 loss: 1.5164118911758337e-06
Iter: 1431 loss: 1.515713426552975e-06
Iter: 1432 loss: 1.5266766499298919e-06
Iter: 1433 loss: 1.5157133849169754e-06
Iter: 1434 loss: 1.5152425802582038e-06
Iter: 1435 loss: 1.5162915298600576e-06
Iter: 1436 loss: 1.5150633631669475e-06
Iter: 1437 loss: 1.5144604935252085e-06
Iter: 1438 loss: 1.5184127025346377e-06
Iter: 1439 loss: 1.51439682790542e-06
Iter: 1440 loss: 1.5139863148365525e-06
Iter: 1441 loss: 1.513676057677071e-06
Iter: 1442 loss: 1.5135422124840387e-06
Iter: 1443 loss: 1.5129905625433246e-06
Iter: 1444 loss: 1.5153816554031193e-06
Iter: 1445 loss: 1.5128780107345696e-06
Iter: 1446 loss: 1.5124069482029388e-06
Iter: 1447 loss: 1.5144606032414709e-06
Iter: 1448 loss: 1.5123114893952264e-06
Iter: 1449 loss: 1.5117513859474172e-06
Iter: 1450 loss: 1.5139086300029682e-06
Iter: 1451 loss: 1.5116197851721696e-06
Iter: 1452 loss: 1.5112840231791496e-06
Iter: 1453 loss: 1.512178298055038e-06
Iter: 1454 loss: 1.5111719843810264e-06
Iter: 1455 loss: 1.5106866509415962e-06
Iter: 1456 loss: 1.5113789470698868e-06
Iter: 1457 loss: 1.5104492405005908e-06
Iter: 1458 loss: 1.5101286227728549e-06
Iter: 1459 loss: 1.5095264705150483e-06
Iter: 1460 loss: 1.5228480532364154e-06
Iter: 1461 loss: 1.5095249325734927e-06
Iter: 1462 loss: 1.5090931011893404e-06
Iter: 1463 loss: 1.5090473091587347e-06
Iter: 1464 loss: 1.5086510379788084e-06
Iter: 1465 loss: 1.50905792935323e-06
Iter: 1466 loss: 1.5084301195357428e-06
Iter: 1467 loss: 1.5080397970711313e-06
Iter: 1468 loss: 1.507290907760094e-06
Iter: 1469 loss: 1.5230431756283594e-06
Iter: 1470 loss: 1.5072871395181406e-06
Iter: 1471 loss: 1.5064547050098747e-06
Iter: 1472 loss: 1.5088722139757544e-06
Iter: 1473 loss: 1.5061961790896384e-06
Iter: 1474 loss: 1.505589476081497e-06
Iter: 1475 loss: 1.5130937188973885e-06
Iter: 1476 loss: 1.505583367273711e-06
Iter: 1477 loss: 1.5051172875033876e-06
Iter: 1478 loss: 1.509726204458531e-06
Iter: 1479 loss: 1.505101618684835e-06
Iter: 1480 loss: 1.5047732412302159e-06
Iter: 1481 loss: 1.5043614170837e-06
Iter: 1482 loss: 1.5043282140200609e-06
Iter: 1483 loss: 1.503708917894259e-06
Iter: 1484 loss: 1.5039368503214221e-06
Iter: 1485 loss: 1.5032758516724872e-06
Iter: 1486 loss: 1.5032713545268667e-06
Iter: 1487 loss: 1.5029860495952278e-06
Iter: 1488 loss: 1.5027351194426654e-06
Iter: 1489 loss: 1.50232361306986e-06
Iter: 1490 loss: 1.5023215644562101e-06
Iter: 1491 loss: 1.5019473805174955e-06
Iter: 1492 loss: 1.5019453430894859e-06
Iter: 1493 loss: 1.5016794594058396e-06
Iter: 1494 loss: 1.5011638823554768e-06
Iter: 1495 loss: 1.5117314790809596e-06
Iter: 1496 loss: 1.5011604787362304e-06
Iter: 1497 loss: 1.5005971487048154e-06
Iter: 1498 loss: 1.5029346480881481e-06
Iter: 1499 loss: 1.5004757557315367e-06
Iter: 1500 loss: 1.500077432465327e-06
Iter: 1501 loss: 1.5045859432214986e-06
Iter: 1502 loss: 1.5000703097959446e-06
Iter: 1503 loss: 1.4996210443395066e-06
Iter: 1504 loss: 1.49883920061402e-06
Iter: 1505 loss: 1.4988389184920993e-06
Iter: 1506 loss: 1.4982912883740718e-06
Iter: 1507 loss: 1.501772252768871e-06
Iter: 1508 loss: 1.4982297939211461e-06
Iter: 1509 loss: 1.4977362657646311e-06
Iter: 1510 loss: 1.4970414020674216e-06
Iter: 1511 loss: 1.4970150426115967e-06
Iter: 1512 loss: 1.4962725987888637e-06
Iter: 1513 loss: 1.5013431427990555e-06
Iter: 1514 loss: 1.4962005116734848e-06
Iter: 1515 loss: 1.4958837304662965e-06
Iter: 1516 loss: 1.4958507204350724e-06
Iter: 1517 loss: 1.4955258952082286e-06
Iter: 1518 loss: 1.4955887418549572e-06
Iter: 1519 loss: 1.4952839837399277e-06
Iter: 1520 loss: 1.4948899151814389e-06
Iter: 1521 loss: 1.4948739522666246e-06
Iter: 1522 loss: 1.4945698232494867e-06
Iter: 1523 loss: 1.4941959381233567e-06
Iter: 1524 loss: 1.5000832394093169e-06
Iter: 1525 loss: 1.4941959225036808e-06
Iter: 1526 loss: 1.4937842693853716e-06
Iter: 1527 loss: 1.4940778728661763e-06
Iter: 1528 loss: 1.4935291620371886e-06
Iter: 1529 loss: 1.4932329005759552e-06
Iter: 1530 loss: 1.4943730632452057e-06
Iter: 1531 loss: 1.4931632257980667e-06
Iter: 1532 loss: 1.4927286987597553e-06
Iter: 1533 loss: 1.4920746952228697e-06
Iter: 1534 loss: 1.4920615859454702e-06
Iter: 1535 loss: 1.4915161216802871e-06
Iter: 1536 loss: 1.4933933494884842e-06
Iter: 1537 loss: 1.4913715867108011e-06
Iter: 1538 loss: 1.4908875929295984e-06
Iter: 1539 loss: 1.4971260015427281e-06
Iter: 1540 loss: 1.4908841255226065e-06
Iter: 1541 loss: 1.4904783229935708e-06
Iter: 1542 loss: 1.4904693106561221e-06
Iter: 1543 loss: 1.4901509796090331e-06
Iter: 1544 loss: 1.4897502674189203e-06
Iter: 1545 loss: 1.4892125177202061e-06
Iter: 1546 loss: 1.4891835096251201e-06
Iter: 1547 loss: 1.4885589050974221e-06
Iter: 1548 loss: 1.4948927068631144e-06
Iter: 1549 loss: 1.488539968007813e-06
Iter: 1550 loss: 1.4880509235440168e-06
Iter: 1551 loss: 1.4887182497126769e-06
Iter: 1552 loss: 1.4878068047677618e-06
Iter: 1553 loss: 1.4872933585350813e-06
Iter: 1554 loss: 1.4892613422219402e-06
Iter: 1555 loss: 1.4871720540109504e-06
Iter: 1556 loss: 1.48669480356192e-06
Iter: 1557 loss: 1.4925639770498212e-06
Iter: 1558 loss: 1.4866897910178284e-06
Iter: 1559 loss: 1.4863931919945287e-06
Iter: 1560 loss: 1.4858990275921976e-06
Iter: 1561 loss: 1.4858974210487665e-06
Iter: 1562 loss: 1.4854921245953467e-06
Iter: 1563 loss: 1.4854832619296995e-06
Iter: 1564 loss: 1.4851019821198164e-06
Iter: 1565 loss: 1.4855750566930889e-06
Iter: 1566 loss: 1.4849038666116148e-06
Iter: 1567 loss: 1.4846197308096266e-06
Iter: 1568 loss: 1.4842513646552961e-06
Iter: 1569 loss: 1.4842267104647151e-06
Iter: 1570 loss: 1.4836108853136259e-06
Iter: 1571 loss: 1.4897041778555093e-06
Iter: 1572 loss: 1.4835902288228368e-06
Iter: 1573 loss: 1.4833409588823921e-06
Iter: 1574 loss: 1.4828114939584418e-06
Iter: 1575 loss: 1.4913875144015839e-06
Iter: 1576 loss: 1.4827953621742333e-06
Iter: 1577 loss: 1.482556941977811e-06
Iter: 1578 loss: 1.4824417083913708e-06
Iter: 1579 loss: 1.4821580055506053e-06
Iter: 1580 loss: 1.4814954497071857e-06
Iter: 1581 loss: 1.4895152448260529e-06
Iter: 1582 loss: 1.4814417883364562e-06
Iter: 1583 loss: 1.480764839405306e-06
Iter: 1584 loss: 1.4828170694577662e-06
Iter: 1585 loss: 1.4805623940582073e-06
Iter: 1586 loss: 1.4799152674266344e-06
Iter: 1587 loss: 1.4817359270922109e-06
Iter: 1588 loss: 1.4797088821174477e-06
Iter: 1589 loss: 1.4791756360825691e-06
Iter: 1590 loss: 1.4803848914734363e-06
Iter: 1591 loss: 1.4789750468084346e-06
Iter: 1592 loss: 1.4785456115855825e-06
Iter: 1593 loss: 1.4785452089738312e-06
Iter: 1594 loss: 1.4781843195887356e-06
Iter: 1595 loss: 1.4790739410706857e-06
Iter: 1596 loss: 1.4780564935254839e-06
Iter: 1597 loss: 1.4776770164940946e-06
Iter: 1598 loss: 1.4778056315439741e-06
Iter: 1599 loss: 1.4774088744699621e-06
Iter: 1600 loss: 1.4770605825980914e-06
Iter: 1601 loss: 1.4770557968750837e-06
Iter: 1602 loss: 1.476803302054239e-06
Iter: 1603 loss: 1.4761583715196689e-06
Iter: 1604 loss: 1.4816921068951382e-06
Iter: 1605 loss: 1.4760517388411215e-06
Iter: 1606 loss: 1.4756206460183879e-06
Iter: 1607 loss: 1.4756148875277928e-06
Iter: 1608 loss: 1.475232661820919e-06
Iter: 1609 loss: 1.4761854066427497e-06
Iter: 1610 loss: 1.4750983881008905e-06
Iter: 1611 loss: 1.4747173381474132e-06
Iter: 1612 loss: 1.4744393142148769e-06
Iter: 1613 loss: 1.4743097087873964e-06
Iter: 1614 loss: 1.4741060893872118e-06
Iter: 1615 loss: 1.4740406365524175e-06
Iter: 1616 loss: 1.4738047088237623e-06
Iter: 1617 loss: 1.4732729516841868e-06
Iter: 1618 loss: 1.4805017280828747e-06
Iter: 1619 loss: 1.4732419412781234e-06
Iter: 1620 loss: 1.472715897259513e-06
Iter: 1621 loss: 1.4724811524344175e-06
Iter: 1622 loss: 1.4722154231366952e-06
Iter: 1623 loss: 1.4714733480951272e-06
Iter: 1624 loss: 1.4791724189560195e-06
Iter: 1625 loss: 1.4714529279157966e-06
Iter: 1626 loss: 1.470938253806806e-06
Iter: 1627 loss: 1.4714442380994531e-06
Iter: 1628 loss: 1.4706471006689478e-06
Iter: 1629 loss: 1.4702701239894279e-06
Iter: 1630 loss: 1.4702536550030966e-06
Iter: 1631 loss: 1.469869406915673e-06
Iter: 1632 loss: 1.4698059499923908e-06
Iter: 1633 loss: 1.4695421346951936e-06
Iter: 1634 loss: 1.4693397101641358e-06
Iter: 1635 loss: 1.4693202909635501e-06
Iter: 1636 loss: 1.4691178619237764e-06
Iter: 1637 loss: 1.46857748899897e-06
Iter: 1638 loss: 1.4723388747816692e-06
Iter: 1639 loss: 1.4684550024303012e-06
Iter: 1640 loss: 1.4677527403694218e-06
Iter: 1641 loss: 1.4710867055336049e-06
Iter: 1642 loss: 1.4676256505227741e-06
Iter: 1643 loss: 1.4673257520282356e-06
Iter: 1644 loss: 1.46731309815662e-06
Iter: 1645 loss: 1.4670264437342098e-06
Iter: 1646 loss: 1.4663652085195053e-06
Iter: 1647 loss: 1.4747062143026024e-06
Iter: 1648 loss: 1.4663172418397815e-06
Iter: 1649 loss: 1.4662185365725449e-06
Iter: 1650 loss: 1.4660663856644975e-06
Iter: 1651 loss: 1.4658242889044245e-06
Iter: 1652 loss: 1.4654150818320455e-06
Iter: 1653 loss: 1.4654142680207448e-06
Iter: 1654 loss: 1.4649336819010542e-06
Iter: 1655 loss: 1.465539601169602e-06
Iter: 1656 loss: 1.4646856525279421e-06
Iter: 1657 loss: 1.4640706079746663e-06
Iter: 1658 loss: 1.4648998866079994e-06
Iter: 1659 loss: 1.4637619162509697e-06
Iter: 1660 loss: 1.4632316239093965e-06
Iter: 1661 loss: 1.4654412311818581e-06
Iter: 1662 loss: 1.4631180001170649e-06
Iter: 1663 loss: 1.4626775865110118e-06
Iter: 1664 loss: 1.4648236541392456e-06
Iter: 1665 loss: 1.4626007405635932e-06
Iter: 1666 loss: 1.4622605720137059e-06
Iter: 1667 loss: 1.4666813655596634e-06
Iter: 1668 loss: 1.4622583158974556e-06
Iter: 1669 loss: 1.461925444846653e-06
Iter: 1670 loss: 1.461664750491933e-06
Iter: 1671 loss: 1.4615610975571871e-06
Iter: 1672 loss: 1.4612203467311842e-06
Iter: 1673 loss: 1.4612168022061556e-06
Iter: 1674 loss: 1.4609940931071862e-06
Iter: 1675 loss: 1.4603752342358137e-06
Iter: 1676 loss: 1.4638096099235877e-06
Iter: 1677 loss: 1.460190244748649e-06
Iter: 1678 loss: 1.4597051144567685e-06
Iter: 1679 loss: 1.4597025638029839e-06
Iter: 1680 loss: 1.4593037281681637e-06
Iter: 1681 loss: 1.4617280542468483e-06
Iter: 1682 loss: 1.4592550383016997e-06
Iter: 1683 loss: 1.4589262121983395e-06
Iter: 1684 loss: 1.4586433693248325e-06
Iter: 1685 loss: 1.4585540827672844e-06
Iter: 1686 loss: 1.4582937589311633e-06
Iter: 1687 loss: 1.4582767232070716e-06
Iter: 1688 loss: 1.4580204389557133e-06
Iter: 1689 loss: 1.4574676539652329e-06
Iter: 1690 loss: 1.4660412843573097e-06
Iter: 1691 loss: 1.4574474178365985e-06
Iter: 1692 loss: 1.4568920832380558e-06
Iter: 1693 loss: 1.4571539807214043e-06
Iter: 1694 loss: 1.4565177331559422e-06
Iter: 1695 loss: 1.4558627346120715e-06
Iter: 1696 loss: 1.4579518461767818e-06
Iter: 1697 loss: 1.4556758726705452e-06
Iter: 1698 loss: 1.4550040133280416e-06
Iter: 1699 loss: 1.4588387290098147e-06
Iter: 1700 loss: 1.4549123346126071e-06
Iter: 1701 loss: 1.4545716850042226e-06
Iter: 1702 loss: 1.4545716745977291e-06
Iter: 1703 loss: 1.454180847331209e-06
Iter: 1704 loss: 1.4541722682476563e-06
Iter: 1705 loss: 1.4538655669603147e-06
Iter: 1706 loss: 1.4534990643997889e-06
Iter: 1707 loss: 1.4568426615596243e-06
Iter: 1708 loss: 1.4534825591312271e-06
Iter: 1709 loss: 1.4531290221030483e-06
Iter: 1710 loss: 1.4526547944333042e-06
Iter: 1711 loss: 1.4526291483767524e-06
Iter: 1712 loss: 1.4522532664465779e-06
Iter: 1713 loss: 1.4523325088331171e-06
Iter: 1714 loss: 1.4519751091425104e-06
Iter: 1715 loss: 1.4514383627578108e-06
Iter: 1716 loss: 1.4593554186725319e-06
Iter: 1717 loss: 1.4514378037183176e-06
Iter: 1718 loss: 1.4511459822128078e-06
Iter: 1719 loss: 1.4511223886283992e-06
Iter: 1720 loss: 1.4509052805407162e-06
Iter: 1721 loss: 1.4505837118786398e-06
Iter: 1722 loss: 1.4534584223396129e-06
Iter: 1723 loss: 1.450568227667665e-06
Iter: 1724 loss: 1.4502456566437837e-06
Iter: 1725 loss: 1.4501663156128787e-06
Iter: 1726 loss: 1.4499621818190383e-06
Iter: 1727 loss: 1.4495737119092698e-06
Iter: 1728 loss: 1.4494463393772343e-06
Iter: 1729 loss: 1.4492211350775621e-06
Iter: 1730 loss: 1.4487088673397848e-06
Iter: 1731 loss: 1.4507858182128938e-06
Iter: 1732 loss: 1.4485954674115561e-06
Iter: 1733 loss: 1.4480220985141692e-06
Iter: 1734 loss: 1.4484862206516006e-06
Iter: 1735 loss: 1.4476781572524268e-06
Iter: 1736 loss: 1.4471612674611231e-06
Iter: 1737 loss: 1.4492750663417238e-06
Iter: 1738 loss: 1.4470479955734976e-06
Iter: 1739 loss: 1.4465882988763271e-06
Iter: 1740 loss: 1.4465861678104955e-06
Iter: 1741 loss: 1.4463571382008878e-06
Iter: 1742 loss: 1.4461672691508885e-06
Iter: 1743 loss: 1.4461014454876003e-06
Iter: 1744 loss: 1.4456870561527446e-06
Iter: 1745 loss: 1.4477270069330558e-06
Iter: 1746 loss: 1.4456158168595821e-06
Iter: 1747 loss: 1.4452718800419129e-06
Iter: 1748 loss: 1.4451680176201008e-06
Iter: 1749 loss: 1.4449629139393e-06
Iter: 1750 loss: 1.4446079197100873e-06
Iter: 1751 loss: 1.445150386314721e-06
Iter: 1752 loss: 1.4444398017938657e-06
Iter: 1753 loss: 1.4439175655824736e-06
Iter: 1754 loss: 1.4477045810459796e-06
Iter: 1755 loss: 1.4438731645146556e-06
Iter: 1756 loss: 1.4436089823229411e-06
Iter: 1757 loss: 1.4434683972840566e-06
Iter: 1758 loss: 1.4433487212251952e-06
Iter: 1759 loss: 1.4429306478326354e-06
Iter: 1760 loss: 1.4468677063102045e-06
Iter: 1761 loss: 1.4429137670346126e-06
Iter: 1762 loss: 1.4425971274984975e-06
Iter: 1763 loss: 1.4422651028539396e-06
Iter: 1764 loss: 1.4422073416758679e-06
Iter: 1765 loss: 1.4416984495141696e-06
Iter: 1766 loss: 1.44152816658531e-06
Iter: 1767 loss: 1.4412353895006587e-06
Iter: 1768 loss: 1.4405442373682582e-06
Iter: 1769 loss: 1.4427909155661608e-06
Iter: 1770 loss: 1.4403505155146909e-06
Iter: 1771 loss: 1.4398597861311842e-06
Iter: 1772 loss: 1.4419307222314843e-06
Iter: 1773 loss: 1.4397562615465288e-06
Iter: 1774 loss: 1.4392171243576448e-06
Iter: 1775 loss: 1.4462541513196015e-06
Iter: 1776 loss: 1.4392136815238264e-06
Iter: 1777 loss: 1.4389745319159585e-06
Iter: 1778 loss: 1.4389446846160005e-06
Iter: 1779 loss: 1.438773956560071e-06
Iter: 1780 loss: 1.4384539561930654e-06
Iter: 1781 loss: 1.440230278119069e-06
Iter: 1782 loss: 1.4384082267780824e-06
Iter: 1783 loss: 1.4380772600885356e-06
Iter: 1784 loss: 1.4375717079561769e-06
Iter: 1785 loss: 1.4375632222241383e-06
Iter: 1786 loss: 1.4371017009806842e-06
Iter: 1787 loss: 1.4396237092719714e-06
Iter: 1788 loss: 1.4370340203592236e-06
Iter: 1789 loss: 1.436570033787481e-06
Iter: 1790 loss: 1.4402854896106622e-06
Iter: 1791 loss: 1.436539175514108e-06
Iter: 1792 loss: 1.4362369375958406e-06
Iter: 1793 loss: 1.4361596728750834e-06
Iter: 1794 loss: 1.4359703448659949e-06
Iter: 1795 loss: 1.4355523354133365e-06
Iter: 1796 loss: 1.4381987429206081e-06
Iter: 1797 loss: 1.4355050464386618e-06
Iter: 1798 loss: 1.4351179797451304e-06
Iter: 1799 loss: 1.4359132619851699e-06
Iter: 1800 loss: 1.4349625418624538e-06
Iter: 1801 loss: 1.434683956831192e-06
Iter: 1802 loss: 1.4341862009049951e-06
Iter: 1803 loss: 1.446492753069007e-06
Iter: 1804 loss: 1.4341861906113374e-06
Iter: 1805 loss: 1.4336366603277551e-06
Iter: 1806 loss: 1.4362842733811626e-06
Iter: 1807 loss: 1.4335392543012153e-06
Iter: 1808 loss: 1.4329798111876134e-06
Iter: 1809 loss: 1.4340872653719591e-06
Iter: 1810 loss: 1.4327499041419757e-06
Iter: 1811 loss: 1.4322176095871627e-06
Iter: 1812 loss: 1.4351075732624738e-06
Iter: 1813 loss: 1.4321387299679992e-06
Iter: 1814 loss: 1.4318615299904296e-06
Iter: 1815 loss: 1.4318226625194214e-06
Iter: 1816 loss: 1.4316434002610603e-06
Iter: 1817 loss: 1.4311214816367361e-06
Iter: 1818 loss: 1.4331954147808659e-06
Iter: 1819 loss: 1.430907182665544e-06
Iter: 1820 loss: 1.4306367253905239e-06
Iter: 1821 loss: 1.4305537043414689e-06
Iter: 1822 loss: 1.4302348394664959e-06
Iter: 1823 loss: 1.4301726253251167e-06
Iter: 1824 loss: 1.429960088151474e-06
Iter: 1825 loss: 1.4295216898526226e-06
Iter: 1826 loss: 1.4297377538627622e-06
Iter: 1827 loss: 1.4292283697641577e-06
Iter: 1828 loss: 1.4289277960333126e-06
Iter: 1829 loss: 1.4289065237415892e-06
Iter: 1830 loss: 1.4286173081330077e-06
Iter: 1831 loss: 1.428029390714895e-06
Iter: 1832 loss: 1.4387408276979512e-06
Iter: 1833 loss: 1.4280197050807626e-06
Iter: 1834 loss: 1.4276898605831428e-06
Iter: 1835 loss: 1.4276799284051868e-06
Iter: 1836 loss: 1.4273326927665952e-06
Iter: 1837 loss: 1.4269500058945035e-06
Iter: 1838 loss: 1.4268946014831362e-06
Iter: 1839 loss: 1.4263963659738247e-06
Iter: 1840 loss: 1.4277345105794357e-06
Iter: 1841 loss: 1.4262313085539003e-06
Iter: 1842 loss: 1.4257665699707795e-06
Iter: 1843 loss: 1.4266032426069236e-06
Iter: 1844 loss: 1.4255645708754341e-06
Iter: 1845 loss: 1.42506380586785e-06
Iter: 1846 loss: 1.4249967281448023e-06
Iter: 1847 loss: 1.4246423563453438e-06
Iter: 1848 loss: 1.4243210909548045e-06
Iter: 1849 loss: 1.4242738535938995e-06
Iter: 1850 loss: 1.4239687645507451e-06
Iter: 1851 loss: 1.4260227500361511e-06
Iter: 1852 loss: 1.4239382355645963e-06
Iter: 1853 loss: 1.4236595840818318e-06
Iter: 1854 loss: 1.4228804261269379e-06
Iter: 1855 loss: 1.4270337373121142e-06
Iter: 1856 loss: 1.4226373230138309e-06
Iter: 1857 loss: 1.4221462859499847e-06
Iter: 1858 loss: 1.4299896966368965e-06
Iter: 1859 loss: 1.4221462857101801e-06
Iter: 1860 loss: 1.421690447674525e-06
Iter: 1861 loss: 1.4247169328194014e-06
Iter: 1862 loss: 1.4216435124719853e-06
Iter: 1863 loss: 1.4213230047597736e-06
Iter: 1864 loss: 1.4210533463961785e-06
Iter: 1865 loss: 1.4209632630295359e-06
Iter: 1866 loss: 1.420605282008391e-06
Iter: 1867 loss: 1.420604744018001e-06
Iter: 1868 loss: 1.4202792639088187e-06
Iter: 1869 loss: 1.4201992884651817e-06
Iter: 1870 loss: 1.4199932442117379e-06
Iter: 1871 loss: 1.4196125313519729e-06
Iter: 1872 loss: 1.4196413886706438e-06
Iter: 1873 loss: 1.4193164491010665e-06
Iter: 1874 loss: 1.4188526107471766e-06
Iter: 1875 loss: 1.4188524148369046e-06
Iter: 1876 loss: 1.4186027087472418e-06
Iter: 1877 loss: 1.4179922889477992e-06
Iter: 1878 loss: 1.4242607972916081e-06
Iter: 1879 loss: 1.4179200166163977e-06
Iter: 1880 loss: 1.4173227826607664e-06
Iter: 1881 loss: 1.4217693113071873e-06
Iter: 1882 loss: 1.4172750106576209e-06
Iter: 1883 loss: 1.4167123192303852e-06
Iter: 1884 loss: 1.4174132675445335e-06
Iter: 1885 loss: 1.4164205014468175e-06
Iter: 1886 loss: 1.4164407453361465e-06
Iter: 1887 loss: 1.4161772077189375e-06
Iter: 1888 loss: 1.4159857813363869e-06
Iter: 1889 loss: 1.4154933017633037e-06
Iter: 1890 loss: 1.4195906934287888e-06
Iter: 1891 loss: 1.4154075217265087e-06
Iter: 1892 loss: 1.4148500871430786e-06
Iter: 1893 loss: 1.4173045412504469e-06
Iter: 1894 loss: 1.414738633746364e-06
Iter: 1895 loss: 1.4142499725316542e-06
Iter: 1896 loss: 1.4152719909595904e-06
Iter: 1897 loss: 1.4140559612080035e-06
Iter: 1898 loss: 1.413638109237062e-06
Iter: 1899 loss: 1.4136361639565644e-06
Iter: 1900 loss: 1.4134194725271673e-06
Iter: 1901 loss: 1.4130440719509778e-06
Iter: 1902 loss: 1.4130438731615183e-06
Iter: 1903 loss: 1.4127010224823465e-06
Iter: 1904 loss: 1.4126951083276614e-06
Iter: 1905 loss: 1.4124468257184167e-06
Iter: 1906 loss: 1.4121440752989038e-06
Iter: 1907 loss: 1.4121158782674746e-06
Iter: 1908 loss: 1.4117136681833115e-06
Iter: 1909 loss: 1.4125238740204876e-06
Iter: 1910 loss: 1.4115501684233956e-06
Iter: 1911 loss: 1.4111771250661824e-06
Iter: 1912 loss: 1.4167368964687016e-06
Iter: 1913 loss: 1.4111768288097346e-06
Iter: 1914 loss: 1.4109105660684387e-06
Iter: 1915 loss: 1.4102306592444001e-06
Iter: 1916 loss: 1.4160747701686269e-06
Iter: 1917 loss: 1.4101185041453047e-06
Iter: 1918 loss: 1.4095283364913779e-06
Iter: 1919 loss: 1.4125902846503535e-06
Iter: 1920 loss: 1.4094344960870875e-06
Iter: 1921 loss: 1.4094034444654865e-06
Iter: 1922 loss: 1.4091927272491261e-06
Iter: 1923 loss: 1.4089974284791356e-06
Iter: 1924 loss: 1.4085448040903363e-06
Iter: 1925 loss: 1.4141662063161713e-06
Iter: 1926 loss: 1.4085105486779083e-06
Iter: 1927 loss: 1.4080315945396667e-06
Iter: 1928 loss: 1.409138453129368e-06
Iter: 1929 loss: 1.4078538133057473e-06
Iter: 1930 loss: 1.4073758442205453e-06
Iter: 1931 loss: 1.4078812730962691e-06
Iter: 1932 loss: 1.4071120623728378e-06
Iter: 1933 loss: 1.4066339298089143e-06
Iter: 1934 loss: 1.4066233395004151e-06
Iter: 1935 loss: 1.4064298822005949e-06
Iter: 1936 loss: 1.4062015825856769e-06
Iter: 1937 loss: 1.4061767676691889e-06
Iter: 1938 loss: 1.4058490076587862e-06
Iter: 1939 loss: 1.4098175874299368e-06
Iter: 1940 loss: 1.4058451544800143e-06
Iter: 1941 loss: 1.4056059360824333e-06
Iter: 1942 loss: 1.4053938237857877e-06
Iter: 1943 loss: 1.405332054632123e-06
Iter: 1944 loss: 1.4049406445420055e-06
Iter: 1945 loss: 1.4050803558547438e-06
Iter: 1946 loss: 1.4046658285490239e-06
Iter: 1947 loss: 1.404172390398117e-06
Iter: 1948 loss: 1.4108524697277023e-06
Iter: 1949 loss: 1.4041702564364923e-06
Iter: 1950 loss: 1.4038349100062454e-06
Iter: 1951 loss: 1.403516905873612e-06
Iter: 1952 loss: 1.4034403710781356e-06
Iter: 1953 loss: 1.4029552027130222e-06
Iter: 1954 loss: 1.4041693788990701e-06
Iter: 1955 loss: 1.4027852495267618e-06
Iter: 1956 loss: 1.4023645315655841e-06
Iter: 1957 loss: 1.4056518082193727e-06
Iter: 1958 loss: 1.4023346860948786e-06
Iter: 1959 loss: 1.4019010882144444e-06
Iter: 1960 loss: 1.4041446892412215e-06
Iter: 1961 loss: 1.4018318840014276e-06
Iter: 1962 loss: 1.401625214019863e-06
Iter: 1963 loss: 1.4010832619108554e-06
Iter: 1964 loss: 1.4052085322310385e-06
Iter: 1965 loss: 1.4009749506304502e-06
Iter: 1966 loss: 1.4004478356290473e-06
Iter: 1967 loss: 1.4065820814886475e-06
Iter: 1968 loss: 1.4004398162334836e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.2/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.6 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi1.6
+ date
Sat Nov  7 22:09:32 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.2/300_300_300_1 --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff509cfd268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff509deb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff509dfba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff509d4b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff509d360d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff509c19a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff509cccd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff509ccca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff50122b620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff50122b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff509ccc840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff509c6f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff509be5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5011a7b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5011562f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff50113dbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff501152400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5010d9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff501125ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff501125ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5011bd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff5011bdb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff50104e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff501036730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff501036840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff500fa2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff500fa2c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff500f63488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff50107f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff50108e048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff500fc92f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff500f1a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff500f1aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff500f06c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff500e729d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff500e27400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.005540978108908458
test_loss: 0.005852697198574783
train_loss: 0.004953859582969414
test_loss: 0.005131800549266383
train_loss: 0.004494348241989964
test_loss: 0.004947944418585491
train_loss: 0.004181294499960069
test_loss: 0.004723621277853375
train_loss: 0.0041624776061487435
test_loss: 0.004851014946920988
train_loss: 0.004321149274280753
test_loss: 0.004730771607558793
train_loss: 0.004162972738094503
test_loss: 0.004945050683854096
train_loss: 0.004165250441197282
test_loss: 0.004950393403422081
train_loss: 0.004154414623836557
test_loss: 0.004456295118726324
train_loss: 0.0042456241763646615
test_loss: 0.004704195430598382
train_loss: 0.004186667507993942
test_loss: 0.004554159029275937
train_loss: 0.004333953560215391
test_loss: 0.004845471286023796
train_loss: 0.004047363765038915
test_loss: 0.004562455972020949
train_loss: 0.0041011078663527566
test_loss: 0.0042787340584023595
train_loss: 0.004581571163241686
test_loss: 0.004708980067145519
train_loss: 0.004190644451749614
test_loss: 0.004752149793362776
train_loss: 0.004172570488934778
test_loss: 0.004711812809447568
train_loss: 0.004047738031655686
test_loss: 0.004534515924707807
train_loss: 0.00396431647386675
test_loss: 0.004590786731243171
train_loss: 0.003810228016161707
test_loss: 0.004609328112974379
train_loss: 0.0038574367677451753
test_loss: 0.004584733478808541
train_loss: 0.004124798548377063
test_loss: 0.0046833288736201776
train_loss: 0.0040215232799013685
test_loss: 0.004590086807859288
train_loss: 0.0039048655459735042
test_loss: 0.00456087572230132
train_loss: 0.004017397870320288
test_loss: 0.004526045207121107
train_loss: 0.003761424754625234
test_loss: 0.004595596904727532
train_loss: 0.004049698000584503
test_loss: 0.004508564256350174
train_loss: 0.003939399657887329
test_loss: 0.004406901684357715
train_loss: 0.003654391776404615
test_loss: 0.00429819357509519
train_loss: 0.003954197122324108
test_loss: 0.004639763312425199
train_loss: 0.004076643473013579
test_loss: 0.004583086551776788
train_loss: 0.0037210767160455642
test_loss: 0.004305983145428273
train_loss: 0.004005609155126204
test_loss: 0.004714128719581972
train_loss: 0.0036222927410086135
test_loss: 0.004590194320506746
train_loss: 0.003961548055475471
test_loss: 0.004323415459303395
train_loss: 0.003921506582048016
test_loss: 0.0048234929543449915
train_loss: 0.0039899521069279445
test_loss: 0.00478619979753744
train_loss: 0.004417638420386237
test_loss: 0.004517290938088182
train_loss: 0.0035604617815669284
test_loss: 0.004305701314214913
train_loss: 0.004055896558761515
test_loss: 0.0046755670865284435
train_loss: 0.003922520804860153
test_loss: 0.004354494234924672
train_loss: 0.0038405854594763577
test_loss: 0.004437317864363219
train_loss: 0.0036753897614705843
test_loss: 0.004310047518329834
train_loss: 0.003989289763269869
test_loss: 0.004300691396060938
train_loss: 0.003985788216889112
test_loss: 0.004534564336338615
train_loss: 0.004066724338405976
test_loss: 0.004535195491926777
train_loss: 0.0036828622077441373
test_loss: 0.004394880659627029
train_loss: 0.003597098461896394
test_loss: 0.004334784923199895
train_loss: 0.003930006434543161
test_loss: 0.0044264319038128976
train_loss: 0.00404234702434038
test_loss: 0.004586168233342395
train_loss: 0.003576078116579499
test_loss: 0.004401394740782555
train_loss: 0.0036516052292144916
test_loss: 0.0044345551875068405
train_loss: 0.0036335701413425857
test_loss: 0.004235536042399925
train_loss: 0.003661654065291193
test_loss: 0.004410485548560283
train_loss: 0.004001686007981609
test_loss: 0.005471355576031582
train_loss: 0.003835702693612947
test_loss: 0.004320663145403978
train_loss: 0.0040175356420284886
test_loss: 0.0045332303111564635
train_loss: 0.003587205335385298
test_loss: 0.00432392591184037
train_loss: 0.0036580593391095824
test_loss: 0.004172574354625283
train_loss: 0.00404275055318055
test_loss: 0.0047384760202588295
train_loss: 0.0036944338725443935
test_loss: 0.004129012523235378
train_loss: 0.0033588900173038387
test_loss: 0.004027429492588496
train_loss: 0.0037153523930975223
test_loss: 0.00423724409676531
train_loss: 0.003935095294776685
test_loss: 0.004267584977530821
train_loss: 0.0036049981249696326
test_loss: 0.00432862198656104
train_loss: 0.003734685360470935
test_loss: 0.004438225509766669
train_loss: 0.0036898234099548436
test_loss: 0.004515132585930294
train_loss: 0.003732539998518249
test_loss: 0.004187647861786241
train_loss: 0.0035474141269149417
test_loss: 0.004228574759876261
train_loss: 0.0036797057693298135
test_loss: 0.0045171884661521795
train_loss: 0.003669709885483001
test_loss: 0.0043703982785382655
train_loss: 0.003713126651691291
test_loss: 0.00424030186133622
train_loss: 0.0038395425982641644
test_loss: 0.004382527303496488
train_loss: 0.00364117013676766
test_loss: 0.00431360332736438
train_loss: 0.0036924961711708084
test_loss: 0.004228998372534867
train_loss: 0.0035888085097195883
test_loss: 0.004252720610526928
train_loss: 0.003538900140295331
test_loss: 0.004466868797583683
train_loss: 0.0037043929386733664
test_loss: 0.004511315370497185
train_loss: 0.0033803690192377774
test_loss: 0.0043097308280200895
train_loss: 0.0037944730169729244
test_loss: 0.004317047406178153
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi1.6/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85505df400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8550605c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8550605ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f855058d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85506b6158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8550565a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f855052be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85504dd1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85504dd730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f855049d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f855045cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8550462e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8550462840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8550408620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85503cfe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8550391ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85503a4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85503a47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f855035a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f855035a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f855033f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f855033f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f855028b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8550259950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f85502596a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f855025a8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f855020a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8521d6f840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8521d6a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8521d596a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8521d6f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8521d6f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8521d002f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8521cad8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8521cc6b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f8521c88f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.2702576435198857e-05
Iter: 2 loss: 1.8248687549219583e-05
Iter: 3 loss: 1.8065359481779119e-05
Iter: 4 loss: 1.6506539725635577e-05
Iter: 5 loss: 2.2576502836761245e-05
Iter: 6 loss: 1.6142513588280173e-05
Iter: 7 loss: 1.5085065797008642e-05
Iter: 8 loss: 2.0562730488702775e-05
Iter: 9 loss: 1.4917057123137066e-05
Iter: 10 loss: 1.3864214965739909e-05
Iter: 11 loss: 1.4034767344094093e-05
Iter: 12 loss: 1.3070988946699793e-05
Iter: 13 loss: 1.2268136036538164e-05
Iter: 14 loss: 2.012082524558937e-05
Iter: 15 loss: 1.2240073143403096e-05
Iter: 16 loss: 1.1622224504957385e-05
Iter: 17 loss: 1.1099383299721769e-05
Iter: 18 loss: 1.0927264189016882e-05
Iter: 19 loss: 1.0216756711105235e-05
Iter: 20 loss: 1.2556813090743782e-05
Iter: 21 loss: 1.0020397489285656e-05
Iter: 22 loss: 9.2345831796842922e-06
Iter: 23 loss: 1.3816668197840586e-05
Iter: 24 loss: 9.131354081259177e-06
Iter: 25 loss: 8.6789360155385053e-06
Iter: 26 loss: 9.409569643921132e-06
Iter: 27 loss: 8.47049998865109e-06
Iter: 28 loss: 8.0105108429839041e-06
Iter: 29 loss: 7.7290097036208e-06
Iter: 30 loss: 7.5420676866870454e-06
Iter: 31 loss: 6.9734095552622643e-06
Iter: 32 loss: 9.4124397621481485e-06
Iter: 33 loss: 6.8558018254981348e-06
Iter: 34 loss: 6.4476902729030815e-06
Iter: 35 loss: 9.6258931277459918e-06
Iter: 36 loss: 6.4187166413342322e-06
Iter: 37 loss: 6.0614626580581592e-06
Iter: 38 loss: 6.5991356317371518e-06
Iter: 39 loss: 5.890852498718727e-06
Iter: 40 loss: 5.5763042608218685e-06
Iter: 41 loss: 6.160770172558438e-06
Iter: 42 loss: 5.4421095076773331e-06
Iter: 43 loss: 5.3278728944513766e-06
Iter: 44 loss: 5.2874022377660074e-06
Iter: 45 loss: 5.1592918728211909e-06
Iter: 46 loss: 5.7356437008340423e-06
Iter: 47 loss: 5.1343765550161683e-06
Iter: 48 loss: 5.023003113940618e-06
Iter: 49 loss: 5.1046745519996887e-06
Iter: 50 loss: 4.9544469524468908e-06
Iter: 51 loss: 4.785933773846367e-06
Iter: 52 loss: 5.0023036486054973e-06
Iter: 53 loss: 4.6996570828879786e-06
Iter: 54 loss: 4.5886480230109867e-06
Iter: 55 loss: 4.99067531170317e-06
Iter: 56 loss: 4.5607508700366658e-06
Iter: 57 loss: 4.4252589958102694e-06
Iter: 58 loss: 4.57443745765663e-06
Iter: 59 loss: 4.3515137465695029e-06
Iter: 60 loss: 4.2593374975802049e-06
Iter: 61 loss: 4.2621706253901655e-06
Iter: 62 loss: 4.1864435969058039e-06
Iter: 63 loss: 4.0551120981915311e-06
Iter: 64 loss: 5.4419774468032222e-06
Iter: 65 loss: 4.0517781932528036e-06
Iter: 66 loss: 3.9796619333965072e-06
Iter: 67 loss: 4.2103984879146631e-06
Iter: 68 loss: 3.9591385242268442e-06
Iter: 69 loss: 3.9010568911332665e-06
Iter: 70 loss: 3.7719480086418366e-06
Iter: 71 loss: 5.605748149344897e-06
Iter: 72 loss: 3.7653980830140552e-06
Iter: 73 loss: 3.6240613271895869e-06
Iter: 74 loss: 4.3539296127991826e-06
Iter: 75 loss: 3.6014199570860325e-06
Iter: 76 loss: 3.5098812052329298e-06
Iter: 77 loss: 4.4928612290173071e-06
Iter: 78 loss: 3.5077264219790205e-06
Iter: 79 loss: 3.4259098241101869e-06
Iter: 80 loss: 3.5095620567430777e-06
Iter: 81 loss: 3.3802437944344717e-06
Iter: 82 loss: 3.3109291220276924e-06
Iter: 83 loss: 3.3106663840482325e-06
Iter: 84 loss: 3.2553838448709032e-06
Iter: 85 loss: 3.3254451070729539e-06
Iter: 86 loss: 3.2203178488100361e-06
Iter: 87 loss: 3.1992737578096839e-06
Iter: 88 loss: 3.1667553690302989e-06
Iter: 89 loss: 3.1662867963863407e-06
Iter: 90 loss: 3.1288455479251808e-06
Iter: 91 loss: 3.12758084713802e-06
Iter: 92 loss: 3.0985085667718105e-06
Iter: 93 loss: 3.070712180040629e-06
Iter: 94 loss: 3.066877965598991e-06
Iter: 95 loss: 3.0481631450925548e-06
Iter: 96 loss: 3.0042728180696867e-06
Iter: 97 loss: 3.5277110781940797e-06
Iter: 98 loss: 3.0005840625669095e-06
Iter: 99 loss: 2.9639858980105869e-06
Iter: 100 loss: 2.9638433156593177e-06
Iter: 101 loss: 2.9326501426018784e-06
Iter: 102 loss: 2.9316097940493759e-06
Iter: 103 loss: 2.9073757956453208e-06
Iter: 104 loss: 2.8697191888342516e-06
Iter: 105 loss: 3.0045639150324067e-06
Iter: 106 loss: 2.8601459331763895e-06
Iter: 107 loss: 2.8316946018291476e-06
Iter: 108 loss: 2.9334081304467694e-06
Iter: 109 loss: 2.8244470935878994e-06
Iter: 110 loss: 2.786652243778746e-06
Iter: 111 loss: 2.8387454139592025e-06
Iter: 112 loss: 2.7678685709264151e-06
Iter: 113 loss: 2.7389899811501551e-06
Iter: 114 loss: 2.7111378727421062e-06
Iter: 115 loss: 2.7047660230245822e-06
Iter: 116 loss: 2.6646534401243387e-06
Iter: 117 loss: 2.872156437347727e-06
Iter: 118 loss: 2.658244752490202e-06
Iter: 119 loss: 2.6434583885491853e-06
Iter: 120 loss: 2.6401810352814611e-06
Iter: 121 loss: 2.6236258386144564e-06
Iter: 122 loss: 2.6769788190752003e-06
Iter: 123 loss: 2.6189465772714185e-06
Iter: 124 loss: 2.605653451733127e-06
Iter: 125 loss: 2.5725486959859341e-06
Iter: 126 loss: 2.8885545911416941e-06
Iter: 127 loss: 2.5680207653145318e-06
Iter: 128 loss: 2.5501115151495944e-06
Iter: 129 loss: 2.548115370408152e-06
Iter: 130 loss: 2.5288885856880017e-06
Iter: 131 loss: 2.5526859865537312e-06
Iter: 132 loss: 2.5188890509237686e-06
Iter: 133 loss: 2.5022158453567408e-06
Iter: 134 loss: 2.5138579851912009e-06
Iter: 135 loss: 2.4918311460361926e-06
Iter: 136 loss: 2.4724899644105836e-06
Iter: 137 loss: 2.5667589498995877e-06
Iter: 138 loss: 2.4691179260143358e-06
Iter: 139 loss: 2.4489045214811174e-06
Iter: 140 loss: 2.50548571466701e-06
Iter: 141 loss: 2.4424271400910428e-06
Iter: 142 loss: 2.4294357522567608e-06
Iter: 143 loss: 2.4139991037755831e-06
Iter: 144 loss: 2.4123734113080496e-06
Iter: 145 loss: 2.395209556648745e-06
Iter: 146 loss: 2.3945039455854784e-06
Iter: 147 loss: 2.385462659518579e-06
Iter: 148 loss: 2.3872264779575432e-06
Iter: 149 loss: 2.378732578787096e-06
Iter: 150 loss: 2.3639477585989397e-06
Iter: 151 loss: 2.3410849447947078e-06
Iter: 152 loss: 2.34075839849845e-06
Iter: 153 loss: 2.321573688629452e-06
Iter: 154 loss: 2.489267286454709e-06
Iter: 155 loss: 2.320581375034709e-06
Iter: 156 loss: 2.3130966841176768e-06
Iter: 157 loss: 2.3109224139506294e-06
Iter: 158 loss: 2.3033058499526444e-06
Iter: 159 loss: 2.29235317604615e-06
Iter: 160 loss: 2.2920057368408325e-06
Iter: 161 loss: 2.2796772244407959e-06
Iter: 162 loss: 2.2694546590118606e-06
Iter: 163 loss: 2.2659115025461433e-06
Iter: 164 loss: 2.2590890332991111e-06
Iter: 165 loss: 2.256948053404632e-06
Iter: 166 loss: 2.24735570824652e-06
Iter: 167 loss: 2.232663667611179e-06
Iter: 168 loss: 2.2324253060250566e-06
Iter: 169 loss: 2.2219490972422281e-06
Iter: 170 loss: 2.2816972859149136e-06
Iter: 171 loss: 2.2205178902583515e-06
Iter: 172 loss: 2.2099699657784548e-06
Iter: 173 loss: 2.2434469122129194e-06
Iter: 174 loss: 2.2069460780274291e-06
Iter: 175 loss: 2.1951745776129052e-06
Iter: 176 loss: 2.2182032701734838e-06
Iter: 177 loss: 2.190302676261978e-06
Iter: 178 loss: 2.1813627630153767e-06
Iter: 179 loss: 2.1754176090829036e-06
Iter: 180 loss: 2.1720568388191641e-06
Iter: 181 loss: 2.1601513280320108e-06
Iter: 182 loss: 2.1600375964626406e-06
Iter: 183 loss: 2.1540563834765586e-06
Iter: 184 loss: 2.1478946630850642e-06
Iter: 185 loss: 2.1467547959445535e-06
Iter: 186 loss: 2.1369681533334841e-06
Iter: 187 loss: 2.1614276315674724e-06
Iter: 188 loss: 2.1335384593127418e-06
Iter: 189 loss: 2.123716699973762e-06
Iter: 190 loss: 2.1690573460104314e-06
Iter: 191 loss: 2.1218698640731687e-06
Iter: 192 loss: 2.1184376532889272e-06
Iter: 193 loss: 2.1169414143333457e-06
Iter: 194 loss: 2.1146520857457354e-06
Iter: 195 loss: 2.1069148654738825e-06
Iter: 196 loss: 2.1023094619642095e-06
Iter: 197 loss: 2.0973255145304326e-06
Iter: 198 loss: 2.0899859201373122e-06
Iter: 199 loss: 2.0891092540469679e-06
Iter: 200 loss: 2.0824513807351985e-06
Iter: 201 loss: 2.128911580968479e-06
Iter: 202 loss: 2.0818343749403316e-06
Iter: 203 loss: 2.0760696240961233e-06
Iter: 204 loss: 2.0723708187274972e-06
Iter: 205 loss: 2.0701274685120115e-06
Iter: 206 loss: 2.0622181575360969e-06
Iter: 207 loss: 2.0696541440763121e-06
Iter: 208 loss: 2.0576791875372982e-06
Iter: 209 loss: 2.0535168789385849e-06
Iter: 210 loss: 2.0529065694501657e-06
Iter: 211 loss: 2.0490353172894696e-06
Iter: 212 loss: 2.0408038413600257e-06
Iter: 213 loss: 2.1737563272930869e-06
Iter: 214 loss: 2.0405497758618438e-06
Iter: 215 loss: 2.0313916319769496e-06
Iter: 216 loss: 2.1047980773055251e-06
Iter: 217 loss: 2.0307839842081757e-06
Iter: 218 loss: 2.0261178499230319e-06
Iter: 219 loss: 2.078012008067031e-06
Iter: 220 loss: 2.0260260393573296e-06
Iter: 221 loss: 2.0214715659707837e-06
Iter: 222 loss: 2.0114811378520967e-06
Iter: 223 loss: 2.1589864666707186e-06
Iter: 224 loss: 2.0110398685632431e-06
Iter: 225 loss: 2.0033734228381439e-06
Iter: 226 loss: 2.0258564733086524e-06
Iter: 227 loss: 2.00101272150464e-06
Iter: 228 loss: 1.9994744972795185e-06
Iter: 229 loss: 1.9966594732086328e-06
Iter: 230 loss: 1.9941285514077646e-06
Iter: 231 loss: 1.9912276565663695e-06
Iter: 232 loss: 1.990869392947645e-06
Iter: 233 loss: 1.9869650112324924e-06
Iter: 234 loss: 1.9783505850277731e-06
Iter: 235 loss: 2.1033362934103761e-06
Iter: 236 loss: 1.9779457268199251e-06
Iter: 237 loss: 1.9719921542402432e-06
Iter: 238 loss: 1.971501155661197e-06
Iter: 239 loss: 1.9658494482842089e-06
Iter: 240 loss: 1.9890604019504717e-06
Iter: 241 loss: 1.9646168421652486e-06
Iter: 242 loss: 1.9610519179927549e-06
Iter: 243 loss: 1.9620331227337343e-06
Iter: 244 loss: 1.9584747780030973e-06
Iter: 245 loss: 1.9538511678791749e-06
Iter: 246 loss: 1.9524712710327243e-06
Iter: 247 loss: 1.949703511804256e-06
Iter: 248 loss: 1.9467696183797552e-06
Iter: 249 loss: 1.9459797423706964e-06
Iter: 250 loss: 1.9432770321753195e-06
Iter: 251 loss: 1.9367661520174614e-06
Iter: 252 loss: 2.007409855067367e-06
Iter: 253 loss: 1.9360807842537458e-06
Iter: 254 loss: 1.9309397988325023e-06
Iter: 255 loss: 1.9309369023903571e-06
Iter: 256 loss: 1.9265385553500113e-06
Iter: 257 loss: 1.9423412673530449e-06
Iter: 258 loss: 1.9254239059233379e-06
Iter: 259 loss: 1.9222711091611213e-06
Iter: 260 loss: 1.9232719288919936e-06
Iter: 261 loss: 1.9200261731814038e-06
Iter: 262 loss: 1.9153598090748588e-06
Iter: 263 loss: 1.92666089597955e-06
Iter: 264 loss: 1.9136849762931116e-06
Iter: 265 loss: 1.9085878158780803e-06
Iter: 266 loss: 1.9769161041121794e-06
Iter: 267 loss: 1.9085631682359402e-06
Iter: 268 loss: 1.9069294961334041e-06
Iter: 269 loss: 1.9025685272112387e-06
Iter: 270 loss: 1.9329317618737069e-06
Iter: 271 loss: 1.9015801628988328e-06
Iter: 272 loss: 1.896380341668001e-06
Iter: 273 loss: 1.9034597879597525e-06
Iter: 274 loss: 1.8937826003886117e-06
Iter: 275 loss: 1.8908047949915954e-06
Iter: 276 loss: 1.8899228696996985e-06
Iter: 277 loss: 1.8874291437472536e-06
Iter: 278 loss: 1.888148583491872e-06
Iter: 279 loss: 1.8856348198301789e-06
Iter: 280 loss: 1.8824384301299071e-06
Iter: 281 loss: 1.8777674026470876e-06
Iter: 282 loss: 1.8776397604424005e-06
Iter: 283 loss: 1.8743023628293548e-06
Iter: 284 loss: 1.8742723884856732e-06
Iter: 285 loss: 1.8708309481186769e-06
Iter: 286 loss: 1.8741535683101952e-06
Iter: 287 loss: 1.8688727423800713e-06
Iter: 288 loss: 1.8649565734956179e-06
Iter: 289 loss: 1.8771035266643223e-06
Iter: 290 loss: 1.8638098798501718e-06
Iter: 291 loss: 1.8613240839945476e-06
Iter: 292 loss: 1.86336871477157e-06
Iter: 293 loss: 1.8598398481664567e-06
Iter: 294 loss: 1.8554829583656981e-06
Iter: 295 loss: 1.8705654809509546e-06
Iter: 296 loss: 1.8543354159883445e-06
Iter: 297 loss: 1.8515636596934809e-06
Iter: 298 loss: 1.853033341799921e-06
Iter: 299 loss: 1.8497334861020052e-06
Iter: 300 loss: 1.8476223834521547e-06
Iter: 301 loss: 1.847356074556568e-06
Iter: 302 loss: 1.8456836065244739e-06
Iter: 303 loss: 1.8424061266772176e-06
Iter: 304 loss: 1.9079091284827157e-06
Iter: 305 loss: 1.8423786634190371e-06
Iter: 306 loss: 1.8386204118455219e-06
Iter: 307 loss: 1.8365055808142706e-06
Iter: 308 loss: 1.8348711112215931e-06
Iter: 309 loss: 1.8313007541847211e-06
Iter: 310 loss: 1.8621685998906165e-06
Iter: 311 loss: 1.831109879022892e-06
Iter: 312 loss: 1.8274843811779649e-06
Iter: 313 loss: 1.8543943451153804e-06
Iter: 314 loss: 1.827192438124352e-06
Iter: 315 loss: 1.8249990801289205e-06
Iter: 316 loss: 1.8250901269919862e-06
Iter: 317 loss: 1.823271420242894e-06
Iter: 318 loss: 1.820258720677766e-06
Iter: 319 loss: 1.8180818356275579e-06
Iter: 320 loss: 1.8170455410452422e-06
Iter: 321 loss: 1.8140658083472983e-06
Iter: 322 loss: 1.8140574337674231e-06
Iter: 323 loss: 1.811067964869604e-06
Iter: 324 loss: 1.8153870305830288e-06
Iter: 325 loss: 1.8096139356969915e-06
Iter: 326 loss: 1.807287797487875e-06
Iter: 327 loss: 1.8091931449962819e-06
Iter: 328 loss: 1.8058972567436942e-06
Iter: 329 loss: 1.8030145100856635e-06
Iter: 330 loss: 1.8192381766194706e-06
Iter: 331 loss: 1.8026118941008152e-06
Iter: 332 loss: 1.7999632238840623e-06
Iter: 333 loss: 1.8102464247552958e-06
Iter: 334 loss: 1.7993463835373377e-06
Iter: 335 loss: 1.7974110036741795e-06
Iter: 336 loss: 1.8052132178133603e-06
Iter: 337 loss: 1.796979539447861e-06
Iter: 338 loss: 1.7945107272048333e-06
Iter: 339 loss: 1.8011134325311464e-06
Iter: 340 loss: 1.7936896975119078e-06
Iter: 341 loss: 1.7922064323638337e-06
Iter: 342 loss: 1.7888958938101126e-06
Iter: 343 loss: 1.8352815754748291e-06
Iter: 344 loss: 1.7887206972211905e-06
Iter: 345 loss: 1.7859993534122886e-06
Iter: 346 loss: 1.82125640188838e-06
Iter: 347 loss: 1.785980763787028e-06
Iter: 348 loss: 1.783777837917434e-06
Iter: 349 loss: 1.7837073290149829e-06
Iter: 350 loss: 1.7819942082776144e-06
Iter: 351 loss: 1.7790044957109043e-06
Iter: 352 loss: 1.7790042836439265e-06
Iter: 353 loss: 1.7777708877401303e-06
Iter: 354 loss: 1.7744367050944403e-06
Iter: 355 loss: 1.7961517096838708e-06
Iter: 356 loss: 1.7736118627538872e-06
Iter: 357 loss: 1.7702926399116504e-06
Iter: 358 loss: 1.8128710557949251e-06
Iter: 359 loss: 1.7702678341788953e-06
Iter: 360 loss: 1.767858256282639e-06
Iter: 361 loss: 1.7860529059006716e-06
Iter: 362 loss: 1.7676720950555858e-06
Iter: 363 loss: 1.7655873025789343e-06
Iter: 364 loss: 1.7691364416596719e-06
Iter: 365 loss: 1.7646531708628521e-06
Iter: 366 loss: 1.7628670123034312e-06
Iter: 367 loss: 1.7617771782948774e-06
Iter: 368 loss: 1.7610498314304013e-06
Iter: 369 loss: 1.7594313102652239e-06
Iter: 370 loss: 1.7593017840483456e-06
Iter: 371 loss: 1.7579497956606752e-06
Iter: 372 loss: 1.7575277022523614e-06
Iter: 373 loss: 1.7567303737074042e-06
Iter: 374 loss: 1.7542316161517781e-06
Iter: 375 loss: 1.768959617913455e-06
Iter: 376 loss: 1.753909329334313e-06
Iter: 377 loss: 1.752362444642569e-06
Iter: 378 loss: 1.7525181386836146e-06
Iter: 379 loss: 1.751170458333695e-06
Iter: 380 loss: 1.7497234975800663e-06
Iter: 381 loss: 1.7464762888373574e-06
Iter: 382 loss: 1.7912154992141929e-06
Iter: 383 loss: 1.7462948013721979e-06
Iter: 384 loss: 1.7449031792452887e-06
Iter: 385 loss: 1.7444716053433154e-06
Iter: 386 loss: 1.7430033772286563e-06
Iter: 387 loss: 1.7500664061770912e-06
Iter: 388 loss: 1.7427425741434231e-06
Iter: 389 loss: 1.7409309591306454e-06
Iter: 390 loss: 1.7394384541810713e-06
Iter: 391 loss: 1.738912929470337e-06
Iter: 392 loss: 1.7369139330702126e-06
Iter: 393 loss: 1.737163545279212e-06
Iter: 394 loss: 1.7353872169046453e-06
Iter: 395 loss: 1.7329103302816659e-06
Iter: 396 loss: 1.7532282229977795e-06
Iter: 397 loss: 1.7327560606357912e-06
Iter: 398 loss: 1.7307090564143965e-06
Iter: 399 loss: 1.7431414121224828e-06
Iter: 400 loss: 1.7304587440065298e-06
Iter: 401 loss: 1.7287458603977391e-06
Iter: 402 loss: 1.7320000591128612e-06
Iter: 403 loss: 1.7280243484491668e-06
Iter: 404 loss: 1.7267407600492054e-06
Iter: 405 loss: 1.7258630313901587e-06
Iter: 406 loss: 1.7253939886609532e-06
Iter: 407 loss: 1.7239581742385245e-06
Iter: 408 loss: 1.7238495998948932e-06
Iter: 409 loss: 1.7228622040086499e-06
Iter: 410 loss: 1.72470991579509e-06
Iter: 411 loss: 1.7224425494590265e-06
Iter: 412 loss: 1.7209893179599526e-06
Iter: 413 loss: 1.7189847385254417e-06
Iter: 414 loss: 1.7188956399189284e-06
Iter: 415 loss: 1.7171437456811765e-06
Iter: 416 loss: 1.7191586560489017e-06
Iter: 417 loss: 1.716206092786824e-06
Iter: 418 loss: 1.714313438580274e-06
Iter: 419 loss: 1.7218941243024219e-06
Iter: 420 loss: 1.7138882276826675e-06
Iter: 421 loss: 1.7118776784102583e-06
Iter: 422 loss: 1.7149194274604964e-06
Iter: 423 loss: 1.7109208753013511e-06
Iter: 424 loss: 1.7087191487981188e-06
Iter: 425 loss: 1.7420040816473469e-06
Iter: 426 loss: 1.7087180283280184e-06
Iter: 427 loss: 1.7080461373525372e-06
Iter: 428 loss: 1.7065085298266957e-06
Iter: 429 loss: 1.7264203174000611e-06
Iter: 430 loss: 1.7064048655742456e-06
Iter: 431 loss: 1.7040884143393672e-06
Iter: 432 loss: 1.7117709158717573e-06
Iter: 433 loss: 1.7034516960686075e-06
Iter: 434 loss: 1.7021927656358732e-06
Iter: 435 loss: 1.7110206341972817e-06
Iter: 436 loss: 1.7020773292805045e-06
Iter: 437 loss: 1.7004670426674486e-06
Iter: 438 loss: 1.7014379314555644e-06
Iter: 439 loss: 1.6994303479986557e-06
Iter: 440 loss: 1.6977713303487345e-06
Iter: 441 loss: 1.7000190566707498e-06
Iter: 442 loss: 1.6969407574531958e-06
Iter: 443 loss: 1.695615964454513e-06
Iter: 444 loss: 1.6956154633640643e-06
Iter: 445 loss: 1.694447052802345e-06
Iter: 446 loss: 1.6980739883230357e-06
Iter: 447 loss: 1.6941051810497906e-06
Iter: 448 loss: 1.6932367074112116e-06
Iter: 449 loss: 1.6945721736896657e-06
Iter: 450 loss: 1.6928266655662989e-06
Iter: 451 loss: 1.6916203414572996e-06
Iter: 452 loss: 1.6897157893659963e-06
Iter: 453 loss: 1.6896958241470049e-06
Iter: 454 loss: 1.6880575043244339e-06
Iter: 455 loss: 1.6913415336740413e-06
Iter: 456 loss: 1.6873894764739957e-06
Iter: 457 loss: 1.685294303177954e-06
Iter: 458 loss: 1.6941966589102108e-06
Iter: 459 loss: 1.6848559200764862e-06
Iter: 460 loss: 1.6837106330609387e-06
Iter: 461 loss: 1.683707127081558e-06
Iter: 462 loss: 1.6825138546168865e-06
Iter: 463 loss: 1.6808545239417293e-06
Iter: 464 loss: 1.6807851013798313e-06
Iter: 465 loss: 1.679231820978729e-06
Iter: 466 loss: 1.6780803519600416e-06
Iter: 467 loss: 1.6775619083643344e-06
Iter: 468 loss: 1.6760000596045512e-06
Iter: 469 loss: 1.6759867583074164e-06
Iter: 470 loss: 1.6746133382424146e-06
Iter: 471 loss: 1.6791010942523221e-06
Iter: 472 loss: 1.6742304053710251e-06
Iter: 473 loss: 1.6727574095661853e-06
Iter: 474 loss: 1.6767605510925715e-06
Iter: 475 loss: 1.672273854410807e-06
Iter: 476 loss: 1.6712374697755083e-06
Iter: 477 loss: 1.672037047345401e-06
Iter: 478 loss: 1.6706078845399661e-06
Iter: 479 loss: 1.6689595382902956e-06
Iter: 480 loss: 1.682232163586409e-06
Iter: 481 loss: 1.66885145752708e-06
Iter: 482 loss: 1.6679566966153771e-06
Iter: 483 loss: 1.6678696966558495e-06
Iter: 484 loss: 1.6672141254257414e-06
Iter: 485 loss: 1.6660144949091152e-06
Iter: 486 loss: 1.6709057760271683e-06
Iter: 487 loss: 1.6657505886662532e-06
Iter: 488 loss: 1.6648608643548484e-06
Iter: 489 loss: 1.6645230449763129e-06
Iter: 490 loss: 1.6640366681176693e-06
Iter: 491 loss: 1.6624922931632568e-06
Iter: 492 loss: 1.6618451807713509e-06
Iter: 493 loss: 1.6610390078832231e-06
Iter: 494 loss: 1.6595733532184345e-06
Iter: 495 loss: 1.6788138271064799e-06
Iter: 496 loss: 1.6595645251878216e-06
Iter: 497 loss: 1.6580683997006314e-06
Iter: 498 loss: 1.665407903551981e-06
Iter: 499 loss: 1.657809922146697e-06
Iter: 500 loss: 1.6568779140383362e-06
Iter: 501 loss: 1.6563973496496268e-06
Iter: 502 loss: 1.6559659199507191e-06
Iter: 503 loss: 1.6545865128809016e-06
Iter: 504 loss: 1.6568034714091533e-06
Iter: 505 loss: 1.6539495087437165e-06
Iter: 506 loss: 1.6526027034212166e-06
Iter: 507 loss: 1.6547649555218666e-06
Iter: 508 loss: 1.6519803966037008e-06
Iter: 509 loss: 1.6510148226824787e-06
Iter: 510 loss: 1.6509440003903785e-06
Iter: 511 loss: 1.6502596818101386e-06
Iter: 512 loss: 1.6497298823229477e-06
Iter: 513 loss: 1.6495135463078768e-06
Iter: 514 loss: 1.6483944155594912e-06
Iter: 515 loss: 1.6576251830904195e-06
Iter: 516 loss: 1.6483258229108271e-06
Iter: 517 loss: 1.6470632848120762e-06
Iter: 518 loss: 1.6474160872524676e-06
Iter: 519 loss: 1.646151805298112e-06
Iter: 520 loss: 1.6454228459498159e-06
Iter: 521 loss: 1.6459055807159416e-06
Iter: 522 loss: 1.6449630771980201e-06
Iter: 523 loss: 1.6435892217579225e-06
Iter: 524 loss: 1.6444832764941374e-06
Iter: 525 loss: 1.6427192881166644e-06
Iter: 526 loss: 1.6414764990105016e-06
Iter: 527 loss: 1.6424546373971666e-06
Iter: 528 loss: 1.6407253902342129e-06
Iter: 529 loss: 1.639131512090691e-06
Iter: 530 loss: 1.6453195900492574e-06
Iter: 531 loss: 1.6387603386934642e-06
Iter: 532 loss: 1.6376603106320431e-06
Iter: 533 loss: 1.6439579221268524e-06
Iter: 534 loss: 1.6375110348744747e-06
Iter: 535 loss: 1.6359681863071442e-06
Iter: 536 loss: 1.6363782293171889e-06
Iter: 537 loss: 1.6348489376824059e-06
Iter: 538 loss: 1.6338858948672502e-06
Iter: 539 loss: 1.6350835676924967e-06
Iter: 540 loss: 1.6333859922767243e-06
Iter: 541 loss: 1.6320403348630459e-06
Iter: 542 loss: 1.6326690654790382e-06
Iter: 543 loss: 1.6311318452196352e-06
Iter: 544 loss: 1.6297745408899716e-06
Iter: 545 loss: 1.6360000951210183e-06
Iter: 546 loss: 1.6295171241527146e-06
Iter: 547 loss: 1.6283608162341442e-06
Iter: 548 loss: 1.6455043514992865e-06
Iter: 549 loss: 1.6283597620569891e-06
Iter: 550 loss: 1.6276653286850465e-06
Iter: 551 loss: 1.6278224908882247e-06
Iter: 552 loss: 1.6271542678507329e-06
Iter: 553 loss: 1.6258291553063512e-06
Iter: 554 loss: 1.6304002045006314e-06
Iter: 555 loss: 1.6254790664118198e-06
Iter: 556 loss: 1.6248536227554369e-06
Iter: 557 loss: 1.6248555384649586e-06
Iter: 558 loss: 1.6243538356427055e-06
Iter: 559 loss: 1.6234922233796121e-06
Iter: 560 loss: 1.6250109918097642e-06
Iter: 561 loss: 1.6231133934415337e-06
Iter: 562 loss: 1.6219155466605076e-06
Iter: 563 loss: 1.6253056703930152e-06
Iter: 564 loss: 1.6215353360845859e-06
Iter: 565 loss: 1.620659844720496e-06
Iter: 566 loss: 1.6192148267158562e-06
Iter: 567 loss: 1.6192087138398459e-06
Iter: 568 loss: 1.6177577735940924e-06
Iter: 569 loss: 1.6408986701592533e-06
Iter: 570 loss: 1.6177577712749348e-06
Iter: 571 loss: 1.617034969573201e-06
Iter: 572 loss: 1.6275689518094715e-06
Iter: 573 loss: 1.6170339802154843e-06
Iter: 574 loss: 1.6163882698984849e-06
Iter: 575 loss: 1.6152583701018078e-06
Iter: 576 loss: 1.6152581433487647e-06
Iter: 577 loss: 1.6139413526143964e-06
Iter: 578 loss: 1.6150723825802635e-06
Iter: 579 loss: 1.6131647238526974e-06
Iter: 580 loss: 1.6120211150412372e-06
Iter: 581 loss: 1.61753018518954e-06
Iter: 582 loss: 1.6118183737690509e-06
Iter: 583 loss: 1.6104217952428683e-06
Iter: 584 loss: 1.6125346801637008e-06
Iter: 585 loss: 1.609757176904604e-06
Iter: 586 loss: 1.6091175576621473e-06
Iter: 587 loss: 1.6089169466161394e-06
Iter: 588 loss: 1.6085043521743372e-06
Iter: 589 loss: 1.6095140538469269e-06
Iter: 590 loss: 1.6083574147821735e-06
Iter: 591 loss: 1.6077298683449106e-06
Iter: 592 loss: 1.6063623027164648e-06
Iter: 593 loss: 1.626941478137005e-06
Iter: 594 loss: 1.6063060482876004e-06
Iter: 595 loss: 1.60524096483096e-06
Iter: 596 loss: 1.6128251504583024e-06
Iter: 597 loss: 1.6051465871799109e-06
Iter: 598 loss: 1.6042991950426589e-06
Iter: 599 loss: 1.6055294299437483e-06
Iter: 600 loss: 1.6038879761872604e-06
Iter: 601 loss: 1.6025428991843609e-06
Iter: 602 loss: 1.605084551754356e-06
Iter: 603 loss: 1.6019745046230829e-06
Iter: 604 loss: 1.6010089585693314e-06
Iter: 605 loss: 1.6003342088505686e-06
Iter: 606 loss: 1.599989475376158e-06
Iter: 607 loss: 1.5991773844075116e-06
Iter: 608 loss: 1.5991647870311991e-06
Iter: 609 loss: 1.5981285030240591e-06
Iter: 610 loss: 1.5974545288500892e-06
Iter: 611 loss: 1.5970564287352037e-06
Iter: 612 loss: 1.5960221494818582e-06
Iter: 613 loss: 1.6009676238414933e-06
Iter: 614 loss: 1.5958368530876152e-06
Iter: 615 loss: 1.5949395373520628e-06
Iter: 616 loss: 1.5938955750412194e-06
Iter: 617 loss: 1.5937746864063267e-06
Iter: 618 loss: 1.5923823969651704e-06
Iter: 619 loss: 1.5982954423731203e-06
Iter: 620 loss: 1.5920909526136415e-06
Iter: 621 loss: 1.5914750294117398e-06
Iter: 622 loss: 1.5912966595472728e-06
Iter: 623 loss: 1.590780491893355e-06
Iter: 624 loss: 1.5920810178482625e-06
Iter: 625 loss: 1.5906006607201715e-06
Iter: 626 loss: 1.5900406992766261e-06
Iter: 627 loss: 1.5905377325645448e-06
Iter: 628 loss: 1.5897136176944937e-06
Iter: 629 loss: 1.589035370754089e-06
Iter: 630 loss: 1.5884840198692124e-06
Iter: 631 loss: 1.5882834196252787e-06
Iter: 632 loss: 1.5873497701933623e-06
Iter: 633 loss: 1.588129719649288e-06
Iter: 634 loss: 1.5867947241947269e-06
Iter: 635 loss: 1.5854935968772353e-06
Iter: 636 loss: 1.5979700728557524e-06
Iter: 637 loss: 1.5854444010177573e-06
Iter: 638 loss: 1.5846728944556093e-06
Iter: 639 loss: 1.5849686979262628e-06
Iter: 640 loss: 1.5841363470000966e-06
Iter: 641 loss: 1.58305791939255e-06
Iter: 642 loss: 1.5830891801883469e-06
Iter: 643 loss: 1.582204495661834e-06
Iter: 644 loss: 1.5817589897355736e-06
Iter: 645 loss: 1.5815643199706931e-06
Iter: 646 loss: 1.5809783077011874e-06
Iter: 647 loss: 1.5804088953599855e-06
Iter: 648 loss: 1.5802815769993464e-06
Iter: 649 loss: 1.5794773936263427e-06
Iter: 650 loss: 1.5786870048186094e-06
Iter: 651 loss: 1.578516470388767e-06
Iter: 652 loss: 1.577341952411898e-06
Iter: 653 loss: 1.5902476628620597e-06
Iter: 654 loss: 1.5773173491843985e-06
Iter: 655 loss: 1.576488092767561e-06
Iter: 656 loss: 1.5768314553533529e-06
Iter: 657 loss: 1.5759176994383348e-06
Iter: 658 loss: 1.5751647057535804e-06
Iter: 659 loss: 1.5750818486500548e-06
Iter: 660 loss: 1.5746719701934095e-06
Iter: 661 loss: 1.5743190917661309e-06
Iter: 662 loss: 1.5742079661413065e-06
Iter: 663 loss: 1.5733073769060238e-06
Iter: 664 loss: 1.5734985010930686e-06
Iter: 665 loss: 1.5726412201022955e-06
Iter: 666 loss: 1.5716542660071583e-06
Iter: 667 loss: 1.57266736867764e-06
Iter: 668 loss: 1.5711039246073158e-06
Iter: 669 loss: 1.569921189983442e-06
Iter: 670 loss: 1.5730934166323203e-06
Iter: 671 loss: 1.5695287900565877e-06
Iter: 672 loss: 1.5688197889730451e-06
Iter: 673 loss: 1.5688159144960274e-06
Iter: 674 loss: 1.5682173572499169e-06
Iter: 675 loss: 1.5671310572937338e-06
Iter: 676 loss: 1.5930934895971002e-06
Iter: 677 loss: 1.5671306763130065e-06
Iter: 678 loss: 1.5662367349588997e-06
Iter: 679 loss: 1.5716973652610871e-06
Iter: 680 loss: 1.5661285785853993e-06
Iter: 681 loss: 1.5650991583464189e-06
Iter: 682 loss: 1.5696941236809706e-06
Iter: 683 loss: 1.5648968142165759e-06
Iter: 684 loss: 1.5638744663611915e-06
Iter: 685 loss: 1.5646741208730934e-06
Iter: 686 loss: 1.5632555999181591e-06
Iter: 687 loss: 1.5625712784208516e-06
Iter: 688 loss: 1.5612087027114077e-06
Iter: 689 loss: 1.5873896072590516e-06
Iter: 690 loss: 1.5611930028859719e-06
Iter: 691 loss: 1.5600713613629238e-06
Iter: 692 loss: 1.5600550632691107e-06
Iter: 693 loss: 1.5594629819342303e-06
Iter: 694 loss: 1.5594627576547378e-06
Iter: 695 loss: 1.5588526365432083e-06
Iter: 696 loss: 1.5592053435640929e-06
Iter: 697 loss: 1.5584564028760621e-06
Iter: 698 loss: 1.5578831339946963e-06
Iter: 699 loss: 1.5576741204188862e-06
Iter: 700 loss: 1.5573552696146922e-06
Iter: 701 loss: 1.5561915363889586e-06
Iter: 702 loss: 1.5590847474664935e-06
Iter: 703 loss: 1.5557819615872264e-06
Iter: 704 loss: 1.5550983614554708e-06
Iter: 705 loss: 1.554302458466469e-06
Iter: 706 loss: 1.5542105904397522e-06
Iter: 707 loss: 1.5529054134414287e-06
Iter: 708 loss: 1.5633597368519493e-06
Iter: 709 loss: 1.5528187059950595e-06
Iter: 710 loss: 1.5520906428006683e-06
Iter: 711 loss: 1.5608956244427172e-06
Iter: 712 loss: 1.5520820142086343e-06
Iter: 713 loss: 1.551553935256947e-06
Iter: 714 loss: 1.5504637035931155e-06
Iter: 715 loss: 1.5695487555430376e-06
Iter: 716 loss: 1.5504409982706032e-06
Iter: 717 loss: 1.5496212797099205e-06
Iter: 718 loss: 1.5496183158427819e-06
Iter: 719 loss: 1.5487902776636563e-06
Iter: 720 loss: 1.5503419905702845e-06
Iter: 721 loss: 1.5484386752056169e-06
Iter: 722 loss: 1.5476867742024024e-06
Iter: 723 loss: 1.5485037598939798e-06
Iter: 724 loss: 1.5472757949393329e-06
Iter: 725 loss: 1.5465222352685876e-06
Iter: 726 loss: 1.5455932262153044e-06
Iter: 727 loss: 1.5455113530378768e-06
Iter: 728 loss: 1.5445690508905353e-06
Iter: 729 loss: 1.5564306004909649e-06
Iter: 730 loss: 1.5445607747597318e-06
Iter: 731 loss: 1.5439119349540971e-06
Iter: 732 loss: 1.5439048750166329e-06
Iter: 733 loss: 1.5434965829206287e-06
Iter: 734 loss: 1.5425392243515755e-06
Iter: 735 loss: 1.5539748710071433e-06
Iter: 736 loss: 1.5424589421599685e-06
Iter: 737 loss: 1.5414793403358841e-06
Iter: 738 loss: 1.5506149368505409e-06
Iter: 739 loss: 1.5414384130164842e-06
Iter: 740 loss: 1.5406687192609298e-06
Iter: 741 loss: 1.5429206966675519e-06
Iter: 742 loss: 1.540431261809674e-06
Iter: 743 loss: 1.5397788793153722e-06
Iter: 744 loss: 1.5386504834320578e-06
Iter: 745 loss: 1.5386498083691588e-06
Iter: 746 loss: 1.537971393010416e-06
Iter: 747 loss: 1.5379364897543178e-06
Iter: 748 loss: 1.5372125186725769e-06
Iter: 749 loss: 1.5374207842863296e-06
Iter: 750 loss: 1.5366914615761125e-06
Iter: 751 loss: 1.5357409003967995e-06
Iter: 752 loss: 1.5383217840821474e-06
Iter: 753 loss: 1.5354285968431664e-06
Iter: 754 loss: 1.5347123278931944e-06
Iter: 755 loss: 1.53842360380048e-06
Iter: 756 loss: 1.5345981974693835e-06
Iter: 757 loss: 1.5337538366640178e-06
Iter: 758 loss: 1.5357904252954348e-06
Iter: 759 loss: 1.533449916439576e-06
Iter: 760 loss: 1.532835832795906e-06
Iter: 761 loss: 1.5315379794883184e-06
Iter: 762 loss: 1.5528613326725945e-06
Iter: 763 loss: 1.5315008540187304e-06
Iter: 764 loss: 1.5304769981363823e-06
Iter: 765 loss: 1.5466481098119205e-06
Iter: 766 loss: 1.530476971568091e-06
Iter: 767 loss: 1.5297421969396059e-06
Iter: 768 loss: 1.5347861613630345e-06
Iter: 769 loss: 1.5296716039297084e-06
Iter: 770 loss: 1.5288466095385908e-06
Iter: 771 loss: 1.5319658884566592e-06
Iter: 772 loss: 1.5286487782689429e-06
Iter: 773 loss: 1.5283120476003842e-06
Iter: 774 loss: 1.5274475992506146e-06
Iter: 775 loss: 1.5347050544933112e-06
Iter: 776 loss: 1.5272993221534979e-06
Iter: 777 loss: 1.5263418862322615e-06
Iter: 778 loss: 1.5263393653776065e-06
Iter: 779 loss: 1.5257898258665453e-06
Iter: 780 loss: 1.5272308303968478e-06
Iter: 781 loss: 1.525604188047523e-06
Iter: 782 loss: 1.5250910415061695e-06
Iter: 783 loss: 1.5238391450422963e-06
Iter: 784 loss: 1.5367975401017496e-06
Iter: 785 loss: 1.5236932862530629e-06
Iter: 786 loss: 1.523181029907233e-06
Iter: 787 loss: 1.5229349390130965e-06
Iter: 788 loss: 1.5222377133118448e-06
Iter: 789 loss: 1.5224647959235208e-06
Iter: 790 loss: 1.5217427082784478e-06
Iter: 791 loss: 1.5210289366231832e-06
Iter: 792 loss: 1.5227438200393366e-06
Iter: 793 loss: 1.5207712788815965e-06
Iter: 794 loss: 1.5199846949336827e-06
Iter: 795 loss: 1.5257090513366774e-06
Iter: 796 loss: 1.5199183647314445e-06
Iter: 797 loss: 1.5192220717523862e-06
Iter: 798 loss: 1.5194745573359817e-06
Iter: 799 loss: 1.5187342241469293e-06
Iter: 800 loss: 1.51814752510954e-06
Iter: 801 loss: 1.5169256365876563e-06
Iter: 802 loss: 1.5378202119358813e-06
Iter: 803 loss: 1.5168968498313346e-06
Iter: 804 loss: 1.5162664310409507e-06
Iter: 805 loss: 1.5160783451588615e-06
Iter: 806 loss: 1.515508829154447e-06
Iter: 807 loss: 1.5224318418379108e-06
Iter: 808 loss: 1.5155023105481989e-06
Iter: 809 loss: 1.5151125062507342e-06
Iter: 810 loss: 1.5142598486776135e-06
Iter: 811 loss: 1.5269545360921911e-06
Iter: 812 loss: 1.5142233154446366e-06
Iter: 813 loss: 1.5134048538567837e-06
Iter: 814 loss: 1.5142304084254278e-06
Iter: 815 loss: 1.5129458075510633e-06
Iter: 816 loss: 1.51224367865115e-06
Iter: 817 loss: 1.5122155181892406e-06
Iter: 818 loss: 1.5117369505301715e-06
Iter: 819 loss: 1.510779382502285e-06
Iter: 820 loss: 1.5289559573852108e-06
Iter: 821 loss: 1.5107673189730359e-06
Iter: 822 loss: 1.5098152292306992e-06
Iter: 823 loss: 1.5168279657471217e-06
Iter: 824 loss: 1.5097371716050007e-06
Iter: 825 loss: 1.5089872081912783e-06
Iter: 826 loss: 1.5118297993190992e-06
Iter: 827 loss: 1.5088078089150851e-06
Iter: 828 loss: 1.5080167511245398e-06
Iter: 829 loss: 1.5115650590163211e-06
Iter: 830 loss: 1.5078622505335955e-06
Iter: 831 loss: 1.5072738080881264e-06
Iter: 832 loss: 1.5071286689083143e-06
Iter: 833 loss: 1.5067565310917251e-06
Iter: 834 loss: 1.5060410113362317e-06
Iter: 835 loss: 1.5060397071262439e-06
Iter: 836 loss: 1.5056568182367585e-06
Iter: 837 loss: 1.5048091942716323e-06
Iter: 838 loss: 1.5169863820207207e-06
Iter: 839 loss: 1.5047679430960847e-06
Iter: 840 loss: 1.5037100439715023e-06
Iter: 841 loss: 1.5085147086368123e-06
Iter: 842 loss: 1.5035067441852251e-06
Iter: 843 loss: 1.5030189455550538e-06
Iter: 844 loss: 1.5030183249800959e-06
Iter: 845 loss: 1.5023406824132189e-06
Iter: 846 loss: 1.5009859214124502e-06
Iter: 847 loss: 1.5267757125190619e-06
Iter: 848 loss: 1.500969129341626e-06
Iter: 849 loss: 1.5000954335204468e-06
Iter: 850 loss: 1.5028322517562452e-06
Iter: 851 loss: 1.4998418679556606e-06
Iter: 852 loss: 1.4990976485438004e-06
Iter: 853 loss: 1.5027117543518661e-06
Iter: 854 loss: 1.4989672236945951e-06
Iter: 855 loss: 1.4983142314615486e-06
Iter: 856 loss: 1.5023215603694218e-06
Iter: 857 loss: 1.4982359035182395e-06
Iter: 858 loss: 1.4975452624628507e-06
Iter: 859 loss: 1.4965211750684689e-06
Iter: 860 loss: 1.4964969947552526e-06
Iter: 861 loss: 1.4956085508706976e-06
Iter: 862 loss: 1.4997931717015323e-06
Iter: 863 loss: 1.4954460392024556e-06
Iter: 864 loss: 1.4945462009009616e-06
Iter: 865 loss: 1.4991399399379894e-06
Iter: 866 loss: 1.4943995486370968e-06
Iter: 867 loss: 1.4937711161136822e-06
Iter: 868 loss: 1.5003686895224015e-06
Iter: 869 loss: 1.4937547153663982e-06
Iter: 870 loss: 1.4934110237642937e-06
Iter: 871 loss: 1.492960679903376e-06
Iter: 872 loss: 1.4929324047305004e-06
Iter: 873 loss: 1.4919382963516428e-06
Iter: 874 loss: 1.4944221982968112e-06
Iter: 875 loss: 1.4915897485136977e-06
Iter: 876 loss: 1.4910260759156126e-06
Iter: 877 loss: 1.4902458108655596e-06
Iter: 878 loss: 1.4902120263484803e-06
Iter: 879 loss: 1.4892784084535282e-06
Iter: 880 loss: 1.5022859013071367e-06
Iter: 881 loss: 1.489275608019455e-06
Iter: 882 loss: 1.4887801219112914e-06
Iter: 883 loss: 1.4887727768590176e-06
Iter: 884 loss: 1.4885221087959361e-06
Iter: 885 loss: 1.487833888268107e-06
Iter: 886 loss: 1.4919399443629933e-06
Iter: 887 loss: 1.4876444063072287e-06
Iter: 888 loss: 1.4867700529450149e-06
Iter: 889 loss: 1.48669838894028e-06
Iter: 890 loss: 1.4860485704136363e-06
Iter: 891 loss: 1.4854971221030188e-06
Iter: 892 loss: 1.4853256258127894e-06
Iter: 893 loss: 1.4847939980867188e-06
Iter: 894 loss: 1.4853566595377747e-06
Iter: 895 loss: 1.4845007102830739e-06
Iter: 896 loss: 1.4837939126535291e-06
Iter: 897 loss: 1.4831118488565788e-06
Iter: 898 loss: 1.4829560537227802e-06
Iter: 899 loss: 1.481932170255099e-06
Iter: 900 loss: 1.4843675153926757e-06
Iter: 901 loss: 1.4815599371662372e-06
Iter: 902 loss: 1.4805358881103845e-06
Iter: 903 loss: 1.4952052528882427e-06
Iter: 904 loss: 1.4805339275005401e-06
Iter: 905 loss: 1.479945013156401e-06
Iter: 906 loss: 1.4813386965891896e-06
Iter: 907 loss: 1.479730091395431e-06
Iter: 908 loss: 1.4792350476962106e-06
Iter: 909 loss: 1.4815525916366759e-06
Iter: 910 loss: 1.4791437309214016e-06
Iter: 911 loss: 1.4785585941271808e-06
Iter: 912 loss: 1.4781541215633839e-06
Iter: 913 loss: 1.4779427449379371e-06
Iter: 914 loss: 1.4769416205117709e-06
Iter: 915 loss: 1.4766219490203282e-06
Iter: 916 loss: 1.4760361707454136e-06
Iter: 917 loss: 1.4767901680195947e-06
Iter: 918 loss: 1.4757071145216976e-06
Iter: 919 loss: 1.4754241662932812e-06
Iter: 920 loss: 1.4747072875572104e-06
Iter: 921 loss: 1.4810782226986686e-06
Iter: 922 loss: 1.4745956564209006e-06
Iter: 923 loss: 1.4737140943852362e-06
Iter: 924 loss: 1.4754877640378892e-06
Iter: 925 loss: 1.4733554100456227e-06
Iter: 926 loss: 1.4726142599992679e-06
Iter: 927 loss: 1.4724407254942224e-06
Iter: 928 loss: 1.4719659050507143e-06
Iter: 929 loss: 1.4713098174891606e-06
Iter: 930 loss: 1.4712216745505909e-06
Iter: 931 loss: 1.4706683134869344e-06
Iter: 932 loss: 1.4698749047403408e-06
Iter: 933 loss: 1.469849075528212e-06
Iter: 934 loss: 1.4689059865611203e-06
Iter: 935 loss: 1.4741979279535647e-06
Iter: 936 loss: 1.4687735810368349e-06
Iter: 937 loss: 1.4681640295348806e-06
Iter: 938 loss: 1.4691144906089969e-06
Iter: 939 loss: 1.4678782407230032e-06
Iter: 940 loss: 1.4669526979680737e-06
Iter: 941 loss: 1.4714166295569226e-06
Iter: 942 loss: 1.4667888830618155e-06
Iter: 943 loss: 1.4661613886307811e-06
Iter: 944 loss: 1.4668575776138596e-06
Iter: 945 loss: 1.4658209978268757e-06
Iter: 946 loss: 1.4648383558567237e-06
Iter: 947 loss: 1.4672428066838793e-06
Iter: 948 loss: 1.4644883235842509e-06
Iter: 949 loss: 1.4638352992033511e-06
Iter: 950 loss: 1.4656358158726015e-06
Iter: 951 loss: 1.4636234461340658e-06
Iter: 952 loss: 1.4631196418444156e-06
Iter: 953 loss: 1.4646292121315614e-06
Iter: 954 loss: 1.4629674674574038e-06
Iter: 955 loss: 1.4622019983108058e-06
Iter: 956 loss: 1.4645086503255003e-06
Iter: 957 loss: 1.4619717921552127e-06
Iter: 958 loss: 1.4615215043586068e-06
Iter: 959 loss: 1.4604574249127921e-06
Iter: 960 loss: 1.4728248567658449e-06
Iter: 961 loss: 1.4603619887360735e-06
Iter: 962 loss: 1.4595658847885952e-06
Iter: 963 loss: 1.4595654065667231e-06
Iter: 964 loss: 1.4589146369890379e-06
Iter: 965 loss: 1.458250268151151e-06
Iter: 966 loss: 1.4581235642207387e-06
Iter: 967 loss: 1.4570530940257713e-06
Iter: 968 loss: 1.4570519033228603e-06
Iter: 969 loss: 1.456648331081297e-06
Iter: 970 loss: 1.4557759479318368e-06
Iter: 971 loss: 1.4692181806075464e-06
Iter: 972 loss: 1.4557431899202805e-06
Iter: 973 loss: 1.4547210993965061e-06
Iter: 974 loss: 1.4642973000191091e-06
Iter: 975 loss: 1.4546790447061896e-06
Iter: 976 loss: 1.4541010898378937e-06
Iter: 977 loss: 1.4618196942336003e-06
Iter: 978 loss: 1.4540981799730982e-06
Iter: 979 loss: 1.4536132857468738e-06
Iter: 980 loss: 1.452932669542605e-06
Iter: 981 loss: 1.4529062069719355e-06
Iter: 982 loss: 1.4521881798782459e-06
Iter: 983 loss: 1.4521881525296678e-06
Iter: 984 loss: 1.451677416959171e-06
Iter: 985 loss: 1.4507026317242321e-06
Iter: 986 loss: 1.4714715319952277e-06
Iter: 987 loss: 1.4506984043336929e-06
Iter: 988 loss: 1.4499621891227978e-06
Iter: 989 loss: 1.4499599746070477e-06
Iter: 990 loss: 1.4493540755784608e-06
Iter: 991 loss: 1.4539637574710144e-06
Iter: 992 loss: 1.4493081700374e-06
Iter: 993 loss: 1.4489473248807681e-06
Iter: 994 loss: 1.4482678653484609e-06
Iter: 995 loss: 1.463210965742103e-06
Iter: 996 loss: 1.4482659583815838e-06
Iter: 997 loss: 1.4475868871152315e-06
Iter: 998 loss: 1.4473596318626354e-06
Iter: 999 loss: 1.4469690003011717e-06
Iter: 1000 loss: 1.4460589903566639e-06
Iter: 1001 loss: 1.4528668047137679e-06
Iter: 1002 loss: 1.4459870609512757e-06
Iter: 1003 loss: 1.4452071986039713e-06
Iter: 1004 loss: 1.4530077387136725e-06
Iter: 1005 loss: 1.445182151784515e-06
Iter: 1006 loss: 1.4446644398853466e-06
Iter: 1007 loss: 1.4450471730633717e-06
Iter: 1008 loss: 1.4443464337694069e-06
Iter: 1009 loss: 1.4436852197454727e-06
Iter: 1010 loss: 1.4430093042151846e-06
Iter: 1011 loss: 1.4428809715531924e-06
Iter: 1012 loss: 1.4424632547368232e-06
Iter: 1013 loss: 1.4423732783882594e-06
Iter: 1014 loss: 1.4418249825344607e-06
Iter: 1015 loss: 1.4412051998621819e-06
Iter: 1016 loss: 1.4411241209161393e-06
Iter: 1017 loss: 1.4402967422410641e-06
Iter: 1018 loss: 1.4476606753350658e-06
Iter: 1019 loss: 1.4402563428567152e-06
Iter: 1020 loss: 1.439687977158049e-06
Iter: 1021 loss: 1.4414472162993402e-06
Iter: 1022 loss: 1.4395212245383583e-06
Iter: 1023 loss: 1.4389976875341429e-06
Iter: 1024 loss: 1.4388846583510472e-06
Iter: 1025 loss: 1.4385429776648313e-06
Iter: 1026 loss: 1.4380560167299249e-06
Iter: 1027 loss: 1.4380105974713025e-06
Iter: 1028 loss: 1.4377243900545481e-06
Iter: 1029 loss: 1.4369127753591458e-06
Iter: 1030 loss: 1.4408499557513634e-06
Iter: 1031 loss: 1.4366341240485393e-06
Iter: 1032 loss: 1.4358322263204221e-06
Iter: 1033 loss: 1.439979629774325e-06
Iter: 1034 loss: 1.4357040752026011e-06
Iter: 1035 loss: 1.434710841276327e-06
Iter: 1036 loss: 1.4345757560638925e-06
Iter: 1037 loss: 1.4338743106131143e-06
Iter: 1038 loss: 1.4336509574961906e-06
Iter: 1039 loss: 1.4334351165271586e-06
Iter: 1040 loss: 1.4330021911011617e-06
Iter: 1041 loss: 1.4323634890644595e-06
Iter: 1042 loss: 1.43234760120951e-06
Iter: 1043 loss: 1.4314594795134733e-06
Iter: 1044 loss: 1.4340255573834827e-06
Iter: 1045 loss: 1.4311825778290286e-06
Iter: 1046 loss: 1.4304249024264165e-06
Iter: 1047 loss: 1.4322501654382849e-06
Iter: 1048 loss: 1.4301519610858279e-06
Iter: 1049 loss: 1.4293570618588455e-06
Iter: 1050 loss: 1.4388162182628793e-06
Iter: 1051 loss: 1.4293465437643274e-06
Iter: 1052 loss: 1.4289152429175711e-06
Iter: 1053 loss: 1.42845121903109e-06
Iter: 1054 loss: 1.4283776369884493e-06
Iter: 1055 loss: 1.4275644921916074e-06
Iter: 1056 loss: 1.4341016320286779e-06
Iter: 1057 loss: 1.4275110089589838e-06
Iter: 1058 loss: 1.42706534272203e-06
Iter: 1059 loss: 1.4274815879128945e-06
Iter: 1060 loss: 1.4268090200550039e-06
Iter: 1061 loss: 1.4261884515668024e-06
Iter: 1062 loss: 1.4311491376799472e-06
Iter: 1063 loss: 1.426147041450931e-06
Iter: 1064 loss: 1.4257027221696496e-06
Iter: 1065 loss: 1.4250558734480326e-06
Iter: 1066 loss: 1.4250375369975216e-06
Iter: 1067 loss: 1.4243899152904419e-06
Iter: 1068 loss: 1.4234228101448436e-06
Iter: 1069 loss: 1.4234016504204329e-06
Iter: 1070 loss: 1.4223803671060966e-06
Iter: 1071 loss: 1.4367264954488519e-06
Iter: 1072 loss: 1.422377657473105e-06
Iter: 1073 loss: 1.4215593323457241e-06
Iter: 1074 loss: 1.4235146355467668e-06
Iter: 1075 loss: 1.4212627867826467e-06
Iter: 1076 loss: 1.420703285900278e-06
Iter: 1077 loss: 1.4206977284234462e-06
Iter: 1078 loss: 1.4202962226381091e-06
Iter: 1079 loss: 1.4196477146768971e-06
Iter: 1080 loss: 1.4196432069607549e-06
Iter: 1081 loss: 1.4189334252690167e-06
Iter: 1082 loss: 1.4209222187102822e-06
Iter: 1083 loss: 1.4187062361036822e-06
Iter: 1084 loss: 1.4178926884457333e-06
Iter: 1085 loss: 1.4255978799496984e-06
Iter: 1086 loss: 1.4178605288238096e-06
Iter: 1087 loss: 1.4172820393268238e-06
Iter: 1088 loss: 1.4174320502637466e-06
Iter: 1089 loss: 1.4168613649623033e-06
Iter: 1090 loss: 1.4162085608103348e-06
Iter: 1091 loss: 1.4197181691464269e-06
Iter: 1092 loss: 1.4161103057927168e-06
Iter: 1093 loss: 1.4155224232590396e-06
Iter: 1094 loss: 1.4177313483415671e-06
Iter: 1095 loss: 1.4153805021590705e-06
Iter: 1096 loss: 1.4149737681891951e-06
Iter: 1097 loss: 1.419548998320927e-06
Iter: 1098 loss: 1.4149662334301092e-06
Iter: 1099 loss: 1.4146289536921418e-06
Iter: 1100 loss: 1.4137639575539506e-06
Iter: 1101 loss: 1.4210556004269471e-06
Iter: 1102 loss: 1.4136166349726292e-06
Iter: 1103 loss: 1.4128003875768308e-06
Iter: 1104 loss: 1.4159904977018005e-06
Iter: 1105 loss: 1.4126117310568432e-06
Iter: 1106 loss: 1.4116201462029332e-06
Iter: 1107 loss: 1.4113289737626607e-06
Iter: 1108 loss: 1.4107323247660077e-06
Iter: 1109 loss: 1.4098590913117932e-06
Iter: 1110 loss: 1.4140846289552764e-06
Iter: 1111 loss: 1.4097052409364366e-06
Iter: 1112 loss: 1.4089999927190416e-06
Iter: 1113 loss: 1.4089958181110759e-06
Iter: 1114 loss: 1.4085675843227273e-06
Iter: 1115 loss: 1.4081641355098229e-06
Iter: 1116 loss: 1.4080651278829937e-06
Iter: 1117 loss: 1.4072742924481666e-06
Iter: 1118 loss: 1.4089609143079855e-06
Iter: 1119 loss: 1.4069642749572553e-06
Iter: 1120 loss: 1.4065463577240725e-06
Iter: 1121 loss: 1.406526786561679e-06
Iter: 1122 loss: 1.4061180689869633e-06
Iter: 1123 loss: 1.4053720352315305e-06
Iter: 1124 loss: 1.4229689010801288e-06
Iter: 1125 loss: 1.4053716045478004e-06
Iter: 1126 loss: 1.4048389399419282e-06
Iter: 1127 loss: 1.4048383786951483e-06
Iter: 1128 loss: 1.4043407087428391e-06
Iter: 1129 loss: 1.4048077170900462e-06
Iter: 1130 loss: 1.404054865742037e-06
Iter: 1131 loss: 1.4033915115116725e-06
Iter: 1132 loss: 1.40694771276506e-06
Iter: 1133 loss: 1.4032913032556721e-06
Iter: 1134 loss: 1.4028282579916455e-06
Iter: 1135 loss: 1.4025035747723047e-06
Iter: 1136 loss: 1.4023388468005984e-06
Iter: 1137 loss: 1.4016641540750374e-06
Iter: 1138 loss: 1.4010916012128132e-06
Iter: 1139 loss: 1.4009044328074123e-06
Iter: 1140 loss: 1.4000068434083432e-06
Iter: 1141 loss: 1.4040615002032902e-06
Iter: 1142 loss: 1.3998331854986357e-06
Iter: 1143 loss: 1.3990134306537202e-06
Iter: 1144 loss: 1.4038675021990556e-06
Iter: 1145 loss: 1.3989085282334333e-06
Iter: 1146 loss: 1.3982917506599924e-06
Iter: 1147 loss: 1.4002323910316833e-06
Iter: 1148 loss: 1.3981135021800685e-06
Iter: 1149 loss: 1.3973951868919063e-06
Iter: 1150 loss: 1.4018560699577464e-06
Iter: 1151 loss: 1.3973108970355044e-06
Iter: 1152 loss: 1.3968857256987716e-06
Iter: 1153 loss: 1.396080156774145e-06
Iter: 1154 loss: 1.4135404929639364e-06
Iter: 1155 loss: 1.3960773642676651e-06
Iter: 1156 loss: 1.3954572725482006e-06
Iter: 1157 loss: 1.3954247616013957e-06
Iter: 1158 loss: 1.39484921205006e-06
Iter: 1159 loss: 1.3954282077611778e-06
Iter: 1160 loss: 1.3945261180648423e-06
Iter: 1161 loss: 1.3939626618082123e-06
Iter: 1162 loss: 1.3945877040842163e-06
Iter: 1163 loss: 1.3936569629789622e-06
Iter: 1164 loss: 1.3930792846272566e-06
Iter: 1165 loss: 1.4016712486981885e-06
Iter: 1166 loss: 1.3930788055657811e-06
Iter: 1167 loss: 1.3926509352984675e-06
Iter: 1168 loss: 1.3926921551177821e-06
Iter: 1169 loss: 1.3923206608958584e-06
Iter: 1170 loss: 1.3917309994720117e-06
Iter: 1171 loss: 1.3933180911746679e-06
Iter: 1172 loss: 1.3915359195400946e-06
Iter: 1173 loss: 1.3911747073385457e-06
Iter: 1174 loss: 1.3903764740945507e-06
Iter: 1175 loss: 1.4018974702093319e-06
Iter: 1176 loss: 1.3903383109052431e-06
Iter: 1177 loss: 1.3894363464721073e-06
Iter: 1178 loss: 1.4002558247866958e-06
Iter: 1179 loss: 1.3894250222871913e-06
Iter: 1180 loss: 1.3888822523986679e-06
Iter: 1181 loss: 1.387931836249168e-06
Iter: 1182 loss: 1.3879316626868389e-06
Iter: 1183 loss: 1.3873784905720693e-06
Iter: 1184 loss: 1.3872754657112035e-06
Iter: 1185 loss: 1.3867552297574962e-06
Iter: 1186 loss: 1.3888313482042763e-06
Iter: 1187 loss: 1.3866378529290548e-06
Iter: 1188 loss: 1.3861142974144558e-06
Iter: 1189 loss: 1.385969321531777e-06
Iter: 1190 loss: 1.3856485992262375e-06
Iter: 1191 loss: 1.3849131944525041e-06
Iter: 1192 loss: 1.3859608998355841e-06
Iter: 1193 loss: 1.3845531879739688e-06
Iter: 1194 loss: 1.3838738135050182e-06
Iter: 1195 loss: 1.3838647796891022e-06
Iter: 1196 loss: 1.3836038095941029e-06
Iter: 1197 loss: 1.383160430528657e-06
Iter: 1198 loss: 1.3831597184430988e-06
Iter: 1199 loss: 1.3825605597170048e-06
Iter: 1200 loss: 1.390239776055455e-06
Iter: 1201 loss: 1.3825560272017871e-06
Iter: 1202 loss: 1.3822195755811193e-06
Iter: 1203 loss: 1.3824207404549826e-06
Iter: 1204 loss: 1.382002604337812e-06
Iter: 1205 loss: 1.3815698978809486e-06
Iter: 1206 loss: 1.381780888996811e-06
Iter: 1207 loss: 1.3812798598357705e-06
Iter: 1208 loss: 1.3806267136137272e-06
Iter: 1209 loss: 1.3804407714055252e-06
Iter: 1210 loss: 1.3800439460978182e-06
Iter: 1211 loss: 1.3792594803419559e-06
Iter: 1212 loss: 1.3791344840050587e-06
Iter: 1213 loss: 1.378592854048785e-06
Iter: 1214 loss: 1.3776647555319673e-06
Iter: 1215 loss: 1.3881538677137059e-06
Iter: 1216 loss: 1.3776479911884808e-06
Iter: 1217 loss: 1.3769867797765731e-06
Iter: 1218 loss: 1.3782676149179055e-06
Iter: 1219 loss: 1.3767114704919953e-06
Iter: 1220 loss: 1.3762557521207952e-06
Iter: 1221 loss: 1.376253867691139e-06
Iter: 1222 loss: 1.3757909496205047e-06
Iter: 1223 loss: 1.3750971389902029e-06
Iter: 1224 loss: 1.3750825452257665e-06
Iter: 1225 loss: 1.3743700696828639e-06
Iter: 1226 loss: 1.3790186041023389e-06
Iter: 1227 loss: 1.3742941156716857e-06
Iter: 1228 loss: 1.3737362277201189e-06
Iter: 1229 loss: 1.3787597657795777e-06
Iter: 1230 loss: 1.3737099922767738e-06
Iter: 1231 loss: 1.3732293279886484e-06
Iter: 1232 loss: 1.3729901117320288e-06
Iter: 1233 loss: 1.372762374008729e-06
Iter: 1234 loss: 1.3722976906578751e-06
Iter: 1235 loss: 1.3722942396616493e-06
Iter: 1236 loss: 1.3718492473668196e-06
Iter: 1237 loss: 1.3712474988299583e-06
Iter: 1238 loss: 1.3712166774144891e-06
Iter: 1239 loss: 1.3708104234649612e-06
Iter: 1240 loss: 1.3743544405083113e-06
Iter: 1241 loss: 1.3707892965277619e-06
Iter: 1242 loss: 1.3703330896667398e-06
Iter: 1243 loss: 1.3693621570647593e-06
Iter: 1244 loss: 1.385001258975106e-06
Iter: 1245 loss: 1.3693318411678605e-06
Iter: 1246 loss: 1.368431678788524e-06
Iter: 1247 loss: 1.3757319358002146e-06
Iter: 1248 loss: 1.3683738101629441e-06
Iter: 1249 loss: 1.3677057836329102e-06
Iter: 1250 loss: 1.3678004609334145e-06
Iter: 1251 loss: 1.3671987533035855e-06
Iter: 1252 loss: 1.3663310386219306e-06
Iter: 1253 loss: 1.3679579686757562e-06
Iter: 1254 loss: 1.3659626511514443e-06
Iter: 1255 loss: 1.3657225350749336e-06
Iter: 1256 loss: 1.3655541510008285e-06
Iter: 1257 loss: 1.3652266567565408e-06
Iter: 1258 loss: 1.3651900013327269e-06
Iter: 1259 loss: 1.3649533459702645e-06
Iter: 1260 loss: 1.3643360892955342e-06
Iter: 1261 loss: 1.3638298896557374e-06
Iter: 1262 loss: 1.3636495913438003e-06
Iter: 1263 loss: 1.3632372174561236e-06
Iter: 1264 loss: 1.3631696666768886e-06
Iter: 1265 loss: 1.3627124354117258e-06
Iter: 1266 loss: 1.3621721289021546e-06
Iter: 1267 loss: 1.3621137833736571e-06
Iter: 1268 loss: 1.3619889403546135e-06
Iter: 1269 loss: 1.3618177293729101e-06
Iter: 1270 loss: 1.361613371238665e-06
Iter: 1271 loss: 1.3610868320618798e-06
Iter: 1272 loss: 1.3654347537995564e-06
Iter: 1273 loss: 1.3609940781836313e-06
Iter: 1274 loss: 1.3603355305886327e-06
Iter: 1275 loss: 1.3637657392699669e-06
Iter: 1276 loss: 1.3602314329002311e-06
Iter: 1277 loss: 1.359689727536719e-06
Iter: 1278 loss: 1.3623076133544634e-06
Iter: 1279 loss: 1.3595941276340783e-06
Iter: 1280 loss: 1.3591057284606367e-06
Iter: 1281 loss: 1.3584586698929899e-06
Iter: 1282 loss: 1.358420748080886e-06
Iter: 1283 loss: 1.3577656394368044e-06
Iter: 1284 loss: 1.3599807610350224e-06
Iter: 1285 loss: 1.3575889874425886e-06
Iter: 1286 loss: 1.3567608404528138e-06
Iter: 1287 loss: 1.358331961719585e-06
Iter: 1288 loss: 1.3564116894022686e-06
Iter: 1289 loss: 1.3557345156698167e-06
Iter: 1290 loss: 1.3575938107528102e-06
Iter: 1291 loss: 1.3555140752519492e-06
Iter: 1292 loss: 1.3550074854121134e-06
Iter: 1293 loss: 1.3549977445441979e-06
Iter: 1294 loss: 1.3545829664835145e-06
Iter: 1295 loss: 1.3539864166194978e-06
Iter: 1296 loss: 1.3539675164414511e-06
Iter: 1297 loss: 1.3534435118065067e-06
Iter: 1298 loss: 1.3611408728238937e-06
Iter: 1299 loss: 1.3534429099674553e-06
Iter: 1300 loss: 1.3529714868426685e-06
Iter: 1301 loss: 1.3541320437237975e-06
Iter: 1302 loss: 1.3528043205208054e-06
Iter: 1303 loss: 1.3523664502377107e-06
Iter: 1304 loss: 1.3539038314779503e-06
Iter: 1305 loss: 1.3522528249759015e-06
Iter: 1306 loss: 1.3517327051507245e-06
Iter: 1307 loss: 1.3527052905032309e-06
Iter: 1308 loss: 1.351511537157248e-06
Iter: 1309 loss: 1.3511972489196529e-06
Iter: 1310 loss: 1.3505754399662662e-06
Iter: 1311 loss: 1.3627149780549847e-06
Iter: 1312 loss: 1.3505691002291402e-06
Iter: 1313 loss: 1.3500851687284104e-06
Iter: 1314 loss: 1.3500651992364889e-06
Iter: 1315 loss: 1.3496469215769538e-06
Iter: 1316 loss: 1.3490488838367039e-06
Iter: 1317 loss: 1.3490289307463085e-06
Iter: 1318 loss: 1.3483422279356917e-06
Iter: 1319 loss: 1.3514036024491981e-06
Iter: 1320 loss: 1.3482069892364125e-06
Iter: 1321 loss: 1.3474688118545096e-06
Iter: 1322 loss: 1.347068909354057e-06
Iter: 1323 loss: 1.3467387703075708e-06
Iter: 1324 loss: 1.3459425314287532e-06
Iter: 1325 loss: 1.3489253704045339e-06
Iter: 1326 loss: 1.3457495788624171e-06
Iter: 1327 loss: 1.344970197467774e-06
Iter: 1328 loss: 1.3537857278872683e-06
Iter: 1329 loss: 1.3449562138534381e-06
Iter: 1330 loss: 1.3445006177461204e-06
Iter: 1331 loss: 1.348759992767522e-06
Iter: 1332 loss: 1.3444817455345961e-06
Iter: 1333 loss: 1.3441903068673257e-06
Iter: 1334 loss: 1.3436027809253776e-06
Iter: 1335 loss: 1.354546021541799e-06
Iter: 1336 loss: 1.3435943480218574e-06
Iter: 1337 loss: 1.3430428708733536e-06
Iter: 1338 loss: 1.3430212118061289e-06
Iter: 1339 loss: 1.3427079869480798e-06
Iter: 1340 loss: 1.3428382463981933e-06
Iter: 1341 loss: 1.3424926301049853e-06
Iter: 1342 loss: 1.3419204051539059e-06
Iter: 1343 loss: 1.342373717119177e-06
Iter: 1344 loss: 1.3415752563494629e-06
Iter: 1345 loss: 1.3411427158197185e-06
Iter: 1346 loss: 1.3411938499271976e-06
Iter: 1347 loss: 1.3408115784956255e-06
Iter: 1348 loss: 1.3401751438794028e-06
Iter: 1349 loss: 1.3409585216538062e-06
Iter: 1350 loss: 1.3398433818517045e-06
Iter: 1351 loss: 1.3391803380378055e-06
Iter: 1352 loss: 1.3475136278354508e-06
Iter: 1353 loss: 1.3391744427471748e-06
Iter: 1354 loss: 1.33876632890742e-06
Iter: 1355 loss: 1.3380577868053627e-06
Iter: 1356 loss: 1.3380574709413052e-06
Iter: 1357 loss: 1.3374609486439573e-06
Iter: 1358 loss: 1.3422666500855215e-06
Iter: 1359 loss: 1.3374219049229575e-06
Iter: 1360 loss: 1.3368359626862181e-06
Iter: 1361 loss: 1.3365805985233931e-06
Iter: 1362 loss: 1.3362808757759597e-06
Iter: 1363 loss: 1.3355200519841644e-06
Iter: 1364 loss: 1.3400404600203865e-06
Iter: 1365 loss: 1.3354232182936393e-06
Iter: 1366 loss: 1.3347766116072278e-06
Iter: 1367 loss: 1.3430302327496674e-06
Iter: 1368 loss: 1.3347715637910953e-06
Iter: 1369 loss: 1.3343561532273356e-06
Iter: 1370 loss: 1.334095133524272e-06
Iter: 1371 loss: 1.3339303396549073e-06
Iter: 1372 loss: 1.3333553191878817e-06
Iter: 1373 loss: 1.3409945456939571e-06
Iter: 1374 loss: 1.3333522438961646e-06
Iter: 1375 loss: 1.3329021398919776e-06
Iter: 1376 loss: 1.3335980407306531e-06
Iter: 1377 loss: 1.3326902400147266e-06
Iter: 1378 loss: 1.3322958089257053e-06
Iter: 1379 loss: 1.3345064101298563e-06
Iter: 1380 loss: 1.3322403205164333e-06
Iter: 1381 loss: 1.3318685627695233e-06
Iter: 1382 loss: 1.3311490775927773e-06
Iter: 1383 loss: 1.3459673198325528e-06
Iter: 1384 loss: 1.3311445468057738e-06
Iter: 1385 loss: 1.3304711248943529e-06
Iter: 1386 loss: 1.3315177298103523e-06
Iter: 1387 loss: 1.3301548248180983e-06
Iter: 1388 loss: 1.329791105988231e-06
Iter: 1389 loss: 1.3297306670698062e-06
Iter: 1390 loss: 1.3293928208308509e-06
Iter: 1391 loss: 1.3286969708024455e-06
Iter: 1392 loss: 1.3409502947911246e-06
Iter: 1393 loss: 1.3286829620351823e-06
Iter: 1394 loss: 1.3280410994630728e-06
Iter: 1395 loss: 1.33404885163032e-06
Iter: 1396 loss: 1.328014630742832e-06
Iter: 1397 loss: 1.3275196199908383e-06
Iter: 1398 loss: 1.3265453170203526e-06
Iter: 1399 loss: 1.3458219917124387e-06
Iter: 1400 loss: 1.3265363765580498e-06
Iter: 1401 loss: 1.3259601804208768e-06
Iter: 1402 loss: 1.3258947916357013e-06
Iter: 1403 loss: 1.3254031883301874e-06
Iter: 1404 loss: 1.3280113077447972e-06
Iter: 1405 loss: 1.3253276587197502e-06
Iter: 1406 loss: 1.3248525629738955e-06
Iter: 1407 loss: 1.3261142266744574e-06
Iter: 1408 loss: 1.3246936553043078e-06
Iter: 1409 loss: 1.3243623673859179e-06
Iter: 1410 loss: 1.3269694254218263e-06
Iter: 1411 loss: 1.3243393144065761e-06
Iter: 1412 loss: 1.3239435665109515e-06
Iter: 1413 loss: 1.3234402459514904e-06
Iter: 1414 loss: 1.3234026338753209e-06
Iter: 1415 loss: 1.3229330144890459e-06
Iter: 1416 loss: 1.3229329354145919e-06
Iter: 1417 loss: 1.322610331250029e-06
Iter: 1418 loss: 1.3218732952867852e-06
Iter: 1419 loss: 1.3314736188065322e-06
Iter: 1420 loss: 1.3218243901470453e-06
Iter: 1421 loss: 1.3211249582731188e-06
Iter: 1422 loss: 1.3262278479343786e-06
Iter: 1423 loss: 1.321066334434457e-06
Iter: 1424 loss: 1.3205422117174858e-06
Iter: 1425 loss: 1.3217081970001647e-06
Iter: 1426 loss: 1.3203424816106334e-06
Iter: 1427 loss: 1.3196583682061848e-06
Iter: 1428 loss: 1.3228684990923819e-06
Iter: 1429 loss: 1.319532568483817e-06
Iter: 1430 loss: 1.319142834997928e-06
Iter: 1431 loss: 1.3188350616648688e-06
Iter: 1432 loss: 1.3187150595700528e-06
Iter: 1433 loss: 1.3181420720015463e-06
Iter: 1434 loss: 1.3213010422090313e-06
Iter: 1435 loss: 1.3180592668996159e-06
Iter: 1436 loss: 1.3174942507193736e-06
Iter: 1437 loss: 1.3172611931440259e-06
Iter: 1438 loss: 1.31696396983495e-06
Iter: 1439 loss: 1.3164144341654957e-06
Iter: 1440 loss: 1.3164144202978689e-06
Iter: 1441 loss: 1.3158308483359818e-06
Iter: 1442 loss: 1.3172513221004775e-06
Iter: 1443 loss: 1.3156222184479227e-06
Iter: 1444 loss: 1.3152434087257152e-06
Iter: 1445 loss: 1.3197737882244631e-06
Iter: 1446 loss: 1.3152385580777341e-06
Iter: 1447 loss: 1.3149213865763879e-06
Iter: 1448 loss: 1.3150385005472996e-06
Iter: 1449 loss: 1.3146997058319276e-06
Iter: 1450 loss: 1.3143719069486934e-06
Iter: 1451 loss: 1.315347679146363e-06
Iter: 1452 loss: 1.3142722914160779e-06
Iter: 1453 loss: 1.3138548668908358e-06
Iter: 1454 loss: 1.3137123601522035e-06
Iter: 1455 loss: 1.3134740237054212e-06
Iter: 1456 loss: 1.3128430257382452e-06
Iter: 1457 loss: 1.3124414477496255e-06
Iter: 1458 loss: 1.3121940056810408e-06
Iter: 1459 loss: 1.3115586533934586e-06
Iter: 1460 loss: 1.3144679573840422e-06
Iter: 1461 loss: 1.3114379205206686e-06
Iter: 1462 loss: 1.3105883937667079e-06
Iter: 1463 loss: 1.3137766526348887e-06
Iter: 1464 loss: 1.3103829681276443e-06
Iter: 1465 loss: 1.3099383853967068e-06
Iter: 1466 loss: 1.3110470609674034e-06
Iter: 1467 loss: 1.3097822496085483e-06
Iter: 1468 loss: 1.3092834932428414e-06
Iter: 1469 loss: 1.3088830571044963e-06
Iter: 1470 loss: 1.3087329491249969e-06
Iter: 1471 loss: 1.3080075085543637e-06
Iter: 1472 loss: 1.3106780146957947e-06
Iter: 1473 loss: 1.3078283230916281e-06
Iter: 1474 loss: 1.3072204995077394e-06
Iter: 1475 loss: 1.3135924162303117e-06
Iter: 1476 loss: 1.3072045436101651e-06
Iter: 1477 loss: 1.3068076760465951e-06
Iter: 1478 loss: 1.3114139878854536e-06
Iter: 1479 loss: 1.3068015392762087e-06
Iter: 1480 loss: 1.306496140321148e-06
Iter: 1481 loss: 1.3066673565364625e-06
Iter: 1482 loss: 1.3062966292376722e-06
Iter: 1483 loss: 1.3058157231971916e-06
Iter: 1484 loss: 1.3071268113141996e-06
Iter: 1485 loss: 1.3056582146543388e-06
Iter: 1486 loss: 1.3053731936180849e-06
Iter: 1487 loss: 1.3055692779099053e-06
Iter: 1488 loss: 1.3051950145094928e-06
Iter: 1489 loss: 1.3046356393192433e-06
Iter: 1490 loss: 1.3042882395204441e-06
Iter: 1491 loss: 1.3040639771928593e-06
Iter: 1492 loss: 1.3035307804245208e-06
Iter: 1493 loss: 1.3045315290475311e-06
Iter: 1494 loss: 1.3033045709169336e-06
Iter: 1495 loss: 1.3026557004919869e-06
Iter: 1496 loss: 1.3039845980377446e-06
Iter: 1497 loss: 1.302394642256891e-06
Iter: 1498 loss: 1.3019641059612067e-06
Iter: 1499 loss: 1.3019639649298733e-06
Iter: 1500 loss: 1.3015564037007854e-06
Iter: 1501 loss: 1.3010685788325759e-06
Iter: 1502 loss: 1.3010189085084664e-06
Iter: 1503 loss: 1.3004203806621762e-06
Iter: 1504 loss: 1.3015493119687005e-06
Iter: 1505 loss: 1.3001671802999549e-06
Iter: 1506 loss: 1.299466768976843e-06
Iter: 1507 loss: 1.3016871662891076e-06
Iter: 1508 loss: 1.2992658244131144e-06
Iter: 1509 loss: 1.2986844965114114e-06
Iter: 1510 loss: 1.2997613910064684e-06
Iter: 1511 loss: 1.2984359379377203e-06
Iter: 1512 loss: 1.2980631319598625e-06
Iter: 1513 loss: 1.2979970595203452e-06
Iter: 1514 loss: 1.2977435172583309e-06
Iter: 1515 loss: 1.2978800769149551e-06
Iter: 1516 loss: 1.2975765770298513e-06
Iter: 1517 loss: 1.2971673380102105e-06
Iter: 1518 loss: 1.2975760645194208e-06
Iter: 1519 loss: 1.2969370709609318e-06
Iter: 1520 loss: 1.296589922598827e-06
Iter: 1521 loss: 1.2971965328994244e-06
Iter: 1522 loss: 1.2964365418325566e-06
Iter: 1523 loss: 1.2959381530623704e-06
Iter: 1524 loss: 1.2966459816258068e-06
Iter: 1525 loss: 1.2956938156799183e-06
Iter: 1526 loss: 1.2952014025823871e-06
Iter: 1527 loss: 1.2962265776602267e-06
Iter: 1528 loss: 1.2950053862505851e-06
Iter: 1529 loss: 1.29456039972864e-06
Iter: 1530 loss: 1.29369409511564e-06
Iter: 1531 loss: 1.3112941920250291e-06
Iter: 1532 loss: 1.2936878283064393e-06
Iter: 1533 loss: 1.2936007777229951e-06
Iter: 1534 loss: 1.2932669391244385e-06
Iter: 1535 loss: 1.2928862547415926e-06
Iter: 1536 loss: 1.2925347465165602e-06
Iter: 1537 loss: 1.2924432934099537e-06
Iter: 1538 loss: 1.2918227595256483e-06
Iter: 1539 loss: 1.2930623194024361e-06
Iter: 1540 loss: 1.2915692004700636e-06
Iter: 1541 loss: 1.2909581291309128e-06
Iter: 1542 loss: 1.2916896188000498e-06
Iter: 1543 loss: 1.2906360585925789e-06
Iter: 1544 loss: 1.2901456987773125e-06
Iter: 1545 loss: 1.2934333663058381e-06
Iter: 1546 loss: 1.2900962397961618e-06
Iter: 1547 loss: 1.2896409467694545e-06
Iter: 1548 loss: 1.2951860234123425e-06
Iter: 1549 loss: 1.2896358056644779e-06
Iter: 1550 loss: 1.289342253703803e-06
Iter: 1551 loss: 1.2898719699192785e-06
Iter: 1552 loss: 1.2892148723607272e-06
Iter: 1553 loss: 1.2889463692473404e-06
Iter: 1554 loss: 1.2899953499399104e-06
Iter: 1555 loss: 1.2888842832209887e-06
Iter: 1556 loss: 1.2886240385395337e-06
Iter: 1557 loss: 1.2881251248774e-06
Iter: 1558 loss: 1.2986379651749209e-06
Iter: 1559 loss: 1.2881226682898645e-06
Iter: 1560 loss: 1.287619978591013e-06
Iter: 1561 loss: 1.2876197380606702e-06
Iter: 1562 loss: 1.2872830936818953e-06
Iter: 1563 loss: 1.2866138909092527e-06
Iter: 1564 loss: 1.2995282676708556e-06
Iter: 1565 loss: 1.2866064176522653e-06
Iter: 1566 loss: 1.2860812525550683e-06
Iter: 1567 loss: 1.2901833785253311e-06
Iter: 1568 loss: 1.2860439907253018e-06
Iter: 1569 loss: 1.2855512394548748e-06
Iter: 1570 loss: 1.2861275011956083e-06
Iter: 1571 loss: 1.285289170362653e-06
Iter: 1572 loss: 1.2848945100954947e-06
Iter: 1573 loss: 1.2848880579302386e-06
Iter: 1574 loss: 1.2846458776418428e-06
Iter: 1575 loss: 1.2841666888186963e-06
Iter: 1576 loss: 1.2935173162904066e-06
Iter: 1577 loss: 1.2841617923133469e-06
Iter: 1578 loss: 1.2835839741111936e-06
Iter: 1579 loss: 1.2854722830565233e-06
Iter: 1580 loss: 1.2834228559749407e-06
Iter: 1581 loss: 1.2827807947999572e-06
Iter: 1582 loss: 1.28415233736805e-06
Iter: 1583 loss: 1.2825293985847586e-06
Iter: 1584 loss: 1.2822513465076471e-06
Iter: 1585 loss: 1.2821931591342543e-06
Iter: 1586 loss: 1.2818295374548843e-06
Iter: 1587 loss: 1.2814640972664296e-06
Iter: 1588 loss: 1.2813906679511617e-06
Iter: 1589 loss: 1.2810982963218154e-06
Iter: 1590 loss: 1.2854839441261325e-06
Iter: 1591 loss: 1.281098104488928e-06
Iter: 1592 loss: 1.2808346134456968e-06
Iter: 1593 loss: 1.2802629776730605e-06
Iter: 1594 loss: 1.2889813209131069e-06
Iter: 1595 loss: 1.2802406191447034e-06
Iter: 1596 loss: 1.2798716067463084e-06
Iter: 1597 loss: 1.279855630780438e-06
Iter: 1598 loss: 1.2795747610191483e-06
Iter: 1599 loss: 1.2795985774219967e-06
Iter: 1600 loss: 1.2793570558662048e-06
Iter: 1601 loss: 1.2789385220783153e-06
Iter: 1602 loss: 1.2785363836916397e-06
Iter: 1603 loss: 1.2784433266384165e-06
Iter: 1604 loss: 1.2777627825982329e-06
Iter: 1605 loss: 1.278990162332635e-06
Iter: 1606 loss: 1.2774673589408412e-06
Iter: 1607 loss: 1.277217697115689e-06
Iter: 1608 loss: 1.2771249580019941e-06
Iter: 1609 loss: 1.276802920463985e-06
Iter: 1610 loss: 1.2761634115154497e-06
Iter: 1611 loss: 1.2885335620468314e-06
Iter: 1612 loss: 1.2761564051342728e-06
Iter: 1613 loss: 1.275586226132283e-06
Iter: 1614 loss: 1.2810085883951061e-06
Iter: 1615 loss: 1.2755640107665417e-06
Iter: 1616 loss: 1.2751357719897876e-06
Iter: 1617 loss: 1.2745165139577441e-06
Iter: 1618 loss: 1.2744978266364137e-06
Iter: 1619 loss: 1.2749797115076182e-06
Iter: 1620 loss: 1.2742214583809596e-06
Iter: 1621 loss: 1.2740032844692279e-06
Iter: 1622 loss: 1.273682827887313e-06
Iter: 1623 loss: 1.2736745006010006e-06
Iter: 1624 loss: 1.2732666913929582e-06
Iter: 1625 loss: 1.2744532939582483e-06
Iter: 1626 loss: 1.2731402559807885e-06
Iter: 1627 loss: 1.2726904332426436e-06
Iter: 1628 loss: 1.2743152858526681e-06
Iter: 1629 loss: 1.2725770710450006e-06
Iter: 1630 loss: 1.2722425663839557e-06
Iter: 1631 loss: 1.2718126276942494e-06
Iter: 1632 loss: 1.2717823710278343e-06
Iter: 1633 loss: 1.2711328790403814e-06
Iter: 1634 loss: 1.2785540696166943e-06
Iter: 1635 loss: 1.2711218743467824e-06
Iter: 1636 loss: 1.270791943994537e-06
Iter: 1637 loss: 1.2702905217104664e-06
Iter: 1638 loss: 1.270281555457776e-06
Iter: 1639 loss: 1.269772189748531e-06
Iter: 1640 loss: 1.2724860074851248e-06
Iter: 1641 loss: 1.2696944466102435e-06
Iter: 1642 loss: 1.2691837588744383e-06
Iter: 1643 loss: 1.2696265931715961e-06
Iter: 1644 loss: 1.2688833981822194e-06
Iter: 1645 loss: 1.2685304774791561e-06
Iter: 1646 loss: 1.2685155196666491e-06
Iter: 1647 loss: 1.2681906184865288e-06
Iter: 1648 loss: 1.2678097825561992e-06
Iter: 1649 loss: 1.2677671133932215e-06
Iter: 1650 loss: 1.2672218856598577e-06
Iter: 1651 loss: 1.2679142735786389e-06
Iter: 1652 loss: 1.2669412971244142e-06
Iter: 1653 loss: 1.2663721719268381e-06
Iter: 1654 loss: 1.2711382546211437e-06
Iter: 1655 loss: 1.2663387584343719e-06
Iter: 1656 loss: 1.2661306047915134e-06
Iter: 1657 loss: 1.2660842669467028e-06
Iter: 1658 loss: 1.2659335041096721e-06
Iter: 1659 loss: 1.2654748003798067e-06
Iter: 1660 loss: 1.2666454496391089e-06
Iter: 1661 loss: 1.2652197559074028e-06
Iter: 1662 loss: 1.2651326877936966e-06
Iter: 1663 loss: 1.2649374626459169e-06
Iter: 1664 loss: 1.2646999118262698e-06
Iter: 1665 loss: 1.2642118802440963e-06
Iter: 1666 loss: 1.2728645854719078e-06
Iter: 1667 loss: 1.26420241947649e-06
Iter: 1668 loss: 1.2638529627691867e-06
Iter: 1669 loss: 1.2688081229906212e-06
Iter: 1670 loss: 1.2638521669743903e-06
Iter: 1671 loss: 1.2634761446830077e-06
Iter: 1672 loss: 1.263145041153565e-06
Iter: 1673 loss: 1.2630467881836269e-06
Iter: 1674 loss: 1.26255382974972e-06
Iter: 1675 loss: 1.264735753052336e-06
Iter: 1676 loss: 1.2624558709132853e-06
Iter: 1677 loss: 1.2620183658423641e-06
Iter: 1678 loss: 1.2612956877382545e-06
Iter: 1679 loss: 1.2612926978317637e-06
Iter: 1680 loss: 1.2609184569713534e-06
Iter: 1681 loss: 1.2608553154338743e-06
Iter: 1682 loss: 1.2604586258746727e-06
Iter: 1683 loss: 1.2612747591574127e-06
Iter: 1684 loss: 1.2602994873179448e-06
Iter: 1685 loss: 1.2598399987552762e-06
Iter: 1686 loss: 1.2607067348540264e-06
Iter: 1687 loss: 1.2596456386612379e-06
Iter: 1688 loss: 1.2592594261924467e-06
Iter: 1689 loss: 1.2591109171024809e-06
Iter: 1690 loss: 1.2589009488204272e-06
Iter: 1691 loss: 1.2590245685510901e-06
Iter: 1692 loss: 1.2586893552704091e-06
Iter: 1693 loss: 1.2585133396151783e-06
Iter: 1694 loss: 1.2580876454560273e-06
Iter: 1695 loss: 1.2626396802106348e-06
Iter: 1696 loss: 1.2580413819438269e-06
Iter: 1697 loss: 1.2575248758279414e-06
Iter: 1698 loss: 1.2585600590412666e-06
Iter: 1699 loss: 1.2573142361199248e-06
Iter: 1700 loss: 1.2569768986721954e-06
Iter: 1701 loss: 1.2569680516077085e-06
Iter: 1702 loss: 1.2566764564793126e-06
Iter: 1703 loss: 1.2560136518508442e-06
Iter: 1704 loss: 1.2647824706368213e-06
Iter: 1705 loss: 1.255971709284089e-06
Iter: 1706 loss: 1.2555076101695588e-06
Iter: 1707 loss: 1.2629284049188812e-06
Iter: 1708 loss: 1.255507610142472e-06
Iter: 1709 loss: 1.2550882479558791e-06
Iter: 1710 loss: 1.2561351696185349e-06
Iter: 1711 loss: 1.2549410981477478e-06
Iter: 1712 loss: 1.2546405107707615e-06
Iter: 1713 loss: 1.2545586164822206e-06
Iter: 1714 loss: 1.2543736032249894e-06
Iter: 1715 loss: 1.2538778347940742e-06
Iter: 1716 loss: 1.2545410526427731e-06
Iter: 1717 loss: 1.2536282282800306e-06
Iter: 1718 loss: 1.253066226033404e-06
Iter: 1719 loss: 1.2535476253207038e-06
Iter: 1720 loss: 1.2527345146352903e-06
Iter: 1721 loss: 1.2522936935303372e-06
Iter: 1722 loss: 1.2522622408598398e-06
Iter: 1723 loss: 1.2519410438001748e-06
Iter: 1724 loss: 1.2519766114832471e-06
Iter: 1725 loss: 1.2516944544216812e-06
Iter: 1726 loss: 1.2513453784027615e-06
Iter: 1727 loss: 1.253413530788249e-06
Iter: 1728 loss: 1.2513007346256222e-06
Iter: 1729 loss: 1.2510073307015333e-06
Iter: 1730 loss: 1.2543900573100145e-06
Iter: 1731 loss: 1.2510026120916082e-06
Iter: 1732 loss: 1.2508393977967103e-06
Iter: 1733 loss: 1.2503726584012944e-06
Iter: 1734 loss: 1.2525058258927911e-06
Iter: 1735 loss: 1.2502030330437154e-06
Iter: 1736 loss: 1.2497459687460587e-06
Iter: 1737 loss: 1.2548666403453243e-06
Iter: 1738 loss: 1.2497373236754813e-06
Iter: 1739 loss: 1.2491302129417418e-06
Iter: 1740 loss: 1.2491051968887542e-06
Iter: 1741 loss: 1.248636923362259e-06
Iter: 1742 loss: 1.2482630331829862e-06
Iter: 1743 loss: 1.2493391065380127e-06
Iter: 1744 loss: 1.248146034359869e-06
Iter: 1745 loss: 1.2476856930825675e-06
Iter: 1746 loss: 1.249051329907074e-06
Iter: 1747 loss: 1.24754539111664e-06
Iter: 1748 loss: 1.2470070948693923e-06
Iter: 1749 loss: 1.2479468385604523e-06
Iter: 1750 loss: 1.246769134727172e-06
Iter: 1751 loss: 1.2464186812136132e-06
Iter: 1752 loss: 1.2460685103804304e-06
Iter: 1753 loss: 1.2459968365159426e-06
Iter: 1754 loss: 1.2453990310620208e-06
Iter: 1755 loss: 1.2487805789432293e-06
Iter: 1756 loss: 1.2453162131328309e-06
Iter: 1757 loss: 1.2447724670326958e-06
Iter: 1758 loss: 1.2471336624913187e-06
Iter: 1759 loss: 1.2446617544393161e-06
Iter: 1760 loss: 1.2443050433404482e-06
Iter: 1761 loss: 1.2489871388732769e-06
Iter: 1762 loss: 1.2443028922758833e-06
Iter: 1763 loss: 1.2440101898932876e-06
Iter: 1764 loss: 1.2436550512391866e-06
Iter: 1765 loss: 1.2436211729277323e-06
Iter: 1766 loss: 1.2433032611860433e-06
Iter: 1767 loss: 1.243256520477705e-06
Iter: 1768 loss: 1.2431132032566003e-06
Iter: 1769 loss: 1.2427266931063957e-06
Iter: 1770 loss: 1.2452757299584507e-06
Iter: 1771 loss: 1.2426326150505141e-06
Iter: 1772 loss: 1.2420748990739509e-06
Iter: 1773 loss: 1.2436152307777052e-06
Iter: 1774 loss: 1.24189418789664e-06
Iter: 1775 loss: 1.2415023177457339e-06
Iter: 1776 loss: 1.2414960303299529e-06
Iter: 1777 loss: 1.2411781791892498e-06
Iter: 1778 loss: 1.2406866848189627e-06
Iter: 1779 loss: 1.2406796589939605e-06
Iter: 1780 loss: 1.2402775649448811e-06
Iter: 1781 loss: 1.2420768630376684e-06
Iter: 1782 loss: 1.2401987789864381e-06
Iter: 1783 loss: 1.2398696449384576e-06
Iter: 1784 loss: 1.2439957307066621e-06
Iter: 1785 loss: 1.2398666597117087e-06
Iter: 1786 loss: 1.2396386203112769e-06
Iter: 1787 loss: 1.2392072112643598e-06
Iter: 1788 loss: 1.2485875824815592e-06
Iter: 1789 loss: 1.2392057866778495e-06
Iter: 1790 loss: 1.2385928734109367e-06
Iter: 1791 loss: 1.2401537964437624e-06
Iter: 1792 loss: 1.2383810628391305e-06
Iter: 1793 loss: 1.23789685342309e-06
Iter: 1794 loss: 1.2382900303999415e-06
Iter: 1795 loss: 1.237606690125018e-06
Iter: 1796 loss: 1.2372297330507715e-06
Iter: 1797 loss: 1.2372231919491505e-06
Iter: 1798 loss: 1.2368665168459064e-06
Iter: 1799 loss: 1.2376123753091627e-06
Iter: 1800 loss: 1.2367249008014148e-06
Iter: 1801 loss: 1.2363887621837969e-06
Iter: 1802 loss: 1.2397659656711022e-06
Iter: 1803 loss: 1.2363781692529203e-06
Iter: 1804 loss: 1.2361323514882896e-06
Iter: 1805 loss: 1.2362062707560775e-06
Iter: 1806 loss: 1.2359562514583208e-06
Iter: 1807 loss: 1.2356895014519139e-06
Iter: 1808 loss: 1.2350621832504276e-06
Iter: 1809 loss: 1.2424772948137977e-06
Iter: 1810 loss: 1.2350082061480088e-06
Iter: 1811 loss: 1.2347133394957087e-06
Iter: 1812 loss: 1.234646260690571e-06
Iter: 1813 loss: 1.2342155165627347e-06
Iter: 1814 loss: 1.2339454352767436e-06
Iter: 1815 loss: 1.2337742093380054e-06
Iter: 1816 loss: 1.233333200112558e-06
Iter: 1817 loss: 1.2343166160982324e-06
Iter: 1818 loss: 1.2331654265072036e-06
Iter: 1819 loss: 1.2327418695067157e-06
Iter: 1820 loss: 1.2347342750262637e-06
Iter: 1821 loss: 1.232664246523464e-06
Iter: 1822 loss: 1.2322560368121934e-06
Iter: 1823 loss: 1.2346395681578162e-06
Iter: 1824 loss: 1.2322024896204229e-06
Iter: 1825 loss: 1.2318997025666019e-06
Iter: 1826 loss: 1.2312813249701153e-06
Iter: 1827 loss: 1.2424130538815826e-06
Iter: 1828 loss: 1.2312703650035003e-06
Iter: 1829 loss: 1.2307539279887195e-06
Iter: 1830 loss: 1.2354731697204342e-06
Iter: 1831 loss: 1.2307307831325559e-06
Iter: 1832 loss: 1.2302456045202486e-06
Iter: 1833 loss: 1.2310906261646988e-06
Iter: 1834 loss: 1.2300308754494839e-06
Iter: 1835 loss: 1.2297731541804236e-06
Iter: 1836 loss: 1.2297681343212388e-06
Iter: 1837 loss: 1.229476799407666e-06
Iter: 1838 loss: 1.2297071028746487e-06
Iter: 1839 loss: 1.2293009294009491e-06
Iter: 1840 loss: 1.2289747862239096e-06
Iter: 1841 loss: 1.2305101888526113e-06
Iter: 1842 loss: 1.2289150955229236e-06
Iter: 1843 loss: 1.22867689781054e-06
Iter: 1844 loss: 1.2283256007594221e-06
Iter: 1845 loss: 1.2283168351516925e-06
Iter: 1846 loss: 1.2278707531869675e-06
Iter: 1847 loss: 1.2288636263065084e-06
Iter: 1848 loss: 1.2277008200307601e-06
Iter: 1849 loss: 1.2273009616851966e-06
Iter: 1850 loss: 1.2326554859482422e-06
Iter: 1851 loss: 1.2272990053172005e-06
Iter: 1852 loss: 1.2269163254494186e-06
Iter: 1853 loss: 1.2268733800536329e-06
Iter: 1854 loss: 1.2265969145822979e-06
Iter: 1855 loss: 1.226244275603302e-06
Iter: 1856 loss: 1.22591713059224e-06
Iter: 1857 loss: 1.2258331506074661e-06
Iter: 1858 loss: 1.2254959478984936e-06
Iter: 1859 loss: 1.2254686789146186e-06
Iter: 1860 loss: 1.2251402621322425e-06
Iter: 1861 loss: 1.2250794259106775e-06
Iter: 1862 loss: 1.2248583574287369e-06
Iter: 1863 loss: 1.2244336043308443e-06
Iter: 1864 loss: 1.2253815094553906e-06
Iter: 1865 loss: 1.2242720890068624e-06
Iter: 1866 loss: 1.2238497072708714e-06
Iter: 1867 loss: 1.223254723777353e-06
Iter: 1868 loss: 1.2232322365603149e-06
Iter: 1869 loss: 1.2228793175992809e-06
Iter: 1870 loss: 1.2228421137364563e-06
Iter: 1871 loss: 1.2225201185950024e-06
Iter: 1872 loss: 1.2259504719556222e-06
Iter: 1873 loss: 1.2225122721818794e-06
Iter: 1874 loss: 1.2222524441267316e-06
Iter: 1875 loss: 1.2219812400502911e-06
Iter: 1876 loss: 1.2219332878942553e-06
Iter: 1877 loss: 1.2216252150829611e-06
Iter: 1878 loss: 1.2242354866913018e-06
Iter: 1879 loss: 1.2216077270129732e-06
Iter: 1880 loss: 1.2213372400367867e-06
Iter: 1881 loss: 1.2208161817781556e-06
Iter: 1882 loss: 1.2316723741195735e-06
Iter: 1883 loss: 1.2208132657177952e-06
Iter: 1884 loss: 1.2202950485092132e-06
Iter: 1885 loss: 1.2233373796388187e-06
Iter: 1886 loss: 1.2202277069996222e-06
Iter: 1887 loss: 1.2197220881980742e-06
Iter: 1888 loss: 1.2245937713039502e-06
Iter: 1889 loss: 1.2197033066011686e-06
Iter: 1890 loss: 1.2194911008167302e-06
Iter: 1891 loss: 1.2192832852109512e-06
Iter: 1892 loss: 1.2192379393859897e-06
Iter: 1893 loss: 1.2188143767451693e-06
Iter: 1894 loss: 1.2188566292224503e-06
Iter: 1895 loss: 1.2184878681224537e-06
Iter: 1896 loss: 1.2182786659049038e-06
Iter: 1897 loss: 1.2182192419780586e-06
Iter: 1898 loss: 1.2179646008703818e-06
Iter: 1899 loss: 1.2173446651727669e-06
Iter: 1900 loss: 1.223809339986942e-06
Iter: 1901 loss: 1.2172736067375513e-06
Iter: 1902 loss: 1.2167140872321329e-06
Iter: 1903 loss: 1.2223303042821902e-06
Iter: 1904 loss: 1.2166963583988541e-06
Iter: 1905 loss: 1.2162920939077962e-06
Iter: 1906 loss: 1.2168386584075323e-06
Iter: 1907 loss: 1.2160895289049378e-06
Iter: 1908 loss: 1.2159057598154644e-06
Iter: 1909 loss: 1.2158534382515196e-06
Iter: 1910 loss: 1.215600620981045e-06
Iter: 1911 loss: 1.2152165398544652e-06
Iter: 1912 loss: 1.2152096423934013e-06
Iter: 1913 loss: 1.2149124135437808e-06
Iter: 1914 loss: 1.2159232319954617e-06
Iter: 1915 loss: 1.2148327182680335e-06
Iter: 1916 loss: 1.2144573316053756e-06
Iter: 1917 loss: 1.2150089845553154e-06
Iter: 1918 loss: 1.2142762125469945e-06
Iter: 1919 loss: 1.2138911442498072e-06
Iter: 1920 loss: 1.2145738003439066e-06
Iter: 1921 loss: 1.2137223635605429e-06
Iter: 1922 loss: 1.2134167513384394e-06
Iter: 1923 loss: 1.215700223789298e-06
Iter: 1924 loss: 1.2133925311113675e-06
Iter: 1925 loss: 1.2130321186176483e-06
Iter: 1926 loss: 1.2132053124867243e-06
Iter: 1927 loss: 1.2127899441370156e-06
Iter: 1928 loss: 1.2124646161379772e-06
Iter: 1929 loss: 1.2119554068339878e-06
Iter: 1930 loss: 1.2119492926034977e-06
Iter: 1931 loss: 1.21152840040404e-06
Iter: 1932 loss: 1.2115263035961145e-06
Iter: 1933 loss: 1.2111324601195085e-06
Iter: 1934 loss: 1.2122548053915343e-06
Iter: 1935 loss: 1.2110081725933029e-06
Iter: 1936 loss: 1.2106363592070493e-06
Iter: 1937 loss: 1.2113589417824217e-06
Iter: 1938 loss: 1.2104818507808991e-06
Iter: 1939 loss: 1.2101445667030875e-06
Iter: 1940 loss: 1.2094205216070078e-06
Iter: 1941 loss: 1.2208081625237713e-06
Iter: 1942 loss: 1.2093954557103662e-06
Iter: 1943 loss: 1.2097696063229757e-06
Iter: 1944 loss: 1.2091201011128603e-06
Iter: 1945 loss: 1.2088330896812521e-06
Iter: 1946 loss: 1.2087313260296537e-06
Iter: 1947 loss: 1.2085698485873613e-06
Iter: 1948 loss: 1.2082614467516908e-06
Iter: 1949 loss: 1.2082666750788925e-06
Iter: 1950 loss: 1.2080162901696056e-06
Iter: 1951 loss: 1.2076572598682562e-06
Iter: 1952 loss: 1.2092883725886351e-06
Iter: 1953 loss: 1.2075882999545635e-06
Iter: 1954 loss: 1.207135141090531e-06
Iter: 1955 loss: 1.2077316296318423e-06
Iter: 1956 loss: 1.2069054128509405e-06
Iter: 1957 loss: 1.206614225879256e-06
Iter: 1958 loss: 1.2071541253904862e-06
Iter: 1959 loss: 1.2064898068769339e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.6/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi2
+ date
Sat Nov  7 23:13:06 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi1.6/300_300_300_1 --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8865a41e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb88668a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8866a9d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8865c5ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8865e37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb864071048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8640446a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb864044c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8640b4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8640b4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c7008c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c7146a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c7148c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb863fe06a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb863fe07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb863fe0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c6cd7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c6cdea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c6b0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb863fc79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb863fc78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb863fc7730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c6228c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c592620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c592158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c51b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c4c9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c4f29d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c4f2510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c574620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c467510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c41a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c41ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c3fc6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c3fc7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb83c5e5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.007938400246712586
test_loss: 0.008891865370088951
train_loss: 0.0064389571741845
test_loss: 0.007093681910222284
train_loss: 0.005819770651481042
test_loss: 0.007003247801674474
train_loss: 0.00579583046778246
test_loss: 0.007050308815691206
train_loss: 0.005947924663449424
test_loss: 0.006436856649337612
train_loss: 0.005716546089573709
test_loss: 0.006433805386497212
train_loss: 0.005812817206151
test_loss: 0.006824643791985325
train_loss: 0.0055772015287460786
test_loss: 0.006640467100649272
train_loss: 0.005201736712421833
test_loss: 0.0059550956729843415
train_loss: 0.006001216572673059
test_loss: 0.0064727945767715745
train_loss: 0.0053955168671155325
test_loss: 0.006318751449997324
train_loss: 0.005483856237619022
test_loss: 0.006180991701541652
train_loss: 0.005286331343022872
test_loss: 0.006318665194365468
train_loss: 0.005315917806378951
test_loss: 0.006916849675464796
train_loss: 0.004941969377163773
test_loss: 0.00597955873504571
train_loss: 0.005676050724349719
test_loss: 0.006284471126143869
train_loss: 0.0052674725829333436
test_loss: 0.006633089717258924
train_loss: 0.004755807357254961
test_loss: 0.005755632280228476
train_loss: 0.005597383978396552
test_loss: 0.006207755099176309
train_loss: 0.005596884884714163
test_loss: 0.006167009804797375
train_loss: 0.0049960267223768
test_loss: 0.005789591019283078
train_loss: 0.005309028932281366
test_loss: 0.006020354538313764
train_loss: 0.005725445951462657
test_loss: 0.006270936533391386
train_loss: 0.005095200367430097
test_loss: 0.006181465591907126
train_loss: 0.005035126119589126
test_loss: 0.0059925416947493705
train_loss: 0.004700772508351901
test_loss: 0.005729911338264831
train_loss: 0.004788035507633242
test_loss: 0.005795441203449338
train_loss: 0.005299782041586456
test_loss: 0.006059320449079102
train_loss: 0.004614667424833777
test_loss: 0.005675917612351706
train_loss: 0.00546877763275574
test_loss: 0.006159841501484338
train_loss: 0.004905897802199441
test_loss: 0.005844216777634981
train_loss: 0.0046950788003897554
test_loss: 0.005972276884488668
train_loss: 0.005095890714583849
test_loss: 0.005821884418030714
train_loss: 0.004944417768449151
test_loss: 0.006114799285760919
train_loss: 0.004750354582349451
test_loss: 0.0061871102779892185
train_loss: 0.005296569357242056
test_loss: 0.005876473273505088
train_loss: 0.0053535763636877835
test_loss: 0.006358099860241313
train_loss: 0.005098853738435406
test_loss: 0.006196180443492552
train_loss: 0.004908948553960536
test_loss: 0.005978279896314086
train_loss: 0.004388268468607635
test_loss: 0.005575956883097505
train_loss: 0.004829323314841061
test_loss: 0.005673574664056414
train_loss: 0.004415815294611336
test_loss: 0.005610246334544263
train_loss: 0.005140457754369938
test_loss: 0.005772774114198459
train_loss: 0.0047351394223198095
test_loss: 0.005846941955762363
train_loss: 0.004872319475629689
test_loss: 0.005973808376090448
train_loss: 0.004952498827815393
test_loss: 0.005618055624388934
train_loss: 0.004440476829318448
test_loss: 0.00573897861788737
train_loss: 0.00430116398770531
test_loss: 0.00578741049582571
train_loss: 0.0048090488303971606
test_loss: 0.005551263909787403
train_loss: 0.004727879826772834
test_loss: 0.005735929922035652
train_loss: 0.00471476118839331
test_loss: 0.005664103750432579
train_loss: 0.004987592560593395
test_loss: 0.005675569722425724
train_loss: 0.00479718224219417
test_loss: 0.005742774897858885
train_loss: 0.004905156846261902
test_loss: 0.005830671169568634
train_loss: 0.004543008791560761
test_loss: 0.005471332130033014
train_loss: 0.00504410621844519
test_loss: 0.005846321741372796
train_loss: 0.004864564653538315
test_loss: 0.00583977956436309
train_loss: 0.0054504366628814905
test_loss: 0.005913997461431124
train_loss: 0.004446998205801996
test_loss: 0.0056384132762467435
train_loss: 0.00421776378674531
test_loss: 0.005539848266468509
train_loss: 0.004766545792849459
test_loss: 0.005566457224129882
train_loss: 0.004538478646596043
test_loss: 0.005365887009728988
train_loss: 0.00478948447894478
test_loss: 0.005959118260800813
train_loss: 0.004500104425090693
test_loss: 0.0055042765588099955
train_loss: 0.004499677681181833
test_loss: 0.005809712090830595
train_loss: 0.004563426678964635
test_loss: 0.00560519865852589
train_loss: 0.004616286417505047
test_loss: 0.006085688799555144
train_loss: 0.00436091624579881
test_loss: 0.005513271490748285
train_loss: 0.00446339453296107
test_loss: 0.005811118212725428
train_loss: 0.00484606258114207
test_loss: 0.005760137732135001
train_loss: 0.004431398393158151
test_loss: 0.00564618303568168
train_loss: 0.004371935087928177
test_loss: 0.005513664364489181
train_loss: 0.004473107090085028
test_loss: 0.005757363498062199
train_loss: 0.004444100870232054
test_loss: 0.005638734605100644
train_loss: 0.004575603588876952
test_loss: 0.0056826570801520445
train_loss: 0.004353266775814986
test_loss: 0.005589191790442164
train_loss: 0.004245778398000892
test_loss: 0.0054829805043371226
train_loss: 0.004417234641573909
test_loss: 0.0055677617981845
train_loss: 0.004524597511800523
test_loss: 0.0054374620050161955
train_loss: 0.004601989539522676
test_loss: 0.005601628028924634
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi2/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38155e31e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38155e39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38155b11e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38156909d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38156902f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f381554b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38155107b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38154af510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38154af730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f381546eb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f381542e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f381545c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f381545c6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f381540d730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3815375840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38153ce268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38153c6400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38153c6ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f381534c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38152ed8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38152ed730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38152b66a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3815281510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3815293620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3815293378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3815255268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3815214510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38151b7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38151b2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38151ac6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f38151929d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3815151158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3815151a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3815158268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3815151950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f3815151488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.5010672197728725e-05
Iter: 2 loss: 3.0133715296982803e-05
Iter: 3 loss: 0.00010841401054061131
Iter: 4 loss: 3.0133712081867508e-05
Iter: 5 loss: 2.7608847082637518e-05
Iter: 6 loss: 3.9517721317948366e-05
Iter: 7 loss: 2.7149778313175188e-05
Iter: 8 loss: 2.4963140086402891e-05
Iter: 9 loss: 3.6910306526409863e-05
Iter: 10 loss: 2.4639546231814804e-05
Iter: 11 loss: 2.2739253035340741e-05
Iter: 12 loss: 2.4466217659381058e-05
Iter: 13 loss: 2.1637539274887166e-05
Iter: 14 loss: 2.0454071104087812e-05
Iter: 15 loss: 1.9183568450067809e-05
Iter: 16 loss: 1.8981078437572308e-05
Iter: 17 loss: 1.7569061853344119e-05
Iter: 18 loss: 3.9159950817313639e-05
Iter: 19 loss: 1.7568600980045048e-05
Iter: 20 loss: 1.6492659133115527e-05
Iter: 21 loss: 1.6026560682330769e-05
Iter: 22 loss: 1.5474418112397902e-05
Iter: 23 loss: 1.4357978466413539e-05
Iter: 24 loss: 2.5000552917593924e-05
Iter: 25 loss: 1.4314854335941932e-05
Iter: 26 loss: 1.363203872391251e-05
Iter: 27 loss: 1.5048857957236906e-05
Iter: 28 loss: 1.335933024850946e-05
Iter: 29 loss: 1.2584753040146849e-05
Iter: 30 loss: 1.5743941399128483e-05
Iter: 31 loss: 1.2413909297791493e-05
Iter: 32 loss: 1.1841930814364711e-05
Iter: 33 loss: 1.1681286628823033e-05
Iter: 34 loss: 1.1332502911905611e-05
Iter: 35 loss: 1.0675901851645497e-05
Iter: 36 loss: 1.6024988973074367e-05
Iter: 37 loss: 1.0633915856268609e-05
Iter: 38 loss: 1.0075534057513678e-05
Iter: 39 loss: 1.0526804681290829e-05
Iter: 40 loss: 9.7404811840883681e-06
Iter: 41 loss: 9.3578703448774641e-06
Iter: 42 loss: 9.277274110595224e-06
Iter: 43 loss: 9.087721966915843e-06
Iter: 44 loss: 9.7609943676021759e-06
Iter: 45 loss: 9.0391760880926979e-06
Iter: 46 loss: 8.8504211175499771e-06
Iter: 47 loss: 8.45256507123072e-06
Iter: 48 loss: 1.5046882180531399e-05
Iter: 49 loss: 8.441586328327508e-06
Iter: 50 loss: 8.0942643375182616e-06
Iter: 51 loss: 1.0060253973193088e-05
Iter: 52 loss: 8.0461999992028735e-06
Iter: 53 loss: 7.7683455232348431e-06
Iter: 54 loss: 8.416336675317935e-06
Iter: 55 loss: 7.6659163631716859e-06
Iter: 56 loss: 7.3763404990363568e-06
Iter: 57 loss: 8.952528937775593e-06
Iter: 58 loss: 7.3336781128793066e-06
Iter: 59 loss: 7.13048279496765e-06
Iter: 60 loss: 7.0983881870697219e-06
Iter: 61 loss: 6.9579105217335249e-06
Iter: 62 loss: 6.7537548011323966e-06
Iter: 63 loss: 9.1557432211515124e-06
Iter: 64 loss: 6.7508659391268341e-06
Iter: 65 loss: 6.5745666921817568e-06
Iter: 66 loss: 6.53320881418283e-06
Iter: 67 loss: 6.4203603042410711e-06
Iter: 68 loss: 6.1839443686451564e-06
Iter: 69 loss: 7.76188349004618e-06
Iter: 70 loss: 6.1597949441716692e-06
Iter: 71 loss: 6.0116534335790922e-06
Iter: 72 loss: 5.9225614624469632e-06
Iter: 73 loss: 5.8614593253537543e-06
Iter: 74 loss: 5.7920847803604772e-06
Iter: 75 loss: 5.7670056847262815e-06
Iter: 76 loss: 5.6799882708414712e-06
Iter: 77 loss: 6.1995887752433817e-06
Iter: 78 loss: 5.6690372390108077e-06
Iter: 79 loss: 5.5983520868716838e-06
Iter: 80 loss: 5.5875275964857133e-06
Iter: 81 loss: 5.5384206840638826e-06
Iter: 82 loss: 5.4456767167870919e-06
Iter: 83 loss: 5.7280012833239415e-06
Iter: 84 loss: 5.4180361624275121e-06
Iter: 85 loss: 5.3420299061499906e-06
Iter: 86 loss: 5.1399541639803481e-06
Iter: 87 loss: 6.5763044499207294e-06
Iter: 88 loss: 5.09543302259407e-06
Iter: 89 loss: 5.0997471317333286e-06
Iter: 90 loss: 5.006065412805207e-06
Iter: 91 loss: 4.9367570739624254e-06
Iter: 92 loss: 4.9015381521567179e-06
Iter: 93 loss: 4.8691425953946819e-06
Iter: 94 loss: 4.7561546610912492e-06
Iter: 95 loss: 5.1453381183038427e-06
Iter: 96 loss: 4.7262583464929523e-06
Iter: 97 loss: 4.6378600258275616e-06
Iter: 98 loss: 4.778863557534626e-06
Iter: 99 loss: 4.5968815466023579e-06
Iter: 100 loss: 4.5121119088834672e-06
Iter: 101 loss: 5.4279374380794811e-06
Iter: 102 loss: 4.510180238593611e-06
Iter: 103 loss: 4.4542967581018483e-06
Iter: 104 loss: 4.4094652571470478e-06
Iter: 105 loss: 4.3926284562027653e-06
Iter: 106 loss: 4.3169170435320453e-06
Iter: 107 loss: 4.7609639638868948e-06
Iter: 108 loss: 4.3070589879417416e-06
Iter: 109 loss: 4.2535319203779357e-06
Iter: 110 loss: 4.7874006087139367e-06
Iter: 111 loss: 4.2517970233463039e-06
Iter: 112 loss: 4.2075968609775635e-06
Iter: 113 loss: 4.6758474983101321e-06
Iter: 114 loss: 4.2064896421969485e-06
Iter: 115 loss: 4.1886061093601009e-06
Iter: 116 loss: 4.1518698934623732e-06
Iter: 117 loss: 4.8032628988146349e-06
Iter: 118 loss: 4.1511587870684e-06
Iter: 119 loss: 4.0857352092809276e-06
Iter: 120 loss: 4.16450706507737e-06
Iter: 121 loss: 4.05132740873849e-06
Iter: 122 loss: 4.0167644754714088e-06
Iter: 123 loss: 4.0660870119596641e-06
Iter: 124 loss: 3.9998578557270962e-06
Iter: 125 loss: 3.949535355516561e-06
Iter: 126 loss: 3.9413351045220816e-06
Iter: 127 loss: 3.9067091130604194e-06
Iter: 128 loss: 3.8688812497578552e-06
Iter: 129 loss: 3.8674395995471636e-06
Iter: 130 loss: 3.834580804897699e-06
Iter: 131 loss: 3.8095364223121277e-06
Iter: 132 loss: 3.7989355207806521e-06
Iter: 133 loss: 3.7531862755087866e-06
Iter: 134 loss: 4.01171695352429e-06
Iter: 135 loss: 3.7468431451616541e-06
Iter: 136 loss: 3.7052481674590449e-06
Iter: 137 loss: 3.6859526892108356e-06
Iter: 138 loss: 3.6653948503066948e-06
Iter: 139 loss: 3.6304570637839688e-06
Iter: 140 loss: 3.6273796614886847e-06
Iter: 141 loss: 3.6083596902344343e-06
Iter: 142 loss: 3.5619598447247942e-06
Iter: 143 loss: 4.0424208789692439e-06
Iter: 144 loss: 3.5565563788252455e-06
Iter: 145 loss: 3.6176468857631217e-06
Iter: 146 loss: 3.5414309743861306e-06
Iter: 147 loss: 3.528699846187602e-06
Iter: 148 loss: 3.5069840865528937e-06
Iter: 149 loss: 3.50695498640359e-06
Iter: 150 loss: 3.485238977026753e-06
Iter: 151 loss: 3.5400538162338611e-06
Iter: 152 loss: 3.4776815839131938e-06
Iter: 153 loss: 3.4530037317381213e-06
Iter: 154 loss: 3.53894249489261e-06
Iter: 155 loss: 3.4465434439192019e-06
Iter: 156 loss: 3.42418108211948e-06
Iter: 157 loss: 3.4032764873246008e-06
Iter: 158 loss: 3.3980278411723636e-06
Iter: 159 loss: 3.3715253984395989e-06
Iter: 160 loss: 3.3932919162463473e-06
Iter: 161 loss: 3.3556948411871504e-06
Iter: 162 loss: 3.3250350211055338e-06
Iter: 163 loss: 3.6905013872934005e-06
Iter: 164 loss: 3.3246344312859735e-06
Iter: 165 loss: 3.3001279207316612e-06
Iter: 166 loss: 3.3350692549887095e-06
Iter: 167 loss: 3.2881347805595824e-06
Iter: 168 loss: 3.2578598436908882e-06
Iter: 169 loss: 3.3648877769182713e-06
Iter: 170 loss: 3.2500538194897471e-06
Iter: 171 loss: 3.2302434922469196e-06
Iter: 172 loss: 3.2223060288270039e-06
Iter: 173 loss: 3.2117350319745497e-06
Iter: 174 loss: 3.1826267760656781e-06
Iter: 175 loss: 3.4738697045944597e-06
Iter: 176 loss: 3.1816937695111303e-06
Iter: 177 loss: 3.1629905655976339e-06
Iter: 178 loss: 3.2114472665669683e-06
Iter: 179 loss: 3.156615495782032e-06
Iter: 180 loss: 3.1417666580636169e-06
Iter: 181 loss: 3.3595462227099614e-06
Iter: 182 loss: 3.1417490150292882e-06
Iter: 183 loss: 3.1274195184280258e-06
Iter: 184 loss: 3.150515944514426e-06
Iter: 185 loss: 3.120811851403176e-06
Iter: 186 loss: 3.1123515180509928e-06
Iter: 187 loss: 3.0947262909138533e-06
Iter: 188 loss: 3.3956861934694622e-06
Iter: 189 loss: 3.0943090225482176e-06
Iter: 190 loss: 3.079439056603239e-06
Iter: 191 loss: 3.0789750858018681e-06
Iter: 192 loss: 3.0682634998407104e-06
Iter: 193 loss: 3.0476875035818055e-06
Iter: 194 loss: 3.4791669821371011e-06
Iter: 195 loss: 3.0475805582510269e-06
Iter: 196 loss: 3.0296494563292129e-06
Iter: 197 loss: 3.2086865594397981e-06
Iter: 198 loss: 3.0290685746846195e-06
Iter: 199 loss: 3.0149011398636354e-06
Iter: 200 loss: 2.9910064510768221e-06
Iter: 201 loss: 2.9909549714732602e-06
Iter: 202 loss: 2.9899226611197255e-06
Iter: 203 loss: 2.9803720138489116e-06
Iter: 204 loss: 2.9711179471795483e-06
Iter: 205 loss: 2.9507220971997926e-06
Iter: 206 loss: 3.2477858735541166e-06
Iter: 207 loss: 2.94977482685948e-06
Iter: 208 loss: 2.926649237262187e-06
Iter: 209 loss: 3.1375496322237803e-06
Iter: 210 loss: 2.9256064356939234e-06
Iter: 211 loss: 2.9132199914953776e-06
Iter: 212 loss: 2.9554005088443269e-06
Iter: 213 loss: 2.9099035043423079e-06
Iter: 214 loss: 2.8973440775131981e-06
Iter: 215 loss: 2.9775102539803925e-06
Iter: 216 loss: 2.8959446979772239e-06
Iter: 217 loss: 2.888762445061859e-06
Iter: 218 loss: 2.888692495704371e-06
Iter: 219 loss: 2.8829384138042462e-06
Iter: 220 loss: 2.8672001690185868e-06
Iter: 221 loss: 2.9632161814668481e-06
Iter: 222 loss: 2.8629790625717998e-06
Iter: 223 loss: 2.8509668473953039e-06
Iter: 224 loss: 3.0076764744301327e-06
Iter: 225 loss: 2.8508899760320172e-06
Iter: 226 loss: 2.841768008342278e-06
Iter: 227 loss: 2.8789780734019081e-06
Iter: 228 loss: 2.8397626656049572e-06
Iter: 229 loss: 2.8302885920712964e-06
Iter: 230 loss: 2.8233111462651312e-06
Iter: 231 loss: 2.8201238145097873e-06
Iter: 232 loss: 2.8071288739116419e-06
Iter: 233 loss: 2.8269730532999168e-06
Iter: 234 loss: 2.8009725494534186e-06
Iter: 235 loss: 2.7890119849699207e-06
Iter: 236 loss: 2.8484924405018624e-06
Iter: 237 loss: 2.7869853225637318e-06
Iter: 238 loss: 2.7735359838304434e-06
Iter: 239 loss: 2.7843529066203085e-06
Iter: 240 loss: 2.765455607428334e-06
Iter: 241 loss: 2.7558684482195133e-06
Iter: 242 loss: 2.7555372233946176e-06
Iter: 243 loss: 2.7494693722167764e-06
Iter: 244 loss: 2.734737684547571e-06
Iter: 245 loss: 2.8899460094175656e-06
Iter: 246 loss: 2.7330858655502123e-06
Iter: 247 loss: 2.7230349523386783e-06
Iter: 248 loss: 2.7223290533999897e-06
Iter: 249 loss: 2.7159168061007248e-06
Iter: 250 loss: 2.7770536370183644e-06
Iter: 251 loss: 2.7156691604494256e-06
Iter: 252 loss: 2.7077681330080875e-06
Iter: 253 loss: 2.7020676041893959e-06
Iter: 254 loss: 2.6993441912688322e-06
Iter: 255 loss: 2.6908791419718356e-06
Iter: 256 loss: 2.7261999450844292e-06
Iter: 257 loss: 2.6890678613329521e-06
Iter: 258 loss: 2.6833026240333371e-06
Iter: 259 loss: 2.6744414814934164e-06
Iter: 260 loss: 2.6743040566185643e-06
Iter: 261 loss: 2.6654164122946321e-06
Iter: 262 loss: 2.6651287779745883e-06
Iter: 263 loss: 2.6603368601726516e-06
Iter: 264 loss: 2.6483582237383481e-06
Iter: 265 loss: 2.7609591062897539e-06
Iter: 266 loss: 2.6466721285996317e-06
Iter: 267 loss: 2.6344889889924052e-06
Iter: 268 loss: 2.7087929699550703e-06
Iter: 269 loss: 2.6330113603186764e-06
Iter: 270 loss: 2.6216532223107435e-06
Iter: 271 loss: 2.6653040571066541e-06
Iter: 272 loss: 2.6189772127411222e-06
Iter: 273 loss: 2.6098349742576852e-06
Iter: 274 loss: 2.6575017281608468e-06
Iter: 275 loss: 2.6083920131653807e-06
Iter: 276 loss: 2.6002385574168729e-06
Iter: 277 loss: 2.6390005780719896e-06
Iter: 278 loss: 2.5987661923667244e-06
Iter: 279 loss: 2.5926579244037573e-06
Iter: 280 loss: 2.5859934392762762e-06
Iter: 281 loss: 2.5849908671638262e-06
Iter: 282 loss: 2.5743851707744889e-06
Iter: 283 loss: 2.6779418612063444e-06
Iter: 284 loss: 2.5740110187604312e-06
Iter: 285 loss: 2.5717206081087227e-06
Iter: 286 loss: 2.5701576261901993e-06
Iter: 287 loss: 2.5679984849903452e-06
Iter: 288 loss: 2.5616105624807396e-06
Iter: 289 loss: 2.5836339531578666e-06
Iter: 290 loss: 2.5586837627448527e-06
Iter: 291 loss: 2.5472825332923231e-06
Iter: 292 loss: 2.5959025973631e-06
Iter: 293 loss: 2.5449071123075995e-06
Iter: 294 loss: 2.5395851766478885e-06
Iter: 295 loss: 2.5588218531224534e-06
Iter: 296 loss: 2.5382450587534475e-06
Iter: 297 loss: 2.5313210612461476e-06
Iter: 298 loss: 2.547523144949423e-06
Iter: 299 loss: 2.52877410249935e-06
Iter: 300 loss: 2.5235841244744882e-06
Iter: 301 loss: 2.5230507234898045e-06
Iter: 302 loss: 2.5192673532033203e-06
Iter: 303 loss: 2.5105385914517496e-06
Iter: 304 loss: 2.5203065481481857e-06
Iter: 305 loss: 2.5058185111378018e-06
Iter: 306 loss: 2.4986670454913881e-06
Iter: 307 loss: 2.508190213527178e-06
Iter: 308 loss: 2.4950599083246126e-06
Iter: 309 loss: 2.4867449993995892e-06
Iter: 310 loss: 2.5731655955611822e-06
Iter: 311 loss: 2.4865179462886e-06
Iter: 312 loss: 2.4794730013820325e-06
Iter: 313 loss: 2.4910660024692756e-06
Iter: 314 loss: 2.476258513081225e-06
Iter: 315 loss: 2.4693645136477324e-06
Iter: 316 loss: 2.4941954750231464e-06
Iter: 317 loss: 2.4676224463117464e-06
Iter: 318 loss: 2.4631492607919357e-06
Iter: 319 loss: 2.4989812437209148e-06
Iter: 320 loss: 2.4628519783569467e-06
Iter: 321 loss: 2.4568026314950491e-06
Iter: 322 loss: 2.4604380143229531e-06
Iter: 323 loss: 2.4529059387012179e-06
Iter: 324 loss: 2.4492809534600111e-06
Iter: 325 loss: 2.4510031022060987e-06
Iter: 326 loss: 2.4468405425315681e-06
Iter: 327 loss: 2.4415247291695795e-06
Iter: 328 loss: 2.4471128156205044e-06
Iter: 329 loss: 2.4385853925984713e-06
Iter: 330 loss: 2.4336610271666438e-06
Iter: 331 loss: 2.4710376341751364e-06
Iter: 332 loss: 2.4332854466931059e-06
Iter: 333 loss: 2.4286907085730842e-06
Iter: 334 loss: 2.4337951104946975e-06
Iter: 335 loss: 2.4261993256798881e-06
Iter: 336 loss: 2.4198186346184837e-06
Iter: 337 loss: 2.4222874838936393e-06
Iter: 338 loss: 2.4153866794330611e-06
Iter: 339 loss: 2.4099656004692778e-06
Iter: 340 loss: 2.4134366916768571e-06
Iter: 341 loss: 2.406520349026046e-06
Iter: 342 loss: 2.399593561842847e-06
Iter: 343 loss: 2.4392006692182864e-06
Iter: 344 loss: 2.3986512872052708e-06
Iter: 345 loss: 2.3935713594966123e-06
Iter: 346 loss: 2.4094803499096496e-06
Iter: 347 loss: 2.3920972153902189e-06
Iter: 348 loss: 2.3880538674456457e-06
Iter: 349 loss: 2.4204992279572275e-06
Iter: 350 loss: 2.387786499955895e-06
Iter: 351 loss: 2.3834691694341594e-06
Iter: 352 loss: 2.3802047293821794e-06
Iter: 353 loss: 2.37879797687394e-06
Iter: 354 loss: 2.377821952520389e-06
Iter: 355 loss: 2.3761053044038049e-06
Iter: 356 loss: 2.3737622437999328e-06
Iter: 357 loss: 2.3752361019124693e-06
Iter: 358 loss: 2.3722673455606754e-06
Iter: 359 loss: 2.3699712773697367e-06
Iter: 360 loss: 2.3638080503507588e-06
Iter: 361 loss: 2.4055103017771062e-06
Iter: 362 loss: 2.3623564072306598e-06
Iter: 363 loss: 2.3583149364563762e-06
Iter: 364 loss: 2.3580918405423674e-06
Iter: 365 loss: 2.3536027526896593e-06
Iter: 366 loss: 2.3525761129917526e-06
Iter: 367 loss: 2.3496839150497618e-06
Iter: 368 loss: 2.345730628147645e-06
Iter: 369 loss: 2.3457050770491605e-06
Iter: 370 loss: 2.3431983362999642e-06
Iter: 371 loss: 2.3379634839065617e-06
Iter: 372 loss: 2.4267581532452587e-06
Iter: 373 loss: 2.3378353881593272e-06
Iter: 374 loss: 2.3311277590286839e-06
Iter: 375 loss: 2.3692766125642834e-06
Iter: 376 loss: 2.3302070823417585e-06
Iter: 377 loss: 2.3256589044684491e-06
Iter: 378 loss: 2.3227567994592542e-06
Iter: 379 loss: 2.3209772666057783e-06
Iter: 380 loss: 2.3155732014604784e-06
Iter: 381 loss: 2.35858379173673e-06
Iter: 382 loss: 2.31520799455108e-06
Iter: 383 loss: 2.3105266209694878e-06
Iter: 384 loss: 2.3401668809309144e-06
Iter: 385 loss: 2.3099970384945346e-06
Iter: 386 loss: 2.30594081547787e-06
Iter: 387 loss: 2.3178007794717296e-06
Iter: 388 loss: 2.3046889084859455e-06
Iter: 389 loss: 2.3023289654907064e-06
Iter: 390 loss: 2.3021766165271552e-06
Iter: 391 loss: 2.3002140172133311e-06
Iter: 392 loss: 2.2973857470638048e-06
Iter: 393 loss: 2.2972977420440606e-06
Iter: 394 loss: 2.2942549683312288e-06
Iter: 395 loss: 2.2886179004030104e-06
Iter: 396 loss: 2.4173108695398921e-06
Iter: 397 loss: 2.2886098840110574e-06
Iter: 398 loss: 2.2850005142675888e-06
Iter: 399 loss: 2.2846151734377483e-06
Iter: 400 loss: 2.2822379702138443e-06
Iter: 401 loss: 2.2892988928622648e-06
Iter: 402 loss: 2.28151417978508e-06
Iter: 403 loss: 2.2782311096752332e-06
Iter: 404 loss: 2.2760543874778273e-06
Iter: 405 loss: 2.2748165778431028e-06
Iter: 406 loss: 2.2711441522137033e-06
Iter: 407 loss: 2.3096242796029667e-06
Iter: 408 loss: 2.2710475247268152e-06
Iter: 409 loss: 2.2681002818261337e-06
Iter: 410 loss: 2.2636120041449906e-06
Iter: 411 loss: 2.2635337253165746e-06
Iter: 412 loss: 2.2590632125832377e-06
Iter: 413 loss: 2.263990728501685e-06
Iter: 414 loss: 2.2566325255232257e-06
Iter: 415 loss: 2.25318567844998e-06
Iter: 416 loss: 2.2530839212938858e-06
Iter: 417 loss: 2.2506709363462678e-06
Iter: 418 loss: 2.2481554144242337e-06
Iter: 419 loss: 2.2477088263018556e-06
Iter: 420 loss: 2.243924766622961e-06
Iter: 421 loss: 2.2439000425232124e-06
Iter: 422 loss: 2.2414206739880514e-06
Iter: 423 loss: 2.2628542737297879e-06
Iter: 424 loss: 2.241288153465906e-06
Iter: 425 loss: 2.2396392689467912e-06
Iter: 426 loss: 2.2364744352248676e-06
Iter: 427 loss: 2.3030014012678734e-06
Iter: 428 loss: 2.2364583547084838e-06
Iter: 429 loss: 2.2335070805569823e-06
Iter: 430 loss: 2.2365967563528743e-06
Iter: 431 loss: 2.2318727055499914e-06
Iter: 432 loss: 2.2276413035739835e-06
Iter: 433 loss: 2.2342782264906408e-06
Iter: 434 loss: 2.225663268250592e-06
Iter: 435 loss: 2.2235726655061814e-06
Iter: 436 loss: 2.223265174405886e-06
Iter: 437 loss: 2.2213216371321277e-06
Iter: 438 loss: 2.2173394285298397e-06
Iter: 439 loss: 2.2884567480621409e-06
Iter: 440 loss: 2.2172652917652434e-06
Iter: 441 loss: 2.2131717937248886e-06
Iter: 442 loss: 2.2610301007122268e-06
Iter: 443 loss: 2.2131112161820105e-06
Iter: 444 loss: 2.2103649839512504e-06
Iter: 445 loss: 2.2086630476164079e-06
Iter: 446 loss: 2.2075598934039974e-06
Iter: 447 loss: 2.2032787852887444e-06
Iter: 448 loss: 2.2216693523456793e-06
Iter: 449 loss: 2.2023951533100538e-06
Iter: 450 loss: 2.1995042479759803e-06
Iter: 451 loss: 2.2024765990191828e-06
Iter: 452 loss: 2.1978933756530958e-06
Iter: 453 loss: 2.1946410926190804e-06
Iter: 454 loss: 2.2154460613026551e-06
Iter: 455 loss: 2.1942803803497618e-06
Iter: 456 loss: 2.1924131405649508e-06
Iter: 457 loss: 2.1923767496534933e-06
Iter: 458 loss: 2.1904911935923371e-06
Iter: 459 loss: 2.19186409202304e-06
Iter: 460 loss: 2.1893284625828494e-06
Iter: 461 loss: 2.1879133924316579e-06
Iter: 462 loss: 2.1893578758409304e-06
Iter: 463 loss: 2.1871228929168838e-06
Iter: 464 loss: 2.1849949909710234e-06
Iter: 465 loss: 2.1810082302131117e-06
Iter: 466 loss: 2.2696624613656914e-06
Iter: 467 loss: 2.1809989310048565e-06
Iter: 468 loss: 2.1767873005779011e-06
Iter: 469 loss: 2.188299280197826e-06
Iter: 470 loss: 2.1754111096030768e-06
Iter: 471 loss: 2.1729317628146291e-06
Iter: 472 loss: 2.1726172132138727e-06
Iter: 473 loss: 2.1709432659614031e-06
Iter: 474 loss: 2.1719770424948189e-06
Iter: 475 loss: 2.16987105362506e-06
Iter: 476 loss: 2.1674141717228567e-06
Iter: 477 loss: 2.1648214088582e-06
Iter: 478 loss: 2.1643804168037785e-06
Iter: 479 loss: 2.1626827247425278e-06
Iter: 480 loss: 2.1624495581951689e-06
Iter: 481 loss: 2.1610262394757489e-06
Iter: 482 loss: 2.1574874216134959e-06
Iter: 483 loss: 2.1915069212665631e-06
Iter: 484 loss: 2.1570095178350609e-06
Iter: 485 loss: 2.1534387491546359e-06
Iter: 486 loss: 2.1862374355897775e-06
Iter: 487 loss: 2.1532815140306154e-06
Iter: 488 loss: 2.1504686124381418e-06
Iter: 489 loss: 2.1638257544581059e-06
Iter: 490 loss: 2.1499600152759325e-06
Iter: 491 loss: 2.1491984488731097e-06
Iter: 492 loss: 2.1486489187191596e-06
Iter: 493 loss: 2.1476901582485265e-06
Iter: 494 loss: 2.1451404687304294e-06
Iter: 495 loss: 2.1632371509600595e-06
Iter: 496 loss: 2.1445776687337097e-06
Iter: 497 loss: 2.1417576805375891e-06
Iter: 498 loss: 2.1658006047165824e-06
Iter: 499 loss: 2.1416004277298255e-06
Iter: 500 loss: 2.1395880907284056e-06
Iter: 501 loss: 2.1383031037004423e-06
Iter: 502 loss: 2.1375163857484149e-06
Iter: 503 loss: 2.1348943629815365e-06
Iter: 504 loss: 2.1486792342113941e-06
Iter: 505 loss: 2.1344858403083123e-06
Iter: 506 loss: 2.131858498994129e-06
Iter: 507 loss: 2.1363456268133053e-06
Iter: 508 loss: 2.1306832983334311e-06
Iter: 509 loss: 2.1273940138972674e-06
Iter: 510 loss: 2.1515367824362197e-06
Iter: 511 loss: 2.1271220092895286e-06
Iter: 512 loss: 2.1257208400531746e-06
Iter: 513 loss: 2.1233593135718184e-06
Iter: 514 loss: 2.1233540846016024e-06
Iter: 515 loss: 2.1202644099074296e-06
Iter: 516 loss: 2.1470945671454877e-06
Iter: 517 loss: 2.1201014677545934e-06
Iter: 518 loss: 2.1181630311070715e-06
Iter: 519 loss: 2.1217441863278555e-06
Iter: 520 loss: 2.1173330818786511e-06
Iter: 521 loss: 2.1148350050098221e-06
Iter: 522 loss: 2.1177798086936352e-06
Iter: 523 loss: 2.1135105295458989e-06
Iter: 524 loss: 2.1113165943359573e-06
Iter: 525 loss: 2.1123657119003445e-06
Iter: 526 loss: 2.1098408994102824e-06
Iter: 527 loss: 2.1081890532068331e-06
Iter: 528 loss: 2.107721433452786e-06
Iter: 529 loss: 2.1069216807659846e-06
Iter: 530 loss: 2.1051594914987833e-06
Iter: 531 loss: 2.1308347282045235e-06
Iter: 532 loss: 2.1050778421703081e-06
Iter: 533 loss: 2.1032354857099372e-06
Iter: 534 loss: 2.1097073888929015e-06
Iter: 535 loss: 2.102757507297299e-06
Iter: 536 loss: 2.10091119859584e-06
Iter: 537 loss: 2.1066380957566174e-06
Iter: 538 loss: 2.1003705868104073e-06
Iter: 539 loss: 2.0984400313257131e-06
Iter: 540 loss: 2.0972951794592627e-06
Iter: 541 loss: 2.0964898536321488e-06
Iter: 542 loss: 2.0946679911871355e-06
Iter: 543 loss: 2.1208994359313666e-06
Iter: 544 loss: 2.0946648072905441e-06
Iter: 545 loss: 2.0927530925807806e-06
Iter: 546 loss: 2.0947124082959628e-06
Iter: 547 loss: 2.0916865879717276e-06
Iter: 548 loss: 2.0898247024850821e-06
Iter: 549 loss: 2.0944354132315249e-06
Iter: 550 loss: 2.0891674461964279e-06
Iter: 551 loss: 2.0874536587534859e-06
Iter: 552 loss: 2.0859106477047526e-06
Iter: 553 loss: 2.0854796964850185e-06
Iter: 554 loss: 2.0833229304632546e-06
Iter: 555 loss: 2.0833132065289304e-06
Iter: 556 loss: 2.0816508800635914e-06
Iter: 557 loss: 2.0797882810425847e-06
Iter: 558 loss: 2.079535734469443e-06
Iter: 559 loss: 2.0767974952313654e-06
Iter: 560 loss: 2.1002543788536855e-06
Iter: 561 loss: 2.0766470973548405e-06
Iter: 562 loss: 2.075491974262707e-06
Iter: 563 loss: 2.0752924958416787e-06
Iter: 564 loss: 2.0745866223078181e-06
Iter: 565 loss: 2.0727169607644661e-06
Iter: 566 loss: 2.0862603039278258e-06
Iter: 567 loss: 2.0723157612721021e-06
Iter: 568 loss: 2.0704092883021832e-06
Iter: 569 loss: 2.0717691205399073e-06
Iter: 570 loss: 2.069227753805584e-06
Iter: 571 loss: 2.0668472394566369e-06
Iter: 572 loss: 2.096682520177654e-06
Iter: 573 loss: 2.0668256020734993e-06
Iter: 574 loss: 2.0654243664860633e-06
Iter: 575 loss: 2.0655313683549439e-06
Iter: 576 loss: 2.0643348517918079e-06
Iter: 577 loss: 2.0621511218793481e-06
Iter: 578 loss: 2.0681342265876893e-06
Iter: 579 loss: 2.0614389658845341e-06
Iter: 580 loss: 2.0599006818305016e-06
Iter: 581 loss: 2.0784937885277257e-06
Iter: 582 loss: 2.0598823679861493e-06
Iter: 583 loss: 2.0583773645012755e-06
Iter: 584 loss: 2.0560272441533736e-06
Iter: 585 loss: 2.0559980405968177e-06
Iter: 586 loss: 2.0542368784813518e-06
Iter: 587 loss: 2.053495809902541e-06
Iter: 588 loss: 2.05257838128822e-06
Iter: 589 loss: 2.0502272632005712e-06
Iter: 590 loss: 2.0502128262115778e-06
Iter: 591 loss: 2.0489432665947434e-06
Iter: 592 loss: 2.0508542612487021e-06
Iter: 593 loss: 2.0483376982893233e-06
Iter: 594 loss: 2.0467969970868341e-06
Iter: 595 loss: 2.0545001156960359e-06
Iter: 596 loss: 2.0465379178476284e-06
Iter: 597 loss: 2.0446645662139305e-06
Iter: 598 loss: 2.0526734078542112e-06
Iter: 599 loss: 2.0442757599836398e-06
Iter: 600 loss: 2.0434412459129594e-06
Iter: 601 loss: 2.0413538060328335e-06
Iter: 602 loss: 2.0609386400773837e-06
Iter: 603 loss: 2.0410586462514133e-06
Iter: 604 loss: 2.0389818708156766e-06
Iter: 605 loss: 2.0542105492946351e-06
Iter: 606 loss: 2.0388098234185728e-06
Iter: 607 loss: 2.0368574776842058e-06
Iter: 608 loss: 2.0420279664424554e-06
Iter: 609 loss: 2.0362030793569193e-06
Iter: 610 loss: 2.0342100715677117e-06
Iter: 611 loss: 2.04371357713112e-06
Iter: 612 loss: 2.0338516618361994e-06
Iter: 613 loss: 2.0323443620779925e-06
Iter: 614 loss: 2.0336610923863076e-06
Iter: 615 loss: 2.0314597956861821e-06
Iter: 616 loss: 2.0305400501460586e-06
Iter: 617 loss: 2.0304714228827254e-06
Iter: 618 loss: 2.0297333855083876e-06
Iter: 619 loss: 2.0279068601592841e-06
Iter: 620 loss: 2.0457923574335696e-06
Iter: 621 loss: 2.0276688140620458e-06
Iter: 622 loss: 2.0254615982035786e-06
Iter: 623 loss: 2.0418213035856195e-06
Iter: 624 loss: 2.0252832189028277e-06
Iter: 625 loss: 2.0235314448315732e-06
Iter: 626 loss: 2.0212616709631829e-06
Iter: 627 loss: 2.021109247091002e-06
Iter: 628 loss: 2.0203320370454332e-06
Iter: 629 loss: 2.0195890019439309e-06
Iter: 630 loss: 2.0187726096295417e-06
Iter: 631 loss: 2.0271985982664741e-06
Iter: 632 loss: 2.0187496392748029e-06
Iter: 633 loss: 2.0178079269781576e-06
Iter: 634 loss: 2.0158971795678738e-06
Iter: 635 loss: 2.050912538448986e-06
Iter: 636 loss: 2.0158666724107854e-06
Iter: 637 loss: 2.0143186031324084e-06
Iter: 638 loss: 2.0169215463995066e-06
Iter: 639 loss: 2.0136202720748721e-06
Iter: 640 loss: 2.0117525112656678e-06
Iter: 641 loss: 2.0124974771565565e-06
Iter: 642 loss: 2.0104607052450332e-06
Iter: 643 loss: 2.0084162475953696e-06
Iter: 644 loss: 2.0219071258867863e-06
Iter: 645 loss: 2.008203119771922e-06
Iter: 646 loss: 2.0066926222283028e-06
Iter: 647 loss: 2.0226497448456353e-06
Iter: 648 loss: 2.0066542994106036e-06
Iter: 649 loss: 2.0059226661523567e-06
Iter: 650 loss: 2.005994589346019e-06
Iter: 651 loss: 2.0053583901339544e-06
Iter: 652 loss: 2.0039936504289661e-06
Iter: 653 loss: 2.0065311204598957e-06
Iter: 654 loss: 2.0034114261625485e-06
Iter: 655 loss: 2.00168292850801e-06
Iter: 656 loss: 2.00625247974654e-06
Iter: 657 loss: 2.0011028197061747e-06
Iter: 658 loss: 1.999930062892088e-06
Iter: 659 loss: 1.9981909666699533e-06
Iter: 660 loss: 1.9981499504862449e-06
Iter: 661 loss: 1.9959766436049968e-06
Iter: 662 loss: 2.0096210051102677e-06
Iter: 663 loss: 1.995726791003088e-06
Iter: 664 loss: 1.9940486717211634e-06
Iter: 665 loss: 2.0109928590529806e-06
Iter: 666 loss: 1.9939968902038023e-06
Iter: 667 loss: 1.992808174831509e-06
Iter: 668 loss: 2.0102386743113932e-06
Iter: 669 loss: 1.9928067436321549e-06
Iter: 670 loss: 1.992152323775336e-06
Iter: 671 loss: 1.991195541496742e-06
Iter: 672 loss: 1.9911695154181312e-06
Iter: 673 loss: 1.9897197930805684e-06
Iter: 674 loss: 1.9880616424773391e-06
Iter: 675 loss: 1.9878550652126925e-06
Iter: 676 loss: 1.9856915440227692e-06
Iter: 677 loss: 1.9941044642087845e-06
Iter: 678 loss: 1.985188722573313e-06
Iter: 679 loss: 1.9834412611234097e-06
Iter: 680 loss: 1.9965889120023303e-06
Iter: 681 loss: 1.9833050772257311e-06
Iter: 682 loss: 1.9818323827534783e-06
Iter: 683 loss: 1.9831886545844676e-06
Iter: 684 loss: 1.9809817806091693e-06
Iter: 685 loss: 1.9790501769123641e-06
Iter: 686 loss: 1.9932430836193309e-06
Iter: 687 loss: 1.9788908780127508e-06
Iter: 688 loss: 1.9780431844574477e-06
Iter: 689 loss: 1.9804795954511383e-06
Iter: 690 loss: 1.97777761331999e-06
Iter: 691 loss: 1.9764558825801713e-06
Iter: 692 loss: 1.9750539031455263e-06
Iter: 693 loss: 1.9748197840384975e-06
Iter: 694 loss: 1.9732053314197167e-06
Iter: 695 loss: 1.9749531787947277e-06
Iter: 696 loss: 1.9723217914690848e-06
Iter: 697 loss: 1.9701218866896042e-06
Iter: 698 loss: 1.9786694680791027e-06
Iter: 699 loss: 1.9696101080692166e-06
Iter: 700 loss: 1.9690435759302543e-06
Iter: 701 loss: 1.9688859409474887e-06
Iter: 702 loss: 1.9679716363894273e-06
Iter: 703 loss: 1.9666640733798241e-06
Iter: 704 loss: 1.9666205660974149e-06
Iter: 705 loss: 1.9651478821986223e-06
Iter: 706 loss: 1.9691262498550274e-06
Iter: 707 loss: 1.9646620265055242e-06
Iter: 708 loss: 1.9633909096872165e-06
Iter: 709 loss: 1.9634367216725296e-06
Iter: 710 loss: 1.9623875694038968e-06
Iter: 711 loss: 1.9607284086468615e-06
Iter: 712 loss: 1.9678365103576683e-06
Iter: 713 loss: 1.9603847370055149e-06
Iter: 714 loss: 1.9589243789928481e-06
Iter: 715 loss: 1.9584489209449679e-06
Iter: 716 loss: 1.9576002807863573e-06
Iter: 717 loss: 1.9561458775277689e-06
Iter: 718 loss: 1.9561359379848156e-06
Iter: 719 loss: 1.9548040006560152e-06
Iter: 720 loss: 1.9556792293329124e-06
Iter: 721 loss: 1.9539624582965837e-06
Iter: 722 loss: 1.9528752622109402e-06
Iter: 723 loss: 1.9641755341299774e-06
Iter: 724 loss: 1.9528455880769271e-06
Iter: 725 loss: 1.9518288913841027e-06
Iter: 726 loss: 1.9506621513274746e-06
Iter: 727 loss: 1.9505188229461888e-06
Iter: 728 loss: 1.9486721615671468e-06
Iter: 729 loss: 1.9565482888590486e-06
Iter: 730 loss: 1.9482874857197627e-06
Iter: 731 loss: 1.9471107841633669e-06
Iter: 732 loss: 1.9453053947905588e-06
Iter: 733 loss: 1.9452767241689827e-06
Iter: 734 loss: 1.9464726075263215e-06
Iter: 735 loss: 1.9444144716004418e-06
Iter: 736 loss: 1.9436001553310935e-06
Iter: 737 loss: 1.9424894005910682e-06
Iter: 738 loss: 1.9424358337338754e-06
Iter: 739 loss: 1.9411722859102761e-06
Iter: 740 loss: 1.940904574746625e-06
Iter: 741 loss: 1.9400764475638038e-06
Iter: 742 loss: 1.9385429740735263e-06
Iter: 743 loss: 1.9457672467267e-06
Iter: 744 loss: 1.9382625161944361e-06
Iter: 745 loss: 1.9366495587734789e-06
Iter: 746 loss: 1.9395796974669404e-06
Iter: 747 loss: 1.9359522773429045e-06
Iter: 748 loss: 1.9347170393657276e-06
Iter: 749 loss: 1.9359695105558843e-06
Iter: 750 loss: 1.9340254373561882e-06
Iter: 751 loss: 1.932398794857216e-06
Iter: 752 loss: 1.9388001089076521e-06
Iter: 753 loss: 1.9320258025781564e-06
Iter: 754 loss: 1.9307167773905112e-06
Iter: 755 loss: 1.9413594040101621e-06
Iter: 756 loss: 1.9306332299064349e-06
Iter: 757 loss: 1.9292351450535875e-06
Iter: 758 loss: 1.928454812151454e-06
Iter: 759 loss: 1.9278429911779864e-06
Iter: 760 loss: 1.9268209816903743e-06
Iter: 761 loss: 1.9267872360376241e-06
Iter: 762 loss: 1.9260352235967184e-06
Iter: 763 loss: 1.9242391880165328e-06
Iter: 764 loss: 1.9443403377445321e-06
Iter: 765 loss: 1.9240630542758294e-06
Iter: 766 loss: 1.9222956153543792e-06
Iter: 767 loss: 1.9397840967357129e-06
Iter: 768 loss: 1.9222363975188375e-06
Iter: 769 loss: 1.9214584087774992e-06
Iter: 770 loss: 1.9214269656547855e-06
Iter: 771 loss: 1.9204677313647032e-06
Iter: 772 loss: 1.9191583953011449e-06
Iter: 773 loss: 1.9190956217171092e-06
Iter: 774 loss: 1.9180168333540014e-06
Iter: 775 loss: 1.9167329168542905e-06
Iter: 776 loss: 1.9165986682767632e-06
Iter: 777 loss: 1.9151195175861206e-06
Iter: 778 loss: 1.9359833576258822e-06
Iter: 779 loss: 1.9151158536026821e-06
Iter: 780 loss: 1.9140050949257459e-06
Iter: 781 loss: 1.9150741186392486e-06
Iter: 782 loss: 1.9133724432090438e-06
Iter: 783 loss: 1.9120877518219858e-06
Iter: 784 loss: 1.9163489567077544e-06
Iter: 785 loss: 1.9117346899607766e-06
Iter: 786 loss: 1.9103538727476742e-06
Iter: 787 loss: 1.9092191720433375e-06
Iter: 788 loss: 1.908817053117249e-06
Iter: 789 loss: 1.9083929446357889e-06
Iter: 790 loss: 1.9079353736176554e-06
Iter: 791 loss: 1.90719228087258e-06
Iter: 792 loss: 1.9063290878303316e-06
Iter: 793 loss: 1.9062284476593694e-06
Iter: 794 loss: 1.9047579874800884e-06
Iter: 795 loss: 1.9109529130328494e-06
Iter: 796 loss: 1.9044470605172195e-06
Iter: 797 loss: 1.9033648281889431e-06
Iter: 798 loss: 1.9077334505878421e-06
Iter: 799 loss: 1.9031239052395314e-06
Iter: 800 loss: 1.9020299308674686e-06
Iter: 801 loss: 1.9010808060578116e-06
Iter: 802 loss: 1.9007878308091913e-06
Iter: 803 loss: 1.9009033506440122e-06
Iter: 804 loss: 1.900262718946682e-06
Iter: 805 loss: 1.8996510527550241e-06
Iter: 806 loss: 1.8983142096010807e-06
Iter: 807 loss: 1.9182787535541436e-06
Iter: 808 loss: 1.8982574801154707e-06
Iter: 809 loss: 1.8966852953878641e-06
Iter: 810 loss: 1.8970416911466624e-06
Iter: 811 loss: 1.8955285608087692e-06
Iter: 812 loss: 1.8939895676747449e-06
Iter: 813 loss: 1.8964921826583625e-06
Iter: 814 loss: 1.8932830691198188e-06
Iter: 815 loss: 1.891907815570884e-06
Iter: 816 loss: 1.9019413876245329e-06
Iter: 817 loss: 1.8917924952024358e-06
Iter: 818 loss: 1.8901793642720177e-06
Iter: 819 loss: 1.8919823294030164e-06
Iter: 820 loss: 1.8893066639971297e-06
Iter: 821 loss: 1.8879782116343248e-06
Iter: 822 loss: 1.8889507340143071e-06
Iter: 823 loss: 1.8871601898739688e-06
Iter: 824 loss: 1.8857927743888005e-06
Iter: 825 loss: 1.9056091206774227e-06
Iter: 826 loss: 1.8857906708041899e-06
Iter: 827 loss: 1.884784957880576e-06
Iter: 828 loss: 1.8882793108647617e-06
Iter: 829 loss: 1.884521092067258e-06
Iter: 830 loss: 1.8835405854848046e-06
Iter: 831 loss: 1.8831834565581635e-06
Iter: 832 loss: 1.8826378411815051e-06
Iter: 833 loss: 1.8815848477922393e-06
Iter: 834 loss: 1.8947132911436124e-06
Iter: 835 loss: 1.8815748760145506e-06
Iter: 836 loss: 1.8807691021150811e-06
Iter: 837 loss: 1.8794973089028015e-06
Iter: 838 loss: 1.8794839226819647e-06
Iter: 839 loss: 1.8790341543761287e-06
Iter: 840 loss: 1.8786376390096638e-06
Iter: 841 loss: 1.8778612934460917e-06
Iter: 842 loss: 1.8771967642014442e-06
Iter: 843 loss: 1.8769843456868887e-06
Iter: 844 loss: 1.876149604211562e-06
Iter: 845 loss: 1.87546322237072e-06
Iter: 846 loss: 1.875220385235289e-06
Iter: 847 loss: 1.8739446894303924e-06
Iter: 848 loss: 1.8740129251671011e-06
Iter: 849 loss: 1.8729443258075896e-06
Iter: 850 loss: 1.8715576141261475e-06
Iter: 851 loss: 1.8933187832374051e-06
Iter: 852 loss: 1.8715575313973239e-06
Iter: 853 loss: 1.8705043563814586e-06
Iter: 854 loss: 1.8697395916852209e-06
Iter: 855 loss: 1.8693793745486395e-06
Iter: 856 loss: 1.8675978785586833e-06
Iter: 857 loss: 1.8856832587403827e-06
Iter: 858 loss: 1.8675440712817985e-06
Iter: 859 loss: 1.8667269355864909e-06
Iter: 860 loss: 1.8663906058433875e-06
Iter: 861 loss: 1.8659602539533925e-06
Iter: 862 loss: 1.8649829669737272e-06
Iter: 863 loss: 1.8649770948205035e-06
Iter: 864 loss: 1.8642895438527597e-06
Iter: 865 loss: 1.8636325964679949e-06
Iter: 866 loss: 1.8634779868875345e-06
Iter: 867 loss: 1.8620354868138497e-06
Iter: 868 loss: 1.8630161670012833e-06
Iter: 869 loss: 1.8611312782839337e-06
Iter: 870 loss: 1.8600794947052177e-06
Iter: 871 loss: 1.8600678433860396e-06
Iter: 872 loss: 1.8594404661054628e-06
Iter: 873 loss: 1.8642617221416433e-06
Iter: 874 loss: 1.85939407707455e-06
Iter: 875 loss: 1.8586543311722371e-06
Iter: 876 loss: 1.8577359935112306e-06
Iter: 877 loss: 1.8576578434583635e-06
Iter: 878 loss: 1.8568227957056036e-06
Iter: 879 loss: 1.8559341583399189e-06
Iter: 880 loss: 1.8557875056362509e-06
Iter: 881 loss: 1.8541409707296964e-06
Iter: 882 loss: 1.8595058364204472e-06
Iter: 883 loss: 1.8536806158738439e-06
Iter: 884 loss: 1.852268416011313e-06
Iter: 885 loss: 1.8542540697265119e-06
Iter: 886 loss: 1.8515729978040543e-06
Iter: 887 loss: 1.8501326068385882e-06
Iter: 888 loss: 1.854371411400223e-06
Iter: 889 loss: 1.8496904548456181e-06
Iter: 890 loss: 1.8479867986712256e-06
Iter: 891 loss: 1.8602844485120783e-06
Iter: 892 loss: 1.8478403868481674e-06
Iter: 893 loss: 1.8469468795177645e-06
Iter: 894 loss: 1.8497001396581779e-06
Iter: 895 loss: 1.84668364311075e-06
Iter: 896 loss: 1.8456313494714448e-06
Iter: 897 loss: 1.8465401056866372e-06
Iter: 898 loss: 1.8450117253682925e-06
Iter: 899 loss: 1.8434773662284349e-06
Iter: 900 loss: 1.8513541320969269e-06
Iter: 901 loss: 1.8432294342244339e-06
Iter: 902 loss: 1.8424896098230755e-06
Iter: 903 loss: 1.8420735963219177e-06
Iter: 904 loss: 1.8417516666297655e-06
Iter: 905 loss: 1.8406307940785755e-06
Iter: 906 loss: 1.8517794728899958e-06
Iter: 907 loss: 1.8405940270242771e-06
Iter: 908 loss: 1.8397104545926062e-06
Iter: 909 loss: 1.8473126499044877e-06
Iter: 910 loss: 1.8396624706929286e-06
Iter: 911 loss: 1.8388881131205813e-06
Iter: 912 loss: 1.8377205222138061e-06
Iter: 913 loss: 1.837697610216198e-06
Iter: 914 loss: 1.8366735865553045e-06
Iter: 915 loss: 1.8415385922847607e-06
Iter: 916 loss: 1.8364884648557413e-06
Iter: 917 loss: 1.8357123878112702e-06
Iter: 918 loss: 1.8340619170599381e-06
Iter: 919 loss: 1.8606980528642929e-06
Iter: 920 loss: 1.8340108467853926e-06
Iter: 921 loss: 1.832339904686276e-06
Iter: 922 loss: 1.8464880359505443e-06
Iter: 923 loss: 1.8322448236802916e-06
Iter: 924 loss: 1.8307762705146915e-06
Iter: 925 loss: 1.8364144797063742e-06
Iter: 926 loss: 1.8304299136464942e-06
Iter: 927 loss: 1.8292727273287272e-06
Iter: 928 loss: 1.8314444997343631e-06
Iter: 929 loss: 1.8287817840600269e-06
Iter: 930 loss: 1.8271883088177027e-06
Iter: 931 loss: 1.8360906777408074e-06
Iter: 932 loss: 1.8269630526083349e-06
Iter: 933 loss: 1.8261149293386095e-06
Iter: 934 loss: 1.8298065987615693e-06
Iter: 935 loss: 1.8259427426471298e-06
Iter: 936 loss: 1.8249563052744705e-06
Iter: 937 loss: 1.8264837463944475e-06
Iter: 938 loss: 1.8244921237539432e-06
Iter: 939 loss: 1.8235285832732554e-06
Iter: 940 loss: 1.8243895073231714e-06
Iter: 941 loss: 1.8229669414237231e-06
Iter: 942 loss: 1.8219464497494299e-06
Iter: 943 loss: 1.8299805995676242e-06
Iter: 944 loss: 1.8218754058903948e-06
Iter: 945 loss: 1.8208810919703613e-06
Iter: 946 loss: 1.8277974965026492e-06
Iter: 947 loss: 1.8207883603517153e-06
Iter: 948 loss: 1.8202940785980396e-06
Iter: 949 loss: 1.8198683665877726e-06
Iter: 950 loss: 1.8197344290135489e-06
Iter: 951 loss: 1.8188105402133932e-06
Iter: 952 loss: 1.8191804501818244e-06
Iter: 953 loss: 1.8181719291286109e-06
Iter: 954 loss: 1.8172104508736193e-06
Iter: 955 loss: 1.8156835075435163e-06
Iter: 956 loss: 1.8156689786180106e-06
Iter: 957 loss: 1.8144557061408931e-06
Iter: 958 loss: 1.8143833357309161e-06
Iter: 959 loss: 1.8134380897761342e-06
Iter: 960 loss: 1.8118856505963237e-06
Iter: 961 loss: 1.8118782112774867e-06
Iter: 962 loss: 1.8104052866408833e-06
Iter: 963 loss: 1.8318813978753064e-06
Iter: 964 loss: 1.81040328910686e-06
Iter: 965 loss: 1.8092726928516247e-06
Iter: 966 loss: 1.8128510126939624e-06
Iter: 967 loss: 1.8089477420554886e-06
Iter: 968 loss: 1.8079519283240634e-06
Iter: 969 loss: 1.8158177732407427e-06
Iter: 970 loss: 1.8078832563025978e-06
Iter: 971 loss: 1.8070905515907002e-06
Iter: 972 loss: 1.8072910672875731e-06
Iter: 973 loss: 1.8065128277701573e-06
Iter: 974 loss: 1.8052514087195143e-06
Iter: 975 loss: 1.8088328537857988e-06
Iter: 976 loss: 1.8048520441423159e-06
Iter: 977 loss: 1.8041952378802904e-06
Iter: 978 loss: 1.8103215794897662e-06
Iter: 979 loss: 1.8041678333638952e-06
Iter: 980 loss: 1.8032900665126374e-06
Iter: 981 loss: 1.802881629826073e-06
Iter: 982 loss: 1.8024484116655983e-06
Iter: 983 loss: 1.8015772222883512e-06
Iter: 984 loss: 1.8023216536443479e-06
Iter: 985 loss: 1.8010626706159682e-06
Iter: 986 loss: 1.7998765521818772e-06
Iter: 987 loss: 1.801701241589508e-06
Iter: 988 loss: 1.7993166331564817e-06
Iter: 989 loss: 1.7982901474928437e-06
Iter: 990 loss: 1.8021405519793521e-06
Iter: 991 loss: 1.7980417650971887e-06
Iter: 992 loss: 1.7970315322953542e-06
Iter: 993 loss: 1.7955893388169403e-06
Iter: 994 loss: 1.7955406049645324e-06
Iter: 995 loss: 1.7942001913913212e-06
Iter: 996 loss: 1.8075597126593807e-06
Iter: 997 loss: 1.7941565359539805e-06
Iter: 998 loss: 1.7929023848683786e-06
Iter: 999 loss: 1.7954080816438507e-06
Iter: 1000 loss: 1.792389889637722e-06
Iter: 1001 loss: 1.7911145893114004e-06
Iter: 1002 loss: 1.7942146364175568e-06
Iter: 1003 loss: 1.790658189726974e-06
Iter: 1004 loss: 1.7895987829781882e-06
Iter: 1005 loss: 1.7895979400762617e-06
Iter: 1006 loss: 1.7888570582247642e-06
Iter: 1007 loss: 1.7887576458153481e-06
Iter: 1008 loss: 1.7882334469256968e-06
Iter: 1009 loss: 1.787048063017916e-06
Iter: 1010 loss: 1.7906992487224703e-06
Iter: 1011 loss: 1.7866987678427724e-06
Iter: 1012 loss: 1.7858891852266795e-06
Iter: 1013 loss: 1.7958746148772827e-06
Iter: 1014 loss: 1.7858808432742674e-06
Iter: 1015 loss: 1.7850216120589403e-06
Iter: 1016 loss: 1.785364196417846e-06
Iter: 1017 loss: 1.7844273830543498e-06
Iter: 1018 loss: 1.7835136986864085e-06
Iter: 1019 loss: 1.7827888561985131e-06
Iter: 1020 loss: 1.7825092059764254e-06
Iter: 1021 loss: 1.7815421870612107e-06
Iter: 1022 loss: 1.7853041220646993e-06
Iter: 1023 loss: 1.781317544376315e-06
Iter: 1024 loss: 1.7799899669928909e-06
Iter: 1025 loss: 1.780360289912167e-06
Iter: 1026 loss: 1.7790315183896849e-06
Iter: 1027 loss: 1.7778661875770562e-06
Iter: 1028 loss: 1.7838445832322416e-06
Iter: 1029 loss: 1.7776776907137109e-06
Iter: 1030 loss: 1.7766060360429731e-06
Iter: 1031 loss: 1.7770834039293107e-06
Iter: 1032 loss: 1.7758769742984499e-06
Iter: 1033 loss: 1.7748480166252624e-06
Iter: 1034 loss: 1.7769081830914593e-06
Iter: 1035 loss: 1.7744281536074802e-06
Iter: 1036 loss: 1.7731483913653131e-06
Iter: 1037 loss: 1.7807372426937483e-06
Iter: 1038 loss: 1.7729850094654632e-06
Iter: 1039 loss: 1.772130568095153e-06
Iter: 1040 loss: 1.773337691479667e-06
Iter: 1041 loss: 1.7717106631897802e-06
Iter: 1042 loss: 1.7704242312559394e-06
Iter: 1043 loss: 1.7781349370992422e-06
Iter: 1044 loss: 1.770263155135e-06
Iter: 1045 loss: 1.7693889879277507e-06
Iter: 1046 loss: 1.7690771807816339e-06
Iter: 1047 loss: 1.7685865004239066e-06
Iter: 1048 loss: 1.7680364165241429e-06
Iter: 1049 loss: 1.767841517239612e-06
Iter: 1050 loss: 1.7673879116162655e-06
Iter: 1051 loss: 1.7677688070957965e-06
Iter: 1052 loss: 1.7671186255729573e-06
Iter: 1053 loss: 1.7666145349911414e-06
Iter: 1054 loss: 1.7653491387163561e-06
Iter: 1055 loss: 1.7770422379090035e-06
Iter: 1056 loss: 1.7651652979744543e-06
Iter: 1057 loss: 1.7640460284014503e-06
Iter: 1058 loss: 1.7769062060378428e-06
Iter: 1059 loss: 1.7640276484243469e-06
Iter: 1060 loss: 1.7629995864012135e-06
Iter: 1061 loss: 1.7649884787476988e-06
Iter: 1062 loss: 1.7625711881082876e-06
Iter: 1063 loss: 1.7615622890167241e-06
Iter: 1064 loss: 1.7650807209413964e-06
Iter: 1065 loss: 1.7612985820334096e-06
Iter: 1066 loss: 1.7604355241493264e-06
Iter: 1067 loss: 1.7592210848380212e-06
Iter: 1068 loss: 1.7591747873747402e-06
Iter: 1069 loss: 1.7578474114380237e-06
Iter: 1070 loss: 1.7715822175278094e-06
Iter: 1071 loss: 1.75781046644528e-06
Iter: 1072 loss: 1.7566459859308698e-06
Iter: 1073 loss: 1.7590412359345791e-06
Iter: 1074 loss: 1.7561787785086097e-06
Iter: 1075 loss: 1.7549429859342097e-06
Iter: 1076 loss: 1.7563056807860003e-06
Iter: 1077 loss: 1.7542711207932368e-06
Iter: 1078 loss: 1.7532678509475926e-06
Iter: 1079 loss: 1.7532024618730554e-06
Iter: 1080 loss: 1.7527332402585096e-06
Iter: 1081 loss: 1.7522763099525583e-06
Iter: 1082 loss: 1.7521748540343194e-06
Iter: 1083 loss: 1.7513057236358704e-06
Iter: 1084 loss: 1.762215977545884e-06
Iter: 1085 loss: 1.7512979033278915e-06
Iter: 1086 loss: 1.7506979140477696e-06
Iter: 1087 loss: 1.7502685799592869e-06
Iter: 1088 loss: 1.7500599106374842e-06
Iter: 1089 loss: 1.7493738225037134e-06
Iter: 1090 loss: 1.7505557680679057e-06
Iter: 1091 loss: 1.749068338601396e-06
Iter: 1092 loss: 1.7482618307890746e-06
Iter: 1093 loss: 1.7485973103129764e-06
Iter: 1094 loss: 1.7477073895273905e-06
Iter: 1095 loss: 1.7468462730199108e-06
Iter: 1096 loss: 1.7498059959819674e-06
Iter: 1097 loss: 1.7466178376325116e-06
Iter: 1098 loss: 1.745474919443621e-06
Iter: 1099 loss: 1.7476090589268726e-06
Iter: 1100 loss: 1.7449886033034871e-06
Iter: 1101 loss: 1.7441115031137788e-06
Iter: 1102 loss: 1.7468616834687719e-06
Iter: 1103 loss: 1.7438572175328359e-06
Iter: 1104 loss: 1.742864380416772e-06
Iter: 1105 loss: 1.7420937709547741e-06
Iter: 1106 loss: 1.7417809077199991e-06
Iter: 1107 loss: 1.7403626648774312e-06
Iter: 1108 loss: 1.7456971470362979e-06
Iter: 1109 loss: 1.7400205141065696e-06
Iter: 1110 loss: 1.7389080394472699e-06
Iter: 1111 loss: 1.7469685527170022e-06
Iter: 1112 loss: 1.7388132663750969e-06
Iter: 1113 loss: 1.7378596205458843e-06
Iter: 1114 loss: 1.7457674442796813e-06
Iter: 1115 loss: 1.7378020253605607e-06
Iter: 1116 loss: 1.7370763290593112e-06
Iter: 1117 loss: 1.7394510074481167e-06
Iter: 1118 loss: 1.7368742186747968e-06
Iter: 1119 loss: 1.7364417965500789e-06
Iter: 1120 loss: 1.73643828197739e-06
Iter: 1121 loss: 1.7360731657253911e-06
Iter: 1122 loss: 1.7350588494315133e-06
Iter: 1123 loss: 1.74069715400203e-06
Iter: 1124 loss: 1.7347561872220559e-06
Iter: 1125 loss: 1.7340767503880597e-06
Iter: 1126 loss: 1.7340755351740927e-06
Iter: 1127 loss: 1.7334518151659054e-06
Iter: 1128 loss: 1.7321840076234118e-06
Iter: 1129 loss: 1.755299462624364e-06
Iter: 1130 loss: 1.7321631606313291e-06
Iter: 1131 loss: 1.7309556580571321e-06
Iter: 1132 loss: 1.7492866779965078e-06
Iter: 1133 loss: 1.7309551341246965e-06
Iter: 1134 loss: 1.7302552958862736e-06
Iter: 1135 loss: 1.7308305219981831e-06
Iter: 1136 loss: 1.729837355030281e-06
Iter: 1137 loss: 1.7287697415045785e-06
Iter: 1138 loss: 1.7319731651032445e-06
Iter: 1139 loss: 1.7284475582289413e-06
Iter: 1140 loss: 1.7275502398161158e-06
Iter: 1141 loss: 1.7270277819168827e-06
Iter: 1142 loss: 1.7266478228269889e-06
Iter: 1143 loss: 1.72563608691162e-06
Iter: 1144 loss: 1.7364124931089775e-06
Iter: 1145 loss: 1.7256114013377804e-06
Iter: 1146 loss: 1.7247454765254456e-06
Iter: 1147 loss: 1.7245519673511014e-06
Iter: 1148 loss: 1.7239911105074702e-06
Iter: 1149 loss: 1.7234191259450668e-06
Iter: 1150 loss: 1.723350468153133e-06
Iter: 1151 loss: 1.7227542028664816e-06
Iter: 1152 loss: 1.7238421318090602e-06
Iter: 1153 loss: 1.7224971074633711e-06
Iter: 1154 loss: 1.7219762770830626e-06
Iter: 1155 loss: 1.7293574849765011e-06
Iter: 1156 loss: 1.7219750778684655e-06
Iter: 1157 loss: 1.7216818956506444e-06
Iter: 1158 loss: 1.7208637450403518e-06
Iter: 1159 loss: 1.7252823062916483e-06
Iter: 1160 loss: 1.720611957250774e-06
Iter: 1161 loss: 1.7192092734031543e-06
Iter: 1162 loss: 1.7209511228109824e-06
Iter: 1163 loss: 1.7184807324186555e-06
Iter: 1164 loss: 1.7175304736429524e-06
Iter: 1165 loss: 1.7175290007917232e-06
Iter: 1166 loss: 1.7168038610319397e-06
Iter: 1167 loss: 1.7160590684057704e-06
Iter: 1168 loss: 1.7159198909355414e-06
Iter: 1169 loss: 1.7148841279103504e-06
Iter: 1170 loss: 1.7221601589233619e-06
Iter: 1171 loss: 1.7147895308272141e-06
Iter: 1172 loss: 1.7138765537582931e-06
Iter: 1173 loss: 1.716421630670433e-06
Iter: 1174 loss: 1.7135830622634893e-06
Iter: 1175 loss: 1.712712539636245e-06
Iter: 1176 loss: 1.7145988671295544e-06
Iter: 1177 loss: 1.7123749084239749e-06
Iter: 1178 loss: 1.711372538143489e-06
Iter: 1179 loss: 1.7110801982186556e-06
Iter: 1180 loss: 1.7104757405831742e-06
Iter: 1181 loss: 1.7093444957498156e-06
Iter: 1182 loss: 1.7123260759897376e-06
Iter: 1183 loss: 1.7089638261447219e-06
Iter: 1184 loss: 1.7083898599347269e-06
Iter: 1185 loss: 1.7082969631557472e-06
Iter: 1186 loss: 1.7077706311954366e-06
Iter: 1187 loss: 1.7103344594268696e-06
Iter: 1188 loss: 1.7076787584992637e-06
Iter: 1189 loss: 1.7071290374416662e-06
Iter: 1190 loss: 1.7077636776906161e-06
Iter: 1191 loss: 1.7068352520262719e-06
Iter: 1192 loss: 1.7061179164610214e-06
Iter: 1193 loss: 1.705380450193078e-06
Iter: 1194 loss: 1.705243091884819e-06
Iter: 1195 loss: 1.704497981427248e-06
Iter: 1196 loss: 1.7051912685338002e-06
Iter: 1197 loss: 1.7040690016077326e-06
Iter: 1198 loss: 1.7030926508110807e-06
Iter: 1199 loss: 1.7086819491271942e-06
Iter: 1200 loss: 1.7029601357099754e-06
Iter: 1201 loss: 1.7023294760755651e-06
Iter: 1202 loss: 1.7053838125216232e-06
Iter: 1203 loss: 1.7022185104285682e-06
Iter: 1204 loss: 1.7014988664269025e-06
Iter: 1205 loss: 1.7007015074180838e-06
Iter: 1206 loss: 1.700588490040571e-06
Iter: 1207 loss: 1.6997700747134185e-06
Iter: 1208 loss: 1.7081066790453203e-06
Iter: 1209 loss: 1.6997457058313733e-06
Iter: 1210 loss: 1.6988262708411659e-06
Iter: 1211 loss: 1.6985216904594383e-06
Iter: 1212 loss: 1.6979907418279902e-06
Iter: 1213 loss: 1.6967473000137866e-06
Iter: 1214 loss: 1.6987639996629694e-06
Iter: 1215 loss: 1.6961758110936886e-06
Iter: 1216 loss: 1.6952224939690864e-06
Iter: 1217 loss: 1.7042285783471216e-06
Iter: 1218 loss: 1.6951844364261166e-06
Iter: 1219 loss: 1.6944148410201464e-06
Iter: 1220 loss: 1.6953507446892364e-06
Iter: 1221 loss: 1.6940116829215736e-06
Iter: 1222 loss: 1.6929141516642277e-06
Iter: 1223 loss: 1.70474805661608e-06
Iter: 1224 loss: 1.6928889552229994e-06
Iter: 1225 loss: 1.6924665274678086e-06
Iter: 1226 loss: 1.6946804947621317e-06
Iter: 1227 loss: 1.6924003825102073e-06
Iter: 1228 loss: 1.6920854534677749e-06
Iter: 1229 loss: 1.6912105116070174e-06
Iter: 1230 loss: 1.6960705475662133e-06
Iter: 1231 loss: 1.6909493002295524e-06
Iter: 1232 loss: 1.6898776980442042e-06
Iter: 1233 loss: 1.696865578558224e-06
Iter: 1234 loss: 1.6897632933691351e-06
Iter: 1235 loss: 1.688867131913317e-06
Iter: 1236 loss: 1.6915941941588142e-06
Iter: 1237 loss: 1.688600123423584e-06
Iter: 1238 loss: 1.6878601766301962e-06
Iter: 1239 loss: 1.6931487820713615e-06
Iter: 1240 loss: 1.6877951568241742e-06
Iter: 1241 loss: 1.6871103633105592e-06
Iter: 1242 loss: 1.6864063012223658e-06
Iter: 1243 loss: 1.6862751886662783e-06
Iter: 1244 loss: 1.6852980236808812e-06
Iter: 1245 loss: 1.6943706648368115e-06
Iter: 1246 loss: 1.6852565579019101e-06
Iter: 1247 loss: 1.6844987238791366e-06
Iter: 1248 loss: 1.6858055691743293e-06
Iter: 1249 loss: 1.6841614853041812e-06
Iter: 1250 loss: 1.6833189392297397e-06
Iter: 1251 loss: 1.6863266104868176e-06
Iter: 1252 loss: 1.6831039539026918e-06
Iter: 1253 loss: 1.6823590354749393e-06
Iter: 1254 loss: 1.6812506824105697e-06
Iter: 1255 loss: 1.6812254525673161e-06
Iter: 1256 loss: 1.6811902602707023e-06
Iter: 1257 loss: 1.6807256708986143e-06
Iter: 1258 loss: 1.6801327896537911e-06
Iter: 1259 loss: 1.6803534728747439e-06
Iter: 1260 loss: 1.6797187678134462e-06
Iter: 1261 loss: 1.679015746834465e-06
Iter: 1262 loss: 1.6804115596215872e-06
Iter: 1263 loss: 1.6787274028643878e-06
Iter: 1264 loss: 1.678033441991903e-06
Iter: 1265 loss: 1.6793329436957053e-06
Iter: 1266 loss: 1.6777386322139444e-06
Iter: 1267 loss: 1.6771229838695937e-06
Iter: 1268 loss: 1.6760587449222787e-06
Iter: 1269 loss: 1.6760580825013617e-06
Iter: 1270 loss: 1.6750395239379653e-06
Iter: 1271 loss: 1.6834867225064503e-06
Iter: 1272 loss: 1.6749780214701946e-06
Iter: 1273 loss: 1.6739976259733325e-06
Iter: 1274 loss: 1.677831702804e-06
Iter: 1275 loss: 1.6737712208170244e-06
Iter: 1276 loss: 1.67283356934288e-06
Iter: 1277 loss: 1.6741943287305813e-06
Iter: 1278 loss: 1.6723784587050339e-06
Iter: 1279 loss: 1.6713829283680592e-06
Iter: 1280 loss: 1.6749280017071707e-06
Iter: 1281 loss: 1.6711282984044116e-06
Iter: 1282 loss: 1.6703736177994094e-06
Iter: 1283 loss: 1.6726401011063231e-06
Iter: 1284 loss: 1.6701460933948788e-06
Iter: 1285 loss: 1.6691501292994629e-06
Iter: 1286 loss: 1.67010502633464e-06
Iter: 1287 loss: 1.6685821568165322e-06
Iter: 1288 loss: 1.6678271648821446e-06
Iter: 1289 loss: 1.6711779708748755e-06
Iter: 1290 loss: 1.6676776667504282e-06
Iter: 1291 loss: 1.6670058173103462e-06
Iter: 1292 loss: 1.6709502513556012e-06
Iter: 1293 loss: 1.6669184930907258e-06
Iter: 1294 loss: 1.6661786151371743e-06
Iter: 1295 loss: 1.6710363808833118e-06
Iter: 1296 loss: 1.6661007464613689e-06
Iter: 1297 loss: 1.6657450963589468e-06
Iter: 1298 loss: 1.6650733144486381e-06
Iter: 1299 loss: 1.6797355656129795e-06
Iter: 1300 loss: 1.6650712094654311e-06
Iter: 1301 loss: 1.6640543845855124e-06
Iter: 1302 loss: 1.6689586420543115e-06
Iter: 1303 loss: 1.6638744509263238e-06
Iter: 1304 loss: 1.6634263495575897e-06
Iter: 1305 loss: 1.6629517235123877e-06
Iter: 1306 loss: 1.6628720636625176e-06
Iter: 1307 loss: 1.6618437420152068e-06
Iter: 1308 loss: 1.6630417179732771e-06
Iter: 1309 loss: 1.6612959912113814e-06
Iter: 1310 loss: 1.6603296940555776e-06
Iter: 1311 loss: 1.6639622769608971e-06
Iter: 1312 loss: 1.6600964390017689e-06
Iter: 1313 loss: 1.6589732409244755e-06
Iter: 1314 loss: 1.6645586270931961e-06
Iter: 1315 loss: 1.6587829857949405e-06
Iter: 1316 loss: 1.657895052424777e-06
Iter: 1317 loss: 1.6574626305107026e-06
Iter: 1318 loss: 1.6570362571884065e-06
Iter: 1319 loss: 1.6562927015388834e-06
Iter: 1320 loss: 1.6562810632438878e-06
Iter: 1321 loss: 1.6557442460909238e-06
Iter: 1322 loss: 1.6555252248981506e-06
Iter: 1323 loss: 1.6552413060921103e-06
Iter: 1324 loss: 1.6542867281349143e-06
Iter: 1325 loss: 1.6570526030518169e-06
Iter: 1326 loss: 1.6539897295915099e-06
Iter: 1327 loss: 1.6535973196234546e-06
Iter: 1328 loss: 1.6535687066346681e-06
Iter: 1329 loss: 1.6530446455034058e-06
Iter: 1330 loss: 1.6521981096777279e-06
Iter: 1331 loss: 1.6521922322387492e-06
Iter: 1332 loss: 1.6515592186674949e-06
Iter: 1333 loss: 1.6548095361593814e-06
Iter: 1334 loss: 1.6514569655783366e-06
Iter: 1335 loss: 1.6508749259162458e-06
Iter: 1336 loss: 1.652510417457826e-06
Iter: 1337 loss: 1.650689050471826e-06
Iter: 1338 loss: 1.6500865781997419e-06
Iter: 1339 loss: 1.6495635425347e-06
Iter: 1340 loss: 1.64940237998961e-06
Iter: 1341 loss: 1.6485383034651512e-06
Iter: 1342 loss: 1.6502259014883494e-06
Iter: 1343 loss: 1.6481803054271744e-06
Iter: 1344 loss: 1.6473710664025193e-06
Iter: 1345 loss: 1.6522630845227279e-06
Iter: 1346 loss: 1.6472712770985089e-06
Iter: 1347 loss: 1.6464564428296713e-06
Iter: 1348 loss: 1.6472552360186179e-06
Iter: 1349 loss: 1.6459950861231397e-06
Iter: 1350 loss: 1.6451955547537836e-06
Iter: 1351 loss: 1.6555984410222918e-06
Iter: 1352 loss: 1.6451903044445372e-06
Iter: 1353 loss: 1.6447303115963193e-06
Iter: 1354 loss: 1.6438769030038885e-06
Iter: 1355 loss: 1.6632907030389035e-06
Iter: 1356 loss: 1.6438756001105779e-06
Iter: 1357 loss: 1.6431618778830554e-06
Iter: 1358 loss: 1.6431579539840643e-06
Iter: 1359 loss: 1.6424595024978561e-06
Iter: 1360 loss: 1.6415910114072433e-06
Iter: 1361 loss: 1.6415177866732438e-06
Iter: 1362 loss: 1.6418265932329431e-06
Iter: 1363 loss: 1.6410806076182247e-06
Iter: 1364 loss: 1.6407471443388629e-06
Iter: 1365 loss: 1.6403874185860078e-06
Iter: 1366 loss: 1.6403309471441025e-06
Iter: 1367 loss: 1.6397696290910747e-06
Iter: 1368 loss: 1.6386426095708642e-06
Iter: 1369 loss: 1.6598482653989316e-06
Iter: 1370 loss: 1.6386275169627575e-06
Iter: 1371 loss: 1.6379738715712971e-06
Iter: 1372 loss: 1.637815474768977e-06
Iter: 1373 loss: 1.6374045802688981e-06
Iter: 1374 loss: 1.6366512116661525e-06
Iter: 1375 loss: 1.6542495275778971e-06
Iter: 1376 loss: 1.6366506157903178e-06
Iter: 1377 loss: 1.6357806819692331e-06
Iter: 1378 loss: 1.6380296205886897e-06
Iter: 1379 loss: 1.6354835466354522e-06
Iter: 1380 loss: 1.6344702431511915e-06
Iter: 1381 loss: 1.6360276038695225e-06
Iter: 1382 loss: 1.633991668908291e-06
Iter: 1383 loss: 1.6332392468298227e-06
Iter: 1384 loss: 1.6414694273107667e-06
Iter: 1385 loss: 1.6332231155539261e-06
Iter: 1386 loss: 1.6325187622854197e-06
Iter: 1387 loss: 1.633829941286043e-06
Iter: 1388 loss: 1.6322185258525723e-06
Iter: 1389 loss: 1.6315775195088703e-06
Iter: 1390 loss: 1.6322727387217579e-06
Iter: 1391 loss: 1.6312269364904778e-06
Iter: 1392 loss: 1.6302708170164729e-06
Iter: 1393 loss: 1.6315119113246011e-06
Iter: 1394 loss: 1.6297832313443946e-06
Iter: 1395 loss: 1.6290296786973909e-06
Iter: 1396 loss: 1.6384289118218733e-06
Iter: 1397 loss: 1.6290225614360585e-06
Iter: 1398 loss: 1.628428771101393e-06
Iter: 1399 loss: 1.6333827439517445e-06
Iter: 1400 loss: 1.6283935405699742e-06
Iter: 1401 loss: 1.6277912989360427e-06
Iter: 1402 loss: 1.6270756820438705e-06
Iter: 1403 loss: 1.6270002932943547e-06
Iter: 1404 loss: 1.6264211499806661e-06
Iter: 1405 loss: 1.6266584397862221e-06
Iter: 1406 loss: 1.6260221316548396e-06
Iter: 1407 loss: 1.6252753253422224e-06
Iter: 1408 loss: 1.632435701238655e-06
Iter: 1409 loss: 1.6252470801791769e-06
Iter: 1410 loss: 1.6246619771821937e-06
Iter: 1411 loss: 1.6250186925147726e-06
Iter: 1412 loss: 1.6242861850378429e-06
Iter: 1413 loss: 1.6235290654404598e-06
Iter: 1414 loss: 1.6226708545216657e-06
Iter: 1415 loss: 1.6225598404523865e-06
Iter: 1416 loss: 1.6214238274656267e-06
Iter: 1417 loss: 1.6256626960844197e-06
Iter: 1418 loss: 1.6211473898275676e-06
Iter: 1419 loss: 1.6202863635826576e-06
Iter: 1420 loss: 1.6284834363423184e-06
Iter: 1421 loss: 1.620252941443577e-06
Iter: 1422 loss: 1.6195104006022351e-06
Iter: 1423 loss: 1.6205062306275535e-06
Iter: 1424 loss: 1.619136967307289e-06
Iter: 1425 loss: 1.6180270015066947e-06
Iter: 1426 loss: 1.6206826654149302e-06
Iter: 1427 loss: 1.6176251638821606e-06
Iter: 1428 loss: 1.6169652164549087e-06
Iter: 1429 loss: 1.6168802566983559e-06
Iter: 1430 loss: 1.6164108706801776e-06
Iter: 1431 loss: 1.6156190383869882e-06
Iter: 1432 loss: 1.6156107039891223e-06
Iter: 1433 loss: 1.6151361147900117e-06
Iter: 1434 loss: 1.62109871653015e-06
Iter: 1435 loss: 1.615131881002519e-06
Iter: 1436 loss: 1.6148632806187427e-06
Iter: 1437 loss: 1.6142103906074822e-06
Iter: 1438 loss: 1.6210616403698758e-06
Iter: 1439 loss: 1.6141365052728527e-06
Iter: 1440 loss: 1.6133002402215591e-06
Iter: 1441 loss: 1.6159669992683565e-06
Iter: 1442 loss: 1.6130616017551739e-06
Iter: 1443 loss: 1.6123662518126272e-06
Iter: 1444 loss: 1.6163404142260813e-06
Iter: 1445 loss: 1.6122716314391496e-06
Iter: 1446 loss: 1.6114978764305895e-06
Iter: 1447 loss: 1.6121810535896518e-06
Iter: 1448 loss: 1.6110452194249919e-06
Iter: 1449 loss: 1.6103975640635769e-06
Iter: 1450 loss: 1.6100333476183402e-06
Iter: 1451 loss: 1.6097515602277264e-06
Iter: 1452 loss: 1.608808688828233e-06
Iter: 1453 loss: 1.614225218328279e-06
Iter: 1454 loss: 1.6086814896031972e-06
Iter: 1455 loss: 1.6077689064067471e-06
Iter: 1456 loss: 1.6078476670083103e-06
Iter: 1457 loss: 1.6070619855882096e-06
Iter: 1458 loss: 1.6065086078223057e-06
Iter: 1459 loss: 1.6064804206619246e-06
Iter: 1460 loss: 1.6058818115414509e-06
Iter: 1461 loss: 1.6052321206681904e-06
Iter: 1462 loss: 1.6051323938143404e-06
Iter: 1463 loss: 1.6042763304435043e-06
Iter: 1464 loss: 1.6090579634309584e-06
Iter: 1465 loss: 1.6041552895561328e-06
Iter: 1466 loss: 1.6035723166218718e-06
Iter: 1467 loss: 1.6121938700334578e-06
Iter: 1468 loss: 1.6035717444817851e-06
Iter: 1469 loss: 1.6029475874188975e-06
Iter: 1470 loss: 1.603438581509923e-06
Iter: 1471 loss: 1.6025703902737903e-06
Iter: 1472 loss: 1.6021346733886176e-06
Iter: 1473 loss: 1.6024853143508237e-06
Iter: 1474 loss: 1.6018729330861854e-06
Iter: 1475 loss: 1.6012365119413691e-06
Iter: 1476 loss: 1.6005524927249802e-06
Iter: 1477 loss: 1.600443632240154e-06
Iter: 1478 loss: 1.6000208935345051e-06
Iter: 1479 loss: 1.5999345739714276e-06
Iter: 1480 loss: 1.599518475873204e-06
Iter: 1481 loss: 1.599123482455396e-06
Iter: 1482 loss: 1.599028687484361e-06
Iter: 1483 loss: 1.5982586570094701e-06
Iter: 1484 loss: 1.5985417476754338e-06
Iter: 1485 loss: 1.5977200982962202e-06
Iter: 1486 loss: 1.5966933197288146e-06
Iter: 1487 loss: 1.5984367458107139e-06
Iter: 1488 loss: 1.59623263825442e-06
Iter: 1489 loss: 1.595440902234374e-06
Iter: 1490 loss: 1.6002288474725818e-06
Iter: 1491 loss: 1.5953433291132845e-06
Iter: 1492 loss: 1.5944471944694483e-06
Iter: 1493 loss: 1.5946454930846782e-06
Iter: 1494 loss: 1.5937864907178858e-06
Iter: 1495 loss: 1.59304724641485e-06
Iter: 1496 loss: 1.5930404346881948e-06
Iter: 1497 loss: 1.592420714730514e-06
Iter: 1498 loss: 1.5918517315825032e-06
Iter: 1499 loss: 1.591701261474245e-06
Iter: 1500 loss: 1.5919863141108354e-06
Iter: 1501 loss: 1.5914081543216568e-06
Iter: 1502 loss: 1.5911525876935082e-06
Iter: 1503 loss: 1.5905992528751886e-06
Iter: 1504 loss: 1.5990902456684523e-06
Iter: 1505 loss: 1.59057810113014e-06
Iter: 1506 loss: 1.5899156060460484e-06
Iter: 1507 loss: 1.5901578904966209e-06
Iter: 1508 loss: 1.5894519688129263e-06
Iter: 1509 loss: 1.5886461519618137e-06
Iter: 1510 loss: 1.593536212135907e-06
Iter: 1511 loss: 1.5885474824246449e-06
Iter: 1512 loss: 1.5879137538087914e-06
Iter: 1513 loss: 1.5898422969435953e-06
Iter: 1514 loss: 1.587724937664379e-06
Iter: 1515 loss: 1.5871930525596061e-06
Iter: 1516 loss: 1.5910917717811228e-06
Iter: 1517 loss: 1.5871489454432587e-06
Iter: 1518 loss: 1.5867103807795671e-06
Iter: 1519 loss: 1.5857854604237178e-06
Iter: 1520 loss: 1.6010600452932532e-06
Iter: 1521 loss: 1.5857596944645834e-06
Iter: 1522 loss: 1.5849231890919723e-06
Iter: 1523 loss: 1.5903772333107655e-06
Iter: 1524 loss: 1.5848338502017372e-06
Iter: 1525 loss: 1.5839395111944733e-06
Iter: 1526 loss: 1.5842431103610217e-06
Iter: 1527 loss: 1.5833076927853383e-06
Iter: 1528 loss: 1.5822482746121868e-06
Iter: 1529 loss: 1.5847954260134379e-06
Iter: 1530 loss: 1.581866105112349e-06
Iter: 1531 loss: 1.5808823404382979e-06
Iter: 1532 loss: 1.5956450649892985e-06
Iter: 1533 loss: 1.5808816998507029e-06
Iter: 1534 loss: 1.5803580060343542e-06
Iter: 1535 loss: 1.5814499402796345e-06
Iter: 1536 loss: 1.5801497391258918e-06
Iter: 1537 loss: 1.5795189079714938e-06
Iter: 1538 loss: 1.5848835662527002e-06
Iter: 1539 loss: 1.5794834423209083e-06
Iter: 1540 loss: 1.5788411005812154e-06
Iter: 1541 loss: 1.5779827231605293e-06
Iter: 1542 loss: 1.5779351590297143e-06
Iter: 1543 loss: 1.5774103557892557e-06
Iter: 1544 loss: 1.5774110053013762e-06
Iter: 1545 loss: 1.57699070371644e-06
Iter: 1546 loss: 1.5761819359417338e-06
Iter: 1547 loss: 1.5824791539260139e-06
Iter: 1548 loss: 1.576124080579185e-06
Iter: 1549 loss: 1.5755268948984543e-06
Iter: 1550 loss: 1.5763189096703969e-06
Iter: 1551 loss: 1.5752251254977498e-06
Iter: 1552 loss: 1.5744966375699192e-06
Iter: 1553 loss: 1.5786759371554891e-06
Iter: 1554 loss: 1.5743981335535615e-06
Iter: 1555 loss: 1.5739284738239176e-06
Iter: 1556 loss: 1.5731458322633081e-06
Iter: 1557 loss: 1.5731433026005146e-06
Iter: 1558 loss: 1.572059288417077e-06
Iter: 1559 loss: 1.5776993144625789e-06
Iter: 1560 loss: 1.5718876836797698e-06
Iter: 1561 loss: 1.5712148394616044e-06
Iter: 1562 loss: 1.571436098191314e-06
Iter: 1563 loss: 1.5707377041365466e-06
Iter: 1564 loss: 1.5698603691649517e-06
Iter: 1565 loss: 1.5753546167335853e-06
Iter: 1566 loss: 1.569759039347038e-06
Iter: 1567 loss: 1.5688702731828816e-06
Iter: 1568 loss: 1.5703848416805139e-06
Iter: 1569 loss: 1.5684723136920146e-06
Iter: 1570 loss: 1.5676311365201387e-06
Iter: 1571 loss: 1.580162262245974e-06
Iter: 1572 loss: 1.5676304500470628e-06
Iter: 1573 loss: 1.567103423161306e-06
Iter: 1574 loss: 1.5722994332140788e-06
Iter: 1575 loss: 1.5670855110731712e-06
Iter: 1576 loss: 1.5667828488324972e-06
Iter: 1577 loss: 1.5660249304332382e-06
Iter: 1578 loss: 1.5731022676798999e-06
Iter: 1579 loss: 1.5659168439220522e-06
Iter: 1580 loss: 1.565219211789588e-06
Iter: 1581 loss: 1.5692149500218267e-06
Iter: 1582 loss: 1.5651246295202666e-06
Iter: 1583 loss: 1.5644528988368533e-06
Iter: 1584 loss: 1.5659051873957835e-06
Iter: 1585 loss: 1.5641919694411477e-06
Iter: 1586 loss: 1.563513103329518e-06
Iter: 1587 loss: 1.5690515194070001e-06
Iter: 1588 loss: 1.5634701733797978e-06
Iter: 1589 loss: 1.563021592081422e-06
Iter: 1590 loss: 1.5634943983379608e-06
Iter: 1591 loss: 1.5627737578362905e-06
Iter: 1592 loss: 1.5621392968686574e-06
Iter: 1593 loss: 1.5628135690307777e-06
Iter: 1594 loss: 1.5617897833510265e-06
Iter: 1595 loss: 1.5611683233765916e-06
Iter: 1596 loss: 1.5604391895689733e-06
Iter: 1597 loss: 1.5603578489280054e-06
Iter: 1598 loss: 1.5595700058744067e-06
Iter: 1599 loss: 1.559565586562677e-06
Iter: 1600 loss: 1.5589719896119741e-06
Iter: 1601 loss: 1.5578388568425785e-06
Iter: 1602 loss: 1.5819587614198743e-06
Iter: 1603 loss: 1.5578339107886969e-06
Iter: 1604 loss: 1.5576078704689976e-06
Iter: 1605 loss: 1.5572985213827872e-06
Iter: 1606 loss: 1.5568800716914829e-06
Iter: 1607 loss: 1.5603041948374116e-06
Iter: 1608 loss: 1.5568538469726646e-06
Iter: 1609 loss: 1.5564502278612921e-06
Iter: 1610 loss: 1.556423638072434e-06
Iter: 1611 loss: 1.5561192040939044e-06
Iter: 1612 loss: 1.5556554609832869e-06
Iter: 1613 loss: 1.5552494149425343e-06
Iter: 1614 loss: 1.555127113747461e-06
Iter: 1615 loss: 1.5544033431321387e-06
Iter: 1616 loss: 1.5563929396105637e-06
Iter: 1617 loss: 1.5541679317684479e-06
Iter: 1618 loss: 1.5536308163607073e-06
Iter: 1619 loss: 1.557942163091342e-06
Iter: 1620 loss: 1.5535953296238632e-06
Iter: 1621 loss: 1.5530086603888045e-06
Iter: 1622 loss: 1.55344940450343e-06
Iter: 1623 loss: 1.5526497632309337e-06
Iter: 1624 loss: 1.5520141013013065e-06
Iter: 1625 loss: 1.5547649827706727e-06
Iter: 1626 loss: 1.551884109474652e-06
Iter: 1627 loss: 1.5512298684332148e-06
Iter: 1628 loss: 1.5510126370282314e-06
Iter: 1629 loss: 1.5506351821535692e-06
Iter: 1630 loss: 1.5499452575620821e-06
Iter: 1631 loss: 1.5542633738059026e-06
Iter: 1632 loss: 1.5498654919972539e-06
Iter: 1633 loss: 1.5491839092897e-06
Iter: 1634 loss: 1.5480762252138386e-06
Iter: 1635 loss: 1.5480694582231148e-06
Iter: 1636 loss: 1.5473585016863827e-06
Iter: 1637 loss: 1.5473563750060491e-06
Iter: 1638 loss: 1.5467227926935921e-06
Iter: 1639 loss: 1.5492402955037302e-06
Iter: 1640 loss: 1.5465791723478015e-06
Iter: 1641 loss: 1.5460168510384787e-06
Iter: 1642 loss: 1.5550060061382429e-06
Iter: 1643 loss: 1.5460168509341838e-06
Iter: 1644 loss: 1.5457465998002844e-06
Iter: 1645 loss: 1.5453268539924572e-06
Iter: 1646 loss: 1.5453212137776629e-06
Iter: 1647 loss: 1.544680640632884e-06
Iter: 1648 loss: 1.5450464771548778e-06
Iter: 1649 loss: 1.5442636534494538e-06
Iter: 1650 loss: 1.5436372046630203e-06
Iter: 1651 loss: 1.5442268123727155e-06
Iter: 1652 loss: 1.5432778246538723e-06
Iter: 1653 loss: 1.5425914547719975e-06
Iter: 1654 loss: 1.5489195457952917e-06
Iter: 1655 loss: 1.5425616245460965e-06
Iter: 1656 loss: 1.5419746714600206e-06
Iter: 1657 loss: 1.5440003389169351e-06
Iter: 1658 loss: 1.5418196282682122e-06
Iter: 1659 loss: 1.5412447117587131e-06
Iter: 1660 loss: 1.5411577204644376e-06
Iter: 1661 loss: 1.5407576126839398e-06
Iter: 1662 loss: 1.5401566145227744e-06
Iter: 1663 loss: 1.546282434042643e-06
Iter: 1664 loss: 1.5401387645953226e-06
Iter: 1665 loss: 1.539665618553278e-06
Iter: 1666 loss: 1.538737333650041e-06
Iter: 1667 loss: 1.5572346266777745e-06
Iter: 1668 loss: 1.5387293577548007e-06
Iter: 1669 loss: 1.5378758715109955e-06
Iter: 1670 loss: 1.5469068070195309e-06
Iter: 1671 loss: 1.5378543773518335e-06
Iter: 1672 loss: 1.5371021666071779e-06
Iter: 1673 loss: 1.5371734833348526e-06
Iter: 1674 loss: 1.5365213512062704e-06
Iter: 1675 loss: 1.5363166591613034e-06
Iter: 1676 loss: 1.5361140282789663e-06
Iter: 1677 loss: 1.5356315704788715e-06
Iter: 1678 loss: 1.5353224804431411e-06
Iter: 1679 loss: 1.5351344805136315e-06
Iter: 1680 loss: 1.5346898588986221e-06
Iter: 1681 loss: 1.5363059244881264e-06
Iter: 1682 loss: 1.5345785494129102e-06
Iter: 1683 loss: 1.5341492964709477e-06
Iter: 1684 loss: 1.5337083036971388e-06
Iter: 1685 loss: 1.533625970777889e-06
Iter: 1686 loss: 1.5328225650785305e-06
Iter: 1687 loss: 1.5356592917294993e-06
Iter: 1688 loss: 1.5326152549696948e-06
Iter: 1689 loss: 1.5318473883358924e-06
Iter: 1690 loss: 1.5331276811680845e-06
Iter: 1691 loss: 1.5314995070092762e-06
Iter: 1692 loss: 1.5308581958613644e-06
Iter: 1693 loss: 1.5308535776616751e-06
Iter: 1694 loss: 1.5305536767512093e-06
Iter: 1695 loss: 1.5299019243787603e-06
Iter: 1696 loss: 1.5397918926546594e-06
Iter: 1697 loss: 1.5298759314864218e-06
Iter: 1698 loss: 1.5291191695334136e-06
Iter: 1699 loss: 1.5380187699004394e-06
Iter: 1700 loss: 1.5291083835523365e-06
Iter: 1701 loss: 1.5286722657365504e-06
Iter: 1702 loss: 1.5280945546527555e-06
Iter: 1703 loss: 1.5280606737787297e-06
Iter: 1704 loss: 1.5271148811632434e-06
Iter: 1705 loss: 1.5309468259983109e-06
Iter: 1706 loss: 1.5269052777624683e-06
Iter: 1707 loss: 1.5260972363598241e-06
Iter: 1708 loss: 1.525604770384572e-06
Iter: 1709 loss: 1.5252752989258197e-06
Iter: 1710 loss: 1.5268181134761523e-06
Iter: 1711 loss: 1.5249205297116621e-06
Iter: 1712 loss: 1.5247024160024804e-06
Iter: 1713 loss: 1.5242345135941456e-06
Iter: 1714 loss: 1.5316040757374914e-06
Iter: 1715 loss: 1.5242184375006825e-06
Iter: 1716 loss: 1.523685303606377e-06
Iter: 1717 loss: 1.5240742604919565e-06
Iter: 1718 loss: 1.5233567348698578e-06
Iter: 1719 loss: 1.5226512867559963e-06
Iter: 1720 loss: 1.5256546555973416e-06
Iter: 1721 loss: 1.5225040383234883e-06
Iter: 1722 loss: 1.5219009794635033e-06
Iter: 1723 loss: 1.5223503360569904e-06
Iter: 1724 loss: 1.5215312935093517e-06
Iter: 1725 loss: 1.5209721042266374e-06
Iter: 1726 loss: 1.5247211977100517e-06
Iter: 1727 loss: 1.5209156807209608e-06
Iter: 1728 loss: 1.5202475697035883e-06
Iter: 1729 loss: 1.5207971890152455e-06
Iter: 1730 loss: 1.5198486627222885e-06
Iter: 1731 loss: 1.5191636297433183e-06
Iter: 1732 loss: 1.5218682830987431e-06
Iter: 1733 loss: 1.5190071578788227e-06
Iter: 1734 loss: 1.5185283893102778e-06
Iter: 1735 loss: 1.518720776807565e-06
Iter: 1736 loss: 1.5181976102412735e-06
Iter: 1737 loss: 1.5174571930182103e-06
Iter: 1738 loss: 1.5207469264245277e-06
Iter: 1739 loss: 1.5173108008772967e-06
Iter: 1740 loss: 1.5167705874978019e-06
Iter: 1741 loss: 1.5163382863828161e-06
Iter: 1742 loss: 1.5161749138576708e-06
Iter: 1743 loss: 1.515469205740074e-06
Iter: 1744 loss: 1.5233483307572126e-06
Iter: 1745 loss: 1.5154556286347908e-06
Iter: 1746 loss: 1.5152176249919934e-06
Iter: 1747 loss: 1.515158228056351e-06
Iter: 1748 loss: 1.5148728302112219e-06
Iter: 1749 loss: 1.5141189263102407e-06
Iter: 1750 loss: 1.5196526867769161e-06
Iter: 1751 loss: 1.5139601829273687e-06
Iter: 1752 loss: 1.5133128953863447e-06
Iter: 1753 loss: 1.5146437992432882e-06
Iter: 1754 loss: 1.5130531390342967e-06
Iter: 1755 loss: 1.5123011138508823e-06
Iter: 1756 loss: 1.5180688058819662e-06
Iter: 1757 loss: 1.5122452153624982e-06
Iter: 1758 loss: 1.5117014809528741e-06
Iter: 1759 loss: 1.5117076541062729e-06
Iter: 1760 loss: 1.5112683557614114e-06
Iter: 1761 loss: 1.51053621729599e-06
Iter: 1762 loss: 1.5144972261914859e-06
Iter: 1763 loss: 1.5104271482086145e-06
Iter: 1764 loss: 1.5099682403340947e-06
Iter: 1765 loss: 1.5156647736563643e-06
Iter: 1766 loss: 1.5099637444725221e-06
Iter: 1767 loss: 1.5096109137822116e-06
Iter: 1768 loss: 1.5088359594651576e-06
Iter: 1769 loss: 1.5202317655926715e-06
Iter: 1770 loss: 1.5088012441327321e-06
Iter: 1771 loss: 1.5080814300165173e-06
Iter: 1772 loss: 1.5150209394485945e-06
Iter: 1773 loss: 1.5080547565789566e-06
Iter: 1774 loss: 1.507298119840496e-06
Iter: 1775 loss: 1.506908822362129e-06
Iter: 1776 loss: 1.5065580437698596e-06
Iter: 1777 loss: 1.505771682462048e-06
Iter: 1778 loss: 1.5177778331172235e-06
Iter: 1779 loss: 1.5057714103712705e-06
Iter: 1780 loss: 1.5053751811056043e-06
Iter: 1781 loss: 1.5049324862886215e-06
Iter: 1782 loss: 1.5048717603383118e-06
Iter: 1783 loss: 1.5045373756478065e-06
Iter: 1784 loss: 1.5043902470430033e-06
Iter: 1785 loss: 1.5041741122162404e-06
Iter: 1786 loss: 1.5036344760898755e-06
Iter: 1787 loss: 1.5087346212569165e-06
Iter: 1788 loss: 1.5035592283282689e-06
Iter: 1789 loss: 1.5029951733119786e-06
Iter: 1790 loss: 1.5049559507906715e-06
Iter: 1791 loss: 1.5028472629548331e-06
Iter: 1792 loss: 1.5023178836826075e-06
Iter: 1793 loss: 1.5027686382594038e-06
Iter: 1794 loss: 1.5020048771500813e-06
Iter: 1795 loss: 1.5012777476853045e-06
Iter: 1796 loss: 1.50700902267423e-06
Iter: 1797 loss: 1.5012273298719713e-06
Iter: 1798 loss: 1.5007943925768083e-06
Iter: 1799 loss: 1.50055177234384e-06
Iter: 1800 loss: 1.5003629083020305e-06
Iter: 1801 loss: 1.4998654518600162e-06
Iter: 1802 loss: 1.4998571539396494e-06
Iter: 1803 loss: 1.4995363021990752e-06
Iter: 1804 loss: 1.498855447757231e-06
Iter: 1805 loss: 1.5099138525308317e-06
Iter: 1806 loss: 1.4988349510407914e-06
Iter: 1807 loss: 1.497954720705902e-06
Iter: 1808 loss: 1.5025439703861171e-06
Iter: 1809 loss: 1.497815799020831e-06
Iter: 1810 loss: 1.4972739230928712e-06
Iter: 1811 loss: 1.4990731302894586e-06
Iter: 1812 loss: 1.4971251674879185e-06
Iter: 1813 loss: 1.4963810948336392e-06
Iter: 1814 loss: 1.4966760441481013e-06
Iter: 1815 loss: 1.4958659870098556e-06
Iter: 1816 loss: 1.4955071311589831e-06
Iter: 1817 loss: 1.4954889821605834e-06
Iter: 1818 loss: 1.4951466572516919e-06
Iter: 1819 loss: 1.4970807961830674e-06
Iter: 1820 loss: 1.4950991539602064e-06
Iter: 1821 loss: 1.4948544280472476e-06
Iter: 1822 loss: 1.4942079569119751e-06
Iter: 1823 loss: 1.4989539497225408e-06
Iter: 1824 loss: 1.4940718392312474e-06
Iter: 1825 loss: 1.4932894805312736e-06
Iter: 1826 loss: 1.4941286431605568e-06
Iter: 1827 loss: 1.4928598716978257e-06
Iter: 1828 loss: 1.4921853360887805e-06
Iter: 1829 loss: 1.4921849453985337e-06
Iter: 1830 loss: 1.4916138589415922e-06
Iter: 1831 loss: 1.4923191973560166e-06
Iter: 1832 loss: 1.4913166095039423e-06
Iter: 1833 loss: 1.4907482420449322e-06
Iter: 1834 loss: 1.4938471116220211e-06
Iter: 1835 loss: 1.4906646225785129e-06
Iter: 1836 loss: 1.490219490304424e-06
Iter: 1837 loss: 1.4916476101010842e-06
Iter: 1838 loss: 1.4900931807696541e-06
Iter: 1839 loss: 1.4895025450070705e-06
Iter: 1840 loss: 1.4893582744939559e-06
Iter: 1841 loss: 1.4889838313896989e-06
Iter: 1842 loss: 1.4884625712767099e-06
Iter: 1843 loss: 1.4888020053266371e-06
Iter: 1844 loss: 1.4881325585412275e-06
Iter: 1845 loss: 1.4873667296976039e-06
Iter: 1846 loss: 1.49116076302839e-06
Iter: 1847 loss: 1.4872362908895659e-06
Iter: 1848 loss: 1.48666122741484e-06
Iter: 1849 loss: 1.4889327467760798e-06
Iter: 1850 loss: 1.486529962770672e-06
Iter: 1851 loss: 1.4860672059550224e-06
Iter: 1852 loss: 1.4912186122192869e-06
Iter: 1853 loss: 1.4860581406354871e-06
Iter: 1854 loss: 1.4856410479268041e-06
Iter: 1855 loss: 1.4874009488462071e-06
Iter: 1856 loss: 1.4855530314453041e-06
Iter: 1857 loss: 1.4853086776054611e-06
Iter: 1858 loss: 1.4846381982468476e-06
Iter: 1859 loss: 1.4886531228396373e-06
Iter: 1860 loss: 1.4844543746377834e-06
Iter: 1861 loss: 1.4837211743265935e-06
Iter: 1862 loss: 1.4873657892466893e-06
Iter: 1863 loss: 1.4835969262999595e-06
Iter: 1864 loss: 1.482891153380685e-06
Iter: 1865 loss: 1.4857450580414167e-06
Iter: 1866 loss: 1.4827343580201489e-06
Iter: 1867 loss: 1.4821523228140588e-06
Iter: 1868 loss: 1.4859449613739288e-06
Iter: 1869 loss: 1.4820900940930633e-06
Iter: 1870 loss: 1.4814505734475532e-06
Iter: 1871 loss: 1.4813614844351394e-06
Iter: 1872 loss: 1.4809112262988539e-06
Iter: 1873 loss: 1.4806502238111912e-06
Iter: 1874 loss: 1.4805834003042062e-06
Iter: 1875 loss: 1.4803255305751156e-06
Iter: 1876 loss: 1.479608161227303e-06
Iter: 1877 loss: 1.4835590975202276e-06
Iter: 1878 loss: 1.4793920246285267e-06
Iter: 1879 loss: 1.4785910160448025e-06
Iter: 1880 loss: 1.4891434198088346e-06
Iter: 1881 loss: 1.4785863599152414e-06
Iter: 1882 loss: 1.478070185381185e-06
Iter: 1883 loss: 1.4783459276367434e-06
Iter: 1884 loss: 1.4777298197194515e-06
Iter: 1885 loss: 1.4773093047330931e-06
Iter: 1886 loss: 1.477306135947675e-06
Iter: 1887 loss: 1.4770174585439694e-06
Iter: 1888 loss: 1.4798572468696107e-06
Iter: 1889 loss: 1.4770075583397662e-06
Iter: 1890 loss: 1.4767081363013922e-06
Iter: 1891 loss: 1.4761010871727917e-06
Iter: 1892 loss: 1.4872434165349655e-06
Iter: 1893 loss: 1.4760915131409291e-06
Iter: 1894 loss: 1.4755556384890583e-06
Iter: 1895 loss: 1.4762576021599684e-06
Iter: 1896 loss: 1.4752834228897515e-06
Iter: 1897 loss: 1.4746105310632511e-06
Iter: 1898 loss: 1.4751128999550305e-06
Iter: 1899 loss: 1.4741982039310469e-06
Iter: 1900 loss: 1.4734525172431543e-06
Iter: 1901 loss: 1.4761221411403926e-06
Iter: 1902 loss: 1.4732628382629665e-06
Iter: 1903 loss: 1.4727467253724153e-06
Iter: 1904 loss: 1.472746103471364e-06
Iter: 1905 loss: 1.4723655230102708e-06
Iter: 1906 loss: 1.4724015174980113e-06
Iter: 1907 loss: 1.4720716006827053e-06
Iter: 1908 loss: 1.4714537602366248e-06
Iter: 1909 loss: 1.473054989432094e-06
Iter: 1910 loss: 1.4712431168434497e-06
Iter: 1911 loss: 1.4706294318227314e-06
Iter: 1912 loss: 1.4734274582671233e-06
Iter: 1913 loss: 1.470512142149795e-06
Iter: 1914 loss: 1.4700610104045846e-06
Iter: 1915 loss: 1.469348691169097e-06
Iter: 1916 loss: 1.4693412301645229e-06
Iter: 1917 loss: 1.468715410023809e-06
Iter: 1918 loss: 1.4748854522419272e-06
Iter: 1919 loss: 1.4686941198448061e-06
Iter: 1920 loss: 1.4681327484519058e-06
Iter: 1921 loss: 1.4709444068006699e-06
Iter: 1922 loss: 1.4680386954308486e-06
Iter: 1923 loss: 1.4674410405073832e-06
Iter: 1924 loss: 1.4721781135193114e-06
Iter: 1925 loss: 1.4674001835326324e-06
Iter: 1926 loss: 1.467105257805353e-06
Iter: 1927 loss: 1.4668506235645985e-06
Iter: 1928 loss: 1.4667710241739954e-06
Iter: 1929 loss: 1.466277868277549e-06
Iter: 1930 loss: 1.4662589814526898e-06
Iter: 1931 loss: 1.4658776146073439e-06
Iter: 1932 loss: 1.4652746140830867e-06
Iter: 1933 loss: 1.4653718610010822e-06
Iter: 1934 loss: 1.4648202069958636e-06
Iter: 1935 loss: 1.4643069565251637e-06
Iter: 1936 loss: 1.4643051754432633e-06
Iter: 1937 loss: 1.4638885071337719e-06
Iter: 1938 loss: 1.4640248602567023e-06
Iter: 1939 loss: 1.46359286309246e-06
Iter: 1940 loss: 1.4629791420896169e-06
Iter: 1941 loss: 1.4678306262065762e-06
Iter: 1942 loss: 1.4629369135339808e-06
Iter: 1943 loss: 1.4626069072485764e-06
Iter: 1944 loss: 1.4630384636695119e-06
Iter: 1945 loss: 1.4624391432688442e-06
Iter: 1946 loss: 1.461889743589726e-06
Iter: 1947 loss: 1.4618566976361377e-06
Iter: 1948 loss: 1.4614401554553657e-06
Iter: 1949 loss: 1.460892937643568e-06
Iter: 1950 loss: 1.4622946885356024e-06
Iter: 1951 loss: 1.4607046863341383e-06
Iter: 1952 loss: 1.4600060274982173e-06
Iter: 1953 loss: 1.4600011074972747e-06
Iter: 1954 loss: 1.4594456769221981e-06
Iter: 1955 loss: 1.4595848255288388e-06
Iter: 1956 loss: 1.4591339903327984e-06
Iter: 1957 loss: 1.4588144519324134e-06
Iter: 1958 loss: 1.4583408022518645e-06
Iter: 1959 loss: 1.4583295864864081e-06
Iter: 1960 loss: 1.4578775644881743e-06
Iter: 1961 loss: 1.4588241501124466e-06
Iter: 1962 loss: 1.4576982719843609e-06
Iter: 1963 loss: 1.4571383569867879e-06
Iter: 1964 loss: 1.4568881716631365e-06
Iter: 1965 loss: 1.4566055413474333e-06
Iter: 1966 loss: 1.4560450289905165e-06
Iter: 1967 loss: 1.4614351596556811e-06
Iter: 1968 loss: 1.4560240628289746e-06
Iter: 1969 loss: 1.4555898067394409e-06
Iter: 1970 loss: 1.4549128200620258e-06
Iter: 1971 loss: 1.45490419610981e-06
Iter: 1972 loss: 1.4543204396735419e-06
Iter: 1973 loss: 1.4543060123778781e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi2.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.4 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi2.4
+ date
Sun Nov  8 00:19:40 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2/300_300_300_1 --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8f622f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8e9f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8daa158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8de3e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8ddd7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8dddc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8d2a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8cef840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8cef598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8d2a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8c5b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca52258c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca5225488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca52252f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca51c52f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca51bcbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca51b9400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca516c950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca5127ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8cb8048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8cb80d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8cb8d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca50996a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca51f0048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca8cb8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca50e96a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca50e99d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca503b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca4ffb6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca4ffc488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca4fab400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca4f8e488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca4f57268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca4f66c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca4f2fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcca4eaf1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.02933329507424925
test_loss: 0.02807164120509614
train_loss: 0.015312677163713604
test_loss: 0.01839448360509448
train_loss: 0.013584453047343684
test_loss: 0.02000440295482006
train_loss: 0.012586285183114056
test_loss: 0.015360337573291099
train_loss: 0.011341514206870865
test_loss: 0.01591575261735615
train_loss: 0.011111924516268013
test_loss: 0.01564041098102745
train_loss: 0.010045539860345665
test_loss: 0.014298692566862625
train_loss: 0.010585235134771147
test_loss: 0.014338716189455692
train_loss: 0.011020584009932256
test_loss: 0.014266558252019971
train_loss: 0.010702548584112674
test_loss: 0.014241856221784211
train_loss: 0.011207550259629596
test_loss: 0.01450335549018446
train_loss: 0.009185973007169796
test_loss: 0.013366171469186738
train_loss: 0.010057802155279495
test_loss: 0.013899863933436304
train_loss: 0.010314261542646096
test_loss: 0.013738964815467724
train_loss: 0.008654653905047213
test_loss: 0.012980794400512637
train_loss: 0.008587477015320576
test_loss: 0.013250880078281114
train_loss: 0.009663739174723815
test_loss: 0.013361839495084763
train_loss: 0.009874951091768771
test_loss: 0.012996716756575889
train_loss: 0.008470964206763795
test_loss: 0.013894196137260197
train_loss: 0.00954272010320473
test_loss: 0.01283269399551506
train_loss: 0.00857455568519675
test_loss: 0.01312458128558014
train_loss: 0.009386879272699686
test_loss: 0.015066836755797041
train_loss: 0.009647867035308368
test_loss: 0.013898797132763267
train_loss: 0.008944985109317728
test_loss: 0.013274375103059108
train_loss: 0.008374549854019548
test_loss: 0.014019263409523513
train_loss: 0.00964915872535046
test_loss: 0.015352542981477464
train_loss: 0.00828140994623348
test_loss: 0.01351405066234398
train_loss: 0.010394900165888371
test_loss: 0.013080651822116936
train_loss: 0.008394650716527563
test_loss: 0.013067573018589398
train_loss: 0.008513274814849384
test_loss: 0.013217223183631305
train_loss: 0.009124851057472357
test_loss: 0.012932741439905786
train_loss: 0.008792358113469352
test_loss: 0.013312525439410193
train_loss: 0.007956787728321247
test_loss: 0.013283364019280954
train_loss: 0.008430315624577958
test_loss: 0.013615436623783405
train_loss: 0.007611550527918549
test_loss: 0.013264947783271258
train_loss: 0.01026272932652887
test_loss: 0.012983168323127679
train_loss: 0.008424313006339317
test_loss: 0.013473408390655578
train_loss: 0.008393059019162102
test_loss: 0.01359969374594241
train_loss: 0.008402558886847183
test_loss: 0.014311875914886679
train_loss: 0.009243031114358207
test_loss: 0.014261108377174342
train_loss: 0.0077200794858005364
test_loss: 0.014128910974026207
train_loss: 0.008209773602776797
test_loss: 0.014168422628009882
train_loss: 0.007714493293676104
test_loss: 0.013421932817911583
train_loss: 0.008028546454518119
test_loss: 0.013191268246054632
train_loss: 0.00858723308220777
test_loss: 0.013082460560088091
train_loss: 0.007968848794970384
test_loss: 0.013258197431517932
train_loss: 0.008681931397414269
test_loss: 0.014552759288893864
train_loss: 0.007761862541043413
test_loss: 0.013792295638855586
train_loss: 0.007476807225642286
test_loss: 0.012962016219726015
train_loss: 0.009224076092196182
test_loss: 0.013685042573231748
train_loss: 0.008285635267181898
test_loss: 0.013928225456108348
train_loss: 0.00893198275149295
test_loss: 0.01385906016041214
train_loss: 0.00854359655927221
test_loss: 0.013956314276415579
train_loss: 0.007680030578985465
test_loss: 0.012840542763433493
train_loss: 0.009606519576980055
test_loss: 0.014357939463433144
train_loss: 0.009016050128627398
test_loss: 0.014118911465523767
train_loss: 0.007849611552298599
test_loss: 0.013538235309785361
train_loss: 0.007797033532255662
test_loss: 0.013252171521770785
train_loss: 0.008426294040960541
test_loss: 0.013676850709663086
train_loss: 0.007435424132101233
test_loss: 0.013211640427518982
train_loss: 0.007932426071071867
test_loss: 0.013940708866542056
train_loss: 0.007366033546323076
test_loss: 0.013349632039565606
train_loss: 0.007566583737598014
test_loss: 0.014100150338772259
train_loss: 0.007818062063750127
test_loss: 0.013184495454237596
train_loss: 0.007527031136207367
test_loss: 0.013735352575073781
train_loss: 0.007366241168249924
test_loss: 0.013375798517898212
train_loss: 0.007118922755949212
test_loss: 0.013401836875735195
train_loss: 0.0069939879511156326
test_loss: 0.013120158832891247
train_loss: 0.007025163911833842
test_loss: 0.013516490722881
train_loss: 0.008607592706748848
test_loss: 0.014013364974512589
train_loss: 0.0063956937775590085
test_loss: 0.013226524365852116
train_loss: 0.007474496223856527
test_loss: 0.013220594911159435
train_loss: 0.008366664396264309
test_loss: 0.013934098341564019
train_loss: 0.008155815123270769
test_loss: 0.014868957382841606
train_loss: 0.006700217261311803
test_loss: 0.013195339848989907
train_loss: 0.008011481768254006
test_loss: 0.01429355414242618
train_loss: 0.009050643000989057
test_loss: 0.014475011657267167
train_loss: 0.0073850275618897575
test_loss: 0.013194811048854362
train_loss: 0.00795399897430369
test_loss: 0.014729482114284424
train_loss: 0.008314484915021392
test_loss: 0.014114104859438937
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi2.4/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi2.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31e8113400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31e8179d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31e8179488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31e81799d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31e80d7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d067cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31e803fd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d0633ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d0667510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d0605840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d05b01e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d05b6ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d05b6f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d056f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d0530a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d05578c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d0557a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d0524620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d04d5620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d04d5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d04a5488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d04a5b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d03f2a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d041f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d03c9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d03e3ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d03c90d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d0349730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d0349620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d035c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d02eeae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d02ce6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d02de400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d02707b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d02a0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f31d0243378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 0.00011067214869364732
Iter: 2 loss: 0.00053051443516092026
Iter: 3 loss: 9.2224228590447044e-05
Iter: 4 loss: 8.22701265298998e-05
Iter: 5 loss: 7.1694178204042e-05
Iter: 6 loss: 6.99457796874701e-05
Iter: 7 loss: 6.3022598493928659e-05
Iter: 8 loss: 7.2229030945176721e-05
Iter: 9 loss: 5.9536573352487063e-05
Iter: 10 loss: 5.3607043304217712e-05
Iter: 11 loss: 6.70690082318314e-05
Iter: 12 loss: 5.1379958764495751e-05
Iter: 13 loss: 4.715575592517092e-05
Iter: 14 loss: 5.5727376719982835e-05
Iter: 15 loss: 4.5443526751348071e-05
Iter: 16 loss: 4.3290816904910952e-05
Iter: 17 loss: 4.2964269653114769e-05
Iter: 18 loss: 4.1463347427867277e-05
Iter: 19 loss: 3.9734686481987659e-05
Iter: 20 loss: 3.625527317950881e-05
Iter: 21 loss: 9.9620493706850533e-05
Iter: 22 loss: 3.6204728925616272e-05
Iter: 23 loss: 3.3661578360439776e-05
Iter: 24 loss: 3.91223157547707e-05
Iter: 25 loss: 3.2662122869150733e-05
Iter: 26 loss: 3.1441422336137118e-05
Iter: 27 loss: 3.0337154409578305e-05
Iter: 28 loss: 3.0033942259226904e-05
Iter: 29 loss: 2.7690450631781413e-05
Iter: 30 loss: 3.9657932678076931e-05
Iter: 31 loss: 2.731598077258076e-05
Iter: 32 loss: 2.617240049461851e-05
Iter: 33 loss: 3.1754433164569414e-05
Iter: 34 loss: 2.5973569876840182e-05
Iter: 35 loss: 2.4846444949524774e-05
Iter: 36 loss: 2.5279257383922951e-05
Iter: 37 loss: 2.4061542608346352e-05
Iter: 38 loss: 2.3022904736435405e-05
Iter: 39 loss: 3.0236433460135434e-05
Iter: 40 loss: 2.292622827558194e-05
Iter: 41 loss: 2.1853052459978949e-05
Iter: 42 loss: 2.1739655023546596e-05
Iter: 43 loss: 2.0957377116964897e-05
Iter: 44 loss: 2.0547373487956316e-05
Iter: 45 loss: 2.0462437443212934e-05
Iter: 46 loss: 1.9926385657243261e-05
Iter: 47 loss: 2.0543946362701928e-05
Iter: 48 loss: 1.963906361154376e-05
Iter: 49 loss: 1.914884115772955e-05
Iter: 50 loss: 2.0731967965593152e-05
Iter: 51 loss: 1.9010781765262534e-05
Iter: 52 loss: 1.8794955947401644e-05
Iter: 53 loss: 1.873025519956619e-05
Iter: 54 loss: 1.8603033814826828e-05
Iter: 55 loss: 1.8185709624405396e-05
Iter: 56 loss: 1.8288164325321957e-05
Iter: 57 loss: 1.7780376653245349e-05
Iter: 58 loss: 1.6911047740209278e-05
Iter: 59 loss: 2.2880231732189554e-05
Iter: 60 loss: 1.6827163455899498e-05
Iter: 61 loss: 1.6485008313369608e-05
Iter: 62 loss: 1.6155121622472005e-05
Iter: 63 loss: 1.6079808129070023e-05
Iter: 64 loss: 1.53227216130952e-05
Iter: 65 loss: 1.9748941606139889e-05
Iter: 66 loss: 1.5224187423912328e-05
Iter: 67 loss: 1.4876009912673329e-05
Iter: 68 loss: 1.6154441134481847e-05
Iter: 69 loss: 1.4789888268548863e-05
Iter: 70 loss: 1.4406721611961944e-05
Iter: 71 loss: 1.5011575358160571e-05
Iter: 72 loss: 1.422816996103829e-05
Iter: 73 loss: 1.3891491162353749e-05
Iter: 74 loss: 1.4928003933126377e-05
Iter: 75 loss: 1.3792005385858642e-05
Iter: 76 loss: 1.3441460644534065e-05
Iter: 77 loss: 1.4060934904089565e-05
Iter: 78 loss: 1.3287220500675988e-05
Iter: 79 loss: 1.3346645795213188e-05
Iter: 80 loss: 1.3060815609335451e-05
Iter: 81 loss: 1.2961637447005496e-05
Iter: 82 loss: 1.2817532525986936e-05
Iter: 83 loss: 1.2813337853496281e-05
Iter: 84 loss: 1.2613408284379444e-05
Iter: 85 loss: 1.4255277775852573e-05
Iter: 86 loss: 1.2601206703195806e-05
Iter: 87 loss: 1.2509255058362829e-05
Iter: 88 loss: 1.2327183702989692e-05
Iter: 89 loss: 1.5901538948013526e-05
Iter: 90 loss: 1.2325321550280725e-05
Iter: 91 loss: 1.2138988652453291e-05
Iter: 92 loss: 1.2365631543492264e-05
Iter: 93 loss: 1.2041567873186334e-05
Iter: 94 loss: 1.1856601403859431e-05
Iter: 95 loss: 1.3300989279220488e-05
Iter: 96 loss: 1.1843369762601129e-05
Iter: 97 loss: 1.1720687470401506e-05
Iter: 98 loss: 1.1556118280173398e-05
Iter: 99 loss: 1.1547209063702107e-05
Iter: 100 loss: 1.1242320658860697e-05
Iter: 101 loss: 1.1936509148600736e-05
Iter: 102 loss: 1.1128091676207657e-05
Iter: 103 loss: 1.0955769623281437e-05
Iter: 104 loss: 1.3169207142757024e-05
Iter: 105 loss: 1.0954516171754933e-05
Iter: 106 loss: 1.0808218795470435e-05
Iter: 107 loss: 1.073427928703235e-05
Iter: 108 loss: 1.0665637302049119e-05
Iter: 109 loss: 1.0493461369076172e-05
Iter: 110 loss: 1.2160231277484345e-05
Iter: 111 loss: 1.0487193913418508e-05
Iter: 112 loss: 1.0352257594397773e-05
Iter: 113 loss: 1.0319572933051964e-05
Iter: 114 loss: 1.0233732403875529e-05
Iter: 115 loss: 1.0188819958904378e-05
Iter: 116 loss: 1.0127833141020166e-05
Iter: 117 loss: 1.0080381891566566e-05
Iter: 118 loss: 1.0447644140872169e-05
Iter: 119 loss: 1.0076933447098632e-05
Iter: 120 loss: 1.0032246769440056e-05
Iter: 121 loss: 9.9832499204400686e-06
Iter: 122 loss: 9.9760225504107954e-06
Iter: 123 loss: 9.9095843489748709e-06
Iter: 124 loss: 9.7668722193182275e-06
Iter: 125 loss: 1.2009460971633311e-05
Iter: 126 loss: 9.761898339794243e-06
Iter: 127 loss: 9.6307693622805854e-06
Iter: 128 loss: 1.0978247378882135e-05
Iter: 129 loss: 9.6270215711347383e-06
Iter: 130 loss: 9.5399912272235563e-06
Iter: 131 loss: 9.9415899156981014e-06
Iter: 132 loss: 9.5235807607473883e-06
Iter: 133 loss: 9.4436803203385477e-06
Iter: 134 loss: 9.6042833282059944e-06
Iter: 135 loss: 9.411194850151488e-06
Iter: 136 loss: 9.3254634824476419e-06
Iter: 137 loss: 9.3121393175901062e-06
Iter: 138 loss: 9.2526861646941153e-06
Iter: 139 loss: 9.1137121294852846e-06
Iter: 140 loss: 9.5543296205887112e-06
Iter: 141 loss: 9.0738745689555528e-06
Iter: 142 loss: 8.980504838958781e-06
Iter: 143 loss: 9.3446164577242557e-06
Iter: 144 loss: 8.9588881112068239e-06
Iter: 145 loss: 8.85393948799689e-06
Iter: 146 loss: 9.0322593951353568e-06
Iter: 147 loss: 8.8068614295538791e-06
Iter: 148 loss: 8.76946405915327e-06
Iter: 149 loss: 8.75993181328566e-06
Iter: 150 loss: 8.712715783107628e-06
Iter: 151 loss: 8.9043170207081144e-06
Iter: 152 loss: 8.7022789495103165e-06
Iter: 153 loss: 8.648232997284707e-06
Iter: 154 loss: 8.71071286111687e-06
Iter: 155 loss: 8.6193371753345514e-06
Iter: 156 loss: 8.5688259155470284e-06
Iter: 157 loss: 8.50296929993629e-06
Iter: 158 loss: 8.4987150642136709e-06
Iter: 159 loss: 8.4467874339933289e-06
Iter: 160 loss: 8.4390885077241481e-06
Iter: 161 loss: 8.4028416816156821e-06
Iter: 162 loss: 8.3133266493176442e-06
Iter: 163 loss: 8.691096404124423e-06
Iter: 164 loss: 8.2944872135510882e-06
Iter: 165 loss: 8.2260712882608869e-06
Iter: 166 loss: 8.665586786626927e-06
Iter: 167 loss: 8.2185175841340688e-06
Iter: 168 loss: 8.1654139255624276e-06
Iter: 169 loss: 8.1911728995731784e-06
Iter: 170 loss: 8.1298061441012034e-06
Iter: 171 loss: 8.06336142075731e-06
Iter: 172 loss: 8.2640436260051327e-06
Iter: 173 loss: 8.0434082933381689e-06
Iter: 174 loss: 7.9843896393998787e-06
Iter: 175 loss: 8.0765302843800076e-06
Iter: 176 loss: 7.9567299706612147e-06
Iter: 177 loss: 7.8942236753600188e-06
Iter: 178 loss: 8.22426010822606e-06
Iter: 179 loss: 7.88455073980585e-06
Iter: 180 loss: 7.8379257317297369e-06
Iter: 181 loss: 7.98714455427561e-06
Iter: 182 loss: 7.82466781330837e-06
Iter: 183 loss: 7.8026870292404448e-06
Iter: 184 loss: 7.7957321831140336e-06
Iter: 185 loss: 7.7705805949739942e-06
Iter: 186 loss: 7.7980443517501783e-06
Iter: 187 loss: 7.7568648280731075e-06
Iter: 188 loss: 7.7303025721940848e-06
Iter: 189 loss: 7.7378701416432386e-06
Iter: 190 loss: 7.71116003455649e-06
Iter: 191 loss: 7.6858665880692347e-06
Iter: 192 loss: 7.6223249451014136e-06
Iter: 193 loss: 8.2071474915622386e-06
Iter: 194 loss: 7.6130357955349777e-06
Iter: 195 loss: 7.5529994589760673e-06
Iter: 196 loss: 7.5529993551157748e-06
Iter: 197 loss: 7.5184222551703991e-06
Iter: 198 loss: 7.5105168601962466e-06
Iter: 199 loss: 7.4882474277838086e-06
Iter: 200 loss: 7.42390779761233e-06
Iter: 201 loss: 7.71084676573431e-06
Iter: 202 loss: 7.4111891436851967e-06
Iter: 203 loss: 7.3659007514544831e-06
Iter: 204 loss: 7.5861503470073411e-06
Iter: 205 loss: 7.3579832567220964e-06
Iter: 206 loss: 7.3243258920853418e-06
Iter: 207 loss: 7.3270598679502709e-06
Iter: 208 loss: 7.2981966325439251e-06
Iter: 209 loss: 7.2560032650812236e-06
Iter: 210 loss: 7.4593990698122284e-06
Iter: 211 loss: 7.2485291141181023e-06
Iter: 212 loss: 7.2125726012536866e-06
Iter: 213 loss: 7.2674257448498115e-06
Iter: 214 loss: 7.1955371258990728e-06
Iter: 215 loss: 7.1841989218472112e-06
Iter: 216 loss: 7.1753572274373645e-06
Iter: 217 loss: 7.1550016225027626e-06
Iter: 218 loss: 7.2072145567173608e-06
Iter: 219 loss: 7.1479940057951608e-06
Iter: 220 loss: 7.1316003185574257e-06
Iter: 221 loss: 7.1465825644151471e-06
Iter: 222 loss: 7.1221111564271454e-06
Iter: 223 loss: 7.10405669443832e-06
Iter: 224 loss: 7.0602120132627844e-06
Iter: 225 loss: 7.5227808486924513e-06
Iter: 226 loss: 7.055292138959846e-06
Iter: 227 loss: 7.016872678964789e-06
Iter: 228 loss: 7.29846194632294e-06
Iter: 229 loss: 7.0136852092107794e-06
Iter: 230 loss: 6.9757381653829464e-06
Iter: 231 loss: 6.9677676211543664e-06
Iter: 232 loss: 6.9428447525449755e-06
Iter: 233 loss: 6.9159786651351589e-06
Iter: 234 loss: 6.9141424477327995e-06
Iter: 235 loss: 6.8941908701962068e-06
Iter: 236 loss: 6.8800385263466029e-06
Iter: 237 loss: 6.8730274049116869e-06
Iter: 238 loss: 6.8349182136482126e-06
Iter: 239 loss: 6.9421848448188741e-06
Iter: 240 loss: 6.8227642672051728e-06
Iter: 241 loss: 6.7953313431188377e-06
Iter: 242 loss: 6.889559876975991e-06
Iter: 243 loss: 6.7880465336128048e-06
Iter: 244 loss: 6.7589031419936527e-06
Iter: 245 loss: 6.8097127853292945e-06
Iter: 246 loss: 6.7460148550221716e-06
Iter: 247 loss: 6.7227299183662227e-06
Iter: 248 loss: 6.9921922012469818e-06
Iter: 249 loss: 6.7223611288008212e-06
Iter: 250 loss: 6.7106942147572707e-06
Iter: 251 loss: 6.7091748721869838e-06
Iter: 252 loss: 6.7022405437459931e-06
Iter: 253 loss: 6.6914399840845451e-06
Iter: 254 loss: 6.6913004338970807e-06
Iter: 255 loss: 6.6731111102383978e-06
Iter: 256 loss: 6.6486272238931941e-06
Iter: 257 loss: 6.6473365790869154e-06
Iter: 258 loss: 6.6242791204876842e-06
Iter: 259 loss: 6.6745288754626376e-06
Iter: 260 loss: 6.6153646665928279e-06
Iter: 261 loss: 6.5885107082642734e-06
Iter: 262 loss: 6.6152640753074351e-06
Iter: 263 loss: 6.5733857378100928e-06
Iter: 264 loss: 6.5502155242780745e-06
Iter: 265 loss: 6.6688860446185818e-06
Iter: 266 loss: 6.5464539820416427e-06
Iter: 267 loss: 6.5210545017608966e-06
Iter: 268 loss: 6.5592959450385163e-06
Iter: 269 loss: 6.508951411829257e-06
Iter: 270 loss: 6.4878057968864422e-06
Iter: 271 loss: 6.7398029650285991e-06
Iter: 272 loss: 6.4875289858493756e-06
Iter: 273 loss: 6.4720372432579873e-06
Iter: 274 loss: 6.4481248900694334e-06
Iter: 275 loss: 6.44777517378316e-06
Iter: 276 loss: 6.4216296253432515e-06
Iter: 277 loss: 6.6772385628465104e-06
Iter: 278 loss: 6.4207111353209209e-06
Iter: 279 loss: 6.4042140092920755e-06
Iter: 280 loss: 6.4265043127135767e-06
Iter: 281 loss: 6.395948647446859e-06
Iter: 282 loss: 6.400027006295133e-06
Iter: 283 loss: 6.387093738138723e-06
Iter: 284 loss: 6.3805430099951085e-06
Iter: 285 loss: 6.36675515800237e-06
Iter: 286 loss: 6.5955233204153685e-06
Iter: 287 loss: 6.3663804806719979e-06
Iter: 288 loss: 6.3498329857840851e-06
Iter: 289 loss: 6.4109427953689432e-06
Iter: 290 loss: 6.3457594674335334e-06
Iter: 291 loss: 6.3358483286125449e-06
Iter: 292 loss: 6.3187275889187977e-06
Iter: 293 loss: 6.3187164690945285e-06
Iter: 294 loss: 6.2940417475710147e-06
Iter: 295 loss: 6.3725893819426953e-06
Iter: 296 loss: 6.2869863530515615e-06
Iter: 297 loss: 6.2673096486466982e-06
Iter: 298 loss: 6.2942940220385711e-06
Iter: 299 loss: 6.2575092699075139e-06
Iter: 300 loss: 6.2335205144450745e-06
Iter: 301 loss: 6.351371347268275e-06
Iter: 302 loss: 6.22937853015411e-06
Iter: 303 loss: 6.2126948669564777e-06
Iter: 304 loss: 6.2848159251463017e-06
Iter: 305 loss: 6.2092810175208677e-06
Iter: 306 loss: 6.1900291469674112e-06
Iter: 307 loss: 6.2078276240101885e-06
Iter: 308 loss: 6.1789200916673345e-06
Iter: 309 loss: 6.1618201900348018e-06
Iter: 310 loss: 6.2315782828513515e-06
Iter: 311 loss: 6.158063617209412e-06
Iter: 312 loss: 6.1405877666469536e-06
Iter: 313 loss: 6.1508125446291687e-06
Iter: 314 loss: 6.1292625136395224e-06
Iter: 315 loss: 6.1477962009164537e-06
Iter: 316 loss: 6.12427345163312e-06
Iter: 317 loss: 6.11906665270321e-06
Iter: 318 loss: 6.1110766282376691e-06
Iter: 319 loss: 6.1109499193044869e-06
Iter: 320 loss: 6.1027476484624631e-06
Iter: 321 loss: 6.12880816494147e-06
Iter: 322 loss: 6.1003999046002124e-06
Iter: 323 loss: 6.0913782880867436e-06
Iter: 324 loss: 6.0729262004562264e-06
Iter: 325 loss: 6.404080649136744e-06
Iter: 326 loss: 6.0725919431338822e-06
Iter: 327 loss: 6.05464723767566e-06
Iter: 328 loss: 6.161843307650432e-06
Iter: 329 loss: 6.0523884809049924e-06
Iter: 330 loss: 6.0378145978503537e-06
Iter: 331 loss: 6.0255180054139113e-06
Iter: 332 loss: 6.0214389493285287e-06
Iter: 333 loss: 6.00179495237621e-06
Iter: 334 loss: 6.2777503811192366e-06
Iter: 335 loss: 6.0017434108210995e-06
Iter: 336 loss: 5.988810755014427e-06
Iter: 337 loss: 5.9979423072985444e-06
Iter: 338 loss: 5.9807722290520327e-06
Iter: 339 loss: 5.9648533048605671e-06
Iter: 340 loss: 6.0797183333399667e-06
Iter: 341 loss: 5.9634855731168522e-06
Iter: 342 loss: 5.9531948527476967e-06
Iter: 343 loss: 5.9652016872690346e-06
Iter: 344 loss: 5.9477161888593205e-06
Iter: 345 loss: 5.933097256599149e-06
Iter: 346 loss: 5.9525968451015549e-06
Iter: 347 loss: 5.9257324859250039e-06
Iter: 348 loss: 5.9183149272642381e-06
Iter: 349 loss: 5.9180797544016666e-06
Iter: 350 loss: 5.9083730541340124e-06
Iter: 351 loss: 5.9245658650521367e-06
Iter: 352 loss: 5.9039798841340243e-06
Iter: 353 loss: 5.8975032287023472e-06
Iter: 354 loss: 5.89893711289589e-06
Iter: 355 loss: 5.892727844136397e-06
Iter: 356 loss: 5.8831488493632968e-06
Iter: 357 loss: 5.8906643859274944e-06
Iter: 358 loss: 5.87735612488575e-06
Iter: 359 loss: 5.8691896876650463e-06
Iter: 360 loss: 5.8597894237464356e-06
Iter: 361 loss: 5.8586497075315209e-06
Iter: 362 loss: 5.8400349526123885e-06
Iter: 363 loss: 5.8571466525403939e-06
Iter: 364 loss: 5.82928310706745e-06
Iter: 365 loss: 5.8151816157879428e-06
Iter: 366 loss: 5.9598354039176025e-06
Iter: 367 loss: 5.8147728630530108e-06
Iter: 368 loss: 5.8018318310719724e-06
Iter: 369 loss: 5.8153474671403444e-06
Iter: 370 loss: 5.7946616248831245e-06
Iter: 371 loss: 5.7826263108390864e-06
Iter: 372 loss: 5.88353088598975e-06
Iter: 373 loss: 5.7819188783077675e-06
Iter: 374 loss: 5.77177256786918e-06
Iter: 375 loss: 5.7697485897171074e-06
Iter: 376 loss: 5.7630158374669247e-06
Iter: 377 loss: 5.7481117355959012e-06
Iter: 378 loss: 5.8426374574585366e-06
Iter: 379 loss: 5.7464299309929984e-06
Iter: 380 loss: 5.7381032159297691e-06
Iter: 381 loss: 5.773127358317059e-06
Iter: 382 loss: 5.7363415535577135e-06
Iter: 383 loss: 5.727552937478781e-06
Iter: 384 loss: 5.8528470777716831e-06
Iter: 385 loss: 5.7275342588572805e-06
Iter: 386 loss: 5.7239860072030917e-06
Iter: 387 loss: 5.717341398581e-06
Iter: 388 loss: 5.8652101861959667e-06
Iter: 389 loss: 5.717326175186227e-06
Iter: 390 loss: 5.7085694528437079e-06
Iter: 391 loss: 5.7389553021649041e-06
Iter: 392 loss: 5.70626812501535e-06
Iter: 393 loss: 5.6988121538525724e-06
Iter: 394 loss: 5.6842849900575245e-06
Iter: 395 loss: 5.9789851738915311e-06
Iter: 396 loss: 5.68417802648227e-06
Iter: 397 loss: 5.6702171105695635e-06
Iter: 398 loss: 5.7840431602309781e-06
Iter: 399 loss: 5.6693323644285167e-06
Iter: 400 loss: 5.6595163664782349e-06
Iter: 401 loss: 5.6614159531204916e-06
Iter: 402 loss: 5.65220756657539e-06
Iter: 403 loss: 5.6384167949487013e-06
Iter: 404 loss: 5.751157109800382e-06
Iter: 405 loss: 5.6375490625704124e-06
Iter: 406 loss: 5.6290547105295636e-06
Iter: 407 loss: 5.666127045252429e-06
Iter: 408 loss: 5.62733761051143e-06
Iter: 409 loss: 5.6184673737144331e-06
Iter: 410 loss: 5.6199700563926593e-06
Iter: 411 loss: 5.611801536086908e-06
Iter: 412 loss: 5.6014330198532128e-06
Iter: 413 loss: 5.6833853387518057e-06
Iter: 414 loss: 5.6007194315296424e-06
Iter: 415 loss: 5.5936334760820351e-06
Iter: 416 loss: 5.6025730151022754e-06
Iter: 417 loss: 5.589974112042466e-06
Iter: 418 loss: 5.5870639004969027e-06
Iter: 419 loss: 5.584742246900547e-06
Iter: 420 loss: 5.5821409476848963e-06
Iter: 421 loss: 5.57599650528427e-06
Iter: 422 loss: 5.6475415653477634e-06
Iter: 423 loss: 5.5754475852228427e-06
Iter: 424 loss: 5.5687893404961375e-06
Iter: 425 loss: 5.6096068938114016e-06
Iter: 426 loss: 5.567989488543214e-06
Iter: 427 loss: 5.5624473468608657e-06
Iter: 428 loss: 5.5528090951849048e-06
Iter: 429 loss: 5.5528053978461448e-06
Iter: 430 loss: 5.5429697825108489e-06
Iter: 431 loss: 5.5723121887370709e-06
Iter: 432 loss: 5.5399880316064593e-06
Iter: 433 loss: 5.5280339463373168e-06
Iter: 434 loss: 5.5290113613229346e-06
Iter: 435 loss: 5.5187549354888694e-06
Iter: 436 loss: 5.5071401157142632e-06
Iter: 437 loss: 5.6561037585583025e-06
Iter: 438 loss: 5.5070537654329474e-06
Iter: 439 loss: 5.4988923679522742e-06
Iter: 440 loss: 5.5035070777976383e-06
Iter: 441 loss: 5.4935669940128085e-06
Iter: 442 loss: 5.48101135486024e-06
Iter: 443 loss: 5.5363754191556787e-06
Iter: 444 loss: 5.4785082714446239e-06
Iter: 445 loss: 5.4703714988687838e-06
Iter: 446 loss: 5.5127486001656422e-06
Iter: 447 loss: 5.4690844616251172e-06
Iter: 448 loss: 5.4621774992271689e-06
Iter: 449 loss: 5.4660529202031917e-06
Iter: 450 loss: 5.4576671437780067e-06
Iter: 451 loss: 5.4600964803811889e-06
Iter: 452 loss: 5.4540709606506845e-06
Iter: 453 loss: 5.4516302926333856e-06
Iter: 454 loss: 5.4460055587127393e-06
Iter: 455 loss: 5.5171910342426027e-06
Iter: 456 loss: 5.4456010361715495e-06
Iter: 457 loss: 5.4402577578697794e-06
Iter: 458 loss: 5.4652447390043016e-06
Iter: 459 loss: 5.4392702920767454e-06
Iter: 460 loss: 5.43340194135244e-06
Iter: 461 loss: 5.4280390193048765e-06
Iter: 462 loss: 5.4266018319097983e-06
Iter: 463 loss: 5.4194245235702915e-06
Iter: 464 loss: 5.426994466144912e-06
Iter: 465 loss: 5.4154598659615963e-06
Iter: 466 loss: 5.40527030374497e-06
Iter: 467 loss: 5.415622707215435e-06
Iter: 468 loss: 5.3995718122217115e-06
Iter: 469 loss: 5.3893863608840179e-06
Iter: 470 loss: 5.429800677711732e-06
Iter: 471 loss: 5.3870722428747372e-06
Iter: 472 loss: 5.376218802175643e-06
Iter: 473 loss: 5.406827464814064e-06
Iter: 474 loss: 5.372764846186012e-06
Iter: 475 loss: 5.3649778397016915e-06
Iter: 476 loss: 5.4546021195655647e-06
Iter: 477 loss: 5.3648506207899761e-06
Iter: 478 loss: 5.3596809694166254e-06
Iter: 479 loss: 5.3619586496820783e-06
Iter: 480 loss: 5.356158329246736e-06
Iter: 481 loss: 5.3482219854121488e-06
Iter: 482 loss: 5.36620001208237e-06
Iter: 483 loss: 5.3452333168090819e-06
Iter: 484 loss: 5.3492490278841521e-06
Iter: 485 loss: 5.3426671462668727e-06
Iter: 486 loss: 5.3405427868628834e-06
Iter: 487 loss: 5.3357977872360845e-06
Iter: 488 loss: 5.4021673928442233e-06
Iter: 489 loss: 5.3355448773684941e-06
Iter: 490 loss: 5.3306604607325537e-06
Iter: 491 loss: 5.3414220128117507e-06
Iter: 492 loss: 5.3287872359277443e-06
Iter: 493 loss: 5.3224747395247837e-06
Iter: 494 loss: 5.3307398430974616e-06
Iter: 495 loss: 5.3192672959321222e-06
Iter: 496 loss: 5.3145530362670147e-06
Iter: 497 loss: 5.3099306195547591e-06
Iter: 498 loss: 5.3089258307749276e-06
Iter: 499 loss: 5.2998552764852769e-06
Iter: 500 loss: 5.3255349624311479e-06
Iter: 501 loss: 5.2969757102935719e-06
Iter: 502 loss: 5.2889895748846007e-06
Iter: 503 loss: 5.3020813131407327e-06
Iter: 504 loss: 5.285339551653569e-06
Iter: 505 loss: 5.2764860096211205e-06
Iter: 506 loss: 5.33624664398728e-06
Iter: 507 loss: 5.2756050556337638e-06
Iter: 508 loss: 5.26972667038463e-06
Iter: 509 loss: 5.3001437692578226e-06
Iter: 510 loss: 5.2687891174222219e-06
Iter: 511 loss: 5.2628927845736684e-06
Iter: 512 loss: 5.2648184088068834e-06
Iter: 513 loss: 5.2587071148518182e-06
Iter: 514 loss: 5.2518167380010188e-06
Iter: 515 loss: 5.3039374582983236e-06
Iter: 516 loss: 5.25128727281632e-06
Iter: 517 loss: 5.2506742419313191e-06
Iter: 518 loss: 5.2493239091580573e-06
Iter: 519 loss: 5.2471089866081e-06
Iter: 520 loss: 5.2422302675642918e-06
Iter: 521 loss: 5.313308243698227e-06
Iter: 522 loss: 5.24200482522385e-06
Iter: 523 loss: 5.2366452045959258e-06
Iter: 524 loss: 5.24299156695417e-06
Iter: 525 loss: 5.2338084608588621e-06
Iter: 526 loss: 5.2270664880304871e-06
Iter: 527 loss: 5.2558811369617e-06
Iter: 528 loss: 5.2256657620136473e-06
Iter: 529 loss: 5.2214476084580775e-06
Iter: 530 loss: 5.2140048680719231e-06
Iter: 531 loss: 5.2140045286915311e-06
Iter: 532 loss: 5.2052640822700408e-06
Iter: 533 loss: 5.2581331789660008e-06
Iter: 534 loss: 5.2041880053803064e-06
Iter: 535 loss: 5.1975183607422316e-06
Iter: 536 loss: 5.2004599416505017e-06
Iter: 537 loss: 5.1929726966688123e-06
Iter: 538 loss: 5.1850006202102959e-06
Iter: 539 loss: 5.2458101347583913e-06
Iter: 540 loss: 5.1844003135802926e-06
Iter: 541 loss: 5.1783875037785456e-06
Iter: 542 loss: 5.1879254367261218e-06
Iter: 543 loss: 5.1755914113055421e-06
Iter: 544 loss: 5.1676459614879626e-06
Iter: 545 loss: 5.1972661789936985e-06
Iter: 546 loss: 5.1657117998391823e-06
Iter: 547 loss: 5.1596379564227657e-06
Iter: 548 loss: 5.1893820400848226e-06
Iter: 549 loss: 5.158585700963053e-06
Iter: 550 loss: 5.1564519669371337e-06
Iter: 551 loss: 5.1560596438646647e-06
Iter: 552 loss: 5.1529661926548774e-06
Iter: 553 loss: 5.1486599324079966e-06
Iter: 554 loss: 5.1484810769299095e-06
Iter: 555 loss: 5.1445132615438131e-06
Iter: 556 loss: 5.1537804276805325e-06
Iter: 557 loss: 5.1430514950529589e-06
Iter: 558 loss: 5.1396392405052432e-06
Iter: 559 loss: 5.159086835064162e-06
Iter: 560 loss: 5.1391727197407325e-06
Iter: 561 loss: 5.1365980146191318e-06
Iter: 562 loss: 5.1305849452791477e-06
Iter: 563 loss: 5.20335411215876e-06
Iter: 564 loss: 5.1300977964515671e-06
Iter: 565 loss: 5.1234426010225334e-06
Iter: 566 loss: 5.1736971406916485e-06
Iter: 567 loss: 5.1229282404796515e-06
Iter: 568 loss: 5.1176892151543189e-06
Iter: 569 loss: 5.1162541711620411e-06
Iter: 570 loss: 5.1130347779578234e-06
Iter: 571 loss: 5.1058256121779621e-06
Iter: 572 loss: 5.1549512690701e-06
Iter: 573 loss: 5.1051219037892162e-06
Iter: 574 loss: 5.0987982068455412e-06
Iter: 575 loss: 5.106793984568012e-06
Iter: 576 loss: 5.0955383460543115e-06
Iter: 577 loss: 5.08871739502881e-06
Iter: 578 loss: 5.149877578220582e-06
Iter: 579 loss: 5.0883912222126773e-06
Iter: 580 loss: 5.0841949236051863e-06
Iter: 581 loss: 5.0940568358921284e-06
Iter: 582 loss: 5.082656188862844e-06
Iter: 583 loss: 5.0798868978091016e-06
Iter: 584 loss: 5.0798382721680411e-06
Iter: 585 loss: 5.0764465929975819e-06
Iter: 586 loss: 5.0778277709545329e-06
Iter: 587 loss: 5.0741082548925787e-06
Iter: 588 loss: 5.0716234096189308e-06
Iter: 589 loss: 5.0710373954194732e-06
Iter: 590 loss: 5.0694482390911216e-06
Iter: 591 loss: 5.0656682608197146e-06
Iter: 592 loss: 5.077852959881261e-06
Iter: 593 loss: 5.0646005744923535e-06
Iter: 594 loss: 5.060321187231303e-06
Iter: 595 loss: 5.05305624947037e-06
Iter: 596 loss: 5.0530441589138071e-06
Iter: 597 loss: 5.0466757128504891e-06
Iter: 598 loss: 5.0893976098673948e-06
Iter: 599 loss: 5.0460342234579244e-06
Iter: 600 loss: 5.0403572328942151e-06
Iter: 601 loss: 5.038580975625824e-06
Iter: 602 loss: 5.0352355741452341e-06
Iter: 603 loss: 5.029302854771635e-06
Iter: 604 loss: 5.09815506658899e-06
Iter: 605 loss: 5.0292112332282149e-06
Iter: 606 loss: 5.0246829931333357e-06
Iter: 607 loss: 5.0294348024113187e-06
Iter: 608 loss: 5.0221770694997567e-06
Iter: 609 loss: 5.0175637964921427e-06
Iter: 610 loss: 5.0621926433366661e-06
Iter: 611 loss: 5.0173954660192419e-06
Iter: 612 loss: 5.0138249436082439e-06
Iter: 613 loss: 5.0162018124053668e-06
Iter: 614 loss: 5.0115753155586122e-06
Iter: 615 loss: 5.0087967806536595e-06
Iter: 616 loss: 5.0086933293181386e-06
Iter: 617 loss: 5.0056749659111473e-06
Iter: 618 loss: 5.0147569312468936e-06
Iter: 619 loss: 5.0047661356683137e-06
Iter: 620 loss: 5.0030468549993144e-06
Iter: 621 loss: 5.00012306774053e-06
Iter: 622 loss: 5.0001185602484942e-06
Iter: 623 loss: 4.9959965676360659e-06
Iter: 624 loss: 5.0116626526358865e-06
Iter: 625 loss: 4.995013773693545e-06
Iter: 626 loss: 4.9908803876487553e-06
Iter: 627 loss: 4.9896180928672961e-06
Iter: 628 loss: 4.9871626056298512e-06
Iter: 629 loss: 4.9831415713399862e-06
Iter: 630 loss: 4.9915908443702754e-06
Iter: 631 loss: 4.9815500971123534e-06
Iter: 632 loss: 4.9764771518518919e-06
Iter: 633 loss: 4.9772248416231691e-06
Iter: 634 loss: 4.9726345996625791e-06
Iter: 635 loss: 4.96699445171383e-06
Iter: 636 loss: 5.0035170121125359e-06
Iter: 637 loss: 4.9663834420719947e-06
Iter: 638 loss: 4.9607760274870553e-06
Iter: 639 loss: 4.9624343713925214e-06
Iter: 640 loss: 4.9567528861147333e-06
Iter: 641 loss: 4.9518443513206869e-06
Iter: 642 loss: 4.9518353320293467e-06
Iter: 643 loss: 4.9483087903351426e-06
Iter: 644 loss: 4.9505129133969942e-06
Iter: 645 loss: 4.9460563810476846e-06
Iter: 646 loss: 4.9432568259789373e-06
Iter: 647 loss: 4.9432051095727986e-06
Iter: 648 loss: 4.9405465015979387e-06
Iter: 649 loss: 4.9583123087029253e-06
Iter: 650 loss: 4.9402765685382569e-06
Iter: 651 loss: 4.9389664205711266e-06
Iter: 652 loss: 4.9355828086637428e-06
Iter: 653 loss: 4.9632399232585757e-06
Iter: 654 loss: 4.9349766442662806e-06
Iter: 655 loss: 4.9304809507736936e-06
Iter: 656 loss: 4.9635447482732809e-06
Iter: 657 loss: 4.9301110003232334e-06
Iter: 658 loss: 4.926814538387665e-06
Iter: 659 loss: 4.9300815524504983e-06
Iter: 660 loss: 4.9249546867744232e-06
Iter: 661 loss: 4.9219788545197405e-06
Iter: 662 loss: 4.9188300810890575e-06
Iter: 663 loss: 4.91829972029259e-06
Iter: 664 loss: 4.9118335416777557e-06
Iter: 665 loss: 4.9252086410389425e-06
Iter: 666 loss: 4.9092488111378246e-06
Iter: 667 loss: 4.9043831695809234e-06
Iter: 668 loss: 4.93281707145753e-06
Iter: 669 loss: 4.9037462801202849e-06
Iter: 670 loss: 4.8988505589756035e-06
Iter: 671 loss: 4.9023605573088475e-06
Iter: 672 loss: 4.8958196479557839e-06
Iter: 673 loss: 4.8914428493024054e-06
Iter: 674 loss: 4.9420391493398431e-06
Iter: 675 loss: 4.8913736671505353e-06
Iter: 676 loss: 4.88741174866123e-06
Iter: 677 loss: 4.8876404070037423e-06
Iter: 678 loss: 4.8843089221035751e-06
Iter: 679 loss: 4.8813240298384618e-06
Iter: 680 loss: 4.8811314272987929e-06
Iter: 681 loss: 4.87870485144704e-06
Iter: 682 loss: 4.9046072811822629e-06
Iter: 683 loss: 4.8786461372669189e-06
Iter: 684 loss: 4.8773630920852332e-06
Iter: 685 loss: 4.8739108459709339e-06
Iter: 686 loss: 4.8969564691848017e-06
Iter: 687 loss: 4.8730838291551642e-06
Iter: 688 loss: 4.8692145991654832e-06
Iter: 689 loss: 4.9122260713141632e-06
Iter: 690 loss: 4.869138333317809e-06
Iter: 691 loss: 4.8664821193076914e-06
Iter: 692 loss: 4.8675232790339472e-06
Iter: 693 loss: 4.8646405686408547e-06
Iter: 694 loss: 4.8613412371126494e-06
Iter: 695 loss: 4.85716789004757e-06
Iter: 696 loss: 4.85684664470938e-06
Iter: 697 loss: 4.8512676400782878e-06
Iter: 698 loss: 4.8911805264901963e-06
Iter: 699 loss: 4.8507784083839048e-06
Iter: 700 loss: 4.8467729745028152e-06
Iter: 701 loss: 4.8470959288122675e-06
Iter: 702 loss: 4.8436631728738667e-06
Iter: 703 loss: 4.83706949980453e-06
Iter: 704 loss: 4.862183395707983e-06
Iter: 705 loss: 4.8355017310881746e-06
Iter: 706 loss: 4.8312265475466559e-06
Iter: 707 loss: 4.8605189255314558e-06
Iter: 708 loss: 4.8308141770836882e-06
Iter: 709 loss: 4.8264607033170662e-06
Iter: 710 loss: 4.8320350727420159e-06
Iter: 711 loss: 4.8242288694563906e-06
Iter: 712 loss: 4.8211010474006766e-06
Iter: 713 loss: 4.8210870227210944e-06
Iter: 714 loss: 4.8191063054701775e-06
Iter: 715 loss: 4.8190971429883165e-06
Iter: 716 loss: 4.8180549665615854e-06
Iter: 717 loss: 4.815075224224909e-06
Iter: 718 loss: 4.8287196336979547e-06
Iter: 719 loss: 4.8139938527795957e-06
Iter: 720 loss: 4.8106414711314192e-06
Iter: 721 loss: 4.8507183186997159e-06
Iter: 722 loss: 4.8105984327469962e-06
Iter: 723 loss: 4.8081227824843618e-06
Iter: 724 loss: 4.8104847589036689e-06
Iter: 725 loss: 4.8067086911576645e-06
Iter: 726 loss: 4.8039348799160423e-06
Iter: 727 loss: 4.7999303589241974e-06
Iter: 728 loss: 4.7998077791813495e-06
Iter: 729 loss: 4.7951541168607416e-06
Iter: 730 loss: 4.83090651393693e-06
Iter: 731 loss: 4.7948097979117785e-06
Iter: 732 loss: 4.7912850166617513e-06
Iter: 733 loss: 4.7909154332312973e-06
Iter: 734 loss: 4.7883514944987984e-06
Iter: 735 loss: 4.7838883869343172e-06
Iter: 736 loss: 4.8249375975686777e-06
Iter: 737 loss: 4.783692526208126e-06
Iter: 738 loss: 4.7805422535588047e-06
Iter: 739 loss: 4.7826106753989047e-06
Iter: 740 loss: 4.7785517522752609e-06
Iter: 741 loss: 4.7736924185094547e-06
Iter: 742 loss: 4.79539912747717e-06
Iter: 743 loss: 4.7727374367691177e-06
Iter: 744 loss: 4.77017991414281e-06
Iter: 745 loss: 4.8060372450015693e-06
Iter: 746 loss: 4.7701730209389416e-06
Iter: 747 loss: 4.7686649756281e-06
Iter: 748 loss: 4.7686435963107321e-06
Iter: 749 loss: 4.7676230340699812e-06
Iter: 750 loss: 4.7647159811882782e-06
Iter: 751 loss: 4.7783748724667611e-06
Iter: 752 loss: 4.7636869718936859e-06
Iter: 753 loss: 4.760352490212693e-06
Iter: 754 loss: 4.7903046815188467e-06
Iter: 755 loss: 4.7601944668809378e-06
Iter: 756 loss: 4.7573351569701723e-06
Iter: 757 loss: 4.7599849701799162e-06
Iter: 758 loss: 4.7556869763491311e-06
Iter: 759 loss: 4.7524240238873467e-06
Iter: 760 loss: 4.7504124020527424e-06
Iter: 761 loss: 4.7490954562188111e-06
Iter: 762 loss: 4.7451293605892888e-06
Iter: 763 loss: 4.7686575964206752e-06
Iter: 764 loss: 4.7446235069077345e-06
Iter: 765 loss: 4.7411580621514962e-06
Iter: 766 loss: 4.7379970601744508e-06
Iter: 767 loss: 4.7371453293351044e-06
Iter: 768 loss: 4.7325211363027478e-06
Iter: 769 loss: 4.7954046700138279e-06
Iter: 770 loss: 4.7325023535874796e-06
Iter: 771 loss: 4.7290088277676633e-06
Iter: 772 loss: 4.7275186502091288e-06
Iter: 773 loss: 4.7257110406952147e-06
Iter: 774 loss: 4.720602215711231e-06
Iter: 775 loss: 4.7790804416985068e-06
Iter: 776 loss: 4.7205169624675853e-06
Iter: 777 loss: 4.7177930863296149e-06
Iter: 778 loss: 4.7318910030525254e-06
Iter: 779 loss: 4.7173577101365748e-06
Iter: 780 loss: 4.7156221984256608e-06
Iter: 781 loss: 4.7155156257777349e-06
Iter: 782 loss: 4.7142277132173365e-06
Iter: 783 loss: 4.7107427145713358e-06
Iter: 784 loss: 4.7333509694078571e-06
Iter: 785 loss: 4.7098751718076894e-06
Iter: 786 loss: 4.7067573610864145e-06
Iter: 787 loss: 4.7371209519019938e-06
Iter: 788 loss: 4.7066461324421707e-06
Iter: 789 loss: 4.7040303911519552e-06
Iter: 790 loss: 4.7083297805070648e-06
Iter: 791 loss: 4.7028363725070351e-06
Iter: 792 loss: 4.7000651870071339e-06
Iter: 793 loss: 4.6970609241228575e-06
Iter: 794 loss: 4.6965981072260037e-06
Iter: 795 loss: 4.6922018543089924e-06
Iter: 796 loss: 4.7081009751157434e-06
Iter: 797 loss: 4.6910954763302615e-06
Iter: 798 loss: 4.6866866289315251e-06
Iter: 799 loss: 4.6898527581812953e-06
Iter: 800 loss: 4.6839592142378723e-06
Iter: 801 loss: 4.6796355532948443e-06
Iter: 802 loss: 4.7109299908649763e-06
Iter: 803 loss: 4.6792658643443325e-06
Iter: 804 loss: 4.6752128999755973e-06
Iter: 805 loss: 4.6760944994748184e-06
Iter: 806 loss: 4.6722212983606843e-06
Iter: 807 loss: 4.6687687565733785e-06
Iter: 808 loss: 4.6687223982681088e-06
Iter: 809 loss: 4.6667224542752553e-06
Iter: 810 loss: 4.67329361664056e-06
Iter: 811 loss: 4.6661681216881619e-06
Iter: 812 loss: 4.664692442569965e-06
Iter: 813 loss: 4.6646322798180586e-06
Iter: 814 loss: 4.6633241171281841e-06
Iter: 815 loss: 4.6599919742507236e-06
Iter: 816 loss: 4.6889151651387838e-06
Iter: 817 loss: 4.659451986578494e-06
Iter: 818 loss: 4.6567616897793011e-06
Iter: 819 loss: 4.6762125688779353e-06
Iter: 820 loss: 4.6565313304223926e-06
Iter: 821 loss: 4.6540014786616957e-06
Iter: 822 loss: 4.65716887912683e-06
Iter: 823 loss: 4.6526918236969455e-06
Iter: 824 loss: 4.6495632936119407e-06
Iter: 825 loss: 4.6478808351395623e-06
Iter: 826 loss: 4.6464739248317476e-06
Iter: 827 loss: 4.6427786630927637e-06
Iter: 828 loss: 4.6543305260777064e-06
Iter: 829 loss: 4.6417045769605333e-06
Iter: 830 loss: 4.6376683528248882e-06
Iter: 831 loss: 4.6406249254550614e-06
Iter: 832 loss: 4.6351834393094676e-06
Iter: 833 loss: 4.6312885269085361e-06
Iter: 834 loss: 4.6540398510494969e-06
Iter: 835 loss: 4.6307783688155835e-06
Iter: 836 loss: 4.6270625277115545e-06
Iter: 837 loss: 4.6337376729218256e-06
Iter: 838 loss: 4.6254455332010565e-06
Iter: 839 loss: 4.6223124026257181e-06
Iter: 840 loss: 4.6508746341274623e-06
Iter: 841 loss: 4.6221711082466714e-06
Iter: 842 loss: 4.6194499439372239e-06
Iter: 843 loss: 4.623527473256362e-06
Iter: 844 loss: 4.6181485244784713e-06
Iter: 845 loss: 4.6169982371408424e-06
Iter: 846 loss: 4.6163285570506029e-06
Iter: 847 loss: 4.6149021724691338e-06
Iter: 848 loss: 4.6118453312693478e-06
Iter: 849 loss: 4.6601625377505676e-06
Iter: 850 loss: 4.6117416199723884e-06
Iter: 851 loss: 4.60950362574981e-06
Iter: 852 loss: 4.6168420959466062e-06
Iter: 853 loss: 4.608881704698397e-06
Iter: 854 loss: 4.6062525805092419e-06
Iter: 855 loss: 4.6118404267581736e-06
Iter: 856 loss: 4.6052197531769278e-06
Iter: 857 loss: 4.602312109479445e-06
Iter: 858 loss: 4.6015283709869189e-06
Iter: 859 loss: 4.5997331031598814e-06
Iter: 860 loss: 4.5967487152091987e-06
Iter: 861 loss: 4.6039809132848716e-06
Iter: 862 loss: 4.5956780929715346e-06
Iter: 863 loss: 4.5922422315219038e-06
Iter: 864 loss: 4.5968373986984279e-06
Iter: 865 loss: 4.5905123637654251e-06
Iter: 866 loss: 4.5872170823722623e-06
Iter: 867 loss: 4.595579671265257e-06
Iter: 868 loss: 4.5860749410276e-06
Iter: 869 loss: 4.58208970685255e-06
Iter: 870 loss: 4.5947376806693234e-06
Iter: 871 loss: 4.5809478171172257e-06
Iter: 872 loss: 4.5780106130533439e-06
Iter: 873 loss: 4.5975153138387816e-06
Iter: 874 loss: 4.5777080830979153e-06
Iter: 875 loss: 4.5749882775092548e-06
Iter: 876 loss: 4.5838599916885993e-06
Iter: 877 loss: 4.5742289878459813e-06
Iter: 878 loss: 4.5736363293564879e-06
Iter: 879 loss: 4.5729524939467588e-06
Iter: 880 loss: 4.57186591694704e-06
Iter: 881 loss: 4.5697265273259406e-06
Iter: 882 loss: 4.6119610756427536e-06
Iter: 883 loss: 4.5697067069207054e-06
Iter: 884 loss: 4.5678463096759474e-06
Iter: 885 loss: 4.568571662411933e-06
Iter: 886 loss: 4.5665554994494231e-06
Iter: 887 loss: 4.5636768844282705e-06
Iter: 888 loss: 4.5764575502585717e-06
Iter: 889 loss: 4.5631073092880621e-06
Iter: 890 loss: 4.5607960238326996e-06
Iter: 891 loss: 4.5629304223315935e-06
Iter: 892 loss: 4.5594623202327086e-06
Iter: 893 loss: 4.557368669721852e-06
Iter: 894 loss: 4.5563962774606225e-06
Iter: 895 loss: 4.555362219991751e-06
Iter: 896 loss: 4.5514878724495793e-06
Iter: 897 loss: 4.5604433818239963e-06
Iter: 898 loss: 4.5500497731271817e-06
Iter: 899 loss: 4.5469474974323064e-06
Iter: 900 loss: 4.5556319269414382e-06
Iter: 901 loss: 4.5459540166030191e-06
Iter: 902 loss: 4.5423811189789137e-06
Iter: 903 loss: 4.5527572958271993e-06
Iter: 904 loss: 4.54127119867095e-06
Iter: 905 loss: 4.5381896155310034e-06
Iter: 906 loss: 4.5521414702151369e-06
Iter: 907 loss: 4.537595584420133e-06
Iter: 908 loss: 4.5344480153103372e-06
Iter: 909 loss: 4.5464199785691e-06
Iter: 910 loss: 4.5336974325735805e-06
Iter: 911 loss: 4.5335590470164209e-06
Iter: 912 loss: 4.5325000585170469e-06
Iter: 913 loss: 4.5314830455073735e-06
Iter: 914 loss: 4.5297127601229821e-06
Iter: 915 loss: 4.5297121285175286e-06
Iter: 916 loss: 4.5280279740272471e-06
Iter: 917 loss: 4.5271484256299486e-06
Iter: 918 loss: 4.5263754812120884e-06
Iter: 919 loss: 4.5236639762741246e-06
Iter: 920 loss: 4.5447078386163449e-06
Iter: 921 loss: 4.5234682181023619e-06
Iter: 922 loss: 4.5216546111584181e-06
Iter: 923 loss: 4.5225441882595429e-06
Iter: 924 loss: 4.5204401978330704e-06
Iter: 925 loss: 4.5182653145543295e-06
Iter: 926 loss: 4.5158448549196121e-06
Iter: 927 loss: 4.5155076944455736e-06
Iter: 928 loss: 4.5119764058504491e-06
Iter: 929 loss: 4.5413036139395555e-06
Iter: 930 loss: 4.5117642309225982e-06
Iter: 931 loss: 4.5094187561385604e-06
Iter: 932 loss: 4.5083236496532905e-06
Iter: 933 loss: 4.5071686256573618e-06
Iter: 934 loss: 4.5032309630811222e-06
Iter: 935 loss: 4.5266745209509877e-06
Iter: 936 loss: 4.502732289130171e-06
Iter: 937 loss: 4.5000595224502715e-06
Iter: 938 loss: 4.5115594726630079e-06
Iter: 939 loss: 4.4995087102981839e-06
Iter: 940 loss: 4.4969595283877455e-06
Iter: 941 loss: 4.5101500722526892e-06
Iter: 942 loss: 4.4965527194241725e-06
Iter: 943 loss: 4.4958489871111317e-06
Iter: 944 loss: 4.4954796081178453e-06
Iter: 945 loss: 4.4944079693072706e-06
Iter: 946 loss: 4.4928520508393242e-06
Iter: 947 loss: 4.4928068575313668e-06
Iter: 948 loss: 4.4912332207812609e-06
Iter: 949 loss: 4.4897963693385096e-06
Iter: 950 loss: 4.4894104704146453e-06
Iter: 951 loss: 4.4870588228433404e-06
Iter: 952 loss: 4.5141783623133742e-06
Iter: 953 loss: 4.48702108600619e-06
Iter: 954 loss: 4.4854931995078505e-06
Iter: 955 loss: 4.486318503409374e-06
Iter: 956 loss: 4.4844878360274214e-06
Iter: 957 loss: 4.482593791282057e-06
Iter: 958 loss: 4.4796320784009794e-06
Iter: 959 loss: 4.479595974016359e-06
Iter: 960 loss: 4.4763673831947515e-06
Iter: 961 loss: 4.509512655404778e-06
Iter: 962 loss: 4.4762743052432214e-06
Iter: 963 loss: 4.47398636981013e-06
Iter: 964 loss: 4.4723712644839528e-06
Iter: 965 loss: 4.4715633075929032e-06
Iter: 966 loss: 4.4682509435934674e-06
Iter: 967 loss: 4.4997591030921361e-06
Iter: 968 loss: 4.4681217817821415e-06
Iter: 969 loss: 4.4657082408694739e-06
Iter: 970 loss: 4.4686900940664438e-06
Iter: 971 loss: 4.46445234492253e-06
Iter: 972 loss: 4.4615729133991542e-06
Iter: 973 loss: 4.4832704452730548e-06
Iter: 974 loss: 4.4613491225056787e-06
Iter: 975 loss: 4.4608089051834632e-06
Iter: 976 loss: 4.4603693808619492e-06
Iter: 977 loss: 4.4593980703028888e-06
Iter: 978 loss: 4.4582557008073443e-06
Iter: 979 loss: 4.458129596181392e-06
Iter: 980 loss: 4.4567706970422222e-06
Iter: 981 loss: 4.4552522964037367e-06
Iter: 982 loss: 4.4550441395280588e-06
Iter: 983 loss: 4.4529975340005643e-06
Iter: 984 loss: 4.4764315462801476e-06
Iter: 985 loss: 4.452963236456115e-06
Iter: 986 loss: 4.4514389793011959e-06
Iter: 987 loss: 4.4507743770268886e-06
Iter: 988 loss: 4.4499948003004489e-06
Iter: 989 loss: 4.4475310644353133e-06
Iter: 990 loss: 4.44674545088626e-06
Iter: 991 loss: 4.4453032688607134e-06
Iter: 992 loss: 4.4426753798835312e-06
Iter: 993 loss: 4.46250365949264e-06
Iter: 994 loss: 4.442472163660065e-06
Iter: 995 loss: 4.440158527894916e-06
Iter: 996 loss: 4.4376425947600475e-06
Iter: 997 loss: 4.4372591962489943e-06
Iter: 998 loss: 4.4341083922681067e-06
Iter: 999 loss: 4.4776153909108588e-06
Iter: 1000 loss: 4.4340978144470084e-06
Iter: 1001 loss: 4.4318774122136061e-06
Iter: 1002 loss: 4.4330294122994525e-06
Iter: 1003 loss: 4.430405125568519e-06
Iter: 1004 loss: 4.4276699348118815e-06
Iter: 1005 loss: 4.4553534697154475e-06
Iter: 1006 loss: 4.427586432083756e-06
Iter: 1007 loss: 4.4267715853744346e-06
Iter: 1008 loss: 4.4266072738187146e-06
Iter: 1009 loss: 4.4255425905956606e-06
Iter: 1010 loss: 4.4247590412398471e-06
Iter: 1011 loss: 4.4244006168176045e-06
Iter: 1012 loss: 4.4230589208474879e-06
Iter: 1013 loss: 4.4210001176719783e-06
Iter: 1014 loss: 4.4209675021101521e-06
Iter: 1015 loss: 4.4189418884877991e-06
Iter: 1016 loss: 4.4477263670732051e-06
Iter: 1017 loss: 4.4189374361037734e-06
Iter: 1018 loss: 4.4173927882837356e-06
Iter: 1019 loss: 4.4173672010216732e-06
Iter: 1020 loss: 4.4161493607146445e-06
Iter: 1021 loss: 4.4139967487430809e-06
Iter: 1022 loss: 4.4135434159306662e-06
Iter: 1023 loss: 4.41213070435245e-06
Iter: 1024 loss: 4.409616903149299e-06
Iter: 1025 loss: 4.4183302991398659e-06
Iter: 1026 loss: 4.4089556647274418e-06
Iter: 1027 loss: 4.4062360226987893e-06
Iter: 1028 loss: 4.4092987638590381e-06
Iter: 1029 loss: 4.4047691278938884e-06
Iter: 1030 loss: 4.4023376081444253e-06
Iter: 1031 loss: 4.415972474678224e-06
Iter: 1032 loss: 4.4019959894765595e-06
Iter: 1033 loss: 4.3994397730323922e-06
Iter: 1034 loss: 4.3998859603999121e-06
Iter: 1035 loss: 4.3975231615186208e-06
Iter: 1036 loss: 4.3948624507477471e-06
Iter: 1037 loss: 4.3948622302113928e-06
Iter: 1038 loss: 4.3941378413951472e-06
Iter: 1039 loss: 4.3940017900547282e-06
Iter: 1040 loss: 4.3930733710303987e-06
Iter: 1041 loss: 4.392670247213341e-06
Iter: 1042 loss: 4.3921943325976108e-06
Iter: 1043 loss: 4.3909599605625726e-06
Iter: 1044 loss: 4.3885330151447719e-06
Iter: 1045 loss: 4.4366462473931789e-06
Iter: 1046 loss: 4.3885112088227642e-06
Iter: 1047 loss: 4.386505821555774e-06
Iter: 1048 loss: 4.3865030432999466e-06
Iter: 1049 loss: 4.3850272782333275e-06
Iter: 1050 loss: 4.3850273582444993e-06
Iter: 1051 loss: 4.3838466938893542e-06
Iter: 1052 loss: 4.3817527919211734e-06
Iter: 1053 loss: 4.3821718511786628e-06
Iter: 1054 loss: 4.3801972599319034e-06
Iter: 1055 loss: 4.377940126796904e-06
Iter: 1056 loss: 4.3829711922878937e-06
Iter: 1057 loss: 4.3770812126163241e-06
Iter: 1058 loss: 4.37422916594959e-06
Iter: 1059 loss: 4.3772126649843052e-06
Iter: 1060 loss: 4.3726489718808882e-06
Iter: 1061 loss: 4.369921513912334e-06
Iter: 1062 loss: 4.3809976013151786e-06
Iter: 1063 loss: 4.3693187262301492e-06
Iter: 1064 loss: 4.3662016223204732e-06
Iter: 1065 loss: 4.3702231094690377e-06
Iter: 1066 loss: 4.36460737007017e-06
Iter: 1067 loss: 4.3622423484814419e-06
Iter: 1068 loss: 4.3622415241785221e-06
Iter: 1069 loss: 4.3610604802549733e-06
Iter: 1070 loss: 4.3792010108536021e-06
Iter: 1071 loss: 4.3610601656351679e-06
Iter: 1072 loss: 4.3596722026847e-06
Iter: 1073 loss: 4.3603415938364959e-06
Iter: 1074 loss: 4.3587403105243034e-06
Iter: 1075 loss: 4.35738909369065e-06
Iter: 1076 loss: 4.3552455073178816e-06
Iter: 1077 loss: 4.3552247458535336e-06
Iter: 1078 loss: 4.3535042995574226e-06
Iter: 1079 loss: 4.3768713901090912e-06
Iter: 1080 loss: 4.3534971568207412e-06
Iter: 1081 loss: 4.3520573125138622e-06
Iter: 1082 loss: 4.3525875283074883e-06
Iter: 1083 loss: 4.3510505156963957e-06
Iter: 1084 loss: 4.3491662481823689e-06
Iter: 1085 loss: 4.348841822213626e-06
Iter: 1086 loss: 4.3475569078624271e-06
Iter: 1087 loss: 4.3452201087067448e-06
Iter: 1088 loss: 4.3502047053522555e-06
Iter: 1089 loss: 4.3443042216370255e-06
Iter: 1090 loss: 4.3416981445950588e-06
Iter: 1091 loss: 4.3485241677267433e-06
Iter: 1092 loss: 4.3408173720197053e-06
Iter: 1093 loss: 4.3384360985643223e-06
Iter: 1094 loss: 4.3411789380839423e-06
Iter: 1095 loss: 4.3371622021438411e-06
Iter: 1096 loss: 4.3336260041364767e-06
Iter: 1097 loss: 4.343026971331828e-06
Iter: 1098 loss: 4.3324449571427985e-06
Iter: 1099 loss: 4.330469135978494e-06
Iter: 1100 loss: 4.3304689805924233e-06
Iter: 1101 loss: 4.3293553006364433e-06
Iter: 1102 loss: 4.3444206460246453e-06
Iter: 1103 loss: 4.3293504590090933e-06
Iter: 1104 loss: 4.3280555709044132e-06
Iter: 1105 loss: 4.3292409166206708e-06
Iter: 1106 loss: 4.3273061362811229e-06
Iter: 1107 loss: 4.3260549542088731e-06
Iter: 1108 loss: 4.3241893285194644e-06
Iter: 1109 loss: 4.3241478271870839e-06
Iter: 1110 loss: 4.3226316659254105e-06
Iter: 1111 loss: 4.339440349250106e-06
Iter: 1112 loss: 4.32260134207923e-06
Iter: 1113 loss: 4.3212291369440665e-06
Iter: 1114 loss: 4.3209830130262113e-06
Iter: 1115 loss: 4.3200539499042509e-06
Iter: 1116 loss: 4.3180137137078164e-06
Iter: 1117 loss: 4.3200703024267509e-06
Iter: 1118 loss: 4.3168692262480283e-06
Iter: 1119 loss: 4.3148588095633514e-06
Iter: 1120 loss: 4.3168181481724052e-06
Iter: 1121 loss: 4.3137183632211634e-06
Iter: 1122 loss: 4.3111091873563591e-06
Iter: 1123 loss: 4.3173606345670205e-06
Iter: 1124 loss: 4.3101653051014717e-06
Iter: 1125 loss: 4.3075865919432942e-06
Iter: 1126 loss: 4.3122814130679486e-06
Iter: 1127 loss: 4.3064733967686526e-06
Iter: 1128 loss: 4.3035023510705038e-06
Iter: 1129 loss: 4.3154811993922954e-06
Iter: 1130 loss: 4.3028397608965784e-06
Iter: 1131 loss: 4.3008524794945807e-06
Iter: 1132 loss: 4.317025602438397e-06
Iter: 1133 loss: 4.3007260940219409e-06
Iter: 1134 loss: 4.2992050243054984e-06
Iter: 1135 loss: 4.3126874193850692e-06
Iter: 1136 loss: 4.2991296636675524e-06
Iter: 1137 loss: 4.297383898945921e-06
Iter: 1138 loss: 4.3038290342142429e-06
Iter: 1139 loss: 4.2969544071980863e-06
Iter: 1140 loss: 4.2959423206303645e-06
Iter: 1141 loss: 4.294513823658048e-06
Iter: 1142 loss: 4.2944606902079064e-06
Iter: 1143 loss: 4.2931150415324282e-06
Iter: 1144 loss: 4.2987756058557351e-06
Iter: 1145 loss: 4.292829963576329e-06
Iter: 1146 loss: 4.2910969715798258e-06
Iter: 1147 loss: 4.2918034299477543e-06
Iter: 1148 loss: 4.2899020463887638e-06
Iter: 1149 loss: 4.2877907378489443e-06
Iter: 1150 loss: 4.2912487303059179e-06
Iter: 1151 loss: 4.286825049523324e-06
Iter: 1152 loss: 4.2849946176934522e-06
Iter: 1153 loss: 4.2856564085447139e-06
Iter: 1154 loss: 4.2837116343993047e-06
Iter: 1155 loss: 4.281217323995833e-06
Iter: 1156 loss: 4.2901267921302394e-06
Iter: 1157 loss: 4.280581557305513e-06
Iter: 1158 loss: 4.2782677933688063e-06
Iter: 1159 loss: 4.2797415706489591e-06
Iter: 1160 loss: 4.2767954692613612e-06
Iter: 1161 loss: 4.2738881394642004e-06
Iter: 1162 loss: 4.2916919326793525e-06
Iter: 1163 loss: 4.2735380874461793e-06
Iter: 1164 loss: 4.2715323004756348e-06
Iter: 1165 loss: 4.2793265412958215e-06
Iter: 1166 loss: 4.2710655257492793e-06
Iter: 1167 loss: 4.2694783996860125e-06
Iter: 1168 loss: 4.2947070361256805e-06
Iter: 1169 loss: 4.2694783904664036e-06
Iter: 1170 loss: 4.2679901554498647e-06
Iter: 1171 loss: 4.2765280051135366e-06
Iter: 1172 loss: 4.267788783170737e-06
Iter: 1173 loss: 4.2669493675940562e-06
Iter: 1174 loss: 4.265388462721665e-06
Iter: 1175 loss: 4.3006972196332574e-06
Iter: 1176 loss: 4.2653858072725944e-06
Iter: 1177 loss: 4.2638119226235581e-06
Iter: 1178 loss: 4.2680695355787268e-06
Iter: 1179 loss: 4.2632933349383804e-06
Iter: 1180 loss: 4.261153951419916e-06
Iter: 1181 loss: 4.2645293937904323e-06
Iter: 1182 loss: 4.260156788902267e-06
Iter: 1183 loss: 4.2581261550621714e-06
Iter: 1184 loss: 4.2624631557661774e-06
Iter: 1185 loss: 4.2573309827731808e-06
Iter: 1186 loss: 4.2555918741284893e-06
Iter: 1187 loss: 4.2546211622239405e-06
Iter: 1188 loss: 4.2538601728563893e-06
Iter: 1189 loss: 4.2507823585763676e-06
Iter: 1190 loss: 4.2600135371158616e-06
Iter: 1191 loss: 4.2498532090189309e-06
Iter: 1192 loss: 4.2471255817352406e-06
Iter: 1193 loss: 4.2525858321291869e-06
Iter: 1194 loss: 4.24601277706423e-06
Iter: 1195 loss: 4.2432502334945468e-06
Iter: 1196 loss: 4.2578504699411164e-06
Iter: 1197 loss: 4.2428230644206705e-06
Iter: 1198 loss: 4.2406191463585926e-06
Iter: 1199 loss: 4.2464535694141885e-06
Iter: 1200 loss: 4.2398804698093544e-06
Iter: 1201 loss: 4.2380620118145785e-06
Iter: 1202 loss: 4.23805972356261e-06
Iter: 1203 loss: 4.2366425600199659e-06
Iter: 1204 loss: 4.2526390545479822e-06
Iter: 1205 loss: 4.2366168712701418e-06
Iter: 1206 loss: 4.2360078258287435e-06
Iter: 1207 loss: 4.23465347458874e-06
Iter: 1208 loss: 4.2538499265658179e-06
Iter: 1209 loss: 4.2345844461731867e-06
Iter: 1210 loss: 4.2330133350040765e-06
Iter: 1211 loss: 4.235019577825522e-06
Iter: 1212 loss: 4.2322066297815778e-06
Iter: 1213 loss: 4.2300060188316862e-06
Iter: 1214 loss: 4.241144240808129e-06
Iter: 1215 loss: 4.2296427725675771e-06
Iter: 1216 loss: 4.2282350697252928e-06
Iter: 1217 loss: 4.2295822085487562e-06
Iter: 1218 loss: 4.2274318013251616e-06
Iter: 1219 loss: 4.2258478982355532e-06
Iter: 1220 loss: 4.2248319618845387e-06
Iter: 1221 loss: 4.22421538170223e-06
Iter: 1222 loss: 4.22168300363365e-06
Iter: 1223 loss: 4.23379308686293e-06
Iter: 1224 loss: 4.2212295501690366e-06
Iter: 1225 loss: 4.2192264111860622e-06
Iter: 1226 loss: 4.2209175179218112e-06
Iter: 1227 loss: 4.2180390412451878e-06
Iter: 1228 loss: 4.2152964081581222e-06
Iter: 1229 loss: 4.225797738418172e-06
Iter: 1230 loss: 4.2146479498433285e-06
Iter: 1231 loss: 4.212329486207357e-06
Iter: 1232 loss: 4.2182139499161917e-06
Iter: 1233 loss: 4.211525965683273e-06
Iter: 1234 loss: 4.2098577675085771e-06
Iter: 1235 loss: 4.2098054137567612e-06
Iter: 1236 loss: 4.2085803429400845e-06
Iter: 1237 loss: 4.223917345225238e-06
Iter: 1238 loss: 4.2085690931219188e-06
Iter: 1239 loss: 4.207905890195817e-06
Iter: 1240 loss: 4.2062702749580775e-06
Iter: 1241 loss: 4.2225014012762633e-06
Iter: 1242 loss: 4.2060627400337367e-06
Iter: 1243 loss: 4.2039683123263162e-06
Iter: 1244 loss: 4.2062705948597881e-06
Iter: 1245 loss: 4.2028283077692636e-06
Iter: 1246 loss: 4.2003845650060083e-06
Iter: 1247 loss: 4.2231896192250086e-06
Iter: 1248 loss: 4.200282637520671e-06
Iter: 1249 loss: 4.1989411405491936e-06
Iter: 1250 loss: 4.1994100059552191e-06
Iter: 1251 loss: 4.1979968434996063e-06
Iter: 1252 loss: 4.1961578692787163e-06
Iter: 1253 loss: 4.1943300924217395e-06
Iter: 1254 loss: 4.1939495204586987e-06
Iter: 1255 loss: 4.1910435110308737e-06
Iter: 1256 loss: 4.2092110985258886e-06
Iter: 1257 loss: 4.1907066175740276e-06
Iter: 1258 loss: 4.1884736458307028e-06
Iter: 1259 loss: 4.1904075567812486e-06
Iter: 1260 loss: 4.1871600676926565e-06
Iter: 1261 loss: 4.1843904902115288e-06
Iter: 1262 loss: 4.1963491716513536e-06
Iter: 1263 loss: 4.1838221858030016e-06
Iter: 1264 loss: 4.1814426679474785e-06
Iter: 1265 loss: 4.1873672958823368e-06
Iter: 1266 loss: 4.1806063618145085e-06
Iter: 1267 loss: 4.1787534163984842e-06
Iter: 1268 loss: 4.178744423375556e-06
Iter: 1269 loss: 4.1775032038698225e-06
Iter: 1270 loss: 4.1775025710831036e-06
Iter: 1271 loss: 4.1768502501569293e-06
Iter: 1272 loss: 4.1751824888506907e-06
Iter: 1273 loss: 4.1894440090646963e-06
Iter: 1274 loss: 4.1749049199118134e-06
Iter: 1275 loss: 4.17281808409271e-06
Iter: 1276 loss: 4.1766502201086112e-06
Iter: 1277 loss: 4.1719216944429677e-06
Iter: 1278 loss: 4.1700045831709957e-06
Iter: 1279 loss: 4.1935819172042987e-06
Iter: 1280 loss: 4.1699844511460093e-06
Iter: 1281 loss: 4.1688627571748127e-06
Iter: 1282 loss: 4.168088794308335e-06
Iter: 1283 loss: 4.1676828049743274e-06
Iter: 1284 loss: 4.165601380053579e-06
Iter: 1285 loss: 4.1645690961314786e-06
Iter: 1286 loss: 4.16358084806322e-06
Iter: 1287 loss: 4.1612698993794772e-06
Iter: 1288 loss: 4.1820014675797893e-06
Iter: 1289 loss: 4.1611598823598035e-06
Iter: 1290 loss: 4.1593013926022092e-06
Iter: 1291 loss: 4.1574165539619681e-06
Iter: 1292 loss: 4.1570490315997957e-06
Iter: 1293 loss: 4.1538737526623592e-06
Iter: 1294 loss: 4.1791835771984613e-06
Iter: 1295 loss: 4.1536603700120632e-06
Iter: 1296 loss: 4.1512857735818044e-06
Iter: 1297 loss: 4.1543209593811747e-06
Iter: 1298 loss: 4.1500666604679976e-06
Iter: 1299 loss: 4.1484239663751796e-06
Iter: 1300 loss: 4.1483118700777092e-06
Iter: 1301 loss: 4.1471624390109831e-06
Iter: 1302 loss: 4.1652449396300121e-06
Iter: 1303 loss: 4.1471623861973057e-06
Iter: 1304 loss: 4.1464121449380685e-06
Iter: 1305 loss: 4.14453584959162e-06
Iter: 1306 loss: 4.1621439545562054e-06
Iter: 1307 loss: 4.1442708437834441e-06
Iter: 1308 loss: 4.1422352742990583e-06
Iter: 1309 loss: 4.1476633218677331e-06
Iter: 1310 loss: 4.1415566522526786e-06
Iter: 1311 loss: 4.1398737003592689e-06
Iter: 1312 loss: 4.1574641339252774e-06
Iter: 1313 loss: 4.1398288847669688e-06
Iter: 1314 loss: 4.138623384630969e-06
Iter: 1315 loss: 4.1378895052601214e-06
Iter: 1316 loss: 4.1373976793395206e-06
Iter: 1317 loss: 4.1353435220045377e-06
Iter: 1318 loss: 4.1351219859252014e-06
Iter: 1319 loss: 4.1336320103401273e-06
Iter: 1320 loss: 4.1312725297818143e-06
Iter: 1321 loss: 4.1440225623582618e-06
Iter: 1322 loss: 4.1309204288997511e-06
Iter: 1323 loss: 4.1286052796066094e-06
Iter: 1324 loss: 4.1271758182979068e-06
Iter: 1325 loss: 4.1262428314158242e-06
Iter: 1326 loss: 4.1235042809826665e-06
Iter: 1327 loss: 4.1562328747491309e-06
Iter: 1328 loss: 4.1234689927544825e-06
Iter: 1329 loss: 4.121329298765966e-06
Iter: 1330 loss: 4.1217085646880059e-06
Iter: 1331 loss: 4.1197265093298321e-06
Iter: 1332 loss: 4.1181055790330009e-06
Iter: 1333 loss: 4.1179693329124564e-06
Iter: 1334 loss: 4.1169855197744213e-06
Iter: 1335 loss: 4.1169599483954192e-06
Iter: 1336 loss: 4.1163424571023868e-06
Iter: 1337 loss: 4.1147239139708213e-06
Iter: 1338 loss: 4.1270666850875832e-06
Iter: 1339 loss: 4.1144014075860389e-06
Iter: 1340 loss: 4.1125159597208894e-06
Iter: 1341 loss: 4.1174596978970263e-06
Iter: 1342 loss: 4.1118791072544369e-06
Iter: 1343 loss: 4.110237767429e-06
Iter: 1344 loss: 4.1254130152054841e-06
Iter: 1345 loss: 4.1101671091184559e-06
Iter: 1346 loss: 4.1087808467522107e-06
Iter: 1347 loss: 4.1070705345299259e-06
Iter: 1348 loss: 4.1069203378655279e-06
Iter: 1349 loss: 4.1041437184231554e-06
Iter: 1350 loss: 4.1078800571048729e-06
Iter: 1351 loss: 4.102749190481443e-06
Iter: 1352 loss: 4.1003791405147986e-06
Iter: 1353 loss: 4.11041062895489e-06
Iter: 1354 loss: 4.0998808168190726e-06
Iter: 1355 loss: 4.0974599458217719e-06
Iter: 1356 loss: 4.0968275858854407e-06
Iter: 1357 loss: 4.0953196503700741e-06
Iter: 1358 loss: 4.0926679592698039e-06
Iter: 1359 loss: 4.1209708021132159e-06
Iter: 1360 loss: 4.0926040411281528e-06
Iter: 1361 loss: 4.0904222860230736e-06
Iter: 1362 loss: 4.089872505278762e-06
Iter: 1363 loss: 4.0885001710735045e-06
Iter: 1364 loss: 4.0872596375653376e-06
Iter: 1365 loss: 4.0868797095487551e-06
Iter: 1366 loss: 4.0859783898744029e-06
Iter: 1367 loss: 4.0859654018983315e-06
Iter: 1368 loss: 4.0852719292805939e-06
Iter: 1369 loss: 4.0834857383331962e-06
Iter: 1370 loss: 4.09825623712164e-06
Iter: 1371 loss: 4.0831718184228009e-06
Iter: 1372 loss: 4.0811721849991269e-06
Iter: 1373 loss: 4.0857373551476385e-06
Iter: 1374 loss: 4.0804235676440173e-06
Iter: 1375 loss: 4.0787635633182327e-06
Iter: 1376 loss: 4.0945834593662469e-06
Iter: 1377 loss: 4.0786993661446462e-06
Iter: 1378 loss: 4.0772684875060552e-06
Iter: 1379 loss: 4.0765193239535318e-06
Iter: 1380 loss: 4.07586382464041e-06
Iter: 1381 loss: 4.0738058564767126e-06
Iter: 1382 loss: 4.0772363219377769e-06
Iter: 1383 loss: 4.0728735039944426e-06
Iter: 1384 loss: 4.0708680889125769e-06
Iter: 1385 loss: 4.0725903011352153e-06
Iter: 1386 loss: 4.0696853334432373e-06
Iter: 1387 loss: 4.0668428871927593e-06
Iter: 1388 loss: 4.0727515890379444e-06
Iter: 1389 loss: 4.0657102192711494e-06
Iter: 1390 loss: 4.06328384472562e-06
Iter: 1391 loss: 4.0713682165837095e-06
Iter: 1392 loss: 4.06261959023203e-06
Iter: 1393 loss: 4.0595992874173557e-06
Iter: 1394 loss: 4.0614657354929485e-06
Iter: 1395 loss: 4.057665463214644e-06
Iter: 1396 loss: 4.0557856315620066e-06
Iter: 1397 loss: 4.0556380662292296e-06
Iter: 1398 loss: 4.054832562428967e-06
Iter: 1399 loss: 4.0546837037299842e-06
Iter: 1400 loss: 4.0540216145035428e-06
Iter: 1401 loss: 4.0522995637378787e-06
Iter: 1402 loss: 4.0659230518785445e-06
Iter: 1403 loss: 4.0519749230939073e-06
Iter: 1404 loss: 4.0499304634911462e-06
Iter: 1405 loss: 4.0537001758585731e-06
Iter: 1406 loss: 4.0490542663694087e-06
Iter: 1407 loss: 4.0474756429055267e-06
Iter: 1408 loss: 4.067040973510835e-06
Iter: 1409 loss: 4.0474599978728389e-06
Iter: 1410 loss: 4.0461542846429481e-06
Iter: 1411 loss: 4.0449474229580395e-06
Iter: 1412 loss: 4.0446342851101312e-06
Iter: 1413 loss: 4.0422806354241039e-06
Iter: 1414 loss: 4.0453202370591412e-06
Iter: 1415 loss: 4.0410777476204237e-06
Iter: 1416 loss: 4.0388084969559458e-06
Iter: 1417 loss: 4.0422863837256341e-06
Iter: 1418 loss: 4.0377353778462251e-06
Iter: 1419 loss: 4.0348488547238763e-06
Iter: 1420 loss: 4.04086208581577e-06
Iter: 1421 loss: 4.03370004655305e-06
Iter: 1422 loss: 4.0308643535020961e-06
Iter: 1423 loss: 4.0349023614130595e-06
Iter: 1424 loss: 4.0294765662992305e-06
Iter: 1425 loss: 4.0258562301479374e-06
Iter: 1426 loss: 4.040672940667593e-06
Iter: 1427 loss: 4.025062687221719e-06
Iter: 1428 loss: 4.023152413266778e-06
Iter: 1429 loss: 4.0519417315360592e-06
Iter: 1430 loss: 4.0231513698004529e-06
Iter: 1431 loss: 4.0222157722338406e-06
Iter: 1432 loss: 4.0221321020931963e-06
Iter: 1433 loss: 4.0212564142189669e-06
Iter: 1434 loss: 4.0192043648834805e-06
Iter: 1435 loss: 4.0437561589840555e-06
Iter: 1436 loss: 4.0190331429220624e-06
Iter: 1437 loss: 4.0170109279022775e-06
Iter: 1438 loss: 4.0203181195239742e-06
Iter: 1439 loss: 4.0160852428218829e-06
Iter: 1440 loss: 4.014428417053879e-06
Iter: 1441 loss: 4.0294946538102667e-06
Iter: 1442 loss: 4.0143529763987168e-06
Iter: 1443 loss: 4.01278385903463e-06
Iter: 1444 loss: 4.012478468644667e-06
Iter: 1445 loss: 4.0114321907340366e-06
Iter: 1446 loss: 4.0092297156175014e-06
Iter: 1447 loss: 4.0119875202778534e-06
Iter: 1448 loss: 4.008089697860466e-06
Iter: 1449 loss: 4.0059080457961e-06
Iter: 1450 loss: 4.0073047059794786e-06
Iter: 1451 loss: 4.00452152090798e-06
Iter: 1452 loss: 4.0014220791207671e-06
Iter: 1453 loss: 4.0115131705649013e-06
Iter: 1454 loss: 4.0005549758778388e-06
Iter: 1455 loss: 3.9981080539089208e-06
Iter: 1456 loss: 4.0003687615199778e-06
Iter: 1457 loss: 3.9966956106085671e-06
Iter: 1458 loss: 3.99338744443212e-06
Iter: 1459 loss: 4.0099628125485563e-06
Iter: 1460 loss: 3.9928337884735264e-06
Iter: 1461 loss: 3.9905941095157737e-06
Iter: 1462 loss: 4.0023383277491613e-06
Iter: 1463 loss: 3.9902431627248705e-06
Iter: 1464 loss: 3.9896560492166594e-06
Iter: 1465 loss: 3.989101392656512e-06
Iter: 1466 loss: 3.9882555659952307e-06
Iter: 1467 loss: 3.9865345173308642e-06
Iter: 1468 loss: 4.0178391925996516e-06
Iter: 1469 loss: 3.986505760194641e-06
Iter: 1470 loss: 3.98496042116087e-06
Iter: 1471 loss: 3.9864470266458531e-06
Iter: 1472 loss: 3.9840801224015108e-06
Iter: 1473 loss: 3.9824058410068293e-06
Iter: 1474 loss: 3.9914594184394193e-06
Iter: 1475 loss: 3.9821562551399087e-06
Iter: 1476 loss: 3.9802696461743355e-06
Iter: 1477 loss: 3.9802579447589641e-06
Iter: 1478 loss: 3.9787567902536805e-06
Iter: 1479 loss: 3.9764387882758814e-06
Iter: 1480 loss: 3.9824821569441144e-06
Iter: 1481 loss: 3.9756523564411884e-06
Iter: 1482 loss: 3.9737275327002092e-06
Iter: 1483 loss: 3.9740459773731974e-06
Iter: 1484 loss: 3.9722793152990413e-06
Iter: 1485 loss: 3.9693643579491333e-06
Iter: 1486 loss: 3.9781014082845278e-06
Iter: 1487 loss: 3.9684838062624609e-06
Iter: 1488 loss: 3.9660055006027248e-06
Iter: 1489 loss: 3.970225094337166e-06
Iter: 1490 loss: 3.96489525728978e-06
Iter: 1491 loss: 3.962052847532258e-06
Iter: 1492 loss: 3.9745230203853618e-06
Iter: 1493 loss: 3.9614811831178e-06
Iter: 1494 loss: 3.9595034052226853e-06
Iter: 1495 loss: 3.9723649219960307e-06
Iter: 1496 loss: 3.9592916198744977e-06
Iter: 1497 loss: 3.9586131676485584e-06
Iter: 1498 loss: 3.9583036443847477e-06
Iter: 1499 loss: 3.9574117937248649e-06
Iter: 1500 loss: 3.9558489913630924e-06
Iter: 1501 loss: 3.9558487333277983e-06
Iter: 1502 loss: 3.9544950025180888e-06
Iter: 1503 loss: 3.954589060904953e-06
Iter: 1504 loss: 3.953439719218614e-06
Iter: 1505 loss: 3.9514927921151129e-06
Iter: 1506 loss: 3.9571831466088291e-06
Iter: 1507 loss: 3.9508915088636372e-06
Iter: 1508 loss: 3.9484433897604077e-06
Iter: 1509 loss: 3.9553499823025419e-06
Iter: 1510 loss: 3.9476642041960768e-06
Iter: 1511 loss: 3.9459874861992854e-06
Iter: 1512 loss: 3.9483152729533283e-06
Iter: 1513 loss: 3.9451568921485313e-06
Iter: 1514 loss: 3.9431374767104662e-06
Iter: 1515 loss: 3.9411407525807384e-06
Iter: 1516 loss: 3.940717999190363e-06
Iter: 1517 loss: 3.9376964847536848e-06
Iter: 1518 loss: 3.9657654786642235e-06
Iter: 1519 loss: 3.9375687825415292e-06
Iter: 1520 loss: 3.9356737168615378e-06
Iter: 1521 loss: 3.9360754110866144e-06
Iter: 1522 loss: 3.9342717342334388e-06
Iter: 1523 loss: 3.931304936260555e-06
Iter: 1524 loss: 3.9407540897887988e-06
Iter: 1525 loss: 3.9304578572375849e-06
Iter: 1526 loss: 3.9281656652613319e-06
Iter: 1527 loss: 3.9409648106187246e-06
Iter: 1528 loss: 3.9278405128252205e-06
Iter: 1529 loss: 3.9278824237898578e-06
Iter: 1530 loss: 3.9269360335306392e-06
Iter: 1531 loss: 3.9261652246865336e-06
Iter: 1532 loss: 3.9248087444867319e-06
Iter: 1533 loss: 3.924808630194983e-06
Iter: 1534 loss: 3.92349218418774e-06
Iter: 1535 loss: 3.9232595280942594e-06
Iter: 1536 loss: 3.9223659137493263e-06
Iter: 1537 loss: 3.9204698909673462e-06
Iter: 1538 loss: 3.9269973553878205e-06
Iter: 1539 loss: 3.9199678552534116e-06
Iter: 1540 loss: 3.9179073742118252e-06
Iter: 1541 loss: 3.9261812942184217e-06
Iter: 1542 loss: 3.9174459164203131e-06
Iter: 1543 loss: 3.9161503229218335e-06
Iter: 1544 loss: 3.9173755060084185e-06
Iter: 1545 loss: 3.9154082762730476e-06
Iter: 1546 loss: 3.9136710577640828e-06
Iter: 1547 loss: 3.9120520667775455e-06
Iter: 1548 loss: 3.91164186810731e-06
Iter: 1549 loss: 3.9091601081382844e-06
Iter: 1550 loss: 3.9280025857113017e-06
Iter: 1551 loss: 3.90897083109105e-06
Iter: 1552 loss: 3.9068142572745682e-06
Iter: 1553 loss: 3.90604861833213e-06
Iter: 1554 loss: 3.9048361469186944e-06
Iter: 1555 loss: 3.9017050980325971e-06
Iter: 1556 loss: 3.92068367001395e-06
Iter: 1557 loss: 3.901320495389778e-06
Iter: 1558 loss: 3.8991986504544517e-06
Iter: 1559 loss: 3.9099331615272839e-06
Iter: 1560 loss: 3.8988489032223757e-06
Iter: 1561 loss: 3.8983262735787644e-06
Iter: 1562 loss: 3.89787124318756e-06
Iter: 1563 loss: 3.89686679042931e-06
Iter: 1564 loss: 3.895466784159703e-06
Iter: 1565 loss: 3.89540929435011e-06
Iter: 1566 loss: 3.8940876892661338e-06
Iter: 1567 loss: 3.8937437402230724e-06
Iter: 1568 loss: 3.8929196764900071e-06
Iter: 1569 loss: 3.8910952518635557e-06
Iter: 1570 loss: 3.8970917186186874e-06
Iter: 1571 loss: 3.8905893497561631e-06
Iter: 1572 loss: 3.8887080619341537e-06
Iter: 1573 loss: 3.9002938688991483e-06
Iter: 1574 loss: 3.8884837835646689e-06
Iter: 1575 loss: 3.8872910871495144e-06
Iter: 1576 loss: 3.886555008530836e-06
Iter: 1577 loss: 3.8860741283181059e-06
Iter: 1578 loss: 3.8837664622799457e-06
Iter: 1579 loss: 3.883166559065993e-06
Iter: 1580 loss: 3.8817274763474815e-06
Iter: 1581 loss: 3.8794603048883392e-06
Iter: 1582 loss: 3.9010417547295428e-06
Iter: 1583 loss: 3.87937241575642e-06
Iter: 1584 loss: 3.8774453909827327e-06
Iter: 1585 loss: 3.8757336292205868e-06
Iter: 1586 loss: 3.8752374375635564e-06
Iter: 1587 loss: 3.8724705008598485e-06
Iter: 1588 loss: 3.8994235538937871e-06
Iter: 1589 loss: 3.8723720946612813e-06
Iter: 1590 loss: 3.8704830542270076e-06
Iter: 1591 loss: 3.8730573069419321e-06
Iter: 1592 loss: 3.8695386740701643e-06
Iter: 1593 loss: 3.8702256735171662e-06
Iter: 1594 loss: 3.8686891715617452e-06
Iter: 1595 loss: 3.8679034706625144e-06
Iter: 1596 loss: 3.8668515890791339e-06
Iter: 1597 loss: 3.8667939338846055e-06
Iter: 1598 loss: 3.8655903034535819e-06
Iter: 1599 loss: 3.8645675756086627e-06
Iter: 1600 loss: 3.8642344532589775e-06
Iter: 1601 loss: 3.8622908758703233e-06
Iter: 1602 loss: 3.8689302933542012e-06
Iter: 1603 loss: 3.8617722591329912e-06
Iter: 1604 loss: 3.8598615816257231e-06
Iter: 1605 loss: 3.8711663181310736e-06
Iter: 1606 loss: 3.85961669073622e-06
Iter: 1607 loss: 3.8582307852248736e-06
Iter: 1608 loss: 3.8576648511808686e-06
Iter: 1609 loss: 3.856932177220783e-06
Iter: 1610 loss: 3.8547485835578638e-06
Iter: 1611 loss: 3.8570791560325617e-06
Iter: 1612 loss: 3.8535473802427511e-06
Iter: 1613 loss: 3.851440837303823e-06
Iter: 1614 loss: 3.8583526691429143e-06
Iter: 1615 loss: 3.8508556397881637e-06
Iter: 1616 loss: 3.8484139632972376e-06
Iter: 1617 loss: 3.8492050085113537e-06
Iter: 1618 loss: 3.84667958409256e-06
Iter: 1619 loss: 3.84447265186789e-06
Iter: 1620 loss: 3.8655048054670871e-06
Iter: 1621 loss: 3.8443872481583895e-06
Iter: 1622 loss: 3.8424708482251e-06
Iter: 1623 loss: 3.842689206124057e-06
Iter: 1624 loss: 3.8410022299138745e-06
Iter: 1625 loss: 3.8416637929595045e-06
Iter: 1626 loss: 3.8399933264676793e-06
Iter: 1627 loss: 3.8389756362613767e-06
Iter: 1628 loss: 3.838554128618952e-06
Iter: 1629 loss: 3.8380199288676089e-06
Iter: 1630 loss: 3.8370056547890284e-06
Iter: 1631 loss: 3.8355492266461773e-06
Iter: 1632 loss: 3.8355023960764377e-06
Iter: 1633 loss: 3.8335081853245527e-06
Iter: 1634 loss: 3.84053915689099e-06
Iter: 1635 loss: 3.8329926711695783e-06
Iter: 1636 loss: 3.8312062524332024e-06
Iter: 1637 loss: 3.8461535463018694e-06
Iter: 1638 loss: 3.8311010807239262e-06
Iter: 1639 loss: 3.8298301254999071e-06
Iter: 1640 loss: 3.8286904082340583e-06
Iter: 1641 loss: 3.8283685127028948e-06
Iter: 1642 loss: 3.826216198151123e-06
Iter: 1643 loss: 3.8304125364926525e-06
Iter: 1644 loss: 3.8253236309497467e-06
Iter: 1645 loss: 3.8233845286311932e-06
Iter: 1646 loss: 3.8288152189069847e-06
Iter: 1647 loss: 3.8227636952886555e-06
Iter: 1648 loss: 3.8205834048538006e-06
Iter: 1649 loss: 3.821492930040396e-06
Iter: 1650 loss: 3.8190849996405889e-06
Iter: 1651 loss: 3.8167343799713415e-06
Iter: 1652 loss: 3.8330045254102475e-06
Iter: 1653 loss: 3.8165128572852385e-06
Iter: 1654 loss: 3.814286125750954e-06
Iter: 1655 loss: 3.8138640237609224e-06
Iter: 1656 loss: 3.8123701493395011e-06
Iter: 1657 loss: 3.8157017332025442e-06
Iter: 1658 loss: 3.811596790280127e-06
Iter: 1659 loss: 3.810857401718e-06
Iter: 1660 loss: 3.8104659969348166e-06
Iter: 1661 loss: 3.8101297376670053e-06
Iter: 1662 loss: 3.8091488174713421e-06
Iter: 1663 loss: 3.807368603779576e-06
Iter: 1664 loss: 3.8498811736231032e-06
Iter: 1665 loss: 3.80736797549895e-06
Iter: 1666 loss: 3.8053253183716945e-06
Iter: 1667 loss: 3.8163756752479871e-06
Iter: 1668 loss: 3.8050210296474812e-06
Iter: 1669 loss: 3.8036400010533517e-06
Iter: 1670 loss: 3.8153702555633168e-06
Iter: 1671 loss: 3.803562130377326e-06
Iter: 1672 loss: 3.8024641093959893e-06
Iter: 1673 loss: 3.8014310580988394e-06
Iter: 1674 loss: 3.8011764975228843e-06
Iter: 1675 loss: 3.7993573709046042e-06
Iter: 1676 loss: 3.8030574446066537e-06
Iter: 1677 loss: 3.7986222892836559e-06
Iter: 1678 loss: 3.7967450964142526e-06
Iter: 1679 loss: 3.798676410286112e-06
Iter: 1680 loss: 3.7956992192830134e-06
Iter: 1681 loss: 3.7932928477742391e-06
Iter: 1682 loss: 3.7993911240990266e-06
Iter: 1683 loss: 3.7924584475157773e-06
Iter: 1684 loss: 3.7904541326926066e-06
Iter: 1685 loss: 3.7977179430494503e-06
Iter: 1686 loss: 3.78995053853405e-06
Iter: 1687 loss: 3.7876692295135661e-06
Iter: 1688 loss: 3.7902399288724542e-06
Iter: 1689 loss: 3.7864399673128547e-06
Iter: 1690 loss: 3.7868824052800576e-06
Iter: 1691 loss: 3.785620974419963e-06
Iter: 1692 loss: 3.78470299834932e-06
Iter: 1693 loss: 3.7852678293576784e-06
Iter: 1694 loss: 3.7841146965376837e-06
Iter: 1695 loss: 3.7832615549339634e-06
Iter: 1696 loss: 3.7815192392094426e-06
Iter: 1697 loss: 3.8129278448195926e-06
Iter: 1698 loss: 3.78148842489623e-06
Iter: 1699 loss: 3.7796430873733e-06
Iter: 1700 loss: 3.7916790184688746e-06
Iter: 1701 loss: 3.7794462287549589e-06
Iter: 1702 loss: 3.7781936688411626e-06
Iter: 1703 loss: 3.7875769923899963e-06
Iter: 1704 loss: 3.7780950686125342e-06
Iter: 1705 loss: 3.776911352940995e-06
Iter: 1706 loss: 3.7752306152425573e-06
Iter: 1707 loss: 3.7751710918275831e-06
Iter: 1708 loss: 3.773053158047e-06
Iter: 1709 loss: 3.7823757444336187e-06
Iter: 1710 loss: 3.7726292846106e-06
Iter: 1711 loss: 3.7710423620202527e-06
Iter: 1712 loss: 3.77238707493611e-06
Iter: 1713 loss: 3.7701028758289726e-06
Iter: 1714 loss: 3.768019442517705e-06
Iter: 1715 loss: 3.7728418628977212e-06
Iter: 1716 loss: 3.7672470163407969e-06
Iter: 1717 loss: 3.7652366781641163e-06
Iter: 1718 loss: 3.7705813260741977e-06
Iter: 1719 loss: 3.7645654677750687e-06
Iter: 1720 loss: 3.7624265289035845e-06
Iter: 1721 loss: 3.7681906634675816e-06
Iter: 1722 loss: 3.7617184311457773e-06
Iter: 1723 loss: 3.7615105334088165e-06
Iter: 1724 loss: 3.7609416652076926e-06
Iter: 1725 loss: 3.7600433032374984e-06
Iter: 1726 loss: 3.7608916421250346e-06
Iter: 1727 loss: 3.7595281727898865e-06
Iter: 1728 loss: 3.7586257276566773e-06
Iter: 1729 loss: 3.7570071782874019e-06
Iter: 1730 loss: 3.7967004999021e-06
Iter: 1731 loss: 3.7570070755084643e-06
Iter: 1732 loss: 3.7554734619787636e-06
Iter: 1733 loss: 3.7647363615261767e-06
Iter: 1734 loss: 3.7552839878180088e-06
Iter: 1735 loss: 3.7541544272889458e-06
Iter: 1736 loss: 3.7590665976763219e-06
Iter: 1737 loss: 3.75392480146707e-06
Iter: 1738 loss: 3.7525917109722338e-06
Iter: 1739 loss: 3.7518195762493793e-06
Iter: 1740 loss: 3.7512528926379833e-06
Iter: 1741 loss: 3.7495950622303728e-06
Iter: 1742 loss: 3.7565240390422737e-06
Iter: 1743 loss: 3.7492410980210765e-06
Iter: 1744 loss: 3.747817680655773e-06
Iter: 1745 loss: 3.7469581082633556e-06
Iter: 1746 loss: 3.7463730529831218e-06
Iter: 1747 loss: 3.7440446041782959e-06
Iter: 1748 loss: 3.757415339690323e-06
Iter: 1749 loss: 3.7437302701265753e-06
Iter: 1750 loss: 3.742149944168505e-06
Iter: 1751 loss: 3.7430318603360442e-06
Iter: 1752 loss: 3.7411162254666783e-06
Iter: 1753 loss: 3.7388759750538248e-06
Iter: 1754 loss: 3.7491734341182289e-06
Iter: 1755 loss: 3.738453397536197e-06
Iter: 1756 loss: 3.7372412554897348e-06
Iter: 1757 loss: 3.7372404125306387e-06
Iter: 1758 loss: 3.7359557631798514e-06
Iter: 1759 loss: 3.7438860599980189e-06
Iter: 1760 loss: 3.73580409790602e-06
Iter: 1761 loss: 3.7352101810521305e-06
Iter: 1762 loss: 3.7340581700568694e-06
Iter: 1763 loss: 3.7576542820075407e-06
Iter: 1764 loss: 3.7340505104020957e-06
Iter: 1765 loss: 3.7328445200391564e-06
Iter: 1766 loss: 3.7360561108704309e-06
Iter: 1767 loss: 3.7324420537054213e-06
Iter: 1768 loss: 3.7312836961413066e-06
Iter: 1769 loss: 3.7382547329222579e-06
Iter: 1770 loss: 3.7311396988468851e-06
Iter: 1771 loss: 3.7299677377366214e-06
Iter: 1772 loss: 3.7304675276257538e-06
Iter: 1773 loss: 3.7291647360590638e-06
Iter: 1774 loss: 3.7279883333003812e-06
Iter: 1775 loss: 3.7308115012277672e-06
Iter: 1776 loss: 3.7275634808876005e-06
Iter: 1777 loss: 3.7263335011953187e-06
Iter: 1778 loss: 3.725875013123085e-06
Iter: 1779 loss: 3.7251975684981536e-06
Iter: 1780 loss: 3.7234946833750419e-06
Iter: 1781 loss: 3.7355186129494039e-06
Iter: 1782 loss: 3.7233408845379914e-06
Iter: 1783 loss: 3.7220322040813957e-06
Iter: 1784 loss: 3.721111364527519e-06
Iter: 1785 loss: 3.7206476897980534e-06
Iter: 1786 loss: 3.7188432079791688e-06
Iter: 1787 loss: 3.7392232980387714e-06
Iter: 1788 loss: 3.7188102391742417e-06
Iter: 1789 loss: 3.7179884521953367e-06
Iter: 1790 loss: 3.7283396882578041e-06
Iter: 1791 loss: 3.7179813867439133e-06
Iter: 1792 loss: 3.7171515542695606e-06
Iter: 1793 loss: 3.7232129258159112e-06
Iter: 1794 loss: 3.7170818164627392e-06
Iter: 1795 loss: 3.716621708899314e-06
Iter: 1796 loss: 3.7157005285325043e-06
Iter: 1797 loss: 3.7331586811945262e-06
Iter: 1798 loss: 3.7156887974121372e-06
Iter: 1799 loss: 3.7147168610475637e-06
Iter: 1800 loss: 3.7164217027212996e-06
Iter: 1801 loss: 3.7142884243139668e-06
Iter: 1802 loss: 3.7132538937909264e-06
Iter: 1803 loss: 3.7190389230841834e-06
Iter: 1804 loss: 3.7131079798343913e-06
Iter: 1805 loss: 3.7121065180920167e-06
Iter: 1806 loss: 3.7139815792315684e-06
Iter: 1807 loss: 3.7116812449217257e-06
Iter: 1808 loss: 3.7107749293518034e-06
Iter: 1809 loss: 3.7107083135481539e-06
Iter: 1810 loss: 3.7100293546184328e-06
Iter: 1811 loss: 3.7085465031341331e-06
Iter: 1812 loss: 3.7093571948522862e-06
Iter: 1813 loss: 3.7075724684673508e-06
Iter: 1814 loss: 3.7061426258601627e-06
Iter: 1815 loss: 3.7178584472683546e-06
Iter: 1816 loss: 3.7060535379878593e-06
Iter: 1817 loss: 3.7049175264327552e-06
Iter: 1818 loss: 3.7039247411074293e-06
Iter: 1819 loss: 3.7036242171724784e-06
Iter: 1820 loss: 3.7020834579630766e-06
Iter: 1821 loss: 3.7200396761454096e-06
Iter: 1822 loss: 3.7020603635228319e-06
Iter: 1823 loss: 3.7011251718498218e-06
Iter: 1824 loss: 3.7057442809759353e-06
Iter: 1825 loss: 3.7009644630920354e-06
Iter: 1826 loss: 3.7001637094110076e-06
Iter: 1827 loss: 3.7001515528504318e-06
Iter: 1828 loss: 3.6997898632372403e-06
Iter: 1829 loss: 3.6989513749878739e-06
Iter: 1830 loss: 3.709356979870596e-06
Iter: 1831 loss: 3.6988877620584975e-06
Iter: 1832 loss: 3.697955841257026e-06
Iter: 1833 loss: 3.6993716008739686e-06
Iter: 1834 loss: 3.6975131702256088e-06
Iter: 1835 loss: 3.6965109929618349e-06
Iter: 1836 loss: 3.7025916903277861e-06
Iter: 1837 loss: 3.696388162705968e-06
Iter: 1838 loss: 3.6954465186734774e-06
Iter: 1839 loss: 3.6979042198215e-06
Iter: 1840 loss: 3.6951271176819877e-06
Iter: 1841 loss: 3.694280930306072e-06
Iter: 1842 loss: 3.6939586473545127e-06
Iter: 1843 loss: 3.69349691498874e-06
Iter: 1844 loss: 3.6922145740319337e-06
Iter: 1845 loss: 3.6955255602147542e-06
Iter: 1846 loss: 3.6917763020378867e-06
Iter: 1847 loss: 3.6906775238369592e-06
Iter: 1848 loss: 3.6927641742948415e-06
Iter: 1849 loss: 3.6902143126353163e-06
Iter: 1850 loss: 3.6888044917033986e-06
Iter: 1851 loss: 3.6904723442498511e-06
Iter: 1852 loss: 3.688058183911293e-06
Iter: 1853 loss: 3.6868994344845025e-06
Iter: 1854 loss: 3.6957507761060074e-06
Iter: 1855 loss: 3.6868123361701967e-06
Iter: 1856 loss: 3.6858803104797922e-06
Iter: 1857 loss: 3.688216801701511e-06
Iter: 1858 loss: 3.6855554555084072e-06
Iter: 1859 loss: 3.6850379792787078e-06
Iter: 1860 loss: 3.684866238436525e-06
Iter: 1861 loss: 3.6845477422772667e-06
Iter: 1862 loss: 3.6837872388190981e-06
Iter: 1863 loss: 3.6923059787110859e-06
Iter: 1864 loss: 3.683712793620251e-06
Iter: 1865 loss: 3.6829236209531175e-06
Iter: 1866 loss: 3.6840356058611234e-06
Iter: 1867 loss: 3.6825355528906841e-06
Iter: 1868 loss: 3.6816447227531731e-06
Iter: 1869 loss: 3.686266528085953e-06
Iter: 1870 loss: 3.6815031559761033e-06
Iter: 1871 loss: 3.6807382179711955e-06
Iter: 1872 loss: 3.6842418222356544e-06
Iter: 1873 loss: 3.68059293321247e-06
Iter: 1874 loss: 3.6799962384570976e-06
Iter: 1875 loss: 3.6794786879821927e-06
Iter: 1876 loss: 3.6793187225524142e-06
Iter: 1877 loss: 3.67831795442856e-06
Iter: 1878 loss: 3.6814815452467823e-06
Iter: 1879 loss: 3.6780299669849996e-06
Iter: 1880 loss: 3.6771509596601764e-06
Iter: 1881 loss: 3.6778484467236867e-06
Iter: 1882 loss: 3.6766211809810325e-06
Iter: 1883 loss: 3.6754364768262486e-06
Iter: 1884 loss: 3.67917565354997e-06
Iter: 1885 loss: 3.6750949359774702e-06
Iter: 1886 loss: 3.6741144985105376e-06
Iter: 1887 loss: 3.6759918678371687e-06
Iter: 1888 loss: 3.6737034907487905e-06
Iter: 1889 loss: 3.6724686352037638e-06
Iter: 1890 loss: 3.6748380328484287e-06
Iter: 1891 loss: 3.6719495714295676e-06
Iter: 1892 loss: 3.6734137602216586e-06
Iter: 1893 loss: 3.6715029807767128e-06
Iter: 1894 loss: 3.6712742947485802e-06
Iter: 1895 loss: 3.6706754553642837e-06
Iter: 1896 loss: 3.6752692623561357e-06
Iter: 1897 loss: 3.6705570243398165e-06
Iter: 1898 loss: 3.6698562300956504e-06
Iter: 1899 loss: 3.669793235578815e-06
Iter: 1900 loss: 3.6692761877107867e-06
Iter: 1901 loss: 3.6683056405564323e-06
Iter: 1902 loss: 3.6751788783393297e-06
Iter: 1903 loss: 3.6682186028609381e-06
Iter: 1904 loss: 3.6675453835101807e-06
Iter: 1905 loss: 3.6727649279658862e-06
Iter: 1906 loss: 3.6674966914807526e-06
Iter: 1907 loss: 3.6670311975205553e-06
Iter: 1908 loss: 3.6664702890339915e-06
Iter: 1909 loss: 3.6664150411909288e-06
Iter: 1910 loss: 3.6655864131745225e-06
Iter: 1911 loss: 3.6683506907162265e-06
Iter: 1912 loss: 3.6653599329244945e-06
Iter: 1913 loss: 3.6645744502504108e-06
Iter: 1914 loss: 3.6646599639425448e-06
Iter: 1915 loss: 3.6639708306945753e-06
Iter: 1916 loss: 3.6629921238857655e-06
Iter: 1917 loss: 3.6699528122512418e-06
Iter: 1918 loss: 3.6629052808914973e-06
Iter: 1919 loss: 3.6622035417101476e-06
Iter: 1920 loss: 3.66286674374629e-06
Iter: 1921 loss: 3.6618014862909968e-06
Iter: 1922 loss: 3.6609141007127383e-06
Iter: 1923 loss: 3.6632031279410637e-06
Iter: 1924 loss: 3.6606110460295189e-06
Iter: 1925 loss: 3.6617272589106824e-06
Iter: 1926 loss: 3.660370745120691e-06
Iter: 1927 loss: 3.6601920780861919e-06
Iter: 1928 loss: 3.6596807707289203e-06
Iter: 1929 loss: 3.6620044653181992e-06
Iter: 1930 loss: 3.6594939987532756e-06
Iter: 1931 loss: 3.6587410914487829e-06
Iter: 1932 loss: 3.6592377171213019e-06
Iter: 1933 loss: 3.6582658469287396e-06
Iter: 1934 loss: 3.6574892117226666e-06
Iter: 1935 loss: 3.6631192001202265e-06
Iter: 1936 loss: 3.6574231172327585e-06
Iter: 1937 loss: 3.6568168268289388e-06
Iter: 1938 loss: 3.6594323768959692e-06
Iter: 1939 loss: 3.6566923235053088e-06
Iter: 1940 loss: 3.6559984776669424e-06
Iter: 1941 loss: 3.6548689219190703e-06
Iter: 1942 loss: 3.6548622420671845e-06
Iter: 1943 loss: 3.6535303846596319e-06
Iter: 1944 loss: 3.6607961442512729e-06
Iter: 1945 loss: 3.653335015358463e-06
Iter: 1946 loss: 3.6523436095561303e-06
Iter: 1947 loss: 3.6531898037110913e-06
Iter: 1948 loss: 3.6517582004469576e-06
Iter: 1949 loss: 3.6506867659973362e-06
Iter: 1950 loss: 3.6548177846945893e-06
Iter: 1951 loss: 3.6504351173409791e-06
Iter: 1952 loss: 3.6493469084337819e-06
Iter: 1953 loss: 3.6501981776941429e-06
Iter: 1954 loss: 3.6486881974074929e-06
Iter: 1955 loss: 3.6475469406966616e-06
Iter: 1956 loss: 3.6546276914754543e-06
Iter: 1957 loss: 3.6474123474444403e-06
Iter: 1958 loss: 3.6480248425534697e-06
Iter: 1959 loss: 3.6471453680499263e-06
Iter: 1960 loss: 3.6468636425095333e-06
Iter: 1961 loss: 3.6461335342118857e-06
Iter: 1962 loss: 3.6520216168139737e-06
Iter: 1963 loss: 3.6459995809008124e-06
Iter: 1964 loss: 3.6452464966850637e-06
Iter: 1965 loss: 3.6463226174672576e-06
Iter: 1966 loss: 3.6448783059832346e-06
Iter: 1967 loss: 3.6442356563021413e-06
Iter: 1968 loss: 3.6481666040921466e-06
Iter: 1969 loss: 3.6441581243892031e-06
Iter: 1970 loss: 3.6435961132775743e-06
Iter: 1971 loss: 3.6461379930058003e-06
Iter: 1972 loss: 3.6434875575379265e-06
Iter: 1973 loss: 3.6428967687924874e-06
Iter: 1974 loss: 3.6424304795291353e-06
Iter: 1975 loss: 3.642248502218825e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.4/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi2.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.8 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi2.8
+ date
Sun Nov  8 01:34:23 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.4/300_300_300_1 --function f1 --psi 2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e904d6598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e9050c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e9050c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e90454ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e90472b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e903d3378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e90351e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e90472510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e903bd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e903bc488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e903d3158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e902f6d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e902f68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e880e8f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e8808c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e88066d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e88085400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e88085b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87fec158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87fecea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87fbd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87fd3158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87ede950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87fbdea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87f03840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87e8a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87f426a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87f42730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87f4b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87f6abf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87dd8d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87da07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87dbbb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87dad9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87e9e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f0e87ebeea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.029511556384762463
test_loss: 0.030132697625736463
train_loss: 0.019448268610956623
test_loss: 0.02209886089723116
train_loss: 0.017166258483287587
test_loss: 0.0202995238707992
train_loss: 0.01473816742222599
test_loss: 0.019305158990628526
train_loss: 0.014955735588029899
test_loss: 0.01742731959031537
train_loss: 0.012215818884089739
test_loss: 0.01632923244100515
train_loss: 0.012698332931839567
test_loss: 0.016697771352843484
train_loss: 0.012833171462303544
test_loss: 0.015194662497050758
train_loss: 0.01258236188899093
test_loss: 0.016085758108463456
train_loss: 0.011721891534336715
test_loss: 0.015149882736790002
train_loss: 0.011492625446692318
test_loss: 0.015177495427978884
train_loss: 0.009942772033485791
test_loss: 0.014829117347863522
train_loss: 0.010423349346286546
test_loss: 0.014060586037069875
train_loss: 0.010342030782706735
test_loss: 0.014079077381778039
train_loss: 0.010236041392825659
test_loss: 0.014043385365106386
train_loss: 0.01155203649774691
test_loss: 0.013735432835331398
train_loss: 0.010625445518862646
test_loss: 0.014351323428862073
train_loss: 0.009245982905240025
test_loss: 0.0135296902396087
train_loss: 0.009742371095958316
test_loss: 0.013420842674522154
train_loss: 0.009397608944317225
test_loss: 0.013230563958781364
train_loss: 0.009111172928183444
test_loss: 0.013508699416478804
train_loss: 0.009753343614206007
test_loss: 0.013768056795789161
train_loss: 0.00932565100007996
test_loss: 0.013669309987854844
train_loss: 0.009237788988530561
test_loss: 0.0129948948180019
train_loss: 0.009119415062623697
test_loss: 0.012745940149255957
train_loss: 0.009045240826226694
test_loss: 0.013305879674205595
train_loss: 0.008431447571501268
test_loss: 0.012276444860294672
train_loss: 0.008141192503774056
test_loss: 0.012037059046674257
train_loss: 0.008500545725214647
test_loss: 0.012952465366857478
train_loss: 0.008799977232488635
test_loss: 0.012406585287313776
train_loss: 0.008478583379632356
test_loss: 0.012366535542603283
train_loss: 0.007967350102541758
test_loss: 0.012655370664195434
train_loss: 0.007820958241787065
test_loss: 0.012185470548108904
train_loss: 0.0076204681986212965
test_loss: 0.012584668616677586
train_loss: 0.007929692961381083
test_loss: 0.012050467117029352
train_loss: 0.007551630964897318
test_loss: 0.01186339479459078
train_loss: 0.008551920204697378
test_loss: 0.011927220735395656
train_loss: 0.006986207768517795
test_loss: 0.01216481377901565
train_loss: 0.008794220636617096
test_loss: 0.011936279225213179
train_loss: 0.007074222423208907
test_loss: 0.01167661790947477
train_loss: 0.0075252964734216984
test_loss: 0.011497080426943597
train_loss: 0.007838346775842709
test_loss: 0.011662820670178231
train_loss: 0.007373403696586324
test_loss: 0.011865843871575982
train_loss: 0.008537105661025586
test_loss: 0.011867040128329545
train_loss: 0.006914505530715377
test_loss: 0.011370476198804552
train_loss: 0.007612128407347326
test_loss: 0.011515362226733952
train_loss: 0.0082442491999297
test_loss: 0.011591232768105652
train_loss: 0.0071302207008893705
test_loss: 0.011341976026570968
train_loss: 0.007138035899889704
test_loss: 0.01193097942078603
train_loss: 0.007728792569086608
test_loss: 0.012112610437284309
train_loss: 0.0069164677784680495
test_loss: 0.011071122319578422
train_loss: 0.008401355114946146
test_loss: 0.011529802929443691
train_loss: 0.007548773432191428
test_loss: 0.011048199396497628
train_loss: 0.00665149829975124
test_loss: 0.011303374155869125
train_loss: 0.006997352219231159
test_loss: 0.011278878115336427
train_loss: 0.0064306160889930275
test_loss: 0.01097186290290471
train_loss: 0.007241325756567731
test_loss: 0.01172715373097159
train_loss: 0.0074723250802196245
test_loss: 0.011452660387699367
train_loss: 0.007932060409494224
test_loss: 0.011317338220620385
train_loss: 0.00693919956072268
test_loss: 0.01089330899488611
train_loss: 0.0068351056746950075
test_loss: 0.010801634579074994
train_loss: 0.006941389618778609
test_loss: 0.011022379335398845
train_loss: 0.00826270386919758
test_loss: 0.011143827293358041
train_loss: 0.006609042515381632
test_loss: 0.010996666790056791
train_loss: 0.007909934638951427
test_loss: 0.011791759953525396
train_loss: 0.006421579606958272
test_loss: 0.011022216177097822
train_loss: 0.007193414983575024
test_loss: 0.011322784073483805
train_loss: 0.007077647612684583
test_loss: 0.011038478855335225
train_loss: 0.006236767274546114
test_loss: 0.010943894027670447
train_loss: 0.007094504718396275
test_loss: 0.010936559537335232
train_loss: 0.006491863634058214
test_loss: 0.011273272491292594
train_loss: 0.0064158086164853335
test_loss: 0.010499076315847074
train_loss: 0.007198052081576984
test_loss: 0.011226790779111996
train_loss: 0.0069922522837676596
test_loss: 0.01076141033231373
train_loss: 0.0064963623674442455
test_loss: 0.011225056942488296
train_loss: 0.006314209890671167
test_loss: 0.01128593767092484
train_loss: 0.007497252415572317
test_loss: 0.011259731863949934
train_loss: 0.006643406807462919
test_loss: 0.010661811504223247
train_loss: 0.006607799954175142
test_loss: 0.010924172629925523
train_loss: 0.006645806580786542
test_loss: 0.011044882425788062
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi2.8/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 2.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi2.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f741cd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f741cd598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f741b8c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f741b86a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f74174488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f74174b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f74181730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f740e9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f740d7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f740d7950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f740d7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f74076e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f74076840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f740477b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f607d7840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f607a3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f607a3950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f60783620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f606fd620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f606fd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f6070a6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f6070ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f6068d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f606ac0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f606ac8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f6061e378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f6064aae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f605d0840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f605d0158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f605fa598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f605b0bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f6053f488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f6057c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f6057cc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f60533a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f5f60533510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 9.9394629922353527e-05
Iter: 2 loss: 9.4349403043428318e-05
Iter: 3 loss: 9.1749269800082251e-05
Iter: 4 loss: 8.4234553578510526e-05
Iter: 5 loss: 8.4218108968144186e-05
Iter: 6 loss: 7.8298744612806907e-05
Iter: 7 loss: 7.0300894431559146e-05
Iter: 8 loss: 6.2059993396611278e-05
Iter: 9 loss: 6.0533349658203756e-05
Iter: 10 loss: 5.5237830298281138e-05
Iter: 11 loss: 0.00012961686957678735
Iter: 12 loss: 5.52253784542753e-05
Iter: 13 loss: 5.1274551670843926e-05
Iter: 14 loss: 4.5351552871505311e-05
Iter: 15 loss: 4.5224559312933234e-05
Iter: 16 loss: 4.1230616751812981e-05
Iter: 17 loss: 4.1177583653403e-05
Iter: 18 loss: 3.823757166356339e-05
Iter: 19 loss: 3.630222560925783e-05
Iter: 20 loss: 3.5184105541967066e-05
Iter: 21 loss: 3.20310719316444e-05
Iter: 22 loss: 6.6371820530769624e-05
Iter: 23 loss: 3.196251868725137e-05
Iter: 24 loss: 3.0000035443888334e-05
Iter: 25 loss: 3.5060000137274774e-05
Iter: 26 loss: 2.9323155656877975e-05
Iter: 27 loss: 2.7415036771442964e-05
Iter: 28 loss: 3.0205369667658195e-05
Iter: 29 loss: 2.6492666598839891e-05
Iter: 30 loss: 2.4923873555111956e-05
Iter: 31 loss: 3.5740324726507219e-05
Iter: 32 loss: 2.4773154508719748e-05
Iter: 33 loss: 2.3977173246843355e-05
Iter: 34 loss: 2.9186172601830785e-05
Iter: 35 loss: 2.3892902468827734e-05
Iter: 36 loss: 2.29104375771943e-05
Iter: 37 loss: 3.8317132433666314e-05
Iter: 38 loss: 2.2910425909517282e-05
Iter: 39 loss: 2.2635262762985818e-05
Iter: 40 loss: 2.1907434857228967e-05
Iter: 41 loss: 2.7225347243228808e-05
Iter: 42 loss: 2.1752884404293911e-05
Iter: 43 loss: 2.0773007113013574e-05
Iter: 44 loss: 2.33984681012387e-05
Iter: 45 loss: 2.0447579633463019e-05
Iter: 46 loss: 1.9608649154169478e-05
Iter: 47 loss: 2.2357117305471675e-05
Iter: 48 loss: 1.9375434350434427e-05
Iter: 49 loss: 1.873152459524099e-05
Iter: 50 loss: 2.3876382319528327e-05
Iter: 51 loss: 1.8688500448064219e-05
Iter: 52 loss: 1.8120719546146932e-05
Iter: 53 loss: 1.9232508407411146e-05
Iter: 54 loss: 1.7885956085743113e-05
Iter: 55 loss: 1.7385883219141409e-05
Iter: 56 loss: 1.8574758107795075e-05
Iter: 57 loss: 1.7203637277566985e-05
Iter: 58 loss: 1.6670934574990979e-05
Iter: 59 loss: 1.7884552824712237e-05
Iter: 60 loss: 1.6471413615294614e-05
Iter: 61 loss: 1.6012842080139337e-05
Iter: 62 loss: 1.8751928984964052e-05
Iter: 63 loss: 1.5955060673314275e-05
Iter: 64 loss: 1.5667381441830898e-05
Iter: 65 loss: 1.5702532081241087e-05
Iter: 66 loss: 1.544758830905341e-05
Iter: 67 loss: 1.5107949406041468e-05
Iter: 68 loss: 1.931218876651994e-05
Iter: 69 loss: 1.5104411579421572e-05
Iter: 70 loss: 1.5087779257476177e-05
Iter: 71 loss: 1.5017364947564399e-05
Iter: 72 loss: 1.4943610284687017e-05
Iter: 73 loss: 1.4783752907455078e-05
Iter: 74 loss: 1.7219614382938258e-05
Iter: 75 loss: 1.4777545661632958e-05
Iter: 76 loss: 1.4556780341046093e-05
Iter: 77 loss: 1.4265389934378989e-05
Iter: 78 loss: 1.4247853780280189e-05
Iter: 79 loss: 1.3977932165299426e-05
Iter: 80 loss: 1.4594769169423704e-05
Iter: 81 loss: 1.3876965438173793e-05
Iter: 82 loss: 1.3535922117327843e-05
Iter: 83 loss: 1.5326684939202305e-05
Iter: 84 loss: 1.3482661465227787e-05
Iter: 85 loss: 1.3276908157113651e-05
Iter: 86 loss: 1.341002256872268e-05
Iter: 87 loss: 1.3146331519834997e-05
Iter: 88 loss: 1.2886090524849705e-05
Iter: 89 loss: 1.4544360732242175e-05
Iter: 90 loss: 1.2856954436546771e-05
Iter: 91 loss: 1.2672311129707781e-05
Iter: 92 loss: 1.3154741355006958e-05
Iter: 93 loss: 1.2609788259194789e-05
Iter: 94 loss: 1.2409718246856255e-05
Iter: 95 loss: 1.2888322837610848e-05
Iter: 96 loss: 1.2337279071662251e-05
Iter: 97 loss: 1.2159509413649731e-05
Iter: 98 loss: 1.3320250741823219e-05
Iter: 99 loss: 1.2140553789898139e-05
Iter: 100 loss: 1.2014172559125106e-05
Iter: 101 loss: 1.2478823182643844e-05
Iter: 102 loss: 1.1982904209444628e-05
Iter: 103 loss: 1.1890846134235163e-05
Iter: 104 loss: 1.3237703718875228e-05
Iter: 105 loss: 1.1890728997815286e-05
Iter: 106 loss: 1.1789532457147965e-05
Iter: 107 loss: 1.1886106212221966e-05
Iter: 108 loss: 1.1731714821462508e-05
Iter: 109 loss: 1.1662966191909267e-05
Iter: 110 loss: 1.1623672345358338e-05
Iter: 111 loss: 1.1594137934683889e-05
Iter: 112 loss: 1.149648889535075e-05
Iter: 113 loss: 1.1443068584806759e-05
Iter: 114 loss: 1.139970850693032e-05
Iter: 115 loss: 1.1258842968560484e-05
Iter: 116 loss: 1.2450284677921608e-05
Iter: 117 loss: 1.1250735282603337e-05
Iter: 118 loss: 1.1167815269167602e-05
Iter: 119 loss: 1.1301504337465584e-05
Iter: 120 loss: 1.1129610724485274e-05
Iter: 121 loss: 1.1017959615311628e-05
Iter: 122 loss: 1.1250065504534511e-05
Iter: 123 loss: 1.0973447848454009e-05
Iter: 124 loss: 1.0879643311951497e-05
Iter: 125 loss: 1.0973236168139911e-05
Iter: 126 loss: 1.0826869504100306e-05
Iter: 127 loss: 1.0714365732815251e-05
Iter: 128 loss: 1.1470701108787251e-05
Iter: 129 loss: 1.0703081616006128e-05
Iter: 130 loss: 1.0627514339098091e-05
Iter: 131 loss: 1.0673248869829766e-05
Iter: 132 loss: 1.0578890899775282e-05
Iter: 133 loss: 1.0482262322590827e-05
Iter: 134 loss: 1.1018937175392575e-05
Iter: 135 loss: 1.0468457731270695e-05
Iter: 136 loss: 1.0419893723913826e-05
Iter: 137 loss: 1.0419503447655739e-05
Iter: 138 loss: 1.0374668570355949e-05
Iter: 139 loss: 1.066835853055482e-05
Iter: 140 loss: 1.0369928371466947e-05
Iter: 141 loss: 1.0345493433878113e-05
Iter: 142 loss: 1.030225112146305e-05
Iter: 143 loss: 1.0302250396393009e-05
Iter: 144 loss: 1.024295662095018e-05
Iter: 145 loss: 1.0230206060289712e-05
Iter: 146 loss: 1.0191462212531241e-05
Iter: 147 loss: 1.0123085119709527e-05
Iter: 148 loss: 1.0421658117176841e-05
Iter: 149 loss: 1.0109234738598204e-05
Iter: 150 loss: 1.0040170296646912e-05
Iter: 151 loss: 1.006935603273462e-05
Iter: 152 loss: 9.9927985214913091e-06
Iter: 153 loss: 9.918855682623318e-06
Iter: 154 loss: 1.0675689846729921e-05
Iter: 155 loss: 9.9167107389385617e-06
Iter: 156 loss: 9.86730238066473e-06
Iter: 157 loss: 9.8323949691027764e-06
Iter: 158 loss: 9.8149569096948621e-06
Iter: 159 loss: 9.75118269076291e-06
Iter: 160 loss: 1.0502081592855667e-05
Iter: 161 loss: 9.7502848110010636e-06
Iter: 162 loss: 9.700145870422541e-06
Iter: 163 loss: 9.71676524558805e-06
Iter: 164 loss: 9.66460795457401e-06
Iter: 165 loss: 9.5961382732198268e-06
Iter: 166 loss: 9.97576670361399e-06
Iter: 167 loss: 9.5863446686032889e-06
Iter: 168 loss: 9.5448899656820638e-06
Iter: 169 loss: 9.6553043190367788e-06
Iter: 170 loss: 9.53106459813343e-06
Iter: 171 loss: 9.5093809590856766e-06
Iter: 172 loss: 9.5026299699404438e-06
Iter: 173 loss: 9.4837709039962689e-06
Iter: 174 loss: 9.4367060587400886e-06
Iter: 175 loss: 9.8817831042646153e-06
Iter: 176 loss: 9.4301617049696113e-06
Iter: 177 loss: 9.3780973228380809e-06
Iter: 178 loss: 9.4768587247324019e-06
Iter: 179 loss: 9.3561446982331113e-06
Iter: 180 loss: 9.3089284564564091e-06
Iter: 181 loss: 9.52184865392013e-06
Iter: 182 loss: 9.2997650528581962e-06
Iter: 183 loss: 9.2548218312879065e-06
Iter: 184 loss: 9.2512033801246684e-06
Iter: 185 loss: 9.217757164994792e-06
Iter: 186 loss: 9.1717952015838148e-06
Iter: 187 loss: 9.5944034939779722e-06
Iter: 188 loss: 9.1697820170454088e-06
Iter: 189 loss: 9.1341161832923749e-06
Iter: 190 loss: 9.1088971319926176e-06
Iter: 191 loss: 9.0963208798124339e-06
Iter: 192 loss: 9.0515415667130449e-06
Iter: 193 loss: 9.4842495613453365e-06
Iter: 194 loss: 9.0498961186903311e-06
Iter: 195 loss: 9.0111658273714274e-06
Iter: 196 loss: 9.0397102852134184e-06
Iter: 197 loss: 8.9873496706857616e-06
Iter: 198 loss: 8.9415768791885222e-06
Iter: 199 loss: 9.2997853848818179e-06
Iter: 200 loss: 8.9383510545858643e-06
Iter: 201 loss: 8.90473743487323e-06
Iter: 202 loss: 8.9130834597375282e-06
Iter: 203 loss: 8.8802021575635973e-06
Iter: 204 loss: 8.8830925687465349e-06
Iter: 205 loss: 8.8601795161265426e-06
Iter: 206 loss: 8.844553157698605e-06
Iter: 207 loss: 8.81875942341083e-06
Iter: 208 loss: 8.818650901732972e-06
Iter: 209 loss: 8.7948141052719711e-06
Iter: 210 loss: 8.7808256394987866e-06
Iter: 211 loss: 8.77079506731291e-06
Iter: 212 loss: 8.7357383048383781e-06
Iter: 213 loss: 8.8892308518615991e-06
Iter: 214 loss: 8.7286690761881523e-06
Iter: 215 loss: 8.6934737362510719e-06
Iter: 216 loss: 8.7165628425821058e-06
Iter: 217 loss: 8.671228840748931e-06
Iter: 218 loss: 8.6374637580733377e-06
Iter: 219 loss: 8.8186776595052512e-06
Iter: 220 loss: 8.63236855891891e-06
Iter: 221 loss: 8.6016795307545058e-06
Iter: 222 loss: 8.6138833369609829e-06
Iter: 223 loss: 8.5804447206966232e-06
Iter: 224 loss: 8.5495649440581168e-06
Iter: 225 loss: 8.7800198542047253e-06
Iter: 226 loss: 8.5471094525533877e-06
Iter: 227 loss: 8.5164929338313071e-06
Iter: 228 loss: 8.5076194180418964e-06
Iter: 229 loss: 8.4891205881581736e-06
Iter: 230 loss: 8.45243115895288e-06
Iter: 231 loss: 8.7843097692882157e-06
Iter: 232 loss: 8.4507343819867429e-06
Iter: 233 loss: 8.4220155531955824e-06
Iter: 234 loss: 8.46570647421164e-06
Iter: 235 loss: 8.4083817171642874e-06
Iter: 236 loss: 8.40161969684458e-06
Iter: 237 loss: 8.3929829363809e-06
Iter: 238 loss: 8.3788391173744551e-06
Iter: 239 loss: 8.3752388369598939e-06
Iter: 240 loss: 8.3663695302009851e-06
Iter: 241 loss: 8.3517370757810541e-06
Iter: 242 loss: 8.3267167319419829e-06
Iter: 243 loss: 8.3266868501422031e-06
Iter: 244 loss: 8.3004168462805988e-06
Iter: 245 loss: 8.4721769037046142e-06
Iter: 246 loss: 8.297625691982128e-06
Iter: 247 loss: 8.2762013975172749e-06
Iter: 248 loss: 8.2968742235882317e-06
Iter: 249 loss: 8.264009909519566e-06
Iter: 250 loss: 8.239161349439218e-06
Iter: 251 loss: 8.3298003836816045e-06
Iter: 252 loss: 8.2329636534354486e-06
Iter: 253 loss: 8.2084215835084359e-06
Iter: 254 loss: 8.2226083196270679e-06
Iter: 255 loss: 8.1924868936262137e-06
Iter: 256 loss: 8.1679532908083576e-06
Iter: 257 loss: 8.31587713343149e-06
Iter: 258 loss: 8.1649113766785591e-06
Iter: 259 loss: 8.1385535595678323e-06
Iter: 260 loss: 8.1349458359838738e-06
Iter: 261 loss: 8.1163473174376968e-06
Iter: 262 loss: 8.0881104995324735e-06
Iter: 263 loss: 8.3368658602396976e-06
Iter: 264 loss: 8.0866885711328683e-06
Iter: 265 loss: 8.064863352991362e-06
Iter: 266 loss: 8.088487143473246e-06
Iter: 267 loss: 8.0529170188645658e-06
Iter: 268 loss: 8.05281760028082e-06
Iter: 269 loss: 8.0426340207241548e-06
Iter: 270 loss: 8.0327143791864353e-06
Iter: 271 loss: 8.035772876056069e-06
Iter: 272 loss: 8.02562832448643e-06
Iter: 273 loss: 8.01459667850798e-06
Iter: 274 loss: 7.9934291124590835e-06
Iter: 275 loss: 8.4389666688735533e-06
Iter: 276 loss: 7.993322581782239e-06
Iter: 277 loss: 7.9713453816783815e-06
Iter: 278 loss: 8.074922169314083e-06
Iter: 279 loss: 7.9673300431457945e-06
Iter: 280 loss: 7.94758810096798e-06
Iter: 281 loss: 7.9688485278717614e-06
Iter: 282 loss: 7.9367659458676984e-06
Iter: 283 loss: 7.9143676778812941e-06
Iter: 284 loss: 8.0257841973635148e-06
Iter: 285 loss: 7.9105737476361123e-06
Iter: 286 loss: 7.89122849375976e-06
Iter: 287 loss: 7.8988554150231e-06
Iter: 288 loss: 7.8778278697264963e-06
Iter: 289 loss: 7.8554781576423312e-06
Iter: 290 loss: 7.93315391850666e-06
Iter: 291 loss: 7.84961556470096e-06
Iter: 292 loss: 7.8236918348862014e-06
Iter: 293 loss: 7.8738771519225772e-06
Iter: 294 loss: 7.8128913239733257e-06
Iter: 295 loss: 7.7944981295464819e-06
Iter: 296 loss: 7.9135604644938461e-06
Iter: 297 loss: 7.7925070407151538e-06
Iter: 298 loss: 7.7753511267461827e-06
Iter: 299 loss: 7.7787353580492866e-06
Iter: 300 loss: 7.762588973502081e-06
Iter: 301 loss: 7.7591902558026e-06
Iter: 302 loss: 7.7531358525252811e-06
Iter: 303 loss: 7.7432277890990255e-06
Iter: 304 loss: 7.758311018355641e-06
Iter: 305 loss: 7.7385278193531347e-06
Iter: 306 loss: 7.7296295644527245e-06
Iter: 307 loss: 7.713980140462633e-06
Iter: 308 loss: 7.7139786547808941e-06
Iter: 309 loss: 7.69742011588293e-06
Iter: 310 loss: 7.75433410365124e-06
Iter: 311 loss: 7.6930295092858424e-06
Iter: 312 loss: 7.6769062552685616e-06
Iter: 313 loss: 7.6890592045195815e-06
Iter: 314 loss: 7.6670523143976729e-06
Iter: 315 loss: 7.64848058452691e-06
Iter: 316 loss: 7.7586033322321009e-06
Iter: 317 loss: 7.6461085453200653e-06
Iter: 318 loss: 7.6304031648282126e-06
Iter: 319 loss: 7.635344741262166e-06
Iter: 320 loss: 7.6192096376625754e-06
Iter: 321 loss: 7.6010274883335357e-06
Iter: 322 loss: 7.68678532234484e-06
Iter: 323 loss: 7.5977062712414693e-06
Iter: 324 loss: 7.5788277318705873e-06
Iter: 325 loss: 7.6071424451102863e-06
Iter: 326 loss: 7.5698066502579505e-06
Iter: 327 loss: 7.55470669935677e-06
Iter: 328 loss: 7.6136826860801756e-06
Iter: 329 loss: 7.5512140111782453e-06
Iter: 330 loss: 7.5345332031554211e-06
Iter: 331 loss: 7.5443359762740787e-06
Iter: 332 loss: 7.5237387446174857e-06
Iter: 333 loss: 7.5179342240980543e-06
Iter: 334 loss: 7.5150222239856167e-06
Iter: 335 loss: 7.5055423730562225e-06
Iter: 336 loss: 7.5352134093427664e-06
Iter: 337 loss: 7.5027905968845647e-06
Iter: 338 loss: 7.4954768552825457e-06
Iter: 339 loss: 7.4821951649656717e-06
Iter: 340 loss: 7.7995279792654165e-06
Iter: 341 loss: 7.4821902654571427e-06
Iter: 342 loss: 7.46810427393817e-06
Iter: 343 loss: 7.5059445339411375e-06
Iter: 344 loss: 7.4634384087887816e-06
Iter: 345 loss: 7.4487592156449865e-06
Iter: 346 loss: 7.4609034030245764e-06
Iter: 347 loss: 7.4400088807161367e-06
Iter: 348 loss: 7.42520908409517e-06
Iter: 349 loss: 7.5342670999213162e-06
Iter: 350 loss: 7.423996363989293e-06
Iter: 351 loss: 7.4120197290715881e-06
Iter: 352 loss: 7.4203684145619989e-06
Iter: 353 loss: 7.404556898468723e-06
Iter: 354 loss: 7.3914719127005361e-06
Iter: 355 loss: 7.4334047200672612e-06
Iter: 356 loss: 7.3877536819922523e-06
Iter: 357 loss: 7.3730551309668062e-06
Iter: 358 loss: 7.4042121859275915e-06
Iter: 359 loss: 7.3672711225765946e-06
Iter: 360 loss: 7.354990019316932e-06
Iter: 361 loss: 7.389963945888161e-06
Iter: 362 loss: 7.35111195099232e-06
Iter: 363 loss: 7.3363712092909023e-06
Iter: 364 loss: 7.3558946659086459e-06
Iter: 365 loss: 7.3289176956275817e-06
Iter: 366 loss: 7.31882828187771e-06
Iter: 367 loss: 7.4776678313680344e-06
Iter: 368 loss: 7.3188278467663036e-06
Iter: 369 loss: 7.3091825192654229e-06
Iter: 370 loss: 7.3970639629960805e-06
Iter: 371 loss: 7.3087463304454175e-06
Iter: 372 loss: 7.3030059510515075e-06
Iter: 373 loss: 7.290189127443255e-06
Iter: 374 loss: 7.4697752603523662e-06
Iter: 375 loss: 7.2895089567058708e-06
Iter: 376 loss: 7.2774724631316771e-06
Iter: 377 loss: 7.3352375286279846e-06
Iter: 378 loss: 7.27532849892752e-06
Iter: 379 loss: 7.2647010063804632e-06
Iter: 380 loss: 7.2653746222239813e-06
Iter: 381 loss: 7.2563975050842724e-06
Iter: 382 loss: 7.2437896193068721e-06
Iter: 383 loss: 7.3179707913653326e-06
Iter: 384 loss: 7.2421572240283889e-06
Iter: 385 loss: 7.2303575423265022e-06
Iter: 386 loss: 7.2424126740696815e-06
Iter: 387 loss: 7.2237678682551324e-06
Iter: 388 loss: 7.21159749225476e-06
Iter: 389 loss: 7.25667487699638e-06
Iter: 390 loss: 7.2086111222239168e-06
Iter: 391 loss: 7.1960041119449512e-06
Iter: 392 loss: 7.2220700186581956e-06
Iter: 393 loss: 7.1909625726544511e-06
Iter: 394 loss: 7.1800009876142142e-06
Iter: 395 loss: 7.2129324468221261e-06
Iter: 396 loss: 7.1766966858987113e-06
Iter: 397 loss: 7.16406795567491e-06
Iter: 398 loss: 7.178991706310294e-06
Iter: 399 loss: 7.1573789354688029e-06
Iter: 400 loss: 7.1469322655748116e-06
Iter: 401 loss: 7.2292377549507671e-06
Iter: 402 loss: 7.146206465341978e-06
Iter: 403 loss: 7.139456358043415e-06
Iter: 404 loss: 7.1390298135013777e-06
Iter: 405 loss: 7.134970147932893e-06
Iter: 406 loss: 7.1251381406850638e-06
Iter: 407 loss: 7.2298636985393417e-06
Iter: 408 loss: 7.1240583496946133e-06
Iter: 409 loss: 7.1156437873950729e-06
Iter: 410 loss: 7.1596061638856336e-06
Iter: 411 loss: 7.1143210414362586e-06
Iter: 412 loss: 7.1062785708963413e-06
Iter: 413 loss: 7.101347332214519e-06
Iter: 414 loss: 7.09808594162532e-06
Iter: 415 loss: 7.0862969360710366e-06
Iter: 416 loss: 7.14676203530268e-06
Iter: 417 loss: 7.0843886204413822e-06
Iter: 418 loss: 7.0736749516005743e-06
Iter: 419 loss: 7.0972282208959371e-06
Iter: 420 loss: 7.0695604172591878e-06
Iter: 421 loss: 7.0601348646936266e-06
Iter: 422 loss: 7.0925516438411034e-06
Iter: 423 loss: 7.0576350711523429e-06
Iter: 424 loss: 7.0472465464940008e-06
Iter: 425 loss: 7.0607018022468012e-06
Iter: 426 loss: 7.0419443203680266e-06
Iter: 427 loss: 7.0314217628244444e-06
Iter: 428 loss: 7.0656283594690096e-06
Iter: 429 loss: 7.0284722133100247e-06
Iter: 430 loss: 7.0168433648235122e-06
Iter: 431 loss: 7.0339656155275467e-06
Iter: 432 loss: 7.01123577976409e-06
Iter: 433 loss: 7.0009001240742274e-06
Iter: 434 loss: 7.0378349911060614e-06
Iter: 435 loss: 6.9982649407744683e-06
Iter: 436 loss: 6.9965178364827162e-06
Iter: 437 loss: 6.9926906086408793e-06
Iter: 438 loss: 6.9890056363875481e-06
Iter: 439 loss: 6.97982004091344e-06
Iter: 440 loss: 7.0672978021186256e-06
Iter: 441 loss: 6.9785555981363186e-06
Iter: 442 loss: 6.9710083097043937e-06
Iter: 443 loss: 6.9971654069697534e-06
Iter: 444 loss: 6.9690245805659064e-06
Iter: 445 loss: 6.9600260608410914e-06
Iter: 446 loss: 6.9572562053564159e-06
Iter: 447 loss: 6.9519234682762974e-06
Iter: 448 loss: 6.9401232002167237e-06
Iter: 449 loss: 7.000558838248e-06
Iter: 450 loss: 6.93820970446888e-06
Iter: 451 loss: 6.9284015843297705e-06
Iter: 452 loss: 6.9530995782541153e-06
Iter: 453 loss: 6.92498335691963e-06
Iter: 454 loss: 6.9158926949833884e-06
Iter: 455 loss: 6.9422732478920769e-06
Iter: 456 loss: 6.9130672559219592e-06
Iter: 457 loss: 6.9025365656973647e-06
Iter: 458 loss: 6.916401072280002e-06
Iter: 459 loss: 6.8971989944142594e-06
Iter: 460 loss: 6.8875372236412062e-06
Iter: 461 loss: 6.93303678113276e-06
Iter: 462 loss: 6.8857687069738778e-06
Iter: 463 loss: 6.87624712784814e-06
Iter: 464 loss: 6.8908654588824671e-06
Iter: 465 loss: 6.8717471385122969e-06
Iter: 466 loss: 6.8625679110644561e-06
Iter: 467 loss: 6.87977809841389e-06
Iter: 468 loss: 6.85867017095391e-06
Iter: 469 loss: 6.8596371805651812e-06
Iter: 470 loss: 6.85419885550165e-06
Iter: 471 loss: 6.8501162102386956e-06
Iter: 472 loss: 6.8420002123086345e-06
Iter: 473 loss: 6.9987098913712029e-06
Iter: 474 loss: 6.8419096171918658e-06
Iter: 475 loss: 6.8363209487658214e-06
Iter: 476 loss: 6.8368029912897148e-06
Iter: 477 loss: 6.8319921078039573e-06
Iter: 478 loss: 6.8223532912052219e-06
Iter: 479 loss: 6.83761864847857e-06
Iter: 480 loss: 6.8178678354165766e-06
Iter: 481 loss: 6.8085622634021259e-06
Iter: 482 loss: 6.8572664275239787e-06
Iter: 483 loss: 6.8071026999194316e-06
Iter: 484 loss: 6.7997771013675881e-06
Iter: 485 loss: 6.8056866485482164e-06
Iter: 486 loss: 6.795379643983319e-06
Iter: 487 loss: 6.7868885928925147e-06
Iter: 488 loss: 6.8170316611938144e-06
Iter: 489 loss: 6.7847089733317784e-06
Iter: 490 loss: 6.7760716817727671e-06
Iter: 491 loss: 6.7967148354505277e-06
Iter: 492 loss: 6.7729424660228924e-06
Iter: 493 loss: 6.766070248759415e-06
Iter: 494 loss: 6.8020187144397873e-06
Iter: 495 loss: 6.76499077234973e-06
Iter: 496 loss: 6.7579098794803558e-06
Iter: 497 loss: 6.7598452376871925e-06
Iter: 498 loss: 6.7527869103010026e-06
Iter: 499 loss: 6.7442819131435906e-06
Iter: 500 loss: 6.763039131727216e-06
Iter: 501 loss: 6.7410208425886959e-06
Iter: 502 loss: 6.7402029038943147e-06
Iter: 503 loss: 6.7370462229475006e-06
Iter: 504 loss: 6.7327477302024951e-06
Iter: 505 loss: 6.7294164252826711e-06
Iter: 506 loss: 6.7280590231366782e-06
Iter: 507 loss: 6.7245348616404025e-06
Iter: 508 loss: 6.7181515841179528e-06
Iter: 509 loss: 6.8712357635722366e-06
Iter: 510 loss: 6.71814973894964e-06
Iter: 511 loss: 6.7085943949100948e-06
Iter: 512 loss: 6.7418502428885514e-06
Iter: 513 loss: 6.7060906958671151e-06
Iter: 514 loss: 6.6986268979809253e-06
Iter: 515 loss: 6.7414837202894464e-06
Iter: 516 loss: 6.69761969506866e-06
Iter: 517 loss: 6.6916023133076846e-06
Iter: 518 loss: 6.6919476189097422e-06
Iter: 519 loss: 6.6868904416138734e-06
Iter: 520 loss: 6.6786766655023054e-06
Iter: 521 loss: 6.7097867596016319e-06
Iter: 522 loss: 6.6767098218210366e-06
Iter: 523 loss: 6.6688580576200582e-06
Iter: 524 loss: 6.6865682241291272e-06
Iter: 525 loss: 6.6658945242073139e-06
Iter: 526 loss: 6.6590259875381146e-06
Iter: 527 loss: 6.6849883624933741e-06
Iter: 528 loss: 6.6573773626905138e-06
Iter: 529 loss: 6.6492570465353768e-06
Iter: 530 loss: 6.6579137892953275e-06
Iter: 531 loss: 6.6447888883780018e-06
Iter: 532 loss: 6.6367562521129839e-06
Iter: 533 loss: 6.6585968360102923e-06
Iter: 534 loss: 6.63411959844989e-06
Iter: 535 loss: 6.6303176198002525e-06
Iter: 536 loss: 6.629684732880065e-06
Iter: 537 loss: 6.6247744352880143e-06
Iter: 538 loss: 6.6323663089177606e-06
Iter: 539 loss: 6.6224619100978448e-06
Iter: 540 loss: 6.6198096137486422e-06
Iter: 541 loss: 6.6129315704894213e-06
Iter: 542 loss: 6.6680657288685348e-06
Iter: 543 loss: 6.611661896751909e-06
Iter: 544 loss: 6.6025457085522241e-06
Iter: 545 loss: 6.6641872652429896e-06
Iter: 546 loss: 6.6016413879750627e-06
Iter: 547 loss: 6.59625322427394e-06
Iter: 548 loss: 6.6300601077240358e-06
Iter: 549 loss: 6.5956332248574483e-06
Iter: 550 loss: 6.5910235207534441e-06
Iter: 551 loss: 6.5895145264271542e-06
Iter: 552 loss: 6.5868410940434837e-06
Iter: 553 loss: 6.5798463609579694e-06
Iter: 554 loss: 6.6048995904923714e-06
Iter: 555 loss: 6.578067821057864e-06
Iter: 556 loss: 6.5714028414371917e-06
Iter: 557 loss: 6.5859176902282952e-06
Iter: 558 loss: 6.5688266089312819e-06
Iter: 559 loss: 6.5628820707802383e-06
Iter: 560 loss: 6.5817094647765747e-06
Iter: 561 loss: 6.5611744421751471e-06
Iter: 562 loss: 6.5542057825514742e-06
Iter: 563 loss: 6.5719255827260084e-06
Iter: 564 loss: 6.5517952712505871e-06
Iter: 565 loss: 6.5456911188688559e-06
Iter: 566 loss: 6.5580198434613194e-06
Iter: 567 loss: 6.5432129227166809e-06
Iter: 568 loss: 6.5380216132617436e-06
Iter: 569 loss: 6.6083479309871e-06
Iter: 570 loss: 6.5379994954694031e-06
Iter: 571 loss: 6.5329483958249687e-06
Iter: 572 loss: 6.56847438683342e-06
Iter: 573 loss: 6.5324882576596114e-06
Iter: 574 loss: 6.5306075609797e-06
Iter: 575 loss: 6.5249860508196969e-06
Iter: 576 loss: 6.5425056884892267e-06
Iter: 577 loss: 6.5222222940068692e-06
Iter: 578 loss: 6.5143664734725212e-06
Iter: 579 loss: 6.5789544423421536e-06
Iter: 580 loss: 6.513879986310283e-06
Iter: 581 loss: 6.5086956854218846e-06
Iter: 582 loss: 6.52994955378269e-06
Iter: 583 loss: 6.5075628542738037e-06
Iter: 584 loss: 6.501996817888931e-06
Iter: 585 loss: 6.5014637454196307e-06
Iter: 586 loss: 6.4973802262564239e-06
Iter: 587 loss: 6.4903705018008893e-06
Iter: 588 loss: 6.5246928861151679e-06
Iter: 589 loss: 6.4891563595906888e-06
Iter: 590 loss: 6.4829752355400746e-06
Iter: 591 loss: 6.4943141120411185e-06
Iter: 592 loss: 6.4803180096661084e-06
Iter: 593 loss: 6.4739005093010914e-06
Iter: 594 loss: 6.4853677679094951e-06
Iter: 595 loss: 6.4710998232550048e-06
Iter: 596 loss: 6.4633753728736813e-06
Iter: 597 loss: 6.4992705947599254e-06
Iter: 598 loss: 6.4619365497704225e-06
Iter: 599 loss: 6.4564111206758165e-06
Iter: 600 loss: 6.4686879058357471e-06
Iter: 601 loss: 6.454302936830569e-06
Iter: 602 loss: 6.4489649848834567e-06
Iter: 603 loss: 6.4909998420225407e-06
Iter: 604 loss: 6.4485946455592742e-06
Iter: 605 loss: 6.4442123384372771e-06
Iter: 606 loss: 6.5108213656339e-06
Iter: 607 loss: 6.4442105111441648e-06
Iter: 608 loss: 6.44248177197992e-06
Iter: 609 loss: 6.437011176776599e-06
Iter: 610 loss: 6.4443220404576584e-06
Iter: 611 loss: 6.4329824747880335e-06
Iter: 612 loss: 6.4249993866884781e-06
Iter: 613 loss: 6.5009954201014729e-06
Iter: 614 loss: 6.4246893614379384e-06
Iter: 615 loss: 6.4196557321561442e-06
Iter: 616 loss: 6.4388233994930333e-06
Iter: 617 loss: 6.41845813232733e-06
Iter: 618 loss: 6.4130108832875118e-06
Iter: 619 loss: 6.4182493973872314e-06
Iter: 620 loss: 6.40990741655994e-06
Iter: 621 loss: 6.4039968821393449e-06
Iter: 622 loss: 6.4220753176881592e-06
Iter: 623 loss: 6.4022439493049831e-06
Iter: 624 loss: 6.3958365230804913e-06
Iter: 625 loss: 6.4034010515532727e-06
Iter: 626 loss: 6.392441072955863e-06
Iter: 627 loss: 6.3855190726215823e-06
Iter: 628 loss: 6.40516122617841e-06
Iter: 629 loss: 6.383326613085559e-06
Iter: 630 loss: 6.37693473737024e-06
Iter: 631 loss: 6.418438982905028e-06
Iter: 632 loss: 6.3762466157604343e-06
Iter: 633 loss: 6.3716769408775542e-06
Iter: 634 loss: 6.3763875865388478e-06
Iter: 635 loss: 6.3691322073800207e-06
Iter: 636 loss: 6.36329879702154e-06
Iter: 637 loss: 6.3957020981189736e-06
Iter: 638 loss: 6.3624670122841065e-06
Iter: 639 loss: 6.3591341991780166e-06
Iter: 640 loss: 6.3587532640284382e-06
Iter: 641 loss: 6.3571598832703129e-06
Iter: 642 loss: 6.3521424275515739e-06
Iter: 643 loss: 6.35960138382029e-06
Iter: 644 loss: 6.3485785522424833e-06
Iter: 645 loss: 6.341846664377126e-06
Iter: 646 loss: 6.402978479711068e-06
Iter: 647 loss: 6.341538478354431e-06
Iter: 648 loss: 6.3371042383539329e-06
Iter: 649 loss: 6.3452295076607715e-06
Iter: 650 loss: 6.3351968326091169e-06
Iter: 651 loss: 6.3297266400953926e-06
Iter: 652 loss: 6.3465479258740857e-06
Iter: 653 loss: 6.3281118859088488e-06
Iter: 654 loss: 6.3230599246162558e-06
Iter: 655 loss: 6.3353528923304946e-06
Iter: 656 loss: 6.3212533046047789e-06
Iter: 657 loss: 6.3155615440030269e-06
Iter: 658 loss: 6.318488330498193e-06
Iter: 659 loss: 6.31178188906569e-06
Iter: 660 loss: 6.3054205827021007e-06
Iter: 661 loss: 6.3278334411797813e-06
Iter: 662 loss: 6.30377507014657e-06
Iter: 663 loss: 6.2982117034622766e-06
Iter: 664 loss: 6.3309493086200234e-06
Iter: 665 loss: 6.2974918235463464e-06
Iter: 666 loss: 6.2928839799776081e-06
Iter: 667 loss: 6.2958772971880882e-06
Iter: 668 loss: 6.2899646396591349e-06
Iter: 669 loss: 6.2838281720910476e-06
Iter: 670 loss: 6.3235222482124358e-06
Iter: 671 loss: 6.2831632316487692e-06
Iter: 672 loss: 6.2815207707817425e-06
Iter: 673 loss: 6.2803678568552868e-06
Iter: 674 loss: 6.2789334034519288e-06
Iter: 675 loss: 6.274435183921404e-06
Iter: 676 loss: 6.2816938867298684e-06
Iter: 677 loss: 6.2713353795857937e-06
Iter: 678 loss: 6.26503036610638e-06
Iter: 679 loss: 6.30638327754761e-06
Iter: 680 loss: 6.2643646494372355e-06
Iter: 681 loss: 6.2597310754302162e-06
Iter: 682 loss: 6.2635013688160986e-06
Iter: 683 loss: 6.2569563667461622e-06
Iter: 684 loss: 6.2511568464345719e-06
Iter: 685 loss: 6.2872268319380417e-06
Iter: 686 loss: 6.250478022680324e-06
Iter: 687 loss: 6.245676266598867e-06
Iter: 688 loss: 6.2536699581568489e-06
Iter: 689 loss: 6.2434990370866195e-06
Iter: 690 loss: 6.237543390219169e-06
Iter: 691 loss: 6.2387538867121244e-06
Iter: 692 loss: 6.2331238263147383e-06
Iter: 693 loss: 6.22649753070901e-06
Iter: 694 loss: 6.2541196291914463e-06
Iter: 695 loss: 6.2250781235914622e-06
Iter: 696 loss: 6.2196185641847088e-06
Iter: 697 loss: 6.2505236103373186e-06
Iter: 698 loss: 6.2188636048336841e-06
Iter: 699 loss: 6.2140296156170395e-06
Iter: 700 loss: 6.2167111177486625e-06
Iter: 701 loss: 6.21086431677563e-06
Iter: 702 loss: 6.2048324521808947e-06
Iter: 703 loss: 6.25471316322915e-06
Iter: 704 loss: 6.2044657397855282e-06
Iter: 705 loss: 6.2035687751100441e-06
Iter: 706 loss: 6.2022282728986788e-06
Iter: 707 loss: 6.2008422077329413e-06
Iter: 708 loss: 6.1964542368086135e-06
Iter: 709 loss: 6.2022630050794418e-06
Iter: 710 loss: 6.1932130724156774e-06
Iter: 711 loss: 6.1871606528291375e-06
Iter: 712 loss: 6.22856837961535e-06
Iter: 713 loss: 6.1865752199318313e-06
Iter: 714 loss: 6.1820634487218576e-06
Iter: 715 loss: 6.1853425493994251e-06
Iter: 716 loss: 6.1792802355427e-06
Iter: 717 loss: 6.1739461679318868e-06
Iter: 718 loss: 6.21202364016182e-06
Iter: 719 loss: 6.1734760441476363e-06
Iter: 720 loss: 6.1689136256100273e-06
Iter: 721 loss: 6.1736894970811985e-06
Iter: 722 loss: 6.1663870232950682e-06
Iter: 723 loss: 6.160185225724236e-06
Iter: 724 loss: 6.1650699680009986e-06
Iter: 725 loss: 6.1564381559021668e-06
Iter: 726 loss: 6.15070876637667e-06
Iter: 727 loss: 6.174611402007435e-06
Iter: 728 loss: 6.1494826769919768e-06
Iter: 729 loss: 6.1444699698474082e-06
Iter: 730 loss: 6.1641259357313093e-06
Iter: 731 loss: 6.1433160067934005e-06
Iter: 732 loss: 6.1381669001166506e-06
Iter: 733 loss: 6.1410438654157216e-06
Iter: 734 loss: 6.1347996997551671e-06
Iter: 735 loss: 6.1288428718735077e-06
Iter: 736 loss: 6.1893227138490827e-06
Iter: 737 loss: 6.1286633469843447e-06
Iter: 738 loss: 6.1278037814932994e-06
Iter: 739 loss: 6.1266294008702195e-06
Iter: 740 loss: 6.1250884952441034e-06
Iter: 741 loss: 6.1203031793707475e-06
Iter: 742 loss: 6.1294515817307632e-06
Iter: 743 loss: 6.1172279824899583e-06
Iter: 744 loss: 6.1111727507981995e-06
Iter: 745 loss: 6.1477550594410233e-06
Iter: 746 loss: 6.1104251475809391e-06
Iter: 747 loss: 6.1055219703772347e-06
Iter: 748 loss: 6.1073650828210175e-06
Iter: 749 loss: 6.102102719298667e-06
Iter: 750 loss: 6.0960370909907e-06
Iter: 751 loss: 6.1375155407850574e-06
Iter: 752 loss: 6.095449735764979e-06
Iter: 753 loss: 6.0901137127999406e-06
Iter: 754 loss: 6.098190419821082e-06
Iter: 755 loss: 6.0875750335597358e-06
Iter: 756 loss: 6.0811410384495993e-06
Iter: 757 loss: 6.0940742462006305e-06
Iter: 758 loss: 6.07852181504003e-06
Iter: 759 loss: 6.07366110477451e-06
Iter: 760 loss: 6.0897116564847936e-06
Iter: 761 loss: 6.0723195776499543e-06
Iter: 762 loss: 6.0673499160377494e-06
Iter: 763 loss: 6.0781597741042347e-06
Iter: 764 loss: 6.0654272211641549e-06
Iter: 765 loss: 6.0598160421770785e-06
Iter: 766 loss: 6.0657887922535962e-06
Iter: 767 loss: 6.0567261355089783e-06
Iter: 768 loss: 6.0512512070171894e-06
Iter: 769 loss: 6.1125119533313381e-06
Iter: 770 loss: 6.0511471944861735e-06
Iter: 771 loss: 6.0504356327589163e-06
Iter: 772 loss: 6.049374828960619e-06
Iter: 773 loss: 6.0477967748047409e-06
Iter: 774 loss: 6.0431682095032276e-06
Iter: 775 loss: 6.0604235575610308e-06
Iter: 776 loss: 6.0411690193738425e-06
Iter: 777 loss: 6.0359034271126053e-06
Iter: 778 loss: 6.0637724127455886e-06
Iter: 779 loss: 6.0350914618969464e-06
Iter: 780 loss: 6.0308503601155205e-06
Iter: 781 loss: 6.0301750755110715e-06
Iter: 782 loss: 6.0272461738228834e-06
Iter: 783 loss: 6.0213975824237913e-06
Iter: 784 loss: 6.052713555227379e-06
Iter: 785 loss: 6.0205116268113617e-06
Iter: 786 loss: 6.0152464251841591e-06
Iter: 787 loss: 6.0273762554179227e-06
Iter: 788 loss: 6.0132880072245991e-06
Iter: 789 loss: 6.0073979326395852e-06
Iter: 790 loss: 6.0289750542327915e-06
Iter: 791 loss: 6.0059351407009e-06
Iter: 792 loss: 6.0018202426801931e-06
Iter: 793 loss: 6.0118866020138705e-06
Iter: 794 loss: 6.0003545012302221e-06
Iter: 795 loss: 5.9956750259526761e-06
Iter: 796 loss: 5.9995901799079306e-06
Iter: 797 loss: 5.9928945128013459e-06
Iter: 798 loss: 5.9870226586141864e-06
Iter: 799 loss: 5.9972331341290633e-06
Iter: 800 loss: 5.98442163715416e-06
Iter: 801 loss: 5.978602948767975e-06
Iter: 802 loss: 6.0219150450397353e-06
Iter: 803 loss: 5.9781380681278918e-06
Iter: 804 loss: 5.9779703422021008e-06
Iter: 805 loss: 5.9764216724762577e-06
Iter: 806 loss: 5.974597638823131e-06
Iter: 807 loss: 5.9695057027657634e-06
Iter: 808 loss: 5.9969247986553389e-06
Iter: 809 loss: 5.9679344458667548e-06
Iter: 810 loss: 5.9634157752489113e-06
Iter: 811 loss: 5.9888159785679605e-06
Iter: 812 loss: 5.9627834372434558e-06
Iter: 813 loss: 5.9590166696271909e-06
Iter: 814 loss: 5.9585836157818965e-06
Iter: 815 loss: 5.9558691020414495e-06
Iter: 816 loss: 5.9505658755361069e-06
Iter: 817 loss: 5.97540978889193e-06
Iter: 818 loss: 5.9495884405910233e-06
Iter: 819 loss: 5.9446388718684789e-06
Iter: 820 loss: 5.9520807966665873e-06
Iter: 821 loss: 5.9422764429596779e-06
Iter: 822 loss: 5.9369677061308933e-06
Iter: 823 loss: 5.9704400524417917e-06
Iter: 824 loss: 5.9363620696022396e-06
Iter: 825 loss: 5.932877671862396e-06
Iter: 826 loss: 5.940860718485871e-06
Iter: 827 loss: 5.9315766538438278e-06
Iter: 828 loss: 5.9273539059761506e-06
Iter: 829 loss: 5.9294975946635971e-06
Iter: 830 loss: 5.9245433279259069e-06
Iter: 831 loss: 5.9192650877508518e-06
Iter: 832 loss: 5.9317589530811874e-06
Iter: 833 loss: 5.9173390857559382e-06
Iter: 834 loss: 5.9122780884481976e-06
Iter: 835 loss: 5.9390878685031046e-06
Iter: 836 loss: 5.9114989805480788e-06
Iter: 837 loss: 5.9103696996003825e-06
Iter: 838 loss: 5.9097584657465996e-06
Iter: 839 loss: 5.9075435853143693e-06
Iter: 840 loss: 5.90301018307793e-06
Iter: 841 loss: 5.984122671796702e-06
Iter: 842 loss: 5.9029270289997869e-06
Iter: 843 loss: 5.899619108441635e-06
Iter: 844 loss: 5.9051763402505531e-06
Iter: 845 loss: 5.89812629469274e-06
Iter: 846 loss: 5.8945390669723143e-06
Iter: 847 loss: 5.8923661560306331e-06
Iter: 848 loss: 5.8908959996690859e-06
Iter: 849 loss: 5.8857509149521083e-06
Iter: 850 loss: 5.9282533096393451e-06
Iter: 851 loss: 5.8854367200322336e-06
Iter: 852 loss: 5.8810445327658248e-06
Iter: 853 loss: 5.8871463910397294e-06
Iter: 854 loss: 5.87886970504193e-06
Iter: 855 loss: 5.8742331545579654e-06
Iter: 856 loss: 5.8972857612654108e-06
Iter: 857 loss: 5.87344745702662e-06
Iter: 858 loss: 5.8700660493516007e-06
Iter: 859 loss: 5.8769353918058728e-06
Iter: 860 loss: 5.868698932546483e-06
Iter: 861 loss: 5.8643778559139392e-06
Iter: 862 loss: 5.8715396276675444e-06
Iter: 863 loss: 5.862413594034032e-06
Iter: 864 loss: 5.8580202869614457e-06
Iter: 865 loss: 5.86738610601799e-06
Iter: 866 loss: 5.856297860030514e-06
Iter: 867 loss: 5.8517528408523223e-06
Iter: 868 loss: 5.8673821540500531e-06
Iter: 869 loss: 5.8505478056267593e-06
Iter: 870 loss: 5.8481186736535328e-06
Iter: 871 loss: 5.8480878305436817e-06
Iter: 872 loss: 5.8448998936978553e-06
Iter: 873 loss: 5.84521918054061e-06
Iter: 874 loss: 5.842443128306885e-06
Iter: 875 loss: 5.8400695357603674e-06
Iter: 876 loss: 5.8387193045299523e-06
Iter: 877 loss: 5.8376955941890806e-06
Iter: 878 loss: 5.8342820112311429e-06
Iter: 879 loss: 5.8296879021365356e-06
Iter: 880 loss: 5.829444905038198e-06
Iter: 881 loss: 5.8235627096151913e-06
Iter: 882 loss: 5.8844289946202042e-06
Iter: 883 loss: 5.8233989150970009e-06
Iter: 884 loss: 5.818867719547222e-06
Iter: 885 loss: 5.8282433590608574e-06
Iter: 886 loss: 5.8170564832012707e-06
Iter: 887 loss: 5.81291873678545e-06
Iter: 888 loss: 5.841039309062333e-06
Iter: 889 loss: 5.8125128875897472e-06
Iter: 890 loss: 5.8093819799094184e-06
Iter: 891 loss: 5.811859744413817e-06
Iter: 892 loss: 5.8074928267952131e-06
Iter: 893 loss: 5.8028044976803769e-06
Iter: 894 loss: 5.809678063786771e-06
Iter: 895 loss: 5.800539805346434e-06
Iter: 896 loss: 5.795815268125756e-06
Iter: 897 loss: 5.8081726034249975e-06
Iter: 898 loss: 5.7942161668331356e-06
Iter: 899 loss: 5.7893882901432787e-06
Iter: 900 loss: 5.8086488498273676e-06
Iter: 901 loss: 5.7882982504502517e-06
Iter: 902 loss: 5.7847297096019058e-06
Iter: 903 loss: 5.8115982369858036e-06
Iter: 904 loss: 5.7844518756494908e-06
Iter: 905 loss: 5.780688145176448e-06
Iter: 906 loss: 5.8112312836836e-06
Iter: 907 loss: 5.7804469172299411e-06
Iter: 908 loss: 5.7785947803200558e-06
Iter: 909 loss: 5.7748303031284209e-06
Iter: 910 loss: 5.8434842600672266e-06
Iter: 911 loss: 5.7747684812463312e-06
Iter: 912 loss: 5.7710274257215138e-06
Iter: 913 loss: 5.7665014079211969e-06
Iter: 914 loss: 5.7660635470095644e-06
Iter: 915 loss: 5.7599640860210389e-06
Iter: 916 loss: 5.8349407108924164e-06
Iter: 917 loss: 5.7598996784751e-06
Iter: 918 loss: 5.7560449704774444e-06
Iter: 919 loss: 5.7589364879220351e-06
Iter: 920 loss: 5.7536859706808241e-06
Iter: 921 loss: 5.7495213468078084e-06
Iter: 922 loss: 5.7777020126109139e-06
Iter: 923 loss: 5.7491088544497634e-06
Iter: 924 loss: 5.7455521798843261e-06
Iter: 925 loss: 5.7510865307802342e-06
Iter: 926 loss: 5.7438829233312671e-06
Iter: 927 loss: 5.7389609460768166e-06
Iter: 928 loss: 5.7490301113100412e-06
Iter: 929 loss: 5.736979422082869e-06
Iter: 930 loss: 5.7326604887310081e-06
Iter: 931 loss: 5.740078651448604e-06
Iter: 932 loss: 5.7307344560563663e-06
Iter: 933 loss: 5.7259535890147553e-06
Iter: 934 loss: 5.7417732690797932e-06
Iter: 935 loss: 5.7246364973686939e-06
Iter: 936 loss: 5.7209109947604336e-06
Iter: 937 loss: 5.7478673890073639e-06
Iter: 938 loss: 5.7205922994230218e-06
Iter: 939 loss: 5.7173759676376018e-06
Iter: 940 loss: 5.7670498979461257e-06
Iter: 941 loss: 5.717375330687865e-06
Iter: 942 loss: 5.7159325690602322e-06
Iter: 943 loss: 5.7128177670160745e-06
Iter: 944 loss: 5.7610032327510322e-06
Iter: 945 loss: 5.7127025276943272e-06
Iter: 946 loss: 5.7093745189925371e-06
Iter: 947 loss: 5.7067674309788837e-06
Iter: 948 loss: 5.70573167898085e-06
Iter: 949 loss: 5.7010374319585474e-06
Iter: 950 loss: 5.7543824348014371e-06
Iter: 951 loss: 5.700955237398747e-06
Iter: 952 loss: 5.6981031453946263e-06
Iter: 953 loss: 5.6978934965331236e-06
Iter: 954 loss: 5.6957573247133156e-06
Iter: 955 loss: 5.6917631255519083e-06
Iter: 956 loss: 5.7156716677580039e-06
Iter: 957 loss: 5.6912614929252222e-06
Iter: 958 loss: 5.6879500384365166e-06
Iter: 959 loss: 5.6894634364063745e-06
Iter: 960 loss: 5.6857063668187853e-06
Iter: 961 loss: 5.6811121938192767e-06
Iter: 962 loss: 5.7088328658451512e-06
Iter: 963 loss: 5.680543713105097e-06
Iter: 964 loss: 5.6771818169355e-06
Iter: 965 loss: 5.6870609573684558e-06
Iter: 966 loss: 5.6761487652104584e-06
Iter: 967 loss: 5.6727576667705964e-06
Iter: 968 loss: 5.6772568905439205e-06
Iter: 969 loss: 5.671044295480099e-06
Iter: 970 loss: 5.6676922230676748e-06
Iter: 971 loss: 5.6789966556981436e-06
Iter: 972 loss: 5.6667857614865806e-06
Iter: 973 loss: 5.6663051048427378e-06
Iter: 974 loss: 5.665114448976491e-06
Iter: 975 loss: 5.6639292245919848e-06
Iter: 976 loss: 5.66079136193032e-06
Iter: 977 loss: 5.6835796930923935e-06
Iter: 978 loss: 5.6601203076345449e-06
Iter: 979 loss: 5.6567801827399486e-06
Iter: 980 loss: 5.6545995205930857e-06
Iter: 981 loss: 5.6533210446147514e-06
Iter: 982 loss: 5.6492682445465258e-06
Iter: 983 loss: 5.7004801599883676e-06
Iter: 984 loss: 5.6492337888961687e-06
Iter: 985 loss: 5.646395985981884e-06
Iter: 986 loss: 5.6441919058304572e-06
Iter: 987 loss: 5.6432984890966564e-06
Iter: 988 loss: 5.6388578648774456e-06
Iter: 989 loss: 5.6782809768898284e-06
Iter: 990 loss: 5.638638949557508e-06
Iter: 991 loss: 5.6353026411986434e-06
Iter: 992 loss: 5.6368657011450757e-06
Iter: 993 loss: 5.63305127024776e-06
Iter: 994 loss: 5.6288436535424061e-06
Iter: 995 loss: 5.6596272152332823e-06
Iter: 996 loss: 5.6284930839391893e-06
Iter: 997 loss: 5.6256015815347612e-06
Iter: 998 loss: 5.631762219696435e-06
Iter: 999 loss: 5.6244676232848124e-06
Iter: 1000 loss: 5.6213309497795839e-06
Iter: 1001 loss: 5.6282173161442805e-06
Iter: 1002 loss: 5.6201250828075823e-06
Iter: 1003 loss: 5.6170381476393347e-06
Iter: 1004 loss: 5.6253561301368512e-06
Iter: 1005 loss: 5.61601748663857e-06
Iter: 1006 loss: 5.6150712557513464e-06
Iter: 1007 loss: 5.6143828363952713e-06
Iter: 1008 loss: 5.6127999951251525e-06
Iter: 1009 loss: 5.6097784402946885e-06
Iter: 1010 loss: 5.674113959980787e-06
Iter: 1011 loss: 5.6097652585928782e-06
Iter: 1012 loss: 5.6072513692366862e-06
Iter: 1013 loss: 5.6054421267186756e-06
Iter: 1014 loss: 5.6045734192910387e-06
Iter: 1015 loss: 5.6007604186254923e-06
Iter: 1016 loss: 5.621991861399175e-06
Iter: 1017 loss: 5.600218344699389e-06
Iter: 1018 loss: 5.59730012305798e-06
Iter: 1019 loss: 5.59542060294778e-06
Iter: 1020 loss: 5.5942888987874841e-06
Iter: 1021 loss: 5.5897600054689826e-06
Iter: 1022 loss: 5.6304750748009625e-06
Iter: 1023 loss: 5.5895457780114471e-06
Iter: 1024 loss: 5.5866975111045613e-06
Iter: 1025 loss: 5.587904478537178e-06
Iter: 1026 loss: 5.5847448623511743e-06
Iter: 1027 loss: 5.5809324457619519e-06
Iter: 1028 loss: 5.6045212092992614e-06
Iter: 1029 loss: 5.5804819198222965e-06
Iter: 1030 loss: 5.5776467978506227e-06
Iter: 1031 loss: 5.5853833516984737e-06
Iter: 1032 loss: 5.5767194347033248e-06
Iter: 1033 loss: 5.5736154962256553e-06
Iter: 1034 loss: 5.5835015669587133e-06
Iter: 1035 loss: 5.5727285812201788e-06
Iter: 1036 loss: 5.5698337011537473e-06
Iter: 1037 loss: 5.5736809397738824e-06
Iter: 1038 loss: 5.5683720505212462e-06
Iter: 1039 loss: 5.5672567693859652e-06
Iter: 1040 loss: 5.5667071914643663e-06
Iter: 1041 loss: 5.5648573081269342e-06
Iter: 1042 loss: 5.5626444697322906e-06
Iter: 1043 loss: 5.562418484958926e-06
Iter: 1044 loss: 5.5604596761534953e-06
Iter: 1045 loss: 5.5575765893438518e-06
Iter: 1046 loss: 5.5575031691887195e-06
Iter: 1047 loss: 5.5538498931682543e-06
Iter: 1048 loss: 5.5748139463087968e-06
Iter: 1049 loss: 5.553356171867003e-06
Iter: 1050 loss: 5.5503001826405442e-06
Iter: 1051 loss: 5.5492079666806677e-06
Iter: 1052 loss: 5.547494301340449e-06
Iter: 1053 loss: 5.5438067368032321e-06
Iter: 1054 loss: 5.5932201472676154e-06
Iter: 1055 loss: 5.5437888117077362e-06
Iter: 1056 loss: 5.5414615889315247e-06
Iter: 1057 loss: 5.5412693196391744e-06
Iter: 1058 loss: 5.5395408080298705e-06
Iter: 1059 loss: 5.5355529007262272e-06
Iter: 1060 loss: 5.5470405118257424e-06
Iter: 1061 loss: 5.5343058925231406e-06
Iter: 1062 loss: 5.5308788511775477e-06
Iter: 1063 loss: 5.5377324158339318e-06
Iter: 1064 loss: 5.5294795102211914e-06
Iter: 1065 loss: 5.5257192747147515e-06
Iter: 1066 loss: 5.5440614766400812e-06
Iter: 1067 loss: 5.5250642098591082e-06
Iter: 1068 loss: 5.5218858602459068e-06
Iter: 1069 loss: 5.5326708059799869e-06
Iter: 1070 loss: 5.5210318022319033e-06
Iter: 1071 loss: 5.5187119366826521e-06
Iter: 1072 loss: 5.5510501652358263e-06
Iter: 1073 loss: 5.5187050879580058e-06
Iter: 1074 loss: 5.5162782100170063e-06
Iter: 1075 loss: 5.523524975766947e-06
Iter: 1076 loss: 5.5155425487093932e-06
Iter: 1077 loss: 5.5142508253952168e-06
Iter: 1078 loss: 5.5114233633037532e-06
Iter: 1079 loss: 5.5534233646094357e-06
Iter: 1080 loss: 5.51130127806505e-06
Iter: 1081 loss: 5.5079038577019253e-06
Iter: 1082 loss: 5.5152479888913194e-06
Iter: 1083 loss: 5.506583958156527e-06
Iter: 1084 loss: 5.5032158059959538e-06
Iter: 1085 loss: 5.5056739706478327e-06
Iter: 1086 loss: 5.5011400886362212e-06
Iter: 1087 loss: 5.4975247396695816e-06
Iter: 1088 loss: 5.5361143312426468e-06
Iter: 1089 loss: 5.4974373375159111e-06
Iter: 1090 loss: 5.4950531252069549e-06
Iter: 1091 loss: 5.4934796751909391e-06
Iter: 1092 loss: 5.4925765240273275e-06
Iter: 1093 loss: 5.488106802122458e-06
Iter: 1094 loss: 5.5085164919689232e-06
Iter: 1095 loss: 5.4872541580071573e-06
Iter: 1096 loss: 5.4839084551094782e-06
Iter: 1097 loss: 5.4897589120661511e-06
Iter: 1098 loss: 5.4824308142757156e-06
Iter: 1099 loss: 5.4788016497477212e-06
Iter: 1100 loss: 5.5014117461210431e-06
Iter: 1101 loss: 5.4783783313873662e-06
Iter: 1102 loss: 5.4754344767686589e-06
Iter: 1103 loss: 5.4836360912960889e-06
Iter: 1104 loss: 5.47448762056889e-06
Iter: 1105 loss: 5.4721776273489354e-06
Iter: 1106 loss: 5.5001516560599075e-06
Iter: 1107 loss: 5.4721505471594041e-06
Iter: 1108 loss: 5.4698555024581471e-06
Iter: 1109 loss: 5.4812444260432539e-06
Iter: 1110 loss: 5.4694653407130221e-06
Iter: 1111 loss: 5.4683031286751176e-06
Iter: 1112 loss: 5.46543429656848e-06
Iter: 1113 loss: 5.4938019667213624e-06
Iter: 1114 loss: 5.4650677528023828e-06
Iter: 1115 loss: 5.4613705545231441e-06
Iter: 1116 loss: 5.4703974504104756e-06
Iter: 1117 loss: 5.4600514708568568e-06
Iter: 1118 loss: 5.4564937994285583e-06
Iter: 1119 loss: 5.4622679402483708e-06
Iter: 1120 loss: 5.4548592007795222e-06
Iter: 1121 loss: 5.4509418001348059e-06
Iter: 1122 loss: 5.4768671441848364e-06
Iter: 1123 loss: 5.4505357559209272e-06
Iter: 1124 loss: 5.4478427811597943e-06
Iter: 1125 loss: 5.4464131160505253e-06
Iter: 1126 loss: 5.4451911820515034e-06
Iter: 1127 loss: 5.4402096864228691e-06
Iter: 1128 loss: 5.4600861678082286e-06
Iter: 1129 loss: 5.439085307317093e-06
Iter: 1130 loss: 5.4355175253363168e-06
Iter: 1131 loss: 5.4451077332878e-06
Iter: 1132 loss: 5.4343359624448384e-06
Iter: 1133 loss: 5.4303856218248576e-06
Iter: 1134 loss: 5.4433643794561212e-06
Iter: 1135 loss: 5.4292899338709248e-06
Iter: 1136 loss: 5.4258960073087855e-06
Iter: 1137 loss: 5.4341885040828358e-06
Iter: 1138 loss: 5.4246860037490533e-06
Iter: 1139 loss: 5.422196382179644e-06
Iter: 1140 loss: 5.4221894255058189e-06
Iter: 1141 loss: 5.4198919923265339e-06
Iter: 1142 loss: 5.4350349674855938e-06
Iter: 1143 loss: 5.4196517329171052e-06
Iter: 1144 loss: 5.4182907892577545e-06
Iter: 1145 loss: 5.4154213751477115e-06
Iter: 1146 loss: 5.46282582816513e-06
Iter: 1147 loss: 5.4153416859868789e-06
Iter: 1148 loss: 5.4117710236788266e-06
Iter: 1149 loss: 5.4145227734172246e-06
Iter: 1150 loss: 5.4096007236585131e-06
Iter: 1151 loss: 5.4058172014033495e-06
Iter: 1152 loss: 5.4153356151888686e-06
Iter: 1153 loss: 5.4044975824264934e-06
Iter: 1154 loss: 5.400651673988901e-06
Iter: 1155 loss: 5.4183786343564779e-06
Iter: 1156 loss: 5.3999270266657926e-06
Iter: 1157 loss: 5.3969688237610475e-06
Iter: 1158 loss: 5.3959301231665957e-06
Iter: 1159 loss: 5.3942595187065712e-06
Iter: 1160 loss: 5.3896121791872445e-06
Iter: 1161 loss: 5.422840623224659e-06
Iter: 1162 loss: 5.3892041381358018e-06
Iter: 1163 loss: 5.3861896069177009e-06
Iter: 1164 loss: 5.3896991788006414e-06
Iter: 1165 loss: 5.3845835030083179e-06
Iter: 1166 loss: 5.3807212905924207e-06
Iter: 1167 loss: 5.3980142633544947e-06
Iter: 1168 loss: 5.3799652515546156e-06
Iter: 1169 loss: 5.3768724506856951e-06
Iter: 1170 loss: 5.3827387947986522e-06
Iter: 1171 loss: 5.3755684537719719e-06
Iter: 1172 loss: 5.3728578552115125e-06
Iter: 1173 loss: 5.40833746158322e-06
Iter: 1174 loss: 5.3728411366112204e-06
Iter: 1175 loss: 5.3705681899372763e-06
Iter: 1176 loss: 5.3943937740952447e-06
Iter: 1177 loss: 5.3705083511119117e-06
Iter: 1178 loss: 5.3692045025805655e-06
Iter: 1179 loss: 5.3662007969077664e-06
Iter: 1180 loss: 5.4042453249363809e-06
Iter: 1181 loss: 5.3659854455254725e-06
Iter: 1182 loss: 5.3623686058534535e-06
Iter: 1183 loss: 5.368349290862447e-06
Iter: 1184 loss: 5.3607223381213278e-06
Iter: 1185 loss: 5.357391241839145e-06
Iter: 1186 loss: 5.3660238475182882e-06
Iter: 1187 loss: 5.3562557246289412e-06
Iter: 1188 loss: 5.3527191969358217e-06
Iter: 1189 loss: 5.3654678280628053e-06
Iter: 1190 loss: 5.3518257622610371e-06
Iter: 1191 loss: 5.3488792101500836e-06
Iter: 1192 loss: 5.3499715904075652e-06
Iter: 1193 loss: 5.3468206711493865e-06
Iter: 1194 loss: 5.342253141819e-06
Iter: 1195 loss: 5.3602287478414215e-06
Iter: 1196 loss: 5.34120595245825e-06
Iter: 1197 loss: 5.3379323523283733e-06
Iter: 1198 loss: 5.344436215783025e-06
Iter: 1199 loss: 5.3365901567549089e-06
Iter: 1200 loss: 5.3325320482647278e-06
Iter: 1201 loss: 5.341786964160157e-06
Iter: 1202 loss: 5.33101167918629e-06
Iter: 1203 loss: 5.327558166173893e-06
Iter: 1204 loss: 5.3366762225236328e-06
Iter: 1205 loss: 5.3263977706456218e-06
Iter: 1206 loss: 5.322865892766629e-06
Iter: 1207 loss: 5.34385771129051e-06
Iter: 1208 loss: 5.3224173092344723e-06
Iter: 1209 loss: 5.3209610741921812e-06
Iter: 1210 loss: 5.3204822972018265e-06
Iter: 1211 loss: 5.3193608410632691e-06
Iter: 1212 loss: 5.3163309239649822e-06
Iter: 1213 loss: 5.336118115568693e-06
Iter: 1214 loss: 5.3155841160505077e-06
Iter: 1215 loss: 5.3120994758756443e-06
Iter: 1216 loss: 5.324923300088868e-06
Iter: 1217 loss: 5.3112383858746215e-06
Iter: 1218 loss: 5.3083669906824356e-06
Iter: 1219 loss: 5.3163921998431211e-06
Iter: 1220 loss: 5.3074459675924045e-06
Iter: 1221 loss: 5.3043616642479937e-06
Iter: 1222 loss: 5.3084786219410514e-06
Iter: 1223 loss: 5.3028071880687618e-06
Iter: 1224 loss: 5.2996085959597883e-06
Iter: 1225 loss: 5.3009601584408563e-06
Iter: 1226 loss: 5.2974148206570641e-06
Iter: 1227 loss: 5.29273700874109e-06
Iter: 1228 loss: 5.3168265661971974e-06
Iter: 1229 loss: 5.2919847233870984e-06
Iter: 1230 loss: 5.2887475186211865e-06
Iter: 1231 loss: 5.29305448694216e-06
Iter: 1232 loss: 5.2871138206301522e-06
Iter: 1233 loss: 5.2831245973180148e-06
Iter: 1234 loss: 5.302153450873491e-06
Iter: 1235 loss: 5.2824074444700329e-06
Iter: 1236 loss: 5.2795578796432686e-06
Iter: 1237 loss: 5.2852106791013595e-06
Iter: 1238 loss: 5.2783884904025648e-06
Iter: 1239 loss: 5.2749923491401174e-06
Iter: 1240 loss: 5.2888228297409749e-06
Iter: 1241 loss: 5.2742447053957873e-06
Iter: 1242 loss: 5.2739846592208437e-06
Iter: 1243 loss: 5.2728628778394569e-06
Iter: 1244 loss: 5.2717592526247512e-06
Iter: 1245 loss: 5.2690004123997895e-06
Iter: 1246 loss: 5.2949344156187058e-06
Iter: 1247 loss: 5.268612052759038e-06
Iter: 1248 loss: 5.265878039576812e-06
Iter: 1249 loss: 5.2694919102529484e-06
Iter: 1250 loss: 5.2644943122063456e-06
Iter: 1251 loss: 5.2616840541309936e-06
Iter: 1252 loss: 5.2712795492960533e-06
Iter: 1253 loss: 5.2609336220762991e-06
Iter: 1254 loss: 5.2578517247586966e-06
Iter: 1255 loss: 5.2654416866286325e-06
Iter: 1256 loss: 5.2567594078795955e-06
Iter: 1257 loss: 5.2539032827256544e-06
Iter: 1258 loss: 5.25579111410545e-06
Iter: 1259 loss: 5.2521011755481419e-06
Iter: 1260 loss: 5.2478159831984641e-06
Iter: 1261 loss: 5.2578352011681496e-06
Iter: 1262 loss: 5.2462384213600637e-06
Iter: 1263 loss: 5.2428408013598985e-06
Iter: 1264 loss: 5.2530619568213011e-06
Iter: 1265 loss: 5.2418179357553966e-06
Iter: 1266 loss: 5.2379880946175928e-06
Iter: 1267 loss: 5.247457134394968e-06
Iter: 1268 loss: 5.2366345471165763e-06
Iter: 1269 loss: 5.2335123407314123e-06
Iter: 1270 loss: 5.2410189565419724e-06
Iter: 1271 loss: 5.2323859507094326e-06
Iter: 1272 loss: 5.2285635323129732e-06
Iter: 1273 loss: 5.2378414558587222e-06
Iter: 1274 loss: 5.2271946502037618e-06
Iter: 1275 loss: 5.2283650686723253e-06
Iter: 1276 loss: 5.2259024809477256e-06
Iter: 1277 loss: 5.2246693873311714e-06
Iter: 1278 loss: 5.2217131227509187e-06
Iter: 1279 loss: 5.2543478327717865e-06
Iter: 1280 loss: 5.2214139178385034e-06
Iter: 1281 loss: 5.2186747431942925e-06
Iter: 1282 loss: 5.2209121040533683e-06
Iter: 1283 loss: 5.2170358230453029e-06
Iter: 1284 loss: 5.2140199697829366e-06
Iter: 1285 loss: 5.2246724643275716e-06
Iter: 1286 loss: 5.2132420789175949e-06
Iter: 1287 loss: 5.2099742467803741e-06
Iter: 1288 loss: 5.2190771465413257e-06
Iter: 1289 loss: 5.2089229402411681e-06
Iter: 1290 loss: 5.20597619340843e-06
Iter: 1291 loss: 5.2069949666718581e-06
Iter: 1292 loss: 5.2038990681931369e-06
Iter: 1293 loss: 5.1996302841095047e-06
Iter: 1294 loss: 5.21710839499688e-06
Iter: 1295 loss: 5.1986957978101986e-06
Iter: 1296 loss: 5.1956805262954433e-06
Iter: 1297 loss: 5.2041474345747006e-06
Iter: 1298 loss: 5.1947169613600369e-06
Iter: 1299 loss: 5.1911583895842055e-06
Iter: 1300 loss: 5.1981199859619188e-06
Iter: 1301 loss: 5.1896855633921431e-06
Iter: 1302 loss: 5.1866043998985419e-06
Iter: 1303 loss: 5.1949330291970176e-06
Iter: 1304 loss: 5.1855883933873377e-06
Iter: 1305 loss: 5.1817425077324142e-06
Iter: 1306 loss: 5.1864139043745085e-06
Iter: 1307 loss: 5.179727535751387e-06
Iter: 1308 loss: 5.1788909219571363e-06
Iter: 1309 loss: 5.17796842660332e-06
Iter: 1310 loss: 5.1759233068930788e-06
Iter: 1311 loss: 5.1753448439255372e-06
Iter: 1312 loss: 5.174100086682622e-06
Iter: 1313 loss: 5.1721191555880765e-06
Iter: 1314 loss: 5.1689395588114316e-06
Iter: 1315 loss: 5.16891455713455e-06
Iter: 1316 loss: 5.1653949288127094e-06
Iter: 1317 loss: 5.1748554670414484e-06
Iter: 1318 loss: 5.1642292538701652e-06
Iter: 1319 loss: 5.1604385907278437e-06
Iter: 1320 loss: 5.17454311892375e-06
Iter: 1321 loss: 5.1595132772883149e-06
Iter: 1322 loss: 5.1561833112666324e-06
Iter: 1323 loss: 5.165419473156593e-06
Iter: 1324 loss: 5.1551083569088369e-06
Iter: 1325 loss: 5.1513098945643652e-06
Iter: 1326 loss: 5.1621363144198085e-06
Iter: 1327 loss: 5.1501113026217711e-06
Iter: 1328 loss: 5.1473741387992951e-06
Iter: 1329 loss: 5.1537336750141249e-06
Iter: 1330 loss: 5.1463621253172605e-06
Iter: 1331 loss: 5.143016221520434e-06
Iter: 1332 loss: 5.1451924416117182e-06
Iter: 1333 loss: 5.1408972756666511e-06
Iter: 1334 loss: 5.1374926721727084e-06
Iter: 1335 loss: 5.1496005707653594e-06
Iter: 1336 loss: 5.1366205908846315e-06
Iter: 1337 loss: 5.1327862556166366e-06
Iter: 1338 loss: 5.1428135013967941e-06
Iter: 1339 loss: 5.1314883769509335e-06
Iter: 1340 loss: 5.1300828036362189e-06
Iter: 1341 loss: 5.1298162613161686e-06
Iter: 1342 loss: 5.1278354340573033e-06
Iter: 1343 loss: 5.1298760400655122e-06
Iter: 1344 loss: 5.1267324830857669e-06
Iter: 1345 loss: 5.1252574413115065e-06
Iter: 1346 loss: 5.1224500336881118e-06
Iter: 1347 loss: 5.1826602432330631e-06
Iter: 1348 loss: 5.122438826336003e-06
Iter: 1349 loss: 5.1195244827641291e-06
Iter: 1350 loss: 5.1256241854933167e-06
Iter: 1351 loss: 5.1183682170562008e-06
Iter: 1352 loss: 5.1150843313186593e-06
Iter: 1353 loss: 5.1293002585669234e-06
Iter: 1354 loss: 5.1144129931769008e-06
Iter: 1355 loss: 5.1115879200648587e-06
Iter: 1356 loss: 5.1181975905664485e-06
Iter: 1357 loss: 5.1105486954523721e-06
Iter: 1358 loss: 5.1074093360116395e-06
Iter: 1359 loss: 5.1203724939585992e-06
Iter: 1360 loss: 5.1067290016665906e-06
Iter: 1361 loss: 5.1043858337846832e-06
Iter: 1362 loss: 5.1064335381750574e-06
Iter: 1363 loss: 5.1030109113036566e-06
Iter: 1364 loss: 5.0995537429956326e-06
Iter: 1365 loss: 5.1045482418350347e-06
Iter: 1366 loss: 5.097872092024286e-06
Iter: 1367 loss: 5.0947858196355587e-06
Iter: 1368 loss: 5.1063246845141294e-06
Iter: 1369 loss: 5.0940363374436206e-06
Iter: 1370 loss: 5.0905344179144276e-06
Iter: 1371 loss: 5.0987092717346362e-06
Iter: 1372 loss: 5.0892440169599534e-06
Iter: 1373 loss: 5.0867090290823405e-06
Iter: 1374 loss: 5.1092704403677375e-06
Iter: 1375 loss: 5.0865851108329884e-06
Iter: 1376 loss: 5.08465939823912e-06
Iter: 1377 loss: 5.0846557593177838e-06
Iter: 1378 loss: 5.0836261104452217e-06
Iter: 1379 loss: 5.0808694621157592e-06
Iter: 1380 loss: 5.0997778395595445e-06
Iter: 1381 loss: 5.0802318451060954e-06
Iter: 1382 loss: 5.0773378168985313e-06
Iter: 1383 loss: 5.0854985729647732e-06
Iter: 1384 loss: 5.076416250047132e-06
Iter: 1385 loss: 5.0734747925894924e-06
Iter: 1386 loss: 5.0794105245785892e-06
Iter: 1387 loss: 5.0722804460451961e-06
Iter: 1388 loss: 5.0693908811217207e-06
Iter: 1389 loss: 5.0761845547756665e-06
Iter: 1390 loss: 5.0683314874093208e-06
Iter: 1391 loss: 5.0652742522662316e-06
Iter: 1392 loss: 5.0809410480509827e-06
Iter: 1393 loss: 5.0647787790111643e-06
Iter: 1394 loss: 5.062532425583142e-06
Iter: 1395 loss: 5.0690455658628065e-06
Iter: 1396 loss: 5.0618340000180918e-06
Iter: 1397 loss: 5.0590558777511415e-06
Iter: 1398 loss: 5.0634760847983353e-06
Iter: 1399 loss: 5.0577661000930766e-06
Iter: 1400 loss: 5.055311677901091e-06
Iter: 1401 loss: 5.0647427128499509e-06
Iter: 1402 loss: 5.0547334598254984e-06
Iter: 1403 loss: 5.0520885238689214e-06
Iter: 1404 loss: 5.0517991476562337e-06
Iter: 1405 loss: 5.0498832634204914e-06
Iter: 1406 loss: 5.0471448112432081e-06
Iter: 1407 loss: 5.0714862348102424e-06
Iter: 1408 loss: 5.047010484663921e-06
Iter: 1409 loss: 5.0464645887689515e-06
Iter: 1410 loss: 5.0458600148901751e-06
Iter: 1411 loss: 5.0449477272174585e-06
Iter: 1412 loss: 5.0425417886786927e-06
Iter: 1413 loss: 5.0603607930743875e-06
Iter: 1414 loss: 5.0420412263277327e-06
Iter: 1415 loss: 5.039680251483947e-06
Iter: 1416 loss: 5.0447832078190234e-06
Iter: 1417 loss: 5.0387630653642175e-06
Iter: 1418 loss: 5.03625636524681e-06
Iter: 1419 loss: 5.0402803568238786e-06
Iter: 1420 loss: 5.0350981199939787e-06
Iter: 1421 loss: 5.0324785377650605e-06
Iter: 1422 loss: 5.040949980408385e-06
Iter: 1423 loss: 5.0317408944945842e-06
Iter: 1424 loss: 5.0289896909255429e-06
Iter: 1425 loss: 5.0339976830694251e-06
Iter: 1426 loss: 5.02780160407729e-06
Iter: 1427 loss: 5.025279838269397e-06
Iter: 1428 loss: 5.0334987971980938e-06
Iter: 1429 loss: 5.02457494072738e-06
Iter: 1430 loss: 5.0216695767572808e-06
Iter: 1431 loss: 5.0267806775863018e-06
Iter: 1432 loss: 5.0203906399097691e-06
Iter: 1433 loss: 5.0180072158895071e-06
Iter: 1434 loss: 5.0342703632048206e-06
Iter: 1435 loss: 5.0177754725563249e-06
Iter: 1436 loss: 5.015419905242719e-06
Iter: 1437 loss: 5.0177771726840962e-06
Iter: 1438 loss: 5.0140952324034855e-06
Iter: 1439 loss: 5.0117810818727051e-06
Iter: 1440 loss: 5.0261632586380375e-06
Iter: 1441 loss: 5.011509907734496e-06
Iter: 1442 loss: 5.0103268994869715e-06
Iter: 1443 loss: 5.0103208667224338e-06
Iter: 1444 loss: 5.0088294611613177e-06
Iter: 1445 loss: 5.0073164800248824e-06
Iter: 1446 loss: 5.007021707927544e-06
Iter: 1447 loss: 5.0055230622401429e-06
Iter: 1448 loss: 5.0044619143498363e-06
Iter: 1449 loss: 5.0039344908683638e-06
Iter: 1450 loss: 5.00173119731758e-06
Iter: 1451 loss: 5.0046309169562454e-06
Iter: 1452 loss: 5.000614140235377e-06
Iter: 1453 loss: 4.9980803349935591e-06
Iter: 1454 loss: 5.0057792587081987e-06
Iter: 1455 loss: 4.9973243731154639e-06
Iter: 1456 loss: 4.9945538079550926e-06
Iter: 1457 loss: 5.0006857672620516e-06
Iter: 1458 loss: 4.9934943320299806e-06
Iter: 1459 loss: 4.9910106513962225e-06
Iter: 1460 loss: 4.9995506393685641e-06
Iter: 1461 loss: 4.99035208446861e-06
Iter: 1462 loss: 4.9875983576747365e-06
Iter: 1463 loss: 4.9937500758945462e-06
Iter: 1464 loss: 4.9865519124839064e-06
Iter: 1465 loss: 4.9843066270211617e-06
Iter: 1466 loss: 4.9951292610813041e-06
Iter: 1467 loss: 4.9839089424178188e-06
Iter: 1468 loss: 4.98147642636137e-06
Iter: 1469 loss: 4.9840617135259217e-06
Iter: 1470 loss: 4.9801364470459851e-06
Iter: 1471 loss: 4.9778719694897008e-06
Iter: 1472 loss: 4.9953388767910774e-06
Iter: 1473 loss: 4.9777060451050952e-06
Iter: 1474 loss: 4.9761967239979623e-06
Iter: 1475 loss: 4.9897724658097207e-06
Iter: 1476 loss: 4.9761255043309907e-06
Iter: 1477 loss: 4.97426348549311e-06
Iter: 1478 loss: 4.9769499997567472e-06
Iter: 1479 loss: 4.9733570947449885e-06
Iter: 1480 loss: 4.9721468158289144e-06
Iter: 1481 loss: 4.9706001850518321e-06
Iter: 1482 loss: 4.9704877192654162e-06
Iter: 1483 loss: 4.96841614578738e-06
Iter: 1484 loss: 4.9701610629973185e-06
Iter: 1485 loss: 4.9671875655590131e-06
Iter: 1486 loss: 4.9645222788368154e-06
Iter: 1487 loss: 4.9718288247764056e-06
Iter: 1488 loss: 4.9636535722112315e-06
Iter: 1489 loss: 4.9609404302300547e-06
Iter: 1490 loss: 4.9656144108166727e-06
Iter: 1491 loss: 4.9597322963763092e-06
Iter: 1492 loss: 4.9572074420490037e-06
Iter: 1493 loss: 4.9696419720215418e-06
Iter: 1494 loss: 4.9567737101830585e-06
Iter: 1495 loss: 4.9542571119166524e-06
Iter: 1496 loss: 4.9554856374398156e-06
Iter: 1497 loss: 4.95257073728037e-06
Iter: 1498 loss: 4.9499671194526755e-06
Iter: 1499 loss: 4.9664409584356944e-06
Iter: 1500 loss: 4.9496722478553364e-06
Iter: 1501 loss: 4.9470848412987256e-06
Iter: 1502 loss: 4.9492055151530857e-06
Iter: 1503 loss: 4.945538449220489e-06
Iter: 1504 loss: 4.9432354029231e-06
Iter: 1505 loss: 4.9650220962917364e-06
Iter: 1506 loss: 4.9431439069811174e-06
Iter: 1507 loss: 4.9413857813701828e-06
Iter: 1508 loss: 4.9536052285928996e-06
Iter: 1509 loss: 4.9412215372122689e-06
Iter: 1510 loss: 4.9391989506184906e-06
Iter: 1511 loss: 4.9478292527811881e-06
Iter: 1512 loss: 4.938777609593591e-06
Iter: 1513 loss: 4.9376417973989939e-06
Iter: 1514 loss: 4.9358432372323058e-06
Iter: 1515 loss: 4.9358252838044786e-06
Iter: 1516 loss: 4.9337429872212908e-06
Iter: 1517 loss: 4.93441745409193e-06
Iter: 1518 loss: 4.9322637681313613e-06
Iter: 1519 loss: 4.9294908093669643e-06
Iter: 1520 loss: 4.9385072066816874e-06
Iter: 1521 loss: 4.9287140066387316e-06
Iter: 1522 loss: 4.9260859124772047e-06
Iter: 1523 loss: 4.93144551287359e-06
Iter: 1524 loss: 4.9250256035966941e-06
Iter: 1525 loss: 4.922484870590007e-06
Iter: 1526 loss: 4.933409747142393e-06
Iter: 1527 loss: 4.9219609411728525e-06
Iter: 1528 loss: 4.9192940583466862e-06
Iter: 1529 loss: 4.9213664237571855e-06
Iter: 1530 loss: 4.917677081179093e-06
Iter: 1531 loss: 4.9149817238006149e-06
Iter: 1532 loss: 4.9354080095585335e-06
Iter: 1533 loss: 4.9147753263388085e-06
Iter: 1534 loss: 4.9124121117972657e-06
Iter: 1535 loss: 4.9114023129958982e-06
Iter: 1536 loss: 4.9101808848467455e-06
Iter: 1537 loss: 4.9072883730316355e-06
Iter: 1538 loss: 4.9321065807522461e-06
Iter: 1539 loss: 4.9071300852651948e-06
Iter: 1540 loss: 4.9048770892778383e-06
Iter: 1541 loss: 4.9129071932151186e-06
Iter: 1542 loss: 4.9043016224751575e-06
Iter: 1543 loss: 4.901681983314901e-06
Iter: 1544 loss: 4.9306254240728166e-06
Iter: 1545 loss: 4.9016284609204429e-06
Iter: 1546 loss: 4.9006307949235741e-06
Iter: 1547 loss: 4.8987614233647984e-06
Iter: 1548 loss: 4.94031714119176e-06
Iter: 1549 loss: 4.8987570449211467e-06
Iter: 1550 loss: 4.8966259080528271e-06
Iter: 1551 loss: 4.8993477530711513e-06
Iter: 1552 loss: 4.8955317550897042e-06
Iter: 1553 loss: 4.8929877517746647e-06
Iter: 1554 loss: 4.89936053935798e-06
Iter: 1555 loss: 4.8920975373355826e-06
Iter: 1556 loss: 4.8896826552896974e-06
Iter: 1557 loss: 4.8933401789545816e-06
Iter: 1558 loss: 4.88853395700166e-06
Iter: 1559 loss: 4.8858910305690782e-06
Iter: 1560 loss: 4.8943816407450082e-06
Iter: 1561 loss: 4.8851419835931513e-06
Iter: 1562 loss: 4.8823657498766471e-06
Iter: 1563 loss: 4.8842598483638024e-06
Iter: 1564 loss: 4.8806268868762825e-06
Iter: 1565 loss: 4.87778706043103e-06
Iter: 1566 loss: 4.9004493355131135e-06
Iter: 1567 loss: 4.8775964818208816e-06
Iter: 1568 loss: 4.8751654764482753e-06
Iter: 1569 loss: 4.8744725947661557e-06
Iter: 1570 loss: 4.872996195435682e-06
Iter: 1571 loss: 4.8701040949804322e-06
Iter: 1572 loss: 4.8956310402341726e-06
Iter: 1573 loss: 4.8699590704614406e-06
Iter: 1574 loss: 4.8675454147933727e-06
Iter: 1575 loss: 4.8731743757732751e-06
Iter: 1576 loss: 4.8666556151178341e-06
Iter: 1577 loss: 4.8647150668872378e-06
Iter: 1578 loss: 4.8645701487698739e-06
Iter: 1579 loss: 4.8637510573091778e-06
Iter: 1580 loss: 4.8619063575164666e-06
Iter: 1581 loss: 4.8870411394412324e-06
Iter: 1582 loss: 4.8617995894380912e-06
Iter: 1583 loss: 4.8597038734890152e-06
Iter: 1584 loss: 4.8610348496314218e-06
Iter: 1585 loss: 4.85836964862284e-06
Iter: 1586 loss: 4.8556599464092356e-06
Iter: 1587 loss: 4.8656580217741779e-06
Iter: 1588 loss: 4.8549923586670585e-06
Iter: 1589 loss: 4.8525023778740509e-06
Iter: 1590 loss: 4.8565904424079006e-06
Iter: 1591 loss: 4.85136494597974e-06
Iter: 1592 loss: 4.8484911015660171e-06
Iter: 1593 loss: 4.8555373787680185e-06
Iter: 1594 loss: 4.8474689900430819e-06
Iter: 1595 loss: 4.8444116464972446e-06
Iter: 1596 loss: 4.8482651857448351e-06
Iter: 1597 loss: 4.8428332400145494e-06
Iter: 1598 loss: 4.8397678240613209e-06
Iter: 1599 loss: 4.86051523209093e-06
Iter: 1600 loss: 4.8394645708411071e-06
Iter: 1601 loss: 4.837081718470397e-06
Iter: 1602 loss: 4.8365538329996122e-06
Iter: 1603 loss: 4.835007285649368e-06
Iter: 1604 loss: 4.8316320253423e-06
Iter: 1605 loss: 4.8552853156167849e-06
Iter: 1606 loss: 4.8313221268515212e-06
Iter: 1607 loss: 4.82872810882843e-06
Iter: 1608 loss: 4.8300835926716839e-06
Iter: 1609 loss: 4.8270109674842252e-06
Iter: 1610 loss: 4.8285230945414e-06
Iter: 1611 loss: 4.8257180442542251e-06
Iter: 1612 loss: 4.824919961852565e-06
Iter: 1613 loss: 4.8230288169653573e-06
Iter: 1614 loss: 4.8447963074589674e-06
Iter: 1615 loss: 4.8228551901817677e-06
Iter: 1616 loss: 4.821029188338607e-06
Iter: 1617 loss: 4.81956096913933e-06
Iter: 1618 loss: 4.81901251192998e-06
Iter: 1619 loss: 4.8160010975661214e-06
Iter: 1620 loss: 4.8337011281519321e-06
Iter: 1621 loss: 4.8156105494998326e-06
Iter: 1622 loss: 4.8129968678884233e-06
Iter: 1623 loss: 4.8203618091693468e-06
Iter: 1624 loss: 4.8121640371803496e-06
Iter: 1625 loss: 4.8094521024081483e-06
Iter: 1626 loss: 4.8140527178847036e-06
Iter: 1627 loss: 4.8082344008482089e-06
Iter: 1628 loss: 4.8054738964610851e-06
Iter: 1629 loss: 4.8101507115423609e-06
Iter: 1630 loss: 4.8042336775464868e-06
Iter: 1631 loss: 4.801662965784452e-06
Iter: 1632 loss: 4.8172823208412459e-06
Iter: 1633 loss: 4.8013489148590535e-06
Iter: 1634 loss: 4.7991295135888e-06
Iter: 1635 loss: 4.7981891009510926e-06
Iter: 1636 loss: 4.7970369733471566e-06
Iter: 1637 loss: 4.7938616876085072e-06
Iter: 1638 loss: 4.8193779990673471e-06
Iter: 1639 loss: 4.7936525090281485e-06
Iter: 1640 loss: 4.791230664810427e-06
Iter: 1641 loss: 4.7925987613278531e-06
Iter: 1642 loss: 4.78965102470468e-06
Iter: 1643 loss: 4.7914847734030974e-06
Iter: 1644 loss: 4.7886074536466207e-06
Iter: 1645 loss: 4.7878500675017626e-06
Iter: 1646 loss: 4.7861334024033728e-06
Iter: 1647 loss: 4.8090534236327941e-06
Iter: 1648 loss: 4.7860276895942482e-06
Iter: 1649 loss: 4.7842730502175125e-06
Iter: 1650 loss: 4.7825935258175806e-06
Iter: 1651 loss: 4.7822003882459926e-06
Iter: 1652 loss: 4.7791299880747511e-06
Iter: 1653 loss: 4.7944424952220081e-06
Iter: 1654 loss: 4.778611892150111e-06
Iter: 1655 loss: 4.77597624522423e-06
Iter: 1656 loss: 4.781884003303129e-06
Iter: 1657 loss: 4.7749768696731454e-06
Iter: 1658 loss: 4.7721636832020151e-06
Iter: 1659 loss: 4.7812444444323865e-06
Iter: 1660 loss: 4.7713699312730132e-06
Iter: 1661 loss: 4.7687874161729523e-06
Iter: 1662 loss: 4.7755965866578564e-06
Iter: 1663 loss: 4.7679186677437752e-06
Iter: 1664 loss: 4.76543362053375e-06
Iter: 1665 loss: 4.7737446613207331e-06
Iter: 1666 loss: 4.7647564141673561e-06
Iter: 1667 loss: 4.7624569138724859e-06
Iter: 1668 loss: 4.7619996162793792e-06
Iter: 1669 loss: 4.7604725288993879e-06
Iter: 1670 loss: 4.7569585644638907e-06
Iter: 1671 loss: 4.7768442821348028e-06
Iter: 1672 loss: 4.7564723212725023e-06
Iter: 1673 loss: 4.7538909144826548e-06
Iter: 1674 loss: 4.7560991559666557e-06
Iter: 1675 loss: 4.7523668129001274e-06
Iter: 1676 loss: 4.7527129195290468e-06
Iter: 1677 loss: 4.7511285790839661e-06
Iter: 1678 loss: 4.749883616407704e-06
Iter: 1679 loss: 4.74891636339016e-06
Iter: 1680 loss: 4.7485246413179443e-06
Iter: 1681 loss: 4.7472093016405422e-06
Iter: 1682 loss: 4.7443383779067515e-06
Iter: 1683 loss: 4.787340037927857e-06
Iter: 1684 loss: 4.74421821919298e-06
Iter: 1685 loss: 4.740897203881862e-06
Iter: 1686 loss: 4.760223386529314e-06
Iter: 1687 loss: 4.7404588385930168e-06
Iter: 1688 loss: 4.737772819096934e-06
Iter: 1689 loss: 4.7444189420864585e-06
Iter: 1690 loss: 4.7368239938828034e-06
Iter: 1691 loss: 4.7339643622307064e-06
Iter: 1692 loss: 4.7460652776037993e-06
Iter: 1693 loss: 4.7333627916397586e-06
Iter: 1694 loss: 4.7308172101774636e-06
Iter: 1695 loss: 4.7362130418751687e-06
Iter: 1696 loss: 4.7298152768412561e-06
Iter: 1697 loss: 4.7273094490224747e-06
Iter: 1698 loss: 4.7341994760438337e-06
Iter: 1699 loss: 4.7264949217828118e-06
Iter: 1700 loss: 4.7238456635205447e-06
Iter: 1701 loss: 4.7233834557149782e-06
Iter: 1702 loss: 4.7215807479986328e-06
Iter: 1703 loss: 4.717692803867039e-06
Iter: 1704 loss: 4.7458358586264408e-06
Iter: 1705 loss: 4.717360904873205e-06
Iter: 1706 loss: 4.71460667142628e-06
Iter: 1707 loss: 4.7176272785381052e-06
Iter: 1708 loss: 4.7131064109988382e-06
Iter: 1709 loss: 4.7113230525840761e-06
Iter: 1710 loss: 4.7111390039380829e-06
Iter: 1711 loss: 4.7091880404027188e-06
Iter: 1712 loss: 4.7156287955961737e-06
Iter: 1713 loss: 4.7086496140094723e-06
Iter: 1714 loss: 4.70763779720999e-06
Iter: 1715 loss: 4.70482151928127e-06
Iter: 1716 loss: 4.72028173871951e-06
Iter: 1717 loss: 4.7039699376379377e-06
Iter: 1718 loss: 4.7005233190415407e-06
Iter: 1719 loss: 4.7220497824784146e-06
Iter: 1720 loss: 4.7001231919074629e-06
Iter: 1721 loss: 4.69733719365769e-06
Iter: 1722 loss: 4.70088239704577e-06
Iter: 1723 loss: 4.6959044065432449e-06
Iter: 1724 loss: 4.6925169814389427e-06
Iter: 1725 loss: 4.707743393652411e-06
Iter: 1726 loss: 4.691857110756411e-06
Iter: 1727 loss: 4.6890173293202089e-06
Iter: 1728 loss: 4.7019780522562491e-06
Iter: 1729 loss: 4.6884751820942892e-06
Iter: 1730 loss: 4.68606810475435e-06
Iter: 1731 loss: 4.6912141287901806e-06
Iter: 1732 loss: 4.6851262114727017e-06
Iter: 1733 loss: 4.6824943103728533e-06
Iter: 1734 loss: 4.6817181166794708e-06
Iter: 1735 loss: 4.6801364338335344e-06
Iter: 1736 loss: 4.676316801697839e-06
Iter: 1737 loss: 4.703388609869438e-06
Iter: 1738 loss: 4.6759746899899941e-06
Iter: 1739 loss: 4.6733287311044874e-06
Iter: 1740 loss: 4.6760600299700253e-06
Iter: 1741 loss: 4.67185640568646e-06
Iter: 1742 loss: 4.6704088956211614e-06
Iter: 1743 loss: 4.6701090181297179e-06
Iter: 1744 loss: 4.668473334352078e-06
Iter: 1745 loss: 4.67604613097252e-06
Iter: 1746 loss: 4.668167097447878e-06
Iter: 1747 loss: 4.6672982205365319e-06
Iter: 1748 loss: 4.6650014907716067e-06
Iter: 1749 loss: 4.6818112898812662e-06
Iter: 1750 loss: 4.66451571508249e-06
Iter: 1751 loss: 4.6619098291715989e-06
Iter: 1752 loss: 4.6769332259841987e-06
Iter: 1753 loss: 4.6615604747439829e-06
Iter: 1754 loss: 4.659326195127458e-06
Iter: 1755 loss: 4.6599003122729076e-06
Iter: 1756 loss: 4.6576999899447883e-06
Iter: 1757 loss: 4.6544249014802883e-06
Iter: 1758 loss: 4.6651631780779159e-06
Iter: 1759 loss: 4.6535148029089736e-06
Iter: 1760 loss: 4.65081488825348e-06
Iter: 1761 loss: 4.6656196665913084e-06
Iter: 1762 loss: 4.6504211399604982e-06
Iter: 1763 loss: 4.648096491412952e-06
Iter: 1764 loss: 4.6572534619001224e-06
Iter: 1765 loss: 4.6475641612576108e-06
Iter: 1766 loss: 4.6452854451307707e-06
Iter: 1767 loss: 4.64436599432784e-06
Iter: 1768 loss: 4.6431543752123792e-06
Iter: 1769 loss: 4.6397084746421186e-06
Iter: 1770 loss: 4.6567117859822453e-06
Iter: 1771 loss: 4.6391178797442907e-06
Iter: 1772 loss: 4.6366680743174e-06
Iter: 1773 loss: 4.6392327168158868e-06
Iter: 1774 loss: 4.6353115341821468e-06
Iter: 1775 loss: 4.6338120129226878e-06
Iter: 1776 loss: 4.6336297054492414e-06
Iter: 1777 loss: 4.6321087223483772e-06
Iter: 1778 loss: 4.6428185003339593e-06
Iter: 1779 loss: 4.6319705567214986e-06
Iter: 1780 loss: 4.631221629541234e-06
Iter: 1781 loss: 4.6291183541205113e-06
Iter: 1782 loss: 4.6400221429208007e-06
Iter: 1783 loss: 4.6284424912188857e-06
Iter: 1784 loss: 4.6258418113576859e-06
Iter: 1785 loss: 4.641137253003155e-06
Iter: 1786 loss: 4.6255048939914316e-06
Iter: 1787 loss: 4.6230877448929449e-06
Iter: 1788 loss: 4.6231039327975452e-06
Iter: 1789 loss: 4.6211586647140326e-06
Iter: 1790 loss: 4.6176768602153422e-06
Iter: 1791 loss: 4.6302898487550519e-06
Iter: 1792 loss: 4.6168021825472384e-06
Iter: 1793 loss: 4.6140615726139459e-06
Iter: 1794 loss: 4.6254649566767272e-06
Iter: 1795 loss: 4.6134732289123619e-06
Iter: 1796 loss: 4.6109404125104278e-06
Iter: 1797 loss: 4.6233324885746021e-06
Iter: 1798 loss: 4.61050130735784e-06
Iter: 1799 loss: 4.6080401535457451e-06
Iter: 1800 loss: 4.6083602825432661e-06
Iter: 1801 loss: 4.6061640821508845e-06
Iter: 1802 loss: 4.60297965452695e-06
Iter: 1803 loss: 4.6178177223845019e-06
Iter: 1804 loss: 4.6023884054212146e-06
Iter: 1805 loss: 4.6000861071453059e-06
Iter: 1806 loss: 4.6022022650478987e-06
Iter: 1807 loss: 4.5987556788423483e-06
Iter: 1808 loss: 4.5966459229827e-06
Iter: 1809 loss: 4.6303646514562411e-06
Iter: 1810 loss: 4.596645922257e-06
Iter: 1811 loss: 4.595283265598104e-06
Iter: 1812 loss: 4.5952710567936715e-06
Iter: 1813 loss: 4.5946569229298961e-06
Iter: 1814 loss: 4.5927278798389337e-06
Iter: 1815 loss: 4.5957437351375792e-06
Iter: 1816 loss: 4.5913824651115517e-06
Iter: 1817 loss: 4.5885755487371662e-06
Iter: 1818 loss: 4.6074484712226319e-06
Iter: 1819 loss: 4.58829397851926e-06
Iter: 1820 loss: 4.5858053895761918e-06
Iter: 1821 loss: 4.5877400354134416e-06
Iter: 1822 loss: 4.5842964337531126e-06
Iter: 1823 loss: 4.581334322392224e-06
Iter: 1824 loss: 4.5946573995496816e-06
Iter: 1825 loss: 4.5807578735737168e-06
Iter: 1826 loss: 4.5785224515461818e-06
Iter: 1827 loss: 4.5862111045092632e-06
Iter: 1828 loss: 4.5779298659447738e-06
Iter: 1829 loss: 4.5758100977263392e-06
Iter: 1830 loss: 4.5820066775354729e-06
Iter: 1831 loss: 4.5751556580789644e-06
Iter: 1832 loss: 4.5729977282780821e-06
Iter: 1833 loss: 4.5777783972666942e-06
Iter: 1834 loss: 4.5721729995065367e-06
Iter: 1835 loss: 4.569869199785783e-06
Iter: 1836 loss: 4.5800541697153325e-06
Iter: 1837 loss: 4.56941074737681e-06
Iter: 1838 loss: 4.5676618052472724e-06
Iter: 1839 loss: 4.5684422136341082e-06
Iter: 1840 loss: 4.5664722802127091e-06
Iter: 1841 loss: 4.5640768178473075e-06
Iter: 1842 loss: 4.5750361002774129e-06
Iter: 1843 loss: 4.56362081515798e-06
Iter: 1844 loss: 4.56389873965408e-06
Iter: 1845 loss: 4.56271798420195e-06
Iter: 1846 loss: 4.562226363466008e-06
Iter: 1847 loss: 4.5606023467428765e-06
Iter: 1848 loss: 4.5607023792875309e-06
Iter: 1849 loss: 4.5589393665310155e-06
Iter: 1850 loss: 4.5562204566713085e-06
Iter: 1851 loss: 4.5745495525999993e-06
Iter: 1852 loss: 4.555949252301383e-06
Iter: 1853 loss: 4.5537558769399592e-06
Iter: 1854 loss: 4.5547152538377672e-06
Iter: 1855 loss: 4.5522593693658885e-06
Iter: 1856 loss: 4.5495343094315871e-06
Iter: 1857 loss: 4.5637686843007447e-06
Iter: 1858 loss: 4.54910533718466e-06
Iter: 1859 loss: 4.5470535971684911e-06
Iter: 1860 loss: 4.5551906566550224e-06
Iter: 1861 loss: 4.546587339137513e-06
Iter: 1862 loss: 4.5446543106575868e-06
Iter: 1863 loss: 4.5504307723686346e-06
Iter: 1864 loss: 4.5440688984854709e-06
Iter: 1865 loss: 4.542126962290148e-06
Iter: 1866 loss: 4.5450961078637562e-06
Iter: 1867 loss: 4.5412074917953758e-06
Iter: 1868 loss: 4.539059797287649e-06
Iter: 1869 loss: 4.5498570011246846e-06
Iter: 1870 loss: 4.5387018721888658e-06
Iter: 1871 loss: 4.5369308041242962e-06
Iter: 1872 loss: 4.5382755508239859e-06
Iter: 1873 loss: 4.53585026150366e-06
Iter: 1874 loss: 4.5334936357336269e-06
Iter: 1875 loss: 4.5427660403338119e-06
Iter: 1876 loss: 4.532953078593897e-06
Iter: 1877 loss: 4.5336437484994657e-06
Iter: 1878 loss: 4.5322674151945532e-06
Iter: 1879 loss: 4.5317453475899081e-06
Iter: 1880 loss: 4.5302298632569456e-06
Iter: 1881 loss: 4.53640025768758e-06
Iter: 1882 loss: 4.5296200513499455e-06
Iter: 1883 loss: 4.5276968495064069e-06
Iter: 1884 loss: 4.5331067072997765e-06
Iter: 1885 loss: 4.5270832384014375e-06
Iter: 1886 loss: 4.5254751598166976e-06
Iter: 1887 loss: 4.5260462154380338e-06
Iter: 1888 loss: 4.5243454450586e-06
Iter: 1889 loss: 4.5221686725564095e-06
Iter: 1890 loss: 4.5317077443076122e-06
Iter: 1891 loss: 4.5217305023888671e-06
Iter: 1892 loss: 4.5198922996575092e-06
Iter: 1893 loss: 4.5250828537161363e-06
Iter: 1894 loss: 4.5193076435951558e-06
Iter: 1895 loss: 4.5174905487875352e-06
Iter: 1896 loss: 4.5240485624118011e-06
Iter: 1897 loss: 4.5170322361176679e-06
Iter: 1898 loss: 4.5152171180945755e-06
Iter: 1899 loss: 4.52011552209674e-06
Iter: 1900 loss: 4.5146178095159681e-06
Iter: 1901 loss: 4.5127036676245842e-06
Iter: 1902 loss: 4.518817278727509e-06
Iter: 1903 loss: 4.5121582456877812e-06
Iter: 1904 loss: 4.5103444838031172e-06
Iter: 1905 loss: 4.5109529899215618e-06
Iter: 1906 loss: 4.5090612750107649e-06
Iter: 1907 loss: 4.5066396432395122e-06
Iter: 1908 loss: 4.5215060833422233e-06
Iter: 1909 loss: 4.5063493064561682e-06
Iter: 1910 loss: 4.5065007897527719e-06
Iter: 1911 loss: 4.5056648423344908e-06
Iter: 1912 loss: 4.5050563824039007e-06
Iter: 1913 loss: 4.5035041986602312e-06
Iter: 1914 loss: 4.5169103683224513e-06
Iter: 1915 loss: 4.5032500697317431e-06
Iter: 1916 loss: 4.5015272290024585e-06
Iter: 1917 loss: 4.5039571616773023e-06
Iter: 1918 loss: 4.5006800138107573e-06
Iter: 1919 loss: 4.4990159654908834e-06
Iter: 1920 loss: 4.4992341875404541e-06
Iter: 1921 loss: 4.4977479390053995e-06
Iter: 1922 loss: 4.4952981576565708e-06
Iter: 1923 loss: 4.5063725118331531e-06
Iter: 1924 loss: 4.4948245747083992e-06
Iter: 1925 loss: 4.4927689627075726e-06
Iter: 1926 loss: 4.49774750293018e-06
Iter: 1927 loss: 4.4920312193216906e-06
Iter: 1928 loss: 4.4899478155632112e-06
Iter: 1929 loss: 4.4959592511529482e-06
Iter: 1930 loss: 4.489297315454346e-06
Iter: 1931 loss: 4.4872521588065368e-06
Iter: 1932 loss: 4.4931662415103935e-06
Iter: 1933 loss: 4.4866146704538339e-06
Iter: 1934 loss: 4.4846499044816238e-06
Iter: 1935 loss: 4.4956905640409821e-06
Iter: 1936 loss: 4.4843748294749219e-06
Iter: 1937 loss: 4.4826804895461636e-06
Iter: 1938 loss: 4.4841823707592494e-06
Iter: 1939 loss: 4.4816904155299109e-06
Iter: 1940 loss: 4.4795585195908811e-06
Iter: 1941 loss: 4.4871835431645058e-06
Iter: 1942 loss: 4.4790156689733708e-06
Iter: 1943 loss: 4.4791859441051626e-06
Iter: 1944 loss: 4.47842390934349e-06
Iter: 1945 loss: 4.4777960013472911e-06
Iter: 1946 loss: 4.4762816786364373e-06
Iter: 1947 loss: 4.4926529818846723e-06
Iter: 1948 loss: 4.4761208722283937e-06
Iter: 1949 loss: 4.4746504911418507e-06
Iter: 1950 loss: 4.4766639926129364e-06
Iter: 1951 loss: 4.473917796321899e-06
Iter: 1952 loss: 4.4724050641337883e-06
Iter: 1953 loss: 4.4727565746257075e-06
Iter: 1954 loss: 4.4712942581314095e-06
Iter: 1955 loss: 4.4689798615716282e-06
Iter: 1956 loss: 4.47967828804558e-06
Iter: 1957 loss: 4.4685455525054488e-06
Iter: 1958 loss: 4.4667422112835687e-06
Iter: 1959 loss: 4.4702408703429529e-06
Iter: 1960 loss: 4.46599204110088e-06
Iter: 1961 loss: 4.4641563133796594e-06
Iter: 1962 loss: 4.4689207229656278e-06
Iter: 1963 loss: 4.4635311381680816e-06
Iter: 1964 loss: 4.4617222476921283e-06
Iter: 1965 loss: 4.4667358121787949e-06
Iter: 1966 loss: 4.4611379059272335e-06
Iter: 1967 loss: 4.4593554852243378e-06
Iter: 1968 loss: 4.46675732350618e-06
Iter: 1969 loss: 4.45897195018008e-06
Iter: 1970 loss: 4.4572698036558063e-06
Iter: 1971 loss: 4.46092965924691e-06
Iter: 1972 loss: 4.4566061785342372e-06
Iter: 1973 loss: 4.4546218680449925e-06
Iter: 1974 loss: 4.4618476189470666e-06
Iter: 1975 loss: 4.4541261228869951e-06
Iter: 1976 loss: 4.4534380162340922e-06
Iter: 1977 loss: 4.4533416707408793e-06
Iter: 1978 loss: 4.4523973263114724e-06
Iter: 1979 loss: 4.4514756932002931e-06
Iter: 1980 loss: 4.4512723815118821e-06
Iter: 1981 loss: 4.4500490956115972e-06
Iter: 1982 loss: 4.4488121330176407e-06
Iter: 1983 loss: 4.4485686095218528e-06
Iter: 1984 loss: 4.44673846323455e-06
Iter: 1985 loss: 4.446804807039557e-06
Iter: 1986 loss: 4.4452940278508277e-06
Iter: 1987 loss: 4.4425080456825566e-06
Iter: 1988 loss: 4.459961602502985e-06
Iter: 1989 loss: 4.4421865433310785e-06
Iter: 1990 loss: 4.4402517219901258e-06
Iter: 1991 loss: 4.4445221577708485e-06
Iter: 1992 loss: 4.4395104420213071e-06
Iter: 1993 loss: 4.4375192905894424e-06
Iter: 1994 loss: 4.44426031003936e-06
Iter: 1995 loss: 4.4369830766371857e-06
Iter: 1996 loss: 4.435082178232085e-06
Iter: 1997 loss: 4.4401110003956052e-06
Iter: 1998 loss: 4.4344444756651012e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.8/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi3
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi3
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi3 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi3
+ date
Sun Nov  8 02:49:11 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi3/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi2.8/300_300_300_1 --function f1 --psi 2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi3/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a04c40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a04f2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a04c4510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a04df158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a04dfb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a0348ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a02de7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a0390840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a0390158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a03b9378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a033c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a0237730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a02372f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a028a9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a026f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a024ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a024fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a01cd488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a017b840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a01ab488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a01ab620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a01b2f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a01ab400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a00b2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a00b2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a006c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a006c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a0021620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a0021400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2a00dc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff254797b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff254793510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2547f0048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2547e5a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff254754598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7ff2546c57b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.006913747961127654
test_loss: 0.011118736489683638
train_loss: 0.006504121564123304
test_loss: 0.010402873959686377
train_loss: 0.006181463101883469
test_loss: 0.010585062301931836
train_loss: 0.0064401258491091785
test_loss: 0.010064485856411731
train_loss: 0.006786603479734628
test_loss: 0.009819430521854937
train_loss: 0.006743586539158025
test_loss: 0.010029477153260965
train_loss: 0.007242534045515772
test_loss: 0.010186461639033449
train_loss: 0.005732073159258147
test_loss: 0.009888476695207168
train_loss: 0.006436176270179717
test_loss: 0.010364156058416148
train_loss: 0.005707831958915774
test_loss: 0.009657160215579897
train_loss: 0.006561475412515408
test_loss: 0.009723181176838264
train_loss: 0.0063218153516582795
test_loss: 0.009997736369472036
train_loss: 0.00614841215084041
test_loss: 0.009645162664887961
train_loss: 0.006221329177493213
test_loss: 0.009473074274419667
train_loss: 0.005417961052158618
test_loss: 0.00914087225027633
train_loss: 0.0060259890563890885
test_loss: 0.009733049050797799
train_loss: 0.005665722054420899
test_loss: 0.009505496842649216
train_loss: 0.0058811921048066845
test_loss: 0.009764861745755546
train_loss: 0.00559147870892654
test_loss: 0.009654070216022952
train_loss: 0.005923026362625446
test_loss: 0.009543520222146981
train_loss: 0.005753724995840181
test_loss: 0.009456022871044682
train_loss: 0.005931135428373571
test_loss: 0.00982670756211525
train_loss: 0.005770075286031216
test_loss: 0.00973157673757263
train_loss: 0.005611577744004568
test_loss: 0.009442194989514124
train_loss: 0.005886423229730292
test_loss: 0.009750471072760544
train_loss: 0.005498090024869578
test_loss: 0.009382219853926663
train_loss: 0.0054396488091125305
test_loss: 0.009296697579294078
train_loss: 0.005298506164730389
test_loss: 0.009202481140711028
train_loss: 0.005714647580813427
test_loss: 0.009405033721499086
train_loss: 0.005550470516389284
test_loss: 0.00945039578689848
train_loss: 0.0054217123203103386
test_loss: 0.009331320874427505
train_loss: 0.005116131132591073
test_loss: 0.0095167797069951
train_loss: 0.0050739302614078476
test_loss: 0.009143899379951846
train_loss: 0.005209397644827533
test_loss: 0.009254823745468703
train_loss: 0.0049022686154933945
test_loss: 0.00959590152978912
train_loss: 0.004885566245613029
test_loss: 0.009111553803558815
train_loss: 0.005226354402847699
test_loss: 0.009305951339692071
train_loss: 0.004943972890017363
test_loss: 0.00911077982482833
train_loss: 0.0047681957866976725
test_loss: 0.009149295165748219
train_loss: 0.0058478949440756624
test_loss: 0.009283310471184924
train_loss: 0.00494145309989324
test_loss: 0.009133613442633727
train_loss: 0.005238328015162828
test_loss: 0.009098834654362599
train_loss: 0.005723369479652258
test_loss: 0.009386009473064809
train_loss: 0.005240403395192423
test_loss: 0.008949660603356082
train_loss: 0.005245548162378231
test_loss: 0.009150249753428128
train_loss: 0.005092962566404345
test_loss: 0.009061704957818075
train_loss: 0.004724796888674661
test_loss: 0.0091930943954181
train_loss: 0.004877550074391417
test_loss: 0.009087394872728324
train_loss: 0.005036142994536219
test_loss: 0.009208679352500646
train_loss: 0.004793185169667933
test_loss: 0.008921308723557796
train_loss: 0.005074744617701455
test_loss: 0.008964748531644706
train_loss: 0.004569417673417781
test_loss: 0.009023221620891612
train_loss: 0.005269358030311346
test_loss: 0.00946967792776024
train_loss: 0.005110424126128458
test_loss: 0.009023158520276794
train_loss: 0.00549278834249802
test_loss: 0.00919247505896217
train_loss: 0.005431155673059741
test_loss: 0.008930366873474392
train_loss: 0.004693474681135522
test_loss: 0.009043010553496695
train_loss: 0.005272823789114453
test_loss: 0.008985551745188681
train_loss: 0.005200157593872289
test_loss: 0.009101558834103397
train_loss: 0.00453556327556632
test_loss: 0.008875629967187564
train_loss: 0.0049838496441017895
test_loss: 0.009208306663654747
train_loss: 0.005318844575006012
test_loss: 0.009072375585189703
train_loss: 0.00498241995171646
test_loss: 0.009377823794806442
train_loss: 0.0049111521907213955
test_loss: 0.009073319943447592
train_loss: 0.0052600184817672355
test_loss: 0.009091598107991051
train_loss: 0.0051846399420074595
test_loss: 0.009046476564174786
train_loss: 0.005447392385864949
test_loss: 0.008930843738914248
train_loss: 0.005238940967459124
test_loss: 0.00887178134188121
train_loss: 0.005514350619018138
test_loss: 0.008980650942993612
train_loss: 0.0042368195741653205
test_loss: 0.008840684789112957
train_loss: 0.005421028665943978
test_loss: 0.009161959136863254
train_loss: 0.005157748164092032
test_loss: 0.008921467750718573
train_loss: 0.004222991336853962
test_loss: 0.009038762493309066
train_loss: 0.004697149627988216
test_loss: 0.008780088156561141
train_loss: 0.004753662342936065
test_loss: 0.00892224109736911
train_loss: 0.004771896811124773
test_loss: 0.009119475633928776
train_loss: 0.004537675217787985
test_loss: 0.008798057957190582
train_loss: 0.004767575689981056
test_loss: 0.008947874276607742
train_loss: 0.004557495026356468
test_loss: 0.00883350075855971
train_loss: 0.004824805104590101
test_loss: 0.008940442806776352
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi3/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi3/300_300_300_1 --optimizer lbfgs --function f1 --psi 2 --phi 3 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi2_phi3/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcfbc2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dd028ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dd028bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dd0282f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcf597b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dce979d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcee58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dce4d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dce4d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcee56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcdc9598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcdf2e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dce03840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcdbd510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcd63400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcd63268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcd7f6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcd2f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dccd97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dccd9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcc951e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcc7dd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcc07730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcc33510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcc338c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52a7f07598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52dcbcd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52a7eb8840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52a7ee1620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52a7ee5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52a7e96a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52a7e3e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52a7e681e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52a7dfaea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52a7e13598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f52a7e68378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 4.9353612134243129e-05
Iter: 2 loss: 4.66710083768193e-05
Iter: 3 loss: 4.422168915312665e-05
Iter: 4 loss: 4.1139418562298123e-05
Iter: 5 loss: 4.4611877746313848e-05
Iter: 6 loss: 3.94709288127025e-05
Iter: 7 loss: 3.7376647058959354e-05
Iter: 8 loss: 3.2628223422621549e-05
Iter: 9 loss: 9.624468531073696e-05
Iter: 10 loss: 3.233586572781258e-05
Iter: 11 loss: 2.7142180280893816e-05
Iter: 12 loss: 7.825156250242274e-05
Iter: 13 loss: 2.696444587074709e-05
Iter: 14 loss: 2.4658996144578385e-05
Iter: 15 loss: 2.988844796561251e-05
Iter: 16 loss: 2.3794729234786722e-05
Iter: 17 loss: 2.1316961355091681e-05
Iter: 18 loss: 3.2229017542166984e-05
Iter: 19 loss: 2.0821615916617753e-05
Iter: 20 loss: 1.9431847614620152e-05
Iter: 21 loss: 2.3853860767596997e-05
Iter: 22 loss: 1.9033899641746921e-05
Iter: 23 loss: 1.7614494943613273e-05
Iter: 24 loss: 2.0095199960028526e-05
Iter: 25 loss: 1.6987115537689583e-05
Iter: 26 loss: 1.6101553633341469e-05
Iter: 27 loss: 2.0938609901196612e-05
Iter: 28 loss: 1.5971833325284886e-05
Iter: 29 loss: 1.5092114603241155e-05
Iter: 30 loss: 1.580331003540498e-05
Iter: 31 loss: 1.4564446630012486e-05
Iter: 32 loss: 1.3893559577090577e-05
Iter: 33 loss: 1.6418592651225442e-05
Iter: 34 loss: 1.3731415089741331e-05
Iter: 35 loss: 1.3053138409451648e-05
Iter: 36 loss: 1.6399030611583909e-05
Iter: 37 loss: 1.2936097876682741e-05
Iter: 38 loss: 1.2774397067546896e-05
Iter: 39 loss: 1.2720992227578817e-05
Iter: 40 loss: 1.2461665619562267e-05
Iter: 41 loss: 1.2181993056582655e-05
Iter: 42 loss: 1.2138120324107257e-05
Iter: 43 loss: 1.1906527220190878e-05
Iter: 44 loss: 1.1987000321281866e-05
Iter: 45 loss: 1.1743264361816871e-05
Iter: 46 loss: 1.1392135788003307e-05
Iter: 47 loss: 1.1475690395023719e-05
Iter: 48 loss: 1.1134890428198164e-05
Iter: 49 loss: 1.0799098410823533e-05
Iter: 50 loss: 1.5756816177537254e-05
Iter: 51 loss: 1.0798757821853649e-05
Iter: 52 loss: 1.0579612730193045e-05
Iter: 53 loss: 1.0430761689551676e-05
Iter: 54 loss: 1.0350045360628157e-05
Iter: 55 loss: 1.0035673034178715e-05
Iter: 56 loss: 1.3115769195854406e-05
Iter: 57 loss: 1.0024788563212362e-05
Iter: 58 loss: 9.8653934612082061e-06
Iter: 59 loss: 1.0067036747916894e-05
Iter: 60 loss: 9.7832498824709242e-06
Iter: 61 loss: 9.5636249362289742e-06
Iter: 62 loss: 1.0066422136559014e-05
Iter: 63 loss: 9.48148681242682e-06
Iter: 64 loss: 9.3070579336119745e-06
Iter: 65 loss: 9.1145527290059732e-06
Iter: 66 loss: 9.086867512082546e-06
Iter: 67 loss: 8.9178939994206719e-06
Iter: 68 loss: 8.9024015236741631e-06
Iter: 69 loss: 8.7777722540168758e-06
Iter: 70 loss: 8.6167936969840382e-06
Iter: 71 loss: 8.6057664125616358e-06
Iter: 72 loss: 8.7740627635869533e-06
Iter: 73 loss: 8.536589156110214e-06
Iter: 74 loss: 8.4772625488122862e-06
Iter: 75 loss: 8.4271745393300469e-06
Iter: 76 loss: 8.4105790699206246e-06
Iter: 77 loss: 8.3373697550921128e-06
Iter: 78 loss: 8.1586344582970083e-06
Iter: 79 loss: 1.0002687839192861e-05
Iter: 80 loss: 8.1376830547998318e-06
Iter: 81 loss: 8.0503712604836266e-06
Iter: 82 loss: 8.03624846819171e-06
Iter: 83 loss: 7.9548375194639054e-06
Iter: 84 loss: 7.8466759034656861e-06
Iter: 85 loss: 7.8404391943344052e-06
Iter: 86 loss: 7.7469932889167039e-06
Iter: 87 loss: 7.7424853441615073e-06
Iter: 88 loss: 7.6863297399640056e-06
Iter: 89 loss: 7.5973064805379861e-06
Iter: 90 loss: 7.5964355422843492e-06
Iter: 91 loss: 7.4945666482149087e-06
Iter: 92 loss: 8.1153798846972323e-06
Iter: 93 loss: 7.4821746574289925e-06
Iter: 94 loss: 7.379872427080958e-06
Iter: 95 loss: 7.4757729287301362e-06
Iter: 96 loss: 7.3211112746719141e-06
Iter: 97 loss: 7.2369757765427886e-06
Iter: 98 loss: 8.0568476632045774e-06
Iter: 99 loss: 7.233973501749786e-06
Iter: 100 loss: 7.1609015815084909e-06
Iter: 101 loss: 7.2365257049130881e-06
Iter: 102 loss: 7.120269498587743e-06
Iter: 103 loss: 7.0686771298613649e-06
Iter: 104 loss: 7.8688818081002534e-06
Iter: 105 loss: 7.0686693569866612e-06
Iter: 106 loss: 7.0297958087015551e-06
Iter: 107 loss: 7.4456032437991607e-06
Iter: 108 loss: 7.0288695224793393e-06
Iter: 109 loss: 6.9994411876900971e-06
Iter: 110 loss: 6.9251611554302563e-06
Iter: 111 loss: 7.5969590286814705e-06
Iter: 112 loss: 6.9139231850338387e-06
Iter: 113 loss: 6.8452710522171809e-06
Iter: 114 loss: 7.1269930556462417e-06
Iter: 115 loss: 6.830281495283e-06
Iter: 116 loss: 6.7628641921763605e-06
Iter: 117 loss: 6.8565083633660108e-06
Iter: 118 loss: 6.7294838230860907e-06
Iter: 119 loss: 6.6756056979268589e-06
Iter: 120 loss: 7.0911344942415837e-06
Iter: 121 loss: 6.67165093752585e-06
Iter: 122 loss: 6.6180214041128131e-06
Iter: 123 loss: 6.6334497103846766e-06
Iter: 124 loss: 6.5794195990328163e-06
Iter: 125 loss: 6.522702657983617e-06
Iter: 126 loss: 7.1837242756262619e-06
Iter: 127 loss: 6.5218455913087069e-06
Iter: 128 loss: 6.490508733326799e-06
Iter: 129 loss: 6.4355481113834918e-06
Iter: 130 loss: 6.4355399665639494e-06
Iter: 131 loss: 6.3806880309687683e-06
Iter: 132 loss: 7.1088448099536674e-06
Iter: 133 loss: 6.3803957028218327e-06
Iter: 134 loss: 6.3382694665944866e-06
Iter: 135 loss: 6.2941556125890463e-06
Iter: 136 loss: 6.2864429765626854e-06
Iter: 137 loss: 6.2465920039854834e-06
Iter: 138 loss: 6.2462919429423153e-06
Iter: 139 loss: 6.2117628172532549e-06
Iter: 140 loss: 6.2573716401727219e-06
Iter: 141 loss: 6.1942758050378958e-06
Iter: 142 loss: 6.1608982642470535e-06
Iter: 143 loss: 6.1594875922058467e-06
Iter: 144 loss: 6.1506152930654726e-06
Iter: 145 loss: 6.1233329250262125e-06
Iter: 146 loss: 6.1838620108009216e-06
Iter: 147 loss: 6.1069887937667931e-06
Iter: 148 loss: 6.062755342628937e-06
Iter: 149 loss: 6.3688421127555773e-06
Iter: 150 loss: 6.0585808383650077e-06
Iter: 151 loss: 6.0269769805411092e-06
Iter: 152 loss: 6.0506686497842073e-06
Iter: 153 loss: 6.0076326267889937e-06
Iter: 154 loss: 5.9670622634755379e-06
Iter: 155 loss: 6.1949055358950394e-06
Iter: 156 loss: 5.9613754124044925e-06
Iter: 157 loss: 5.9343460878877846e-06
Iter: 158 loss: 5.9516677226827972e-06
Iter: 159 loss: 5.9171713389677884e-06
Iter: 160 loss: 5.8802538122546876e-06
Iter: 161 loss: 6.0533639842675109e-06
Iter: 162 loss: 5.873464154936909e-06
Iter: 163 loss: 5.8457129022060571e-06
Iter: 164 loss: 5.9091985588083584e-06
Iter: 165 loss: 5.83533763375249e-06
Iter: 166 loss: 5.8017028342557419e-06
Iter: 167 loss: 5.9284006058110224e-06
Iter: 168 loss: 5.793599865088712e-06
Iter: 169 loss: 5.76951446803083e-06
Iter: 170 loss: 5.7995186522199605e-06
Iter: 171 loss: 5.7570189872475992e-06
Iter: 172 loss: 5.7251143460012483e-06
Iter: 173 loss: 5.7875767135299665e-06
Iter: 174 loss: 5.7119207843270355e-06
Iter: 175 loss: 5.6870345263201116e-06
Iter: 176 loss: 5.755299887171951e-06
Iter: 177 loss: 5.6789217315776893e-06
Iter: 178 loss: 5.6450331187929315e-06
Iter: 179 loss: 5.68766502235736e-06
Iter: 180 loss: 5.6275264016930765e-06
Iter: 181 loss: 5.6118584777241916e-06
Iter: 182 loss: 5.611847207308979e-06
Iter: 183 loss: 5.5903643526311176e-06
Iter: 184 loss: 5.5990237028098566e-06
Iter: 185 loss: 5.57552156016088e-06
Iter: 186 loss: 5.5602103477133992e-06
Iter: 187 loss: 5.5747392251932807e-06
Iter: 188 loss: 5.5514517602023888e-06
Iter: 189 loss: 5.5373787405228371e-06
Iter: 190 loss: 5.5160074055613286e-06
Iter: 191 loss: 5.5156218007675292e-06
Iter: 192 loss: 5.4907280666838628e-06
Iter: 193 loss: 5.8751109604641942e-06
Iter: 194 loss: 5.4907230288553346e-06
Iter: 195 loss: 5.47645819121785e-06
Iter: 196 loss: 5.4687818038536892e-06
Iter: 197 loss: 5.46237057834517e-06
Iter: 198 loss: 5.4348092726474125e-06
Iter: 199 loss: 5.5141539368706121e-06
Iter: 200 loss: 5.4261865955761488e-06
Iter: 201 loss: 5.4060432884656715e-06
Iter: 202 loss: 5.50089223732214e-06
Iter: 203 loss: 5.4023579390296582e-06
Iter: 204 loss: 5.3796846101980526e-06
Iter: 205 loss: 5.4078928087672167e-06
Iter: 206 loss: 5.3679164215374492e-06
Iter: 207 loss: 5.3493957720517117e-06
Iter: 208 loss: 5.443497269950735e-06
Iter: 209 loss: 5.3463561084730228e-06
Iter: 210 loss: 5.3279958969759039e-06
Iter: 211 loss: 5.333820541123308e-06
Iter: 212 loss: 5.3149195008451929e-06
Iter: 213 loss: 5.2947910713202691e-06
Iter: 214 loss: 5.3470997827500954e-06
Iter: 215 loss: 5.2879444998124837e-06
Iter: 216 loss: 5.2694654822496774e-06
Iter: 217 loss: 5.4219068517930923e-06
Iter: 218 loss: 5.2683335940583323e-06
Iter: 219 loss: 5.2647593103524275e-06
Iter: 220 loss: 5.26096377480448e-06
Iter: 221 loss: 5.2561769331403058e-06
Iter: 222 loss: 5.2409584711752834e-06
Iter: 223 loss: 5.2591024081502463e-06
Iter: 224 loss: 5.2293551753736618e-06
Iter: 225 loss: 5.2061950298281745e-06
Iter: 226 loss: 5.3938850588566027e-06
Iter: 227 loss: 5.2047010073743694e-06
Iter: 228 loss: 5.1904289880863694e-06
Iter: 229 loss: 5.1735986166526829e-06
Iter: 230 loss: 5.1717628130888892e-06
Iter: 231 loss: 5.15632832624684e-06
Iter: 232 loss: 5.1547449110250941e-06
Iter: 233 loss: 5.1459064452994013e-06
Iter: 234 loss: 5.1370030455610637e-06
Iter: 235 loss: 5.1352278570240161e-06
Iter: 236 loss: 5.1176064795437657e-06
Iter: 237 loss: 5.2113012010750827e-06
Iter: 238 loss: 5.1149094981382505e-06
Iter: 239 loss: 5.1044615349242e-06
Iter: 240 loss: 5.1384845515543259e-06
Iter: 241 loss: 5.1015387490512252e-06
Iter: 242 loss: 5.08666525968956e-06
Iter: 243 loss: 5.0746796000694071e-06
Iter: 244 loss: 5.0702247203820119e-06
Iter: 245 loss: 5.0570433062736715e-06
Iter: 246 loss: 5.2410040584772662e-06
Iter: 247 loss: 5.0570049635219166e-06
Iter: 248 loss: 5.0459482484038407e-06
Iter: 249 loss: 5.0367708230814172e-06
Iter: 250 loss: 5.0335987647725327e-06
Iter: 251 loss: 5.0253740642585485e-06
Iter: 252 loss: 5.0250811890131517e-06
Iter: 253 loss: 5.0163054256117977e-06
Iter: 254 loss: 5.0697345252039424e-06
Iter: 255 loss: 5.0152366407968681e-06
Iter: 256 loss: 5.0088283014680639e-06
Iter: 257 loss: 4.9943590119011588e-06
Iter: 258 loss: 5.1898342967207062e-06
Iter: 259 loss: 4.9935001225272957e-06
Iter: 260 loss: 4.9824589872331743e-06
Iter: 261 loss: 5.00693796299255e-06
Iter: 262 loss: 4.9782400262491134e-06
Iter: 263 loss: 4.9636660381410965e-06
Iter: 264 loss: 4.9938736550229144e-06
Iter: 265 loss: 4.95784844811696e-06
Iter: 266 loss: 4.945072817035585e-06
Iter: 267 loss: 4.9838996694771152e-06
Iter: 268 loss: 4.941261107160772e-06
Iter: 269 loss: 4.928107998513218e-06
Iter: 270 loss: 5.0154789863942171e-06
Iter: 271 loss: 4.926754832902509e-06
Iter: 272 loss: 4.9176586440090352e-06
Iter: 273 loss: 4.9213062361496326e-06
Iter: 274 loss: 4.9113729889336869e-06
Iter: 275 loss: 4.8980051675579179e-06
Iter: 276 loss: 4.9457886728750991e-06
Iter: 277 loss: 4.89459853685018e-06
Iter: 278 loss: 4.8849886960341891e-06
Iter: 279 loss: 4.9354773422609528e-06
Iter: 280 loss: 4.8834896587702014e-06
Iter: 281 loss: 4.8735156444146188e-06
Iter: 282 loss: 4.8715559561152989e-06
Iter: 283 loss: 4.8649168118492241e-06
Iter: 284 loss: 4.8552527042346115e-06
Iter: 285 loss: 4.9239170612146309e-06
Iter: 286 loss: 4.8543918269966717e-06
Iter: 287 loss: 4.8448812691977243e-06
Iter: 288 loss: 4.8507626846041022e-06
Iter: 289 loss: 4.838792371824438e-06
Iter: 290 loss: 4.8347203433177088e-06
Iter: 291 loss: 4.8315479016548253e-06
Iter: 292 loss: 4.8284886174059e-06
Iter: 293 loss: 4.8193302548234173e-06
Iter: 294 loss: 4.847416684877208e-06
Iter: 295 loss: 4.8147796367328634e-06
Iter: 296 loss: 4.8012249942062022e-06
Iter: 297 loss: 4.8641042222969164e-06
Iter: 298 loss: 4.7986932720868857e-06
Iter: 299 loss: 4.7898713522418871e-06
Iter: 300 loss: 4.7906601805339244e-06
Iter: 301 loss: 4.7830452974242874e-06
Iter: 302 loss: 4.7701886525065973e-06
Iter: 303 loss: 4.8741045898976331e-06
Iter: 304 loss: 4.76935373466741e-06
Iter: 305 loss: 4.7619940231950568e-06
Iter: 306 loss: 4.762385800211818e-06
Iter: 307 loss: 4.756222389922236e-06
Iter: 308 loss: 4.74293036331499e-06
Iter: 309 loss: 4.8075913312887031e-06
Iter: 310 loss: 4.7406058409179025e-06
Iter: 311 loss: 4.73374680455521e-06
Iter: 312 loss: 4.7555453732728206e-06
Iter: 313 loss: 4.7317826372691411e-06
Iter: 314 loss: 4.7230550131676811e-06
Iter: 315 loss: 4.7215057904474008e-06
Iter: 316 loss: 4.71558579059501e-06
Iter: 317 loss: 4.7073854135443732e-06
Iter: 318 loss: 4.8350651096308e-06
Iter: 319 loss: 4.7073844489383392e-06
Iter: 320 loss: 4.7015774296413532e-06
Iter: 321 loss: 4.6920233383556656e-06
Iter: 322 loss: 4.691979407257618e-06
Iter: 323 loss: 4.6898210173690623e-06
Iter: 324 loss: 4.6865559998451241e-06
Iter: 325 loss: 4.6818818032365409e-06
Iter: 326 loss: 4.7029108026611019e-06
Iter: 327 loss: 4.6809725136783111e-06
Iter: 328 loss: 4.6778552325741087e-06
Iter: 329 loss: 4.6680192993496906e-06
Iter: 330 loss: 4.6820364441994448e-06
Iter: 331 loss: 4.6609291425776872e-06
Iter: 332 loss: 4.6518696865231892e-06
Iter: 333 loss: 4.6518339869953962e-06
Iter: 334 loss: 4.643060902293853e-06
Iter: 335 loss: 4.6418277288706423e-06
Iter: 336 loss: 4.6356585318208249e-06
Iter: 337 loss: 4.6271487000937049e-06
Iter: 338 loss: 4.6950978622335289e-06
Iter: 339 loss: 4.626577931558218e-06
Iter: 340 loss: 4.6187471687192719e-06
Iter: 341 loss: 4.6242309164037836e-06
Iter: 342 loss: 4.6138728104947353e-06
Iter: 343 loss: 4.6059724613260335e-06
Iter: 344 loss: 4.6876526118654637e-06
Iter: 345 loss: 4.6057515751731075e-06
Iter: 346 loss: 4.5988324631925336e-06
Iter: 347 loss: 4.5963861201519046e-06
Iter: 348 loss: 4.5924887071921853e-06
Iter: 349 loss: 4.5854390784630084e-06
Iter: 350 loss: 4.6654174366244966e-06
Iter: 351 loss: 4.5853148894861606e-06
Iter: 352 loss: 4.5796812931793408e-06
Iter: 353 loss: 4.5778598116748488e-06
Iter: 354 loss: 4.5745779803643465e-06
Iter: 355 loss: 4.5664452187904413e-06
Iter: 356 loss: 4.6149202485051029e-06
Iter: 357 loss: 4.5654157166191889e-06
Iter: 358 loss: 4.5609664050970268e-06
Iter: 359 loss: 4.5609071630250727e-06
Iter: 360 loss: 4.55557829180892e-06
Iter: 361 loss: 4.5461121212185589e-06
Iter: 362 loss: 4.5461121150729892e-06
Iter: 363 loss: 4.5402133100402912e-06
Iter: 364 loss: 4.5531293092165575e-06
Iter: 365 loss: 4.5379409407449037e-06
Iter: 366 loss: 4.5313877079136495e-06
Iter: 367 loss: 4.5265716059383721e-06
Iter: 368 loss: 4.5243617039173027e-06
Iter: 369 loss: 4.5165682358226877e-06
Iter: 370 loss: 4.5734272596713168e-06
Iter: 371 loss: 4.5159149951914647e-06
Iter: 372 loss: 4.5070751052265248e-06
Iter: 373 loss: 4.5126153130420973e-06
Iter: 374 loss: 4.50143027211606e-06
Iter: 375 loss: 4.4952637438589467e-06
Iter: 376 loss: 4.5279285480439823e-06
Iter: 377 loss: 4.4943144679273431e-06
Iter: 378 loss: 4.4864642153244353e-06
Iter: 379 loss: 4.4914503259386991e-06
Iter: 380 loss: 4.4814660273695536e-06
Iter: 381 loss: 4.475775599063273e-06
Iter: 382 loss: 4.475768221076716e-06
Iter: 383 loss: 4.4714535157570174e-06
Iter: 384 loss: 4.4623803341012753e-06
Iter: 385 loss: 4.6134213702720805e-06
Iter: 386 loss: 4.4621369118723285e-06
Iter: 387 loss: 4.4548166884643762e-06
Iter: 388 loss: 4.454585962852372e-06
Iter: 389 loss: 4.4504126097247325e-06
Iter: 390 loss: 4.4555204782652161e-06
Iter: 391 loss: 4.4482319071702882e-06
Iter: 392 loss: 4.4429628136281488e-06
Iter: 393 loss: 4.5169279442821743e-06
Iter: 394 loss: 4.442948801679792e-06
Iter: 395 loss: 4.4402892366722087e-06
Iter: 396 loss: 4.4366572999797423e-06
Iter: 397 loss: 4.4364836759490606e-06
Iter: 398 loss: 4.4324124647496593e-06
Iter: 399 loss: 4.4247209502087387e-06
Iter: 400 loss: 4.592547154760912e-06
Iter: 401 loss: 4.42469670476656e-06
Iter: 402 loss: 4.4166254103405706e-06
Iter: 403 loss: 4.522221224757247e-06
Iter: 404 loss: 4.4165752746545211e-06
Iter: 405 loss: 4.4111081870008548e-06
Iter: 406 loss: 4.4053122401158643e-06
Iter: 407 loss: 4.4043426978595806e-06
Iter: 408 loss: 4.3974911506014977e-06
Iter: 409 loss: 4.3974645222267131e-06
Iter: 410 loss: 4.3924463219739586e-06
Iter: 411 loss: 4.3865476278509932e-06
Iter: 412 loss: 4.3858950902396108e-06
Iter: 413 loss: 4.3800998803057367e-06
Iter: 414 loss: 4.37999839702758e-06
Iter: 415 loss: 4.3758131341470349e-06
Iter: 416 loss: 4.3751302518989116e-06
Iter: 417 loss: 4.3722510164484106e-06
Iter: 418 loss: 4.3651179295290484e-06
Iter: 419 loss: 4.3927032875095471e-06
Iter: 420 loss: 4.3634501666481417e-06
Iter: 421 loss: 4.3581097042220585e-06
Iter: 422 loss: 4.3643228138039159e-06
Iter: 423 loss: 4.3552639645073378e-06
Iter: 424 loss: 4.3521069634306326e-06
Iter: 425 loss: 4.3515097844558982e-06
Iter: 426 loss: 4.3481171889479171e-06
Iter: 427 loss: 4.3521657877781693e-06
Iter: 428 loss: 4.346327196910049e-06
Iter: 429 loss: 4.34347198899452e-06
Iter: 430 loss: 4.336922723200997e-06
Iter: 431 loss: 4.421099538837479e-06
Iter: 432 loss: 4.3364715823247181e-06
Iter: 433 loss: 4.3291294688071711e-06
Iter: 434 loss: 4.3834816068793659e-06
Iter: 435 loss: 4.3285345267804081e-06
Iter: 436 loss: 4.3235433792814073e-06
Iter: 437 loss: 4.3195298097279265e-06
Iter: 438 loss: 4.3180309338826306e-06
Iter: 439 loss: 4.3119566703223283e-06
Iter: 440 loss: 4.3972221817585874e-06
Iter: 441 loss: 4.3119404108358928e-06
Iter: 442 loss: 4.30728660346678e-06
Iter: 443 loss: 4.3003951810310757e-06
Iter: 444 loss: 4.3002302423446036e-06
Iter: 445 loss: 4.2950665794107574e-06
Iter: 446 loss: 4.2947896579673582e-06
Iter: 447 loss: 4.2901563113410108e-06
Iter: 448 loss: 4.2829291313400473e-06
Iter: 449 loss: 4.28283775365649e-06
Iter: 450 loss: 4.2784621727319222e-06
Iter: 451 loss: 4.2777142239838815e-06
Iter: 452 loss: 4.2740530371072133e-06
Iter: 453 loss: 4.2693187490156837e-06
Iter: 454 loss: 4.2689970391595362e-06
Iter: 455 loss: 4.2629640563627289e-06
Iter: 456 loss: 4.3309883279976873e-06
Iter: 457 loss: 4.2628540188812617e-06
Iter: 458 loss: 4.2609797405655759e-06
Iter: 459 loss: 4.2605780410913116e-06
Iter: 460 loss: 4.2588849596450124e-06
Iter: 461 loss: 4.2542484241693695e-06
Iter: 462 loss: 4.2823116721356105e-06
Iter: 463 loss: 4.2529938810666063e-06
Iter: 464 loss: 4.2467309142100231e-06
Iter: 465 loss: 4.2687701596379305e-06
Iter: 466 loss: 4.2451090369351163e-06
Iter: 467 loss: 4.2404448050170058e-06
Iter: 468 loss: 4.2402106126572531e-06
Iter: 469 loss: 4.23664213811102e-06
Iter: 470 loss: 4.2297937432143091e-06
Iter: 471 loss: 4.2786997041126283e-06
Iter: 472 loss: 4.2291909801353042e-06
Iter: 473 loss: 4.2252839368397944e-06
Iter: 474 loss: 4.2191334907693051e-06
Iter: 475 loss: 4.2190658589997416e-06
Iter: 476 loss: 4.2137866667168846e-06
Iter: 477 loss: 4.213638479981669e-06
Iter: 478 loss: 4.20962714162421e-06
Iter: 479 loss: 4.2043720121479043e-06
Iter: 480 loss: 4.2040416852544515e-06
Iter: 481 loss: 4.1979995869709906e-06
Iter: 482 loss: 4.288906114326115e-06
Iter: 483 loss: 4.1979959854962118e-06
Iter: 484 loss: 4.1938745393361364e-06
Iter: 485 loss: 4.1989428379837551e-06
Iter: 486 loss: 4.1917254450395225e-06
Iter: 487 loss: 4.1869013002118427e-06
Iter: 488 loss: 4.2178669329567667e-06
Iter: 489 loss: 4.186369859156798e-06
Iter: 490 loss: 4.18351552864879e-06
Iter: 491 loss: 4.2064405273853884e-06
Iter: 492 loss: 4.1833272394758467e-06
Iter: 493 loss: 4.179718517630495e-06
Iter: 494 loss: 4.1836340864460461e-06
Iter: 495 loss: 4.1777447276935522e-06
Iter: 496 loss: 4.1749601036650721e-06
Iter: 497 loss: 4.1734923073980869e-06
Iter: 498 loss: 4.1722225278539826e-06
Iter: 499 loss: 4.1679576301761223e-06
Iter: 500 loss: 4.1663954510200825e-06
Iter: 501 loss: 4.1640276146636663e-06
Iter: 502 loss: 4.1584365915139924e-06
Iter: 503 loss: 4.2127883445057725e-06
Iter: 504 loss: 4.1582358109501477e-06
Iter: 505 loss: 4.1548806414403633e-06
Iter: 506 loss: 4.1518990509506689e-06
Iter: 507 loss: 4.151035949737822e-06
Iter: 508 loss: 4.1443118656822355e-06
Iter: 509 loss: 4.1792372186001405e-06
Iter: 510 loss: 4.1432439857960062e-06
Iter: 511 loss: 4.1389466823754694e-06
Iter: 512 loss: 4.1350748266366422e-06
Iter: 513 loss: 4.133995659071472e-06
Iter: 514 loss: 4.1297621448287351e-06
Iter: 515 loss: 4.1296031117341728e-06
Iter: 516 loss: 4.1258204684791831e-06
Iter: 517 loss: 4.119726742423782e-06
Iter: 518 loss: 4.1196821384068654e-06
Iter: 519 loss: 4.11655166369889e-06
Iter: 520 loss: 4.1158747443372815e-06
Iter: 521 loss: 4.1129461761747114e-06
Iter: 522 loss: 4.1131984566507146e-06
Iter: 523 loss: 4.1106771245865194e-06
Iter: 524 loss: 4.1080084653193949e-06
Iter: 525 loss: 4.1077608998286807e-06
Iter: 526 loss: 4.1056443549877953e-06
Iter: 527 loss: 4.1040591360612661e-06
Iter: 528 loss: 4.1033613859011892e-06
Iter: 529 loss: 4.1011258389554832e-06
Iter: 530 loss: 4.0964914082439343e-06
Iter: 531 loss: 4.1766919878237031e-06
Iter: 532 loss: 4.0963889649004465e-06
Iter: 533 loss: 4.0909706304355781e-06
Iter: 534 loss: 4.149995869981182e-06
Iter: 535 loss: 4.0908519914489987e-06
Iter: 536 loss: 4.0874830115170108e-06
Iter: 537 loss: 4.0846015270490449e-06
Iter: 538 loss: 4.0836785157025415e-06
Iter: 539 loss: 4.0782085799624446e-06
Iter: 540 loss: 4.1317936088811426e-06
Iter: 541 loss: 4.0780178338464794e-06
Iter: 542 loss: 4.074892888120522e-06
Iter: 543 loss: 4.07266377935891e-06
Iter: 544 loss: 4.0715729357733757e-06
Iter: 545 loss: 4.0667394233816781e-06
Iter: 546 loss: 4.1031721591675027e-06
Iter: 547 loss: 4.0663645243405227e-06
Iter: 548 loss: 4.062219113909533e-06
Iter: 549 loss: 4.0605702795246717e-06
Iter: 550 loss: 4.0583508948742811e-06
Iter: 551 loss: 4.0530817236678974e-06
Iter: 552 loss: 4.09770173897345e-06
Iter: 553 loss: 4.0527821911432374e-06
Iter: 554 loss: 4.0480410937509768e-06
Iter: 555 loss: 4.0516265946208472e-06
Iter: 556 loss: 4.0451459208662506e-06
Iter: 557 loss: 4.0443276917713219e-06
Iter: 558 loss: 4.0428360217013236e-06
Iter: 559 loss: 4.0409410886185091e-06
Iter: 560 loss: 4.0466732385583839e-06
Iter: 561 loss: 4.0403735164391658e-06
Iter: 562 loss: 4.03847643009489e-06
Iter: 563 loss: 4.0338114558044136e-06
Iter: 564 loss: 4.08066939185504e-06
Iter: 565 loss: 4.0332331266168736e-06
Iter: 566 loss: 4.0295254452474344e-06
Iter: 567 loss: 4.0688599549010757e-06
Iter: 568 loss: 4.0294332051004022e-06
Iter: 569 loss: 4.0266983226802991e-06
Iter: 570 loss: 4.02279311552542e-06
Iter: 571 loss: 4.022661302164006e-06
Iter: 572 loss: 4.0175577853231904e-06
Iter: 573 loss: 4.0793190476374992e-06
Iter: 574 loss: 4.0174975613289546e-06
Iter: 575 loss: 4.0143965371467176e-06
Iter: 576 loss: 4.0119974457209841e-06
Iter: 577 loss: 4.01101610685232e-06
Iter: 578 loss: 4.0064656662870242e-06
Iter: 579 loss: 4.0530358311754307e-06
Iter: 580 loss: 4.0063329403358906e-06
Iter: 581 loss: 4.0031285104319835e-06
Iter: 582 loss: 3.999500202699856e-06
Iter: 583 loss: 3.9990287682326574e-06
Iter: 584 loss: 3.9944641598032292e-06
Iter: 585 loss: 4.0562995539090442e-06
Iter: 586 loss: 3.9944445705813373e-06
Iter: 587 loss: 3.9912835106813015e-06
Iter: 588 loss: 3.9879248995105849e-06
Iter: 589 loss: 3.98736740536226e-06
Iter: 590 loss: 3.9843955325976341e-06
Iter: 591 loss: 3.9841358205340471e-06
Iter: 592 loss: 3.9828191875576726e-06
Iter: 593 loss: 3.982760038634007e-06
Iter: 594 loss: 3.9814074604543723e-06
Iter: 595 loss: 3.97810291961117e-06
Iter: 596 loss: 4.01209938423471e-06
Iter: 597 loss: 3.9777133640322074e-06
Iter: 598 loss: 3.9738662191695859e-06
Iter: 599 loss: 3.99222953132226e-06
Iter: 600 loss: 3.9731753848704941e-06
Iter: 601 loss: 3.9706670131686905e-06
Iter: 602 loss: 3.96709638932644e-06
Iter: 603 loss: 3.9669726373996895e-06
Iter: 604 loss: 3.9618323332262774e-06
Iter: 605 loss: 3.9974335155485095e-06
Iter: 606 loss: 3.9613480191343389e-06
Iter: 607 loss: 3.9575077909483088e-06
Iter: 608 loss: 3.9639803964407723e-06
Iter: 609 loss: 3.9557778062280649e-06
Iter: 610 loss: 3.9522578442516664e-06
Iter: 611 loss: 3.9829641217610629e-06
Iter: 612 loss: 3.952074744011089e-06
Iter: 613 loss: 3.9497341925420376e-06
Iter: 614 loss: 3.9464435660547267e-06
Iter: 615 loss: 3.9463173056032825e-06
Iter: 616 loss: 3.9413987018449936e-06
Iter: 617 loss: 3.9811447012243555e-06
Iter: 618 loss: 3.9410795216533173e-06
Iter: 619 loss: 3.937687713398835e-06
Iter: 620 loss: 3.9355266336770286e-06
Iter: 621 loss: 3.9341980216263325e-06
Iter: 622 loss: 3.9304873729959458e-06
Iter: 623 loss: 3.9861110238570386e-06
Iter: 624 loss: 3.9304848914589244e-06
Iter: 625 loss: 3.9276121950295282e-06
Iter: 626 loss: 3.9334421952869343e-06
Iter: 627 loss: 3.9264501442031769e-06
Iter: 628 loss: 3.9226070931226628e-06
Iter: 629 loss: 3.9614817793306852e-06
Iter: 630 loss: 3.9224890278807993e-06
Iter: 631 loss: 3.9212106443677511e-06
Iter: 632 loss: 3.9201264281728063e-06
Iter: 633 loss: 3.9197715311236248e-06
Iter: 634 loss: 3.9175520501208673e-06
Iter: 635 loss: 3.9153750572324558e-06
Iter: 636 loss: 3.9149023574056572e-06
Iter: 637 loss: 3.9107400654093766e-06
Iter: 638 loss: 3.9340282275225015e-06
Iter: 639 loss: 3.9101530348006248e-06
Iter: 640 loss: 3.907854708157836e-06
Iter: 641 loss: 3.9058180232186444e-06
Iter: 642 loss: 3.9052239239765451e-06
Iter: 643 loss: 3.9016542573247184e-06
Iter: 644 loss: 3.943604170949221e-06
Iter: 645 loss: 3.9016031357607737e-06
Iter: 646 loss: 3.8991193181068342e-06
Iter: 647 loss: 3.8998311921452678e-06
Iter: 648 loss: 3.8973309520740294e-06
Iter: 649 loss: 3.8934522724253128e-06
Iter: 650 loss: 3.9044598081912138e-06
Iter: 651 loss: 3.8922241705069467e-06
Iter: 652 loss: 3.8892648957084105e-06
Iter: 653 loss: 3.8936719122578286e-06
Iter: 654 loss: 3.8878458702601543e-06
Iter: 655 loss: 3.8838413411425886e-06
Iter: 656 loss: 3.8982380674245e-06
Iter: 657 loss: 3.882827323900964e-06
Iter: 658 loss: 3.8798381089935267e-06
Iter: 659 loss: 3.8799613617541262e-06
Iter: 660 loss: 3.8774831723927193e-06
Iter: 661 loss: 3.8789116315698285e-06
Iter: 662 loss: 3.8758372917914524e-06
Iter: 663 loss: 3.8745071511106887e-06
Iter: 664 loss: 3.8727779101647785e-06
Iter: 665 loss: 3.8726640420664557e-06
Iter: 666 loss: 3.8704543731651342e-06
Iter: 667 loss: 3.8672413703188045e-06
Iter: 668 loss: 3.8671492829379717e-06
Iter: 669 loss: 3.8641299708900191e-06
Iter: 670 loss: 3.9122912241553722e-06
Iter: 671 loss: 3.8641299665816131e-06
Iter: 672 loss: 3.8618563129033193e-06
Iter: 673 loss: 3.85875236323511e-06
Iter: 674 loss: 3.8586036155116908e-06
Iter: 675 loss: 3.8550927833986835e-06
Iter: 676 loss: 3.8880121988391939e-06
Iter: 677 loss: 3.8549488975150942e-06
Iter: 678 loss: 3.8522295751765576e-06
Iter: 679 loss: 3.8500369536609458e-06
Iter: 680 loss: 3.8492232983071375e-06
Iter: 681 loss: 3.8461514424999311e-06
Iter: 682 loss: 3.84611618614439e-06
Iter: 683 loss: 3.84404759659453e-06
Iter: 684 loss: 3.8406405349737914e-06
Iter: 685 loss: 3.8406253249811107e-06
Iter: 686 loss: 3.83622428803574e-06
Iter: 687 loss: 3.876805962418387e-06
Iter: 688 loss: 3.836033002499759e-06
Iter: 689 loss: 3.8330280562067647e-06
Iter: 690 loss: 3.8303467279671526e-06
Iter: 691 loss: 3.8295792498750953e-06
Iter: 692 loss: 3.827001455995959e-06
Iter: 693 loss: 3.826770015858681e-06
Iter: 694 loss: 3.8254360981471264e-06
Iter: 695 loss: 3.8253420702421896e-06
Iter: 696 loss: 3.8243552994848619e-06
Iter: 697 loss: 3.8212453230391576e-06
Iter: 698 loss: 3.8257862155435778e-06
Iter: 699 loss: 3.8190222740487471e-06
Iter: 700 loss: 3.8161726735543664e-06
Iter: 701 loss: 3.8161447840155113e-06
Iter: 702 loss: 3.8140613821110023e-06
Iter: 703 loss: 3.8140108017294185e-06
Iter: 704 loss: 3.812379379657417e-06
Iter: 705 loss: 3.8090708764877236e-06
Iter: 706 loss: 3.8235323192584144e-06
Iter: 707 loss: 3.8084027153304712e-06
Iter: 708 loss: 3.8061412108817283e-06
Iter: 709 loss: 3.8052024325880653e-06
Iter: 710 loss: 3.8040164404622943e-06
Iter: 711 loss: 3.8003397805923523e-06
Iter: 712 loss: 3.8206207742050468e-06
Iter: 713 loss: 3.7998088603695067e-06
Iter: 714 loss: 3.7968682426691533e-06
Iter: 715 loss: 3.7990469313049603e-06
Iter: 716 loss: 3.7950629392759089e-06
Iter: 717 loss: 3.7917903468795016e-06
Iter: 718 loss: 3.8261535472714521e-06
Iter: 719 loss: 3.7917050049817345e-06
Iter: 720 loss: 3.7897491094926836e-06
Iter: 721 loss: 3.7879988491600371e-06
Iter: 722 loss: 3.7875016725352425e-06
Iter: 723 loss: 3.78358271085395e-06
Iter: 724 loss: 3.801603064421738e-06
Iter: 725 loss: 3.7828424312958333e-06
Iter: 726 loss: 3.7805685423439025e-06
Iter: 727 loss: 3.8026152120035571e-06
Iter: 728 loss: 3.7804859325814329e-06
Iter: 729 loss: 3.7777223340762979e-06
Iter: 730 loss: 3.7858721650438477e-06
Iter: 731 loss: 3.7768759952107719e-06
Iter: 732 loss: 3.7756112078133452e-06
Iter: 733 loss: 3.773639029680664e-06
Iter: 734 loss: 3.773613984514014e-06
Iter: 735 loss: 3.7706645964598742e-06
Iter: 736 loss: 3.7699489381375818e-06
Iter: 737 loss: 3.7680759190076826e-06
Iter: 738 loss: 3.7655225632001263e-06
Iter: 739 loss: 3.7654590997479781e-06
Iter: 740 loss: 3.7635949695885173e-06
Iter: 741 loss: 3.7616392631424539e-06
Iter: 742 loss: 3.7612996772661673e-06
Iter: 743 loss: 3.7582313431891651e-06
Iter: 744 loss: 3.7848106574369523e-06
Iter: 745 loss: 3.7580683048476262e-06
Iter: 746 loss: 3.7564496663176641e-06
Iter: 747 loss: 3.7536917343388479e-06
Iter: 748 loss: 3.7536878374803655e-06
Iter: 749 loss: 3.7504059740710419e-06
Iter: 750 loss: 3.7946988194151333e-06
Iter: 751 loss: 3.7503912855322947e-06
Iter: 752 loss: 3.7484296458087956e-06
Iter: 753 loss: 3.747889146257927e-06
Iter: 754 loss: 3.7466856534876212e-06
Iter: 755 loss: 3.7430237797033126e-06
Iter: 756 loss: 3.7581529654528973e-06
Iter: 757 loss: 3.74223087599753e-06
Iter: 758 loss: 3.7400516516978235e-06
Iter: 759 loss: 3.742202228751217e-06
Iter: 760 loss: 3.7388207221083064e-06
Iter: 761 loss: 3.7374579084772391e-06
Iter: 762 loss: 3.7370643190718636e-06
Iter: 763 loss: 3.7355311872174747e-06
Iter: 764 loss: 3.7337116414312555e-06
Iter: 765 loss: 3.7335189563434287e-06
Iter: 766 loss: 3.7317881587287148e-06
Iter: 767 loss: 3.7284563965373253e-06
Iter: 768 loss: 3.7980160416772434e-06
Iter: 769 loss: 3.7284381097360753e-06
Iter: 770 loss: 3.7253202353793013e-06
Iter: 771 loss: 3.7680964184464896e-06
Iter: 772 loss: 3.7253087893199863e-06
Iter: 773 loss: 3.7231419643404388e-06
Iter: 774 loss: 3.7276586915711561e-06
Iter: 775 loss: 3.7222799785458584e-06
Iter: 776 loss: 3.7196257597805903e-06
Iter: 777 loss: 3.72752958003275e-06
Iter: 778 loss: 3.7188195280093292e-06
Iter: 779 loss: 3.7165384192540943e-06
Iter: 780 loss: 3.7174572738974906e-06
Iter: 781 loss: 3.7149628709798225e-06
Iter: 782 loss: 3.7119968652162638e-06
Iter: 783 loss: 3.7289177421530472e-06
Iter: 784 loss: 3.7115919329582095e-06
Iter: 785 loss: 3.7093965079418085e-06
Iter: 786 loss: 3.7071116900788671e-06
Iter: 787 loss: 3.7067035907061308e-06
Iter: 788 loss: 3.7034548131586646e-06
Iter: 789 loss: 3.7550834913170074e-06
Iter: 790 loss: 3.7034547888851237e-06
Iter: 791 loss: 3.7016446255928772e-06
Iter: 792 loss: 3.70192600991185e-06
Iter: 793 loss: 3.7002775953595567e-06
Iter: 794 loss: 3.6975313384556577e-06
Iter: 795 loss: 3.7137942002562377e-06
Iter: 796 loss: 3.697180084115291e-06
Iter: 797 loss: 3.6965172563348372e-06
Iter: 798 loss: 3.6960150779785287e-06
Iter: 799 loss: 3.6953864266360725e-06
Iter: 800 loss: 3.69342005553751e-06
Iter: 801 loss: 3.696744893587116e-06
Iter: 802 loss: 3.6920895071163729e-06
Iter: 803 loss: 3.688642112956099e-06
Iter: 804 loss: 3.7035442735502438e-06
Iter: 805 loss: 3.6879361542345158e-06
Iter: 806 loss: 3.6853488367526847e-06
Iter: 807 loss: 3.6827183371141354e-06
Iter: 808 loss: 3.6822096316224862e-06
Iter: 809 loss: 3.6801933695823418e-06
Iter: 810 loss: 3.6796953991308192e-06
Iter: 811 loss: 3.6779236203292405e-06
Iter: 812 loss: 3.6760606963897681e-06
Iter: 813 loss: 3.6757397455422742e-06
Iter: 814 loss: 3.6721587211084933e-06
Iter: 815 loss: 3.6889958167466145e-06
Iter: 816 loss: 3.6715019318258061e-06
Iter: 817 loss: 3.6696076723192296e-06
Iter: 818 loss: 3.6704830380427124e-06
Iter: 819 loss: 3.6683265528192065e-06
Iter: 820 loss: 3.6652893073965115e-06
Iter: 821 loss: 3.6745925464795128e-06
Iter: 822 loss: 3.6643895703543987e-06
Iter: 823 loss: 3.6622077402082422e-06
Iter: 824 loss: 3.6684978436198376e-06
Iter: 825 loss: 3.6615259255048913e-06
Iter: 826 loss: 3.6588857359746874e-06
Iter: 827 loss: 3.6671608352263621e-06
Iter: 828 loss: 3.658119998141062e-06
Iter: 829 loss: 3.6572207023464088e-06
Iter: 830 loss: 3.6570075285655038e-06
Iter: 831 loss: 3.6557203153323352e-06
Iter: 832 loss: 3.6541423345383461e-06
Iter: 833 loss: 3.6539992067896337e-06
Iter: 834 loss: 3.6520670235354329e-06
Iter: 835 loss: 3.652464828536393e-06
Iter: 836 loss: 3.6506345596823045e-06
Iter: 837 loss: 3.6484212915009106e-06
Iter: 838 loss: 3.647550116236353e-06
Iter: 839 loss: 3.6463596795506162e-06
Iter: 840 loss: 3.6428124117330637e-06
Iter: 841 loss: 3.6636255037903375e-06
Iter: 842 loss: 3.6423511878585595e-06
Iter: 843 loss: 3.6398714441692302e-06
Iter: 844 loss: 3.6425661447688997e-06
Iter: 845 loss: 3.6385161535571059e-06
Iter: 846 loss: 3.6353236940087358e-06
Iter: 847 loss: 3.6622486357708332e-06
Iter: 848 loss: 3.6351399521291418e-06
Iter: 849 loss: 3.633193992313276e-06
Iter: 850 loss: 3.6326516051065327e-06
Iter: 851 loss: 3.6314617593991496e-06
Iter: 852 loss: 3.6283630252543037e-06
Iter: 853 loss: 3.6411998697272648e-06
Iter: 854 loss: 3.6276944076711171e-06
Iter: 855 loss: 3.6251109137076862e-06
Iter: 856 loss: 3.6228802390205737e-06
Iter: 857 loss: 3.622182963026723e-06
Iter: 858 loss: 3.618560172370714e-06
Iter: 859 loss: 3.6725405974292138e-06
Iter: 860 loss: 3.6185572738802306e-06
Iter: 861 loss: 3.6167632714546209e-06
Iter: 862 loss: 3.6200371402512983e-06
Iter: 863 loss: 3.6159896005685956e-06
Iter: 864 loss: 3.6141288896999591e-06
Iter: 865 loss: 3.61411706498057e-06
Iter: 866 loss: 3.6127188075631302e-06
Iter: 867 loss: 3.6111226919579086e-06
Iter: 868 loss: 3.61092215104094e-06
Iter: 869 loss: 3.6095777535477608e-06
Iter: 870 loss: 3.6077536691949319e-06
Iter: 871 loss: 3.6076624050586097e-06
Iter: 872 loss: 3.6043916178084533e-06
Iter: 873 loss: 3.6179323919302793e-06
Iter: 874 loss: 3.6036850458179456e-06
Iter: 875 loss: 3.60176605214612e-06
Iter: 876 loss: 3.601046175748846e-06
Iter: 877 loss: 3.5999916375463518e-06
Iter: 878 loss: 3.5966396862945461e-06
Iter: 879 loss: 3.6185910023458628e-06
Iter: 880 loss: 3.5962849287561418e-06
Iter: 881 loss: 3.593883789379458e-06
Iter: 882 loss: 3.6030316713589e-06
Iter: 883 loss: 3.5933127091689e-06
Iter: 884 loss: 3.5904745042765067e-06
Iter: 885 loss: 3.5918815290357635e-06
Iter: 886 loss: 3.58857733197313e-06
Iter: 887 loss: 3.5864452146418138e-06
Iter: 888 loss: 3.5959611660379271e-06
Iter: 889 loss: 3.5860261696337217e-06
Iter: 890 loss: 3.5834835056114886e-06
Iter: 891 loss: 3.5828452781010061e-06
Iter: 892 loss: 3.5812445609468968e-06
Iter: 893 loss: 3.5780190530494107e-06
Iter: 894 loss: 3.5896971537169967e-06
Iter: 895 loss: 3.5772083529653034e-06
Iter: 896 loss: 3.5754460370390115e-06
Iter: 897 loss: 3.5752541384490619e-06
Iter: 898 loss: 3.5734848675718053e-06
Iter: 899 loss: 3.5803190157828637e-06
Iter: 900 loss: 3.5730706201088834e-06
Iter: 901 loss: 3.5720704250191347e-06
Iter: 902 loss: 3.569715538094197e-06
Iter: 903 loss: 3.5974431731322416e-06
Iter: 904 loss: 3.569510902684305e-06
Iter: 905 loss: 3.5670240932425419e-06
Iter: 906 loss: 3.5854937136627543e-06
Iter: 907 loss: 3.5668242031072345e-06
Iter: 908 loss: 3.5650195401596106e-06
Iter: 909 loss: 3.5639021663272092e-06
Iter: 910 loss: 3.5631766150784357e-06
Iter: 911 loss: 3.5599578364135009e-06
Iter: 912 loss: 3.5773048767447541e-06
Iter: 913 loss: 3.5594753672154988e-06
Iter: 914 loss: 3.5572658321056881e-06
Iter: 915 loss: 3.5563011591650526e-06
Iter: 916 loss: 3.5551717987346633e-06
Iter: 917 loss: 3.551173684228913e-06
Iter: 918 loss: 3.5814190222056726e-06
Iter: 919 loss: 3.5508664485575031e-06
Iter: 920 loss: 3.5488362496159915e-06
Iter: 921 loss: 3.5593864783985659e-06
Iter: 922 loss: 3.5485141622452604e-06
Iter: 923 loss: 3.5464611464792988e-06
Iter: 924 loss: 3.5436862262128317e-06
Iter: 925 loss: 3.5435436694926169e-06
Iter: 926 loss: 3.5408581303974522e-06
Iter: 927 loss: 3.5695897132857732e-06
Iter: 928 loss: 3.5407939010244922e-06
Iter: 929 loss: 3.53838034912957e-06
Iter: 930 loss: 3.5385756299789205e-06
Iter: 931 loss: 3.5365071367018155e-06
Iter: 932 loss: 3.54019164301701e-06
Iter: 933 loss: 3.535756857383355e-06
Iter: 934 loss: 3.5352649079957403e-06
Iter: 935 loss: 3.5337862888795126e-06
Iter: 936 loss: 3.5381326149382715e-06
Iter: 937 loss: 3.5330310475437566e-06
Iter: 938 loss: 3.5304217962939043e-06
Iter: 939 loss: 3.5362382324206839e-06
Iter: 940 loss: 3.529428879814835e-06
Iter: 941 loss: 3.5275127362811126e-06
Iter: 942 loss: 3.5315236591602387e-06
Iter: 943 loss: 3.5267525754250753e-06
Iter: 944 loss: 3.5240892578886178e-06
Iter: 945 loss: 3.5271403352022519e-06
Iter: 946 loss: 3.5226615540900831e-06
Iter: 947 loss: 3.5203674936649123e-06
Iter: 948 loss: 3.531116642500372e-06
Iter: 949 loss: 3.5199448851185775e-06
Iter: 950 loss: 3.5176321170794266e-06
Iter: 951 loss: 3.5223265321344553e-06
Iter: 952 loss: 3.5166963470270455e-06
Iter: 953 loss: 3.5145783324572487e-06
Iter: 954 loss: 3.52227347344998e-06
Iter: 955 loss: 3.514047928208077e-06
Iter: 956 loss: 3.5115300608783193e-06
Iter: 957 loss: 3.5185408986606486e-06
Iter: 958 loss: 3.5107199364877694e-06
Iter: 959 loss: 3.5091058830459687e-06
Iter: 960 loss: 3.5123811868020582e-06
Iter: 961 loss: 3.5084527810671613e-06
Iter: 962 loss: 3.5061884003517705e-06
Iter: 963 loss: 3.5054853063542804e-06
Iter: 964 loss: 3.5041472876088364e-06
Iter: 965 loss: 3.5024444673261142e-06
Iter: 966 loss: 3.5023895196647704e-06
Iter: 967 loss: 3.5006827665389067e-06
Iter: 968 loss: 3.5122765622533862e-06
Iter: 969 loss: 3.5005151209026428e-06
Iter: 970 loss: 3.4994783776026114e-06
Iter: 971 loss: 3.4973043201958234e-06
Iter: 972 loss: 3.5337751557157188e-06
Iter: 973 loss: 3.4972480924561464e-06
Iter: 974 loss: 3.4952891405468125e-06
Iter: 975 loss: 3.4967149517138273e-06
Iter: 976 loss: 3.494081140120218e-06
Iter: 977 loss: 3.4911929019803179e-06
Iter: 978 loss: 3.5072883876023469e-06
Iter: 979 loss: 3.4907828238783305e-06
Iter: 980 loss: 3.4891688220598054e-06
Iter: 981 loss: 3.4913721927267577e-06
Iter: 982 loss: 3.4883633981064872e-06
Iter: 983 loss: 3.4858344059635177e-06
Iter: 984 loss: 3.4867872568714323e-06
Iter: 985 loss: 3.4840713763753582e-06
Iter: 986 loss: 3.4818031517494738e-06
Iter: 987 loss: 3.4988234211658203e-06
Iter: 988 loss: 3.4816252109249236e-06
Iter: 989 loss: 3.4791867340559796e-06
Iter: 990 loss: 3.4793668387594272e-06
Iter: 991 loss: 3.4772890103246507e-06
Iter: 992 loss: 3.4752976450614792e-06
Iter: 993 loss: 3.47528348233426e-06
Iter: 994 loss: 3.4738251934675248e-06
Iter: 995 loss: 3.47091837779444e-06
Iter: 996 loss: 3.5266062803889221e-06
Iter: 997 loss: 3.4708841882239148e-06
Iter: 998 loss: 3.4684934947766575e-06
Iter: 999 loss: 3.5067518095925961e-06
Iter: 1000 loss: 3.4684934947725172e-06
Iter: 1001 loss: 3.4673659573021859e-06
Iter: 1002 loss: 3.4673480398870236e-06
Iter: 1003 loss: 3.4660070595419937e-06
Iter: 1004 loss: 3.46439276806604e-06
Iter: 1005 loss: 3.464232744278817e-06
Iter: 1006 loss: 3.4629657727808166e-06
Iter: 1007 loss: 3.4616120193478448e-06
Iter: 1008 loss: 3.4613918788908605e-06
Iter: 1009 loss: 3.4585168219872431e-06
Iter: 1010 loss: 3.4621508782650539e-06
Iter: 1011 loss: 3.4570343774926909e-06
Iter: 1012 loss: 3.4551153464295738e-06
Iter: 1013 loss: 3.4791119002048632e-06
Iter: 1014 loss: 3.4550975947217294e-06
Iter: 1015 loss: 3.4533796761106456e-06
Iter: 1016 loss: 3.4524465185353474e-06
Iter: 1017 loss: 3.4516795474810088e-06
Iter: 1018 loss: 3.4493018841309951e-06
Iter: 1019 loss: 3.4582193878413912e-06
Iter: 1020 loss: 3.448726607539899e-06
Iter: 1021 loss: 3.4462422348148057e-06
Iter: 1022 loss: 3.4556077234565264e-06
Iter: 1023 loss: 3.4456443225576485e-06
Iter: 1024 loss: 3.443884504828069e-06
Iter: 1025 loss: 3.4475469072672622e-06
Iter: 1026 loss: 3.4431837173691247e-06
Iter: 1027 loss: 3.4404971943977545e-06
Iter: 1028 loss: 3.4444067629604686e-06
Iter: 1029 loss: 3.4391948437054904e-06
Iter: 1030 loss: 3.4373650917481228e-06
Iter: 1031 loss: 3.4502505232091452e-06
Iter: 1032 loss: 3.4371988846936828e-06
Iter: 1033 loss: 3.4355850315807883e-06
Iter: 1034 loss: 3.4349209457922494e-06
Iter: 1035 loss: 3.4340708016237781e-06
Iter: 1036 loss: 3.4334394565680089e-06
Iter: 1037 loss: 3.4326527140261784e-06
Iter: 1038 loss: 3.4320559713958289e-06
Iter: 1039 loss: 3.4306537855968313e-06
Iter: 1040 loss: 3.4472776823807377e-06
Iter: 1041 loss: 3.4305340145347153e-06
Iter: 1042 loss: 3.4286466540698113e-06
Iter: 1043 loss: 3.425960153799542e-06
Iter: 1044 loss: 3.4258670134114147e-06
Iter: 1045 loss: 3.4235436300999918e-06
Iter: 1046 loss: 3.4503886478046746e-06
Iter: 1047 loss: 3.4235067439327975e-06
Iter: 1048 loss: 3.4212404667411051e-06
Iter: 1049 loss: 3.4228703342063669e-06
Iter: 1050 loss: 3.4198388348624717e-06
Iter: 1051 loss: 3.4176763953774552e-06
Iter: 1052 loss: 3.4368943698272042e-06
Iter: 1053 loss: 3.4175702934087212e-06
Iter: 1054 loss: 3.4156929337549451e-06
Iter: 1055 loss: 3.4143158605352965e-06
Iter: 1056 loss: 3.4136813615553704e-06
Iter: 1057 loss: 3.4120051389271397e-06
Iter: 1058 loss: 3.411999917244658e-06
Iter: 1059 loss: 3.410545109373657e-06
Iter: 1060 loss: 3.4082362178839456e-06
Iter: 1061 loss: 3.4082140259148026e-06
Iter: 1062 loss: 3.4062234444060676e-06
Iter: 1063 loss: 3.4061395225392503e-06
Iter: 1064 loss: 3.4049721858644715e-06
Iter: 1065 loss: 3.4037506010320409e-06
Iter: 1066 loss: 3.4035365934293841e-06
Iter: 1067 loss: 3.4027386965285505e-06
Iter: 1068 loss: 3.4024790708836821e-06
Iter: 1069 loss: 3.4013774846146446e-06
Iter: 1070 loss: 3.4022557252271173e-06
Iter: 1071 loss: 3.4007141131438416e-06
Iter: 1072 loss: 3.3996964785778204e-06
Iter: 1073 loss: 3.3972773099803562e-06
Iter: 1074 loss: 3.4248173694767152e-06
Iter: 1075 loss: 3.3970491507782611e-06
Iter: 1076 loss: 3.3948468519022175e-06
Iter: 1077 loss: 3.4161590160143987e-06
Iter: 1078 loss: 3.3947664119501777e-06
Iter: 1079 loss: 3.39289662657417e-06
Iter: 1080 loss: 3.3924622696877115e-06
Iter: 1081 loss: 3.3912620789940524e-06
Iter: 1082 loss: 3.3893134337701954e-06
Iter: 1083 loss: 3.4005613537004522e-06
Iter: 1084 loss: 3.3890526590597851e-06
Iter: 1085 loss: 3.3868324815458694e-06
Iter: 1086 loss: 3.3919912075620071e-06
Iter: 1087 loss: 3.3860115937835973e-06
Iter: 1088 loss: 3.3842532652664339e-06
Iter: 1089 loss: 3.3881502645551297e-06
Iter: 1090 loss: 3.3835815517751018e-06
Iter: 1091 loss: 3.381163483325902e-06
Iter: 1092 loss: 3.3840210357186408e-06
Iter: 1093 loss: 3.3798825769315634e-06
Iter: 1094 loss: 3.3782693273484746e-06
Iter: 1095 loss: 3.3938515365648416e-06
Iter: 1096 loss: 3.378209991151228e-06
Iter: 1097 loss: 3.3765689834371459e-06
Iter: 1098 loss: 3.3759670529941768e-06
Iter: 1099 loss: 3.3750565831329323e-06
Iter: 1100 loss: 3.3729164771455955e-06
Iter: 1101 loss: 3.3949279554407317e-06
Iter: 1102 loss: 3.3728553688761423e-06
Iter: 1103 loss: 3.3724731424764841e-06
Iter: 1104 loss: 3.3722317363320094e-06
Iter: 1105 loss: 3.371683735720345e-06
Iter: 1106 loss: 3.3699046752997961e-06
Iter: 1107 loss: 3.3709189086815284e-06
Iter: 1108 loss: 3.368316276031711e-06
Iter: 1109 loss: 3.3659813390672673e-06
Iter: 1110 loss: 3.3961751239085477e-06
Iter: 1111 loss: 3.3659651149595265e-06
Iter: 1112 loss: 3.3644680719007683e-06
Iter: 1113 loss: 3.3619927960886296e-06
Iter: 1114 loss: 3.3619828202473075e-06
Iter: 1115 loss: 3.3590985055361762e-06
Iter: 1116 loss: 3.3953313926509786e-06
Iter: 1117 loss: 3.3590728118424635e-06
Iter: 1118 loss: 3.3572833877210974e-06
Iter: 1119 loss: 3.3575697063606495e-06
Iter: 1120 loss: 3.3559341919206354e-06
Iter: 1121 loss: 3.3540387303469918e-06
Iter: 1122 loss: 3.3762193080337442e-06
Iter: 1123 loss: 3.3540108480465182e-06
Iter: 1124 loss: 3.3523874246959282e-06
Iter: 1125 loss: 3.35105043659818e-06
Iter: 1126 loss: 3.3505792585463808e-06
Iter: 1127 loss: 3.3482906522109732e-06
Iter: 1128 loss: 3.3694474380684968e-06
Iter: 1129 loss: 3.3481920716437121e-06
Iter: 1130 loss: 3.3463060503358191e-06
Iter: 1131 loss: 3.3460705908008632e-06
Iter: 1132 loss: 3.3447242125128544e-06
Iter: 1133 loss: 3.3429024399821693e-06
Iter: 1134 loss: 3.342897698072526e-06
Iter: 1135 loss: 3.34175996655159e-06
Iter: 1136 loss: 3.3464479720444978e-06
Iter: 1137 loss: 3.3415128380056983e-06
Iter: 1138 loss: 3.3398718635528526e-06
Iter: 1139 loss: 3.3427134969973678e-06
Iter: 1140 loss: 3.3391431285760913e-06
Iter: 1141 loss: 3.33794376478998e-06
Iter: 1142 loss: 3.3371895460282405e-06
Iter: 1143 loss: 3.3367140470036664e-06
Iter: 1144 loss: 3.3351276249574919e-06
Iter: 1145 loss: 3.3331355108582694e-06
Iter: 1146 loss: 3.3329759491987546e-06
Iter: 1147 loss: 3.3311452152742756e-06
Iter: 1148 loss: 3.3311223713375936e-06
Iter: 1149 loss: 3.3296132526470676e-06
Iter: 1150 loss: 3.326861713299021e-06
Iter: 1151 loss: 3.3919366604632407e-06
Iter: 1152 loss: 3.3268602592313588e-06
Iter: 1153 loss: 3.3248900284233784e-06
Iter: 1154 loss: 3.324812443012228e-06
Iter: 1155 loss: 3.32319536916262e-06
Iter: 1156 loss: 3.3208035761323759e-06
Iter: 1157 loss: 3.320745622449508e-06
Iter: 1158 loss: 3.3181648077630968e-06
Iter: 1159 loss: 3.3181619188675032e-06
Iter: 1160 loss: 3.3166290192704662e-06
Iter: 1161 loss: 3.3154449627888595e-06
Iter: 1162 loss: 3.314958919158583e-06
Iter: 1163 loss: 3.3126564947856887e-06
Iter: 1164 loss: 3.3362807191161809e-06
Iter: 1165 loss: 3.3125900527547091e-06
Iter: 1166 loss: 3.3112519242849589e-06
Iter: 1167 loss: 3.3149534920088539e-06
Iter: 1168 loss: 3.3108189178890695e-06
Iter: 1169 loss: 3.3094460779351665e-06
Iter: 1170 loss: 3.329078296849738e-06
Iter: 1171 loss: 3.3094433816659913e-06
Iter: 1172 loss: 3.3083717892465586e-06
Iter: 1173 loss: 3.3074653074983914e-06
Iter: 1174 loss: 3.3071665839779511e-06
Iter: 1175 loss: 3.3059743623116846e-06
Iter: 1176 loss: 3.3046838970186256e-06
Iter: 1177 loss: 3.3044838590226093e-06
Iter: 1178 loss: 3.30234795223464e-06
Iter: 1179 loss: 3.3108965854769648e-06
Iter: 1180 loss: 3.3018677055381761e-06
Iter: 1181 loss: 3.3000692097888659e-06
Iter: 1182 loss: 3.2995882673025327e-06
Iter: 1183 loss: 3.2984752452812193e-06
Iter: 1184 loss: 3.296056014909322e-06
Iter: 1185 loss: 3.3222759299930574e-06
Iter: 1186 loss: 3.2960017423601441e-06
Iter: 1187 loss: 3.2940550283066516e-06
Iter: 1188 loss: 3.2925230386990798e-06
Iter: 1189 loss: 3.2919208443467173e-06
Iter: 1190 loss: 3.2899606972469888e-06
Iter: 1191 loss: 3.2899561557460032e-06
Iter: 1192 loss: 3.2884575857737389e-06
Iter: 1193 loss: 3.2876663291718719e-06
Iter: 1194 loss: 3.286983747534031e-06
Iter: 1195 loss: 3.2849690157125455e-06
Iter: 1196 loss: 3.3085551985046329e-06
Iter: 1197 loss: 3.2849394666853575e-06
Iter: 1198 loss: 3.2837023382379026e-06
Iter: 1199 loss: 3.2824801999682639e-06
Iter: 1200 loss: 3.2822207826409578e-06
Iter: 1201 loss: 3.2809673630156353e-06
Iter: 1202 loss: 3.2807611993066992e-06
Iter: 1203 loss: 3.2796209819125964e-06
Iter: 1204 loss: 3.28418850709389e-06
Iter: 1205 loss: 3.2793649059183179e-06
Iter: 1206 loss: 3.2786752630072224e-06
Iter: 1207 loss: 3.2769917213412053e-06
Iter: 1208 loss: 3.2943749955000876e-06
Iter: 1209 loss: 3.2767945827810048e-06
Iter: 1210 loss: 3.2744209539529625e-06
Iter: 1211 loss: 3.2836970201881135e-06
Iter: 1212 loss: 3.2738723360033091e-06
Iter: 1213 loss: 3.272238461275438e-06
Iter: 1214 loss: 3.2704499588680282e-06
Iter: 1215 loss: 3.27018427553237e-06
Iter: 1216 loss: 3.2680655982337659e-06
Iter: 1217 loss: 3.2680592821289764e-06
Iter: 1218 loss: 3.2666234302030167e-06
Iter: 1219 loss: 3.265042223356387e-06
Iter: 1220 loss: 3.2648126414604908e-06
Iter: 1221 loss: 3.2628657959694388e-06
Iter: 1222 loss: 3.2917499505092857e-06
Iter: 1223 loss: 3.2628640450682874e-06
Iter: 1224 loss: 3.261326052542171e-06
Iter: 1225 loss: 3.2596325414905272e-06
Iter: 1226 loss: 3.2593866438778052e-06
Iter: 1227 loss: 3.2572543380060557e-06
Iter: 1228 loss: 3.2888417170909669e-06
Iter: 1229 loss: 3.2572523407609495e-06
Iter: 1230 loss: 3.2556000648330231e-06
Iter: 1231 loss: 3.2552791754783681e-06
Iter: 1232 loss: 3.2541767702830868e-06
Iter: 1233 loss: 3.252830326469787e-06
Iter: 1234 loss: 3.2528068775025852e-06
Iter: 1235 loss: 3.2518659751194033e-06
Iter: 1236 loss: 3.2632291670083619e-06
Iter: 1237 loss: 3.251854711649343e-06
Iter: 1238 loss: 3.251071134096149e-06
Iter: 1239 loss: 3.2493242202932087e-06
Iter: 1240 loss: 3.2738808120708987e-06
Iter: 1241 loss: 3.2492328057737346e-06
Iter: 1242 loss: 3.2477088541797332e-06
Iter: 1243 loss: 3.2573095810165519e-06
Iter: 1244 loss: 3.2475348655563787e-06
Iter: 1245 loss: 3.24626991405987e-06
Iter: 1246 loss: 3.2447912889073582e-06
Iter: 1247 loss: 3.2446235838779445e-06
Iter: 1248 loss: 3.2423759751654627e-06
Iter: 1249 loss: 3.2575096449203864e-06
Iter: 1250 loss: 3.2421511193630357e-06
Iter: 1251 loss: 3.2405976727716597e-06
Iter: 1252 loss: 3.2396104203192234e-06
Iter: 1253 loss: 3.2390004490463794e-06
Iter: 1254 loss: 3.2370060899275282e-06
Iter: 1255 loss: 3.2626726676968e-06
Iter: 1256 loss: 3.2369916291365232e-06
Iter: 1257 loss: 3.2353197145610292e-06
Iter: 1258 loss: 3.2331887505494278e-06
Iter: 1259 loss: 3.2330314706821231e-06
Iter: 1260 loss: 3.2316133829152493e-06
Iter: 1261 loss: 3.2314851462802675e-06
Iter: 1262 loss: 3.2302489095323817e-06
Iter: 1263 loss: 3.2281834054449397e-06
Iter: 1264 loss: 3.2281772541420603e-06
Iter: 1265 loss: 3.2258266851791053e-06
Iter: 1266 loss: 3.2618482153802881e-06
Iter: 1267 loss: 3.2258259847397108e-06
Iter: 1268 loss: 3.2252099224344917e-06
Iter: 1269 loss: 3.2251706776684702e-06
Iter: 1270 loss: 3.2243901098226316e-06
Iter: 1271 loss: 3.222817014485114e-06
Iter: 1272 loss: 3.2521500423098326e-06
Iter: 1273 loss: 3.2227945650833154e-06
Iter: 1274 loss: 3.2211179756275956e-06
Iter: 1275 loss: 3.2261964408332627e-06
Iter: 1276 loss: 3.2206163074345615e-06
Iter: 1277 loss: 3.2194034435803352e-06
Iter: 1278 loss: 3.2197706056560665e-06
Iter: 1279 loss: 3.2185352487898308e-06
Iter: 1280 loss: 3.216569254324563e-06
Iter: 1281 loss: 3.2213655802250463e-06
Iter: 1282 loss: 3.2158675691573846e-06
Iter: 1283 loss: 3.2142557673801683e-06
Iter: 1284 loss: 3.215987945034766e-06
Iter: 1285 loss: 3.2133713158194093e-06
Iter: 1286 loss: 3.2115728257059188e-06
Iter: 1287 loss: 3.2215232763230975e-06
Iter: 1288 loss: 3.2113144226307593e-06
Iter: 1289 loss: 3.2097243864605819e-06
Iter: 1290 loss: 3.2083488313575365e-06
Iter: 1291 loss: 3.2079210890395839e-06
Iter: 1292 loss: 3.2060936135730764e-06
Iter: 1293 loss: 3.2060749922848631e-06
Iter: 1294 loss: 3.2048168208645003e-06
Iter: 1295 loss: 3.2025135023053205e-06
Iter: 1296 loss: 3.2565102496539512e-06
Iter: 1297 loss: 3.2025118600070546e-06
Iter: 1298 loss: 3.2008839007556971e-06
Iter: 1299 loss: 3.2007593081079667e-06
Iter: 1300 loss: 3.19948404024314e-06
Iter: 1301 loss: 3.2006375703775489e-06
Iter: 1302 loss: 3.1987433122213263e-06
Iter: 1303 loss: 3.1972753217267886e-06
Iter: 1304 loss: 3.1972667248550762e-06
Iter: 1305 loss: 3.1965567123802624e-06
Iter: 1306 loss: 3.1956076780563714e-06
Iter: 1307 loss: 3.1955551479494303e-06
Iter: 1308 loss: 3.1943111762719051e-06
Iter: 1309 loss: 3.1930854321472974e-06
Iter: 1310 loss: 3.1928230478325144e-06
Iter: 1311 loss: 3.190655439653466e-06
Iter: 1312 loss: 3.2079867826284156e-06
Iter: 1313 loss: 3.190510735529509e-06
Iter: 1314 loss: 3.189093487709265e-06
Iter: 1315 loss: 3.1876435937860481e-06
Iter: 1316 loss: 3.1873690470493314e-06
Iter: 1317 loss: 3.1844510625610178e-06
Iter: 1318 loss: 3.2005704524344985e-06
Iter: 1319 loss: 3.184030732183375e-06
Iter: 1320 loss: 3.1827528012187531e-06
Iter: 1321 loss: 3.1842605736010124e-06
Iter: 1322 loss: 3.1820754378710092e-06
Iter: 1323 loss: 3.1802538872162839e-06
Iter: 1324 loss: 3.1857484945139446e-06
Iter: 1325 loss: 3.1797067595881076e-06
Iter: 1326 loss: 3.1780951604530351e-06
Iter: 1327 loss: 3.1785587439793678e-06
Iter: 1328 loss: 3.1769352783822688e-06
Iter: 1329 loss: 3.1746767166544919e-06
Iter: 1330 loss: 3.1898075399166513e-06
Iter: 1331 loss: 3.1744484720733354e-06
Iter: 1332 loss: 3.1728361799440576e-06
Iter: 1333 loss: 3.1758304637177759e-06
Iter: 1334 loss: 3.17214798628668e-06
Iter: 1335 loss: 3.1718575424290417e-06
Iter: 1336 loss: 3.1713137895735243e-06
Iter: 1337 loss: 3.1706291311669117e-06
Iter: 1338 loss: 3.1703276800258091e-06
Iter: 1339 loss: 3.1699793340495793e-06
Iter: 1340 loss: 3.1690907913571434e-06
Iter: 1341 loss: 3.1672836804633667e-06
Iter: 1342 loss: 3.2001738367286582e-06
Iter: 1343 loss: 3.1672536861182154e-06
Iter: 1344 loss: 3.1652322148291804e-06
Iter: 1345 loss: 3.1833606249231678e-06
Iter: 1346 loss: 3.1651358472153951e-06
Iter: 1347 loss: 3.1636587264939781e-06
Iter: 1348 loss: 3.1635244220823906e-06
Iter: 1349 loss: 3.162435779220951e-06
Iter: 1350 loss: 3.1600175749490096e-06
Iter: 1351 loss: 3.1697816754476055e-06
Iter: 1352 loss: 3.1594793518979821e-06
Iter: 1353 loss: 3.1581376418728784e-06
Iter: 1354 loss: 3.160780260953202e-06
Iter: 1355 loss: 3.157584625897099e-06
Iter: 1356 loss: 3.15573611313808e-06
Iter: 1357 loss: 3.158579801273374e-06
Iter: 1358 loss: 3.1548636018879282e-06
Iter: 1359 loss: 3.1533963589443641e-06
Iter: 1360 loss: 3.1545235869323591e-06
Iter: 1361 loss: 3.1525039071535397e-06
Iter: 1362 loss: 3.15068160192674e-06
Iter: 1363 loss: 3.1681943858999766e-06
Iter: 1364 loss: 3.150613287911274e-06
Iter: 1365 loss: 3.1493997519713066e-06
Iter: 1366 loss: 3.1476850866068659e-06
Iter: 1367 loss: 3.1476218753943108e-06
Iter: 1368 loss: 3.1465189650566728e-06
Iter: 1369 loss: 3.1462209989048008e-06
Iter: 1370 loss: 3.1452685841256393e-06
Iter: 1371 loss: 3.157495254969811e-06
Iter: 1372 loss: 3.145261509147821e-06
Iter: 1373 loss: 3.144774073721851e-06
Iter: 1374 loss: 3.1433963651574517e-06
Iter: 1375 loss: 3.1502354713459845e-06
Iter: 1376 loss: 3.142933869075745e-06
Iter: 1377 loss: 3.141287479195296e-06
Iter: 1378 loss: 3.1583125057981844e-06
Iter: 1379 loss: 3.1412415045761983e-06
Iter: 1380 loss: 3.1401961021111346e-06
Iter: 1381 loss: 3.1391485146870663e-06
Iter: 1382 loss: 3.1389360675338482e-06
Iter: 1383 loss: 3.1371052423721094e-06
Iter: 1384 loss: 3.1538127210936582e-06
Iter: 1385 loss: 3.1370228626576448e-06
Iter: 1386 loss: 3.1358376881731209e-06
Iter: 1387 loss: 3.1347546596167543e-06
Iter: 1388 loss: 3.1344644088271474e-06
Iter: 1389 loss: 3.1326150921291374e-06
Iter: 1390 loss: 3.1500972304561256e-06
Iter: 1391 loss: 3.1325414294714555e-06
Iter: 1392 loss: 3.131330720756076e-06
Iter: 1393 loss: 3.130688430853515e-06
Iter: 1394 loss: 3.1301388020521665e-06
Iter: 1395 loss: 3.1281887586701475e-06
Iter: 1396 loss: 3.1398505351357294e-06
Iter: 1397 loss: 3.1279436222147696e-06
Iter: 1398 loss: 3.1263483125948271e-06
Iter: 1399 loss: 3.1264433649926739e-06
Iter: 1400 loss: 3.1251001120676021e-06
Iter: 1401 loss: 3.1238267231781718e-06
Iter: 1402 loss: 3.1238161049031773e-06
Iter: 1403 loss: 3.1232069070788687e-06
Iter: 1404 loss: 3.1231976076444997e-06
Iter: 1405 loss: 3.122446651281341e-06
Iter: 1406 loss: 3.1206341954877308e-06
Iter: 1407 loss: 3.1401762917259654e-06
Iter: 1408 loss: 3.1204405528868333e-06
Iter: 1409 loss: 3.1193376159466945e-06
Iter: 1410 loss: 3.1254699798219104e-06
Iter: 1411 loss: 3.1191804347105002e-06
Iter: 1412 loss: 3.1179307898181212e-06
Iter: 1413 loss: 3.1166981841079276e-06
Iter: 1414 loss: 3.1164352049691973e-06
Iter: 1415 loss: 3.1146749043987404e-06
Iter: 1416 loss: 3.1347627421093051e-06
Iter: 1417 loss: 3.1146448410013876e-06
Iter: 1418 loss: 3.1136095632326374e-06
Iter: 1419 loss: 3.1138984664497208e-06
Iter: 1420 loss: 3.112862145072324e-06
Iter: 1421 loss: 3.1112536093484159e-06
Iter: 1422 loss: 3.1169755772068215e-06
Iter: 1423 loss: 3.1108417198694926e-06
Iter: 1424 loss: 3.1096625741857594e-06
Iter: 1425 loss: 3.1095889353195204e-06
Iter: 1426 loss: 3.1086968408348492e-06
Iter: 1427 loss: 3.106612426408646e-06
Iter: 1428 loss: 3.1142683953487038e-06
Iter: 1429 loss: 3.1060964020707824e-06
Iter: 1430 loss: 3.1043495699986135e-06
Iter: 1431 loss: 3.104089772364027e-06
Iter: 1432 loss: 3.1028710846509703e-06
Iter: 1433 loss: 3.1007698668756603e-06
Iter: 1434 loss: 3.1271388148849572e-06
Iter: 1435 loss: 3.10075100886153e-06
Iter: 1436 loss: 3.0995370180668757e-06
Iter: 1437 loss: 3.1038248766423807e-06
Iter: 1438 loss: 3.0992237473043717e-06
Iter: 1439 loss: 3.0976174083660388e-06
Iter: 1440 loss: 3.1087893694320449e-06
Iter: 1441 loss: 3.0974675670135569e-06
Iter: 1442 loss: 3.0966656641412647e-06
Iter: 1443 loss: 3.0952740908998053e-06
Iter: 1444 loss: 3.0952734440993286e-06
Iter: 1445 loss: 3.0937381443437853e-06
Iter: 1446 loss: 3.0949142612145075e-06
Iter: 1447 loss: 3.0928037026401794e-06
Iter: 1448 loss: 3.0915205144940595e-06
Iter: 1449 loss: 3.0915204768677883e-06
Iter: 1450 loss: 3.0905239531472993e-06
Iter: 1451 loss: 3.0888790123042719e-06
Iter: 1452 loss: 3.0888720710980519e-06
Iter: 1453 loss: 3.0874455597438445e-06
Iter: 1454 loss: 3.0874371633396819e-06
Iter: 1455 loss: 3.08650189437117e-06
Iter: 1456 loss: 3.0852291799571059e-06
Iter: 1457 loss: 3.0851667966244568e-06
Iter: 1458 loss: 3.08312797110731e-06
Iter: 1459 loss: 3.0957446556784096e-06
Iter: 1460 loss: 3.0828871777610677e-06
Iter: 1461 loss: 3.0816428230719248e-06
Iter: 1462 loss: 3.0812325219497683e-06
Iter: 1463 loss: 3.0805127217682083e-06
Iter: 1464 loss: 3.078290787333094e-06
Iter: 1465 loss: 3.0893060336341943e-06
Iter: 1466 loss: 3.0779126630862505e-06
Iter: 1467 loss: 3.0766518073640127e-06
Iter: 1468 loss: 3.0772672952148929e-06
Iter: 1469 loss: 3.0758069410832082e-06
Iter: 1470 loss: 3.0756518990722944e-06
Iter: 1471 loss: 3.0749331170581151e-06
Iter: 1472 loss: 3.0743223716042711e-06
Iter: 1473 loss: 3.0733864488652273e-06
Iter: 1474 loss: 3.0733713665630636e-06
Iter: 1475 loss: 3.0723088816658138e-06
Iter: 1476 loss: 3.0703357737640031e-06
Iter: 1477 loss: 3.1151327514163467e-06
Iter: 1478 loss: 3.0703326231166813e-06
Iter: 1479 loss: 3.0691268141258026e-06
Iter: 1480 loss: 3.0690912326741734e-06
Iter: 1481 loss: 3.0679711015907023e-06
Iter: 1482 loss: 3.0677946079774363e-06
Iter: 1483 loss: 3.0670198172301086e-06
Iter: 1484 loss: 3.0654333979984615e-06
Iter: 1485 loss: 3.0729325489330523e-06
Iter: 1486 loss: 3.0651446598117773e-06
Iter: 1487 loss: 3.0637593705750282e-06
Iter: 1488 loss: 3.0642213658444933e-06
Iter: 1489 loss: 3.0627785595677556e-06
Iter: 1490 loss: 3.0612653370208694e-06
Iter: 1491 loss: 3.0759236438491225e-06
Iter: 1492 loss: 3.0612102588646158e-06
Iter: 1493 loss: 3.0600191966991589e-06
Iter: 1494 loss: 3.0582138268045294e-06
Iter: 1495 loss: 3.0581805106391893e-06
Iter: 1496 loss: 3.0564071753575194e-06
Iter: 1497 loss: 3.05640641369946e-06
Iter: 1498 loss: 3.055151859195175e-06
Iter: 1499 loss: 3.0529226994669e-06
Iter: 1500 loss: 3.0529226988674397e-06
Iter: 1501 loss: 3.0535881268567783e-06
Iter: 1502 loss: 3.0520435359011124e-06
Iter: 1503 loss: 3.0511784583089918e-06
Iter: 1504 loss: 3.05339176580348e-06
Iter: 1505 loss: 3.0508805580007673e-06
Iter: 1506 loss: 3.0501575323706203e-06
Iter: 1507 loss: 3.0483099216952362e-06
Iter: 1508 loss: 3.0641363292318943e-06
Iter: 1509 loss: 3.0480034669108162e-06
Iter: 1510 loss: 3.0468351563028323e-06
Iter: 1511 loss: 3.0468331945613517e-06
Iter: 1512 loss: 3.0458601970749424e-06
Iter: 1513 loss: 3.0444810760673389e-06
Iter: 1514 loss: 3.0444315280663193e-06
Iter: 1515 loss: 3.042756572405223e-06
Iter: 1516 loss: 3.0682126820056257e-06
Iter: 1517 loss: 3.0427558705703387e-06
Iter: 1518 loss: 3.0417636453628566e-06
Iter: 1519 loss: 3.0409480124851365e-06
Iter: 1520 loss: 3.0406592564934983e-06
Iter: 1521 loss: 3.03887905544301e-06
Iter: 1522 loss: 3.0481462005323716e-06
Iter: 1523 loss: 3.0385974472560214e-06
Iter: 1524 loss: 3.0372863101631151e-06
Iter: 1525 loss: 3.0387409016346363e-06
Iter: 1526 loss: 3.0365750046437338e-06
Iter: 1527 loss: 3.0349909275339471e-06
Iter: 1528 loss: 3.0435437356123427e-06
Iter: 1529 loss: 3.0347541690253079e-06
Iter: 1530 loss: 3.0335428803389211e-06
Iter: 1531 loss: 3.0330834933336119e-06
Iter: 1532 loss: 3.0324210280894903e-06
Iter: 1533 loss: 3.0304031777042442e-06
Iter: 1534 loss: 3.0411337792894746e-06
Iter: 1535 loss: 3.030094211635145e-06
Iter: 1536 loss: 3.0302388920026655e-06
Iter: 1537 loss: 3.0295342802285678e-06
Iter: 1538 loss: 3.0289885237975765e-06
Iter: 1539 loss: 3.0274271457845364e-06
Iter: 1540 loss: 3.0345453827107343e-06
Iter: 1541 loss: 3.0268581341796419e-06
Iter: 1542 loss: 3.0255870190204287e-06
Iter: 1543 loss: 3.0330197558692686e-06
Iter: 1544 loss: 3.0254207304341386e-06
Iter: 1545 loss: 3.0240733672610433e-06
Iter: 1546 loss: 3.023100235709233e-06
Iter: 1547 loss: 3.0226365462098324e-06
Iter: 1548 loss: 3.0212666532390461e-06
Iter: 1549 loss: 3.0212648843512721e-06
Iter: 1550 loss: 3.0201187727720282e-06
Iter: 1551 loss: 3.0197704015516183e-06
Iter: 1552 loss: 3.01908841066693e-06
Iter: 1553 loss: 3.0175033481501071e-06
Iter: 1554 loss: 3.0299270872010569e-06
Iter: 1555 loss: 3.0173918449210871e-06
Iter: 1556 loss: 3.01647556247717e-06
Iter: 1557 loss: 3.0156517969931286e-06
Iter: 1558 loss: 3.0154207809546086e-06
Iter: 1559 loss: 3.0137175198338978e-06
Iter: 1560 loss: 3.0262544400338191e-06
Iter: 1561 loss: 3.0135776364866543e-06
Iter: 1562 loss: 3.0124430202497493e-06
Iter: 1563 loss: 3.01203130255584e-06
Iter: 1564 loss: 3.0113989779064744e-06
Iter: 1565 loss: 3.0094717407392583e-06
Iter: 1566 loss: 3.0204279242331818e-06
Iter: 1567 loss: 3.0092071099580122e-06
Iter: 1568 loss: 3.0081643903731469e-06
Iter: 1569 loss: 3.0131672610785004e-06
Iter: 1570 loss: 3.0079784126684495e-06
Iter: 1571 loss: 3.006527023894628e-06
Iter: 1572 loss: 3.0123731021685242e-06
Iter: 1573 loss: 3.0062032943856367e-06
Iter: 1574 loss: 3.0053570692398175e-06
Iter: 1575 loss: 3.0044492944180123e-06
Iter: 1576 loss: 3.0043037967494513e-06
Iter: 1577 loss: 3.0031965093218148e-06
Iter: 1578 loss: 3.0021584547741653e-06
Iter: 1579 loss: 3.001899946248741e-06
Iter: 1580 loss: 3.0005123157248986e-06
Iter: 1581 loss: 3.0182816055931209e-06
Iter: 1582 loss: 3.000501790157069e-06
Iter: 1583 loss: 2.9992039101571797e-06
Iter: 1584 loss: 2.9995878547305539e-06
Iter: 1585 loss: 2.9982725466143385e-06
Iter: 1586 loss: 2.9970127417764525e-06
Iter: 1587 loss: 3.0123178458663624e-06
Iter: 1588 loss: 2.9969982837635427e-06
Iter: 1589 loss: 2.9959316076374096e-06
Iter: 1590 loss: 2.9944650729797539e-06
Iter: 1591 loss: 2.9943982644415292e-06
Iter: 1592 loss: 2.992947413505138e-06
Iter: 1593 loss: 3.0121447871434889e-06
Iter: 1594 loss: 2.9929393527712279e-06
Iter: 1595 loss: 2.9918226555629631e-06
Iter: 1596 loss: 2.9909094287062064e-06
Iter: 1597 loss: 2.9905820050704169e-06
Iter: 1598 loss: 2.9891788850532937e-06
Iter: 1599 loss: 3.0092249349963504e-06
Iter: 1600 loss: 2.9891760683094814e-06
Iter: 1601 loss: 2.9881948314173408e-06
Iter: 1602 loss: 2.9870462886736086e-06
Iter: 1603 loss: 2.9869168559441885e-06
Iter: 1604 loss: 2.9868706402364767e-06
Iter: 1605 loss: 2.986052318685398e-06
Iter: 1606 loss: 2.9854052483316945e-06
Iter: 1607 loss: 2.9844652546391655e-06
Iter: 1608 loss: 2.9844380837392818e-06
Iter: 1609 loss: 2.98341220786978e-06
Iter: 1610 loss: 2.9817413287571947e-06
Iter: 1611 loss: 2.9817316052517373e-06
Iter: 1612 loss: 2.980390792847885e-06
Iter: 1613 loss: 2.9939373200957394e-06
Iter: 1614 loss: 2.9803494950719933e-06
Iter: 1615 loss: 2.9789326544058724e-06
Iter: 1616 loss: 2.9774484345571861e-06
Iter: 1617 loss: 2.97718931164556e-06
Iter: 1618 loss: 2.976035780843072e-06
Iter: 1619 loss: 2.9759059731668377e-06
Iter: 1620 loss: 2.9749863881746604e-06
Iter: 1621 loss: 2.9744409383182238e-06
Iter: 1622 loss: 2.9740574364042865e-06
Iter: 1623 loss: 2.9723231631182743e-06
Iter: 1624 loss: 2.9783738593092812e-06
Iter: 1625 loss: 2.9718700939105521e-06
Iter: 1626 loss: 2.9707021185504872e-06
Iter: 1627 loss: 2.9716752679059628e-06
Iter: 1628 loss: 2.9700072612425885e-06
Iter: 1629 loss: 2.9685959376754561e-06
Iter: 1630 loss: 2.9781611493686373e-06
Iter: 1631 loss: 2.9684567185380603e-06
Iter: 1632 loss: 2.9672784757891581e-06
Iter: 1633 loss: 2.9667953700086148e-06
Iter: 1634 loss: 2.9661736900373395e-06
Iter: 1635 loss: 2.9650895197858005e-06
Iter: 1636 loss: 2.9650243560934746e-06
Iter: 1637 loss: 2.9641185849732195e-06
Iter: 1638 loss: 2.9716784809695183e-06
Iter: 1639 loss: 2.9640648379632229e-06
Iter: 1640 loss: 2.9635691128329312e-06
Iter: 1641 loss: 2.962173307788514e-06
Iter: 1642 loss: 2.9692853081196461e-06
Iter: 1643 loss: 2.9617167794881687e-06
Iter: 1644 loss: 2.9604206698659158e-06
Iter: 1645 loss: 2.976840002061119e-06
Iter: 1646 loss: 2.96040987339543e-06
Iter: 1647 loss: 2.9594506335122925e-06
Iter: 1648 loss: 2.9573623476922586e-06
Iter: 1649 loss: 2.9888868748302396e-06
Iter: 1650 loss: 2.957277420670393e-06
Iter: 1651 loss: 2.9561185950249546e-06
Iter: 1652 loss: 2.9559565533830482e-06
Iter: 1653 loss: 2.9547044052506858e-06
Iter: 1654 loss: 2.9540935107692116e-06
Iter: 1655 loss: 2.9534928797090655e-06
Iter: 1656 loss: 2.95235615889006e-06
Iter: 1657 loss: 2.9523370695382744e-06
Iter: 1658 loss: 2.9515062230379317e-06
Iter: 1659 loss: 2.9498157123473567e-06
Iter: 1660 loss: 2.98054856055613e-06
Iter: 1661 loss: 2.9497874566697349e-06
Iter: 1662 loss: 2.9482335671676353e-06
Iter: 1663 loss: 2.9715733318366212e-06
Iter: 1664 loss: 2.948232591331772e-06
Iter: 1665 loss: 2.9470901725162543e-06
Iter: 1666 loss: 2.9465426254776283e-06
Iter: 1667 loss: 2.9459886603300817e-06
Iter: 1668 loss: 2.9447095126244714e-06
Iter: 1669 loss: 2.944709492116185e-06
Iter: 1670 loss: 2.944368727422433e-06
Iter: 1671 loss: 2.9442187061952525e-06
Iter: 1672 loss: 2.9438320628525532e-06
Iter: 1673 loss: 2.9426264340144486e-06
Iter: 1674 loss: 2.9447810664974844e-06
Iter: 1675 loss: 2.9418287933888263e-06
Iter: 1676 loss: 2.9403582520337522e-06
Iter: 1677 loss: 2.9538839741928989e-06
Iter: 1678 loss: 2.9402937939898193e-06
Iter: 1679 loss: 2.9391203552714332e-06
Iter: 1680 loss: 2.9384782084851818e-06
Iter: 1681 loss: 2.9379571629735262e-06
Iter: 1682 loss: 2.9364623544581366e-06
Iter: 1683 loss: 2.9432838414346977e-06
Iter: 1684 loss: 2.9361770144484275e-06
Iter: 1685 loss: 2.9346255764727573e-06
Iter: 1686 loss: 2.9400358903006202e-06
Iter: 1687 loss: 2.9342201638991943e-06
Iter: 1688 loss: 2.933145120877677e-06
Iter: 1689 loss: 2.9370708610694371e-06
Iter: 1690 loss: 2.9328773553240092e-06
Iter: 1691 loss: 2.9313630646223128e-06
Iter: 1692 loss: 2.9319087154494965e-06
Iter: 1693 loss: 2.93030119525447e-06
Iter: 1694 loss: 2.9291357616438696e-06
Iter: 1695 loss: 2.9402917125169908e-06
Iter: 1696 loss: 2.92909140872104e-06
Iter: 1697 loss: 2.9280995423633375e-06
Iter: 1698 loss: 2.9264312861632485e-06
Iter: 1699 loss: 2.9264273112680886e-06
Iter: 1700 loss: 2.9252699680475728e-06
Iter: 1701 loss: 2.9252172796254572e-06
Iter: 1702 loss: 2.9244156309713542e-06
Iter: 1703 loss: 2.9300305437042531e-06
Iter: 1704 loss: 2.9243418893944665e-06
Iter: 1705 loss: 2.9232395250277492e-06
Iter: 1706 loss: 2.9230600102253391e-06
Iter: 1707 loss: 2.9223015404374057e-06
Iter: 1708 loss: 2.9214962032737007e-06
Iter: 1709 loss: 2.9214792777630052e-06
Iter: 1710 loss: 2.9208468358177872e-06
Iter: 1711 loss: 2.9196016929215187e-06
Iter: 1712 loss: 2.91899705011314e-06
Iter: 1713 loss: 2.9183980803188225e-06
Iter: 1714 loss: 2.9170510076396524e-06
Iter: 1715 loss: 2.9335706329895127e-06
Iter: 1716 loss: 2.9170365535371918e-06
Iter: 1717 loss: 2.9159793789770551e-06
Iter: 1718 loss: 2.9146163723047778e-06
Iter: 1719 loss: 2.914522113534952e-06
Iter: 1720 loss: 2.9129763217725985e-06
Iter: 1721 loss: 2.9318724359876606e-06
Iter: 1722 loss: 2.9129593586018441e-06
Iter: 1723 loss: 2.9116740787426482e-06
Iter: 1724 loss: 2.91372402004752e-06
Iter: 1725 loss: 2.9110782573663868e-06
Iter: 1726 loss: 2.9100478571457455e-06
Iter: 1727 loss: 2.9214964819660692e-06
Iter: 1728 loss: 2.9100274845688384e-06
Iter: 1729 loss: 2.90922161173101e-06
Iter: 1730 loss: 2.9082545752735855e-06
Iter: 1731 loss: 2.9081572837414504e-06
Iter: 1732 loss: 2.9069470455024019e-06
Iter: 1733 loss: 2.9159694514429118e-06
Iter: 1734 loss: 2.9068506176042894e-06
Iter: 1735 loss: 2.9057339230467269e-06
Iter: 1736 loss: 2.9058555680304811e-06
Iter: 1737 loss: 2.9048762380817644e-06
Iter: 1738 loss: 2.9046758944784638e-06
Iter: 1739 loss: 2.9040538584718761e-06
Iter: 1740 loss: 2.9036769447954402e-06
Iter: 1741 loss: 2.9027865861136835e-06
Iter: 1742 loss: 2.9131478423945931e-06
Iter: 1743 loss: 2.902706980003896e-06
Iter: 1744 loss: 2.9013995096642683e-06
Iter: 1745 loss: 2.9001550008889391e-06
Iter: 1746 loss: 2.8998587231619593e-06
Iter: 1747 loss: 2.8985354601742738e-06
Iter: 1748 loss: 2.9139627665431173e-06
Iter: 1749 loss: 2.898515551396359e-06
Iter: 1750 loss: 2.8973229602835295e-06
Iter: 1751 loss: 2.8964357783792424e-06
Iter: 1752 loss: 2.8960394379360537e-06
Iter: 1753 loss: 2.8947045759447439e-06
Iter: 1754 loss: 2.9108094140587635e-06
Iter: 1755 loss: 2.894688481867153e-06
Iter: 1756 loss: 2.8935242042929371e-06
Iter: 1757 loss: 2.89263519184042e-06
Iter: 1758 loss: 2.8922605215662831e-06
Iter: 1759 loss: 2.8912379597238283e-06
Iter: 1760 loss: 2.89122887787002e-06
Iter: 1761 loss: 2.8902269821413949e-06
Iter: 1762 loss: 2.8895777483356338e-06
Iter: 1763 loss: 2.8891915113889554e-06
Iter: 1764 loss: 2.8878217331650847e-06
Iter: 1765 loss: 2.8987461644204308e-06
Iter: 1766 loss: 2.8877296562001669e-06
Iter: 1767 loss: 2.886646361188029e-06
Iter: 1768 loss: 2.8852110708738827e-06
Iter: 1769 loss: 2.8851269390848792e-06
Iter: 1770 loss: 2.885831603275771e-06
Iter: 1771 loss: 2.8844860871658788e-06
Iter: 1772 loss: 2.8838504678256433e-06
Iter: 1773 loss: 2.8836213814001487e-06
Iter: 1774 loss: 2.8832661352729912e-06
Iter: 1775 loss: 2.8824809442643351e-06
Iter: 1776 loss: 2.8809697013419545e-06
Iter: 1777 loss: 2.9125176506183264e-06
Iter: 1778 loss: 2.8809614315588397e-06
Iter: 1779 loss: 2.87974079799451e-06
Iter: 1780 loss: 2.8956732954499633e-06
Iter: 1781 loss: 2.8797330279147039e-06
Iter: 1782 loss: 2.8787816453907991e-06
Iter: 1783 loss: 2.8780640515865463e-06
Iter: 1784 loss: 2.8777531461773804e-06
Iter: 1785 loss: 2.8765149232053931e-06
Iter: 1786 loss: 2.8846449153302893e-06
Iter: 1787 loss: 2.8763845465420678e-06
Iter: 1788 loss: 2.8750702532206658e-06
Iter: 1789 loss: 2.8754857106783305e-06
Iter: 1790 loss: 2.8741339716436229e-06
Iter: 1791 loss: 2.8726952357266241e-06
Iter: 1792 loss: 2.8756148227375449e-06
Iter: 1793 loss: 2.872113058120596e-06
Iter: 1794 loss: 2.8701264311273587e-06
Iter: 1795 loss: 2.8785491281185863e-06
Iter: 1796 loss: 2.8697095147123342e-06
Iter: 1797 loss: 2.8686258207214239e-06
Iter: 1798 loss: 2.8777209530605388e-06
Iter: 1799 loss: 2.8685625816041311e-06
Iter: 1800 loss: 2.8675399804494012e-06
Iter: 1801 loss: 2.8666110428357897e-06
Iter: 1802 loss: 2.8663579851589455e-06
Iter: 1803 loss: 2.8652090833571816e-06
Iter: 1804 loss: 2.878297022596044e-06
Iter: 1805 loss: 2.8651892407025897e-06
Iter: 1806 loss: 2.8645322689277637e-06
Iter: 1807 loss: 2.8645150963792577e-06
Iter: 1808 loss: 2.8639112835283145e-06
Iter: 1809 loss: 2.8622741226070728e-06
Iter: 1810 loss: 2.8727689798413829e-06
Iter: 1811 loss: 2.8618607187809365e-06
Iter: 1812 loss: 2.8608677799028995e-06
Iter: 1813 loss: 2.865532403507564e-06
Iter: 1814 loss: 2.8606854825131021e-06
Iter: 1815 loss: 2.8594107067097744e-06
Iter: 1816 loss: 2.8597277260429826e-06
Iter: 1817 loss: 2.8584802133766814e-06
Iter: 1818 loss: 2.85737372044081e-06
Iter: 1819 loss: 2.8643553701352878e-06
Iter: 1820 loss: 2.8572477252121849e-06
Iter: 1821 loss: 2.8560285608109605e-06
Iter: 1822 loss: 2.8558536958453612e-06
Iter: 1823 loss: 2.8549987080084154e-06
Iter: 1824 loss: 2.8537102913065993e-06
Iter: 1825 loss: 2.8611409279706137e-06
Iter: 1826 loss: 2.8535376593013665e-06
Iter: 1827 loss: 2.8523965059182527e-06
Iter: 1828 loss: 2.8561569753191694e-06
Iter: 1829 loss: 2.8520809097249927e-06
Iter: 1830 loss: 2.8511185361959712e-06
Iter: 1831 loss: 2.8537484688829424e-06
Iter: 1832 loss: 2.8508040459180851e-06
Iter: 1833 loss: 2.8494568600820879e-06
Iter: 1834 loss: 2.8508644921383137e-06
Iter: 1835 loss: 2.8487102643829253e-06
Iter: 1836 loss: 2.847514676099928e-06
Iter: 1837 loss: 2.854647835467002e-06
Iter: 1838 loss: 2.8473637297864549e-06
Iter: 1839 loss: 2.8464293624769884e-06
Iter: 1840 loss: 2.8526193088745528e-06
Iter: 1841 loss: 2.8463326913916836e-06
Iter: 1842 loss: 2.8451582483468029e-06
Iter: 1843 loss: 2.8478418620597833e-06
Iter: 1844 loss: 2.8447190256322033e-06
Iter: 1845 loss: 2.8441510787070722e-06
Iter: 1846 loss: 2.8428451497364624e-06
Iter: 1847 loss: 2.85949179117806e-06
Iter: 1848 loss: 2.8427531332267794e-06
Iter: 1849 loss: 2.8410539549789586e-06
Iter: 1850 loss: 2.8488046474891833e-06
Iter: 1851 loss: 2.8407292834902908e-06
Iter: 1852 loss: 2.8397078440439493e-06
Iter: 1853 loss: 2.8438250024474367e-06
Iter: 1854 loss: 2.8394800904187833e-06
Iter: 1855 loss: 2.8382375437344658e-06
Iter: 1856 loss: 2.8388784039803794e-06
Iter: 1857 loss: 2.8374129272022496e-06
Iter: 1858 loss: 2.8363957774109659e-06
Iter: 1859 loss: 2.8427806851433868e-06
Iter: 1860 loss: 2.8362788256997987e-06
Iter: 1861 loss: 2.8351350116023241e-06
Iter: 1862 loss: 2.8343427714657393e-06
Iter: 1863 loss: 2.8339304680838627e-06
Iter: 1864 loss: 2.8326779971304538e-06
Iter: 1865 loss: 2.8428947961170444e-06
Iter: 1866 loss: 2.8325988174329927e-06
Iter: 1867 loss: 2.8312678288314273e-06
Iter: 1868 loss: 2.8325940876348965e-06
Iter: 1869 loss: 2.8305182454566874e-06
Iter: 1870 loss: 2.8294527716720161e-06
Iter: 1871 loss: 2.8441875270761474e-06
Iter: 1872 loss: 2.8294492504474749e-06
Iter: 1873 loss: 2.8286577368815696e-06
Iter: 1874 loss: 2.8277776577081488e-06
Iter: 1875 loss: 2.8276546555191122e-06
Iter: 1876 loss: 2.8269899314956034e-06
Iter: 1877 loss: 2.8267187415839827e-06
Iter: 1878 loss: 2.8262530536006508e-06
Iter: 1879 loss: 2.8253733062826414e-06
Iter: 1880 loss: 2.8445630801509715e-06
Iter: 1881 loss: 2.8253705348030873e-06
Iter: 1882 loss: 2.8244173902886275e-06
Iter: 1883 loss: 2.8235464390612131e-06
Iter: 1884 loss: 2.823313029243053e-06
Iter: 1885 loss: 2.821891290498365e-06
Iter: 1886 loss: 2.82868192886841e-06
Iter: 1887 loss: 2.8216361543873256e-06
Iter: 1888 loss: 2.8201608904850419e-06
Iter: 1889 loss: 2.8240696104690981e-06
Iter: 1890 loss: 2.819666567124243e-06
Iter: 1891 loss: 2.8185797809149374e-06
Iter: 1892 loss: 2.8243964161014993e-06
Iter: 1893 loss: 2.8184151171848019e-06
Iter: 1894 loss: 2.8172123789304112e-06
Iter: 1895 loss: 2.8164717483819488e-06
Iter: 1896 loss: 2.8159858926319322e-06
Iter: 1897 loss: 2.814662573128154e-06
Iter: 1898 loss: 2.8263296496296926e-06
Iter: 1899 loss: 2.8145959669529503e-06
Iter: 1900 loss: 2.81329112260115e-06
Iter: 1901 loss: 2.8130094799192147e-06
Iter: 1902 loss: 2.8121577578748596e-06
Iter: 1903 loss: 2.8112109272891671e-06
Iter: 1904 loss: 2.8112066472788479e-06
Iter: 1905 loss: 2.810238045504005e-06
Iter: 1906 loss: 2.8090933701104127e-06
Iter: 1907 loss: 2.8089697335149663e-06
Iter: 1908 loss: 2.8086505489807291e-06
Iter: 1909 loss: 2.8081860518182283e-06
Iter: 1910 loss: 2.8074986118451034e-06
Iter: 1911 loss: 2.808153829951637e-06
Iter: 1912 loss: 2.8071057994216938e-06
Iter: 1913 loss: 2.8064800770485424e-06
Iter: 1914 loss: 2.8048567244575479e-06
Iter: 1915 loss: 2.81784659537142e-06
Iter: 1916 loss: 2.8045561541850273e-06
Iter: 1917 loss: 2.8032366388993396e-06
Iter: 1918 loss: 2.8032366091333232e-06
Iter: 1919 loss: 2.8022019609852132e-06
Iter: 1920 loss: 2.8009594059952048e-06
Iter: 1921 loss: 2.8008348661518751e-06
Iter: 1922 loss: 2.7997603365988463e-06
Iter: 1923 loss: 2.7997434181641819e-06
Iter: 1924 loss: 2.7987762999093567e-06
Iter: 1925 loss: 2.797969922164294e-06
Iter: 1926 loss: 2.7976943241698946e-06
Iter: 1927 loss: 2.7963767170312032e-06
Iter: 1928 loss: 2.8057283848822722e-06
Iter: 1929 loss: 2.7962590780340658e-06
Iter: 1930 loss: 2.7949754061397497e-06
Iter: 1931 loss: 2.7953402996584605e-06
Iter: 1932 loss: 2.7940505014183563e-06
Iter: 1933 loss: 2.7928924179182996e-06
Iter: 1934 loss: 2.8017086331031852e-06
Iter: 1935 loss: 2.7928047350456615e-06
Iter: 1936 loss: 2.7917398555168258e-06
Iter: 1937 loss: 2.794030707719817e-06
Iter: 1938 loss: 2.7913248728555476e-06
Iter: 1939 loss: 2.7904326966541377e-06
Iter: 1940 loss: 2.80064781236637e-06
Iter: 1941 loss: 2.7904177408474742e-06
Iter: 1942 loss: 2.7898189535519762e-06
Iter: 1943 loss: 2.7977667020980287e-06
Iter: 1944 loss: 2.7898157349103987e-06
Iter: 1945 loss: 2.7893437001490594e-06
Iter: 1946 loss: 2.7880690536275594e-06
Iter: 1947 loss: 2.7964194484340428e-06
Iter: 1948 loss: 2.7877560756811727e-06
Iter: 1949 loss: 2.7866677786160688e-06
Iter: 1950 loss: 2.7923241164250532e-06
Iter: 1951 loss: 2.7864951900298453e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi2_phi3/300_300_300_1'
+ for psi in $PSI
+ MODEL=
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi0
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi0
+ date
Sun Nov  8 03:53:19 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --function f1 --psi 3 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde60a9c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde60a9c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde60a91378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde60ae2d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde60ae20d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde60af4d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde60a6d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde8102c048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde60a82a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c25b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c25bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c2a9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c2a9c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c1cf400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c2a9e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c29c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c29ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c1b3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c1b38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c2e3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c2e3d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c17aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c068950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c07cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c07b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c07bc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c07b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c2408c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde202012f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde201d5400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde20201bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde201d5268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde3c0180d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde20106840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde200bd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fde200c7ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.07658198530548876
test_loss: 0.07701715662678058
train_loss: 0.05940959447362576
test_loss: 0.0606991101968853
train_loss: 0.04421146697749661
test_loss: 0.04567884501759108
train_loss: 0.03895693512018455
test_loss: 0.039777848589326495
train_loss: 0.03144702510187142
test_loss: 0.03349717997876031
train_loss: 0.02563882910950836
test_loss: 0.02637473039813871
train_loss: 0.020376147697261218
test_loss: 0.02093788781864313
train_loss: 0.017654957860554238
test_loss: 0.01830029699092103
train_loss: 0.014817425193136315
test_loss: 0.015539229081898826
train_loss: 0.013499051861295068
test_loss: 0.01325109961067175
train_loss: 0.01246282982429792
test_loss: 0.012508739960847937
train_loss: 0.011813820538214979
test_loss: 0.012880054161656744
train_loss: 0.01076156413299402
test_loss: 0.011332439897667036
train_loss: 0.012922924129806338
test_loss: 0.011741352651008863
train_loss: 0.010157274521398847
test_loss: 0.010443935098183033
train_loss: 0.010283611540935345
test_loss: 0.010578597550702139
train_loss: 0.009792921616464435
test_loss: 0.010848007636305551
train_loss: 0.008813497651859428
test_loss: 0.010040768937711522
train_loss: 0.00942649959272929
test_loss: 0.009895638633434293
train_loss: 0.00857356962936638
test_loss: 0.009875757096408143
train_loss: 0.008689952581520562
test_loss: 0.00895932061760458
train_loss: 0.008301889487206025
test_loss: 0.008812981970462944
train_loss: 0.008180808960642338
test_loss: 0.009354721409387937
train_loss: 0.008362785856862127
test_loss: 0.008395124535529664
train_loss: 0.008189593284129732
test_loss: 0.00839871121679748
train_loss: 0.007807607154816066
test_loss: 0.008679974496339008
train_loss: 0.008297613271580634
test_loss: 0.008192555245227095
train_loss: 0.007705957067776009
test_loss: 0.0077312041086058355
train_loss: 0.007629852336302498
test_loss: 0.008384374041828449
train_loss: 0.0076166741546637605
test_loss: 0.00916244980541649
train_loss: 0.00691627914834912
test_loss: 0.007614188547690403
train_loss: 0.008388063076454278
test_loss: 0.008460855313436112
train_loss: 0.006904953373916393
test_loss: 0.0072377146375868975
train_loss: 0.007284469684658047
test_loss: 0.007837771884083265
train_loss: 0.007147680887896402
test_loss: 0.008240380287667699
train_loss: 0.007053356575369261
test_loss: 0.007639397314037209
train_loss: 0.006993875375582807
test_loss: 0.007583652851655702
train_loss: 0.006664642766112006
test_loss: 0.007038008719021123
train_loss: 0.007273663118361698
test_loss: 0.007591033365289478
train_loss: 0.0070184884539516625
test_loss: 0.006942645356490986
train_loss: 0.007056696451087581
test_loss: 0.007416260300699755
train_loss: 0.007207615719260736
test_loss: 0.007686787706062064
train_loss: 0.007215783216673304
test_loss: 0.007310231105230711
train_loss: 0.006662353247116418
test_loss: 0.006869653237437802
train_loss: 0.006703242601185489
test_loss: 0.006856233623998972
train_loss: 0.00646531798863787
test_loss: 0.006936610741263164
train_loss: 0.007385870848105184
test_loss: 0.0070693745712711335
train_loss: 0.006805852320389959
test_loss: 0.007482808234622201
train_loss: 0.006648396872337737
test_loss: 0.006951164668513924
train_loss: 0.00692315927428664
test_loss: 0.006728134332906906
train_loss: 0.0066581070211357875
test_loss: 0.006790828013576268
train_loss: 0.007563973106868861
test_loss: 0.006989586593126836
train_loss: 0.007051096606789495
test_loss: 0.007681598514015753
train_loss: 0.006279471336403374
test_loss: 0.006885176670097548
train_loss: 0.006642235539687222
test_loss: 0.006690418602051039
train_loss: 0.00725285644668716
test_loss: 0.0069388930345673895
train_loss: 0.006417742156681417
test_loss: 0.006792716940373607
train_loss: 0.0063111072573285645
test_loss: 0.006895588934456713
train_loss: 0.006592136430629199
test_loss: 0.006507951176038244
train_loss: 0.0069379419637132945
test_loss: 0.006750316505826709
train_loss: 0.006101862611220933
test_loss: 0.006577669082354401
train_loss: 0.006214690321179995
test_loss: 0.0069257203025296496
train_loss: 0.005844890464535461
test_loss: 0.006207206630643315
train_loss: 0.006320752366591115
test_loss: 0.006506030435589521
train_loss: 0.006265022911349694
test_loss: 0.006333643849558435
train_loss: 0.00587560928651448
test_loss: 0.00652055547375246
train_loss: 0.005888858201848607
test_loss: 0.0066827712239071885
train_loss: 0.006120135251734502
test_loss: 0.006697896739796008
train_loss: 0.0056595023179451
test_loss: 0.006352161181357881
train_loss: 0.006550277274611679
test_loss: 0.00653802466203176
train_loss: 0.005649228031679851
test_loss: 0.006162316079003175
train_loss: 0.005704373501007982
test_loss: 0.006165563948365062
train_loss: 0.0058308319153538065
test_loss: 0.0064979327167396786
train_loss: 0.005773341147417203
test_loss: 0.005879555326550321
train_loss: 0.005855238715752495
test_loss: 0.00630792061788028
train_loss: 0.006113242694502095
test_loss: 0.0061740456421202445
train_loss: 0.005686397783980226
test_loss: 0.006126201400940538
train_loss: 0.005991449531249344
test_loss: 0.0064377509640793145
train_loss: 0.005948409387551884
test_loss: 0.006158538496670937
train_loss: 0.005734866767788954
test_loss: 0.006241907962631744
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi0/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0/300_300_300_1 --optimizer lbfgs --function f1 --psi 3 --phi 0 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi0/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b75c8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b75c8620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b75acae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b74b58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2d238cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b74b59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b7506620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b7506a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b74d5378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b7657048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b7435950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b74069d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b7406488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b7371400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b7406b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b7391840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b7391488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b736ed90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2b736eae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa298e39a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa298e39b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa298e39ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa298de1a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa298de12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa298deeea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2746488c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa274662950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa274648158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2746091e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa27461b268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa274609840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa274591730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa274591598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa27455d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa27455d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa2744f7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 5.6918011299748314e-05
Iter: 2 loss: 6.0331926715088879e-05
Iter: 3 loss: 4.3781219741966295e-05
Iter: 4 loss: 3.7956508958948839e-05
Iter: 5 loss: 4.4618132097991303e-05
Iter: 6 loss: 3.4831983632375725e-05
Iter: 7 loss: 3.0980159031629138e-05
Iter: 8 loss: 7.3136294876399651e-05
Iter: 9 loss: 3.0890956693780441e-05
Iter: 10 loss: 2.9048653916745212e-05
Iter: 11 loss: 2.6946756858117902e-05
Iter: 12 loss: 2.6682064442211753e-05
Iter: 13 loss: 2.5806708536124757e-05
Iter: 14 loss: 2.5441480342601397e-05
Iter: 15 loss: 2.448356529025638e-05
Iter: 16 loss: 2.2307567743321968e-05
Iter: 17 loss: 5.1577780668010804e-05
Iter: 18 loss: 2.2172191632501419e-05
Iter: 19 loss: 2.0646071630957715e-05
Iter: 20 loss: 2.2878684371203974e-05
Iter: 21 loss: 1.9910321596303638e-05
Iter: 22 loss: 1.9043127933221688e-05
Iter: 23 loss: 1.7324519519848526e-05
Iter: 24 loss: 5.0942845141402609e-05
Iter: 25 loss: 1.7306599159391047e-05
Iter: 26 loss: 1.624823502299889e-05
Iter: 27 loss: 1.6106104835724281e-05
Iter: 28 loss: 1.5338778343135259e-05
Iter: 29 loss: 1.515774972457715e-05
Iter: 30 loss: 1.4667836983237655e-05
Iter: 31 loss: 1.4039733192903887e-05
Iter: 32 loss: 1.4191014007028419e-05
Iter: 33 loss: 1.3579758068419178e-05
Iter: 34 loss: 1.3328021166267593e-05
Iter: 35 loss: 1.3249356874410484e-05
Iter: 36 loss: 1.296212817489708e-05
Iter: 37 loss: 1.2426023585054177e-05
Iter: 38 loss: 2.4410316625905719e-05
Iter: 39 loss: 1.2424936717847531e-05
Iter: 40 loss: 1.191361687993038e-05
Iter: 41 loss: 1.4291909857211938e-05
Iter: 42 loss: 1.1818061039786314e-05
Iter: 43 loss: 1.1654609830439777e-05
Iter: 44 loss: 1.1564656187878573e-05
Iter: 45 loss: 1.1442767844360832e-05
Iter: 46 loss: 1.112912516589926e-05
Iter: 47 loss: 1.3735328964587477e-05
Iter: 48 loss: 1.1074406020208768e-05
Iter: 49 loss: 1.0859519426724467e-05
Iter: 50 loss: 1.0844496715642479e-05
Iter: 51 loss: 1.0615436674056275e-05
Iter: 52 loss: 1.040996506424601e-05
Iter: 53 loss: 1.0351938065777208e-05
Iter: 54 loss: 1.0142683325548105e-05
Iter: 55 loss: 1.1653048586275592e-05
Iter: 56 loss: 1.0124785979179939e-05
Iter: 57 loss: 9.9233892657579463e-06
Iter: 58 loss: 1.0334334426147705e-05
Iter: 59 loss: 9.8420713195586775e-06
Iter: 60 loss: 9.7446559274212379e-06
Iter: 61 loss: 9.7442451862131381e-06
Iter: 62 loss: 9.6530839398434087e-06
Iter: 63 loss: 9.490484563329185e-06
Iter: 64 loss: 1.3524084194686441e-05
Iter: 65 loss: 9.4904830788195567e-06
Iter: 66 loss: 9.3077983253192431e-06
Iter: 67 loss: 9.8563816915650785e-06
Iter: 68 loss: 9.2526625131424441e-06
Iter: 69 loss: 9.1102004825732349e-06
Iter: 70 loss: 9.8420196792659869e-06
Iter: 71 loss: 9.0872310631367481e-06
Iter: 72 loss: 8.9840383450531974e-06
Iter: 73 loss: 8.98283133533139e-06
Iter: 74 loss: 8.90111055946951e-06
Iter: 75 loss: 8.8480681872549289e-06
Iter: 76 loss: 8.8175141347217522e-06
Iter: 77 loss: 8.7581056791457964e-06
Iter: 78 loss: 8.6351964392252288e-06
Iter: 79 loss: 1.0775983885514205e-05
Iter: 80 loss: 8.632559745002227e-06
Iter: 81 loss: 8.5565893864658687e-06
Iter: 82 loss: 8.5565833394334159e-06
Iter: 83 loss: 8.4768349472053569e-06
Iter: 84 loss: 8.5840075382748575e-06
Iter: 85 loss: 8.4367441936275139e-06
Iter: 86 loss: 8.3789618039589152e-06
Iter: 87 loss: 8.3017337807094568e-06
Iter: 88 loss: 8.2974529653528046e-06
Iter: 89 loss: 8.2721132005581189e-06
Iter: 90 loss: 8.2452063505028468e-06
Iter: 91 loss: 8.20782275654288e-06
Iter: 92 loss: 8.1222218005202885e-06
Iter: 93 loss: 9.2289068233531472e-06
Iter: 94 loss: 8.1164205143216976e-06
Iter: 95 loss: 8.0753461264923864e-06
Iter: 96 loss: 8.0724336163168369e-06
Iter: 97 loss: 8.0275071059373348e-06
Iter: 98 loss: 8.0752706315169589e-06
Iter: 99 loss: 8.0027620982015983e-06
Iter: 100 loss: 7.96335357177364e-06
Iter: 101 loss: 7.9569717221572626e-06
Iter: 102 loss: 7.92984744594005e-06
Iter: 103 loss: 7.8664053736787153e-06
Iter: 104 loss: 8.4402906776827368e-06
Iter: 105 loss: 7.8634479539173551e-06
Iter: 106 loss: 7.83550527187314e-06
Iter: 107 loss: 7.80365495280837e-06
Iter: 108 loss: 7.7996301945523526e-06
Iter: 109 loss: 7.7682909561116267e-06
Iter: 110 loss: 7.7677177129049548e-06
Iter: 111 loss: 7.7347931826488858e-06
Iter: 112 loss: 7.68234350915865e-06
Iter: 113 loss: 7.6818708598697109e-06
Iter: 114 loss: 7.64239671150527e-06
Iter: 115 loss: 7.9318571022705e-06
Iter: 116 loss: 7.6391226415288727e-06
Iter: 117 loss: 7.6016597533047434e-06
Iter: 118 loss: 7.8741482722823029e-06
Iter: 119 loss: 7.5984980937038784e-06
Iter: 120 loss: 7.5813441525105091e-06
Iter: 121 loss: 7.55008816443678e-06
Iter: 122 loss: 8.2905815759055541e-06
Iter: 123 loss: 7.5500725289939333e-06
Iter: 124 loss: 7.5317856889228113e-06
Iter: 125 loss: 7.5289479009312328e-06
Iter: 126 loss: 7.5082359043759261e-06
Iter: 127 loss: 7.4642465679774875e-06
Iter: 128 loss: 8.177395152193996e-06
Iter: 129 loss: 7.4629087537627117e-06
Iter: 130 loss: 7.4272888284845144e-06
Iter: 131 loss: 7.6034612292221056e-06
Iter: 132 loss: 7.4212085769674107e-06
Iter: 133 loss: 7.3954356170026651e-06
Iter: 134 loss: 7.3951751357285373e-06
Iter: 135 loss: 7.38264556373204e-06
Iter: 136 loss: 7.3580993356460809e-06
Iter: 137 loss: 7.8497761123219e-06
Iter: 138 loss: 7.3578955905286889e-06
Iter: 139 loss: 7.3335885312861061e-06
Iter: 140 loss: 7.3335559160402105e-06
Iter: 141 loss: 7.3184101104003244e-06
Iter: 142 loss: 7.287449633121483e-06
Iter: 143 loss: 7.8440486245617014e-06
Iter: 144 loss: 7.2868938570762513e-06
Iter: 145 loss: 7.2676024943624492e-06
Iter: 146 loss: 7.26756661242509e-06
Iter: 147 loss: 7.2479936272050376e-06
Iter: 148 loss: 7.3103902864394119e-06
Iter: 149 loss: 7.242405682841254e-06
Iter: 150 loss: 7.2287046067491548e-06
Iter: 151 loss: 7.2004934006412028e-06
Iter: 152 loss: 7.69777951571087e-06
Iter: 153 loss: 7.1999281103747053e-06
Iter: 154 loss: 7.1905152892906161e-06
Iter: 155 loss: 7.1818081024312909e-06
Iter: 156 loss: 7.1709661887498217e-06
Iter: 157 loss: 7.1502932899925571e-06
Iter: 158 loss: 7.5917698506830774e-06
Iter: 159 loss: 7.1502061355023385e-06
Iter: 160 loss: 7.1382070458629205e-06
Iter: 161 loss: 7.1380858651861238e-06
Iter: 162 loss: 7.1243172495439738e-06
Iter: 163 loss: 7.1062117644346632e-06
Iter: 164 loss: 7.1050992712384766e-06
Iter: 165 loss: 7.0863675387351376e-06
Iter: 166 loss: 7.10320056384296e-06
Iter: 167 loss: 7.0754657959815189e-06
Iter: 168 loss: 7.0655342734542473e-06
Iter: 169 loss: 7.0625657630111854e-06
Iter: 170 loss: 7.0530953240938646e-06
Iter: 171 loss: 7.0348503714259736e-06
Iter: 172 loss: 7.4149825642812285e-06
Iter: 173 loss: 7.0347481090918274e-06
Iter: 174 loss: 7.0264002911146015e-06
Iter: 175 loss: 7.025814747528391e-06
Iter: 176 loss: 7.01636800100122e-06
Iter: 177 loss: 6.9963604382467029e-06
Iter: 178 loss: 7.32285192839958e-06
Iter: 179 loss: 6.9957720824723684e-06
Iter: 180 loss: 6.9778778103195613e-06
Iter: 181 loss: 7.0430992825601261e-06
Iter: 182 loss: 6.9734106864160025e-06
Iter: 183 loss: 6.9680905191696788e-06
Iter: 184 loss: 6.9646304113316342e-06
Iter: 185 loss: 6.9597788866677674e-06
Iter: 186 loss: 6.9463618469570168e-06
Iter: 187 loss: 7.0230347796102e-06
Iter: 188 loss: 6.9424799506007686e-06
Iter: 189 loss: 6.935232594596005e-06
Iter: 190 loss: 6.9328965537886358e-06
Iter: 191 loss: 6.9237772692600774e-06
Iter: 192 loss: 6.9095123784752476e-06
Iter: 193 loss: 6.9093394087635823e-06
Iter: 194 loss: 6.89778384499474e-06
Iter: 195 loss: 6.9306173260573828e-06
Iter: 196 loss: 6.8941275369231123e-06
Iter: 197 loss: 6.8839132721976053e-06
Iter: 198 loss: 6.8838890006880815e-06
Iter: 199 loss: 6.878797553892528e-06
Iter: 200 loss: 6.8652647372739775e-06
Iter: 201 loss: 6.9616128407499659e-06
Iter: 202 loss: 6.8622892178229676e-06
Iter: 203 loss: 6.8527752300097688e-06
Iter: 204 loss: 6.8522204032780358e-06
Iter: 205 loss: 6.8417398815518075e-06
Iter: 206 loss: 6.85757706987874e-06
Iter: 207 loss: 6.8367495445594842e-06
Iter: 208 loss: 6.82954078081135e-06
Iter: 209 loss: 6.8275955775119569e-06
Iter: 210 loss: 6.8231470118007624e-06
Iter: 211 loss: 6.8123962467537157e-06
Iter: 212 loss: 6.9334031880925126e-06
Iter: 213 loss: 6.8121968454419539e-06
Iter: 214 loss: 6.8055484309204165e-06
Iter: 215 loss: 6.7901705000161317e-06
Iter: 216 loss: 6.982532717582483e-06
Iter: 217 loss: 6.7890276462781777e-06
Iter: 218 loss: 6.7743477550738708e-06
Iter: 219 loss: 6.8366288073797934e-06
Iter: 220 loss: 6.7712693426740949e-06
Iter: 221 loss: 6.7623213138797413e-06
Iter: 222 loss: 6.8669021644210517e-06
Iter: 223 loss: 6.762188627966832e-06
Iter: 224 loss: 6.7529610234719771e-06
Iter: 225 loss: 6.8030981889578095e-06
Iter: 226 loss: 6.7515957596001231e-06
Iter: 227 loss: 6.74556600927752e-06
Iter: 228 loss: 6.7344466245707359e-06
Iter: 229 loss: 6.9910126842105532e-06
Iter: 230 loss: 6.7344342155426986e-06
Iter: 231 loss: 6.7334861535703133e-06
Iter: 232 loss: 6.7295734572591663e-06
Iter: 233 loss: 6.72525293097445e-06
Iter: 234 loss: 6.715523153333055e-06
Iter: 235 loss: 6.8481965812763014e-06
Iter: 236 loss: 6.7149606027910823e-06
Iter: 237 loss: 6.7065096865945907e-06
Iter: 238 loss: 6.7349022715099358e-06
Iter: 239 loss: 6.7042165254551318e-06
Iter: 240 loss: 6.694651341599009e-06
Iter: 241 loss: 6.79745933876443e-06
Iter: 242 loss: 6.6944273671526909e-06
Iter: 243 loss: 6.6897492063113563e-06
Iter: 244 loss: 6.681724615800596e-06
Iter: 245 loss: 6.6817165219915749e-06
Iter: 246 loss: 6.679063266760437e-06
Iter: 247 loss: 6.6775453792424068e-06
Iter: 248 loss: 6.67375356815491e-06
Iter: 249 loss: 6.6662801443424605e-06
Iter: 250 loss: 6.8135713235231637e-06
Iter: 251 loss: 6.6662095539421091e-06
Iter: 252 loss: 6.6601937864192715e-06
Iter: 253 loss: 6.6601872677785705e-06
Iter: 254 loss: 6.6544316031009423e-06
Iter: 255 loss: 6.645823073301726e-06
Iter: 256 loss: 6.6456378801445976e-06
Iter: 257 loss: 6.6387400234696337e-06
Iter: 258 loss: 6.6549569572749455e-06
Iter: 259 loss: 6.6362107273358771e-06
Iter: 260 loss: 6.63055196821829e-06
Iter: 261 loss: 6.6304038432845071e-06
Iter: 262 loss: 6.6268614977548008e-06
Iter: 263 loss: 6.6191522573085123e-06
Iter: 264 loss: 6.73566351789377e-06
Iter: 265 loss: 6.61883992792167e-06
Iter: 266 loss: 6.613645756089517e-06
Iter: 267 loss: 6.6136244685478862e-06
Iter: 268 loss: 6.6078846236196263e-06
Iter: 269 loss: 6.6104777501972651e-06
Iter: 270 loss: 6.6039884836748909e-06
Iter: 271 loss: 6.6000314271275573e-06
Iter: 272 loss: 6.5949822086436521e-06
Iter: 273 loss: 6.5946118538142774e-06
Iter: 274 loss: 6.5895511843185288e-06
Iter: 275 loss: 6.5890880178609872e-06
Iter: 276 loss: 6.5849539616545335e-06
Iter: 277 loss: 6.5762956694470592e-06
Iter: 278 loss: 6.7220934819479645e-06
Iter: 279 loss: 6.5760755264541257e-06
Iter: 280 loss: 6.5723228571871282e-06
Iter: 281 loss: 6.5718659392770805e-06
Iter: 282 loss: 6.5673908826988028e-06
Iter: 283 loss: 6.5624156930221382e-06
Iter: 284 loss: 6.5617197373012525e-06
Iter: 285 loss: 6.5576442000020337e-06
Iter: 286 loss: 6.5576411085638734e-06
Iter: 287 loss: 6.5537682617527359e-06
Iter: 288 loss: 6.5472762180767978e-06
Iter: 289 loss: 6.5472588088748026e-06
Iter: 290 loss: 6.5413083845692649e-06
Iter: 291 loss: 6.5601979245919159e-06
Iter: 292 loss: 6.5396028023315844e-06
Iter: 293 loss: 6.5350329369503008e-06
Iter: 294 loss: 6.5349583322210393e-06
Iter: 295 loss: 6.5325190259981915e-06
Iter: 296 loss: 6.5264031950024739e-06
Iter: 297 loss: 6.5832245850692708e-06
Iter: 298 loss: 6.5255229764432481e-06
Iter: 299 loss: 6.5211854084754751e-06
Iter: 300 loss: 6.5210038674933049e-06
Iter: 301 loss: 6.5160272233208189e-06
Iter: 302 loss: 6.51383695130074e-06
Iter: 303 loss: 6.5113040741809787e-06
Iter: 304 loss: 6.5068039485064e-06
Iter: 305 loss: 6.5091769746513268e-06
Iter: 306 loss: 6.5038294200942374e-06
Iter: 307 loss: 6.50134787485671e-06
Iter: 308 loss: 6.5005936294074878e-06
Iter: 309 loss: 6.498121567299873e-06
Iter: 310 loss: 6.4915015394226815e-06
Iter: 311 loss: 6.5368434050907736e-06
Iter: 312 loss: 6.4899675508409046e-06
Iter: 313 loss: 6.4874301225478818e-06
Iter: 314 loss: 6.4860578245000348e-06
Iter: 315 loss: 6.4824656242666316e-06
Iter: 316 loss: 6.4766681850407273e-06
Iter: 317 loss: 6.4766272334237951e-06
Iter: 318 loss: 6.474218763336213e-06
Iter: 319 loss: 6.4737507299950139e-06
Iter: 320 loss: 6.4714588352218952e-06
Iter: 321 loss: 6.46702575756035e-06
Iter: 322 loss: 6.558488198105701e-06
Iter: 323 loss: 6.4669982571942535e-06
Iter: 324 loss: 6.4625287345692974e-06
Iter: 325 loss: 6.4790597630726773e-06
Iter: 326 loss: 6.4614306698911869e-06
Iter: 327 loss: 6.4561607059424912e-06
Iter: 328 loss: 6.4935781432830106e-06
Iter: 329 loss: 6.45569043544784e-06
Iter: 330 loss: 6.4530462754355047e-06
Iter: 331 loss: 6.44796424283158e-06
Iter: 332 loss: 6.5544206512841041e-06
Iter: 333 loss: 6.4479374487626219e-06
Iter: 334 loss: 6.446472358339633e-06
Iter: 335 loss: 6.44516208002274e-06
Iter: 336 loss: 6.4427632196903184e-06
Iter: 337 loss: 6.4371549322917846e-06
Iter: 338 loss: 6.5048103885170805e-06
Iter: 339 loss: 6.4366964951342846e-06
Iter: 340 loss: 6.4311736588087279e-06
Iter: 341 loss: 6.4587803740796949e-06
Iter: 342 loss: 6.4302450561970731e-06
Iter: 343 loss: 6.4259889474331462e-06
Iter: 344 loss: 6.4259784710397618e-06
Iter: 345 loss: 6.4241066929015778e-06
Iter: 346 loss: 6.4199658374043752e-06
Iter: 347 loss: 6.4795674765904751e-06
Iter: 348 loss: 6.4197657085134727e-06
Iter: 349 loss: 6.4176397814770028e-06
Iter: 350 loss: 6.4169978157494344e-06
Iter: 351 loss: 6.4149521758354365e-06
Iter: 352 loss: 6.409981888660623e-06
Iter: 353 loss: 6.4622519319199169e-06
Iter: 354 loss: 6.4094214876470651e-06
Iter: 355 loss: 6.4062160659080489e-06
Iter: 356 loss: 6.4057413070462882e-06
Iter: 357 loss: 6.4035853911354643e-06
Iter: 358 loss: 6.3993486913744915e-06
Iter: 359 loss: 6.4834697752954654e-06
Iter: 360 loss: 6.3993110433433577e-06
Iter: 361 loss: 6.3966978309925423e-06
Iter: 362 loss: 6.3965827807096134e-06
Iter: 363 loss: 6.3937023053033814e-06
Iter: 364 loss: 6.389881975408463e-06
Iter: 365 loss: 6.3896595932803521e-06
Iter: 366 loss: 6.3855412680338223e-06
Iter: 367 loss: 6.3864845633278278e-06
Iter: 368 loss: 6.38251360439332e-06
Iter: 369 loss: 6.3799903878467076e-06
Iter: 370 loss: 6.379156717338185e-06
Iter: 371 loss: 6.3775459963961293e-06
Iter: 372 loss: 6.3736667596482545e-06
Iter: 373 loss: 6.4157928053166579e-06
Iter: 374 loss: 6.373259258673657e-06
Iter: 375 loss: 6.3694365396987319e-06
Iter: 376 loss: 6.4059398029802239e-06
Iter: 377 loss: 6.3692897750871936e-06
Iter: 378 loss: 6.3646911754604624e-06
Iter: 379 loss: 6.3715463178666393e-06
Iter: 380 loss: 6.3624871327666326e-06
Iter: 381 loss: 6.3594631961523759e-06
Iter: 382 loss: 6.3609436850854312e-06
Iter: 383 loss: 6.35743769762903e-06
Iter: 384 loss: 6.353873157629626e-06
Iter: 385 loss: 6.4054701548670821e-06
Iter: 386 loss: 6.3538675616497012e-06
Iter: 387 loss: 6.35216071093814e-06
Iter: 388 loss: 6.34915970185503e-06
Iter: 389 loss: 6.3491594074322476e-06
Iter: 390 loss: 6.345298712276173e-06
Iter: 391 loss: 6.3872745464612472e-06
Iter: 392 loss: 6.3452131664299814e-06
Iter: 393 loss: 6.342772453601398e-06
Iter: 394 loss: 6.337989863851775e-06
Iter: 395 loss: 6.4336422396096445e-06
Iter: 396 loss: 6.3379498987771835e-06
Iter: 397 loss: 6.3366995763616754e-06
Iter: 398 loss: 6.3353340278472464e-06
Iter: 399 loss: 6.3335360973980483e-06
Iter: 400 loss: 6.3293867745233474e-06
Iter: 401 loss: 6.3816599668295443e-06
Iter: 402 loss: 6.3290845334973293e-06
Iter: 403 loss: 6.32536164479726e-06
Iter: 404 loss: 6.3467757105998223e-06
Iter: 405 loss: 6.3248604878083326e-06
Iter: 406 loss: 6.3201373206438142e-06
Iter: 407 loss: 6.3370865694084534e-06
Iter: 408 loss: 6.3189389082759251e-06
Iter: 409 loss: 6.3161333410011317e-06
Iter: 410 loss: 6.3122925387035053e-06
Iter: 411 loss: 6.3121121026461787e-06
Iter: 412 loss: 6.3112025311837289e-06
Iter: 413 loss: 6.3099825702701542e-06
Iter: 414 loss: 6.3080364204102621e-06
Iter: 415 loss: 6.3041613394376308e-06
Iter: 416 loss: 6.3786232261402055e-06
Iter: 417 loss: 6.3041166890137789e-06
Iter: 418 loss: 6.3011671500285462e-06
Iter: 419 loss: 6.331131946789045e-06
Iter: 420 loss: 6.3010783363708182e-06
Iter: 421 loss: 6.2977054716566105e-06
Iter: 422 loss: 6.299578601202526e-06
Iter: 423 loss: 6.2954977527850661e-06
Iter: 424 loss: 6.2930230796707138e-06
Iter: 425 loss: 6.3031676597109645e-06
Iter: 426 loss: 6.2924818953727677e-06
Iter: 427 loss: 6.2894869843102664e-06
Iter: 428 loss: 6.2944951463624823e-06
Iter: 429 loss: 6.2881319891609726e-06
Iter: 430 loss: 6.285582952922997e-06
Iter: 431 loss: 6.2834235346609117e-06
Iter: 432 loss: 6.2827145899863117e-06
Iter: 433 loss: 6.2783459113751728e-06
Iter: 434 loss: 6.3381324449279473e-06
Iter: 435 loss: 6.2783293295396647e-06
Iter: 436 loss: 6.2762312105572821e-06
Iter: 437 loss: 6.2719435382356338e-06
Iter: 438 loss: 6.34903114853054e-06
Iter: 439 loss: 6.27186683783344e-06
Iter: 440 loss: 6.2698923183628293e-06
Iter: 441 loss: 6.2695697286495637e-06
Iter: 442 loss: 6.2670171441952816e-06
Iter: 443 loss: 6.2640241005790574e-06
Iter: 444 loss: 6.2636892614849541e-06
Iter: 445 loss: 6.2602303476311224e-06
Iter: 446 loss: 6.262380060757026e-06
Iter: 447 loss: 6.2580178895106728e-06
Iter: 448 loss: 6.25532347338108e-06
Iter: 449 loss: 6.2549563154501958e-06
Iter: 450 loss: 6.2535051719034773e-06
Iter: 451 loss: 6.2502395060669222e-06
Iter: 452 loss: 6.2948487588169238e-06
Iter: 453 loss: 6.2500519271886142e-06
Iter: 454 loss: 6.2486019705523145e-06
Iter: 455 loss: 6.2481390937474352e-06
Iter: 456 loss: 6.2464245973553466e-06
Iter: 457 loss: 6.2422614464969615e-06
Iter: 458 loss: 6.2861282980440828e-06
Iter: 459 loss: 6.2417942527929448e-06
Iter: 460 loss: 6.2386889223003548e-06
Iter: 461 loss: 6.2385620469186221e-06
Iter: 462 loss: 6.2359549558000974e-06
Iter: 463 loss: 6.2326504020001244e-06
Iter: 464 loss: 6.2323989028486578e-06
Iter: 465 loss: 6.2298592325120574e-06
Iter: 466 loss: 6.2670743535081928e-06
Iter: 467 loss: 6.2298561352558413e-06
Iter: 468 loss: 6.2270730954245426e-06
Iter: 469 loss: 6.2255412945477407e-06
Iter: 470 loss: 6.2243106965201364e-06
Iter: 471 loss: 6.2212060998379079e-06
Iter: 472 loss: 6.2184248426617315e-06
Iter: 473 loss: 6.2176372320191776e-06
Iter: 474 loss: 6.2170985416281434e-06
Iter: 475 loss: 6.2153101427013765e-06
Iter: 476 loss: 6.2137040919368176e-06
Iter: 477 loss: 6.2104733204136772e-06
Iter: 478 loss: 6.2709550716594032e-06
Iter: 479 loss: 6.2104285911324115e-06
Iter: 480 loss: 6.2076174907602809e-06
Iter: 481 loss: 6.2188764053642205e-06
Iter: 482 loss: 6.2069859358297386e-06
Iter: 483 loss: 6.2032496845347365e-06
Iter: 484 loss: 6.220677428250213e-06
Iter: 485 loss: 6.2025570175309092e-06
Iter: 486 loss: 6.2003758642039432e-06
Iter: 487 loss: 6.1966209930932595e-06
Iter: 488 loss: 6.1966179286635028e-06
Iter: 489 loss: 6.1946603266404251e-06
Iter: 490 loss: 6.1939431214584771e-06
Iter: 491 loss: 6.1923062731490534e-06
Iter: 492 loss: 6.1887177274215515e-06
Iter: 493 loss: 6.2417993810988279e-06
Iter: 494 loss: 6.1885601823935138e-06
Iter: 495 loss: 6.18564870753864e-06
Iter: 496 loss: 6.1855767087841309e-06
Iter: 497 loss: 6.1834314022722179e-06
Iter: 498 loss: 6.1793537866643351e-06
Iter: 499 loss: 6.2670861933536683e-06
Iter: 500 loss: 6.1793381654618309e-06
Iter: 501 loss: 6.17731805460825e-06
Iter: 502 loss: 6.176820050416406e-06
Iter: 503 loss: 6.1749433588460249e-06
Iter: 504 loss: 6.1715144402876422e-06
Iter: 505 loss: 6.2522275385015509e-06
Iter: 506 loss: 6.1715123103531019e-06
Iter: 507 loss: 6.1684040533900674e-06
Iter: 508 loss: 6.1740288038141656e-06
Iter: 509 loss: 6.1670572634815495e-06
Iter: 510 loss: 6.1629811206632347e-06
Iter: 511 loss: 6.2068673606082256e-06
Iter: 512 loss: 6.1628866066893195e-06
Iter: 513 loss: 6.1604892406107096e-06
Iter: 514 loss: 6.1552655933251634e-06
Iter: 515 loss: 6.233925310332116e-06
Iter: 516 loss: 6.1550511146942435e-06
Iter: 517 loss: 6.1554780765968629e-06
Iter: 518 loss: 6.1531575375292205e-06
Iter: 519 loss: 6.1514011178308688e-06
Iter: 520 loss: 6.1478638253281749e-06
Iter: 521 loss: 6.2138942241530022e-06
Iter: 522 loss: 6.1478138879778147e-06
Iter: 523 loss: 6.14451462829453e-06
Iter: 524 loss: 6.1561261250784226e-06
Iter: 525 loss: 6.143660071758777e-06
Iter: 526 loss: 6.1395692134876357e-06
Iter: 527 loss: 6.162664241991954e-06
Iter: 528 loss: 6.139001039118118e-06
Iter: 529 loss: 6.1369865138647338e-06
Iter: 530 loss: 6.1352884522078657e-06
Iter: 531 loss: 6.134723797318036e-06
Iter: 532 loss: 6.1321491882662176e-06
Iter: 533 loss: 6.132131400666762e-06
Iter: 534 loss: 6.1306440410856386e-06
Iter: 535 loss: 6.1273372399626426e-06
Iter: 536 loss: 6.1742273069371291e-06
Iter: 537 loss: 6.12716902578638e-06
Iter: 538 loss: 6.1231768174060254e-06
Iter: 539 loss: 6.1800799009401785e-06
Iter: 540 loss: 6.1231684555081407e-06
Iter: 541 loss: 6.1210782918779309e-06
Iter: 542 loss: 6.1172248509167546e-06
Iter: 543 loss: 6.2061527231979826e-06
Iter: 544 loss: 6.1172206032936442e-06
Iter: 545 loss: 6.1140992241962738e-06
Iter: 546 loss: 6.1500462289377628e-06
Iter: 547 loss: 6.1140487122574844e-06
Iter: 548 loss: 6.1108469974623226e-06
Iter: 549 loss: 6.122059033827487e-06
Iter: 550 loss: 6.1100136227938e-06
Iter: 551 loss: 6.1081948935081734e-06
Iter: 552 loss: 6.1048698919423207e-06
Iter: 553 loss: 6.1830386197661052e-06
Iter: 554 loss: 6.1048677354358367e-06
Iter: 555 loss: 6.1025963153532493e-06
Iter: 556 loss: 6.1020929973533249e-06
Iter: 557 loss: 6.1004573951601934e-06
Iter: 558 loss: 6.0967738134608812e-06
Iter: 559 loss: 6.1469743373653839e-06
Iter: 560 loss: 6.0965606502631117e-06
Iter: 561 loss: 6.094405649621641e-06
Iter: 562 loss: 6.09433038003924e-06
Iter: 563 loss: 6.0917318746080526e-06
Iter: 564 loss: 6.0879931984029023e-06
Iter: 565 loss: 6.087875133974e-06
Iter: 566 loss: 6.0846386802782417e-06
Iter: 567 loss: 6.10534184251818e-06
Iter: 568 loss: 6.0842795756652e-06
Iter: 569 loss: 6.0806813337293953e-06
Iter: 570 loss: 6.0900615102134589e-06
Iter: 571 loss: 6.07946032116269e-06
Iter: 572 loss: 6.0773765832262432e-06
Iter: 573 loss: 6.0829218974000511e-06
Iter: 574 loss: 6.0766808570749854e-06
Iter: 575 loss: 6.0737167340649318e-06
Iter: 576 loss: 6.0779181685704383e-06
Iter: 577 loss: 6.0722621531429e-06
Iter: 578 loss: 6.06983744940919e-06
Iter: 579 loss: 6.0658442503709308e-06
Iter: 580 loss: 6.0658263746111993e-06
Iter: 581 loss: 6.0652578250705824e-06
Iter: 582 loss: 6.0637045676863934e-06
Iter: 583 loss: 6.0618498711365971e-06
Iter: 584 loss: 6.0585492379019481e-06
Iter: 585 loss: 6.14088728154034e-06
Iter: 586 loss: 6.0585492351020214e-06
Iter: 587 loss: 6.0557531859109149e-06
Iter: 588 loss: 6.0639880706730109e-06
Iter: 589 loss: 6.0548955070328955e-06
Iter: 590 loss: 6.0512416147787809e-06
Iter: 591 loss: 6.0758153509308692e-06
Iter: 592 loss: 6.0508753381847447e-06
Iter: 593 loss: 6.0486586418641172e-06
Iter: 594 loss: 6.0440228257009852e-06
Iter: 595 loss: 6.1223719631793754e-06
Iter: 596 loss: 6.0439072016028588e-06
Iter: 597 loss: 6.0433010047296351e-06
Iter: 598 loss: 6.0417245650117866e-06
Iter: 599 loss: 6.0397975136344547e-06
Iter: 600 loss: 6.0354946552514831e-06
Iter: 601 loss: 6.0957066756059315e-06
Iter: 602 loss: 6.0352659721703708e-06
Iter: 603 loss: 6.0325088160702157e-06
Iter: 604 loss: 6.032508181716341e-06
Iter: 605 loss: 6.0294741640880147e-06
Iter: 606 loss: 6.0287627073953324e-06
Iter: 607 loss: 6.0268194945206028e-06
Iter: 608 loss: 6.0241501083266868e-06
Iter: 609 loss: 6.0552498860090827e-06
Iter: 610 loss: 6.0241097444702289e-06
Iter: 611 loss: 6.0217041155225779e-06
Iter: 612 loss: 6.020516310752495e-06
Iter: 613 loss: 6.0193709576249146e-06
Iter: 614 loss: 6.0168667256898349e-06
Iter: 615 loss: 6.0153815069333258e-06
Iter: 616 loss: 6.0143368907134821e-06
Iter: 617 loss: 6.0108900631074135e-06
Iter: 618 loss: 6.0108250789534955e-06
Iter: 619 loss: 6.008831265389848e-06
Iter: 620 loss: 6.004504167233047e-06
Iter: 621 loss: 6.0704456336071128e-06
Iter: 622 loss: 6.0043342621039634e-06
Iter: 623 loss: 6.0026155769067849e-06
Iter: 624 loss: 6.0021623865855569e-06
Iter: 625 loss: 5.9998008592031841e-06
Iter: 626 loss: 5.9978884634019637e-06
Iter: 627 loss: 5.9971861549527963e-06
Iter: 628 loss: 5.9944505154008424e-06
Iter: 629 loss: 5.9918698568527658e-06
Iter: 630 loss: 5.9912387584136228e-06
Iter: 631 loss: 5.9894871167832559e-06
Iter: 632 loss: 5.98855636314736e-06
Iter: 633 loss: 5.9868911730452649e-06
Iter: 634 loss: 5.9826707094646178e-06
Iter: 635 loss: 6.0201322101300966e-06
Iter: 636 loss: 5.9820118393518227e-06
Iter: 637 loss: 5.9789359218340215e-06
Iter: 638 loss: 6.0201770331849848e-06
Iter: 639 loss: 5.97892105182317e-06
Iter: 640 loss: 5.9754591488020981e-06
Iter: 641 loss: 5.9816357166221305e-06
Iter: 642 loss: 5.9739472250047038e-06
Iter: 643 loss: 5.9712077261569416e-06
Iter: 644 loss: 5.9792320177227e-06
Iter: 645 loss: 5.9703634232131351e-06
Iter: 646 loss: 5.966688179899507e-06
Iter: 647 loss: 5.9706969086976414e-06
Iter: 648 loss: 5.9646818159503875e-06
Iter: 649 loss: 5.9621912628717156e-06
Iter: 650 loss: 5.9605927103051895e-06
Iter: 651 loss: 5.9596237827529866e-06
Iter: 652 loss: 5.9583681321280049e-06
Iter: 653 loss: 5.9575548042827772e-06
Iter: 654 loss: 5.9561407415528583e-06
Iter: 655 loss: 5.952359920614615e-06
Iter: 656 loss: 5.9784665673851171e-06
Iter: 657 loss: 5.95149336271624e-06
Iter: 658 loss: 5.9486002032501662e-06
Iter: 659 loss: 5.9485333804089114e-06
Iter: 660 loss: 5.9454036207196323e-06
Iter: 661 loss: 5.9488180960459339e-06
Iter: 662 loss: 5.9436954915364313e-06
Iter: 663 loss: 5.9413333466302906e-06
Iter: 664 loss: 5.937617410818266e-06
Iter: 665 loss: 5.9375761452545189e-06
Iter: 666 loss: 5.9347174174757726e-06
Iter: 667 loss: 5.9342481785418376e-06
Iter: 668 loss: 5.9318797482693977e-06
Iter: 669 loss: 5.9260144762964311e-06
Iter: 670 loss: 5.9832934952944e-06
Iter: 671 loss: 5.92524619840132e-06
Iter: 672 loss: 5.9211467305385671e-06
Iter: 673 loss: 5.9570074872592112e-06
Iter: 674 loss: 5.9209354457879567e-06
Iter: 675 loss: 5.9181825153174245e-06
Iter: 676 loss: 5.9181718695123523e-06
Iter: 677 loss: 5.9166165170462479e-06
Iter: 678 loss: 5.9143727913091e-06
Iter: 679 loss: 5.9143035807167747e-06
Iter: 680 loss: 5.910788771795947e-06
Iter: 681 loss: 5.9347471146911261e-06
Iter: 682 loss: 5.9104462224591447e-06
Iter: 683 loss: 5.9081055778197886e-06
Iter: 684 loss: 5.9031899030584876e-06
Iter: 685 loss: 5.9853901031718872e-06
Iter: 686 loss: 5.90306037008471e-06
Iter: 687 loss: 5.90238799481101e-06
Iter: 688 loss: 5.9008530815028107e-06
Iter: 689 loss: 5.8986276211222267e-06
Iter: 690 loss: 5.8940272574124372e-06
Iter: 691 loss: 5.974282501082305e-06
Iter: 692 loss: 5.8939296703435793e-06
Iter: 693 loss: 5.8897110103059877e-06
Iter: 694 loss: 5.9095565076726763e-06
Iter: 695 loss: 5.8889379555962225e-06
Iter: 696 loss: 5.8847757295132642e-06
Iter: 697 loss: 5.9295003086764553e-06
Iter: 698 loss: 5.8846782981946274e-06
Iter: 699 loss: 5.8828144200840553e-06
Iter: 700 loss: 5.8784438662265856e-06
Iter: 701 loss: 5.9306415793497219e-06
Iter: 702 loss: 5.8780772375029e-06
Iter: 703 loss: 5.877560987094115e-06
Iter: 704 loss: 5.87613361986793e-06
Iter: 705 loss: 5.8741669526004955e-06
Iter: 706 loss: 5.8696681685460335e-06
Iter: 707 loss: 5.9280301821349368e-06
Iter: 708 loss: 5.8693661382854423e-06
Iter: 709 loss: 5.8645538197754324e-06
Iter: 710 loss: 5.8688763364245596e-06
Iter: 711 loss: 5.861752815045074e-06
Iter: 712 loss: 5.85840239129786e-06
Iter: 713 loss: 5.9083464149412694e-06
Iter: 714 loss: 5.8583997388848108e-06
Iter: 715 loss: 5.8545605734096046e-06
Iter: 716 loss: 5.8628946668758763e-06
Iter: 717 loss: 5.85307279209721e-06
Iter: 718 loss: 5.8502127236202839e-06
Iter: 719 loss: 5.8543010330006292e-06
Iter: 720 loss: 5.8488144583137913e-06
Iter: 721 loss: 5.8443131272928946e-06
Iter: 722 loss: 5.8533445235144075e-06
Iter: 723 loss: 5.84247908493677e-06
Iter: 724 loss: 5.8394183481176762e-06
Iter: 725 loss: 5.8359495897260594e-06
Iter: 726 loss: 5.8355004895208667e-06
Iter: 727 loss: 5.832949023043768e-06
Iter: 728 loss: 5.8327084187825347e-06
Iter: 729 loss: 5.8296402467548187e-06
Iter: 730 loss: 5.82868512947936e-06
Iter: 731 loss: 5.8268739739067526e-06
Iter: 732 loss: 5.8237869506378314e-06
Iter: 733 loss: 5.8220561243445309e-06
Iter: 734 loss: 5.8207100136132712e-06
Iter: 735 loss: 5.8205153114882168e-06
Iter: 736 loss: 5.8186181599750535e-06
Iter: 737 loss: 5.817327337771562e-06
Iter: 738 loss: 5.8139286172197935e-06
Iter: 739 loss: 5.839285889827741e-06
Iter: 740 loss: 5.8132293768634429e-06
Iter: 741 loss: 5.81024564068827e-06
Iter: 742 loss: 5.8102235930101688e-06
Iter: 743 loss: 5.806819114810019e-06
Iter: 744 loss: 5.8020396989278478e-06
Iter: 745 loss: 5.80185395279617e-06
Iter: 746 loss: 5.7979123900017265e-06
Iter: 747 loss: 5.7980016294865849e-06
Iter: 748 loss: 5.7947856006051214e-06
Iter: 749 loss: 5.7914581571754418e-06
Iter: 750 loss: 5.7914538082170773e-06
Iter: 751 loss: 5.7875980661810353e-06
Iter: 752 loss: 5.7927401561040724e-06
Iter: 753 loss: 5.78565373779531e-06
Iter: 754 loss: 5.7828132156621935e-06
Iter: 755 loss: 5.788065867320256e-06
Iter: 756 loss: 5.7815972957971011e-06
Iter: 757 loss: 5.7778741725031377e-06
Iter: 758 loss: 5.796912553296567e-06
Iter: 759 loss: 5.7772692302419805e-06
Iter: 760 loss: 5.77512948599692e-06
Iter: 761 loss: 5.7699822382460292e-06
Iter: 762 loss: 5.8261646728390886e-06
Iter: 763 loss: 5.7694469761839573e-06
Iter: 764 loss: 5.7662289591955694e-06
Iter: 765 loss: 5.7658051429646165e-06
Iter: 766 loss: 5.7618066698550094e-06
Iter: 767 loss: 5.76229657484614e-06
Iter: 768 loss: 5.7587501275505848e-06
Iter: 769 loss: 5.7558197392178591e-06
Iter: 770 loss: 5.755219719018132e-06
Iter: 771 loss: 5.7532854163469914e-06
Iter: 772 loss: 5.7517096710959277e-06
Iter: 773 loss: 5.7509509047151383e-06
Iter: 774 loss: 5.7492081133572277e-06
Iter: 775 loss: 5.7447418342743779e-06
Iter: 776 loss: 5.7825354814642227e-06
Iter: 777 loss: 5.743985480791982e-06
Iter: 778 loss: 5.7421159882211845e-06
Iter: 779 loss: 5.7414758171580952e-06
Iter: 780 loss: 5.7389305235449108e-06
Iter: 781 loss: 5.73528896781718e-06
Iter: 782 loss: 5.7351680904647431e-06
Iter: 783 loss: 5.732071521112697e-06
Iter: 784 loss: 5.73060809455531e-06
Iter: 785 loss: 5.7290939183029821e-06
Iter: 786 loss: 5.7244265268214895e-06
Iter: 787 loss: 5.7486730907028206e-06
Iter: 788 loss: 5.7236861321908831e-06
Iter: 789 loss: 5.7184950244651576e-06
Iter: 790 loss: 5.7594867240975191e-06
Iter: 791 loss: 5.7181356206406182e-06
Iter: 792 loss: 5.7157711140023332e-06
Iter: 793 loss: 5.7131499167573692e-06
Iter: 794 loss: 5.7127788885944819e-06
Iter: 795 loss: 5.709654148939932e-06
Iter: 796 loss: 5.7096013260465549e-06
Iter: 797 loss: 5.7079787945555877e-06
Iter: 798 loss: 5.7037042302508586e-06
Iter: 799 loss: 5.7355450640696053e-06
Iter: 800 loss: 5.7028216923208749e-06
Iter: 801 loss: 5.6981997085321952e-06
Iter: 802 loss: 5.7435482759461358e-06
Iter: 803 loss: 5.6980396123696829e-06
Iter: 804 loss: 5.6945764199933043e-06
Iter: 805 loss: 5.7444907974046854e-06
Iter: 806 loss: 5.6945705091771791e-06
Iter: 807 loss: 5.6930044754172448e-06
Iter: 808 loss: 5.6888275239881186e-06
Iter: 809 loss: 5.7180287005843406e-06
Iter: 810 loss: 5.6878862603958176e-06
Iter: 811 loss: 5.6845886020677522e-06
Iter: 812 loss: 5.6845273803986393e-06
Iter: 813 loss: 5.6806587752971567e-06
Iter: 814 loss: 5.6851827561284991e-06
Iter: 815 loss: 5.67860143565859e-06
Iter: 816 loss: 5.6759460824650967e-06
Iter: 817 loss: 5.6772270212318474e-06
Iter: 818 loss: 5.6741630510434991e-06
Iter: 819 loss: 5.6712648019992455e-06
Iter: 820 loss: 5.67125289076164e-06
Iter: 821 loss: 5.669791938644734e-06
Iter: 822 loss: 5.6656451661115417e-06
Iter: 823 loss: 5.685642137498972e-06
Iter: 824 loss: 5.66421269527679e-06
Iter: 825 loss: 5.6580043679634163e-06
Iter: 826 loss: 5.6676668299391874e-06
Iter: 827 loss: 5.6550909535818811e-06
Iter: 828 loss: 5.6505619634831924e-06
Iter: 829 loss: 5.7202841316223907e-06
Iter: 830 loss: 5.6505608135627168e-06
Iter: 831 loss: 5.6473703129460556e-06
Iter: 832 loss: 5.687258691169426e-06
Iter: 833 loss: 5.64734077244971e-06
Iter: 834 loss: 5.6453339110243825e-06
Iter: 835 loss: 5.6404027148964443e-06
Iter: 836 loss: 5.6901129312698329e-06
Iter: 837 loss: 5.6397951995664421e-06
Iter: 838 loss: 5.638227652282973e-06
Iter: 839 loss: 5.6367840702214788e-06
Iter: 840 loss: 5.6342430854600851e-06
Iter: 841 loss: 5.6300296256711691e-06
Iter: 842 loss: 5.6300139174947916e-06
Iter: 843 loss: 5.6262306344988156e-06
Iter: 844 loss: 5.62995456080324e-06
Iter: 845 loss: 5.6240914325946878e-06
Iter: 846 loss: 5.620686296905167e-06
Iter: 847 loss: 5.6205032227365357e-06
Iter: 848 loss: 5.6186334319793943e-06
Iter: 849 loss: 5.6138822862149652e-06
Iter: 850 loss: 5.6556124082562764e-06
Iter: 851 loss: 5.6131265498311284e-06
Iter: 852 loss: 5.6118271978770684e-06
Iter: 853 loss: 5.6107523942593264e-06
Iter: 854 loss: 5.6083402305296537e-06
Iter: 855 loss: 5.6082456109416559e-06
Iter: 856 loss: 5.6063816233705474e-06
Iter: 857 loss: 5.6042373212807048e-06
Iter: 858 loss: 5.6110614225299872e-06
Iter: 859 loss: 5.6036240849318029e-06
Iter: 860 loss: 5.6001836231379086e-06
Iter: 861 loss: 5.5985752964095364e-06
Iter: 862 loss: 5.5968825486987128e-06
Iter: 863 loss: 5.5938122066756428e-06
Iter: 864 loss: 5.5988662432869963e-06
Iter: 865 loss: 5.5924107471988538e-06
Iter: 866 loss: 5.5890406789452927e-06
Iter: 867 loss: 5.6277307743537163e-06
Iter: 868 loss: 5.5889852356074633e-06
Iter: 869 loss: 5.5873169103890655e-06
Iter: 870 loss: 5.5845881182599224e-06
Iter: 871 loss: 5.5845736881421992e-06
Iter: 872 loss: 5.58288202560184e-06
Iter: 873 loss: 5.5825933457557292e-06
Iter: 874 loss: 5.58068090269745e-06
Iter: 875 loss: 5.5757181785164778e-06
Iter: 876 loss: 5.6153809162559025e-06
Iter: 877 loss: 5.574797713376524e-06
Iter: 878 loss: 5.5698994778323119e-06
Iter: 879 loss: 5.594217332565228e-06
Iter: 880 loss: 5.5690674971826761e-06
Iter: 881 loss: 5.5674345688749006e-06
Iter: 882 loss: 5.5667904673712105e-06
Iter: 883 loss: 5.5651248962176781e-06
Iter: 884 loss: 5.5607649369497866e-06
Iter: 885 loss: 5.5942304989477164e-06
Iter: 886 loss: 5.55990439527592e-06
Iter: 887 loss: 5.5573580933734567e-06
Iter: 888 loss: 5.5570632764052704e-06
Iter: 889 loss: 5.5540343478599111e-06
Iter: 890 loss: 5.5532784404827448e-06
Iter: 891 loss: 5.5513685590535984e-06
Iter: 892 loss: 5.54880486415559e-06
Iter: 893 loss: 5.5581460685968711e-06
Iter: 894 loss: 5.5481647020027111e-06
Iter: 895 loss: 5.5449400141921483e-06
Iter: 896 loss: 5.5561405436076092e-06
Iter: 897 loss: 5.5440938761760586e-06
Iter: 898 loss: 5.5420743866586e-06
Iter: 899 loss: 5.5369639666593843e-06
Iter: 900 loss: 5.5827480227460038e-06
Iter: 901 loss: 5.5361764716013853e-06
Iter: 902 loss: 5.5328425365843544e-06
Iter: 903 loss: 5.5320733865345851e-06
Iter: 904 loss: 5.5290497074460856e-06
Iter: 905 loss: 5.5257539180140375e-06
Iter: 906 loss: 5.5252560413981821e-06
Iter: 907 loss: 5.5227435969360077e-06
Iter: 908 loss: 5.5227326368154775e-06
Iter: 909 loss: 5.5199058266501774e-06
Iter: 910 loss: 5.5165156912344688e-06
Iter: 911 loss: 5.5161737617261239e-06
Iter: 912 loss: 5.5126161440339021e-06
Iter: 913 loss: 5.5085968299786808e-06
Iter: 914 loss: 5.508069762510882e-06
Iter: 915 loss: 5.50863433716323e-06
Iter: 916 loss: 5.5054314725347316e-06
Iter: 917 loss: 5.5034624456081518e-06
Iter: 918 loss: 5.5001381099797073e-06
Iter: 919 loss: 5.5001312051154672e-06
Iter: 920 loss: 5.49701901086462e-06
Iter: 921 loss: 5.5154870984020547e-06
Iter: 922 loss: 5.4966221696031716e-06
Iter: 923 loss: 5.49260643050594e-06
Iter: 924 loss: 5.4990050425439447e-06
Iter: 925 loss: 5.4907430340349925e-06
Iter: 926 loss: 5.4882770168975416e-06
Iter: 927 loss: 5.4954802625291465e-06
Iter: 928 loss: 5.4875150773573767e-06
Iter: 929 loss: 5.4842539423404532e-06
Iter: 930 loss: 5.49345535188645e-06
Iter: 931 loss: 5.4832161852673778e-06
Iter: 932 loss: 5.4812191406064524e-06
Iter: 933 loss: 5.4807199498827881e-06
Iter: 934 loss: 5.4794612562382025e-06
Iter: 935 loss: 5.4755838538542466e-06
Iter: 936 loss: 5.4894018809149323e-06
Iter: 937 loss: 5.4745928375823972e-06
Iter: 938 loss: 5.4713354227624961e-06
Iter: 939 loss: 5.468725703387734e-06
Iter: 940 loss: 5.46774230607945e-06
Iter: 941 loss: 5.4653344888889989e-06
Iter: 942 loss: 5.46473576732994e-06
Iter: 943 loss: 5.4628844579923064e-06
Iter: 944 loss: 5.4587036762209089e-06
Iter: 945 loss: 5.5151684486163961e-06
Iter: 946 loss: 5.4584551327849444e-06
Iter: 947 loss: 5.4547436384256778e-06
Iter: 948 loss: 5.4671590966555321e-06
Iter: 949 loss: 5.4537320325496652e-06
Iter: 950 loss: 5.4501737791458275e-06
Iter: 951 loss: 5.4501737462481817e-06
Iter: 952 loss: 5.4480474258902949e-06
Iter: 953 loss: 5.443470255493361e-06
Iter: 954 loss: 5.5148715938674753e-06
Iter: 955 loss: 5.4433065345723777e-06
Iter: 956 loss: 5.4432414635821344e-06
Iter: 957 loss: 5.4414152231977483e-06
Iter: 958 loss: 5.4397754927849535e-06
Iter: 959 loss: 5.435819098910226e-06
Iter: 960 loss: 5.4785131760456818e-06
Iter: 961 loss: 5.435397331990795e-06
Iter: 962 loss: 5.4327331354571209e-06
Iter: 963 loss: 5.432561846608385e-06
Iter: 964 loss: 5.4300476401630095e-06
Iter: 965 loss: 5.4258860541296981e-06
Iter: 966 loss: 5.4258697738490171e-06
Iter: 967 loss: 5.4228076147070665e-06
Iter: 968 loss: 5.462372046192689e-06
Iter: 969 loss: 5.4227861157382135e-06
Iter: 970 loss: 5.4198811630389829e-06
Iter: 971 loss: 5.420565831770238e-06
Iter: 972 loss: 5.4177507782834e-06
Iter: 973 loss: 5.4152454969085683e-06
Iter: 974 loss: 5.4220671245906707e-06
Iter: 975 loss: 5.4144244469862413e-06
Iter: 976 loss: 5.4104859958373516e-06
Iter: 977 loss: 5.4157590836241995e-06
Iter: 978 loss: 5.4085036458072473e-06
Iter: 979 loss: 5.4052792257050974e-06
Iter: 980 loss: 5.40214763908361e-06
Iter: 981 loss: 5.4014463911660777e-06
Iter: 982 loss: 5.3984584446703026e-06
Iter: 983 loss: 5.3984516882252745e-06
Iter: 984 loss: 5.3951975294110018e-06
Iter: 985 loss: 5.4002923271481024e-06
Iter: 986 loss: 5.3936748031830175e-06
Iter: 987 loss: 5.390987084762526e-06
Iter: 988 loss: 5.3883304814479276e-06
Iter: 989 loss: 5.3877674832088091e-06
Iter: 990 loss: 5.3855222748890319e-06
Iter: 991 loss: 5.3849231293728455e-06
Iter: 992 loss: 5.3833758165024493e-06
Iter: 993 loss: 5.3798076849476842e-06
Iter: 994 loss: 5.4248712753137491e-06
Iter: 995 loss: 5.3795496100333359e-06
Iter: 996 loss: 5.3772940273745874e-06
Iter: 997 loss: 5.376910334253382e-06
Iter: 998 loss: 5.3755396207999453e-06
Iter: 999 loss: 5.3719758110864041e-06
Iter: 1000 loss: 5.4002027031089646e-06
Iter: 1001 loss: 5.371305575272688e-06
Iter: 1002 loss: 5.3677138987430432e-06
Iter: 1003 loss: 5.3677138232317687e-06
Iter: 1004 loss: 5.3642125371554122e-06
Iter: 1005 loss: 5.3636076043646721e-06
Iter: 1006 loss: 5.3612217530422373e-06
Iter: 1007 loss: 5.3591646160448926e-06
Iter: 1008 loss: 5.3848189646306578e-06
Iter: 1009 loss: 5.3591451830271484e-06
Iter: 1010 loss: 5.3568358386678854e-06
Iter: 1011 loss: 5.3561201375659933e-06
Iter: 1012 loss: 5.3547548649642612e-06
Iter: 1013 loss: 5.3519415673936578e-06
Iter: 1014 loss: 5.3480381553599635e-06
Iter: 1015 loss: 5.3478720610761883e-06
Iter: 1016 loss: 5.34590630777268e-06
Iter: 1017 loss: 5.3453417068266861e-06
Iter: 1018 loss: 5.342633356841217e-06
Iter: 1019 loss: 5.3419874571646238e-06
Iter: 1020 loss: 5.3402601830595384e-06
Iter: 1021 loss: 5.3381562377032613e-06
Iter: 1022 loss: 5.3441875264114787e-06
Iter: 1023 loss: 5.3374955887927947e-06
Iter: 1024 loss: 5.3345277098240961e-06
Iter: 1025 loss: 5.345567130666178e-06
Iter: 1026 loss: 5.3338029712817752e-06
Iter: 1027 loss: 5.3317408261056561e-06
Iter: 1028 loss: 5.3298536721175818e-06
Iter: 1029 loss: 5.3293500412453377e-06
Iter: 1030 loss: 5.3261672286137957e-06
Iter: 1031 loss: 5.374767615718149e-06
Iter: 1032 loss: 5.3261661147673522e-06
Iter: 1033 loss: 5.3247229886258311e-06
Iter: 1034 loss: 5.3213597836287457e-06
Iter: 1035 loss: 5.36235676336709e-06
Iter: 1036 loss: 5.32109235999334e-06
Iter: 1037 loss: 5.3178828144110211e-06
Iter: 1038 loss: 5.3414807076541839e-06
Iter: 1039 loss: 5.3176181240467255e-06
Iter: 1040 loss: 5.3138951930088446e-06
Iter: 1041 loss: 5.3284804014711141e-06
Iter: 1042 loss: 5.3130375014109696e-06
Iter: 1043 loss: 5.310886067191968e-06
Iter: 1044 loss: 5.3125533115464543e-06
Iter: 1045 loss: 5.3095806606411511e-06
Iter: 1046 loss: 5.306786558715849e-06
Iter: 1047 loss: 5.3332602260533273e-06
Iter: 1048 loss: 5.3066760938601424e-06
Iter: 1049 loss: 5.3051732018565868e-06
Iter: 1050 loss: 5.3016407335129509e-06
Iter: 1051 loss: 5.3434654230540977e-06
Iter: 1052 loss: 5.3013381519059267e-06
Iter: 1053 loss: 5.29790491677263e-06
Iter: 1054 loss: 5.3321426256669369e-06
Iter: 1055 loss: 5.2977932617036882e-06
Iter: 1056 loss: 5.2940350769040351e-06
Iter: 1057 loss: 5.308791358705969e-06
Iter: 1058 loss: 5.2931713794292162e-06
Iter: 1059 loss: 5.2913142620261139e-06
Iter: 1060 loss: 5.2910921130529638e-06
Iter: 1061 loss: 5.2897597025469879e-06
Iter: 1062 loss: 5.2874475918212034e-06
Iter: 1063 loss: 5.2874470528443339e-06
Iter: 1064 loss: 5.2860878507451517e-06
Iter: 1065 loss: 5.2826935564056764e-06
Iter: 1066 loss: 5.314731534754279e-06
Iter: 1067 loss: 5.282219391646285e-06
Iter: 1068 loss: 5.2802969617010777e-06
Iter: 1069 loss: 5.2797200517783932e-06
Iter: 1070 loss: 5.2782815070362165e-06
Iter: 1071 loss: 5.2748072691945566e-06
Iter: 1072 loss: 5.3121494788402572e-06
Iter: 1073 loss: 5.2744339492250635e-06
Iter: 1074 loss: 5.2711084776914554e-06
Iter: 1075 loss: 5.2794021383475932e-06
Iter: 1076 loss: 5.2699402605925742e-06
Iter: 1077 loss: 5.2675381391141347e-06
Iter: 1078 loss: 5.2674175157587322e-06
Iter: 1079 loss: 5.2651879671028816e-06
Iter: 1080 loss: 5.2614775052861078e-06
Iter: 1081 loss: 5.2614650210583263e-06
Iter: 1082 loss: 5.2595765005547468e-06
Iter: 1083 loss: 5.2593145247115775e-06
Iter: 1084 loss: 5.2573719982423278e-06
Iter: 1085 loss: 5.2546660476123142e-06
Iter: 1086 loss: 5.2545544280580767e-06
Iter: 1087 loss: 5.2520951184129908e-06
Iter: 1088 loss: 5.2531346325524578e-06
Iter: 1089 loss: 5.2504084973206439e-06
Iter: 1090 loss: 5.2485527125554093e-06
Iter: 1091 loss: 5.248249654206338e-06
Iter: 1092 loss: 5.24673231996982e-06
Iter: 1093 loss: 5.2438701594830575e-06
Iter: 1094 loss: 5.3065528853966361e-06
Iter: 1095 loss: 5.2438616072301177e-06
Iter: 1096 loss: 5.242767255931579e-06
Iter: 1097 loss: 5.2423976549414027e-06
Iter: 1098 loss: 5.2410291210353759e-06
Iter: 1099 loss: 5.2380757244226638e-06
Iter: 1100 loss: 5.2838324621329625e-06
Iter: 1101 loss: 5.2379669847197961e-06
Iter: 1102 loss: 5.235747159643791e-06
Iter: 1103 loss: 5.2708278660471506e-06
Iter: 1104 loss: 5.2357471042586088e-06
Iter: 1105 loss: 5.2333405411885969e-06
Iter: 1106 loss: 5.2315497010526444e-06
Iter: 1107 loss: 5.2307502456566307e-06
Iter: 1108 loss: 5.2283180831488432e-06
Iter: 1109 loss: 5.2294420128789828e-06
Iter: 1110 loss: 5.2266729390440428e-06
Iter: 1111 loss: 5.2249375095388969e-06
Iter: 1112 loss: 5.22478576576275e-06
Iter: 1113 loss: 5.2231018177166214e-06
Iter: 1114 loss: 5.2195969667447268e-06
Iter: 1115 loss: 5.2796067218004296e-06
Iter: 1116 loss: 5.2195150706960506e-06
Iter: 1117 loss: 5.2176766505943772e-06
Iter: 1118 loss: 5.2174154414907775e-06
Iter: 1119 loss: 5.2154949738354122e-06
Iter: 1120 loss: 5.212975818063259e-06
Iter: 1121 loss: 5.2128187661341564e-06
Iter: 1122 loss: 5.2103824887508982e-06
Iter: 1123 loss: 5.211685925620716e-06
Iter: 1124 loss: 5.2087763777542322e-06
Iter: 1125 loss: 5.2069751336361571e-06
Iter: 1126 loss: 5.2067095820105226e-06
Iter: 1127 loss: 5.2050340848901093e-06
Iter: 1128 loss: 5.2013428141516877e-06
Iter: 1129 loss: 5.2551631650749042e-06
Iter: 1130 loss: 5.20117209081525e-06
Iter: 1131 loss: 5.19959276586651e-06
Iter: 1132 loss: 5.1990471604375981e-06
Iter: 1133 loss: 5.1972971415418841e-06
Iter: 1134 loss: 5.1942743795615294e-06
Iter: 1135 loss: 5.1942723872482767e-06
Iter: 1136 loss: 5.1926663602856307e-06
Iter: 1137 loss: 5.192582204878553e-06
Iter: 1138 loss: 5.1909028638223094e-06
Iter: 1139 loss: 5.18747162754861e-06
Iter: 1140 loss: 5.249172884082108e-06
Iter: 1141 loss: 5.187410397234889e-06
Iter: 1142 loss: 5.18378177626441e-06
Iter: 1143 loss: 5.1918299947095952e-06
Iter: 1144 loss: 5.1823957916838921e-06
Iter: 1145 loss: 5.1800166808446578e-06
Iter: 1146 loss: 5.1798430492073424e-06
Iter: 1147 loss: 5.1784444631050214e-06
Iter: 1148 loss: 5.1754155725093071e-06
Iter: 1149 loss: 5.2218282185362364e-06
Iter: 1150 loss: 5.1752993807121473e-06
Iter: 1151 loss: 5.1730045028131515e-06
Iter: 1152 loss: 5.1728345118657272e-06
Iter: 1153 loss: 5.1711215111813084e-06
Iter: 1154 loss: 5.1674137122746568e-06
Iter: 1155 loss: 5.2243086406054889e-06
Iter: 1156 loss: 5.1672723275731691e-06
Iter: 1157 loss: 5.1636291085635727e-06
Iter: 1158 loss: 5.1792994593752563e-06
Iter: 1159 loss: 5.1628782178455314e-06
Iter: 1160 loss: 5.1600383643711423e-06
Iter: 1161 loss: 5.1600160963975148e-06
Iter: 1162 loss: 5.1586151914405941e-06
Iter: 1163 loss: 5.1560659217598445e-06
Iter: 1164 loss: 5.2166264639296162e-06
Iter: 1165 loss: 5.1560647771027672e-06
Iter: 1166 loss: 5.1532778446498513e-06
Iter: 1167 loss: 5.1532767151723091e-06
Iter: 1168 loss: 5.151595509744e-06
Iter: 1169 loss: 5.1482282880106678e-06
Iter: 1170 loss: 5.2119761408635879e-06
Iter: 1171 loss: 5.1481851082562638e-06
Iter: 1172 loss: 5.1458875119816962e-06
Iter: 1173 loss: 5.1456574319042348e-06
Iter: 1174 loss: 5.14408609587e-06
Iter: 1175 loss: 5.1406096693117511e-06
Iter: 1176 loss: 5.1906342692778e-06
Iter: 1177 loss: 5.140441527781135e-06
Iter: 1178 loss: 5.1373116336154568e-06
Iter: 1179 loss: 5.1623269478354474e-06
Iter: 1180 loss: 5.1371024238703106e-06
Iter: 1181 loss: 5.1336284246189965e-06
Iter: 1182 loss: 5.1444322549925284e-06
Iter: 1183 loss: 5.1326137320830448e-06
Iter: 1184 loss: 5.1303650011992412e-06
Iter: 1185 loss: 5.1313794834169883e-06
Iter: 1186 loss: 5.1288383573829047e-06
Iter: 1187 loss: 5.1255452765437458e-06
Iter: 1188 loss: 5.1506635232791783e-06
Iter: 1189 loss: 5.1252969403208393e-06
Iter: 1190 loss: 5.1236454108252309e-06
Iter: 1191 loss: 5.1204867659868913e-06
Iter: 1192 loss: 5.1874246956569232e-06
Iter: 1193 loss: 5.1204722097273377e-06
Iter: 1194 loss: 5.1175343371189226e-06
Iter: 1195 loss: 5.1528809692893034e-06
Iter: 1196 loss: 5.1174982509593477e-06
Iter: 1197 loss: 5.1141634953902564e-06
Iter: 1198 loss: 5.11913538605819e-06
Iter: 1199 loss: 5.1125655306259159e-06
Iter: 1200 loss: 5.1104923107526878e-06
Iter: 1201 loss: 5.1180488151804931e-06
Iter: 1202 loss: 5.1099747991413664e-06
Iter: 1203 loss: 5.1073629413517081e-06
Iter: 1204 loss: 5.1140121433154285e-06
Iter: 1205 loss: 5.1064600302406779e-06
Iter: 1206 loss: 5.1046267452627769e-06
Iter: 1207 loss: 5.1038351702782694e-06
Iter: 1208 loss: 5.1028927323859719e-06
Iter: 1209 loss: 5.0994190173546354e-06
Iter: 1210 loss: 5.1203783734629936e-06
Iter: 1211 loss: 5.0989891165229994e-06
Iter: 1212 loss: 5.097013546119797e-06
Iter: 1213 loss: 5.0934519725156562e-06
Iter: 1214 loss: 5.1797985473089941e-06
Iter: 1215 loss: 5.0934514001854074e-06
Iter: 1216 loss: 5.0918323302459914e-06
Iter: 1217 loss: 5.0914405190899475e-06
Iter: 1218 loss: 5.089500272637252e-06
Iter: 1219 loss: 5.0880375905815953e-06
Iter: 1220 loss: 5.0874031297663569e-06
Iter: 1221 loss: 5.0851801659919459e-06
Iter: 1222 loss: 5.0981608661580181e-06
Iter: 1223 loss: 5.0848886705739851e-06
Iter: 1224 loss: 5.0820469704421077e-06
Iter: 1225 loss: 5.082057468736426e-06
Iter: 1226 loss: 5.0797765623543981e-06
Iter: 1227 loss: 5.0770966578753553e-06
Iter: 1228 loss: 5.0773912258869093e-06
Iter: 1229 loss: 5.07503869848011e-06
Iter: 1230 loss: 5.07402598888581e-06
Iter: 1231 loss: 5.0734459263650157e-06
Iter: 1232 loss: 5.0719343200161941e-06
Iter: 1233 loss: 5.0686639891429919e-06
Iter: 1234 loss: 5.1189357205423935e-06
Iter: 1235 loss: 5.0685400046821179e-06
Iter: 1236 loss: 5.0666974009862089e-06
Iter: 1237 loss: 5.0665108520108519e-06
Iter: 1238 loss: 5.0646211378823691e-06
Iter: 1239 loss: 5.0620921162410571e-06
Iter: 1240 loss: 5.0619532948670477e-06
Iter: 1241 loss: 5.059735348877964e-06
Iter: 1242 loss: 5.08031271199454e-06
Iter: 1243 loss: 5.0596409886938316e-06
Iter: 1244 loss: 5.0571962183127636e-06
Iter: 1245 loss: 5.0575255400397048e-06
Iter: 1246 loss: 5.0553358222084322e-06
Iter: 1247 loss: 5.0530374219907192e-06
Iter: 1248 loss: 5.0505160383502194e-06
Iter: 1249 loss: 5.0501445886967453e-06
Iter: 1250 loss: 5.04837393288786e-06
Iter: 1251 loss: 5.0477564301257131e-06
Iter: 1252 loss: 5.046085768340508e-06
Iter: 1253 loss: 5.04319668655648e-06
Iter: 1254 loss: 5.043194935992262e-06
Iter: 1255 loss: 5.04094375606637e-06
Iter: 1256 loss: 5.0409238417996439e-06
Iter: 1257 loss: 5.0387382805173987e-06
Iter: 1258 loss: 5.0347475046959695e-06
Iter: 1259 loss: 5.1288915235004482e-06
Iter: 1260 loss: 5.034745151249354e-06
Iter: 1261 loss: 5.0312607405055659e-06
Iter: 1262 loss: 5.043217394606205e-06
Iter: 1263 loss: 5.0303349430605562e-06
Iter: 1264 loss: 5.0279970402150179e-06
Iter: 1265 loss: 5.0278633622831373e-06
Iter: 1266 loss: 5.026604117658435e-06
Iter: 1267 loss: 5.02439523729067e-06
Iter: 1268 loss: 5.02439492008215e-06
Iter: 1269 loss: 5.02245500058633e-06
Iter: 1270 loss: 5.0224061444294015e-06
Iter: 1271 loss: 5.0210919683327118e-06
Iter: 1272 loss: 5.0177994690554994e-06
Iter: 1273 loss: 5.0484862823994718e-06
Iter: 1274 loss: 5.0173282418515913e-06
Iter: 1275 loss: 5.0151083376382721e-06
Iter: 1276 loss: 5.0147814618074988e-06
Iter: 1277 loss: 5.0128949189785665e-06
Iter: 1278 loss: 5.0095607081145253e-06
Iter: 1279 loss: 5.0095606136612342e-06
Iter: 1280 loss: 5.0066410151819874e-06
Iter: 1281 loss: 5.0159812890222288e-06
Iter: 1282 loss: 5.0058104072454868e-06
Iter: 1283 loss: 5.0024637206102615e-06
Iter: 1284 loss: 5.0346969668346981e-06
Iter: 1285 loss: 5.00233925444807e-06
Iter: 1286 loss: 5.0005459817825658e-06
Iter: 1287 loss: 4.9983373263749054e-06
Iter: 1288 loss: 4.9981416737584794e-06
Iter: 1289 loss: 4.9955280452701347e-06
Iter: 1290 loss: 4.9955140050696936e-06
Iter: 1291 loss: 4.994033146941011e-06
Iter: 1292 loss: 4.9908985708366242e-06
Iter: 1293 loss: 5.0421730570557269e-06
Iter: 1294 loss: 4.9908071302800358e-06
Iter: 1295 loss: 4.9883503091201665e-06
Iter: 1296 loss: 5.0239069877684329e-06
Iter: 1297 loss: 4.9883464313849864e-06
Iter: 1298 loss: 4.9855113709978915e-06
Iter: 1299 loss: 4.9872682927548794e-06
Iter: 1300 loss: 4.98369668337254e-06
Iter: 1301 loss: 4.9814345475648e-06
Iter: 1302 loss: 4.9891198389677e-06
Iter: 1303 loss: 4.9808274475754521e-06
Iter: 1304 loss: 4.9779605958070622e-06
Iter: 1305 loss: 4.986919777911873e-06
Iter: 1306 loss: 4.9771267851296158e-06
Iter: 1307 loss: 4.9755101252948829e-06
Iter: 1308 loss: 4.9748079766132234e-06
Iter: 1309 loss: 4.97397943435112e-06
Iter: 1310 loss: 4.9708839534822555e-06
Iter: 1311 loss: 4.9900979990336144e-06
Iter: 1312 loss: 4.9705203800782141e-06
Iter: 1313 loss: 4.9686545055294176e-06
Iter: 1314 loss: 4.9651051784183886e-06
Iter: 1315 loss: 5.0413436296532793e-06
Iter: 1316 loss: 4.9650912518820279e-06
Iter: 1317 loss: 4.9627501930003162e-06
Iter: 1318 loss: 4.9626890259011484e-06
Iter: 1319 loss: 4.960191109127665e-06
Iter: 1320 loss: 4.9627421366600211e-06
Iter: 1321 loss: 4.9587958581295451e-06
Iter: 1322 loss: 4.9569343898240352e-06
Iter: 1323 loss: 4.9601641239534423e-06
Iter: 1324 loss: 4.9561087416909817e-06
Iter: 1325 loss: 4.9531357850706126e-06
Iter: 1326 loss: 4.9588788006211532e-06
Iter: 1327 loss: 4.9518960843340153e-06
Iter: 1328 loss: 4.9497632833490736e-06
Iter: 1329 loss: 4.9481229957720762e-06
Iter: 1330 loss: 4.9474429100585692e-06
Iter: 1331 loss: 4.9463950696336776e-06
Iter: 1332 loss: 4.9458251090125552e-06
Iter: 1333 loss: 4.9443747492117238e-06
Iter: 1334 loss: 4.9414347784140487e-06
Iter: 1335 loss: 4.9954088198808333e-06
Iter: 1336 loss: 4.9413885226096896e-06
Iter: 1337 loss: 4.9396354738622367e-06
Iter: 1338 loss: 4.9395330393058009e-06
Iter: 1339 loss: 4.9377266553549746e-06
Iter: 1340 loss: 4.9348456266632877e-06
Iter: 1341 loss: 4.9348201960931336e-06
Iter: 1342 loss: 4.9326263759451029e-06
Iter: 1343 loss: 4.95677073854775e-06
Iter: 1344 loss: 4.9325808074652866e-06
Iter: 1345 loss: 4.9301921978882481e-06
Iter: 1346 loss: 4.930814951117918e-06
Iter: 1347 loss: 4.928456252098e-06
Iter: 1348 loss: 4.9265001346152238e-06
Iter: 1349 loss: 4.9238673127209855e-06
Iter: 1350 loss: 4.9237281017968921e-06
Iter: 1351 loss: 4.9217520850557854e-06
Iter: 1352 loss: 4.9214949572813789e-06
Iter: 1353 loss: 4.9192292246253716e-06
Iter: 1354 loss: 4.9185570476477891e-06
Iter: 1355 loss: 4.9171980807801639e-06
Iter: 1356 loss: 4.9155222087777421e-06
Iter: 1357 loss: 4.9320347864667746e-06
Iter: 1358 loss: 4.9154650722664547e-06
Iter: 1359 loss: 4.9136315302661789e-06
Iter: 1360 loss: 4.9116871951697032e-06
Iter: 1361 loss: 4.9113622194808322e-06
Iter: 1362 loss: 4.9087852473069782e-06
Iter: 1363 loss: 4.90924483813136e-06
Iter: 1364 loss: 4.9068553499268685e-06
Iter: 1365 loss: 4.90494100400322e-06
Iter: 1366 loss: 4.9045830391139848e-06
Iter: 1367 loss: 4.9030429989873666e-06
Iter: 1368 loss: 4.9002888460730107e-06
Iter: 1369 loss: 4.9682650511083439e-06
Iter: 1370 loss: 4.900288767154492e-06
Iter: 1371 loss: 4.8991422544962025e-06
Iter: 1372 loss: 4.8987590846429266e-06
Iter: 1373 loss: 4.8975805947320389e-06
Iter: 1374 loss: 4.8945457163497413e-06
Iter: 1375 loss: 4.9196653161766119e-06
Iter: 1376 loss: 4.8940131007998529e-06
Iter: 1377 loss: 4.8920502500475108e-06
Iter: 1378 loss: 4.8918696742124863e-06
Iter: 1379 loss: 4.8898421645122095e-06
Iter: 1380 loss: 4.8875666171916733e-06
Iter: 1381 loss: 4.8872601840840326e-06
Iter: 1382 loss: 4.8847973352330192e-06
Iter: 1383 loss: 4.8839428302069146e-06
Iter: 1384 loss: 4.8825451560968042e-06
Iter: 1385 loss: 4.8804424787724466e-06
Iter: 1386 loss: 4.8803594560091719e-06
Iter: 1387 loss: 4.8778916735598126e-06
Iter: 1388 loss: 4.878336307929811e-06
Iter: 1389 loss: 4.8760447533744026e-06
Iter: 1390 loss: 4.8742488529724188e-06
Iter: 1391 loss: 4.8871236535078804e-06
Iter: 1392 loss: 4.8740920845896655e-06
Iter: 1393 loss: 4.8721248281204288e-06
Iter: 1394 loss: 4.8723517534300148e-06
Iter: 1395 loss: 4.8706171521080915e-06
Iter: 1396 loss: 4.8686273926649909e-06
Iter: 1397 loss: 4.8672094369105118e-06
Iter: 1398 loss: 4.8665141362895984e-06
Iter: 1399 loss: 4.8652349263546505e-06
Iter: 1400 loss: 4.8646954673740819e-06
Iter: 1401 loss: 4.8633312498991253e-06
Iter: 1402 loss: 4.8604924278887011e-06
Iter: 1403 loss: 4.9091391662504474e-06
Iter: 1404 loss: 4.8604262998262729e-06
Iter: 1405 loss: 4.858980148154005e-06
Iter: 1406 loss: 4.8586949762613742e-06
Iter: 1407 loss: 4.8573100609838132e-06
Iter: 1408 loss: 4.8544387398399074e-06
Iter: 1409 loss: 4.904117184115663e-06
Iter: 1410 loss: 4.8543751797308034e-06
Iter: 1411 loss: 4.8523843941757647e-06
Iter: 1412 loss: 4.8523795191445862e-06
Iter: 1413 loss: 4.8502583946231685e-06
Iter: 1414 loss: 4.8483016850748708e-06
Iter: 1415 loss: 4.8477912706252114e-06
Iter: 1416 loss: 4.8454775578610507e-06
Iter: 1417 loss: 4.8456886218589907e-06
Iter: 1418 loss: 4.8436883591632026e-06
Iter: 1419 loss: 4.8406755457570236e-06
Iter: 1420 loss: 4.8430745223354115e-06
Iter: 1421 loss: 4.8388605005591695e-06
Iter: 1422 loss: 4.83575931628297e-06
Iter: 1423 loss: 4.8684115418071378e-06
Iter: 1424 loss: 4.83567940388283e-06
Iter: 1425 loss: 4.8332041256819295e-06
Iter: 1426 loss: 4.8671715273797032e-06
Iter: 1427 loss: 4.8331950894112405e-06
Iter: 1428 loss: 4.8320178699510486e-06
Iter: 1429 loss: 4.8298459558392087e-06
Iter: 1430 loss: 4.879860209482008e-06
Iter: 1431 loss: 4.82984345672954e-06
Iter: 1432 loss: 4.8281870543192929e-06
Iter: 1433 loss: 4.8281092455384817e-06
Iter: 1434 loss: 4.8267060369192452e-06
Iter: 1435 loss: 4.8238851458225345e-06
Iter: 1436 loss: 4.8767994577840879e-06
Iter: 1437 loss: 4.82384654748384e-06
Iter: 1438 loss: 4.8223394268925127e-06
Iter: 1439 loss: 4.82199282416553e-06
Iter: 1440 loss: 4.8206975321628923e-06
Iter: 1441 loss: 4.8182269325824475e-06
Iter: 1442 loss: 4.87093635414043e-06
Iter: 1443 loss: 4.8182164121904508e-06
Iter: 1444 loss: 4.8166414389465021e-06
Iter: 1445 loss: 4.8166246410124163e-06
Iter: 1446 loss: 4.814968465152664e-06
Iter: 1447 loss: 4.8134178308301516e-06
Iter: 1448 loss: 4.8130302567459285e-06
Iter: 1449 loss: 4.810793164276092e-06
Iter: 1450 loss: 4.8082221583714715e-06
Iter: 1451 loss: 4.8079083714581975e-06
Iter: 1452 loss: 4.8077761104883254e-06
Iter: 1453 loss: 4.8062287391729607e-06
Iter: 1454 loss: 4.8048017758259328e-06
Iter: 1455 loss: 4.8018563690576086e-06
Iter: 1456 loss: 4.8534297861048926e-06
Iter: 1457 loss: 4.8017951841877658e-06
Iter: 1458 loss: 4.7989128055749292e-06
Iter: 1459 loss: 4.8016227934038118e-06
Iter: 1460 loss: 4.7972585730186925e-06
Iter: 1461 loss: 4.7962234476798481e-06
Iter: 1462 loss: 4.7954638776753642e-06
Iter: 1463 loss: 4.7940134400563978e-06
Iter: 1464 loss: 4.79113131774463e-06
Iter: 1465 loss: 4.8467873232871007e-06
Iter: 1466 loss: 4.7910993557811135e-06
Iter: 1467 loss: 4.7892654010496e-06
Iter: 1468 loss: 4.7892643561025866e-06
Iter: 1469 loss: 4.7873501911901432e-06
Iter: 1470 loss: 4.7872371867908523e-06
Iter: 1471 loss: 4.7857844063364876e-06
Iter: 1472 loss: 4.7841472862541373e-06
Iter: 1473 loss: 4.80575415161517e-06
Iter: 1474 loss: 4.7841379495231733e-06
Iter: 1475 loss: 4.7825755716713424e-06
Iter: 1476 loss: 4.7799745058506957e-06
Iter: 1477 loss: 4.7799658559917836e-06
Iter: 1478 loss: 4.77798341776972e-06
Iter: 1479 loss: 4.8025665395425206e-06
Iter: 1480 loss: 4.7779638496448727e-06
Iter: 1481 loss: 4.7758632681801127e-06
Iter: 1482 loss: 4.7761528841442072e-06
Iter: 1483 loss: 4.7742666883823113e-06
Iter: 1484 loss: 4.7722408766429991e-06
Iter: 1485 loss: 4.7696166054353623e-06
Iter: 1486 loss: 4.76944012923171e-06
Iter: 1487 loss: 4.7679410822903039e-06
Iter: 1488 loss: 4.767533246086927e-06
Iter: 1489 loss: 4.7655966417770788e-06
Iter: 1490 loss: 4.7647682348238084e-06
Iter: 1491 loss: 4.7637678708830841e-06
Iter: 1492 loss: 4.7620225485891414e-06
Iter: 1493 loss: 4.7602540872152454e-06
Iter: 1494 loss: 4.7599082462102052e-06
Iter: 1495 loss: 4.7586255810026775e-06
Iter: 1496 loss: 4.7581759201014495e-06
Iter: 1497 loss: 4.7565330983637e-06
Iter: 1498 loss: 4.7537369306854858e-06
Iter: 1499 loss: 4.7537327769263934e-06
Iter: 1500 loss: 4.751484794607879e-06
Iter: 1501 loss: 4.7652554878273021e-06
Iter: 1502 loss: 4.7512142824502474e-06
Iter: 1503 loss: 4.7487447006503823e-06
Iter: 1504 loss: 4.7592266135464616e-06
Iter: 1505 loss: 4.7482273628039423e-06
Iter: 1506 loss: 4.7467807764118433e-06
Iter: 1507 loss: 4.7514831890014824e-06
Iter: 1508 loss: 4.7463754438134972e-06
Iter: 1509 loss: 4.7443227091986146e-06
Iter: 1510 loss: 4.7429728375808992e-06
Iter: 1511 loss: 4.7421925528565343e-06
Iter: 1512 loss: 4.7399931115237048e-06
Iter: 1513 loss: 4.7497785030142355e-06
Iter: 1514 loss: 4.7395589668934681e-06
Iter: 1515 loss: 4.7372102682115546e-06
Iter: 1516 loss: 4.7496342780275475e-06
Iter: 1517 loss: 4.7368477686279764e-06
Iter: 1518 loss: 4.735598822007986e-06
Iter: 1519 loss: 4.7326727748545946e-06
Iter: 1520 loss: 4.7677119081517905e-06
Iter: 1521 loss: 4.7324291324529458e-06
Iter: 1522 loss: 4.7305855123775226e-06
Iter: 1523 loss: 4.7303784211897752e-06
Iter: 1524 loss: 4.72820799700985e-06
Iter: 1525 loss: 4.7282414334102644e-06
Iter: 1526 loss: 4.7264816913717875e-06
Iter: 1527 loss: 4.7243693008398247e-06
Iter: 1528 loss: 4.7224647272709469e-06
Iter: 1529 loss: 4.7219347299715211e-06
Iter: 1530 loss: 4.7212937376923161e-06
Iter: 1531 loss: 4.7205020302400287e-06
Iter: 1532 loss: 4.7190234531912383e-06
Iter: 1533 loss: 4.715942541132028e-06
Iter: 1534 loss: 4.7685352850818346e-06
Iter: 1535 loss: 4.7158694204193216e-06
Iter: 1536 loss: 4.7132314622411893e-06
Iter: 1537 loss: 4.7290980062929886e-06
Iter: 1538 loss: 4.712903113826151e-06
Iter: 1539 loss: 4.7104396401600016e-06
Iter: 1540 loss: 4.7311374684312959e-06
Iter: 1541 loss: 4.710296349970116e-06
Iter: 1542 loss: 4.7090192604668042e-06
Iter: 1543 loss: 4.71021655125955e-06
Iter: 1544 loss: 4.7082857655779638e-06
Iter: 1545 loss: 4.7060632849560748e-06
Iter: 1546 loss: 4.7073389925587544e-06
Iter: 1547 loss: 4.7046178824173747e-06
Iter: 1548 loss: 4.7027056730405073e-06
Iter: 1549 loss: 4.70602630882445e-06
Iter: 1550 loss: 4.7018579563681741e-06
Iter: 1551 loss: 4.6993947991866206e-06
Iter: 1552 loss: 4.7159089232390847e-06
Iter: 1553 loss: 4.6991462872875223e-06
Iter: 1554 loss: 4.6978613597413983e-06
Iter: 1555 loss: 4.69530982464025e-06
Iter: 1556 loss: 4.7446636617528114e-06
Iter: 1557 loss: 4.69528188590041e-06
Iter: 1558 loss: 4.6938235821192343e-06
Iter: 1559 loss: 4.6936911432097468e-06
Iter: 1560 loss: 4.6918949526461822e-06
Iter: 1561 loss: 4.6900084450973241e-06
Iter: 1562 loss: 4.6896821569922536e-06
Iter: 1563 loss: 4.6871390928054e-06
Iter: 1564 loss: 4.6870897918723011e-06
Iter: 1565 loss: 4.6850896204948784e-06
Iter: 1566 loss: 4.6842545958359554e-06
Iter: 1567 loss: 4.6836667908503742e-06
Iter: 1568 loss: 4.6821437693343242e-06
Iter: 1569 loss: 4.6789126194528309e-06
Iter: 1570 loss: 4.7314112519592127e-06
Iter: 1571 loss: 4.6788155945194129e-06
Iter: 1572 loss: 4.67611104922554e-06
Iter: 1573 loss: 4.6919211041513464e-06
Iter: 1574 loss: 4.6757569423191231e-06
Iter: 1575 loss: 4.6731449525643185e-06
Iter: 1576 loss: 4.6948102225427943e-06
Iter: 1577 loss: 4.6729873662732826e-06
Iter: 1578 loss: 4.6716449084022968e-06
Iter: 1579 loss: 4.6729607989696091e-06
Iter: 1580 loss: 4.6708847692053044e-06
Iter: 1581 loss: 4.6687533145853056e-06
Iter: 1582 loss: 4.6727240958919054e-06
Iter: 1583 loss: 4.6678450190435311e-06
Iter: 1584 loss: 4.6662475831388726e-06
Iter: 1585 loss: 4.6665016883154066e-06
Iter: 1586 loss: 4.6650428330639612e-06
Iter: 1587 loss: 4.6623423391766011e-06
Iter: 1588 loss: 4.6791037423390428e-06
Iter: 1589 loss: 4.66202521830492e-06
Iter: 1590 loss: 4.6605576212253881e-06
Iter: 1591 loss: 4.6579244811610727e-06
Iter: 1592 loss: 4.7224428790285968e-06
Iter: 1593 loss: 4.6579242992642837e-06
Iter: 1594 loss: 4.65670781264201e-06
Iter: 1595 loss: 4.6564133209427528e-06
Iter: 1596 loss: 4.6548366224460628e-06
Iter: 1597 loss: 4.6526393224340606e-06
Iter: 1598 loss: 4.6525489179924315e-06
Iter: 1599 loss: 4.6499448951673484e-06
Iter: 1600 loss: 4.6492814366928989e-06
Iter: 1601 loss: 4.6476484491048839e-06
Iter: 1602 loss: 4.6470813972440261e-06
Iter: 1603 loss: 4.6462177920750393e-06
Iter: 1604 loss: 4.644699910572195e-06
Iter: 1605 loss: 4.6418784083335387e-06
Iter: 1606 loss: 4.705763208813207e-06
Iter: 1607 loss: 4.6418736890190033e-06
Iter: 1608 loss: 4.6392583266346263e-06
Iter: 1609 loss: 4.6478426818626437e-06
Iter: 1610 loss: 4.6385320124135988e-06
Iter: 1611 loss: 4.6357230543024089e-06
Iter: 1612 loss: 4.6633075458994842e-06
Iter: 1613 loss: 4.6356260786055516e-06
Iter: 1614 loss: 4.634130148526868e-06
Iter: 1615 loss: 4.6341385684649961e-06
Iter: 1616 loss: 4.632935976372948e-06
Iter: 1617 loss: 4.6304231376976978e-06
Iter: 1618 loss: 4.6418011365850178e-06
Iter: 1619 loss: 4.6299382933369084e-06
Iter: 1620 loss: 4.6286079103124553e-06
Iter: 1621 loss: 4.6281216562542009e-06
Iter: 1622 loss: 4.627382448943182e-06
Iter: 1623 loss: 4.6247437769334264e-06
Iter: 1624 loss: 4.6370230070834581e-06
Iter: 1625 loss: 4.6242532121204337e-06
Iter: 1626 loss: 4.6224515405688413e-06
Iter: 1627 loss: 4.6193065514432156e-06
Iter: 1628 loss: 4.6193057137059561e-06
Iter: 1629 loss: 4.6181701392213868e-06
Iter: 1630 loss: 4.6176051087035667e-06
Iter: 1631 loss: 4.6159624177641074e-06
Iter: 1632 loss: 4.61342611629193e-06
Iter: 1633 loss: 4.6133891084317594e-06
Iter: 1634 loss: 4.6107564860027121e-06
Iter: 1635 loss: 4.6109393536807546e-06
Iter: 1636 loss: 4.6087041843310624e-06
Iter: 1637 loss: 4.6074680343762573e-06
Iter: 1638 loss: 4.6070220141745317e-06
Iter: 1639 loss: 4.6052048896306328e-06
Iter: 1640 loss: 4.6032013821754141e-06
Iter: 1641 loss: 4.6029118751303342e-06
Iter: 1642 loss: 4.6009752885707137e-06
Iter: 1643 loss: 4.6066018705114761e-06
Iter: 1644 loss: 4.6003741654073884e-06
Iter: 1645 loss: 4.59814360266029e-06
Iter: 1646 loss: 4.616569994629078e-06
Iter: 1647 loss: 4.5980074454014094e-06
Iter: 1648 loss: 4.5966001569888876e-06
Iter: 1649 loss: 4.59545175506109e-06
Iter: 1650 loss: 4.5950378588167887e-06
Iter: 1651 loss: 4.5923745160327308e-06
Iter: 1652 loss: 4.61084356090399e-06
Iter: 1653 loss: 4.5921242820604894e-06
Iter: 1654 loss: 4.5908852658854778e-06
Iter: 1655 loss: 4.5903437269233137e-06
Iter: 1656 loss: 4.589710823441506e-06
Iter: 1657 loss: 4.58737703732551e-06
Iter: 1658 loss: 4.6008527226034031e-06
Iter: 1659 loss: 4.5870649914449484e-06
Iter: 1660 loss: 4.5856239442349689e-06
Iter: 1661 loss: 4.5829727715268227e-06
Iter: 1662 loss: 4.6444212137845309e-06
Iter: 1663 loss: 4.5829701753546907e-06
Iter: 1664 loss: 4.581814146221833e-06
Iter: 1665 loss: 4.5813983952760185e-06
Iter: 1666 loss: 4.5799460457916269e-06
Iter: 1667 loss: 4.5777350632473464e-06
Iter: 1668 loss: 4.577696349286312e-06
Iter: 1669 loss: 4.5755698557915111e-06
Iter: 1670 loss: 4.5750036612887907e-06
Iter: 1671 loss: 4.57368619242647e-06
Iter: 1672 loss: 4.5718230468590591e-06
Iter: 1673 loss: 4.57174654327319e-06
Iter: 1674 loss: 4.5696177071665176e-06
Iter: 1675 loss: 4.5715078675839547e-06
Iter: 1676 loss: 4.5683743546483505e-06
Iter: 1677 loss: 4.5668773766847664e-06
Iter: 1678 loss: 4.5672987168530285e-06
Iter: 1679 loss: 4.5657974745366338e-06
Iter: 1680 loss: 4.563930565986703e-06
Iter: 1681 loss: 4.5917138313220939e-06
Iter: 1682 loss: 4.5639290267209789e-06
Iter: 1683 loss: 4.5628531936451366e-06
Iter: 1684 loss: 4.5611026461240936e-06
Iter: 1685 loss: 4.5610922360803737e-06
Iter: 1686 loss: 4.5585952279795254e-06
Iter: 1687 loss: 4.5801175975236957e-06
Iter: 1688 loss: 4.55846048734244e-06
Iter: 1689 loss: 4.55707775157294e-06
Iter: 1690 loss: 4.5557056320829961e-06
Iter: 1691 loss: 4.5554184497147136e-06
Iter: 1692 loss: 4.5531824514989726e-06
Iter: 1693 loss: 4.5830588090188382e-06
Iter: 1694 loss: 4.5531712514500683e-06
Iter: 1695 loss: 4.5519456889173368e-06
Iter: 1696 loss: 4.5491352360361189e-06
Iter: 1697 loss: 4.5852850720896678e-06
Iter: 1698 loss: 4.548942096096023e-06
Iter: 1699 loss: 4.5474968082105363e-06
Iter: 1700 loss: 4.5471511889201808e-06
Iter: 1701 loss: 4.5454903687691446e-06
Iter: 1702 loss: 4.54347356841234e-06
Iter: 1703 loss: 4.543281927881821e-06
Iter: 1704 loss: 4.541266294595836e-06
Iter: 1705 loss: 4.5402963230748361e-06
Iter: 1706 loss: 4.5393212610458294e-06
Iter: 1707 loss: 4.5361432384561459e-06
Iter: 1708 loss: 4.5427682844334374e-06
Iter: 1709 loss: 4.5348789461247892e-06
Iter: 1710 loss: 4.5338064535046412e-06
Iter: 1711 loss: 4.5332062124586744e-06
Iter: 1712 loss: 4.5314474207404347e-06
Iter: 1713 loss: 4.5298765933267715e-06
Iter: 1714 loss: 4.5294279659450583e-06
Iter: 1715 loss: 4.5275483614905234e-06
Iter: 1716 loss: 4.5359055705840112e-06
Iter: 1717 loss: 4.5271770866375758e-06
Iter: 1718 loss: 4.5247871124701669e-06
Iter: 1719 loss: 4.5308119877948738e-06
Iter: 1720 loss: 4.5239546745103745e-06
Iter: 1721 loss: 4.522318604256601e-06
Iter: 1722 loss: 4.5261844407420791e-06
Iter: 1723 loss: 4.5217208969502065e-06
Iter: 1724 loss: 4.51939034940222e-06
Iter: 1725 loss: 4.5216654958127994e-06
Iter: 1726 loss: 4.5180690070522915e-06
Iter: 1727 loss: 4.5166940480247112e-06
Iter: 1728 loss: 4.5228730323072154e-06
Iter: 1729 loss: 4.5164261822506269e-06
Iter: 1730 loss: 4.5147405257287055e-06
Iter: 1731 loss: 4.5179189882553731e-06
Iter: 1732 loss: 4.5140273203765768e-06
Iter: 1733 loss: 4.512657921459474e-06
Iter: 1734 loss: 4.5109951641592629e-06
Iter: 1735 loss: 4.510837085884071e-06
Iter: 1736 loss: 4.5097432824064287e-06
Iter: 1737 loss: 4.5094387633177514e-06
Iter: 1738 loss: 4.5083335010708941e-06
Iter: 1739 loss: 4.5055143739802e-06
Iter: 1740 loss: 4.529876319871667e-06
Iter: 1741 loss: 4.5050532820928721e-06
Iter: 1742 loss: 4.502230348902967e-06
Iter: 1743 loss: 4.5108789497453e-06
Iter: 1744 loss: 4.501394346078139e-06
Iter: 1745 loss: 4.499475528087961e-06
Iter: 1746 loss: 4.4994596050344139e-06
Iter: 1747 loss: 4.4972982710266039e-06
Iter: 1748 loss: 4.4978326308078182e-06
Iter: 1749 loss: 4.4957198077027548e-06
Iter: 1750 loss: 4.4942650031495078e-06
Iter: 1751 loss: 4.4997043778671724e-06
Iter: 1752 loss: 4.49391175734634e-06
Iter: 1753 loss: 4.4920049330408282e-06
Iter: 1754 loss: 4.4968193315710415e-06
Iter: 1755 loss: 4.4913415769946493e-06
Iter: 1756 loss: 4.4899310063804711e-06
Iter: 1757 loss: 4.4915594572116042e-06
Iter: 1758 loss: 4.48917711061348e-06
Iter: 1759 loss: 4.4868987297206964e-06
Iter: 1760 loss: 4.4910327514167967e-06
Iter: 1761 loss: 4.4859131284035806e-06
Iter: 1762 loss: 4.4843604021369923e-06
Iter: 1763 loss: 4.4866042906825481e-06
Iter: 1764 loss: 4.4836053567908172e-06
Iter: 1765 loss: 4.4812321279402232e-06
Iter: 1766 loss: 4.4896046444335e-06
Iter: 1767 loss: 4.4806191215098945e-06
Iter: 1768 loss: 4.4791278160318494e-06
Iter: 1769 loss: 4.47740718670506e-06
Iter: 1770 loss: 4.4772006194064783e-06
Iter: 1771 loss: 4.4758139871468141e-06
Iter: 1772 loss: 4.4756093891764607e-06
Iter: 1773 loss: 4.4743111107011285e-06
Iter: 1774 loss: 4.4712569305669862e-06
Iter: 1775 loss: 4.5073233421421145e-06
Iter: 1776 loss: 4.4709934167242732e-06
Iter: 1777 loss: 4.4685495654339153e-06
Iter: 1778 loss: 4.4767898271959718e-06
Iter: 1779 loss: 4.4678887541594426e-06
Iter: 1780 loss: 4.466361775406725e-06
Iter: 1781 loss: 4.4663106649366461e-06
Iter: 1782 loss: 4.4645679496482234e-06
Iter: 1783 loss: 4.4632142916627577e-06
Iter: 1784 loss: 4.4626657007070609e-06
Iter: 1785 loss: 4.46095726945323e-06
Iter: 1786 loss: 4.4701018923834987e-06
Iter: 1787 loss: 4.4606984368197781e-06
Iter: 1788 loss: 4.4584761972157813e-06
Iter: 1789 loss: 4.46081004661719e-06
Iter: 1790 loss: 4.4572468627979476e-06
Iter: 1791 loss: 4.4557131710971343e-06
Iter: 1792 loss: 4.4627284637611413e-06
Iter: 1793 loss: 4.4554212998647563e-06
Iter: 1794 loss: 4.4535658424407913e-06
Iter: 1795 loss: 4.4544922862916991e-06
Iter: 1796 loss: 4.4523273015472785e-06
Iter: 1797 loss: 4.4505989843245728e-06
Iter: 1798 loss: 4.4544062892131342e-06
Iter: 1799 loss: 4.4499360046790029e-06
Iter: 1800 loss: 4.4475230785244458e-06
Iter: 1801 loss: 4.45603315766828e-06
Iter: 1802 loss: 4.4468995861559e-06
Iter: 1803 loss: 4.4456142238674311e-06
Iter: 1804 loss: 4.444374852038955e-06
Iter: 1805 loss: 4.4440910937916075e-06
Iter: 1806 loss: 4.4425241185155617e-06
Iter: 1807 loss: 4.442469228582895e-06
Iter: 1808 loss: 4.4412751433257183e-06
Iter: 1809 loss: 4.4383533621998622e-06
Iter: 1810 loss: 4.4682528240452384e-06
Iter: 1811 loss: 4.43800485128501e-06
Iter: 1812 loss: 4.4352212971054124e-06
Iter: 1813 loss: 4.4439144527666235e-06
Iter: 1814 loss: 4.4344113588545514e-06
Iter: 1815 loss: 4.4335295285053792e-06
Iter: 1816 loss: 4.4330476150003509e-06
Iter: 1817 loss: 4.4317051619988575e-06
Iter: 1818 loss: 4.4296233459251982e-06
Iter: 1819 loss: 4.4295947398207643e-06
Iter: 1820 loss: 4.4279777039480026e-06
Iter: 1821 loss: 4.4450385203337062e-06
Iter: 1822 loss: 4.4279364599058448e-06
Iter: 1823 loss: 4.4260403281351435e-06
Iter: 1824 loss: 4.4253026896651755e-06
Iter: 1825 loss: 4.4242772560895049e-06
Iter: 1826 loss: 4.4226563114106132e-06
Iter: 1827 loss: 4.4357986851586909e-06
Iter: 1828 loss: 4.4225521084801738e-06
Iter: 1829 loss: 4.4208730426529977e-06
Iter: 1830 loss: 4.4213906070187847e-06
Iter: 1831 loss: 4.4196734836659575e-06
Iter: 1832 loss: 4.4181004009852008e-06
Iter: 1833 loss: 4.422593220565653e-06
Iter: 1834 loss: 4.4176049230643254e-06
Iter: 1835 loss: 4.4153800073582315e-06
Iter: 1836 loss: 4.4195109812609434e-06
Iter: 1837 loss: 4.4144301226104671e-06
Iter: 1838 loss: 4.4129539994172277e-06
Iter: 1839 loss: 4.4122253062234363e-06
Iter: 1840 loss: 4.4115224288938879e-06
Iter: 1841 loss: 4.4098758405683391e-06
Iter: 1842 loss: 4.4098173495796296e-06
Iter: 1843 loss: 4.4088004580250051e-06
Iter: 1844 loss: 4.4063208893153863e-06
Iter: 1845 loss: 4.4320375519234078e-06
Iter: 1846 loss: 4.4060331823401457e-06
Iter: 1847 loss: 4.4030506822517626e-06
Iter: 1848 loss: 4.406918701226506e-06
Iter: 1849 loss: 4.4015291005835373e-06
Iter: 1850 loss: 4.4019421773323047e-06
Iter: 1851 loss: 4.4002486800587674e-06
Iter: 1852 loss: 4.3991746456958354e-06
Iter: 1853 loss: 4.3973322732080092e-06
Iter: 1854 loss: 4.3973304194956571e-06
Iter: 1855 loss: 4.3958696769390955e-06
Iter: 1856 loss: 4.4118503586886215e-06
Iter: 1857 loss: 4.3958383922547687e-06
Iter: 1858 loss: 4.3940762793524225e-06
Iter: 1859 loss: 4.3921903526600477e-06
Iter: 1860 loss: 4.3918855040124151e-06
Iter: 1861 loss: 4.3902718074769355e-06
Iter: 1862 loss: 4.409948493058737e-06
Iter: 1863 loss: 4.390253773623983e-06
Iter: 1864 loss: 4.3886810308033337e-06
Iter: 1865 loss: 4.3886199788388185e-06
Iter: 1866 loss: 4.3874043519144118e-06
Iter: 1867 loss: 4.3858222963807718e-06
Iter: 1868 loss: 4.3913897563676069e-06
Iter: 1869 loss: 4.3854126307149795e-06
Iter: 1870 loss: 4.3832478206464454e-06
Iter: 1871 loss: 4.3860585159478483e-06
Iter: 1872 loss: 4.38214394010972e-06
Iter: 1873 loss: 4.3804335177259809e-06
Iter: 1874 loss: 4.3795884917743679e-06
Iter: 1875 loss: 4.3787744102351495e-06
Iter: 1876 loss: 4.3769400275521689e-06
Iter: 1877 loss: 4.3768440915008956e-06
Iter: 1878 loss: 4.375832280215989e-06
Iter: 1879 loss: 4.3736441906061143e-06
Iter: 1880 loss: 4.40732735887095e-06
Iter: 1881 loss: 4.373561656728332e-06
Iter: 1882 loss: 4.3712179849419961e-06
Iter: 1883 loss: 4.3727592593424279e-06
Iter: 1884 loss: 4.3697374855414152e-06
Iter: 1885 loss: 4.36931236363705e-06
Iter: 1886 loss: 4.3683000040678056e-06
Iter: 1887 loss: 4.3670980345474962e-06
Iter: 1888 loss: 4.364927724858853e-06
Iter: 1889 loss: 4.4173757457824838e-06
Iter: 1890 loss: 4.3649272944945076e-06
Iter: 1891 loss: 4.3636429932972158e-06
Iter: 1892 loss: 4.3636218787703468e-06
Iter: 1893 loss: 4.3622180169970743e-06
Iter: 1894 loss: 4.3602087196786768e-06
Iter: 1895 loss: 4.3601422745254118e-06
Iter: 1896 loss: 4.35840153291319e-06
Iter: 1897 loss: 4.3738372418501333e-06
Iter: 1898 loss: 4.3583155212507312e-06
Iter: 1899 loss: 4.3563148295612284e-06
Iter: 1900 loss: 4.3552390413245456e-06
Iter: 1901 loss: 4.3543393912921809e-06
Iter: 1902 loss: 4.3525862465925325e-06
Iter: 1903 loss: 4.3682324744467453e-06
Iter: 1904 loss: 4.3525013605550735e-06
Iter: 1905 loss: 4.3507309258155148e-06
Iter: 1906 loss: 4.3531608283170392e-06
Iter: 1907 loss: 4.3498495447653006e-06
Iter: 1908 loss: 4.3484539766130129e-06
Iter: 1909 loss: 4.347621443154983e-06
Iter: 1910 loss: 4.3470421466778333e-06
Iter: 1911 loss: 4.34508641765996e-06
Iter: 1912 loss: 4.3450832945637833e-06
Iter: 1913 loss: 4.3439908278280714e-06
Iter: 1914 loss: 4.3416161124898443e-06
Iter: 1915 loss: 4.3776225819739785e-06
Iter: 1916 loss: 4.3415211535143911e-06
Iter: 1917 loss: 4.3389664564164858e-06
Iter: 1918 loss: 4.3413202279591005e-06
Iter: 1919 loss: 4.3374910745898949e-06
Iter: 1920 loss: 4.3368933526688439e-06
Iter: 1921 loss: 4.3358896542599631e-06
Iter: 1922 loss: 4.3346000329567475e-06
Iter: 1923 loss: 4.3319732998028593e-06
Iter: 1924 loss: 4.3796064748838043e-06
Iter: 1925 loss: 4.3319286728126949e-06
Iter: 1926 loss: 4.3305173091699922e-06
Iter: 1927 loss: 4.3304086914467366e-06
Iter: 1928 loss: 4.3289133534442372e-06
Iter: 1929 loss: 4.3269686507926549e-06
Iter: 1930 loss: 4.3268408893494936e-06
Iter: 1931 loss: 4.3253394274032172e-06
Iter: 1932 loss: 4.3409647392162252e-06
Iter: 1933 loss: 4.325298690252919e-06
Iter: 1934 loss: 4.3236067857338286e-06
Iter: 1935 loss: 4.3224102683537158e-06
Iter: 1936 loss: 4.3218140294292388e-06
Iter: 1937 loss: 4.3202816413498337e-06
Iter: 1938 loss: 4.3339167416396536e-06
Iter: 1939 loss: 4.3202067278004327e-06
Iter: 1940 loss: 4.3186004707733036e-06
Iter: 1941 loss: 4.3201912887520241e-06
Iter: 1942 loss: 4.3176940292878646e-06
Iter: 1943 loss: 4.3162260664411836e-06
Iter: 1944 loss: 4.3152915621201318e-06
Iter: 1945 loss: 4.3147160722944746e-06
Iter: 1946 loss: 4.3125043946214091e-06
Iter: 1947 loss: 4.3471440717944616e-06
Iter: 1948 loss: 4.312504233293332e-06
Iter: 1949 loss: 4.3112777445674836e-06
Iter: 1950 loss: 4.3084255865401237e-06
Iter: 1951 loss: 4.3434627902001055e-06
Iter: 1952 loss: 4.3082031666739277e-06
Iter: 1953 loss: 4.3054272288804259e-06
Iter: 1954 loss: 4.3182883460317236e-06
Iter: 1955 loss: 4.3049079644172558e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.4
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi0.4
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.4 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi0.4
+ date
Sun Nov  8 04:45:36 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.4/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0/300_300_300_1 --function f1 --psi 3 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.4/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5d50510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5e372f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5e37950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5d8cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5d79b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5c75950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5d79c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5d79730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5c36510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5c360d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0dba22510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5cfbf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5cf56a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0e5bf28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db964950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db971c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db94d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db906d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db8d76a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db8d7d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db9ce2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db9cebf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db89e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db815950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db815a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db7ec598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db7989d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db798378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db7ae730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db74c7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db8526a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db99e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db9b90d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db99de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db6f9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fc0db6def28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.008016715468663036
test_loss: 0.008497806543780675
train_loss: 0.007777031966166043
test_loss: 0.007446747726356141
train_loss: 0.007563206073708899
test_loss: 0.007278861711206461
train_loss: 0.006531181992392307
test_loss: 0.0066796048627312575
train_loss: 0.006258356909205508
test_loss: 0.00646012946372971
train_loss: 0.006407678069632997
test_loss: 0.006793118316458651
train_loss: 0.0060214404675245714
test_loss: 0.006514780810213289
train_loss: 0.006349601872180853
test_loss: 0.006414125997343979
train_loss: 0.006159205108499261
test_loss: 0.0068231194719932066
train_loss: 0.005577042873224041
test_loss: 0.006127533075077273
train_loss: 0.005887136311083849
test_loss: 0.006581417209409806
train_loss: 0.006057522982340361
test_loss: 0.006456792986189735
train_loss: 0.005752957643354158
test_loss: 0.006153647496865394
train_loss: 0.006248191063496076
test_loss: 0.006333935482563844
train_loss: 0.005712749708927182
test_loss: 0.005950902198463485
train_loss: 0.005943445327423988
test_loss: 0.006325119473627514
train_loss: 0.0062787380876704385
test_loss: 0.0058882972967901825
train_loss: 0.005591670945228394
test_loss: 0.006062517035373439
train_loss: 0.005870230575508159
test_loss: 0.006232106716724008
train_loss: 0.006285051623429413
test_loss: 0.006570762305257432
train_loss: 0.00633992309720271
test_loss: 0.006088099725880177
train_loss: 0.005368025072060462
test_loss: 0.00581137307635486
train_loss: 0.005669179741300099
test_loss: 0.005945429734238001
train_loss: 0.005074986692776624
test_loss: 0.005734139359973397
train_loss: 0.005490403281776876
test_loss: 0.0058606236904040895
train_loss: 0.00579064477407138
test_loss: 0.006184464676756435
train_loss: 0.005383285025351377
test_loss: 0.0054660830930778875
train_loss: 0.005281732675215788
test_loss: 0.005490791620842403
train_loss: 0.005628969285339053
test_loss: 0.0059518071436718886
train_loss: 0.005277621318987467
test_loss: 0.005681550270143583
train_loss: 0.004855799211582916
test_loss: 0.005787290163564511
train_loss: 0.005098657374352073
test_loss: 0.0054577949771001605
train_loss: 0.005141345732210525
test_loss: 0.00551086340639684
train_loss: 0.005247259382415739
test_loss: 0.0058543272235847205
train_loss: 0.005107476951894242
test_loss: 0.005425322777584851
train_loss: 0.005407849426348185
test_loss: 0.0056377869334358136
train_loss: 0.005757896662839628
test_loss: 0.005762282927693371
train_loss: 0.005062597087069891
test_loss: 0.005259635324109158
train_loss: 0.0053155062852676335
test_loss: 0.006092886802114798
train_loss: 0.005191527050863736
test_loss: 0.005593192915496203
train_loss: 0.005050451880177167
test_loss: 0.005277019657394496
train_loss: 0.005306727802008696
test_loss: 0.005476409317080292
train_loss: 0.005025615193380848
test_loss: 0.0054499601820108285
train_loss: 0.004636379657967971
test_loss: 0.005275632962032718
train_loss: 0.004514944306896815
test_loss: 0.00539059378045366
train_loss: 0.004531413929762674
test_loss: 0.005211620808088495
train_loss: 0.004677391322390294
test_loss: 0.00509156724885325
train_loss: 0.005050210839585209
test_loss: 0.005267896613990113
train_loss: 0.004817182352099846
test_loss: 0.005349822702776413
train_loss: 0.004815278382376139
test_loss: 0.0052217344485576845
train_loss: 0.004821000674192427
test_loss: 0.0053177517358361775
train_loss: 0.004868378705754231
test_loss: 0.005214167602702501
train_loss: 0.00463161131009477
test_loss: 0.004964049472043718
train_loss: 0.004789288475496327
test_loss: 0.0050678552116017045
train_loss: 0.004596622884945053
test_loss: 0.0051814277356057375
train_loss: 0.004401611621491086
test_loss: 0.005130106509975081
train_loss: 0.004432353939346419
test_loss: 0.005106717286569898
train_loss: 0.0046615991867943974
test_loss: 0.0049417675072211285
train_loss: 0.00451351330053076
test_loss: 0.005272929921368189
train_loss: 0.004761090989278815
test_loss: 0.00510499624998456
train_loss: 0.004606414661187121
test_loss: 0.004943181688012218
train_loss: 0.004874129269138078
test_loss: 0.005148934994209487
train_loss: 0.00438656076974661
test_loss: 0.00483419106498471
train_loss: 0.004402067753067475
test_loss: 0.0049623153154218546
train_loss: 0.004282602046182779
test_loss: 0.0049821311931444266
train_loss: 0.004546873664854213
test_loss: 0.004991993200270971
train_loss: 0.00477028355940748
test_loss: 0.005274946433065628
train_loss: 0.004414239178746868
test_loss: 0.005095798635563225
train_loss: 0.004685617953937695
test_loss: 0.0052023053608957785
train_loss: 0.004325078710451956
test_loss: 0.0049493432993673436
train_loss: 0.004617754669037141
test_loss: 0.004766729065917605
train_loss: 0.004248641181590139
test_loss: 0.004921618002730448
train_loss: 0.004453870952714056
test_loss: 0.004906118468255599
train_loss: 0.003980801256247699
test_loss: 0.004759552663615327
train_loss: 0.004700835179204402
test_loss: 0.0051683348837036685
train_loss: 0.004596046631758718
test_loss: 0.005019807713662662
train_loss: 0.004275413643163406
test_loss: 0.00482348266076663
train_loss: 0.0043663595820486005
test_loss: 0.004802710683767329
train_loss: 0.0041956146891879735
test_loss: 0.005234964483249341
train_loss: 0.004394371490507539
test_loss: 0.0051114402680217
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi0.4/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.4/300_300_300_1 --optimizer lbfgs --function f1 --psi 3 --phi 0.4 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi0.4/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e4de2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e492bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e492d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e5491e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e5490d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e549158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e406ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e3ac730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e39e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e37a378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e39e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e319488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e32b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e32b598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e277a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e28bea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e256510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e256c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e2168c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e216598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e1edae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e187268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e132730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e169840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e0fc8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e11fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f143e0d08c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1423c16400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1423c160d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1423c28400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1423c28268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1423bc2158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1423ba9488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1423ba9b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1423b7d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f1423b2be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 3.4222310703709084e-05
Iter: 2 loss: 6.5442586078190158e-05
Iter: 3 loss: 2.6277459197815382e-05
Iter: 4 loss: 2.4736884587001964e-05
Iter: 5 loss: 2.1650964273787748e-05
Iter: 6 loss: 7.9992224593686858e-05
Iter: 7 loss: 2.1611237528424765e-05
Iter: 8 loss: 2.0128502813187084e-05
Iter: 9 loss: 2.0014971491065957e-05
Iter: 10 loss: 1.8676438513815126e-05
Iter: 11 loss: 1.768785553840759e-05
Iter: 12 loss: 1.7239703313727121e-05
Iter: 13 loss: 1.6493574859474665e-05
Iter: 14 loss: 2.4367161319005076e-05
Iter: 15 loss: 1.6474389479884991e-05
Iter: 16 loss: 1.5619180328900572e-05
Iter: 17 loss: 1.5457249975172744e-05
Iter: 18 loss: 1.488362494213496e-05
Iter: 19 loss: 1.3892157608694472e-05
Iter: 20 loss: 1.6636295136675097e-05
Iter: 21 loss: 1.3573951011001364e-05
Iter: 22 loss: 1.2855404894962459e-05
Iter: 23 loss: 1.5384031978628432e-05
Iter: 24 loss: 1.2668348658229927e-05
Iter: 25 loss: 1.2161820299449844e-05
Iter: 26 loss: 1.6333374781609455e-05
Iter: 27 loss: 1.2130655900164497e-05
Iter: 28 loss: 1.1561937099070423e-05
Iter: 29 loss: 1.1560358236818715e-05
Iter: 30 loss: 1.1106707256421861e-05
Iter: 31 loss: 1.0558787625457123e-05
Iter: 32 loss: 9.720042633717647e-06
Iter: 33 loss: 9.70633517230051e-06
Iter: 34 loss: 9.3908720974768584e-06
Iter: 35 loss: 9.2555732643346368e-06
Iter: 36 loss: 8.8571641927191964e-06
Iter: 37 loss: 8.8576465852205915e-06
Iter: 38 loss: 8.538604099762386e-06
Iter: 39 loss: 8.1887421607187723e-06
Iter: 40 loss: 8.5386074911347384e-06
Iter: 41 loss: 7.9919939222724563e-06
Iter: 42 loss: 7.81908739471009e-06
Iter: 43 loss: 7.7578976450648841e-06
Iter: 44 loss: 7.6335475072013951e-06
Iter: 45 loss: 7.3469600720949458e-06
Iter: 46 loss: 1.0977603637712002e-05
Iter: 47 loss: 7.3263581039838686e-06
Iter: 48 loss: 7.08935532766457e-06
Iter: 49 loss: 7.0856219371207918e-06
Iter: 50 loss: 6.8765587207147456e-06
Iter: 51 loss: 6.5933907536718412e-06
Iter: 52 loss: 6.5790677105176983e-06
Iter: 53 loss: 6.380782887777579e-06
Iter: 54 loss: 8.153966207558587e-06
Iter: 55 loss: 6.3711281972003017e-06
Iter: 56 loss: 6.2227356604737869e-06
Iter: 57 loss: 6.399896776202189e-06
Iter: 58 loss: 6.1445469739582876e-06
Iter: 59 loss: 6.0198756585402928e-06
Iter: 60 loss: 6.228835379199534e-06
Iter: 61 loss: 5.963548120603161e-06
Iter: 62 loss: 5.8308907977267322e-06
Iter: 63 loss: 7.8662177189686257e-06
Iter: 64 loss: 5.8308517582681555e-06
Iter: 65 loss: 5.74249546054988e-06
Iter: 66 loss: 5.6053848451741615e-06
Iter: 67 loss: 5.6035178341999725e-06
Iter: 68 loss: 5.475805598458584e-06
Iter: 69 loss: 6.0958024561375274e-06
Iter: 70 loss: 5.4534074070929869e-06
Iter: 71 loss: 5.3174366206470743e-06
Iter: 72 loss: 6.6025777165819558e-06
Iter: 73 loss: 5.3120120874633571e-06
Iter: 74 loss: 5.26839184320657e-06
Iter: 75 loss: 5.1948712264079281e-06
Iter: 76 loss: 5.1947086153186513e-06
Iter: 77 loss: 5.1303756743966728e-06
Iter: 78 loss: 5.1277777470868515e-06
Iter: 79 loss: 5.0758808852978879e-06
Iter: 80 loss: 4.9537090656626591e-06
Iter: 81 loss: 6.3948530847818656e-06
Iter: 82 loss: 4.9431138106998077e-06
Iter: 83 loss: 4.8968926425160877e-06
Iter: 84 loss: 4.8798476403915353e-06
Iter: 85 loss: 4.8187251308349361e-06
Iter: 86 loss: 4.7391465845400738e-06
Iter: 87 loss: 4.733951410689852e-06
Iter: 88 loss: 4.6595589951753645e-06
Iter: 89 loss: 4.8303449934755084e-06
Iter: 90 loss: 4.6318312365349221e-06
Iter: 91 loss: 4.5709342142085738e-06
Iter: 92 loss: 5.148196511594761e-06
Iter: 93 loss: 4.568515802372054e-06
Iter: 94 loss: 4.5192561037391512e-06
Iter: 95 loss: 4.7358814698414233e-06
Iter: 96 loss: 4.5093843791154292e-06
Iter: 97 loss: 4.4648928782528012e-06
Iter: 98 loss: 4.6567094518782354e-06
Iter: 99 loss: 4.4557641517059544e-06
Iter: 100 loss: 4.4183548625244573e-06
Iter: 101 loss: 4.3402811200067287e-06
Iter: 102 loss: 5.6695338656852116e-06
Iter: 103 loss: 4.3383911514943687e-06
Iter: 104 loss: 4.32203918740809e-06
Iter: 105 loss: 4.3067618089423355e-06
Iter: 106 loss: 4.2760999124514941e-06
Iter: 107 loss: 4.3168239161263628e-06
Iter: 108 loss: 4.2606165762030646e-06
Iter: 109 loss: 4.23343691131437e-06
Iter: 110 loss: 4.1885191506344709e-06
Iter: 111 loss: 4.1883351392054231e-06
Iter: 112 loss: 4.1704239767301912e-06
Iter: 113 loss: 4.1576681744660849e-06
Iter: 114 loss: 4.1399528388485334e-06
Iter: 115 loss: 4.0939402804829946e-06
Iter: 116 loss: 4.4603282026199848e-06
Iter: 117 loss: 4.0853521964651148e-06
Iter: 118 loss: 4.0505224406778556e-06
Iter: 119 loss: 4.0491145256462031e-06
Iter: 120 loss: 4.01382621526886e-06
Iter: 121 loss: 4.0173323157884251e-06
Iter: 122 loss: 3.986617888629878e-06
Iter: 123 loss: 3.9599631770381528e-06
Iter: 124 loss: 3.9385657648257434e-06
Iter: 125 loss: 3.9305390503908378e-06
Iter: 126 loss: 3.9060869996485127e-06
Iter: 127 loss: 3.9056136447561847e-06
Iter: 128 loss: 3.88168505606488e-06
Iter: 129 loss: 3.8602767887886582e-06
Iter: 130 loss: 3.8541933062920395e-06
Iter: 131 loss: 3.8127239897253774e-06
Iter: 132 loss: 4.0476427438231616e-06
Iter: 133 loss: 3.8069791270808289e-06
Iter: 134 loss: 3.7824989125865496e-06
Iter: 135 loss: 3.7925669169394415e-06
Iter: 136 loss: 3.7656458851287709e-06
Iter: 137 loss: 3.7448503581617761e-06
Iter: 138 loss: 3.9948041710601828e-06
Iter: 139 loss: 3.7445933563055677e-06
Iter: 140 loss: 3.720768177423096e-06
Iter: 141 loss: 3.7357541064381448e-06
Iter: 142 loss: 3.7055664915966223e-06
Iter: 143 loss: 3.6904375342401557e-06
Iter: 144 loss: 3.6923991833809807e-06
Iter: 145 loss: 3.6789034196135185e-06
Iter: 146 loss: 3.6558939256161329e-06
Iter: 147 loss: 3.8855577643800952e-06
Iter: 148 loss: 3.6551486752269697e-06
Iter: 149 loss: 3.638410423560023e-06
Iter: 150 loss: 3.6057919344196951e-06
Iter: 151 loss: 4.2667700459848723e-06
Iter: 152 loss: 3.6055504856885553e-06
Iter: 153 loss: 3.6013297794052954e-06
Iter: 154 loss: 3.5932404003109413e-06
Iter: 155 loss: 3.5819496479353884e-06
Iter: 156 loss: 3.5596190274934571e-06
Iter: 157 loss: 3.9958421581118013e-06
Iter: 158 loss: 3.5593928072488896e-06
Iter: 159 loss: 3.540068799406305e-06
Iter: 160 loss: 3.5686236744778741e-06
Iter: 161 loss: 3.5307710247149586e-06
Iter: 162 loss: 3.5141791176526183e-06
Iter: 163 loss: 3.5139633038978224e-06
Iter: 164 loss: 3.4977788901334052e-06
Iter: 165 loss: 3.4967372389353223e-06
Iter: 166 loss: 3.4845116984395416e-06
Iter: 167 loss: 3.4676533908508369e-06
Iter: 168 loss: 3.5200671720157966e-06
Iter: 169 loss: 3.4627315786885242e-06
Iter: 170 loss: 3.4451933867628557e-06
Iter: 171 loss: 3.44394638140479e-06
Iter: 172 loss: 3.4307800084962251e-06
Iter: 173 loss: 3.4305633085580085e-06
Iter: 174 loss: 3.4218270783584061e-06
Iter: 175 loss: 3.416003912214516e-06
Iter: 176 loss: 3.4022763243702518e-06
Iter: 177 loss: 3.56318242321859e-06
Iter: 178 loss: 3.4010702875039514e-06
Iter: 179 loss: 3.386259118659279e-06
Iter: 180 loss: 3.4644583791271883e-06
Iter: 181 loss: 3.3839660966143851e-06
Iter: 182 loss: 3.3650244027654427e-06
Iter: 183 loss: 3.414604768348188e-06
Iter: 184 loss: 3.3586162781573989e-06
Iter: 185 loss: 3.3481059255197762e-06
Iter: 186 loss: 3.3391747430742836e-06
Iter: 187 loss: 3.3362653358093843e-06
Iter: 188 loss: 3.3274230292140453e-06
Iter: 189 loss: 3.325615693697995e-06
Iter: 190 loss: 3.3201110697975019e-06
Iter: 191 loss: 3.3080130838439231e-06
Iter: 192 loss: 3.48564285221718e-06
Iter: 193 loss: 3.3074676102169769e-06
Iter: 194 loss: 3.2946579373124342e-06
Iter: 195 loss: 3.3279963344786234e-06
Iter: 196 loss: 3.2903041286932408e-06
Iter: 197 loss: 3.2841460774792879e-06
Iter: 198 loss: 3.2827104902656813e-06
Iter: 199 loss: 3.2766762891389813e-06
Iter: 200 loss: 3.2614953865679869e-06
Iter: 201 loss: 3.4004488342147207e-06
Iter: 202 loss: 3.25925247531729e-06
Iter: 203 loss: 3.2445054975687445e-06
Iter: 204 loss: 3.4309828270918589e-06
Iter: 205 loss: 3.2443797339645434e-06
Iter: 206 loss: 3.2360940944830423e-06
Iter: 207 loss: 3.2706905068487685e-06
Iter: 208 loss: 3.2343228622655192e-06
Iter: 209 loss: 3.2262007636985977e-06
Iter: 210 loss: 3.285427115207447e-06
Iter: 211 loss: 3.2255192834972528e-06
Iter: 212 loss: 3.2199564497678188e-06
Iter: 213 loss: 3.2068369638857871e-06
Iter: 214 loss: 3.3604489336166074e-06
Iter: 215 loss: 3.2056804266628524e-06
Iter: 216 loss: 3.2023426373610616e-06
Iter: 217 loss: 3.1988231633060474e-06
Iter: 218 loss: 3.1924753706302193e-06
Iter: 219 loss: 3.1785265631445145e-06
Iter: 220 loss: 3.3834464251488625e-06
Iter: 221 loss: 3.1778987803366324e-06
Iter: 222 loss: 3.1681625542270285e-06
Iter: 223 loss: 3.2781701660620174e-06
Iter: 224 loss: 3.1679867015670207e-06
Iter: 225 loss: 3.1594950525157158e-06
Iter: 226 loss: 3.211910540332647e-06
Iter: 227 loss: 3.1584872243978282e-06
Iter: 228 loss: 3.1542423437041997e-06
Iter: 229 loss: 3.1462617175398554e-06
Iter: 230 loss: 3.322372234430801e-06
Iter: 231 loss: 3.1462405241149505e-06
Iter: 232 loss: 3.13756403440223e-06
Iter: 233 loss: 3.1808727728509507e-06
Iter: 234 loss: 3.1361025921299645e-06
Iter: 235 loss: 3.1263728858341039e-06
Iter: 236 loss: 3.2015115467933403e-06
Iter: 237 loss: 3.1256622026799827e-06
Iter: 238 loss: 3.1198712166266123e-06
Iter: 239 loss: 3.1130614921789353e-06
Iter: 240 loss: 3.1123092308165004e-06
Iter: 241 loss: 3.1060446242715682e-06
Iter: 242 loss: 3.1635779537221842e-06
Iter: 243 loss: 3.105769220710986e-06
Iter: 244 loss: 3.099962150877256e-06
Iter: 245 loss: 3.1255263096119419e-06
Iter: 246 loss: 3.0988002438531951e-06
Iter: 247 loss: 3.0927601768516194e-06
Iter: 248 loss: 3.1057828276167345e-06
Iter: 249 loss: 3.0904090298336826e-06
Iter: 250 loss: 3.0863730285962983e-06
Iter: 251 loss: 3.0802222394328203e-06
Iter: 252 loss: 3.0801159361601148e-06
Iter: 253 loss: 3.0713403441385885e-06
Iter: 254 loss: 3.1877860042162325e-06
Iter: 255 loss: 3.0712929279890448e-06
Iter: 256 loss: 3.0662365742082349e-06
Iter: 257 loss: 3.0589670464851321e-06
Iter: 258 loss: 3.05873596921552e-06
Iter: 259 loss: 3.0546951060904597e-06
Iter: 260 loss: 3.0544755615465682e-06
Iter: 261 loss: 3.049893699235567e-06
Iter: 262 loss: 3.0436681268494694e-06
Iter: 263 loss: 3.0433597027827432e-06
Iter: 264 loss: 3.0374800682228682e-06
Iter: 265 loss: 3.0351866182061997e-06
Iter: 266 loss: 3.0320109833500669e-06
Iter: 267 loss: 3.0284474103864e-06
Iter: 268 loss: 3.0272560860046023e-06
Iter: 269 loss: 3.0226216998410797e-06
Iter: 270 loss: 3.0201498344481781e-06
Iter: 271 loss: 3.01805373406394e-06
Iter: 272 loss: 3.0129084772718168e-06
Iter: 273 loss: 3.0089900918453275e-06
Iter: 274 loss: 3.0073290776929245e-06
Iter: 275 loss: 3.0055345861182998e-06
Iter: 276 loss: 3.0036209824841484e-06
Iter: 277 loss: 3.0010679862608138e-06
Iter: 278 loss: 3.0072234647280754e-06
Iter: 279 loss: 3.0001486234082828e-06
Iter: 280 loss: 2.9971760989128861e-06
Iter: 281 loss: 2.9908361890564913e-06
Iter: 282 loss: 3.0923204446889953e-06
Iter: 283 loss: 2.9906329396999583e-06
Iter: 284 loss: 2.9848141961018371e-06
Iter: 285 loss: 3.0773023155051596e-06
Iter: 286 loss: 2.9848141566504907e-06
Iter: 287 loss: 2.9798142060289422e-06
Iter: 288 loss: 2.9950632708873817e-06
Iter: 289 loss: 2.9783271905494272e-06
Iter: 290 loss: 2.9756090295264852e-06
Iter: 291 loss: 2.9715785638402718e-06
Iter: 292 loss: 2.9714834172898349e-06
Iter: 293 loss: 2.968088240946595e-06
Iter: 294 loss: 2.967723721880049e-06
Iter: 295 loss: 2.9655358270497313e-06
Iter: 296 loss: 2.9606119624388495e-06
Iter: 297 loss: 3.0278581812823407e-06
Iter: 298 loss: 2.9603290027909439e-06
Iter: 299 loss: 2.9542087405759324e-06
Iter: 300 loss: 2.9566749138510139e-06
Iter: 301 loss: 2.9499818231624905e-06
Iter: 302 loss: 2.9468371686972733e-06
Iter: 303 loss: 2.945220274729122e-06
Iter: 304 loss: 2.9419917089472682e-06
Iter: 305 loss: 2.9346475375902327e-06
Iter: 306 loss: 3.0316015861489553e-06
Iter: 307 loss: 2.9341795484179071e-06
Iter: 308 loss: 2.929945104486483e-06
Iter: 309 loss: 2.9733845523218997e-06
Iter: 310 loss: 2.9298225924065581e-06
Iter: 311 loss: 2.9270085223052083e-06
Iter: 312 loss: 2.92700787371959e-06
Iter: 313 loss: 2.924788747579126e-06
Iter: 314 loss: 2.9212467140730345e-06
Iter: 315 loss: 2.9212158956602057e-06
Iter: 316 loss: 2.916605227599248e-06
Iter: 317 loss: 2.9333273215862008e-06
Iter: 318 loss: 2.9154478685753544e-06
Iter: 319 loss: 2.9122991081691294e-06
Iter: 320 loss: 2.9536841722339106e-06
Iter: 321 loss: 2.9122803448943125e-06
Iter: 322 loss: 2.9094569831916791e-06
Iter: 323 loss: 2.9048090446977975e-06
Iter: 324 loss: 2.9047880499641563e-06
Iter: 325 loss: 2.90163189486532e-06
Iter: 326 loss: 2.9366368512100497e-06
Iter: 327 loss: 2.9015689015079022e-06
Iter: 328 loss: 2.898801305599508e-06
Iter: 329 loss: 2.9131516274162107e-06
Iter: 330 loss: 2.8983607720698748e-06
Iter: 331 loss: 2.8965801443352814e-06
Iter: 332 loss: 2.8924613704202098e-06
Iter: 333 loss: 2.9439465970964671e-06
Iter: 334 loss: 2.8921550500931777e-06
Iter: 335 loss: 2.8878368435551429e-06
Iter: 336 loss: 2.9170798172665748e-06
Iter: 337 loss: 2.8874102197569806e-06
Iter: 338 loss: 2.8822407511111606e-06
Iter: 339 loss: 2.9080343145625668e-06
Iter: 340 loss: 2.8813694145769263e-06
Iter: 341 loss: 2.87884361250515e-06
Iter: 342 loss: 2.8757950705914778e-06
Iter: 343 loss: 2.8754967342029055e-06
Iter: 344 loss: 2.87199638893842e-06
Iter: 345 loss: 2.8987136755857703e-06
Iter: 346 loss: 2.8717330966799889e-06
Iter: 347 loss: 2.8678374232362508e-06
Iter: 348 loss: 2.8866766720872994e-06
Iter: 349 loss: 2.8671505497292707e-06
Iter: 350 loss: 2.8650557763536727e-06
Iter: 351 loss: 2.8632286159812454e-06
Iter: 352 loss: 2.8626724207959408e-06
Iter: 353 loss: 2.8586745608210861e-06
Iter: 354 loss: 2.8763338852078194e-06
Iter: 355 loss: 2.8578780432852928e-06
Iter: 356 loss: 2.8539167409545638e-06
Iter: 357 loss: 2.8735880976152244e-06
Iter: 358 loss: 2.853244470507265e-06
Iter: 359 loss: 2.8507098865274772e-06
Iter: 360 loss: 2.8516189769038474e-06
Iter: 361 loss: 2.8489314145761215e-06
Iter: 362 loss: 2.8468187525407088e-06
Iter: 363 loss: 2.8668691758685104e-06
Iter: 364 loss: 2.8467358006243959e-06
Iter: 365 loss: 2.8443183695430248e-06
Iter: 366 loss: 2.8425067782734414e-06
Iter: 367 loss: 2.8417104706839659e-06
Iter: 368 loss: 2.8388704657192622e-06
Iter: 369 loss: 2.8350688861485257e-06
Iter: 370 loss: 2.8348605003721291e-06
Iter: 371 loss: 2.8333704488327552e-06
Iter: 372 loss: 2.8322328892940872e-06
Iter: 373 loss: 2.8297240190303079e-06
Iter: 374 loss: 2.8289960388534591e-06
Iter: 375 loss: 2.8274807834582159e-06
Iter: 376 loss: 2.8247866791004281e-06
Iter: 377 loss: 2.8222827742713678e-06
Iter: 378 loss: 2.8216434113958264e-06
Iter: 379 loss: 2.8220285968100991e-06
Iter: 380 loss: 2.8202555728226843e-06
Iter: 381 loss: 2.8188279601510995e-06
Iter: 382 loss: 2.8155660444808939e-06
Iter: 383 loss: 2.8580268976549031e-06
Iter: 384 loss: 2.8153493494095889e-06
Iter: 385 loss: 2.8114755848982615e-06
Iter: 386 loss: 2.8212333593383454e-06
Iter: 387 loss: 2.8101255553309604e-06
Iter: 388 loss: 2.806737747406666e-06
Iter: 389 loss: 2.8067375786372435e-06
Iter: 390 loss: 2.8047441155824182e-06
Iter: 391 loss: 2.8049073068605e-06
Iter: 392 loss: 2.803197065938861e-06
Iter: 393 loss: 2.8004357508685177e-06
Iter: 394 loss: 2.8025733010911684e-06
Iter: 395 loss: 2.798759718796175e-06
Iter: 396 loss: 2.7967984868870009e-06
Iter: 397 loss: 2.7967206556598492e-06
Iter: 398 loss: 2.7953173137390313e-06
Iter: 399 loss: 2.7924870763761318e-06
Iter: 400 loss: 2.8451387846191889e-06
Iter: 401 loss: 2.7924461585413791e-06
Iter: 402 loss: 2.7894316043814653e-06
Iter: 403 loss: 2.79123947271297e-06
Iter: 404 loss: 2.7874887987662622e-06
Iter: 405 loss: 2.7860313003265152e-06
Iter: 406 loss: 2.785263534267445e-06
Iter: 407 loss: 2.7836894757823202e-06
Iter: 408 loss: 2.7802711997994429e-06
Iter: 409 loss: 2.832246304232785e-06
Iter: 410 loss: 2.7801359744036672e-06
Iter: 411 loss: 2.7774999764637294e-06
Iter: 412 loss: 2.7899087813613152e-06
Iter: 413 loss: 2.7770174537765847e-06
Iter: 414 loss: 2.7752457094330755e-06
Iter: 415 loss: 2.7751832617101188e-06
Iter: 416 loss: 2.7738366558200449e-06
Iter: 417 loss: 2.7704173416715988e-06
Iter: 418 loss: 2.8005364896938634e-06
Iter: 419 loss: 2.7698762486610268e-06
Iter: 420 loss: 2.7679681158112154e-06
Iter: 421 loss: 2.7678565059398816e-06
Iter: 422 loss: 2.7655551625758638e-06
Iter: 423 loss: 2.7643054019931241e-06
Iter: 424 loss: 2.763277675527942e-06
Iter: 425 loss: 2.7607778505604349e-06
Iter: 426 loss: 2.7771898933621658e-06
Iter: 427 loss: 2.7605146765497167e-06
Iter: 428 loss: 2.7590054423242618e-06
Iter: 429 loss: 2.7650230551792208e-06
Iter: 430 loss: 2.7586645887914416e-06
Iter: 431 loss: 2.7568844277468533e-06
Iter: 432 loss: 2.7596671580201845e-06
Iter: 433 loss: 2.7560506856865874e-06
Iter: 434 loss: 2.75423422978027e-06
Iter: 435 loss: 2.7513185480211167e-06
Iter: 436 loss: 2.7512956703892188e-06
Iter: 437 loss: 2.7480352632889381e-06
Iter: 438 loss: 2.7722371763206829e-06
Iter: 439 loss: 2.7477728393030714e-06
Iter: 440 loss: 2.7447106779532562e-06
Iter: 441 loss: 2.7721131932281191e-06
Iter: 442 loss: 2.7445636977600095e-06
Iter: 443 loss: 2.7432114623013139e-06
Iter: 444 loss: 2.7405321473624061e-06
Iter: 445 loss: 2.7926392602245589e-06
Iter: 446 loss: 2.7405040194718818e-06
Iter: 447 loss: 2.738224104706739e-06
Iter: 448 loss: 2.7683989921388721e-06
Iter: 449 loss: 2.7382114693808822e-06
Iter: 450 loss: 2.7359198221537166e-06
Iter: 451 loss: 2.7459522880695331e-06
Iter: 452 loss: 2.735457965946618e-06
Iter: 453 loss: 2.7342668334422932e-06
Iter: 454 loss: 2.7310568137297014e-06
Iter: 455 loss: 2.7523138060268177e-06
Iter: 456 loss: 2.7302794663215381e-06
Iter: 457 loss: 2.7285812191200425e-06
Iter: 458 loss: 2.7276776072512155e-06
Iter: 459 loss: 2.7262028204039961e-06
Iter: 460 loss: 2.724692202969493e-06
Iter: 461 loss: 2.7244073264524721e-06
Iter: 462 loss: 2.7226437905390423e-06
Iter: 463 loss: 2.7349311033805377e-06
Iter: 464 loss: 2.7224797395843317e-06
Iter: 465 loss: 2.7208075119310435e-06
Iter: 466 loss: 2.7255299413679745e-06
Iter: 467 loss: 2.72027570615272e-06
Iter: 468 loss: 2.7184423362853234e-06
Iter: 469 loss: 2.7197995723082068e-06
Iter: 470 loss: 2.7173166469520615e-06
Iter: 471 loss: 2.7154028482172915e-06
Iter: 472 loss: 2.7119265390729533e-06
Iter: 473 loss: 2.79483174748671e-06
Iter: 474 loss: 2.7119252113930922e-06
Iter: 475 loss: 2.712164948927623e-06
Iter: 476 loss: 2.710139439802386e-06
Iter: 477 loss: 2.7086905419103621e-06
Iter: 478 loss: 2.7057044694943752e-06
Iter: 479 loss: 2.7581997104482229e-06
Iter: 480 loss: 2.7056438101100659e-06
Iter: 481 loss: 2.7030079260960344e-06
Iter: 482 loss: 2.7116655837482062e-06
Iter: 483 loss: 2.7022766176657841e-06
Iter: 484 loss: 2.7014822386033166e-06
Iter: 485 loss: 2.7010403543924356e-06
Iter: 486 loss: 2.7000125441017564e-06
Iter: 487 loss: 2.6969969025410514e-06
Iter: 488 loss: 2.7082115348651043e-06
Iter: 489 loss: 2.6956916220290649e-06
Iter: 490 loss: 2.6932268972787242e-06
Iter: 491 loss: 2.6931776712815121e-06
Iter: 492 loss: 2.6908980594028982e-06
Iter: 493 loss: 2.7014417098297739e-06
Iter: 494 loss: 2.6904705742467891e-06
Iter: 495 loss: 2.689188332657074e-06
Iter: 496 loss: 2.6874482337543902e-06
Iter: 497 loss: 2.6873612845570809e-06
Iter: 498 loss: 2.6857305582670886e-06
Iter: 499 loss: 2.6856798802125882e-06
Iter: 500 loss: 2.6845690665815939e-06
Iter: 501 loss: 2.6853180446500886e-06
Iter: 502 loss: 2.6838713786753436e-06
Iter: 503 loss: 2.6822805357440837e-06
Iter: 504 loss: 2.680131153052936e-06
Iter: 505 loss: 2.680020426004867e-06
Iter: 506 loss: 2.6770449254859659e-06
Iter: 507 loss: 2.6889467534177095e-06
Iter: 508 loss: 2.6763754297101944e-06
Iter: 509 loss: 2.6741477826107881e-06
Iter: 510 loss: 2.6741331499149822e-06
Iter: 511 loss: 2.6728581050752549e-06
Iter: 512 loss: 2.6702012042280348e-06
Iter: 513 loss: 2.7155593418776783e-06
Iter: 514 loss: 2.6701381301205291e-06
Iter: 515 loss: 2.6683947824612472e-06
Iter: 516 loss: 2.6853222301454524e-06
Iter: 517 loss: 2.6683318900358369e-06
Iter: 518 loss: 2.6662138628630187e-06
Iter: 519 loss: 2.6719798692937219e-06
Iter: 520 loss: 2.6655195361228196e-06
Iter: 521 loss: 2.6641048411372466e-06
Iter: 522 loss: 2.6620495197739309e-06
Iter: 523 loss: 2.6619901324088983e-06
Iter: 524 loss: 2.6603918005856143e-06
Iter: 525 loss: 2.6602940411073865e-06
Iter: 526 loss: 2.6585163772932983e-06
Iter: 527 loss: 2.6553964850145447e-06
Iter: 528 loss: 2.6553960707607639e-06
Iter: 529 loss: 2.6537608851224905e-06
Iter: 530 loss: 2.6747052986713578e-06
Iter: 531 loss: 2.6537485025954782e-06
Iter: 532 loss: 2.6522109118818173e-06
Iter: 533 loss: 2.6568394041711353e-06
Iter: 534 loss: 2.651748322046988e-06
Iter: 535 loss: 2.6503020656372136e-06
Iter: 536 loss: 2.64975292798654e-06
Iter: 537 loss: 2.648962265159776e-06
Iter: 538 loss: 2.6466371928234003e-06
Iter: 539 loss: 2.65032493505477e-06
Iter: 540 loss: 2.6455562651789541e-06
Iter: 541 loss: 2.6435753091044541e-06
Iter: 542 loss: 2.6635287920155687e-06
Iter: 543 loss: 2.643513495992176e-06
Iter: 544 loss: 2.6415740311349986e-06
Iter: 545 loss: 2.64574639632466e-06
Iter: 546 loss: 2.6408181501717056e-06
Iter: 547 loss: 2.6395964073145117e-06
Iter: 548 loss: 2.6371023828907387e-06
Iter: 549 loss: 2.6820702245627624e-06
Iter: 550 loss: 2.6370585115970215e-06
Iter: 551 loss: 2.6373234125718738e-06
Iter: 552 loss: 2.6358970978029957e-06
Iter: 553 loss: 2.634852129985366e-06
Iter: 554 loss: 2.6327922878916425e-06
Iter: 555 loss: 2.6733770402028613e-06
Iter: 556 loss: 2.6327727797547932e-06
Iter: 557 loss: 2.6305375973777644e-06
Iter: 558 loss: 2.6309556046420804e-06
Iter: 559 loss: 2.6288690262628509e-06
Iter: 560 loss: 2.6262614812407404e-06
Iter: 561 loss: 2.6262056337875691e-06
Iter: 562 loss: 2.6252003102382784e-06
Iter: 563 loss: 2.6233238720234265e-06
Iter: 564 loss: 2.6654170432740203e-06
Iter: 565 loss: 2.623320115923975e-06
Iter: 566 loss: 2.621845521807905e-06
Iter: 567 loss: 2.6218334849373791e-06
Iter: 568 loss: 2.6203762434333868e-06
Iter: 569 loss: 2.6195358277779766e-06
Iter: 570 loss: 2.6189140660091298e-06
Iter: 571 loss: 2.6172131074057592e-06
Iter: 572 loss: 2.6202407139021656e-06
Iter: 573 loss: 2.6164693187023671e-06
Iter: 574 loss: 2.6142097844245679e-06
Iter: 575 loss: 2.6179220841120032e-06
Iter: 576 loss: 2.6131780817356894e-06
Iter: 577 loss: 2.6114532311740959e-06
Iter: 578 loss: 2.6114355038225611e-06
Iter: 579 loss: 2.610234564885623e-06
Iter: 580 loss: 2.6084159487352e-06
Iter: 581 loss: 2.6083819917887375e-06
Iter: 582 loss: 2.6066102965300586e-06
Iter: 583 loss: 2.6107558234731895e-06
Iter: 584 loss: 2.6059584866709645e-06
Iter: 585 loss: 2.6046068292612907e-06
Iter: 586 loss: 2.6045391478617413e-06
Iter: 587 loss: 2.6036594769859672e-06
Iter: 588 loss: 2.6010421564715545e-06
Iter: 589 loss: 2.6095895562456076e-06
Iter: 590 loss: 2.5997955951340331e-06
Iter: 591 loss: 2.5986321331809334e-06
Iter: 592 loss: 2.5980128575148405e-06
Iter: 593 loss: 2.5962645248973659e-06
Iter: 594 loss: 2.5971034666495232e-06
Iter: 595 loss: 2.5950894711809254e-06
Iter: 596 loss: 2.5934577098675638e-06
Iter: 597 loss: 2.5919765852019229e-06
Iter: 598 loss: 2.5915721149431582e-06
Iter: 599 loss: 2.5905579697967803e-06
Iter: 600 loss: 2.59006352462672e-06
Iter: 601 loss: 2.5892252112113913e-06
Iter: 602 loss: 2.5870752606684528e-06
Iter: 603 loss: 2.6052049315180185e-06
Iter: 604 loss: 2.5867091604235219e-06
Iter: 605 loss: 2.5837789238986567e-06
Iter: 606 loss: 2.5981803042078565e-06
Iter: 607 loss: 2.5832740538722929e-06
Iter: 608 loss: 2.580913609578027e-06
Iter: 609 loss: 2.6029768374043456e-06
Iter: 610 loss: 2.5808157822837872e-06
Iter: 611 loss: 2.57906966503567e-06
Iter: 612 loss: 2.5861081882463284e-06
Iter: 613 loss: 2.5786803425519453e-06
Iter: 614 loss: 2.5774618169542257e-06
Iter: 615 loss: 2.5757850420572875e-06
Iter: 616 loss: 2.575709158225709e-06
Iter: 617 loss: 2.5746242052710475e-06
Iter: 618 loss: 2.5745760558121936e-06
Iter: 619 loss: 2.5731845436778713e-06
Iter: 620 loss: 2.5712377646278397e-06
Iter: 621 loss: 2.5711600684884606e-06
Iter: 622 loss: 2.56928940204799e-06
Iter: 623 loss: 2.5707345161583918e-06
Iter: 624 loss: 2.5681532568537686e-06
Iter: 625 loss: 2.5666635420852679e-06
Iter: 626 loss: 2.5665523642170879e-06
Iter: 627 loss: 2.5653242185973134e-06
Iter: 628 loss: 2.5626427623255086e-06
Iter: 629 loss: 2.6027832762165823e-06
Iter: 630 loss: 2.5625301793732437e-06
Iter: 631 loss: 2.5613573051051263e-06
Iter: 632 loss: 2.5613089970491913e-06
Iter: 633 loss: 2.5599120165976214e-06
Iter: 634 loss: 2.5590221809657505e-06
Iter: 635 loss: 2.5584748137577821e-06
Iter: 636 loss: 2.5564514681805007e-06
Iter: 637 loss: 2.555040733839751e-06
Iter: 638 loss: 2.5543164888727632e-06
Iter: 639 loss: 2.5518747654893427e-06
Iter: 640 loss: 2.5518747631728774e-06
Iter: 641 loss: 2.5499349296755802e-06
Iter: 642 loss: 2.5579536428510878e-06
Iter: 643 loss: 2.54951513968915e-06
Iter: 644 loss: 2.5478573733506442e-06
Iter: 645 loss: 2.5492408487884717e-06
Iter: 646 loss: 2.54687158954305e-06
Iter: 647 loss: 2.5452546500603448e-06
Iter: 648 loss: 2.5447981850787498e-06
Iter: 649 loss: 2.5438133173162118e-06
Iter: 650 loss: 2.5428876655865062e-06
Iter: 651 loss: 2.5424647877005692e-06
Iter: 652 loss: 2.5416479406570691e-06
Iter: 653 loss: 2.5397435559858605e-06
Iter: 654 loss: 2.5629325637622972e-06
Iter: 655 loss: 2.5395916422270512e-06
Iter: 656 loss: 2.5375424719669278e-06
Iter: 657 loss: 2.5453365746365367e-06
Iter: 658 loss: 2.5370542052024803e-06
Iter: 659 loss: 2.5345318765278845e-06
Iter: 660 loss: 2.5517493229160954e-06
Iter: 661 loss: 2.5342867341145739e-06
Iter: 662 loss: 2.5334193084201185e-06
Iter: 663 loss: 2.5320014785459129e-06
Iter: 664 loss: 2.5319938616427868e-06
Iter: 665 loss: 2.5309254241014522e-06
Iter: 666 loss: 2.5308316971124526e-06
Iter: 667 loss: 2.5298123051163416e-06
Iter: 668 loss: 2.5276095169335065e-06
Iter: 669 loss: 2.56159571936185e-06
Iter: 670 loss: 2.5275271598820483e-06
Iter: 671 loss: 2.5252265001811384e-06
Iter: 672 loss: 2.5269542401530569e-06
Iter: 673 loss: 2.5238189310321145e-06
Iter: 674 loss: 2.5214372147137525e-06
Iter: 675 loss: 2.521398820875689e-06
Iter: 676 loss: 2.5197625870779947e-06
Iter: 677 loss: 2.52091449271475e-06
Iter: 678 loss: 2.5187452849102141e-06
Iter: 679 loss: 2.5168817580539709e-06
Iter: 680 loss: 2.5185731495374896e-06
Iter: 681 loss: 2.5158006007685394e-06
Iter: 682 loss: 2.5142889457956975e-06
Iter: 683 loss: 2.5273529474313314e-06
Iter: 684 loss: 2.5142080247039859e-06
Iter: 685 loss: 2.512623471853459e-06
Iter: 686 loss: 2.5170215688500832e-06
Iter: 687 loss: 2.5121122313667179e-06
Iter: 688 loss: 2.5108584443458973e-06
Iter: 689 loss: 2.5076504441902046e-06
Iter: 690 loss: 2.5349808576159456e-06
Iter: 691 loss: 2.5071133926041361e-06
Iter: 692 loss: 2.5088470228545073e-06
Iter: 693 loss: 2.5058167748838936e-06
Iter: 694 loss: 2.504920304216266e-06
Iter: 695 loss: 2.5033238432272535e-06
Iter: 696 loss: 2.5430853123736173e-06
Iter: 697 loss: 2.5033238393949617e-06
Iter: 698 loss: 2.5018087012925436e-06
Iter: 699 loss: 2.5066153984060884e-06
Iter: 700 loss: 2.5013742104653761e-06
Iter: 701 loss: 2.499982302612114e-06
Iter: 702 loss: 2.5201545424659737e-06
Iter: 703 loss: 2.4999801639448038e-06
Iter: 704 loss: 2.49925166824094e-06
Iter: 705 loss: 2.4970937876043222e-06
Iter: 706 loss: 2.5044505134698118e-06
Iter: 707 loss: 2.496096966098171e-06
Iter: 708 loss: 2.4933810665691437e-06
Iter: 709 loss: 2.5339574844624272e-06
Iter: 710 loss: 2.493379048255324e-06
Iter: 711 loss: 2.4910156775991411e-06
Iter: 712 loss: 2.5064300523290876e-06
Iter: 713 loss: 2.4907635651634509e-06
Iter: 714 loss: 2.4894724048634949e-06
Iter: 715 loss: 2.4894733904223186e-06
Iter: 716 loss: 2.4884397608245019e-06
Iter: 717 loss: 2.4865999406190111e-06
Iter: 718 loss: 2.4906271634325805e-06
Iter: 719 loss: 2.4858911454816544e-06
Iter: 720 loss: 2.4846336239409261e-06
Iter: 721 loss: 2.4846221948193605e-06
Iter: 722 loss: 2.4834964033357395e-06
Iter: 723 loss: 2.4812525028997266e-06
Iter: 724 loss: 2.5242528730359169e-06
Iter: 725 loss: 2.4812261475920669e-06
Iter: 726 loss: 2.4791681239307295e-06
Iter: 727 loss: 2.4844520391456743e-06
Iter: 728 loss: 2.4784612746013282e-06
Iter: 729 loss: 2.4765258105774488e-06
Iter: 730 loss: 2.4765237030217273e-06
Iter: 731 loss: 2.475485240820371e-06
Iter: 732 loss: 2.4733177380391023e-06
Iter: 733 loss: 2.5101511341044731e-06
Iter: 734 loss: 2.4732650988034141e-06
Iter: 735 loss: 2.4725786755774991e-06
Iter: 736 loss: 2.4723241775775138e-06
Iter: 737 loss: 2.4713329661592091e-06
Iter: 738 loss: 2.4698008727627763e-06
Iter: 739 loss: 2.4697788422186681e-06
Iter: 740 loss: 2.4677467624560359e-06
Iter: 741 loss: 2.4669248759841479e-06
Iter: 742 loss: 2.4658457006579468e-06
Iter: 743 loss: 2.4652892919785138e-06
Iter: 744 loss: 2.4646141627063378e-06
Iter: 745 loss: 2.4634200334055294e-06
Iter: 746 loss: 2.4614228791415374e-06
Iter: 747 loss: 2.4614171152237393e-06
Iter: 748 loss: 2.4596436674059861e-06
Iter: 749 loss: 2.4713900952364335e-06
Iter: 750 loss: 2.4594601695696277e-06
Iter: 751 loss: 2.4580015978804092e-06
Iter: 752 loss: 2.4634897324541958e-06
Iter: 753 loss: 2.4576498867243494e-06
Iter: 754 loss: 2.4559882721374469e-06
Iter: 755 loss: 2.4627645492485673e-06
Iter: 756 loss: 2.4556228296925743e-06
Iter: 757 loss: 2.4544128851442851e-06
Iter: 758 loss: 2.4520371758442431e-06
Iter: 759 loss: 2.4993039154408797e-06
Iter: 760 loss: 2.4520164359972144e-06
Iter: 761 loss: 2.45023508513413e-06
Iter: 762 loss: 2.4502076366707341e-06
Iter: 763 loss: 2.4482288752090794e-06
Iter: 764 loss: 2.4480477565129167e-06
Iter: 765 loss: 2.4465901354207305e-06
Iter: 766 loss: 2.4453555582105405e-06
Iter: 767 loss: 2.4468659497523858e-06
Iter: 768 loss: 2.4447104654003635e-06
Iter: 769 loss: 2.4433573292300752e-06
Iter: 770 loss: 2.4618789522988976e-06
Iter: 771 loss: 2.4433522155684829e-06
Iter: 772 loss: 2.4422829027517463e-06
Iter: 773 loss: 2.4400223325275402e-06
Iter: 774 loss: 2.4771138622859067e-06
Iter: 775 loss: 2.4399574095781177e-06
Iter: 776 loss: 2.4378763330641259e-06
Iter: 777 loss: 2.4437398340273425e-06
Iter: 778 loss: 2.437213314038922e-06
Iter: 779 loss: 2.4353731799966374e-06
Iter: 780 loss: 2.4353556757328345e-06
Iter: 781 loss: 2.4342429378139755e-06
Iter: 782 loss: 2.4323251992819383e-06
Iter: 783 loss: 2.4323237392622391e-06
Iter: 784 loss: 2.4308157871159011e-06
Iter: 785 loss: 2.4485949091597769e-06
Iter: 786 loss: 2.4307946478148908e-06
Iter: 787 loss: 2.4295949338622742e-06
Iter: 788 loss: 2.4369453279853612e-06
Iter: 789 loss: 2.4294505862977891e-06
Iter: 790 loss: 2.42831598155809e-06
Iter: 791 loss: 2.4279163430833431e-06
Iter: 792 loss: 2.4272763202573211e-06
Iter: 793 loss: 2.4255718210392349e-06
Iter: 794 loss: 2.4230534114004808e-06
Iter: 795 loss: 2.4229917448469229e-06
Iter: 796 loss: 2.422947212356137e-06
Iter: 797 loss: 2.421516066590396e-06
Iter: 798 loss: 2.4206780644098543e-06
Iter: 799 loss: 2.4188918365569177e-06
Iter: 800 loss: 2.4475410143711e-06
Iter: 801 loss: 2.4188350125939903e-06
Iter: 802 loss: 2.4173312065589321e-06
Iter: 803 loss: 2.4273405870300239e-06
Iter: 804 loss: 2.4171771912288093e-06
Iter: 805 loss: 2.41545967419399e-06
Iter: 806 loss: 2.4223846001790462e-06
Iter: 807 loss: 2.4150768057125995e-06
Iter: 808 loss: 2.4140510488680027e-06
Iter: 809 loss: 2.4120002370918017e-06
Iter: 810 loss: 2.4509978615904384e-06
Iter: 811 loss: 2.4119747580489942e-06
Iter: 812 loss: 2.4098367623477078e-06
Iter: 813 loss: 2.4341611103960752e-06
Iter: 814 loss: 2.4097996136771342e-06
Iter: 815 loss: 2.4076049863074608e-06
Iter: 816 loss: 2.4156838313309348e-06
Iter: 817 loss: 2.4070631460577636e-06
Iter: 818 loss: 2.4060371890645668e-06
Iter: 819 loss: 2.4048239583305954e-06
Iter: 820 loss: 2.4046933256996605e-06
Iter: 821 loss: 2.4035555855301794e-06
Iter: 822 loss: 2.403502138409769e-06
Iter: 823 loss: 2.4024307732946021e-06
Iter: 824 loss: 2.4027726661618183e-06
Iter: 825 loss: 2.4016683362789946e-06
Iter: 826 loss: 2.4001345191917095e-06
Iter: 827 loss: 2.39956205693074e-06
Iter: 828 loss: 2.3987173301483133e-06
Iter: 829 loss: 2.3969684859811479e-06
Iter: 830 loss: 2.4070415252832319e-06
Iter: 831 loss: 2.3967335902614125e-06
Iter: 832 loss: 2.3950567689179851e-06
Iter: 833 loss: 2.4073969683932524e-06
Iter: 834 loss: 2.3949190246155957e-06
Iter: 835 loss: 2.3938794390716697e-06
Iter: 836 loss: 2.3916227360387259e-06
Iter: 837 loss: 2.4259732626801941e-06
Iter: 838 loss: 2.3915338525289097e-06
Iter: 839 loss: 2.3911691608714432e-06
Iter: 840 loss: 2.3905888391136323e-06
Iter: 841 loss: 2.3896589865092471e-06
Iter: 842 loss: 2.3880835853619559e-06
Iter: 843 loss: 2.3880807378169425e-06
Iter: 844 loss: 2.3860975715350852e-06
Iter: 845 loss: 2.3847307736162664e-06
Iter: 846 loss: 2.3840121168872546e-06
Iter: 847 loss: 2.3839320940281218e-06
Iter: 848 loss: 2.38272715177699e-06
Iter: 849 loss: 2.3816654439554241e-06
Iter: 850 loss: 2.3798314400253002e-06
Iter: 851 loss: 2.3798302390248441e-06
Iter: 852 loss: 2.3782652231279616e-06
Iter: 853 loss: 2.3823370845994087e-06
Iter: 854 loss: 2.3777332758765646e-06
Iter: 855 loss: 2.3763061243703721e-06
Iter: 856 loss: 2.3763036171105995e-06
Iter: 857 loss: 2.3753048023875161e-06
Iter: 858 loss: 2.3740094841762398e-06
Iter: 859 loss: 2.3739229566164847e-06
Iter: 860 loss: 2.3721667662279809e-06
Iter: 861 loss: 2.3780400295428255e-06
Iter: 862 loss: 2.3716879484761291e-06
Iter: 863 loss: 2.3703534723813469e-06
Iter: 864 loss: 2.3805494128079115e-06
Iter: 865 loss: 2.370253337132047e-06
Iter: 866 loss: 2.3686946214715908e-06
Iter: 867 loss: 2.3680165738627051e-06
Iter: 868 loss: 2.36721836271802e-06
Iter: 869 loss: 2.3659389758974231e-06
Iter: 870 loss: 2.3671551413092265e-06
Iter: 871 loss: 2.3652073655680798e-06
Iter: 872 loss: 2.3639154227495943e-06
Iter: 873 loss: 2.3639126883293796e-06
Iter: 874 loss: 2.36282885564636e-06
Iter: 875 loss: 2.3601574809804277e-06
Iter: 876 loss: 2.3867370824463881e-06
Iter: 877 loss: 2.35982016269728e-06
Iter: 878 loss: 2.357443745470356e-06
Iter: 879 loss: 2.3723532434411816e-06
Iter: 880 loss: 2.3571702905752803e-06
Iter: 881 loss: 2.3553693585728394e-06
Iter: 882 loss: 2.3553660999940697e-06
Iter: 883 loss: 2.3543206075550941e-06
Iter: 884 loss: 2.3523820373749507e-06
Iter: 885 loss: 2.3965287966397645e-06
Iter: 886 loss: 2.3523791543276312e-06
Iter: 887 loss: 2.3511898698162395e-06
Iter: 888 loss: 2.368964972149504e-06
Iter: 889 loss: 2.3511889988806556e-06
Iter: 890 loss: 2.3497960187647658e-06
Iter: 891 loss: 2.3509848761829352e-06
Iter: 892 loss: 2.3489729942982288e-06
Iter: 893 loss: 2.3475510314253962e-06
Iter: 894 loss: 2.347238366884082e-06
Iter: 895 loss: 2.3463139313613024e-06
Iter: 896 loss: 2.3442522373491389e-06
Iter: 897 loss: 2.3546163900113814e-06
Iter: 898 loss: 2.3439084874791662e-06
Iter: 899 loss: 2.3423784457241038e-06
Iter: 900 loss: 2.3592891912843827e-06
Iter: 901 loss: 2.3423473450240893e-06
Iter: 902 loss: 2.3412726128913947e-06
Iter: 903 loss: 2.3402584767775026e-06
Iter: 904 loss: 2.3400107426332357e-06
Iter: 905 loss: 2.3388188359135176e-06
Iter: 906 loss: 2.3417761693718462e-06
Iter: 907 loss: 2.3383987224640463e-06
Iter: 908 loss: 2.3367677142046353e-06
Iter: 909 loss: 2.3464072230542094e-06
Iter: 910 loss: 2.3365582790906966e-06
Iter: 911 loss: 2.3355429965641905e-06
Iter: 912 loss: 2.3331295262943693e-06
Iter: 913 loss: 2.3605982873740757e-06
Iter: 914 loss: 2.3329019330955464e-06
Iter: 915 loss: 2.3307727933531387e-06
Iter: 916 loss: 2.3307703769862122e-06
Iter: 917 loss: 2.3285977123201939e-06
Iter: 918 loss: 2.3350887778838595e-06
Iter: 919 loss: 2.3279398037572312e-06
Iter: 920 loss: 2.3268300431073383e-06
Iter: 921 loss: 2.3256508018253876e-06
Iter: 922 loss: 2.3254551424171651e-06
Iter: 923 loss: 2.3247404289273779e-06
Iter: 924 loss: 2.3245095098669429e-06
Iter: 925 loss: 2.3235475443670621e-06
Iter: 926 loss: 2.321661755250319e-06
Iter: 927 loss: 2.3593276713069738e-06
Iter: 928 loss: 2.3216458420629591e-06
Iter: 929 loss: 2.3197774055649315e-06
Iter: 930 loss: 2.3246022040900029e-06
Iter: 931 loss: 2.31913859209488e-06
Iter: 932 loss: 2.3172518479370952e-06
Iter: 933 loss: 2.3354339462495027e-06
Iter: 934 loss: 2.3171818232377175e-06
Iter: 935 loss: 2.3157911923884838e-06
Iter: 936 loss: 2.3209968141235736e-06
Iter: 937 loss: 2.3154539182673965e-06
Iter: 938 loss: 2.3143592129652505e-06
Iter: 939 loss: 2.3128215219880255e-06
Iter: 940 loss: 2.3127620731606433e-06
Iter: 941 loss: 2.3119243617156467e-06
Iter: 942 loss: 2.3118215213131676e-06
Iter: 943 loss: 2.3108667972376511e-06
Iter: 944 loss: 2.3099632209842928e-06
Iter: 945 loss: 2.3097444486983604e-06
Iter: 946 loss: 2.3080404435652277e-06
Iter: 947 loss: 2.3051038163375668e-06
Iter: 948 loss: 2.305101571740278e-06
Iter: 949 loss: 2.3063408151794306e-06
Iter: 950 loss: 2.3039488562476397e-06
Iter: 951 loss: 2.3030051723594125e-06
Iter: 952 loss: 2.3012655080538308e-06
Iter: 953 loss: 2.3414072903497455e-06
Iter: 954 loss: 2.3012635949249834e-06
Iter: 955 loss: 2.2997598697932048e-06
Iter: 956 loss: 2.3038678812480484e-06
Iter: 957 loss: 2.2992683804734256e-06
Iter: 958 loss: 2.2980663041066319e-06
Iter: 959 loss: 2.2980335138975307e-06
Iter: 960 loss: 2.2973339102883796e-06
Iter: 961 loss: 2.2954983880362822e-06
Iter: 962 loss: 2.3094319766768414e-06
Iter: 963 loss: 2.2951301606584762e-06
Iter: 964 loss: 2.2931998799758739e-06
Iter: 965 loss: 2.3146161337749994e-06
Iter: 966 loss: 2.2931613875801804e-06
Iter: 967 loss: 2.2914136191003867e-06
Iter: 968 loss: 2.3001204236755506e-06
Iter: 969 loss: 2.2911183262726469e-06
Iter: 970 loss: 2.2898091754350095e-06
Iter: 971 loss: 2.2919488362197679e-06
Iter: 972 loss: 2.2892098065380576e-06
Iter: 973 loss: 2.2881020565039927e-06
Iter: 974 loss: 2.2883111393193162e-06
Iter: 975 loss: 2.28727565561191e-06
Iter: 976 loss: 2.2861484196770423e-06
Iter: 977 loss: 2.2861455718900705e-06
Iter: 978 loss: 2.2851165258071543e-06
Iter: 979 loss: 2.2826972728868151e-06
Iter: 980 loss: 2.3113278079152081e-06
Iter: 981 loss: 2.2824896901002754e-06
Iter: 982 loss: 2.2804206432313525e-06
Iter: 983 loss: 2.2937149580400439e-06
Iter: 984 loss: 2.2801932888471456e-06
Iter: 985 loss: 2.2790470213615441e-06
Iter: 986 loss: 2.2789902057102359e-06
Iter: 987 loss: 2.2781625228134589e-06
Iter: 988 loss: 2.27627883940229e-06
Iter: 989 loss: 2.3011014211068336e-06
Iter: 990 loss: 2.2761582308827428e-06
Iter: 991 loss: 2.2751686312368703e-06
Iter: 992 loss: 2.2751554199443706e-06
Iter: 993 loss: 2.2739180231721173e-06
Iter: 994 loss: 2.2736535935797445e-06
Iter: 995 loss: 2.272844134326836e-06
Iter: 996 loss: 2.2714000734193568e-06
Iter: 997 loss: 2.2692583146991154e-06
Iter: 998 loss: 2.2692078769425287e-06
Iter: 999 loss: 2.2683532167289869e-06
Iter: 1000 loss: 2.2677987089181991e-06
Iter: 1001 loss: 2.2666811829123459e-06
Iter: 1002 loss: 2.2667983575808816e-06
Iter: 1003 loss: 2.2658213853031653e-06
Iter: 1004 loss: 2.2645653768734952e-06
Iter: 1005 loss: 2.2676412584626058e-06
Iter: 1006 loss: 2.2641182865867081e-06
Iter: 1007 loss: 2.2630032558352107e-06
Iter: 1008 loss: 2.2647636803232933e-06
Iter: 1009 loss: 2.2624836660609552e-06
Iter: 1010 loss: 2.2608714839703514e-06
Iter: 1011 loss: 2.2688711202943027e-06
Iter: 1012 loss: 2.2605975304461481e-06
Iter: 1013 loss: 2.2595008951004849e-06
Iter: 1014 loss: 2.2576732887276865e-06
Iter: 1015 loss: 2.2576674069762624e-06
Iter: 1016 loss: 2.2557420518473256e-06
Iter: 1017 loss: 2.2716482193315466e-06
Iter: 1018 loss: 2.2556245149771713e-06
Iter: 1019 loss: 2.2536737880767153e-06
Iter: 1020 loss: 2.2649004159443359e-06
Iter: 1021 loss: 2.2534115005877097e-06
Iter: 1022 loss: 2.2526024528322389e-06
Iter: 1023 loss: 2.2512502875289464e-06
Iter: 1024 loss: 2.2512462985267526e-06
Iter: 1025 loss: 2.2505214726188692e-06
Iter: 1026 loss: 2.2503142422567722e-06
Iter: 1027 loss: 2.2493905148392454e-06
Iter: 1028 loss: 2.2472568149031013e-06
Iter: 1029 loss: 2.2740616281787224e-06
Iter: 1030 loss: 2.2471001528747917e-06
Iter: 1031 loss: 2.24522952513394e-06
Iter: 1032 loss: 2.2505201567607954e-06
Iter: 1033 loss: 2.2446353819233371e-06
Iter: 1034 loss: 2.2436223824866061e-06
Iter: 1035 loss: 2.2434374190202753e-06
Iter: 1036 loss: 2.2427381436424361e-06
Iter: 1037 loss: 2.2415217755546155e-06
Iter: 1038 loss: 2.2415213152724731e-06
Iter: 1039 loss: 2.2399957939866985e-06
Iter: 1040 loss: 2.2463372535096605e-06
Iter: 1041 loss: 2.239667902855625e-06
Iter: 1042 loss: 2.2384939108508403e-06
Iter: 1043 loss: 2.2464510130429175e-06
Iter: 1044 loss: 2.2383781228035709e-06
Iter: 1045 loss: 2.2370964173843265e-06
Iter: 1046 loss: 2.2371313887976663e-06
Iter: 1047 loss: 2.2360815104055934e-06
Iter: 1048 loss: 2.2345592247902323e-06
Iter: 1049 loss: 2.2330580779116968e-06
Iter: 1050 loss: 2.2327375570333735e-06
Iter: 1051 loss: 2.2328633419063309e-06
Iter: 1052 loss: 2.2318451458583067e-06
Iter: 1053 loss: 2.23113977245317e-06
Iter: 1054 loss: 2.2301996273251928e-06
Iter: 1055 loss: 2.2301466179625145e-06
Iter: 1056 loss: 2.2290389410364913e-06
Iter: 1057 loss: 2.2291184547058234e-06
Iter: 1058 loss: 2.2281762467985438e-06
Iter: 1059 loss: 2.2267647001204467e-06
Iter: 1060 loss: 2.2267560531322406e-06
Iter: 1061 loss: 2.2259381978981206e-06
Iter: 1062 loss: 2.2236344217643083e-06
Iter: 1063 loss: 2.2353413231087511e-06
Iter: 1064 loss: 2.2228788199032085e-06
Iter: 1065 loss: 2.2210911403869783e-06
Iter: 1066 loss: 2.2210429127142252e-06
Iter: 1067 loss: 2.2194069910646921e-06
Iter: 1068 loss: 2.2287245005825891e-06
Iter: 1069 loss: 2.2191830547437436e-06
Iter: 1070 loss: 2.2184336627530345e-06
Iter: 1071 loss: 2.21774872546592e-06
Iter: 1072 loss: 2.2175652754724945e-06
Iter: 1073 loss: 2.2163608091592093e-06
Iter: 1074 loss: 2.2249319553210831e-06
Iter: 1075 loss: 2.21625392684281e-06
Iter: 1076 loss: 2.2152233830802951e-06
Iter: 1077 loss: 2.2189767956463751e-06
Iter: 1078 loss: 2.2149659723830996e-06
Iter: 1079 loss: 2.213750350242461e-06
Iter: 1080 loss: 2.21221257844962e-06
Iter: 1081 loss: 2.21209423328989e-06
Iter: 1082 loss: 2.21048726801602e-06
Iter: 1083 loss: 2.2170612924381914e-06
Iter: 1084 loss: 2.2101352134410039e-06
Iter: 1085 loss: 2.209116377886371e-06
Iter: 1086 loss: 2.2090818673627361e-06
Iter: 1087 loss: 2.2083762351367088e-06
Iter: 1088 loss: 2.2066709510504982e-06
Iter: 1089 loss: 2.22495472186974e-06
Iter: 1090 loss: 2.2064867389125881e-06
Iter: 1091 loss: 2.2054189788513741e-06
Iter: 1092 loss: 2.2053853828660825e-06
Iter: 1093 loss: 2.204129777369932e-06
Iter: 1094 loss: 2.2042305943490354e-06
Iter: 1095 loss: 2.2031549374444965e-06
Iter: 1096 loss: 2.2018149453633773e-06
Iter: 1097 loss: 2.1996338567859069e-06
Iter: 1098 loss: 2.1996209767281476e-06
Iter: 1099 loss: 2.1998571509680482e-06
Iter: 1100 loss: 2.1986885304227549e-06
Iter: 1101 loss: 2.1978856557180556e-06
Iter: 1102 loss: 2.1970732156322153e-06
Iter: 1103 loss: 2.1969136456678219e-06
Iter: 1104 loss: 2.1959584555492979e-06
Iter: 1105 loss: 2.196384960565344e-06
Iter: 1106 loss: 2.1953088600728058e-06
Iter: 1107 loss: 2.1937275940028933e-06
Iter: 1108 loss: 2.2025288564376341e-06
Iter: 1109 loss: 2.1935026530367473e-06
Iter: 1110 loss: 2.1920126744038653e-06
Iter: 1111 loss: 2.1950821166068969e-06
Iter: 1112 loss: 2.19141543144915e-06
Iter: 1113 loss: 2.1900248717737624e-06
Iter: 1114 loss: 2.1907336024777678e-06
Iter: 1115 loss: 2.1891000371267915e-06
Iter: 1116 loss: 2.187844231458322e-06
Iter: 1117 loss: 2.1944360270796631e-06
Iter: 1118 loss: 2.1876480577051492e-06
Iter: 1119 loss: 2.1860403986762213e-06
Iter: 1120 loss: 2.1899082859464825e-06
Iter: 1121 loss: 2.1854607593603114e-06
Iter: 1122 loss: 2.1846162684763048e-06
Iter: 1123 loss: 2.1840266287918718e-06
Iter: 1124 loss: 2.1837248196775021e-06
Iter: 1125 loss: 2.1828014529418446e-06
Iter: 1126 loss: 2.1827567747806681e-06
Iter: 1127 loss: 2.1819029852543381e-06
Iter: 1128 loss: 2.1798673715524993e-06
Iter: 1129 loss: 2.202795418158731e-06
Iter: 1130 loss: 2.1796706006952973e-06
Iter: 1131 loss: 2.1779635946413412e-06
Iter: 1132 loss: 2.1849928567284062e-06
Iter: 1133 loss: 2.1775924841494077e-06
Iter: 1134 loss: 2.176568595194772e-06
Iter: 1135 loss: 2.1764749562137772e-06
Iter: 1136 loss: 2.1758352100534707e-06
Iter: 1137 loss: 2.174395966427843e-06
Iter: 1138 loss: 2.1940731204013155e-06
Iter: 1139 loss: 2.1743135412252138e-06
Iter: 1140 loss: 2.173097589872527e-06
Iter: 1141 loss: 2.1849666725273823e-06
Iter: 1142 loss: 2.1730546046217616e-06
Iter: 1143 loss: 2.1718229391557452e-06
Iter: 1144 loss: 2.1761098953756661e-06
Iter: 1145 loss: 2.1715003570152388e-06
Iter: 1146 loss: 2.1703972846997944e-06
Iter: 1147 loss: 2.1708884231230212e-06
Iter: 1148 loss: 2.1696468153043945e-06
Iter: 1149 loss: 2.1681183066934732e-06
Iter: 1150 loss: 2.1688641156721766e-06
Iter: 1151 loss: 2.1670938811384544e-06
Iter: 1152 loss: 2.1663619763117283e-06
Iter: 1153 loss: 2.1661925106724534e-06
Iter: 1154 loss: 2.1654689421327687e-06
Iter: 1155 loss: 2.1646021016731505e-06
Iter: 1156 loss: 2.1645142139023755e-06
Iter: 1157 loss: 2.1636042005463379e-06
Iter: 1158 loss: 2.1650214724669248e-06
Iter: 1159 loss: 2.163177274362945e-06
Iter: 1160 loss: 2.1616678400453631e-06
Iter: 1161 loss: 2.1685139262586631e-06
Iter: 1162 loss: 2.1613773115186384e-06
Iter: 1163 loss: 2.1604643450077788e-06
Iter: 1164 loss: 2.1588233937881536e-06
Iter: 1165 loss: 2.1988794319125567e-06
Iter: 1166 loss: 2.1588232323883811e-06
Iter: 1167 loss: 2.1573733390162103e-06
Iter: 1168 loss: 2.1746409848388916e-06
Iter: 1169 loss: 2.1573542841660842e-06
Iter: 1170 loss: 2.1558287578948977e-06
Iter: 1171 loss: 2.16091603660041e-06
Iter: 1172 loss: 2.1554117367935271e-06
Iter: 1173 loss: 2.1546112072453191e-06
Iter: 1174 loss: 2.1532710871856358e-06
Iter: 1175 loss: 2.1532673368603651e-06
Iter: 1176 loss: 2.1525711655183891e-06
Iter: 1177 loss: 2.1523944966117389e-06
Iter: 1178 loss: 2.1515705671353236e-06
Iter: 1179 loss: 2.1504656861098841e-06
Iter: 1180 loss: 2.1504058382564697e-06
Iter: 1181 loss: 2.14885540573996e-06
Iter: 1182 loss: 2.1538873930221128e-06
Iter: 1183 loss: 2.1484202614849469e-06
Iter: 1184 loss: 2.1471707585530104e-06
Iter: 1185 loss: 2.1526869963564704e-06
Iter: 1186 loss: 2.1469216431596377e-06
Iter: 1187 loss: 2.146018885119505e-06
Iter: 1188 loss: 2.15795889296835e-06
Iter: 1189 loss: 2.1460138437407513e-06
Iter: 1190 loss: 2.1454501032414743e-06
Iter: 1191 loss: 2.1440802588156162e-06
Iter: 1192 loss: 2.1584722644115665e-06
Iter: 1193 loss: 2.1439256388543157e-06
Iter: 1194 loss: 2.1434844449757428e-06
Iter: 1195 loss: 2.143186834878097e-06
Iter: 1196 loss: 2.1424288291863958e-06
Iter: 1197 loss: 2.141108753562363e-06
Iter: 1198 loss: 2.1411083068676717e-06
Iter: 1199 loss: 2.1397332385881324e-06
Iter: 1200 loss: 2.1394918468208366e-06
Iter: 1201 loss: 2.1385573246958917e-06
Iter: 1202 loss: 2.1382569803119323e-06
Iter: 1203 loss: 2.1376868048937958e-06
Iter: 1204 loss: 2.1369642879384386e-06
Iter: 1205 loss: 2.1358260823431063e-06
Iter: 1206 loss: 2.13581371980873e-06
Iter: 1207 loss: 2.1347611128057446e-06
Iter: 1208 loss: 2.1351495133387277e-06
Iter: 1209 loss: 2.1340253335171747e-06
Iter: 1210 loss: 2.13283394574803e-06
Iter: 1211 loss: 2.1328214139324204e-06
Iter: 1212 loss: 2.1319672777819379e-06
Iter: 1213 loss: 2.130477853997437e-06
Iter: 1214 loss: 2.1304774100457561e-06
Iter: 1215 loss: 2.1291348673690675e-06
Iter: 1216 loss: 2.1437927660591686e-06
Iter: 1217 loss: 2.1291057964101388e-06
Iter: 1218 loss: 2.1281695108691616e-06
Iter: 1219 loss: 2.1317693582822768e-06
Iter: 1220 loss: 2.1279490910828287e-06
Iter: 1221 loss: 2.126821620841483e-06
Iter: 1222 loss: 2.1283387916342475e-06
Iter: 1223 loss: 2.1262554840489451e-06
Iter: 1224 loss: 2.1254371265173671e-06
Iter: 1225 loss: 2.1254891375419237e-06
Iter: 1226 loss: 2.1247977850221384e-06
Iter: 1227 loss: 2.1238958150362478e-06
Iter: 1228 loss: 2.1238853667543296e-06
Iter: 1229 loss: 2.1232044407309524e-06
Iter: 1230 loss: 2.1212598811327986e-06
Iter: 1231 loss: 2.1302364865759675e-06
Iter: 1232 loss: 2.1205597180473066e-06
Iter: 1233 loss: 2.11901240622569e-06
Iter: 1234 loss: 2.1190116700068683e-06
Iter: 1235 loss: 2.1180369943481729e-06
Iter: 1236 loss: 2.1180362746752707e-06
Iter: 1237 loss: 2.1175045641033241e-06
Iter: 1238 loss: 2.1161593859576484e-06
Iter: 1239 loss: 2.12819137840551e-06
Iter: 1240 loss: 2.1159522058332448e-06
Iter: 1241 loss: 2.1148501872725749e-06
Iter: 1242 loss: 2.1303515104361924e-06
Iter: 1243 loss: 2.1148473365338974e-06
Iter: 1244 loss: 2.113611841068151e-06
Iter: 1245 loss: 2.115786605775826e-06
Iter: 1246 loss: 2.1130681872374056e-06
Iter: 1247 loss: 2.1120426006579237e-06
Iter: 1248 loss: 2.1114512303143351e-06
Iter: 1249 loss: 2.1110136168250783e-06
Iter: 1250 loss: 2.1095877398805579e-06
Iter: 1251 loss: 2.1239372697175967e-06
Iter: 1252 loss: 2.1095430704551519e-06
Iter: 1253 loss: 2.1085319826085309e-06
Iter: 1254 loss: 2.1149612269426912e-06
Iter: 1255 loss: 2.108418581556061e-06
Iter: 1256 loss: 2.1076716631473646e-06
Iter: 1257 loss: 2.10788113387468e-06
Iter: 1258 loss: 2.1071326656222473e-06
Iter: 1259 loss: 2.1063839783116344e-06
Iter: 1260 loss: 2.108211422122235e-06
Iter: 1261 loss: 2.1061168539267037e-06
Iter: 1262 loss: 2.1048561339382414e-06
Iter: 1263 loss: 2.1059950005402854e-06
Iter: 1264 loss: 2.104123673780815e-06
Iter: 1265 loss: 2.103108612075547e-06
Iter: 1266 loss: 2.1018670883684494e-06
Iter: 1267 loss: 2.1017532195993874e-06
Iter: 1268 loss: 2.100905318582611e-06
Iter: 1269 loss: 2.1008280276382474e-06
Iter: 1270 loss: 2.0998549171185095e-06
Iter: 1271 loss: 2.1004747610474041e-06
Iter: 1272 loss: 2.0992357776494056e-06
Iter: 1273 loss: 2.0985104112643209e-06
Iter: 1274 loss: 2.0976996284923991e-06
Iter: 1275 loss: 2.0975886236559334e-06
Iter: 1276 loss: 2.0968780290330678e-06
Iter: 1277 loss: 2.0968182149906137e-06
Iter: 1278 loss: 2.0959701547139868e-06
Iter: 1279 loss: 2.0942996352995524e-06
Iter: 1280 loss: 2.127258379912724e-06
Iter: 1281 loss: 2.0942840297129362e-06
Iter: 1282 loss: 2.0927541680932044e-06
Iter: 1283 loss: 2.099636915030987e-06
Iter: 1284 loss: 2.09245653709005e-06
Iter: 1285 loss: 2.091406094115212e-06
Iter: 1286 loss: 2.0914050251345295e-06
Iter: 1287 loss: 2.0907090376172666e-06
Iter: 1288 loss: 2.091576771450149e-06
Iter: 1289 loss: 2.0903481214551952e-06
Iter: 1290 loss: 2.0895419427988549e-06
Iter: 1291 loss: 2.089145610283724e-06
Iter: 1292 loss: 2.0887607548538574e-06
Iter: 1293 loss: 2.0880063604021191e-06
Iter: 1294 loss: 2.0879806960025331e-06
Iter: 1295 loss: 2.0872791337262858e-06
Iter: 1296 loss: 2.0860697118035735e-06
Iter: 1297 loss: 2.0860688058614203e-06
Iter: 1298 loss: 2.0847051341336736e-06
Iter: 1299 loss: 2.0836762030105064e-06
Iter: 1300 loss: 2.0832308025412205e-06
Iter: 1301 loss: 2.0839055784054537e-06
Iter: 1302 loss: 2.0824576279823571e-06
Iter: 1303 loss: 2.081942611084957e-06
Iter: 1304 loss: 2.0807883934465162e-06
Iter: 1305 loss: 2.0967572883032226e-06
Iter: 1306 loss: 2.0807247424425167e-06
Iter: 1307 loss: 2.0796241890795986e-06
Iter: 1308 loss: 2.0814366810335981e-06
Iter: 1309 loss: 2.0791223082849243e-06
Iter: 1310 loss: 2.0779544259734835e-06
Iter: 1311 loss: 2.0779529597326161e-06
Iter: 1312 loss: 2.0771791408340271e-06
Iter: 1313 loss: 2.0754520424549462e-06
Iter: 1314 loss: 2.0996559656145959e-06
Iter: 1315 loss: 2.07536066046137e-06
Iter: 1316 loss: 2.07433106155041e-06
Iter: 1317 loss: 2.0743055818378388e-06
Iter: 1318 loss: 2.0734098937602808e-06
Iter: 1319 loss: 2.077214661597214e-06
Iter: 1320 loss: 2.0732224158732118e-06
Iter: 1321 loss: 2.07252539031066e-06
Iter: 1322 loss: 2.0727123309341254e-06
Iter: 1323 loss: 2.072020188935064e-06
Iter: 1324 loss: 2.0711035458163771e-06
Iter: 1325 loss: 2.0737688147525732e-06
Iter: 1326 loss: 2.0708192173512793e-06
Iter: 1327 loss: 2.0698623511235721e-06
Iter: 1328 loss: 2.07732470200296e-06
Iter: 1329 loss: 2.0697941694635151e-06
Iter: 1330 loss: 2.0690942363506975e-06
Iter: 1331 loss: 2.0673077338754963e-06
Iter: 1332 loss: 2.0826920879158427e-06
Iter: 1333 loss: 2.0670139835454137e-06
Iter: 1334 loss: 2.0653698830313668e-06
Iter: 1335 loss: 2.0840018348368063e-06
Iter: 1336 loss: 2.0653406684475469e-06
Iter: 1337 loss: 2.0642922015467655e-06
Iter: 1338 loss: 2.0642898158214922e-06
Iter: 1339 loss: 2.0638027346184641e-06
Iter: 1340 loss: 2.0625558535642961e-06
Iter: 1341 loss: 2.0731546869042912e-06
Iter: 1342 loss: 2.0623463521017949e-06
Iter: 1343 loss: 2.0611787025294963e-06
Iter: 1344 loss: 2.0746408472937456e-06
Iter: 1345 loss: 2.0611599178440281e-06
Iter: 1346 loss: 2.0598364295668311e-06
Iter: 1347 loss: 2.0629428163229636e-06
Iter: 1348 loss: 2.0593505716779703e-06
Iter: 1349 loss: 2.0584256735914496e-06
Iter: 1350 loss: 2.0571442502519791e-06
Iter: 1351 loss: 2.057089136948841e-06
Iter: 1352 loss: 2.0565861675354564e-06
Iter: 1353 loss: 2.0562822037036532e-06
Iter: 1354 loss: 2.0556009281568233e-06
Iter: 1355 loss: 2.0553086174465294e-06
Iter: 1356 loss: 2.0549572408775885e-06
Iter: 1357 loss: 2.054223979109604e-06
Iter: 1358 loss: 2.0573899763284371e-06
Iter: 1359 loss: 2.0540735660991681e-06
Iter: 1360 loss: 2.0533417990097316e-06
Iter: 1361 loss: 2.0557425836528206e-06
Iter: 1362 loss: 2.0531385473823888e-06
Iter: 1363 loss: 2.0521071924692453e-06
Iter: 1364 loss: 2.0514294328714893e-06
Iter: 1365 loss: 2.0510371300611525e-06
Iter: 1366 loss: 2.0498564849157919e-06
Iter: 1367 loss: 2.0507361151880493e-06
Iter: 1368 loss: 2.0491326988550868e-06
Iter: 1369 loss: 2.048371218830217e-06
Iter: 1370 loss: 2.0483509483772881e-06
Iter: 1371 loss: 2.0474970419435549e-06
Iter: 1372 loss: 2.0470203378483768e-06
Iter: 1373 loss: 2.0466467570081261e-06
Iter: 1374 loss: 2.0458298584493748e-06
Iter: 1375 loss: 2.0454120039730036e-06
Iter: 1376 loss: 2.04503184381331e-06
Iter: 1377 loss: 2.04434564306551e-06
Iter: 1378 loss: 2.0442735597930748e-06
Iter: 1379 loss: 2.0434817513982279e-06
Iter: 1380 loss: 2.0420320112065162e-06
Iter: 1381 loss: 2.0760005715408244e-06
Iter: 1382 loss: 2.0420309667842381e-06
Iter: 1383 loss: 2.0407472250168376e-06
Iter: 1384 loss: 2.0449982908938225e-06
Iter: 1385 loss: 2.0403938865071424e-06
Iter: 1386 loss: 2.039849792872458e-06
Iter: 1387 loss: 2.0397063537697389e-06
Iter: 1388 loss: 2.0392777058270777e-06
Iter: 1389 loss: 2.038312763593116e-06
Iter: 1390 loss: 2.0514789481669109e-06
Iter: 1391 loss: 2.0382571571815704e-06
Iter: 1392 loss: 2.0372568580201195e-06
Iter: 1393 loss: 2.048504967499186e-06
Iter: 1394 loss: 2.0372383059492839e-06
Iter: 1395 loss: 2.0363260755155971e-06
Iter: 1396 loss: 2.03863119057016e-06
Iter: 1397 loss: 2.0360089444392737e-06
Iter: 1398 loss: 2.0350604795776491e-06
Iter: 1399 loss: 2.0349205656453084e-06
Iter: 1400 loss: 2.0342580340713902e-06
Iter: 1401 loss: 2.0331844536766177e-06
Iter: 1402 loss: 2.0330877233232063e-06
Iter: 1403 loss: 2.0322958762204758e-06
Iter: 1404 loss: 2.0318223247283224e-06
Iter: 1405 loss: 2.03148018485594e-06
Iter: 1406 loss: 2.0309930707134222e-06
Iter: 1407 loss: 2.0301152192434683e-06
Iter: 1408 loss: 2.0514195243550382e-06
Iter: 1409 loss: 2.030115085894581e-06
Iter: 1410 loss: 2.0291543263313954e-06
Iter: 1411 loss: 2.0286783451993325e-06
Iter: 1412 loss: 2.0282219145843837e-06
Iter: 1413 loss: 2.0271417870238415e-06
Iter: 1414 loss: 2.0270543916752376e-06
Iter: 1415 loss: 2.0263590303417626e-06
Iter: 1416 loss: 2.0248773504370386e-06
Iter: 1417 loss: 2.0486649541953174e-06
Iter: 1418 loss: 2.024830410879969e-06
Iter: 1419 loss: 2.023925031384182e-06
Iter: 1420 loss: 2.0239245276937436e-06
Iter: 1421 loss: 2.0229879391854955e-06
Iter: 1422 loss: 2.0254162484809375e-06
Iter: 1423 loss: 2.0226687419351177e-06
Iter: 1424 loss: 2.0220141842779289e-06
Iter: 1425 loss: 2.021505427059695e-06
Iter: 1426 loss: 2.0212995587385864e-06
Iter: 1427 loss: 2.0204607924932803e-06
Iter: 1428 loss: 2.0204539962268972e-06
Iter: 1429 loss: 2.0198297295871089e-06
Iter: 1430 loss: 2.0193408184911647e-06
Iter: 1431 loss: 2.0191464350232619e-06
Iter: 1432 loss: 2.0179973806277419e-06
Iter: 1433 loss: 2.0179268557439769e-06
Iter: 1434 loss: 2.01705661359017e-06
Iter: 1435 loss: 2.0160596447879971e-06
Iter: 1436 loss: 2.0266594640904918e-06
Iter: 1437 loss: 2.0160351054551019e-06
Iter: 1438 loss: 2.015160605299637e-06
Iter: 1439 loss: 2.0206857192373631e-06
Iter: 1440 loss: 2.0150612824296557e-06
Iter: 1441 loss: 2.0145492760515627e-06
Iter: 1442 loss: 2.0133255451045477e-06
Iter: 1443 loss: 2.0269885038685575e-06
Iter: 1444 loss: 2.0132048168005988e-06
Iter: 1445 loss: 2.0120556854739652e-06
Iter: 1446 loss: 2.0266193957892326e-06
Iter: 1447 loss: 2.0120461557202162e-06
Iter: 1448 loss: 2.0108484526784911e-06
Iter: 1449 loss: 2.0149374153981837e-06
Iter: 1450 loss: 2.0105285689853665e-06
Iter: 1451 loss: 2.0097434205410012e-06
Iter: 1452 loss: 2.008505361713883e-06
Iter: 1453 loss: 2.0084921180097209e-06
Iter: 1454 loss: 2.008000450060921e-06
Iter: 1455 loss: 2.0077684861451153e-06
Iter: 1456 loss: 2.0070710319051444e-06
Iter: 1457 loss: 2.0062852718653079e-06
Iter: 1458 loss: 2.0061810673660571e-06
Iter: 1459 loss: 2.0054440724551329e-06
Iter: 1460 loss: 2.0087361995101794e-06
Iter: 1461 loss: 2.0052993537075234e-06
Iter: 1462 loss: 2.0042865507270907e-06
Iter: 1463 loss: 2.0062285894399711e-06
Iter: 1464 loss: 2.0038622659968e-06
Iter: 1465 loss: 2.0029128728650155e-06
Iter: 1466 loss: 2.00299088458239e-06
Iter: 1467 loss: 2.002176251842697e-06
Iter: 1468 loss: 2.00093931071214e-06
Iter: 1469 loss: 2.0048753510381645e-06
Iter: 1470 loss: 2.0005856067322761e-06
Iter: 1471 loss: 2.0000551936576035e-06
Iter: 1472 loss: 2.0000395528847342e-06
Iter: 1473 loss: 1.9994985632691867e-06
Iter: 1474 loss: 1.998622537154269e-06
Iter: 1475 loss: 1.9986167590521528e-06
Iter: 1476 loss: 1.9977075005606334e-06
Iter: 1477 loss: 1.998176086180291e-06
Iter: 1478 loss: 1.9971039385706948e-06
Iter: 1479 loss: 1.9963759787496737e-06
Iter: 1480 loss: 1.9963551574383462e-06
Iter: 1481 loss: 1.9955320521478712e-06
Iter: 1482 loss: 1.9942638206815847e-06
Iter: 1483 loss: 1.9942447882627019e-06
Iter: 1484 loss: 1.9931345187326569e-06
Iter: 1485 loss: 1.9952315869941854e-06
Iter: 1486 loss: 1.9926652252741616e-06
Iter: 1487 loss: 1.9920887900504062e-06
Iter: 1488 loss: 1.9919334807014033e-06
Iter: 1489 loss: 1.9915411274768136e-06
Iter: 1490 loss: 1.9905935846216486e-06
Iter: 1491 loss: 2.0007800354390012e-06
Iter: 1492 loss: 1.9904917995715383e-06
Iter: 1493 loss: 1.9897396601038755e-06
Iter: 1494 loss: 1.9897189359033924e-06
Iter: 1495 loss: 1.9888810550138927e-06
Iter: 1496 loss: 1.9874852068981064e-06
Iter: 1497 loss: 1.9874806572643086e-06
Iter: 1498 loss: 1.9862915652991027e-06
Iter: 1499 loss: 1.9939198018260602e-06
Iter: 1500 loss: 1.9861604360418271e-06
Iter: 1501 loss: 1.9851794243975642e-06
Iter: 1502 loss: 1.9869207281871979e-06
Iter: 1503 loss: 1.98474973604072e-06
Iter: 1504 loss: 1.9838514694297326e-06
Iter: 1505 loss: 1.9973272657061828e-06
Iter: 1506 loss: 1.9838508813878697e-06
Iter: 1507 loss: 1.9832916630218425e-06
Iter: 1508 loss: 1.9823704059524411e-06
Iter: 1509 loss: 1.9823663152253207e-06
Iter: 1510 loss: 1.9815240507201874e-06
Iter: 1511 loss: 1.9830407791824985e-06
Iter: 1512 loss: 1.9811581318137281e-06
Iter: 1513 loss: 1.980257559561113e-06
Iter: 1514 loss: 1.9945697453776829e-06
Iter: 1515 loss: 1.9802575530307431e-06
Iter: 1516 loss: 1.9795696736705457e-06
Iter: 1517 loss: 1.978140977721986e-06
Iter: 1518 loss: 2.0027477400595941e-06
Iter: 1519 loss: 1.9781085640080831e-06
Iter: 1520 loss: 1.9772216664690279e-06
Iter: 1521 loss: 1.9904400944284229e-06
Iter: 1522 loss: 1.9772209639504563e-06
Iter: 1523 loss: 1.9763509161789476e-06
Iter: 1524 loss: 1.9799555065711436e-06
Iter: 1525 loss: 1.9761631486239503e-06
Iter: 1526 loss: 1.9756139562227972e-06
Iter: 1527 loss: 1.9746406024740956e-06
Iter: 1528 loss: 1.9746405950738161e-06
Iter: 1529 loss: 1.9740152043006592e-06
Iter: 1530 loss: 1.9738665161046729e-06
Iter: 1531 loss: 1.9734292582509916e-06
Iter: 1532 loss: 1.9723019990445492e-06
Iter: 1533 loss: 1.9815875762205357e-06
Iter: 1534 loss: 1.9721026123792716e-06
Iter: 1535 loss: 1.9708012859967589e-06
Iter: 1536 loss: 1.9805824686266105e-06
Iter: 1537 loss: 1.9706996187771952e-06
Iter: 1538 loss: 1.9697628696394481e-06
Iter: 1539 loss: 1.9779468888634086e-06
Iter: 1540 loss: 1.9697143721392747e-06
Iter: 1541 loss: 1.9690125716222525e-06
Iter: 1542 loss: 1.9720182239126531e-06
Iter: 1543 loss: 1.9688671615655534e-06
Iter: 1544 loss: 1.9683944812092818e-06
Iter: 1545 loss: 1.9673404758996013e-06
Iter: 1546 loss: 1.98215203385989e-06
Iter: 1547 loss: 1.9672852231129032e-06
Iter: 1548 loss: 1.9661052761151556e-06
Iter: 1549 loss: 1.97519917906014e-06
Iter: 1550 loss: 1.9660186409531244e-06
Iter: 1551 loss: 1.9647053504749722e-06
Iter: 1552 loss: 1.9704029207783176e-06
Iter: 1553 loss: 1.9644376445056922e-06
Iter: 1554 loss: 1.9637092225439647e-06
Iter: 1555 loss: 1.9627789636921887e-06
Iter: 1556 loss: 1.9627110697562154e-06
Iter: 1557 loss: 1.9623094354883724e-06
Iter: 1558 loss: 1.9621321402077716e-06
Iter: 1559 loss: 1.9615718537587288e-06
Iter: 1560 loss: 1.9607710800870132e-06
Iter: 1561 loss: 1.9607442685428653e-06
Iter: 1562 loss: 1.9600488188860114e-06
Iter: 1563 loss: 1.9642372223582876e-06
Iter: 1564 loss: 1.9599624779394485e-06
Iter: 1565 loss: 1.9590933062851541e-06
Iter: 1566 loss: 1.9609117373778545e-06
Iter: 1567 loss: 1.9587483422933315e-06
Iter: 1568 loss: 1.9580369439702369e-06
Iter: 1569 loss: 1.956660973260015e-06
Iter: 1570 loss: 1.985061677067274e-06
Iter: 1571 loss: 1.956652450796656e-06
Iter: 1572 loss: 1.9560268491290513e-06
Iter: 1573 loss: 1.9558665106964822e-06
Iter: 1574 loss: 1.9553114733122233e-06
Iter: 1575 loss: 1.9577014321832649e-06
Iter: 1576 loss: 1.9551972440040345e-06
Iter: 1577 loss: 1.9546469125311754e-06
Iter: 1578 loss: 1.9542336690407413e-06
Iter: 1579 loss: 1.954052829556241e-06
Iter: 1580 loss: 1.953133253665038e-06
Iter: 1581 loss: 1.9527585146216132e-06
Iter: 1582 loss: 1.9522718852529515e-06
Iter: 1583 loss: 1.9514690315841756e-06
Iter: 1584 loss: 1.9514422929110387e-06
Iter: 1585 loss: 1.950543165208166e-06
Iter: 1586 loss: 1.9498194304895943e-06
Iter: 1587 loss: 1.9495497642480462e-06
Iter: 1588 loss: 1.9486669373177893e-06
Iter: 1589 loss: 1.9498269557892111e-06
Iter: 1590 loss: 1.9482190272844094e-06
Iter: 1591 loss: 1.9478300918854215e-06
Iter: 1592 loss: 1.9476689361907814e-06
Iter: 1593 loss: 1.9473369073239877e-06
Iter: 1594 loss: 1.9464993164738738e-06
Iter: 1595 loss: 1.9540834547089441e-06
Iter: 1596 loss: 1.9463730587325713e-06
Iter: 1597 loss: 1.9458253911755018e-06
Iter: 1598 loss: 1.9457630451592352e-06
Iter: 1599 loss: 1.9451374000427268e-06
Iter: 1600 loss: 1.9439819099437402e-06
Iter: 1601 loss: 1.9705379012504742e-06
Iter: 1602 loss: 1.9439805059930088e-06
Iter: 1603 loss: 1.9429381727556707e-06
Iter: 1604 loss: 1.9443100635366754e-06
Iter: 1605 loss: 1.9424097470293532e-06
Iter: 1606 loss: 1.9418110300189936e-06
Iter: 1607 loss: 1.941715342108136e-06
Iter: 1608 loss: 1.9411663617546246e-06
Iter: 1609 loss: 1.9412804689974806e-06
Iter: 1610 loss: 1.9407596475031138e-06
Iter: 1611 loss: 1.9400980770056349e-06
Iter: 1612 loss: 1.9409774428024242e-06
Iter: 1613 loss: 1.9397640851170363e-06
Iter: 1614 loss: 1.9390362437173724e-06
Iter: 1615 loss: 1.9386358572746851e-06
Iter: 1616 loss: 1.9383139032249324e-06
Iter: 1617 loss: 1.9374930182548891e-06
Iter: 1618 loss: 1.9374550513238615e-06
Iter: 1619 loss: 1.9368104888103685e-06
Iter: 1620 loss: 1.9355092257944668e-06
Iter: 1621 loss: 1.9596490195449462e-06
Iter: 1622 loss: 1.9354900829174457e-06
Iter: 1623 loss: 1.934590298004426e-06
Iter: 1624 loss: 1.9442573631191417e-06
Iter: 1625 loss: 1.9345692343896488e-06
Iter: 1626 loss: 1.933721531840557e-06
Iter: 1627 loss: 1.9389230538604223e-06
Iter: 1628 loss: 1.9336198061654142e-06
Iter: 1629 loss: 1.9331664372549041e-06
Iter: 1630 loss: 1.932400923113596e-06
Iter: 1631 loss: 1.9323993398949188e-06
Iter: 1632 loss: 1.9317778698632447e-06
Iter: 1633 loss: 1.9317188707582183e-06
Iter: 1634 loss: 1.9313156379869563e-06
Iter: 1635 loss: 1.9302718579478166e-06
Iter: 1636 loss: 1.9387123832437836e-06
Iter: 1637 loss: 1.9300817238014882e-06
Iter: 1638 loss: 1.9288931985452541e-06
Iter: 1639 loss: 1.9340815071656286e-06
Iter: 1640 loss: 1.92865275142911e-06
Iter: 1641 loss: 1.9280326697790575e-06
Iter: 1642 loss: 1.927944613101894e-06
Iter: 1643 loss: 1.927578058860908e-06
Iter: 1644 loss: 1.9271405784665226e-06
Iter: 1645 loss: 1.9270954316957192e-06
Iter: 1646 loss: 1.9263849629081432e-06
Iter: 1647 loss: 1.9270312236260461e-06
Iter: 1648 loss: 1.9259730711205774e-06
Iter: 1649 loss: 1.9249866988394643e-06
Iter: 1650 loss: 1.9266557756828118e-06
Iter: 1651 loss: 1.924543294239372e-06
Iter: 1652 loss: 1.9234147693754234e-06
Iter: 1653 loss: 1.9359310120751774e-06
Iter: 1654 loss: 1.923392248124562e-06
Iter: 1655 loss: 1.9228121279974559e-06
Iter: 1656 loss: 1.9217242697243059e-06
Iter: 1657 loss: 1.9458644036895971e-06
Iter: 1658 loss: 1.9217216431059672e-06
Iter: 1659 loss: 1.9211862559270879e-06
Iter: 1660 loss: 1.9211091432504837e-06
Iter: 1661 loss: 1.9204490476247109e-06
Iter: 1662 loss: 1.9198123049606721e-06
Iter: 1663 loss: 1.9196667145453721e-06
Iter: 1664 loss: 1.9189643831184557e-06
Iter: 1665 loss: 1.922177845541725e-06
Iter: 1666 loss: 1.9188307719275541e-06
Iter: 1667 loss: 1.9179479177138581e-06
Iter: 1668 loss: 1.9207583451167922e-06
Iter: 1669 loss: 1.9176955543282685e-06
Iter: 1670 loss: 1.9170871815275705e-06
Iter: 1671 loss: 1.9158236513361152e-06
Iter: 1672 loss: 1.9375861363458236e-06
Iter: 1673 loss: 1.9157949938081333e-06
Iter: 1674 loss: 1.9153560574871465e-06
Iter: 1675 loss: 1.9152153795820208e-06
Iter: 1676 loss: 1.9146249217647195e-06
Iter: 1677 loss: 1.9156328491679484e-06
Iter: 1678 loss: 1.9143607244139231e-06
Iter: 1679 loss: 1.9138570133080483e-06
Iter: 1680 loss: 1.9132930480495647e-06
Iter: 1681 loss: 1.9132163514056685e-06
Iter: 1682 loss: 1.9121623272729029e-06
Iter: 1683 loss: 1.915997803539968e-06
Iter: 1684 loss: 1.9118988063933968e-06
Iter: 1685 loss: 1.9110897395083739e-06
Iter: 1686 loss: 1.9180059878107498e-06
Iter: 1687 loss: 1.9110449949862042e-06
Iter: 1688 loss: 1.910196891896815e-06
Iter: 1689 loss: 1.9101958990692428e-06
Iter: 1690 loss: 1.9095181065485177e-06
Iter: 1691 loss: 1.9086389582598095e-06
Iter: 1692 loss: 1.9087829613731864e-06
Iter: 1693 loss: 1.90797707714763e-06
Iter: 1694 loss: 1.9076845506746817e-06
Iter: 1695 loss: 1.9074281285200851e-06
Iter: 1696 loss: 1.9070596580087576e-06
Iter: 1697 loss: 1.9062148215424549e-06
Iter: 1698 loss: 1.9170875419717186e-06
Iter: 1699 loss: 1.9061568507612387e-06
Iter: 1700 loss: 1.9055333904444528e-06
Iter: 1701 loss: 1.905523529973836e-06
Iter: 1702 loss: 1.9048022055887453e-06
Iter: 1703 loss: 1.9033303620468541e-06
Iter: 1704 loss: 1.9298947575333977e-06
Iter: 1705 loss: 1.9033046423258173e-06
Iter: 1706 loss: 1.9022059412944342e-06
Iter: 1707 loss: 1.9059642952855316e-06
Iter: 1708 loss: 1.9019130892903314e-06
Iter: 1709 loss: 1.9015512554634291e-06
Iter: 1710 loss: 1.9014169749488919e-06
Iter: 1711 loss: 1.9009562186497203e-06
Iter: 1712 loss: 1.9000125288981433e-06
Iter: 1713 loss: 1.9168813989070654e-06
Iter: 1714 loss: 1.8999950637336707e-06
Iter: 1715 loss: 1.8990938009918287e-06
Iter: 1716 loss: 1.9020898369414589e-06
Iter: 1717 loss: 1.8988466782301808e-06
Iter: 1718 loss: 1.897882434168652e-06
Iter: 1719 loss: 1.9024069066833053e-06
Iter: 1720 loss: 1.8977051383474265e-06
Iter: 1721 loss: 1.8968616720290728e-06
Iter: 1722 loss: 1.9024885309342442e-06
Iter: 1723 loss: 1.8967756872995924e-06
Iter: 1724 loss: 1.8960506779170089e-06
Iter: 1725 loss: 1.89541491805999e-06
Iter: 1726 loss: 1.8952241711277637e-06
Iter: 1727 loss: 1.8944815896309481e-06
Iter: 1728 loss: 1.8993986093069159e-06
Iter: 1729 loss: 1.8944047301974682e-06
Iter: 1730 loss: 1.8936897786973579e-06
Iter: 1731 loss: 1.8987355386727331e-06
Iter: 1732 loss: 1.8936251453815062e-06
Iter: 1733 loss: 1.89319393792095e-06
Iter: 1734 loss: 1.8922588923059848e-06
Iter: 1735 loss: 1.9065388745775878e-06
Iter: 1736 loss: 1.8922225141033234e-06
Iter: 1737 loss: 1.8914003187973501e-06
Iter: 1738 loss: 1.8913541456227742e-06
Iter: 1739 loss: 1.8908352469170994e-06
Iter: 1740 loss: 1.8894922494257275e-06
Iter: 1741 loss: 1.9003602000588676e-06
Iter: 1742 loss: 1.8892478654065404e-06
Iter: 1743 loss: 1.8878955840060506e-06
Iter: 1744 loss: 1.8948855853073988e-06
Iter: 1745 loss: 1.8876792915052493e-06
Iter: 1746 loss: 1.8867130808649169e-06
Iter: 1747 loss: 1.8866890319137986e-06
Iter: 1748 loss: 1.8862501455120053e-06
Iter: 1749 loss: 1.8853158587662625e-06
Iter: 1750 loss: 1.9003539820610069e-06
Iter: 1751 loss: 1.8852866020635358e-06
Iter: 1752 loss: 1.8842825407313959e-06
Iter: 1753 loss: 1.8890529732002465e-06
Iter: 1754 loss: 1.8841010467969772e-06
Iter: 1755 loss: 1.8830794588183637e-06
Iter: 1756 loss: 1.8896288422266363e-06
Iter: 1757 loss: 1.8829666886915519e-06
Iter: 1758 loss: 1.8821226588977439e-06
Iter: 1759 loss: 1.8847584147655309e-06
Iter: 1760 loss: 1.8818770502251885e-06
Iter: 1761 loss: 1.8812106167791482e-06
Iter: 1762 loss: 1.8809857875212216e-06
Iter: 1763 loss: 1.8806035569112292e-06
Iter: 1764 loss: 1.8800823207145153e-06
Iter: 1765 loss: 1.8800703289099681e-06
Iter: 1766 loss: 1.8795231910770071e-06
Iter: 1767 loss: 1.8788706826514226e-06
Iter: 1768 loss: 1.8788031156866579e-06
Iter: 1769 loss: 1.8781057253957035e-06
Iter: 1770 loss: 1.8815955497001896e-06
Iter: 1771 loss: 1.8779886765275157e-06
Iter: 1772 loss: 1.8770663462761729e-06
Iter: 1773 loss: 1.8783622941732815e-06
Iter: 1774 loss: 1.876611961282952e-06
Iter: 1775 loss: 1.8757650134782676e-06
Iter: 1776 loss: 1.8739141641190303e-06
Iter: 1777 loss: 1.9015479772409809e-06
Iter: 1778 loss: 1.8738356822133663e-06
Iter: 1779 loss: 1.8739293203000545e-06
Iter: 1780 loss: 1.8731970659236576e-06
Iter: 1781 loss: 1.8725708092875583e-06
Iter: 1782 loss: 1.87295828051198e-06
Iter: 1783 loss: 1.8721698209728744e-06
Iter: 1784 loss: 1.8716425253931368e-06
Iter: 1785 loss: 1.8707614653487353e-06
Iter: 1786 loss: 1.8707588450765096e-06
Iter: 1787 loss: 1.8698524174546853e-06
Iter: 1788 loss: 1.8829527253689205e-06
Iter: 1789 loss: 1.8698509481869804e-06
Iter: 1790 loss: 1.8689790888944964e-06
Iter: 1791 loss: 1.8712287398490135e-06
Iter: 1792 loss: 1.8686808396013528e-06
Iter: 1793 loss: 1.8678678082181127e-06
Iter: 1794 loss: 1.8691594147763766e-06
Iter: 1795 loss: 1.8674901294932497e-06
Iter: 1796 loss: 1.8666586778239627e-06
Iter: 1797 loss: 1.8672330878618615e-06
Iter: 1798 loss: 1.866139406458305e-06
Iter: 1799 loss: 1.8654777394284161e-06
Iter: 1800 loss: 1.8654471896607686e-06
Iter: 1801 loss: 1.8650130677599639e-06
Iter: 1802 loss: 1.86418596847015e-06
Iter: 1803 loss: 1.8818765300343528e-06
Iter: 1804 loss: 1.8641825606879671e-06
Iter: 1805 loss: 1.8636584554482115e-06
Iter: 1806 loss: 1.8636237763514243e-06
Iter: 1807 loss: 1.8630592181855581e-06
Iter: 1808 loss: 1.8616897133065467e-06
Iter: 1809 loss: 1.876170412629186e-06
Iter: 1810 loss: 1.8615372221413097e-06
Iter: 1811 loss: 1.8603057251145825e-06
Iter: 1812 loss: 1.8654076070149721e-06
Iter: 1813 loss: 1.8600399541555617e-06
Iter: 1814 loss: 1.8599665483494015e-06
Iter: 1815 loss: 1.8596153533185378e-06
Iter: 1816 loss: 1.859284603605999e-06
Iter: 1817 loss: 1.858386366768046e-06
Iter: 1818 loss: 1.8640893775955668e-06
Iter: 1819 loss: 1.8581569419200284e-06
Iter: 1820 loss: 1.8571314707875611e-06
Iter: 1821 loss: 1.8608452481171852e-06
Iter: 1822 loss: 1.856873790804597e-06
Iter: 1823 loss: 1.8561996709654884e-06
Iter: 1824 loss: 1.8561798054712265e-06
Iter: 1825 loss: 1.8555658742380927e-06
Iter: 1826 loss: 1.854884004458252e-06
Iter: 1827 loss: 1.854788249729565e-06
Iter: 1828 loss: 1.8537438588824724e-06
Iter: 1829 loss: 1.8577118226425993e-06
Iter: 1830 loss: 1.8534947045123547e-06
Iter: 1831 loss: 1.8527626048184445e-06
Iter: 1832 loss: 1.8578071657679034e-06
Iter: 1833 loss: 1.8526928879073779e-06
Iter: 1834 loss: 1.8519774173578481e-06
Iter: 1835 loss: 1.854012173295944e-06
Iter: 1836 loss: 1.8517512320860424e-06
Iter: 1837 loss: 1.851234619778993e-06
Iter: 1838 loss: 1.850668122070325e-06
Iter: 1839 loss: 1.8505845196242472e-06
Iter: 1840 loss: 1.8494855224789845e-06
Iter: 1841 loss: 1.8592444622709718e-06
Iter: 1842 loss: 1.8494314723418577e-06
Iter: 1843 loss: 1.8488797475152083e-06
Iter: 1844 loss: 1.8478718518852462e-06
Iter: 1845 loss: 1.8716111380951938e-06
Iter: 1846 loss: 1.847871234402844e-06
Iter: 1847 loss: 1.8469370594700774e-06
Iter: 1848 loss: 1.8525328989368976e-06
Iter: 1849 loss: 1.84681993905772e-06
Iter: 1850 loss: 1.8459625232394241e-06
Iter: 1851 loss: 1.856019131132352e-06
Iter: 1852 loss: 1.8459500922630245e-06
Iter: 1853 loss: 1.845550330856162e-06
Iter: 1854 loss: 1.8446129164977095e-06
Iter: 1855 loss: 1.8558033257028014e-06
Iter: 1856 loss: 1.8445342427145691e-06
Iter: 1857 loss: 1.8434520471375734e-06
Iter: 1858 loss: 1.8476729905619375e-06
Iter: 1859 loss: 1.8432013306630535e-06
Iter: 1860 loss: 1.8421368644945933e-06
Iter: 1861 loss: 1.8560638249885598e-06
Iter: 1862 loss: 1.8421302427272785e-06
Iter: 1863 loss: 1.8414796701401884e-06
Iter: 1864 loss: 1.8407253920834766e-06
Iter: 1865 loss: 1.8406367316374822e-06
Iter: 1866 loss: 1.8398356123550211e-06
Iter: 1867 loss: 1.8500150797049373e-06
Iter: 1868 loss: 1.8398291136605504e-06
Iter: 1869 loss: 1.8393457393164197e-06
Iter: 1870 loss: 1.8424194423537397e-06
Iter: 1871 loss: 1.839291523354839e-06
Iter: 1872 loss: 1.8387887494981628e-06
Iter: 1873 loss: 1.838049734935017e-06
Iter: 1874 loss: 1.8380306627626076e-06
Iter: 1875 loss: 1.8373207236295061e-06
Iter: 1876 loss: 1.8451211169736922e-06
Iter: 1877 loss: 1.8373058514723267e-06
Iter: 1878 loss: 1.8365199468919996e-06
Iter: 1879 loss: 1.8362859730487302e-06
Iter: 1880 loss: 1.8358151304467318e-06
Iter: 1881 loss: 1.8350109042049148e-06
Iter: 1882 loss: 1.834005153260753e-06
Iter: 1883 loss: 1.8339228479085603e-06
Iter: 1884 loss: 1.8337617401393893e-06
Iter: 1885 loss: 1.8333592296992296e-06
Iter: 1886 loss: 1.8328203065154499e-06
Iter: 1887 loss: 1.8325836033111281e-06
Iter: 1888 loss: 1.8323090324678633e-06
Iter: 1889 loss: 1.8317200708293623e-06
Iter: 1890 loss: 1.830591563104046e-06
Iter: 1891 loss: 1.8544071454307463e-06
Iter: 1892 loss: 1.8305860893981243e-06
Iter: 1893 loss: 1.8296388398978848e-06
Iter: 1894 loss: 1.8296142201347739e-06
Iter: 1895 loss: 1.8286315317820811e-06
Iter: 1896 loss: 1.8307323078395491e-06
Iter: 1897 loss: 1.8282469542602223e-06
Iter: 1898 loss: 1.8275920927101774e-06
Iter: 1899 loss: 1.8276230396275789e-06
Iter: 1900 loss: 1.8270773590239193e-06
Iter: 1901 loss: 1.8264055553592266e-06
Iter: 1902 loss: 1.837150195271484e-06
Iter: 1903 loss: 1.8264055553418501e-06
Iter: 1904 loss: 1.8258992556661591e-06
Iter: 1905 loss: 1.8268124198548785e-06
Iter: 1906 loss: 1.8256794808142856e-06
Iter: 1907 loss: 1.8250964381255823e-06
Iter: 1908 loss: 1.8248208112223359e-06
Iter: 1909 loss: 1.8245357716643279e-06
Iter: 1910 loss: 1.8239528668782414e-06
Iter: 1911 loss: 1.8239477230422752e-06
Iter: 1912 loss: 1.8234200891665541e-06
Iter: 1913 loss: 1.8220643336480916e-06
Iter: 1914 loss: 1.8333978187765365e-06
Iter: 1915 loss: 1.8218302169988009e-06
Iter: 1916 loss: 1.8205142466787218e-06
Iter: 1917 loss: 1.8280859193512016e-06
Iter: 1918 loss: 1.8203372063954367e-06
Iter: 1919 loss: 1.820083101414126e-06
Iter: 1920 loss: 1.8198401485047409e-06
Iter: 1921 loss: 1.8194872360639514e-06
Iter: 1922 loss: 1.8186788643309633e-06
Iter: 1923 loss: 1.8291150070619438e-06
Iter: 1924 loss: 1.8186238950825313e-06
Iter: 1925 loss: 1.8177658841295534e-06
Iter: 1926 loss: 1.8177767837014559e-06
Iter: 1927 loss: 1.8170827387454095e-06
Iter: 1928 loss: 1.81662928550506e-06
Iter: 1929 loss: 1.8164127355779617e-06
Iter: 1930 loss: 1.8157896887811519e-06
Iter: 1931 loss: 1.8146913607987666e-06
Iter: 1932 loss: 1.8146912976518683e-06
Iter: 1933 loss: 1.813676352196732e-06
Iter: 1934 loss: 1.8185050475894822e-06
Iter: 1935 loss: 1.8134932700597489e-06
Iter: 1936 loss: 1.8126882376811623e-06
Iter: 1937 loss: 1.8241385252063918e-06
Iter: 1938 loss: 1.8126864952819642e-06
Iter: 1939 loss: 1.8122455566336771e-06
Iter: 1940 loss: 1.8124771362163314e-06
Iter: 1941 loss: 1.8119538837778154e-06
Iter: 1942 loss: 1.8113667964648502e-06
Iter: 1943 loss: 1.8119487909997582e-06
Iter: 1944 loss: 1.8110356033848784e-06
Iter: 1945 loss: 1.8102014052976672e-06
Iter: 1946 loss: 1.8146638484036767e-06
Iter: 1947 loss: 1.8100749133426905e-06
Iter: 1948 loss: 1.8094623714016505e-06
Iter: 1949 loss: 1.8082838226474287e-06
Iter: 1950 loss: 1.8329134925167152e-06
Iter: 1951 loss: 1.8082774346426625e-06
Iter: 1952 loss: 1.80723303390952e-06
Iter: 1953 loss: 1.8143712084812825e-06
Iter: 1954 loss: 1.8071318138639399e-06
Iter: 1955 loss: 1.8061287444711037e-06
Iter: 1956 loss: 1.8161244371956734e-06
Iter: 1957 loss: 1.8060960532177346e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.4/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.8
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi0.8
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.8 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi0.8
+ date
Sun Nov  8 05:48:44 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.8/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.4/300_300_300_1 --function f1 --psi 3 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.8/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9b054510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9b14a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9b186b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9b186c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9b08c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9b08c158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9b03f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9af1a840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9b03f378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9af28378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9af281e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9aea17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9af1a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9aedf8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9af80400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9af61730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9af61488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9adcca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9ae76620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9ae76c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc96bbb488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc96bbbd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9ae186a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9ae32950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9ae328c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc96bfe620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc96bfd8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc96ac4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc96ac4048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc96ac47b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc96ae7bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc96a3e730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc96a3e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc96a36ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc96a12a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fcc9697a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.0053180562333770874
test_loss: 0.005870094260264064
train_loss: 0.005166868591148338
test_loss: 0.005701243563554675
train_loss: 0.0049063369204397555
test_loss: 0.005367626375443743
train_loss: 0.004863938291337501
test_loss: 0.005671268244873516
train_loss: 0.0053447132737817135
test_loss: 0.005364726230582311
train_loss: 0.004998155417210868
test_loss: 0.005666912108746558
train_loss: 0.004830343760183877
test_loss: 0.005282787207365017
train_loss: 0.004535686175521977
test_loss: 0.00527891164429679
train_loss: 0.005170613560969542
test_loss: 0.005239699110467099
train_loss: 0.004767984433670377
test_loss: 0.005233200672322318
train_loss: 0.0046615742026672865
test_loss: 0.00516619004952513
train_loss: 0.004854942522041202
test_loss: 0.005361993059374088
train_loss: 0.004790610459642122
test_loss: 0.005118609021122347
train_loss: 0.0047257582581932515
test_loss: 0.00517756158655993
train_loss: 0.004742215505252979
test_loss: 0.0056842125638049724
train_loss: 0.004811974262525269
test_loss: 0.005307750164926562
train_loss: 0.004641244273557785
test_loss: 0.0053738569474030716
train_loss: 0.004679604668408964
test_loss: 0.0050436508813664896
train_loss: 0.0044069668391626664
test_loss: 0.00526391573854166
train_loss: 0.0045970513551390774
test_loss: 0.00508983601705473
train_loss: 0.004586455580598985
test_loss: 0.005366481933222075
train_loss: 0.004540573663304614
test_loss: 0.005022530757194383
train_loss: 0.004237227558171297
test_loss: 0.0051122400638972205
train_loss: 0.004666755434909964
test_loss: 0.005069010522837146
train_loss: 0.004617489288809288
test_loss: 0.005071496467557978
train_loss: 0.004247123207565342
test_loss: 0.005033230563662579
train_loss: 0.004486533700289824
test_loss: 0.0050608919253141964
train_loss: 0.004381959258872494
test_loss: 0.005032304341770836
train_loss: 0.004643960501693947
test_loss: 0.005222461687647319
train_loss: 0.003945112714381014
test_loss: 0.004666145677013716
train_loss: 0.004383919014252276
test_loss: 0.005192183844395694
train_loss: 0.0041998207544846206
test_loss: 0.0049631086776923935
train_loss: 0.004524391926402631
test_loss: 0.005036406275449747
train_loss: 0.005016119163359192
test_loss: 0.005131188575086142
train_loss: 0.0040796294703177825
test_loss: 0.004577594003181744
train_loss: 0.004764322458117536
test_loss: 0.005135228134985817
train_loss: 0.0040600969031334725
test_loss: 0.004846648081527667
train_loss: 0.0040637444897872466
test_loss: 0.004822480523441468
train_loss: 0.00442474901142544
test_loss: 0.004741560235663882
train_loss: 0.004430754787110992
test_loss: 0.004751257765536454
train_loss: 0.0039978486740739234
test_loss: 0.004725925122726253
train_loss: 0.004317841644728725
test_loss: 0.004946463431758225
train_loss: 0.004096638445490844
test_loss: 0.004718520553034979
train_loss: 0.004273273214201396
test_loss: 0.004724178120764066
train_loss: 0.0043152271834325795
test_loss: 0.0046850805608920675
train_loss: 0.0040485290195944905
test_loss: 0.004673033325672639
train_loss: 0.004042938423564351
test_loss: 0.00492278047029454
train_loss: 0.004427272336344828
test_loss: 0.004801699313754667
train_loss: 0.004123511448711688
test_loss: 0.004918497872411982
train_loss: 0.004228358900279246
test_loss: 0.004619829394938934
train_loss: 0.0041214329651998735
test_loss: 0.004690068055131042
train_loss: 0.004386024489380845
test_loss: 0.0048192570442332295
train_loss: 0.004014446437141486
test_loss: 0.004663161063363855
train_loss: 0.003969372347241564
test_loss: 0.00478577202848877
train_loss: 0.003993741101318253
test_loss: 0.004548880216520198
train_loss: 0.004124911464510211
test_loss: 0.004796162735836279
train_loss: 0.0041468291282726905
test_loss: 0.004663724763844895
train_loss: 0.004134807407133907
test_loss: 0.004770402488114571
train_loss: 0.003990758187376722
test_loss: 0.004614752720526233
train_loss: 0.0037088260447875132
test_loss: 0.004646875924809932
train_loss: 0.0038758502269012956
test_loss: 0.004699741440116183
train_loss: 0.004072476692121585
test_loss: 0.004839199251463281
train_loss: 0.004394353230254563
test_loss: 0.004747190331265748
train_loss: 0.0045627779731159685
test_loss: 0.0046120577212532855
train_loss: 0.0040616626313466075
test_loss: 0.004571219010782623
train_loss: 0.00395023934659779
test_loss: 0.0049956393835052845
train_loss: 0.0037570321019769273
test_loss: 0.004420503427856541
train_loss: 0.004121756384738263
test_loss: 0.004684667110404883
train_loss: 0.003679048273047535
test_loss: 0.004464108310296148
train_loss: 0.0039319390567784385
test_loss: 0.004644829710281466
train_loss: 0.00415664844031259
test_loss: 0.0045373817156021835
train_loss: 0.003696966039508769
test_loss: 0.0043030236195460685
train_loss: 0.003993476211690083
test_loss: 0.004481361557458287
train_loss: 0.003986652019364204
test_loss: 0.004724162878321993
train_loss: 0.00397564743314901
test_loss: 0.004544183451034473
train_loss: 0.003875856864248513
test_loss: 0.004891294023995051
train_loss: 0.003939967146006872
test_loss: 0.004591570657410255
train_loss: 0.004004877151880289
test_loss: 0.004674609468165707
train_loss: 0.00407457258544593
test_loss: 0.004869560614331499
train_loss: 0.003901272399366778
test_loss: 0.004397274478898997
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi0.8/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.8/300_300_300_1 --optimizer lbfgs --function f1 --psi 3 --phi 0.8 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi0.8/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6750034378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6750034840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec7dbea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f67700491e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f6770049c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec7a3598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec760e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec7abbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec71f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec72d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec71fe18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec673158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec66be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec6a07b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec61b8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec5ea400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec5eab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec5cf840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec5cf9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec5657b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec5457b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec545ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec4bb730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec4f59d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec4f5840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec4acb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec441d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec441ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec41d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec42d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec3da8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec3ac158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec394268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec356950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec394840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f66ec3042f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.3434108384696366e-05
Iter: 2 loss: 2.1228772272795965e-05
Iter: 3 loss: 1.9554558982402018e-05
Iter: 4 loss: 1.7737700245624137e-05
Iter: 5 loss: 2.3782609432989368e-05
Iter: 6 loss: 1.7238359753703319e-05
Iter: 7 loss: 1.6016074540041651e-05
Iter: 8 loss: 1.5161762217870203e-05
Iter: 9 loss: 1.4726775870824118e-05
Iter: 10 loss: 1.3687635478105931e-05
Iter: 11 loss: 1.3643341497052929e-05
Iter: 12 loss: 1.3009701372818468e-05
Iter: 13 loss: 1.4110619228665847e-05
Iter: 14 loss: 1.2728434544896449e-05
Iter: 15 loss: 1.2152997682450741e-05
Iter: 16 loss: 1.1694023477821714e-05
Iter: 17 loss: 1.1519610298358953e-05
Iter: 18 loss: 1.1028983922237797e-05
Iter: 19 loss: 1.0980256266985188e-05
Iter: 20 loss: 1.0518586760873416e-05
Iter: 21 loss: 1.0331312491190435e-05
Iter: 22 loss: 1.0086273035099315e-05
Iter: 23 loss: 9.6839706037267248e-06
Iter: 24 loss: 9.3433248989209911e-06
Iter: 25 loss: 9.231343868658388e-06
Iter: 26 loss: 8.7129995026642267e-06
Iter: 27 loss: 1.2895952840118086e-05
Iter: 28 loss: 8.6791344745266334e-06
Iter: 29 loss: 8.3270777326288766e-06
Iter: 30 loss: 8.3236090982570355e-06
Iter: 31 loss: 8.1133708188424547e-06
Iter: 32 loss: 7.6816602889466131e-06
Iter: 33 loss: 1.5354929775242572e-05
Iter: 34 loss: 7.6733644399370323e-06
Iter: 35 loss: 7.2145689924255541e-06
Iter: 36 loss: 8.3022487668095975e-06
Iter: 37 loss: 7.0474392755673688e-06
Iter: 38 loss: 6.8856999806872092e-06
Iter: 39 loss: 6.8245622312124475e-06
Iter: 40 loss: 6.6156656622367459e-06
Iter: 41 loss: 6.3051427742372365e-06
Iter: 42 loss: 6.2979918281935886e-06
Iter: 43 loss: 6.1225504207777937e-06
Iter: 44 loss: 7.8245162193953567e-06
Iter: 45 loss: 6.11622989729191e-06
Iter: 46 loss: 5.9270391877334189e-06
Iter: 47 loss: 6.2234592921855747e-06
Iter: 48 loss: 5.8385828184017134e-06
Iter: 49 loss: 5.6444387996623132e-06
Iter: 50 loss: 5.8119118975848022e-06
Iter: 51 loss: 5.5299715118577531e-06
Iter: 52 loss: 5.3713843782738295e-06
Iter: 53 loss: 6.5506198758616661e-06
Iter: 54 loss: 5.3587021748016206e-06
Iter: 55 loss: 5.2037098859250489e-06
Iter: 56 loss: 5.7872242478708691e-06
Iter: 57 loss: 5.1664106998141343e-06
Iter: 58 loss: 5.0357063625810518e-06
Iter: 59 loss: 5.0524092680638386e-06
Iter: 60 loss: 4.9359208353427349e-06
Iter: 61 loss: 4.8241847100326736e-06
Iter: 62 loss: 4.8892762783390618e-06
Iter: 63 loss: 4.7517488090034806e-06
Iter: 64 loss: 4.621357737928907e-06
Iter: 65 loss: 5.23666110024484e-06
Iter: 66 loss: 4.5975246319153933e-06
Iter: 67 loss: 4.5067037169856193e-06
Iter: 68 loss: 4.50396710996416e-06
Iter: 69 loss: 4.4591837075348571e-06
Iter: 70 loss: 4.3393856749794028e-06
Iter: 71 loss: 5.1648236925246285e-06
Iter: 72 loss: 4.3118382069348015e-06
Iter: 73 loss: 4.1957627809746222e-06
Iter: 74 loss: 5.2982750868507609e-06
Iter: 75 loss: 4.191209846090952e-06
Iter: 76 loss: 4.1399636407956464e-06
Iter: 77 loss: 4.1384497234768644e-06
Iter: 78 loss: 4.0868133347931268e-06
Iter: 79 loss: 4.0418102908087164e-06
Iter: 80 loss: 4.02808485054328e-06
Iter: 81 loss: 3.9688310925034261e-06
Iter: 82 loss: 4.0614426855600806e-06
Iter: 83 loss: 3.9410884469488354e-06
Iter: 84 loss: 3.8987755932779078e-06
Iter: 85 loss: 3.896468535074941e-06
Iter: 86 loss: 3.8699564200987862e-06
Iter: 87 loss: 3.7999825483422028e-06
Iter: 88 loss: 4.3161628543864643e-06
Iter: 89 loss: 3.785342783736855e-06
Iter: 90 loss: 3.7552520273710047e-06
Iter: 91 loss: 3.7406454924759839e-06
Iter: 92 loss: 3.7016379639635652e-06
Iter: 93 loss: 3.6952937983853244e-06
Iter: 94 loss: 3.6684529543449744e-06
Iter: 95 loss: 3.6316696320487593e-06
Iter: 96 loss: 3.6926216050304448e-06
Iter: 97 loss: 3.6149341426598174e-06
Iter: 98 loss: 3.5707570525978471e-06
Iter: 99 loss: 3.6107474753755451e-06
Iter: 100 loss: 3.5451105876319295e-06
Iter: 101 loss: 3.5128127620404293e-06
Iter: 102 loss: 3.9792528853788188e-06
Iter: 103 loss: 3.512759223885692e-06
Iter: 104 loss: 3.4774313920038823e-06
Iter: 105 loss: 3.5179261464135516e-06
Iter: 106 loss: 3.4584939452569521e-06
Iter: 107 loss: 3.4306227054509505e-06
Iter: 108 loss: 3.413727192354042e-06
Iter: 109 loss: 3.4023148233822309e-06
Iter: 110 loss: 3.3577609504941763e-06
Iter: 111 loss: 3.3950251134021569e-06
Iter: 112 loss: 3.3312799267530212e-06
Iter: 113 loss: 3.33570126127687e-06
Iter: 114 loss: 3.3140525182026858e-06
Iter: 115 loss: 3.2978686170961079e-06
Iter: 116 loss: 3.2650817810762662e-06
Iter: 117 loss: 3.8673698907783922e-06
Iter: 118 loss: 3.2645701364189638e-06
Iter: 119 loss: 3.2325499199194503e-06
Iter: 120 loss: 3.3193690601161451e-06
Iter: 121 loss: 3.2220201158308597e-06
Iter: 122 loss: 3.195199899638809e-06
Iter: 123 loss: 3.1950884252643695e-06
Iter: 124 loss: 3.1799458746264164e-06
Iter: 125 loss: 3.1442720064623106e-06
Iter: 126 loss: 3.5632715109222559e-06
Iter: 127 loss: 3.1411549307099117e-06
Iter: 128 loss: 3.1182998766104069e-06
Iter: 129 loss: 3.1153090401667086e-06
Iter: 130 loss: 3.0932372566680497e-06
Iter: 131 loss: 3.09848029647301e-06
Iter: 132 loss: 3.0770630951179517e-06
Iter: 133 loss: 3.0576582632656908e-06
Iter: 134 loss: 3.0379292054836432e-06
Iter: 135 loss: 3.0341125421278209e-06
Iter: 136 loss: 3.0109795657863963e-06
Iter: 137 loss: 3.2835791592438867e-06
Iter: 138 loss: 3.0106554389277897e-06
Iter: 139 loss: 2.9936804936584958e-06
Iter: 140 loss: 3.1521818282620184e-06
Iter: 141 loss: 2.992973702548988e-06
Iter: 142 loss: 2.9779860126970367e-06
Iter: 143 loss: 2.9907383298064444e-06
Iter: 144 loss: 2.9691190194734352e-06
Iter: 145 loss: 2.9533431587521447e-06
Iter: 146 loss: 2.9359332883435368e-06
Iter: 147 loss: 2.9334267911016313e-06
Iter: 148 loss: 2.91145677961298e-06
Iter: 149 loss: 2.9641772449170622e-06
Iter: 150 loss: 2.9035176884924167e-06
Iter: 151 loss: 2.877299707595115e-06
Iter: 152 loss: 2.9833815635626855e-06
Iter: 153 loss: 2.8714810907950609e-06
Iter: 154 loss: 2.867246165518966e-06
Iter: 155 loss: 2.8616695006382884e-06
Iter: 156 loss: 2.8543558977324215e-06
Iter: 157 loss: 2.8344985478775452e-06
Iter: 158 loss: 2.9606980318624257e-06
Iter: 159 loss: 2.8294341432135203e-06
Iter: 160 loss: 2.8115497077192785e-06
Iter: 161 loss: 2.93541341897749e-06
Iter: 162 loss: 2.8098661949009542e-06
Iter: 163 loss: 2.7897871440231815e-06
Iter: 164 loss: 2.9086878550349031e-06
Iter: 165 loss: 2.7872183976655264e-06
Iter: 166 loss: 2.7769147707916739e-06
Iter: 167 loss: 2.77964468949529e-06
Iter: 168 loss: 2.7694378023965357e-06
Iter: 169 loss: 2.760541879812862e-06
Iter: 170 loss: 2.7604676447028879e-06
Iter: 171 loss: 2.753167060078094e-06
Iter: 172 loss: 2.7367212865778441e-06
Iter: 173 loss: 2.960750431636436e-06
Iter: 174 loss: 2.7357675981246105e-06
Iter: 175 loss: 2.7193537989371184e-06
Iter: 176 loss: 2.7833918071028272e-06
Iter: 177 loss: 2.7155528763781926e-06
Iter: 178 loss: 2.7036144298435309e-06
Iter: 179 loss: 2.7506667197923352e-06
Iter: 180 loss: 2.7008810618659286e-06
Iter: 181 loss: 2.6846477644196029e-06
Iter: 182 loss: 2.7636222577580113e-06
Iter: 183 loss: 2.681808630846781e-06
Iter: 184 loss: 2.6736471751890978e-06
Iter: 185 loss: 2.663459757905698e-06
Iter: 186 loss: 2.662617918307731e-06
Iter: 187 loss: 2.6516330468511342e-06
Iter: 188 loss: 2.6973836464511543e-06
Iter: 189 loss: 2.6492763174160547e-06
Iter: 190 loss: 2.6354886690235279e-06
Iter: 191 loss: 2.6470182935080991e-06
Iter: 192 loss: 2.627297991619856e-06
Iter: 193 loss: 2.62060454444055e-06
Iter: 194 loss: 2.6198243627626896e-06
Iter: 195 loss: 2.6128904178407203e-06
Iter: 196 loss: 2.6180124228583727e-06
Iter: 197 loss: 2.6086304097004153e-06
Iter: 198 loss: 2.6003416620872131e-06
Iter: 199 loss: 2.5903682611721185e-06
Iter: 200 loss: 2.5893780083583212e-06
Iter: 201 loss: 2.5834576621844709e-06
Iter: 202 loss: 2.5828826784281374e-06
Iter: 203 loss: 2.5758268432510233e-06
Iter: 204 loss: 2.5714009721589773e-06
Iter: 205 loss: 2.5685970125830467e-06
Iter: 206 loss: 2.5623825323389673e-06
Iter: 207 loss: 2.6033205617007788e-06
Iter: 208 loss: 2.5617325334821012e-06
Iter: 209 loss: 2.5549634898123432e-06
Iter: 210 loss: 2.5724759290379477e-06
Iter: 211 loss: 2.5526525238583798e-06
Iter: 212 loss: 2.5471799800789356e-06
Iter: 213 loss: 2.5364448686738118e-06
Iter: 214 loss: 2.7505908686274273e-06
Iter: 215 loss: 2.5363530643704541e-06
Iter: 216 loss: 2.5271306282987473e-06
Iter: 217 loss: 2.6020625191725077e-06
Iter: 218 loss: 2.5265408128857073e-06
Iter: 219 loss: 2.5198878720277203e-06
Iter: 220 loss: 2.5768599303319174e-06
Iter: 221 loss: 2.5195219182714328e-06
Iter: 222 loss: 2.5109135035201125e-06
Iter: 223 loss: 2.5080812089507782e-06
Iter: 224 loss: 2.5030976946196073e-06
Iter: 225 loss: 2.4951234741829607e-06
Iter: 226 loss: 2.4969443170475447e-06
Iter: 227 loss: 2.4892594142402897e-06
Iter: 228 loss: 2.4802039178635866e-06
Iter: 229 loss: 2.4912821826088635e-06
Iter: 230 loss: 2.4754732076907979e-06
Iter: 231 loss: 2.4719489088563011e-06
Iter: 232 loss: 2.4697236933016874e-06
Iter: 233 loss: 2.4655401445196703e-06
Iter: 234 loss: 2.4666807486792591e-06
Iter: 235 loss: 2.4625126270725237e-06
Iter: 236 loss: 2.4578837314348961e-06
Iter: 237 loss: 2.4535097974304339e-06
Iter: 238 loss: 2.4524458276835259e-06
Iter: 239 loss: 2.4450398778648655e-06
Iter: 240 loss: 2.5194792070155679e-06
Iter: 241 loss: 2.4448065294534491e-06
Iter: 242 loss: 2.4389279068321411e-06
Iter: 243 loss: 2.4697023800720435e-06
Iter: 244 loss: 2.43800564278126e-06
Iter: 245 loss: 2.4334466450775e-06
Iter: 246 loss: 2.4297814153953438e-06
Iter: 247 loss: 2.4284118773508039e-06
Iter: 248 loss: 2.4234905664017626e-06
Iter: 249 loss: 2.4234445526107288e-06
Iter: 250 loss: 2.4196258002014542e-06
Iter: 251 loss: 2.4148259881185126e-06
Iter: 252 loss: 2.4144434824305876e-06
Iter: 253 loss: 2.4087382108917222e-06
Iter: 254 loss: 2.4025509649593216e-06
Iter: 255 loss: 2.4015986122491828e-06
Iter: 256 loss: 2.3932772077589776e-06
Iter: 257 loss: 2.436016694223664e-06
Iter: 258 loss: 2.3919336731048419e-06
Iter: 259 loss: 2.3849814564092144e-06
Iter: 260 loss: 2.3849029559725044e-06
Iter: 261 loss: 2.3808210202262854e-06
Iter: 262 loss: 2.376611904901671e-06
Iter: 263 loss: 2.3758359191944632e-06
Iter: 264 loss: 2.3705848346730016e-06
Iter: 265 loss: 2.3713454854392232e-06
Iter: 266 loss: 2.3666038477806709e-06
Iter: 267 loss: 2.3605232960397287e-06
Iter: 268 loss: 2.39269854946894e-06
Iter: 269 loss: 2.3595852405511941e-06
Iter: 270 loss: 2.356565808167101e-06
Iter: 271 loss: 2.3561221638540843e-06
Iter: 272 loss: 2.3535190889670552e-06
Iter: 273 loss: 2.3460781540228487e-06
Iter: 274 loss: 2.3801929999844609e-06
Iter: 275 loss: 2.3433815018261324e-06
Iter: 276 loss: 2.3369728803885477e-06
Iter: 277 loss: 2.419100229544129e-06
Iter: 278 loss: 2.336924458286719e-06
Iter: 279 loss: 2.3320977722323226e-06
Iter: 280 loss: 2.3917077153520261e-06
Iter: 281 loss: 2.3320486270387519e-06
Iter: 282 loss: 2.3291440331366605e-06
Iter: 283 loss: 2.3286362026054798e-06
Iter: 284 loss: 2.3266608520534994e-06
Iter: 285 loss: 2.32340026184445e-06
Iter: 286 loss: 2.3488794047674982e-06
Iter: 287 loss: 2.3231690650488708e-06
Iter: 288 loss: 2.3201666838088747e-06
Iter: 289 loss: 2.3240944231232976e-06
Iter: 290 loss: 2.3186404977968098e-06
Iter: 291 loss: 2.3153035272177856e-06
Iter: 292 loss: 2.3119280507646858e-06
Iter: 293 loss: 2.311264447953533e-06
Iter: 294 loss: 2.3049853178030744e-06
Iter: 295 loss: 2.3136456857198363e-06
Iter: 296 loss: 2.3018660486742234e-06
Iter: 297 loss: 2.2961331438129555e-06
Iter: 298 loss: 2.3180924926592548e-06
Iter: 299 loss: 2.2947780347030833e-06
Iter: 300 loss: 2.2922093331564518e-06
Iter: 301 loss: 2.2914167701924305e-06
Iter: 302 loss: 2.2896538565983731e-06
Iter: 303 loss: 2.2853015609580676e-06
Iter: 304 loss: 2.32832455603388e-06
Iter: 305 loss: 2.2847448988773063e-06
Iter: 306 loss: 2.2797476660875814e-06
Iter: 307 loss: 2.2964848774763621e-06
Iter: 308 loss: 2.2783873924175179e-06
Iter: 309 loss: 2.2735949684124242e-06
Iter: 310 loss: 2.2768628207203486e-06
Iter: 311 loss: 2.2705924348960882e-06
Iter: 312 loss: 2.2688540162694645e-06
Iter: 313 loss: 2.2674813452852624e-06
Iter: 314 loss: 2.2648371093341991e-06
Iter: 315 loss: 2.261131733562463e-06
Iter: 316 loss: 2.2609857009348743e-06
Iter: 317 loss: 2.2574870695689334e-06
Iter: 318 loss: 2.2536676113788072e-06
Iter: 319 loss: 2.2530942915956876e-06
Iter: 320 loss: 2.253911294933622e-06
Iter: 321 loss: 2.2504559123981204e-06
Iter: 322 loss: 2.2488473467933768e-06
Iter: 323 loss: 2.2458388098943163e-06
Iter: 324 loss: 2.31302531535e-06
Iter: 325 loss: 2.2458322643091711e-06
Iter: 326 loss: 2.242608130821998e-06
Iter: 327 loss: 2.269149873760935e-06
Iter: 328 loss: 2.2424093758248582e-06
Iter: 329 loss: 2.2388982805068463e-06
Iter: 330 loss: 2.23949449520814e-06
Iter: 331 loss: 2.2362607154100444e-06
Iter: 332 loss: 2.2327806343086561e-06
Iter: 333 loss: 2.2365487596271928e-06
Iter: 334 loss: 2.2308760977362615e-06
Iter: 335 loss: 2.2279635358830544e-06
Iter: 336 loss: 2.2411716483146731e-06
Iter: 337 loss: 2.227403090616435e-06
Iter: 338 loss: 2.2240644177061532e-06
Iter: 339 loss: 2.2237396389022752e-06
Iter: 340 loss: 2.2212935024012956e-06
Iter: 341 loss: 2.2184131714637063e-06
Iter: 342 loss: 2.2572736789205253e-06
Iter: 343 loss: 2.2184002357459535e-06
Iter: 344 loss: 2.2147267160374821e-06
Iter: 345 loss: 2.2143461261744443e-06
Iter: 346 loss: 2.2116706103493433e-06
Iter: 347 loss: 2.2081527691570564e-06
Iter: 348 loss: 2.2059593868797365e-06
Iter: 349 loss: 2.2045538507857591e-06
Iter: 350 loss: 2.1997138664102922e-06
Iter: 351 loss: 2.2169719288346192e-06
Iter: 352 loss: 2.1984775065704264e-06
Iter: 353 loss: 2.1954456423946788e-06
Iter: 354 loss: 2.2266263826480558e-06
Iter: 355 loss: 2.1953589895578283e-06
Iter: 356 loss: 2.1930228103040848e-06
Iter: 357 loss: 2.220836146752164e-06
Iter: 358 loss: 2.1929920346436558e-06
Iter: 359 loss: 2.1912473072512436e-06
Iter: 360 loss: 2.186973321953353e-06
Iter: 361 loss: 2.2305288799482986e-06
Iter: 362 loss: 2.1864589973921322e-06
Iter: 363 loss: 2.1818789714815737e-06
Iter: 364 loss: 2.194465633072961e-06
Iter: 365 loss: 2.1803893953154466e-06
Iter: 366 loss: 2.1778163142253915e-06
Iter: 367 loss: 2.1772768753433054e-06
Iter: 368 loss: 2.1754316313328594e-06
Iter: 369 loss: 2.1722910040040026e-06
Iter: 370 loss: 2.1722863370299573e-06
Iter: 371 loss: 2.1699379886844783e-06
Iter: 372 loss: 2.2022289031899973e-06
Iter: 373 loss: 2.1699296201817856e-06
Iter: 374 loss: 2.1677007208727053e-06
Iter: 375 loss: 2.1706467996112779e-06
Iter: 376 loss: 2.1665728207720983e-06
Iter: 377 loss: 2.1645930195949705e-06
Iter: 378 loss: 2.1626852184448753e-06
Iter: 379 loss: 2.1622476437693257e-06
Iter: 380 loss: 2.1588042813963209e-06
Iter: 381 loss: 2.1633315874310657e-06
Iter: 382 loss: 2.1570580992283539e-06
Iter: 383 loss: 2.1523896728126135e-06
Iter: 384 loss: 2.1842950869807656e-06
Iter: 385 loss: 2.1519371192577845e-06
Iter: 386 loss: 2.149889559629915e-06
Iter: 387 loss: 2.1497887336467696e-06
Iter: 388 loss: 2.1485913537923387e-06
Iter: 389 loss: 2.1462512621786389e-06
Iter: 390 loss: 2.1933267936648492e-06
Iter: 391 loss: 2.146232778337125e-06
Iter: 392 loss: 2.1433307560062719e-06
Iter: 393 loss: 2.1428895529724392e-06
Iter: 394 loss: 2.1408713137810649e-06
Iter: 395 loss: 2.1377068295307644e-06
Iter: 396 loss: 2.157528224435081e-06
Iter: 397 loss: 2.137341432605871e-06
Iter: 398 loss: 2.1336336679178657e-06
Iter: 399 loss: 2.1299357462418216e-06
Iter: 400 loss: 2.1291741933654178e-06
Iter: 401 loss: 2.134647890848606e-06
Iter: 402 loss: 2.1277281516993728e-06
Iter: 403 loss: 2.1265929533820642e-06
Iter: 404 loss: 2.1236290494285204e-06
Iter: 405 loss: 2.1466564659545511e-06
Iter: 406 loss: 2.1230547290537727e-06
Iter: 407 loss: 2.119892933850011e-06
Iter: 408 loss: 2.1279478852784311e-06
Iter: 409 loss: 2.1188005477153569e-06
Iter: 410 loss: 2.1177811333635033e-06
Iter: 411 loss: 2.1171534805657846e-06
Iter: 412 loss: 2.115888602676697e-06
Iter: 413 loss: 2.1129530053813923e-06
Iter: 414 loss: 2.1492564892244018e-06
Iter: 415 loss: 2.1127280920479158e-06
Iter: 416 loss: 2.1109118881968821e-06
Iter: 417 loss: 2.1107760839922143e-06
Iter: 418 loss: 2.1088642619837738e-06
Iter: 419 loss: 2.106236112072849e-06
Iter: 420 loss: 2.1061163148113895e-06
Iter: 421 loss: 2.1032603107787977e-06
Iter: 422 loss: 2.108999106884822e-06
Iter: 423 loss: 2.1020975227759735e-06
Iter: 424 loss: 2.0997715440821316e-06
Iter: 425 loss: 2.1072374006290923e-06
Iter: 426 loss: 2.099111865298611e-06
Iter: 427 loss: 2.0973465424975585e-06
Iter: 428 loss: 2.1244225842752215e-06
Iter: 429 loss: 2.09734603904898e-06
Iter: 430 loss: 2.0957005651157203e-06
Iter: 431 loss: 2.0979425516873632e-06
Iter: 432 loss: 2.0948787312035014e-06
Iter: 433 loss: 2.0930243984043404e-06
Iter: 434 loss: 2.0923857712081887e-06
Iter: 435 loss: 2.0913305869420823e-06
Iter: 436 loss: 2.0888531611227425e-06
Iter: 437 loss: 2.0855256325267406e-06
Iter: 438 loss: 2.0853472750861909e-06
Iter: 439 loss: 2.08156352831927e-06
Iter: 440 loss: 2.1310951724552058e-06
Iter: 441 loss: 2.0815401078810233e-06
Iter: 442 loss: 2.0791296990554667e-06
Iter: 443 loss: 2.0832500034020489e-06
Iter: 444 loss: 2.0780519513650741e-06
Iter: 445 loss: 2.0768854130435937e-06
Iter: 446 loss: 2.0764787470163922e-06
Iter: 447 loss: 2.075377870368791e-06
Iter: 448 loss: 2.0729266661785326e-06
Iter: 449 loss: 2.1075273714032792e-06
Iter: 450 loss: 2.072800062825284e-06
Iter: 451 loss: 2.0701412833888218e-06
Iter: 452 loss: 2.0707508900651526e-06
Iter: 453 loss: 2.0681868639639856e-06
Iter: 454 loss: 2.0679060960482972e-06
Iter: 455 loss: 2.0665364269709871e-06
Iter: 456 loss: 2.0654544071787378e-06
Iter: 457 loss: 2.0634641661829393e-06
Iter: 458 loss: 2.1096190989017641e-06
Iter: 459 loss: 2.063462241828535e-06
Iter: 460 loss: 2.0617468280783824e-06
Iter: 461 loss: 2.0855589896421926e-06
Iter: 462 loss: 2.061741440829161e-06
Iter: 463 loss: 2.0602760599634285e-06
Iter: 464 loss: 2.0628769821275148e-06
Iter: 465 loss: 2.0596342040779115e-06
Iter: 466 loss: 2.058375193744443e-06
Iter: 467 loss: 2.0553310726170683e-06
Iter: 468 loss: 2.0879170402494435e-06
Iter: 469 loss: 2.0550009611957157e-06
Iter: 470 loss: 2.0516239996543248e-06
Iter: 471 loss: 2.076830876546404e-06
Iter: 472 loss: 2.0513557487120967e-06
Iter: 473 loss: 2.0496997171200082e-06
Iter: 474 loss: 2.0494833525689234e-06
Iter: 475 loss: 2.0482678071663519e-06
Iter: 476 loss: 2.0470389872901826e-06
Iter: 477 loss: 2.0467968407854706e-06
Iter: 478 loss: 2.0446993934231462e-06
Iter: 479 loss: 2.0464145168284365e-06
Iter: 480 loss: 2.0434450641179034e-06
Iter: 481 loss: 2.0413162496934513e-06
Iter: 482 loss: 2.0443333868505658e-06
Iter: 483 loss: 2.0402716520422075e-06
Iter: 484 loss: 2.0375334862980325e-06
Iter: 485 loss: 2.0429127465239668e-06
Iter: 486 loss: 2.0364030706545566e-06
Iter: 487 loss: 2.0336238982466771e-06
Iter: 488 loss: 2.0386281250415755e-06
Iter: 489 loss: 2.0324165308211818e-06
Iter: 490 loss: 2.0313683644297178e-06
Iter: 491 loss: 2.0306205468752886e-06
Iter: 492 loss: 2.0298751592627446e-06
Iter: 493 loss: 2.027943848881754e-06
Iter: 494 loss: 2.0434959700080464e-06
Iter: 495 loss: 2.0275896343237294e-06
Iter: 496 loss: 2.0251858962148874e-06
Iter: 497 loss: 2.0315922727043834e-06
Iter: 498 loss: 2.0243842779795556e-06
Iter: 499 loss: 2.0238066077332734e-06
Iter: 500 loss: 2.0231953515824378e-06
Iter: 501 loss: 2.0224016441400916e-06
Iter: 502 loss: 2.020093186360875e-06
Iter: 503 loss: 2.029346058327359e-06
Iter: 504 loss: 2.01915205364254e-06
Iter: 505 loss: 2.0171358002437066e-06
Iter: 506 loss: 2.0169067694745316e-06
Iter: 507 loss: 2.0151351980173163e-06
Iter: 508 loss: 2.0139803037476767e-06
Iter: 509 loss: 2.0133012142577397e-06
Iter: 510 loss: 2.0113004711689359e-06
Iter: 511 loss: 2.0112385049330638e-06
Iter: 512 loss: 2.0096811574926159e-06
Iter: 513 loss: 2.0077130308822292e-06
Iter: 514 loss: 2.031661828442681e-06
Iter: 515 loss: 2.0076907064574548e-06
Iter: 516 loss: 2.00589365769685e-06
Iter: 517 loss: 2.0179370784683963e-06
Iter: 518 loss: 2.0057122042439073e-06
Iter: 519 loss: 2.0047421207161727e-06
Iter: 520 loss: 2.003043662971462e-06
Iter: 521 loss: 2.0030433459464197e-06
Iter: 522 loss: 2.0005030027769692e-06
Iter: 523 loss: 2.0062470816295504e-06
Iter: 524 loss: 1.9995458317966027e-06
Iter: 525 loss: 1.9971751307901745e-06
Iter: 526 loss: 2.0000034206071943e-06
Iter: 527 loss: 1.9959239801072466e-06
Iter: 528 loss: 1.9930699781314525e-06
Iter: 529 loss: 2.0020816699233266e-06
Iter: 530 loss: 1.9922478995334846e-06
Iter: 531 loss: 1.9907962156094355e-06
Iter: 532 loss: 2.0111203944620884e-06
Iter: 533 loss: 1.9907921859157049e-06
Iter: 534 loss: 1.9893495176981065e-06
Iter: 535 loss: 1.9957193724947839e-06
Iter: 536 loss: 1.9890619243738813e-06
Iter: 537 loss: 1.9878636145177565e-06
Iter: 538 loss: 1.9854815328473534e-06
Iter: 539 loss: 2.0314527529869057e-06
Iter: 540 loss: 1.985454931562803e-06
Iter: 541 loss: 1.9834578140611221e-06
Iter: 542 loss: 1.9881824203281457e-06
Iter: 543 loss: 1.9827287679936513e-06
Iter: 544 loss: 1.9805514799729046e-06
Iter: 545 loss: 1.9805513767133491e-06
Iter: 546 loss: 1.9793586143483754e-06
Iter: 547 loss: 1.9783706000188067e-06
Iter: 548 loss: 1.9780273605536272e-06
Iter: 549 loss: 1.9766843626605831e-06
Iter: 550 loss: 1.9959985236807941e-06
Iter: 551 loss: 1.9766819697342376e-06
Iter: 552 loss: 1.9754000586632592e-06
Iter: 553 loss: 1.9746272523215878e-06
Iter: 554 loss: 1.9740998558557179e-06
Iter: 555 loss: 1.9725851367383661e-06
Iter: 556 loss: 1.9713038049659169e-06
Iter: 557 loss: 1.9708816065780369e-06
Iter: 558 loss: 1.9694888403490997e-06
Iter: 559 loss: 1.9694268546503218e-06
Iter: 560 loss: 1.9677490316324409e-06
Iter: 561 loss: 1.9678586168238387e-06
Iter: 562 loss: 1.9664390964933186e-06
Iter: 563 loss: 1.9650112680795055e-06
Iter: 564 loss: 1.9641893934055993e-06
Iter: 565 loss: 1.9635792463823181e-06
Iter: 566 loss: 1.9612645191089588e-06
Iter: 567 loss: 1.9730475015283459e-06
Iter: 568 loss: 1.9608856878184104e-06
Iter: 569 loss: 1.9589415078845174e-06
Iter: 570 loss: 1.9611491377307917e-06
Iter: 571 loss: 1.9578958479145319e-06
Iter: 572 loss: 1.9557741028387882e-06
Iter: 573 loss: 1.9606122454380412e-06
Iter: 574 loss: 1.9549791781684685e-06
Iter: 575 loss: 1.9527375917026317e-06
Iter: 576 loss: 1.96268678122051e-06
Iter: 577 loss: 1.9522937301887171e-06
Iter: 578 loss: 1.95125454965695e-06
Iter: 579 loss: 1.95106352547036e-06
Iter: 580 loss: 1.9503726407742142e-06
Iter: 581 loss: 1.948360871818639e-06
Iter: 582 loss: 1.9563444421487605e-06
Iter: 583 loss: 1.9475340310487254e-06
Iter: 584 loss: 1.9453482976936344e-06
Iter: 585 loss: 1.9607491260956337e-06
Iter: 586 loss: 1.945150026982276e-06
Iter: 587 loss: 1.9428486005118704e-06
Iter: 588 loss: 1.9614083378595475e-06
Iter: 589 loss: 1.9426984921797536e-06
Iter: 590 loss: 1.9413104168185549e-06
Iter: 591 loss: 1.9404886423454831e-06
Iter: 592 loss: 1.9399088258079915e-06
Iter: 593 loss: 1.938870718405382e-06
Iter: 594 loss: 1.9388674733315747e-06
Iter: 595 loss: 1.9376973580841833e-06
Iter: 596 loss: 1.936727723304839e-06
Iter: 597 loss: 1.9363911937989248e-06
Iter: 598 loss: 1.9348873415125579e-06
Iter: 599 loss: 1.935226284756991e-06
Iter: 600 loss: 1.9337803243675032e-06
Iter: 601 loss: 1.9324819092161027e-06
Iter: 602 loss: 1.9484392881798476e-06
Iter: 603 loss: 1.9324681943583027e-06
Iter: 604 loss: 1.9307711249523764e-06
Iter: 605 loss: 1.9296143823716573e-06
Iter: 606 loss: 1.9289921673832843e-06
Iter: 607 loss: 1.9273913069878327e-06
Iter: 608 loss: 1.9291877161317657e-06
Iter: 609 loss: 1.9265265155362314e-06
Iter: 610 loss: 1.9247624931393284e-06
Iter: 611 loss: 1.9323999824589174e-06
Iter: 612 loss: 1.9244019874921857e-06
Iter: 613 loss: 1.9225254470106331e-06
Iter: 614 loss: 1.9217114672358897e-06
Iter: 615 loss: 1.9207490998634391e-06
Iter: 616 loss: 1.9184810073636208e-06
Iter: 617 loss: 1.9324675250575503e-06
Iter: 618 loss: 1.9182114543469396e-06
Iter: 619 loss: 1.9164460973390459e-06
Iter: 620 loss: 1.9244666755216676e-06
Iter: 621 loss: 1.916107073272581e-06
Iter: 622 loss: 1.9149407546878029e-06
Iter: 623 loss: 1.9149131825969666e-06
Iter: 624 loss: 1.9141501917296203e-06
Iter: 625 loss: 1.9122072519520771e-06
Iter: 626 loss: 1.9291056831355255e-06
Iter: 627 loss: 1.911893163654148e-06
Iter: 628 loss: 1.9108362304821565e-06
Iter: 629 loss: 1.9107634467020634e-06
Iter: 630 loss: 1.9094939773408322e-06
Iter: 631 loss: 1.9088467668848528e-06
Iter: 632 loss: 1.908254707788393e-06
Iter: 633 loss: 1.906414033966777e-06
Iter: 634 loss: 1.9046311649030255e-06
Iter: 635 loss: 1.9042285900747821e-06
Iter: 636 loss: 1.9048722848182518e-06
Iter: 637 loss: 1.9033501428445834e-06
Iter: 638 loss: 1.9025961554053996e-06
Iter: 639 loss: 1.9012496486216724e-06
Iter: 640 loss: 1.9345886759858449e-06
Iter: 641 loss: 1.9012496263031132e-06
Iter: 642 loss: 1.8996317571091291e-06
Iter: 643 loss: 1.9009939695182505e-06
Iter: 644 loss: 1.8986721055593838e-06
Iter: 645 loss: 1.8976772128827798e-06
Iter: 646 loss: 1.8976766870471561e-06
Iter: 647 loss: 1.8964450983805978e-06
Iter: 648 loss: 1.8958636696585304e-06
Iter: 649 loss: 1.8952610927580587e-06
Iter: 650 loss: 1.8937390828393754e-06
Iter: 651 loss: 1.8918608339024591e-06
Iter: 652 loss: 1.8916961434496535e-06
Iter: 653 loss: 1.8891081959341397e-06
Iter: 654 loss: 1.9057630462508429e-06
Iter: 655 loss: 1.8888246988472109e-06
Iter: 656 loss: 1.8870074075764275e-06
Iter: 657 loss: 1.8978743679876908e-06
Iter: 658 loss: 1.8867787848250526e-06
Iter: 659 loss: 1.8853273698028801e-06
Iter: 660 loss: 1.8872186425587866e-06
Iter: 661 loss: 1.8845884176224355e-06
Iter: 662 loss: 1.8834290032120682e-06
Iter: 663 loss: 1.8833763214311722e-06
Iter: 664 loss: 1.8825932524984487e-06
Iter: 665 loss: 1.8811360546324952e-06
Iter: 666 loss: 1.9140487045020816e-06
Iter: 667 loss: 1.881133493895617e-06
Iter: 668 loss: 1.8793956171427888e-06
Iter: 669 loss: 1.8809808219006142e-06
Iter: 670 loss: 1.8783888811067849e-06
Iter: 671 loss: 1.8762380231125198e-06
Iter: 672 loss: 1.9066615621557747e-06
Iter: 673 loss: 1.8762329158158571e-06
Iter: 674 loss: 1.8751456774116771e-06
Iter: 675 loss: 1.8737062912981465e-06
Iter: 676 loss: 1.8736215536859822e-06
Iter: 677 loss: 1.8727205324995981e-06
Iter: 678 loss: 1.8727177257525906e-06
Iter: 679 loss: 1.8715853972202848e-06
Iter: 680 loss: 1.8703868818635811e-06
Iter: 681 loss: 1.8701852130462653e-06
Iter: 682 loss: 1.86898097457891e-06
Iter: 683 loss: 1.8704162925125012e-06
Iter: 684 loss: 1.8683452148934445e-06
Iter: 685 loss: 1.8667357244027951e-06
Iter: 686 loss: 1.8717139273094993e-06
Iter: 687 loss: 1.8662632257800921e-06
Iter: 688 loss: 1.8638602089530304e-06
Iter: 689 loss: 1.8701850771539244e-06
Iter: 690 loss: 1.8630508934308783e-06
Iter: 691 loss: 1.8618567143927489e-06
Iter: 692 loss: 1.8599643900868556e-06
Iter: 693 loss: 1.8599457140881931e-06
Iter: 694 loss: 1.8578025742594911e-06
Iter: 695 loss: 1.8655868784398492e-06
Iter: 696 loss: 1.8572657186647266e-06
Iter: 697 loss: 1.8557125874620411e-06
Iter: 698 loss: 1.8610352846636649e-06
Iter: 699 loss: 1.8552993777422798e-06
Iter: 700 loss: 1.8533281563083202e-06
Iter: 701 loss: 1.8572511520132531e-06
Iter: 702 loss: 1.8525207429979889e-06
Iter: 703 loss: 1.8510716032902921e-06
Iter: 704 loss: 1.8509683244536323e-06
Iter: 705 loss: 1.8499777618923985e-06
Iter: 706 loss: 1.8479713170876094e-06
Iter: 707 loss: 1.8848797076570724e-06
Iter: 708 loss: 1.8479401314042235e-06
Iter: 709 loss: 1.8460656374488662e-06
Iter: 710 loss: 1.8545182345505804e-06
Iter: 711 loss: 1.845702022503699e-06
Iter: 712 loss: 1.8443221509855793e-06
Iter: 713 loss: 1.8657618058847943e-06
Iter: 714 loss: 1.8443219618305749e-06
Iter: 715 loss: 1.8434886043040268e-06
Iter: 716 loss: 1.8450993780870703e-06
Iter: 717 loss: 1.8431411687696641e-06
Iter: 718 loss: 1.8421904524903785e-06
Iter: 719 loss: 1.8402759511469749e-06
Iter: 720 loss: 1.8760259550376836e-06
Iter: 721 loss: 1.8402489683389173e-06
Iter: 722 loss: 1.8397869551426971e-06
Iter: 723 loss: 1.8390777805546784e-06
Iter: 724 loss: 1.8384850491584116e-06
Iter: 725 loss: 1.8369307169244816e-06
Iter: 726 loss: 1.8487564434838547e-06
Iter: 727 loss: 1.8366200009335381e-06
Iter: 728 loss: 1.8344252603921389e-06
Iter: 729 loss: 1.8407806074728713e-06
Iter: 730 loss: 1.8337420477684462e-06
Iter: 731 loss: 1.8333019704862694e-06
Iter: 732 loss: 1.8328041331004008e-06
Iter: 733 loss: 1.8321295262107676e-06
Iter: 734 loss: 1.83072066178324e-06
Iter: 735 loss: 1.8546246986562881e-06
Iter: 736 loss: 1.8306861786389154e-06
Iter: 737 loss: 1.8292541149695027e-06
Iter: 738 loss: 1.8292510051882383e-06
Iter: 739 loss: 1.8281075136975316e-06
Iter: 740 loss: 1.825831593297794e-06
Iter: 741 loss: 1.8267568319051369e-06
Iter: 742 loss: 1.8242617441812335e-06
Iter: 743 loss: 1.8218512854484337e-06
Iter: 744 loss: 1.8507736974360974e-06
Iter: 745 loss: 1.8218211300157674e-06
Iter: 746 loss: 1.8201000099505955e-06
Iter: 747 loss: 1.8213869843546926e-06
Iter: 748 loss: 1.8190458173599597e-06
Iter: 749 loss: 1.8184782919395575e-06
Iter: 750 loss: 1.8180557708987764e-06
Iter: 751 loss: 1.8171284774158964e-06
Iter: 752 loss: 1.8167418593867174e-06
Iter: 753 loss: 1.8162566204020955e-06
Iter: 754 loss: 1.8152823050896456e-06
Iter: 755 loss: 1.8135925865518891e-06
Iter: 756 loss: 1.8135917606499263e-06
Iter: 757 loss: 1.8124512551952994e-06
Iter: 758 loss: 1.8122271006291022e-06
Iter: 759 loss: 1.8111042395822222e-06
Iter: 760 loss: 1.8107081980641227e-06
Iter: 761 loss: 1.8100751784983173e-06
Iter: 762 loss: 1.8089384431978675e-06
Iter: 763 loss: 1.8118530823039089e-06
Iter: 764 loss: 1.8085476640025983e-06
Iter: 765 loss: 1.8074658254221846e-06
Iter: 766 loss: 1.8210733949421381e-06
Iter: 767 loss: 1.80745627333521e-06
Iter: 768 loss: 1.8066932926726198e-06
Iter: 769 loss: 1.8057194553599734e-06
Iter: 770 loss: 1.805648142210683e-06
Iter: 771 loss: 1.8042108354015242e-06
Iter: 772 loss: 1.8060707861002386e-06
Iter: 773 loss: 1.8034768796494458e-06
Iter: 774 loss: 1.8020701756143269e-06
Iter: 775 loss: 1.8157830469828669e-06
Iter: 776 loss: 1.8020201979862017e-06
Iter: 777 loss: 1.8006542525884156e-06
Iter: 778 loss: 1.8043356427187258e-06
Iter: 779 loss: 1.8002027997412695e-06
Iter: 780 loss: 1.7990558879012211e-06
Iter: 781 loss: 1.796271258275064e-06
Iter: 782 loss: 1.8256205486863306e-06
Iter: 783 loss: 1.7959590140020373e-06
Iter: 784 loss: 1.7939804165119535e-06
Iter: 785 loss: 1.8142409902430808e-06
Iter: 786 loss: 1.7939227984005617e-06
Iter: 787 loss: 1.7923676576370936e-06
Iter: 788 loss: 1.7969194052265679e-06
Iter: 789 loss: 1.7918880135527474e-06
Iter: 790 loss: 1.7908125579117035e-06
Iter: 791 loss: 1.7907527030422753e-06
Iter: 792 loss: 1.7898484407440297e-06
Iter: 793 loss: 1.7886261111678041e-06
Iter: 794 loss: 1.7885633641015578e-06
Iter: 795 loss: 1.7871530162196613e-06
Iter: 796 loss: 1.7860513950528728e-06
Iter: 797 loss: 1.7856107439342151e-06
Iter: 798 loss: 1.7852367427431058e-06
Iter: 799 loss: 1.784577855076806e-06
Iter: 800 loss: 1.7837275026786386e-06
Iter: 801 loss: 1.7828208579988351e-06
Iter: 802 loss: 1.7826722570288088e-06
Iter: 803 loss: 1.7817472494087883e-06
Iter: 804 loss: 1.7861387999341735e-06
Iter: 805 loss: 1.7815798955860113e-06
Iter: 806 loss: 1.7803382261461757e-06
Iter: 807 loss: 1.7828680772858296e-06
Iter: 808 loss: 1.7798370568539105e-06
Iter: 809 loss: 1.7788964093020456e-06
Iter: 810 loss: 1.7768067508924991e-06
Iter: 811 loss: 1.8065106633005368e-06
Iter: 812 loss: 1.7767013143121034e-06
Iter: 813 loss: 1.7760792900053612e-06
Iter: 814 loss: 1.7755807381571517e-06
Iter: 815 loss: 1.7746318120488214e-06
Iter: 816 loss: 1.7768423658154902e-06
Iter: 817 loss: 1.7742815967406022e-06
Iter: 818 loss: 1.7733090005198204e-06
Iter: 819 loss: 1.7718768629267553e-06
Iter: 820 loss: 1.7718405508528919e-06
Iter: 821 loss: 1.7703080230037494e-06
Iter: 822 loss: 1.7812931996326116e-06
Iter: 823 loss: 1.7701741954955084e-06
Iter: 824 loss: 1.7689195039302982e-06
Iter: 825 loss: 1.767683303292344e-06
Iter: 826 loss: 1.7674186408081906e-06
Iter: 827 loss: 1.7664965810924858e-06
Iter: 828 loss: 1.7663740050128679e-06
Iter: 829 loss: 1.7652152699720456e-06
Iter: 830 loss: 1.7664038197606717e-06
Iter: 831 loss: 1.7645690399952733e-06
Iter: 832 loss: 1.7636044263033389e-06
Iter: 833 loss: 1.7629484397185175e-06
Iter: 834 loss: 1.7625939539098981e-06
Iter: 835 loss: 1.761380786425061e-06
Iter: 836 loss: 1.7676490339719882e-06
Iter: 837 loss: 1.761186648200145e-06
Iter: 838 loss: 1.7600591679577538e-06
Iter: 839 loss: 1.7715585700688964e-06
Iter: 840 loss: 1.7600257723216538e-06
Iter: 841 loss: 1.7590807888617484e-06
Iter: 842 loss: 1.7569591929186721e-06
Iter: 843 loss: 1.786151438034337e-06
Iter: 844 loss: 1.7568401157042458e-06
Iter: 845 loss: 1.7565963733816736e-06
Iter: 846 loss: 1.7558668754448928e-06
Iter: 847 loss: 1.7550673357855038e-06
Iter: 848 loss: 1.7532855191579576e-06
Iter: 849 loss: 1.7783687838366989e-06
Iter: 850 loss: 1.7531926598780409e-06
Iter: 851 loss: 1.7514691566370666e-06
Iter: 852 loss: 1.7559929672415516e-06
Iter: 853 loss: 1.7508873694046817e-06
Iter: 854 loss: 1.7509787033899665e-06
Iter: 855 loss: 1.7502589546621815e-06
Iter: 856 loss: 1.7498673645733055e-06
Iter: 857 loss: 1.7489706146395516e-06
Iter: 858 loss: 1.7605578697898587e-06
Iter: 859 loss: 1.7489097758242769e-06
Iter: 860 loss: 1.7477074536030371e-06
Iter: 861 loss: 1.7480339314869046e-06
Iter: 862 loss: 1.746837045316403e-06
Iter: 863 loss: 1.7451753189928152e-06
Iter: 864 loss: 1.7453589233160598e-06
Iter: 865 loss: 1.7438995102279072e-06
Iter: 866 loss: 1.7420030080769238e-06
Iter: 867 loss: 1.7420028197315495e-06
Iter: 868 loss: 1.7407660400488513e-06
Iter: 869 loss: 1.7460233171961994e-06
Iter: 870 loss: 1.7405073732277132e-06
Iter: 871 loss: 1.7392395301688279e-06
Iter: 872 loss: 1.7474807163986712e-06
Iter: 873 loss: 1.7391033210317558e-06
Iter: 874 loss: 1.738451738835479e-06
Iter: 875 loss: 1.7374600684672741e-06
Iter: 876 loss: 1.7374426458970757e-06
Iter: 877 loss: 1.7362378505385108e-06
Iter: 878 loss: 1.7371028049918763e-06
Iter: 879 loss: 1.7354923572398482e-06
Iter: 880 loss: 1.7343970536619459e-06
Iter: 881 loss: 1.7343217627437835e-06
Iter: 882 loss: 1.733546857910549e-06
Iter: 883 loss: 1.7324567071373859e-06
Iter: 884 loss: 1.7324150776004461e-06
Iter: 885 loss: 1.7311503174466906e-06
Iter: 886 loss: 1.7417755451884033e-06
Iter: 887 loss: 1.7310766983433753e-06
Iter: 888 loss: 1.729690433279781e-06
Iter: 889 loss: 1.7303633984769035e-06
Iter: 890 loss: 1.728760552631143e-06
Iter: 891 loss: 1.7278758941623305e-06
Iter: 892 loss: 1.7274456789692529e-06
Iter: 893 loss: 1.7270204859988433e-06
Iter: 894 loss: 1.7261393982576169e-06
Iter: 895 loss: 1.7260976759600894e-06
Iter: 896 loss: 1.7251091614805238e-06
Iter: 897 loss: 1.7231368653144503e-06
Iter: 898 loss: 1.7608304298607327e-06
Iter: 899 loss: 1.7231132551026951e-06
Iter: 900 loss: 1.7214688354498449e-06
Iter: 901 loss: 1.7221156940748004e-06
Iter: 902 loss: 1.7203292566063851e-06
Iter: 903 loss: 1.7183334783153129e-06
Iter: 904 loss: 1.7314299364583912e-06
Iter: 905 loss: 1.7181231588880402e-06
Iter: 906 loss: 1.7168201701857996e-06
Iter: 907 loss: 1.7199307987124186e-06
Iter: 908 loss: 1.7163476800053951e-06
Iter: 909 loss: 1.7151917488881592e-06
Iter: 910 loss: 1.728850397975558e-06
Iter: 911 loss: 1.7151757601913344e-06
Iter: 912 loss: 1.7139787454572084e-06
Iter: 913 loss: 1.7174465150561669e-06
Iter: 914 loss: 1.7136062900643027e-06
Iter: 915 loss: 1.7128876653677781e-06
Iter: 916 loss: 1.7120173412846488e-06
Iter: 917 loss: 1.7119335642341081e-06
Iter: 918 loss: 1.7101963770283689e-06
Iter: 919 loss: 1.7121939834691506e-06
Iter: 920 loss: 1.709266581832088e-06
Iter: 921 loss: 1.7083559035412452e-06
Iter: 922 loss: 1.7083392268561974e-06
Iter: 923 loss: 1.7075183476643927e-06
Iter: 924 loss: 1.7099593686680584e-06
Iter: 925 loss: 1.7072686820556613e-06
Iter: 926 loss: 1.7064267924054163e-06
Iter: 927 loss: 1.7050474232075186e-06
Iter: 928 loss: 1.7050404123148949e-06
Iter: 929 loss: 1.7042228139629115e-06
Iter: 930 loss: 1.7040942990775256e-06
Iter: 931 loss: 1.7033105077528848e-06
Iter: 932 loss: 1.7019356510525965e-06
Iter: 933 loss: 1.7019354530512719e-06
Iter: 934 loss: 1.7005440678523738e-06
Iter: 935 loss: 1.7039136703387506e-06
Iter: 936 loss: 1.700044759808201e-06
Iter: 937 loss: 1.6982979356901054e-06
Iter: 938 loss: 1.7145781207014359e-06
Iter: 939 loss: 1.6982247829020943e-06
Iter: 940 loss: 1.6974832137372997e-06
Iter: 941 loss: 1.6961086456186871e-06
Iter: 942 loss: 1.7274323455052953e-06
Iter: 943 loss: 1.6961066335972213e-06
Iter: 944 loss: 1.6947861833688791e-06
Iter: 945 loss: 1.698591317724273e-06
Iter: 946 loss: 1.6943734435540461e-06
Iter: 947 loss: 1.6929232580463676e-06
Iter: 948 loss: 1.6959061812538283e-06
Iter: 949 loss: 1.6923414628219167e-06
Iter: 950 loss: 1.6907165581536472e-06
Iter: 951 loss: 1.6945273346522502e-06
Iter: 952 loss: 1.6901197145839401e-06
Iter: 953 loss: 1.6903295363757783e-06
Iter: 954 loss: 1.6895067342272375e-06
Iter: 955 loss: 1.6890561541321124e-06
Iter: 956 loss: 1.6877112141414253e-06
Iter: 957 loss: 1.6919635515647576e-06
Iter: 958 loss: 1.6870563184179778e-06
Iter: 959 loss: 1.685671088957832e-06
Iter: 960 loss: 1.6936999971823174e-06
Iter: 961 loss: 1.6854870649887961e-06
Iter: 962 loss: 1.6842614481803696e-06
Iter: 963 loss: 1.6938503489756145e-06
Iter: 964 loss: 1.6841748261655071e-06
Iter: 965 loss: 1.6829520839067972e-06
Iter: 966 loss: 1.6894145920365866e-06
Iter: 967 loss: 1.6827631015382847e-06
Iter: 968 loss: 1.6819098371448936e-06
Iter: 969 loss: 1.6798339444792905e-06
Iter: 970 loss: 1.7015516648550863e-06
Iter: 971 loss: 1.6795973857695605e-06
Iter: 972 loss: 1.6789643445963192e-06
Iter: 973 loss: 1.6782967757003736e-06
Iter: 974 loss: 1.6775367608624127e-06
Iter: 975 loss: 1.6767029058159327e-06
Iter: 976 loss: 1.6765801106180389e-06
Iter: 977 loss: 1.6757296355566552e-06
Iter: 978 loss: 1.6825227284969621e-06
Iter: 979 loss: 1.6756727051586631e-06
Iter: 980 loss: 1.6747610713564039e-06
Iter: 981 loss: 1.6767413848325073e-06
Iter: 982 loss: 1.6744080788008539e-06
Iter: 983 loss: 1.6735805351522978e-06
Iter: 984 loss: 1.6741353951461661e-06
Iter: 985 loss: 1.6730600592281947e-06
Iter: 986 loss: 1.6720638528248691e-06
Iter: 987 loss: 1.6711947678517877e-06
Iter: 988 loss: 1.6709304025712349e-06
Iter: 989 loss: 1.6691715009357762e-06
Iter: 990 loss: 1.674096927459772e-06
Iter: 991 loss: 1.6686082014283757e-06
Iter: 992 loss: 1.6672014288716253e-06
Iter: 993 loss: 1.6817177389920917e-06
Iter: 994 loss: 1.6671618155819432e-06
Iter: 995 loss: 1.6661815990569952e-06
Iter: 996 loss: 1.6798782759431346e-06
Iter: 997 loss: 1.6661787987348123e-06
Iter: 998 loss: 1.6656926413310102e-06
Iter: 999 loss: 1.6645658457912406e-06
Iter: 1000 loss: 1.6785591928311698e-06
Iter: 1001 loss: 1.6644805249125653e-06
Iter: 1002 loss: 1.663147206369067e-06
Iter: 1003 loss: 1.6641162859766586e-06
Iter: 1004 loss: 1.6623247133700131e-06
Iter: 1005 loss: 1.66155817093298e-06
Iter: 1006 loss: 1.6613909137866817e-06
Iter: 1007 loss: 1.6604579443293795e-06
Iter: 1008 loss: 1.6610156397488603e-06
Iter: 1009 loss: 1.659856237775793e-06
Iter: 1010 loss: 1.6588328320666952e-06
Iter: 1011 loss: 1.6574504019986664e-06
Iter: 1012 loss: 1.6573790937433187e-06
Iter: 1013 loss: 1.6572187606138806e-06
Iter: 1014 loss: 1.6566249709965856e-06
Iter: 1015 loss: 1.6560770356571408e-06
Iter: 1016 loss: 1.6550640252617038e-06
Iter: 1017 loss: 1.6782873228934114e-06
Iter: 1018 loss: 1.6550627249284493e-06
Iter: 1019 loss: 1.6540826960892204e-06
Iter: 1020 loss: 1.66185290093557e-06
Iter: 1021 loss: 1.6540157966934082e-06
Iter: 1022 loss: 1.6528246323565141e-06
Iter: 1023 loss: 1.6533582964262037e-06
Iter: 1024 loss: 1.652015009953301e-06
Iter: 1025 loss: 1.6511429453018906e-06
Iter: 1026 loss: 1.6493101853927529e-06
Iter: 1027 loss: 1.6798755370270374e-06
Iter: 1028 loss: 1.6492613977568486e-06
Iter: 1029 loss: 1.64728465902444e-06
Iter: 1030 loss: 1.6670556953537335e-06
Iter: 1031 loss: 1.647221190342422e-06
Iter: 1032 loss: 1.6461818230317066e-06
Iter: 1033 loss: 1.6531928562183257e-06
Iter: 1034 loss: 1.6460782709698457e-06
Iter: 1035 loss: 1.6453223739081137e-06
Iter: 1036 loss: 1.6562837904042933e-06
Iter: 1037 loss: 1.645321225247899e-06
Iter: 1038 loss: 1.6446635955021587e-06
Iter: 1039 loss: 1.6436530146558191e-06
Iter: 1040 loss: 1.64363730071482e-06
Iter: 1041 loss: 1.6424400196820078e-06
Iter: 1042 loss: 1.6450022629371421e-06
Iter: 1043 loss: 1.6419717630900187e-06
Iter: 1044 loss: 1.6405911311981876e-06
Iter: 1045 loss: 1.6423948572681829e-06
Iter: 1046 loss: 1.6398889735909021e-06
Iter: 1047 loss: 1.639149498012554e-06
Iter: 1048 loss: 1.6391084106076251e-06
Iter: 1049 loss: 1.6382701077340211e-06
Iter: 1050 loss: 1.6377818996044536e-06
Iter: 1051 loss: 1.6374270097349144e-06
Iter: 1052 loss: 1.6365119773103282e-06
Iter: 1053 loss: 1.63599763940013e-06
Iter: 1054 loss: 1.6355993830823842e-06
Iter: 1055 loss: 1.6354127536158586e-06
Iter: 1056 loss: 1.634996332016251e-06
Iter: 1057 loss: 1.6345243618877864e-06
Iter: 1058 loss: 1.6333646782138096e-06
Iter: 1059 loss: 1.64504292921086e-06
Iter: 1060 loss: 1.6332217414449655e-06
Iter: 1061 loss: 1.6320673513150254e-06
Iter: 1062 loss: 1.6494241936841623e-06
Iter: 1063 loss: 1.6320666475674509e-06
Iter: 1064 loss: 1.6310759013490791e-06
Iter: 1065 loss: 1.6326352549444543e-06
Iter: 1066 loss: 1.6306135221056583e-06
Iter: 1067 loss: 1.629761846393271e-06
Iter: 1068 loss: 1.6277384771224981e-06
Iter: 1069 loss: 1.6508149582472957e-06
Iter: 1070 loss: 1.6275486057951904e-06
Iter: 1071 loss: 1.6255730542662022e-06
Iter: 1072 loss: 1.6432162870217384e-06
Iter: 1073 loss: 1.6254776033127829e-06
Iter: 1074 loss: 1.6244488277708161e-06
Iter: 1075 loss: 1.6404698811166059e-06
Iter: 1076 loss: 1.6244487073156369e-06
Iter: 1077 loss: 1.6234543800949684e-06
Iter: 1078 loss: 1.6273729028016877e-06
Iter: 1079 loss: 1.6232267736851948e-06
Iter: 1080 loss: 1.6225469309862834e-06
Iter: 1081 loss: 1.6213364870288232e-06
Iter: 1082 loss: 1.6515073544239118e-06
Iter: 1083 loss: 1.6213364848501956e-06
Iter: 1084 loss: 1.6199522399625083e-06
Iter: 1085 loss: 1.62265396657526e-06
Iter: 1086 loss: 1.6193785076967987e-06
Iter: 1087 loss: 1.6187340102724047e-06
Iter: 1088 loss: 1.618629110547149e-06
Iter: 1089 loss: 1.6177672775575336e-06
Iter: 1090 loss: 1.6170192170738306e-06
Iter: 1091 loss: 1.6167886223974854e-06
Iter: 1092 loss: 1.6158615006970018e-06
Iter: 1093 loss: 1.617221293505738e-06
Iter: 1094 loss: 1.6154137626850101e-06
Iter: 1095 loss: 1.6148166652098545e-06
Iter: 1096 loss: 1.61480342486105e-06
Iter: 1097 loss: 1.6142274396606328e-06
Iter: 1098 loss: 1.613954963702488e-06
Iter: 1099 loss: 1.6136734954548353e-06
Iter: 1100 loss: 1.6127207927968897e-06
Iter: 1101 loss: 1.6134972199556006e-06
Iter: 1102 loss: 1.612150535197439e-06
Iter: 1103 loss: 1.6112288472899096e-06
Iter: 1104 loss: 1.6248768938352969e-06
Iter: 1105 loss: 1.6112279773752029e-06
Iter: 1106 loss: 1.6106450294248392e-06
Iter: 1107 loss: 1.6100544396568402e-06
Iter: 1108 loss: 1.6099388963128894e-06
Iter: 1109 loss: 1.6086828177755393e-06
Iter: 1110 loss: 1.6068184875978056e-06
Iter: 1111 loss: 1.6067749185470138e-06
Iter: 1112 loss: 1.6053087517124464e-06
Iter: 1113 loss: 1.6149210391807049e-06
Iter: 1114 loss: 1.6051539074926789e-06
Iter: 1115 loss: 1.6045698094804146e-06
Iter: 1116 loss: 1.6044453201059802e-06
Iter: 1117 loss: 1.6037270249711442e-06
Iter: 1118 loss: 1.6031028365174347e-06
Iter: 1119 loss: 1.602910988956791e-06
Iter: 1120 loss: 1.6020976935554066e-06
Iter: 1121 loss: 1.6009019433304817e-06
Iter: 1122 loss: 1.6008711656772085e-06
Iter: 1123 loss: 1.5995103175868136e-06
Iter: 1124 loss: 1.6145770063876713e-06
Iter: 1125 loss: 1.5994829147143259e-06
Iter: 1126 loss: 1.5987136669970627e-06
Iter: 1127 loss: 1.5987123328504758e-06
Iter: 1128 loss: 1.5979570638568572e-06
Iter: 1129 loss: 1.5968287644414046e-06
Iter: 1130 loss: 1.5968041771981669e-06
Iter: 1131 loss: 1.5958614068409947e-06
Iter: 1132 loss: 1.5966946806946764e-06
Iter: 1133 loss: 1.5953100732373693e-06
Iter: 1134 loss: 1.5949774100117316e-06
Iter: 1135 loss: 1.5947350832242063e-06
Iter: 1136 loss: 1.5942595933268825e-06
Iter: 1137 loss: 1.5931217298340441e-06
Iter: 1138 loss: 1.6057694948914825e-06
Iter: 1139 loss: 1.5930083156001347e-06
Iter: 1140 loss: 1.5919219604845107e-06
Iter: 1141 loss: 1.6032828721006312e-06
Iter: 1142 loss: 1.591893116837544e-06
Iter: 1143 loss: 1.5907540283511366e-06
Iter: 1144 loss: 1.5924697414460643e-06
Iter: 1145 loss: 1.590210820688442e-06
Iter: 1146 loss: 1.5893733727965434e-06
Iter: 1147 loss: 1.5893828745548459e-06
Iter: 1148 loss: 1.5887062406479915e-06
Iter: 1149 loss: 1.5875686285639079e-06
Iter: 1150 loss: 1.5906816548889359e-06
Iter: 1151 loss: 1.5871972227210168e-06
Iter: 1152 loss: 1.5861032447897967e-06
Iter: 1153 loss: 1.591550249221703e-06
Iter: 1154 loss: 1.5859182693402663e-06
Iter: 1155 loss: 1.5854185705682222e-06
Iter: 1156 loss: 1.5853958027114597e-06
Iter: 1157 loss: 1.5849362858591088e-06
Iter: 1158 loss: 1.5840547592066211e-06
Iter: 1159 loss: 1.6026037908745809e-06
Iter: 1160 loss: 1.5840503406613543e-06
Iter: 1161 loss: 1.5828208280058804e-06
Iter: 1162 loss: 1.5811406436884897e-06
Iter: 1163 loss: 1.581060686945456e-06
Iter: 1164 loss: 1.5798971649054399e-06
Iter: 1165 loss: 1.5798940137860837e-06
Iter: 1166 loss: 1.5789446392522859e-06
Iter: 1167 loss: 1.5900488928564248e-06
Iter: 1168 loss: 1.5789306369401168e-06
Iter: 1169 loss: 1.5783155103909264e-06
Iter: 1170 loss: 1.577663618341038e-06
Iter: 1171 loss: 1.57755441766922e-06
Iter: 1172 loss: 1.5767215704588019e-06
Iter: 1173 loss: 1.5763900102167818e-06
Iter: 1174 loss: 1.575944383007441e-06
Iter: 1175 loss: 1.5752872809276319e-06
Iter: 1176 loss: 1.5751047347936741e-06
Iter: 1177 loss: 1.574576989138157e-06
Iter: 1178 loss: 1.5732147389174249e-06
Iter: 1179 loss: 1.5843725934373689e-06
Iter: 1180 loss: 1.5729715844924513e-06
Iter: 1181 loss: 1.572080295192226e-06
Iter: 1182 loss: 1.5720526195542458e-06
Iter: 1183 loss: 1.5711021091062658e-06
Iter: 1184 loss: 1.571835471360383e-06
Iter: 1185 loss: 1.5705246616947835e-06
Iter: 1186 loss: 1.5698036254260971e-06
Iter: 1187 loss: 1.5690720287747015e-06
Iter: 1188 loss: 1.5689296193856188e-06
Iter: 1189 loss: 1.5675709456987497e-06
Iter: 1190 loss: 1.5724966674409612e-06
Iter: 1191 loss: 1.5672299021375264e-06
Iter: 1192 loss: 1.5665405300569715e-06
Iter: 1193 loss: 1.566523161802054e-06
Iter: 1194 loss: 1.5657043856101388e-06
Iter: 1195 loss: 1.5647580436184888e-06
Iter: 1196 loss: 1.5646452813107349e-06
Iter: 1197 loss: 1.5634574896965423e-06
Iter: 1198 loss: 1.5644445812612403e-06
Iter: 1199 loss: 1.5627503911978553e-06
Iter: 1200 loss: 1.5616488484985062e-06
Iter: 1201 loss: 1.5725584141828157e-06
Iter: 1202 loss: 1.5616120526705586e-06
Iter: 1203 loss: 1.5609605312099389e-06
Iter: 1204 loss: 1.5621969423291623e-06
Iter: 1205 loss: 1.5606859142731407e-06
Iter: 1206 loss: 1.55983958174036e-06
Iter: 1207 loss: 1.5666679105806437e-06
Iter: 1208 loss: 1.5597844320291296e-06
Iter: 1209 loss: 1.5593706076279342e-06
Iter: 1210 loss: 1.5584997830073092e-06
Iter: 1211 loss: 1.5729724317108975e-06
Iter: 1212 loss: 1.5584762117203356e-06
Iter: 1213 loss: 1.5572477011266638e-06
Iter: 1214 loss: 1.560642796605832e-06
Iter: 1215 loss: 1.5568499431136147e-06
Iter: 1216 loss: 1.555912321178845e-06
Iter: 1217 loss: 1.5558865490011249e-06
Iter: 1218 loss: 1.5553599520267042e-06
Iter: 1219 loss: 1.5540014669321262e-06
Iter: 1220 loss: 1.5651576212096218e-06
Iter: 1221 loss: 1.5537600105991113e-06
Iter: 1222 loss: 1.5528883207961356e-06
Iter: 1223 loss: 1.5528694820927027e-06
Iter: 1224 loss: 1.5521319998712615e-06
Iter: 1225 loss: 1.5566542709822018e-06
Iter: 1226 loss: 1.5520434080882382e-06
Iter: 1227 loss: 1.5516301710083485e-06
Iter: 1228 loss: 1.5505193567448233e-06
Iter: 1229 loss: 1.5579780532229362e-06
Iter: 1230 loss: 1.5502551039358544e-06
Iter: 1231 loss: 1.5487764114368958e-06
Iter: 1232 loss: 1.5580945790723702e-06
Iter: 1233 loss: 1.5486076496592898e-06
Iter: 1234 loss: 1.5479722769896458e-06
Iter: 1235 loss: 1.5478558991055749e-06
Iter: 1236 loss: 1.5472101304641221e-06
Iter: 1237 loss: 1.5458816229871262e-06
Iter: 1238 loss: 1.5693486046288077e-06
Iter: 1239 loss: 1.5458553363953682e-06
Iter: 1240 loss: 1.5446218588318544e-06
Iter: 1241 loss: 1.5484935627260504e-06
Iter: 1242 loss: 1.5442646228026187e-06
Iter: 1243 loss: 1.5434841706065955e-06
Iter: 1244 loss: 1.5450654145914159e-06
Iter: 1245 loss: 1.5431680461108614e-06
Iter: 1246 loss: 1.5421864455129124e-06
Iter: 1247 loss: 1.5544714723737945e-06
Iter: 1248 loss: 1.5421774280355643e-06
Iter: 1249 loss: 1.5416377878530839e-06
Iter: 1250 loss: 1.540804655054267e-06
Iter: 1251 loss: 1.5407924843683456e-06
Iter: 1252 loss: 1.5398461589884531e-06
Iter: 1253 loss: 1.5420130379657238e-06
Iter: 1254 loss: 1.5394926497741247e-06
Iter: 1255 loss: 1.5388059413583947e-06
Iter: 1256 loss: 1.5387735496684528e-06
Iter: 1257 loss: 1.5382696180084247e-06
Iter: 1258 loss: 1.5377056876939062e-06
Iter: 1259 loss: 1.5376288449824995e-06
Iter: 1260 loss: 1.5367884567429405e-06
Iter: 1261 loss: 1.5380257856506231e-06
Iter: 1262 loss: 1.5363833071920934e-06
Iter: 1263 loss: 1.53556583431396e-06
Iter: 1264 loss: 1.5402883875662257e-06
Iter: 1265 loss: 1.5354566100665204e-06
Iter: 1266 loss: 1.5346935749125752e-06
Iter: 1267 loss: 1.5392094387858974e-06
Iter: 1268 loss: 1.5345958331975193e-06
Iter: 1269 loss: 1.5339419987347836e-06
Iter: 1270 loss: 1.5325932681052177e-06
Iter: 1271 loss: 1.5562491856433387e-06
Iter: 1272 loss: 1.5325655075764068e-06
Iter: 1273 loss: 1.5315014937259876e-06
Iter: 1274 loss: 1.5374338552591516e-06
Iter: 1275 loss: 1.5313505754590121e-06
Iter: 1276 loss: 1.5307439872729263e-06
Iter: 1277 loss: 1.530686040108564e-06
Iter: 1278 loss: 1.5302365216419387e-06
Iter: 1279 loss: 1.5294547097284905e-06
Iter: 1280 loss: 1.5294544105489901e-06
Iter: 1281 loss: 1.5285691055271919e-06
Iter: 1282 loss: 1.5287498975352136e-06
Iter: 1283 loss: 1.5279123521460523e-06
Iter: 1284 loss: 1.5267064686685638e-06
Iter: 1285 loss: 1.5301276184116131e-06
Iter: 1286 loss: 1.5263244773622517e-06
Iter: 1287 loss: 1.5257142337180403e-06
Iter: 1288 loss: 1.5255379136107106e-06
Iter: 1289 loss: 1.5251673710197538e-06
Iter: 1290 loss: 1.5241948415787755e-06
Iter: 1291 loss: 1.5315634642454239e-06
Iter: 1292 loss: 1.5239992258275586e-06
Iter: 1293 loss: 1.5230722040938481e-06
Iter: 1294 loss: 1.5305607413477811e-06
Iter: 1295 loss: 1.5230120031582225e-06
Iter: 1296 loss: 1.522313686143075e-06
Iter: 1297 loss: 1.5328715947702944e-06
Iter: 1298 loss: 1.5223133316385218e-06
Iter: 1299 loss: 1.5218223361188839e-06
Iter: 1300 loss: 1.5206542977309278e-06
Iter: 1301 loss: 1.533915451030095e-06
Iter: 1302 loss: 1.5205434703419707e-06
Iter: 1303 loss: 1.5193616991245531e-06
Iter: 1304 loss: 1.5256621436835429e-06
Iter: 1305 loss: 1.5191815170737289e-06
Iter: 1306 loss: 1.5184227428488723e-06
Iter: 1307 loss: 1.5290026352437522e-06
Iter: 1308 loss: 1.5184205075765867e-06
Iter: 1309 loss: 1.5175938624986602e-06
Iter: 1310 loss: 1.5167392274811856e-06
Iter: 1311 loss: 1.516583060566646e-06
Iter: 1312 loss: 1.5160021595069374e-06
Iter: 1313 loss: 1.51862913476547e-06
Iter: 1314 loss: 1.5158899061040014e-06
Iter: 1315 loss: 1.5152355054407109e-06
Iter: 1316 loss: 1.5170168223677765e-06
Iter: 1317 loss: 1.5150209674184803e-06
Iter: 1318 loss: 1.5143196274279623e-06
Iter: 1319 loss: 1.5198280395328032e-06
Iter: 1320 loss: 1.51427056185316e-06
Iter: 1321 loss: 1.5137520528336106e-06
Iter: 1322 loss: 1.5124817063493836e-06
Iter: 1323 loss: 1.525417552889518e-06
Iter: 1324 loss: 1.5123286462150232e-06
Iter: 1325 loss: 1.5110366056511381e-06
Iter: 1326 loss: 1.5194608534151336e-06
Iter: 1327 loss: 1.5108986366590883e-06
Iter: 1328 loss: 1.510111628869996e-06
Iter: 1329 loss: 1.5161563653771353e-06
Iter: 1330 loss: 1.510053338773597e-06
Iter: 1331 loss: 1.509246609369629e-06
Iter: 1332 loss: 1.5141519651660227e-06
Iter: 1333 loss: 1.5091481774602559e-06
Iter: 1334 loss: 1.5086909170500743e-06
Iter: 1335 loss: 1.5078637356801809e-06
Iter: 1336 loss: 1.5277703480503765e-06
Iter: 1337 loss: 1.5078635295360505e-06
Iter: 1338 loss: 1.5069958898719723e-06
Iter: 1339 loss: 1.5101048425044045e-06
Iter: 1340 loss: 1.5067753946820174e-06
Iter: 1341 loss: 1.5056770678580018e-06
Iter: 1342 loss: 1.5140759458182534e-06
Iter: 1343 loss: 1.505594828591397e-06
Iter: 1344 loss: 1.5050448522203193e-06
Iter: 1345 loss: 1.50407283739785e-06
Iter: 1346 loss: 1.5040728104496652e-06
Iter: 1347 loss: 1.5029845858650114e-06
Iter: 1348 loss: 1.5072625119179483e-06
Iter: 1349 loss: 1.5027347740409102e-06
Iter: 1350 loss: 1.5022440710748791e-06
Iter: 1351 loss: 1.5022059638101598e-06
Iter: 1352 loss: 1.50172695670747e-06
Iter: 1353 loss: 1.5015460040413343e-06
Iter: 1354 loss: 1.5012835702257379e-06
Iter: 1355 loss: 1.5006513157500788e-06
Iter: 1356 loss: 1.499420988286656e-06
Iter: 1357 loss: 1.524438563351773e-06
Iter: 1358 loss: 1.4994121759173102e-06
Iter: 1359 loss: 1.4997689488434155e-06
Iter: 1360 loss: 1.4988133899348638e-06
Iter: 1361 loss: 1.498376810256905e-06
Iter: 1362 loss: 1.4972169189328785e-06
Iter: 1363 loss: 1.5054911461499376e-06
Iter: 1364 loss: 1.4969626518673636e-06
Iter: 1365 loss: 1.4957481241724388e-06
Iter: 1366 loss: 1.5015543340351503e-06
Iter: 1367 loss: 1.4955304815950168e-06
Iter: 1368 loss: 1.4948135232191191e-06
Iter: 1369 loss: 1.4995062297067595e-06
Iter: 1370 loss: 1.494737560541358e-06
Iter: 1371 loss: 1.4940897671291688e-06
Iter: 1372 loss: 1.5006583825523467e-06
Iter: 1373 loss: 1.4940701034071237e-06
Iter: 1374 loss: 1.4934917099664397e-06
Iter: 1375 loss: 1.4923276655646766e-06
Iter: 1376 loss: 1.5140913642470512e-06
Iter: 1377 loss: 1.4923114194776026e-06
Iter: 1378 loss: 1.4913630783346197e-06
Iter: 1379 loss: 1.4963518244750575e-06
Iter: 1380 loss: 1.4912154374307592e-06
Iter: 1381 loss: 1.4903561620129822e-06
Iter: 1382 loss: 1.4975731122939701e-06
Iter: 1383 loss: 1.4903061033112556e-06
Iter: 1384 loss: 1.4895908870115497e-06
Iter: 1385 loss: 1.4922287028874752e-06
Iter: 1386 loss: 1.4894146231825194e-06
Iter: 1387 loss: 1.4889182694731661e-06
Iter: 1388 loss: 1.4882880005013769e-06
Iter: 1389 loss: 1.488240501652186e-06
Iter: 1390 loss: 1.4874343574614343e-06
Iter: 1391 loss: 1.4876515084117362e-06
Iter: 1392 loss: 1.486850300640952e-06
Iter: 1393 loss: 1.4862747251830548e-06
Iter: 1394 loss: 1.4861763806811904e-06
Iter: 1395 loss: 1.4855286189951835e-06
Iter: 1396 loss: 1.4849908874749467e-06
Iter: 1397 loss: 1.4848050787349575e-06
Iter: 1398 loss: 1.4839758342916304e-06
Iter: 1399 loss: 1.4854048409789858e-06
Iter: 1400 loss: 1.4836066830518353e-06
Iter: 1401 loss: 1.4831590120614468e-06
Iter: 1402 loss: 1.483079253700318e-06
Iter: 1403 loss: 1.4827159926832394e-06
Iter: 1404 loss: 1.481798978419448e-06
Iter: 1405 loss: 1.4900787659015034e-06
Iter: 1406 loss: 1.4816600357282104e-06
Iter: 1407 loss: 1.4806605382975327e-06
Iter: 1408 loss: 1.4840783367420679e-06
Iter: 1409 loss: 1.480394035312832e-06
Iter: 1410 loss: 1.4795316028372252e-06
Iter: 1411 loss: 1.4811430452657912e-06
Iter: 1412 loss: 1.4791647693083543e-06
Iter: 1413 loss: 1.4779181440126064e-06
Iter: 1414 loss: 1.4889101382921745e-06
Iter: 1415 loss: 1.4778554145779453e-06
Iter: 1416 loss: 1.4773746829863477e-06
Iter: 1417 loss: 1.4770757285812241e-06
Iter: 1418 loss: 1.4768832140577624e-06
Iter: 1419 loss: 1.4761709302114967e-06
Iter: 1420 loss: 1.4764049835799147e-06
Iter: 1421 loss: 1.4756657575687236e-06
Iter: 1422 loss: 1.475340174256631e-06
Iter: 1423 loss: 1.475134700082387e-06
Iter: 1424 loss: 1.4748670509293404e-06
Iter: 1425 loss: 1.4740799285419906e-06
Iter: 1426 loss: 1.4769482313440431e-06
Iter: 1427 loss: 1.4737338139898168e-06
Iter: 1428 loss: 1.4725987195551359e-06
Iter: 1429 loss: 1.4773759104515059e-06
Iter: 1430 loss: 1.4723583896177947e-06
Iter: 1431 loss: 1.4713654047771886e-06
Iter: 1432 loss: 1.4775935471196943e-06
Iter: 1433 loss: 1.4712510500176944e-06
Iter: 1434 loss: 1.4704333143698941e-06
Iter: 1435 loss: 1.4791640563678895e-06
Iter: 1436 loss: 1.4704135765411057e-06
Iter: 1437 loss: 1.4698543442194106e-06
Iter: 1438 loss: 1.4695530963377631e-06
Iter: 1439 loss: 1.4693019344532222e-06
Iter: 1440 loss: 1.4686659346799289e-06
Iter: 1441 loss: 1.4739417127952323e-06
Iter: 1442 loss: 1.4686275577922925e-06
Iter: 1443 loss: 1.4680744831414118e-06
Iter: 1444 loss: 1.4703784742067175e-06
Iter: 1445 loss: 1.4679559158024109e-06
Iter: 1446 loss: 1.4674570604275357e-06
Iter: 1447 loss: 1.4669021455126065e-06
Iter: 1448 loss: 1.4668247071328455e-06
Iter: 1449 loss: 1.4660128918923576e-06
Iter: 1450 loss: 1.4654166655580729e-06
Iter: 1451 loss: 1.46514268077508e-06
Iter: 1452 loss: 1.4641746126925444e-06
Iter: 1453 loss: 1.4768752524578455e-06
Iter: 1454 loss: 1.4641687480225612e-06
Iter: 1455 loss: 1.4631810101603975e-06
Iter: 1456 loss: 1.4690600891164577e-06
Iter: 1457 loss: 1.4630557748943981e-06
Iter: 1458 loss: 1.4625688447928464e-06
Iter: 1459 loss: 1.4615593216749618e-06
Iter: 1460 loss: 1.4790289432730273e-06
Iter: 1461 loss: 1.4615369847181836e-06
Iter: 1462 loss: 1.4607402552190485e-06
Iter: 1463 loss: 1.4729687655289361e-06
Iter: 1464 loss: 1.4607400364983696e-06
Iter: 1465 loss: 1.4601604919942015e-06
Iter: 1466 loss: 1.4661423875472942e-06
Iter: 1467 loss: 1.4601441821640396e-06
Iter: 1468 loss: 1.4597067713810207e-06
Iter: 1469 loss: 1.45866266405561e-06
Iter: 1470 loss: 1.4703756627686241e-06
Iter: 1471 loss: 1.4585607541864055e-06
Iter: 1472 loss: 1.4576043225012881e-06
Iter: 1473 loss: 1.4576982511406152e-06
Iter: 1474 loss: 1.4568666444636486e-06
Iter: 1475 loss: 1.4559324195414445e-06
Iter: 1476 loss: 1.455913341927451e-06
Iter: 1477 loss: 1.4552559979589152e-06
Iter: 1478 loss: 1.4631899672034814e-06
Iter: 1479 loss: 1.4552481008630055e-06
Iter: 1480 loss: 1.4548515568025797e-06
Iter: 1481 loss: 1.4540084989933897e-06
Iter: 1482 loss: 1.4676281770414775e-06
Iter: 1483 loss: 1.4539825176822509e-06
Iter: 1484 loss: 1.4532535140899643e-06
Iter: 1485 loss: 1.4532316653519167e-06
Iter: 1486 loss: 1.4526512129890515e-06
Iter: 1487 loss: 1.4522163096776927e-06
Iter: 1488 loss: 1.4520250647243984e-06
Iter: 1489 loss: 1.4514266017580644e-06
Iter: 1490 loss: 1.451071471629897e-06
Iter: 1491 loss: 1.4508219670598532e-06
Iter: 1492 loss: 1.4499273760333986e-06
Iter: 1493 loss: 1.4555672925351508e-06
Iter: 1494 loss: 1.4498253623247617e-06
Iter: 1495 loss: 1.4491584785443141e-06
Iter: 1496 loss: 1.4491584202748499e-06
Iter: 1497 loss: 1.4486866614595632e-06
Iter: 1498 loss: 1.4486014892994979e-06
Iter: 1499 loss: 1.4482824396422887e-06
Iter: 1500 loss: 1.4476569124019213e-06
Iter: 1501 loss: 1.4475717838516541e-06
Iter: 1502 loss: 1.4471300217846854e-06
Iter: 1503 loss: 1.4463667970422637e-06
Iter: 1504 loss: 1.4510072431740526e-06
Iter: 1505 loss: 1.4462736502353779e-06
Iter: 1506 loss: 1.4455569755230086e-06
Iter: 1507 loss: 1.4517555333737535e-06
Iter: 1508 loss: 1.4455187041736884e-06
Iter: 1509 loss: 1.4450879940876851e-06
Iter: 1510 loss: 1.4444048885613643e-06
Iter: 1511 loss: 1.4443982434834303e-06
Iter: 1512 loss: 1.4435419050156175e-06
Iter: 1513 loss: 1.4427316905956286e-06
Iter: 1514 loss: 1.4425353449187443e-06
Iter: 1515 loss: 1.4413839009527769e-06
Iter: 1516 loss: 1.4569031767320953e-06
Iter: 1517 loss: 1.4413786697077918e-06
Iter: 1518 loss: 1.4410952668361711e-06
Iter: 1519 loss: 1.4410030598000155e-06
Iter: 1520 loss: 1.4406624679243852e-06
Iter: 1521 loss: 1.4399397764481501e-06
Iter: 1522 loss: 1.4516785681756395e-06
Iter: 1523 loss: 1.4399180368871736e-06
Iter: 1524 loss: 1.4393164808988993e-06
Iter: 1525 loss: 1.4483967919663198e-06
Iter: 1526 loss: 1.439316158630801e-06
Iter: 1527 loss: 1.4387562801531835e-06
Iter: 1528 loss: 1.4391664299919779e-06
Iter: 1529 loss: 1.4384115797231362e-06
Iter: 1530 loss: 1.4378308456016797e-06
Iter: 1531 loss: 1.4369048631934313e-06
Iter: 1532 loss: 1.4368966503164505e-06
Iter: 1533 loss: 1.4358547436201877e-06
Iter: 1534 loss: 1.4393873374888971e-06
Iter: 1535 loss: 1.4355745793248753e-06
Iter: 1536 loss: 1.4355487021279245e-06
Iter: 1537 loss: 1.4351514081184836e-06
Iter: 1538 loss: 1.4348165989130415e-06
Iter: 1539 loss: 1.4341074795980331e-06
Iter: 1540 loss: 1.4456859298851495e-06
Iter: 1541 loss: 1.4340866349314453e-06
Iter: 1542 loss: 1.4334689018674736e-06
Iter: 1543 loss: 1.4330227258674035e-06
Iter: 1544 loss: 1.432810135811251e-06
Iter: 1545 loss: 1.4322357394416213e-06
Iter: 1546 loss: 1.4321123851518696e-06
Iter: 1547 loss: 1.4315029355750622e-06
Iter: 1548 loss: 1.4307324333669535e-06
Iter: 1549 loss: 1.4306729510648704e-06
Iter: 1550 loss: 1.4299011905220242e-06
Iter: 1551 loss: 1.4302325831874867e-06
Iter: 1552 loss: 1.4293731793210389e-06
Iter: 1553 loss: 1.4283170208378937e-06
Iter: 1554 loss: 1.4322088024142241e-06
Iter: 1555 loss: 1.4280565127630862e-06
Iter: 1556 loss: 1.4275069955757316e-06
Iter: 1557 loss: 1.43306120034188e-06
Iter: 1558 loss: 1.4274900956688287e-06
Iter: 1559 loss: 1.4269423321284505e-06
Iter: 1560 loss: 1.4277735997424013e-06
Iter: 1561 loss: 1.4266820458563577e-06
Iter: 1562 loss: 1.4258580365248614e-06
Iter: 1563 loss: 1.4286878471565518e-06
Iter: 1564 loss: 1.4256392976194933e-06
Iter: 1565 loss: 1.4251357538266391e-06
Iter: 1566 loss: 1.4246308105199944e-06
Iter: 1567 loss: 1.4245286431276927e-06
Iter: 1568 loss: 1.4235861509521245e-06
Iter: 1569 loss: 1.4336761324871849e-06
Iter: 1570 loss: 1.4235636858384513e-06
Iter: 1571 loss: 1.4230395565751088e-06
Iter: 1572 loss: 1.4227414617041947e-06
Iter: 1573 loss: 1.4225153887769059e-06
Iter: 1574 loss: 1.4218793754175931e-06
Iter: 1575 loss: 1.4219058944289448e-06
Iter: 1576 loss: 1.421378427405826e-06
Iter: 1577 loss: 1.4208976702921925e-06
Iter: 1578 loss: 1.4208721294328802e-06
Iter: 1579 loss: 1.4203504606160166e-06
Iter: 1580 loss: 1.4212467796923787e-06
Iter: 1581 loss: 1.4201178482922263e-06
Iter: 1582 loss: 1.4196681261421924e-06
Iter: 1583 loss: 1.418624552590333e-06
Iter: 1584 loss: 1.4315305781068788e-06
Iter: 1585 loss: 1.4185446847472386e-06
Iter: 1586 loss: 1.4177933952517673e-06
Iter: 1587 loss: 1.4177827627966109e-06
Iter: 1588 loss: 1.4171484416171565e-06
Iter: 1589 loss: 1.4225362748842666e-06
Iter: 1590 loss: 1.4171126875613765e-06
Iter: 1591 loss: 1.4167286537462117e-06
Iter: 1592 loss: 1.4162198196353714e-06
Iter: 1593 loss: 1.4161900166255066e-06
Iter: 1594 loss: 1.4155443973971507e-06
Iter: 1595 loss: 1.4147859974622103e-06
Iter: 1596 loss: 1.4147018364373865e-06
Iter: 1597 loss: 1.4134977101744471e-06
Iter: 1598 loss: 1.417801162082681e-06
Iter: 1599 loss: 1.4131908937399051e-06
Iter: 1600 loss: 1.4124216887140017e-06
Iter: 1601 loss: 1.4124097600373392e-06
Iter: 1602 loss: 1.4115065175123291e-06
Iter: 1603 loss: 1.4124566726993873e-06
Iter: 1604 loss: 1.4110071333393337e-06
Iter: 1605 loss: 1.4104020859814331e-06
Iter: 1606 loss: 1.4110950038126067e-06
Iter: 1607 loss: 1.4100777266296726e-06
Iter: 1608 loss: 1.4095258746485603e-06
Iter: 1609 loss: 1.4133259731232839e-06
Iter: 1610 loss: 1.4094732441366653e-06
Iter: 1611 loss: 1.4087840964076671e-06
Iter: 1612 loss: 1.4087293908968548e-06
Iter: 1613 loss: 1.4082160134416763e-06
Iter: 1614 loss: 1.4075518862982251e-06
Iter: 1615 loss: 1.407847527138934e-06
Iter: 1616 loss: 1.4071000123559974e-06
Iter: 1617 loss: 1.4063173119461788e-06
Iter: 1618 loss: 1.4136833042237468e-06
Iter: 1619 loss: 1.4062856470461719e-06
Iter: 1620 loss: 1.4057435091186123e-06
Iter: 1621 loss: 1.4108124237186151e-06
Iter: 1622 loss: 1.4057210626122533e-06
Iter: 1623 loss: 1.405329288386116e-06
Iter: 1624 loss: 1.4048763719492614e-06
Iter: 1625 loss: 1.404822457970461e-06
Iter: 1626 loss: 1.4041772558570417e-06
Iter: 1627 loss: 1.4038784592950431e-06
Iter: 1628 loss: 1.4035592318562948e-06
Iter: 1629 loss: 1.403627981230614e-06
Iter: 1630 loss: 1.4032400306630503e-06
Iter: 1631 loss: 1.4029214037888121e-06
Iter: 1632 loss: 1.4021842354518332e-06
Iter: 1633 loss: 1.4113938572531995e-06
Iter: 1634 loss: 1.4021293141962022e-06
Iter: 1635 loss: 1.4013243487451623e-06
Iter: 1636 loss: 1.4031623399589182e-06
Iter: 1637 loss: 1.4010230522838899e-06
Iter: 1638 loss: 1.4001535685571781e-06
Iter: 1639 loss: 1.4000807021079023e-06
Iter: 1640 loss: 1.399435618282197e-06
Iter: 1641 loss: 1.398410453756654e-06
Iter: 1642 loss: 1.4074099214795923e-06
Iter: 1643 loss: 1.3983581752018624e-06
Iter: 1644 loss: 1.3980637902098939e-06
Iter: 1645 loss: 1.3979730087764407e-06
Iter: 1646 loss: 1.3975961548930156e-06
Iter: 1647 loss: 1.3967437636633606e-06
Iter: 1648 loss: 1.408202743094087e-06
Iter: 1649 loss: 1.3966923221068102e-06
Iter: 1650 loss: 1.3958585224052804e-06
Iter: 1651 loss: 1.4002980013842082e-06
Iter: 1652 loss: 1.3957311455989832e-06
Iter: 1653 loss: 1.3951265591729748e-06
Iter: 1654 loss: 1.4040899641701133e-06
Iter: 1655 loss: 1.3951260046180502e-06
Iter: 1656 loss: 1.3947364173690017e-06
Iter: 1657 loss: 1.3939504428391951e-06
Iter: 1658 loss: 1.4085568606726108e-06
Iter: 1659 loss: 1.3939390125190416e-06
Iter: 1660 loss: 1.3932286833036889e-06
Iter: 1661 loss: 1.3963863725060734e-06
Iter: 1662 loss: 1.3930883113820638e-06
Iter: 1663 loss: 1.3926897906508506e-06
Iter: 1664 loss: 1.3926599070862785e-06
Iter: 1665 loss: 1.3922824925242907e-06
Iter: 1666 loss: 1.3915428615189319e-06
Iter: 1667 loss: 1.4063287618799004e-06
Iter: 1668 loss: 1.3915366634532548e-06
Iter: 1669 loss: 1.390821023224875e-06
Iter: 1670 loss: 1.3957043032308775e-06
Iter: 1671 loss: 1.3907514425962823e-06
Iter: 1672 loss: 1.3901983208868169e-06
Iter: 1673 loss: 1.3898213119518172e-06
Iter: 1674 loss: 1.3896185047449363e-06
Iter: 1675 loss: 1.3887392286259666e-06
Iter: 1676 loss: 1.3887391773696789e-06
Iter: 1677 loss: 1.388291824656054e-06
Iter: 1678 loss: 1.3879093378611608e-06
Iter: 1679 loss: 1.3877867042234375e-06
Iter: 1680 loss: 1.387273568552341e-06
Iter: 1681 loss: 1.3871505575130126e-06
Iter: 1682 loss: 1.3868237022915511e-06
Iter: 1683 loss: 1.3859063060574032e-06
Iter: 1684 loss: 1.3876074813111766e-06
Iter: 1685 loss: 1.3855143501140342e-06
Iter: 1686 loss: 1.3845565533631138e-06
Iter: 1687 loss: 1.3842170790952925e-06
Iter: 1688 loss: 1.383678136803962e-06
Iter: 1689 loss: 1.383029515178437e-06
Iter: 1690 loss: 1.3828469891304069e-06
Iter: 1691 loss: 1.3821258437924784e-06
Iter: 1692 loss: 1.3851997614827992e-06
Iter: 1693 loss: 1.3819755295225868e-06
Iter: 1694 loss: 1.3816515877247863e-06
Iter: 1695 loss: 1.381629168125803e-06
Iter: 1696 loss: 1.3813855850678216e-06
Iter: 1697 loss: 1.3809587449982807e-06
Iter: 1698 loss: 1.3859377759871526e-06
Iter: 1699 loss: 1.3809523428664519e-06
Iter: 1700 loss: 1.3805466631980089e-06
Iter: 1701 loss: 1.37982787346582e-06
Iter: 1702 loss: 1.3798278669040518e-06
Iter: 1703 loss: 1.3791101795655275e-06
Iter: 1704 loss: 1.3785432992279114e-06
Iter: 1705 loss: 1.37832236554841e-06
Iter: 1706 loss: 1.3775027073652532e-06
Iter: 1707 loss: 1.37750174906205e-06
Iter: 1708 loss: 1.3767029244156388e-06
Iter: 1709 loss: 1.3814227875493023e-06
Iter: 1710 loss: 1.3766002875748931e-06
Iter: 1711 loss: 1.3761408293345497e-06
Iter: 1712 loss: 1.376494716928768e-06
Iter: 1713 loss: 1.3758615703967239e-06
Iter: 1714 loss: 1.3754212050934029e-06
Iter: 1715 loss: 1.3756469780976929e-06
Iter: 1716 loss: 1.3751286286225458e-06
Iter: 1717 loss: 1.3744606086023867e-06
Iter: 1718 loss: 1.3758178419876292e-06
Iter: 1719 loss: 1.3741904823359556e-06
Iter: 1720 loss: 1.373381996196799e-06
Iter: 1721 loss: 1.3812315668759777e-06
Iter: 1722 loss: 1.3733528220623154e-06
Iter: 1723 loss: 1.3729256063460123e-06
Iter: 1724 loss: 1.3723405123361196e-06
Iter: 1725 loss: 1.3723131099937096e-06
Iter: 1726 loss: 1.3715041228548941e-06
Iter: 1727 loss: 1.3714321847642748e-06
Iter: 1728 loss: 1.3708348229231913e-06
Iter: 1729 loss: 1.3699225973616605e-06
Iter: 1730 loss: 1.3735415416174721e-06
Iter: 1731 loss: 1.369715379198421e-06
Iter: 1732 loss: 1.3690310512208778e-06
Iter: 1733 loss: 1.3767186230175238e-06
Iter: 1734 loss: 1.3690182913965612e-06
Iter: 1735 loss: 1.3683554117701385e-06
Iter: 1736 loss: 1.3718753460756439e-06
Iter: 1737 loss: 1.3682537085661311e-06
Iter: 1738 loss: 1.3677401617821019e-06
Iter: 1739 loss: 1.3670527123899214e-06
Iter: 1740 loss: 1.367015033404894e-06
Iter: 1741 loss: 1.3665822790279715e-06
Iter: 1742 loss: 1.3665115178490029e-06
Iter: 1743 loss: 1.3660574966114374e-06
Iter: 1744 loss: 1.3654409492190646e-06
Iter: 1745 loss: 1.3654102859635538e-06
Iter: 1746 loss: 1.3649324451053009e-06
Iter: 1747 loss: 1.3643880779463635e-06
Iter: 1748 loss: 1.3643191152338878e-06
Iter: 1749 loss: 1.363833431523139e-06
Iter: 1750 loss: 1.3638104368934103e-06
Iter: 1751 loss: 1.3632638019539942e-06
Iter: 1752 loss: 1.3649621348507302e-06
Iter: 1753 loss: 1.3631039758680815e-06
Iter: 1754 loss: 1.3626384426556513e-06
Iter: 1755 loss: 1.3619307047992781e-06
Iter: 1756 loss: 1.3619181022250161e-06
Iter: 1757 loss: 1.3611853995470222e-06
Iter: 1758 loss: 1.3645981281280343e-06
Iter: 1759 loss: 1.3610492898661676e-06
Iter: 1760 loss: 1.3604299468452039e-06
Iter: 1761 loss: 1.365008336086275e-06
Iter: 1762 loss: 1.3603795940212059e-06
Iter: 1763 loss: 1.3598845212880917e-06
Iter: 1764 loss: 1.3641877497999724e-06
Iter: 1765 loss: 1.359858486915605e-06
Iter: 1766 loss: 1.3595563183511606e-06
Iter: 1767 loss: 1.359300617761308e-06
Iter: 1768 loss: 1.3592164321345863e-06
Iter: 1769 loss: 1.3586676852046766e-06
Iter: 1770 loss: 1.3581737569839013e-06
Iter: 1771 loss: 1.3580356977312327e-06
Iter: 1772 loss: 1.3571321994054909e-06
Iter: 1773 loss: 1.3576767298315523e-06
Iter: 1774 loss: 1.3565504439474618e-06
Iter: 1775 loss: 1.3556619391500209e-06
Iter: 1776 loss: 1.3695399565240087e-06
Iter: 1777 loss: 1.3556618574661587e-06
Iter: 1778 loss: 1.3552901549943398e-06
Iter: 1779 loss: 1.3552544067660108e-06
Iter: 1780 loss: 1.3550059550238472e-06
Iter: 1781 loss: 1.3544649868859248e-06
Iter: 1782 loss: 1.3626283579584124e-06
Iter: 1783 loss: 1.3544429506516579e-06
Iter: 1784 loss: 1.353941758364857e-06
Iter: 1785 loss: 1.3539414615740359e-06
Iter: 1786 loss: 1.3535178454130148e-06
Iter: 1787 loss: 1.3534109136673778e-06
Iter: 1788 loss: 1.35314462205067e-06
Iter: 1789 loss: 1.352660880005887e-06
Iter: 1790 loss: 1.3518425545538557e-06
Iter: 1791 loss: 1.351840980950265e-06
Iter: 1792 loss: 1.3513103722618556e-06
Iter: 1793 loss: 1.3512712033914731e-06
Iter: 1794 loss: 1.3507004748320607e-06
Iter: 1795 loss: 1.3530144323333787e-06
Iter: 1796 loss: 1.3505740776250465e-06
Iter: 1797 loss: 1.3502255617119414e-06
Iter: 1798 loss: 1.3495136697886274e-06
Iter: 1799 loss: 1.3623269823067675e-06
Iter: 1800 loss: 1.3495010238393052e-06
Iter: 1801 loss: 1.348685818317867e-06
Iter: 1802 loss: 1.3520207510604211e-06
Iter: 1803 loss: 1.3485072026394069e-06
Iter: 1804 loss: 1.3478824977809154e-06
Iter: 1805 loss: 1.3554116259986761e-06
Iter: 1806 loss: 1.3478749137472285e-06
Iter: 1807 loss: 1.34714688435374e-06
Iter: 1808 loss: 1.3471854108580827e-06
Iter: 1809 loss: 1.3465758474770963e-06
Iter: 1810 loss: 1.3461208387872808e-06
Iter: 1811 loss: 1.3464996482595467e-06
Iter: 1812 loss: 1.3458500815940816e-06
Iter: 1813 loss: 1.3451839758687634e-06
Iter: 1814 loss: 1.34653983655356e-06
Iter: 1815 loss: 1.3449149376752361e-06
Iter: 1816 loss: 1.3442592234574505e-06
Iter: 1817 loss: 1.3451043012490282e-06
Iter: 1818 loss: 1.3439238346735115e-06
Iter: 1819 loss: 1.3432168590928873e-06
Iter: 1820 loss: 1.3460506172604519e-06
Iter: 1821 loss: 1.3430581798778068e-06
Iter: 1822 loss: 1.3424847242945819e-06
Iter: 1823 loss: 1.3424817574457084e-06
Iter: 1824 loss: 1.3421310336996539e-06
Iter: 1825 loss: 1.3415465196499937e-06
Iter: 1826 loss: 1.3415446392126724e-06
Iter: 1827 loss: 1.3409603447608568e-06
Iter: 1828 loss: 1.3426952632603436e-06
Iter: 1829 loss: 1.340782382888524e-06
Iter: 1830 loss: 1.3403094434716743e-06
Iter: 1831 loss: 1.3403059112771744e-06
Iter: 1832 loss: 1.3400459460331566e-06
Iter: 1833 loss: 1.3392924365190141e-06
Iter: 1834 loss: 1.342397888380286e-06
Iter: 1835 loss: 1.3389923148617034e-06
Iter: 1836 loss: 1.3387096306721386e-06
Iter: 1837 loss: 1.3385037940549294e-06
Iter: 1838 loss: 1.3379897269078304e-06
Iter: 1839 loss: 1.3380970526125006e-06
Iter: 1840 loss: 1.3376089988857188e-06
Iter: 1841 loss: 1.3371284995272865e-06
Iter: 1842 loss: 1.3363935936914099e-06
Iter: 1843 loss: 1.3363814515074991e-06
Iter: 1844 loss: 1.3353136202067474e-06
Iter: 1845 loss: 1.3403006216697919e-06
Iter: 1846 loss: 1.3351159781867005e-06
Iter: 1847 loss: 1.3348657542741151e-06
Iter: 1848 loss: 1.3347856290868363e-06
Iter: 1849 loss: 1.3344096144979302e-06
Iter: 1850 loss: 1.3335607190666445e-06
Iter: 1851 loss: 1.3450380203571263e-06
Iter: 1852 loss: 1.3335104035945743e-06
Iter: 1853 loss: 1.3328300375570336e-06
Iter: 1854 loss: 1.3354210432287279e-06
Iter: 1855 loss: 1.3326681372141019e-06
Iter: 1856 loss: 1.332084204192301e-06
Iter: 1857 loss: 1.332600233644644e-06
Iter: 1858 loss: 1.3317426971707475e-06
Iter: 1859 loss: 1.3310983048528962e-06
Iter: 1860 loss: 1.3379245542628227e-06
Iter: 1861 loss: 1.3310821696666819e-06
Iter: 1862 loss: 1.3305914286900022e-06
Iter: 1863 loss: 1.3346773959326569e-06
Iter: 1864 loss: 1.3305621194420304e-06
Iter: 1865 loss: 1.3300996836344429e-06
Iter: 1866 loss: 1.3297904128469416e-06
Iter: 1867 loss: 1.3296175514101407e-06
Iter: 1868 loss: 1.3290448796926923e-06
Iter: 1869 loss: 1.3301526465610809e-06
Iter: 1870 loss: 1.3288062168503292e-06
Iter: 1871 loss: 1.3283321714459294e-06
Iter: 1872 loss: 1.3331441444943596e-06
Iter: 1873 loss: 1.3283178477302073e-06
Iter: 1874 loss: 1.3278100201272082e-06
Iter: 1875 loss: 1.3277381860699387e-06
Iter: 1876 loss: 1.3273813813328224e-06
Iter: 1877 loss: 1.3268570386523663e-06
Iter: 1878 loss: 1.326484313131208e-06
Iter: 1879 loss: 1.326300576418197e-06
Iter: 1880 loss: 1.3261104015346551e-06
Iter: 1881 loss: 1.3259143049461865e-06
Iter: 1882 loss: 1.3255551609655456e-06
Iter: 1883 loss: 1.3249392404334025e-06
Iter: 1884 loss: 1.3249386130495298e-06
Iter: 1885 loss: 1.3243558166735553e-06
Iter: 1886 loss: 1.3249050888708043e-06
Iter: 1887 loss: 1.3240216401870597e-06
Iter: 1888 loss: 1.3233279046711352e-06
Iter: 1889 loss: 1.3252998691303032e-06
Iter: 1890 loss: 1.3231085149814667e-06
Iter: 1891 loss: 1.3227405976608552e-06
Iter: 1892 loss: 1.3226702342315283e-06
Iter: 1893 loss: 1.3223532461000507e-06
Iter: 1894 loss: 1.321574851909866e-06
Iter: 1895 loss: 1.3294329319074563e-06
Iter: 1896 loss: 1.3214793789557527e-06
Iter: 1897 loss: 1.3207304355367117e-06
Iter: 1898 loss: 1.3229859162572981e-06
Iter: 1899 loss: 1.3205051990674315e-06
Iter: 1900 loss: 1.3198203660022159e-06
Iter: 1901 loss: 1.3205098129289841e-06
Iter: 1902 loss: 1.319435997540114e-06
Iter: 1903 loss: 1.3190366446835864e-06
Iter: 1904 loss: 1.3188998846369681e-06
Iter: 1905 loss: 1.3185295704989931e-06
Iter: 1906 loss: 1.3184616874752638e-06
Iter: 1907 loss: 1.3182119359844553e-06
Iter: 1908 loss: 1.3177870964954619e-06
Iter: 1909 loss: 1.3169870080196672e-06
Iter: 1910 loss: 1.3345743303430362e-06
Iter: 1911 loss: 1.3169847486046435e-06
Iter: 1912 loss: 1.3166049266425881e-06
Iter: 1913 loss: 1.3164058534338767e-06
Iter: 1914 loss: 1.3160910975165467e-06
Iter: 1915 loss: 1.3160000327414254e-06
Iter: 1916 loss: 1.3158097527974671e-06
Iter: 1917 loss: 1.3153170595292028e-06
Iter: 1918 loss: 1.3154135248605614e-06
Iter: 1919 loss: 1.3149504402385002e-06
Iter: 1920 loss: 1.3144699208138289e-06
Iter: 1921 loss: 1.3202146520679444e-06
Iter: 1922 loss: 1.3144637628355494e-06
Iter: 1923 loss: 1.3139440685507048e-06
Iter: 1924 loss: 1.3142439003867218e-06
Iter: 1925 loss: 1.3136064721674916e-06
Iter: 1926 loss: 1.313157506482107e-06
Iter: 1927 loss: 1.31293744338939e-06
Iter: 1928 loss: 1.3127227140990603e-06
Iter: 1929 loss: 1.312110623841995e-06
Iter: 1930 loss: 1.3132843411139798e-06
Iter: 1931 loss: 1.3118542140773487e-06
Iter: 1932 loss: 1.3112304455270074e-06
Iter: 1933 loss: 1.3195047963961728e-06
Iter: 1934 loss: 1.3112270675627453e-06
Iter: 1935 loss: 1.3105487906075629e-06
Iter: 1936 loss: 1.3102580119243656e-06
Iter: 1937 loss: 1.3099080166413259e-06
Iter: 1938 loss: 1.3093164421746826e-06
Iter: 1939 loss: 1.3086214028168265e-06
Iter: 1940 loss: 1.3085443393470267e-06
Iter: 1941 loss: 1.3077370476745849e-06
Iter: 1942 loss: 1.3138537600922463e-06
Iter: 1943 loss: 1.307675200056412e-06
Iter: 1944 loss: 1.3073580611038786e-06
Iter: 1945 loss: 1.3073381575605643e-06
Iter: 1946 loss: 1.3069565842389061e-06
Iter: 1947 loss: 1.3068560270218382e-06
Iter: 1948 loss: 1.3066189410152876e-06
Iter: 1949 loss: 1.3061132820088068e-06
Iter: 1950 loss: 1.30565562830414e-06
Iter: 1951 loss: 1.3055296374291686e-06
Iter: 1952 loss: 1.3049394054231994e-06
Iter: 1953 loss: 1.3121813761834522e-06
Iter: 1954 loss: 1.3049331001936129e-06
Iter: 1955 loss: 1.3042574733427561e-06
Iter: 1956 loss: 1.3048765642202791e-06
Iter: 1957 loss: 1.3038666432358988e-06
Iter: 1958 loss: 1.303424136459282e-06
Iter: 1959 loss: 1.3034236681693956e-06
Iter: 1960 loss: 1.3030699932624313e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.8/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi1.2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.2 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi1.2
+ date
Sun Nov  8 06:53:13 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.2/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi0.8/300_300_300_1 --function f1 --psi 3 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb028c268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb022e6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb022e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb02d2ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb0302ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b6fc9f400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb0191c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b6fcd78c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb0171378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb0171158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b6fc62730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b6fbbff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b6fba68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b6fc20730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b48336ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b48304d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b48330378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b48295b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b482e2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b482e2bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b6fbea9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b6fbea950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b482776a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b4826c840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b4826cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b48157378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b481066a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b48106598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b4813e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b4813e268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b482148c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b480591e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b48059378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b48059048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b48086400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b48091d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.005299401783463568
test_loss: 0.005780324285667803
train_loss: 0.0048234272459503034
test_loss: 0.00548075481789445
train_loss: 0.004614444885010911
test_loss: 0.005099522320499258
train_loss: 0.004568879406114047
test_loss: 0.004999420655328382
train_loss: 0.004255943234367706
test_loss: 0.004853678517531168
train_loss: 0.004493464630372468
test_loss: 0.005132560230615924
train_loss: 0.004426481308487527
test_loss: 0.005191915724378816
train_loss: 0.00452174067331231
test_loss: 0.005496081189497891
train_loss: 0.004307591266653726
test_loss: 0.004895823884094238
train_loss: 0.004856871396146173
test_loss: 0.004991120076672037
train_loss: 0.004202239559605615
test_loss: 0.004800837430452822
train_loss: 0.004149193862215712
test_loss: 0.005143965589767421
train_loss: 0.004410376916229594
test_loss: 0.005020965948730591
train_loss: 0.004605344428494302
test_loss: 0.0049526329599015985
train_loss: 0.004067409396632239
test_loss: 0.004724044609161581
train_loss: 0.0047191839330687395
test_loss: 0.005163273404357998
train_loss: 0.0045062082333395
test_loss: 0.004960771038012101
train_loss: 0.004104822451965059
test_loss: 0.00473891904900312
train_loss: 0.0044553821361259065
test_loss: 0.005027808094628966
train_loss: 0.003965795828906154
test_loss: 0.004943907865126033
train_loss: 0.004192193173603283
test_loss: 0.005034245765217505
train_loss: 0.00429812891356679
test_loss: 0.004819449823203388
train_loss: 0.003993808521193524
test_loss: 0.0047481341196481345
train_loss: 0.0038956333906326477
test_loss: 0.004740346468086335
train_loss: 0.004322378117531097
test_loss: 0.005259029203148968
train_loss: 0.004057576954898409
test_loss: 0.004926557169640107
train_loss: 0.003565015002229982
test_loss: 0.004616499101158948
train_loss: 0.003949905757023779
test_loss: 0.005047336794875487
train_loss: 0.004056840333826968
test_loss: 0.004899154316972512
train_loss: 0.004416544673689444
test_loss: 0.0052088805098772254
train_loss: 0.003951843946927828
test_loss: 0.0046204042236190285
train_loss: 0.0038918618705878825
test_loss: 0.00472442150607843
train_loss: 0.003926584351182138
test_loss: 0.004556454255925063
train_loss: 0.0038377088374029382
test_loss: 0.004765523411784914
train_loss: 0.004042541312237962
test_loss: 0.004959457457701228
train_loss: 0.004406612482184295
test_loss: 0.0048377666917959776
train_loss: 0.003797299193821847
test_loss: 0.004707427626097824
train_loss: 0.004078647977013147
test_loss: 0.004957762546104792
train_loss: 0.003931890716737379
test_loss: 0.0047461110198422215
train_loss: 0.004213420294776248
test_loss: 0.004735267286987942
train_loss: 0.0040436584664053435
test_loss: 0.004635032402859072
train_loss: 0.0038596861356305838
test_loss: 0.004757948428135075
train_loss: 0.004342201267827958
test_loss: 0.004993155631934782
train_loss: 0.004005927391738086
test_loss: 0.00491668920943593
train_loss: 0.0038777904074073495
test_loss: 0.004708539291885841
train_loss: 0.0037991694785634027
test_loss: 0.004538623648130916
train_loss: 0.0038047813040708455
test_loss: 0.004527708983869662
train_loss: 0.004353836414795274
test_loss: 0.004766136249541778
train_loss: 0.004097059561407655
test_loss: 0.004620152305789215
train_loss: 0.003765308777903282
test_loss: 0.004355384049023692
train_loss: 0.0038627890980590107
test_loss: 0.004515447947391622
train_loss: 0.003932045622398631
test_loss: 0.004635233248847029
train_loss: 0.003768237955886126
test_loss: 0.004418176072633772
train_loss: 0.0038827031127966114
test_loss: 0.004387290113043131
train_loss: 0.004082214070156458
test_loss: 0.004879712927254999
train_loss: 0.003842396337650326
test_loss: 0.004656313140482132
train_loss: 0.0037393596023445157
test_loss: 0.004523152828097472
train_loss: 0.003709077674868383
test_loss: 0.0047450105928981134
train_loss: 0.00365296361669101
test_loss: 0.004980213159231041
train_loss: 0.0038371244105632448
test_loss: 0.004634441013017586
train_loss: 0.0040483358444197
test_loss: 0.005018183679385895
train_loss: 0.0035314895619365733
test_loss: 0.00446639481183984
train_loss: 0.004065319850254404
test_loss: 0.004834788441617886
train_loss: 0.0038863360176839487
test_loss: 0.004613659592772709
train_loss: 0.0038117515681457327
test_loss: 0.0045405005760851165
train_loss: 0.003940795973883656
test_loss: 0.004397195059304533
train_loss: 0.0036088557566227504
test_loss: 0.004291706781293802
train_loss: 0.003933214870335821
test_loss: 0.0048137736219506745
train_loss: 0.003936138143076336
test_loss: 0.004711219746953753
train_loss: 0.003874223116684298
test_loss: 0.004582635581945649
train_loss: 0.0037588110990554756
test_loss: 0.004528256159247672
train_loss: 0.003555446775645608
test_loss: 0.004339449658008366
train_loss: 0.004178975512998899
test_loss: 0.004740138919863852
train_loss: 0.003902885074800825
test_loss: 0.004595526741864923
train_loss: 0.003580726359668783
test_loss: 0.004364104678798627
train_loss: 0.0036899932818396384
test_loss: 0.004355132584667572
train_loss: 0.0038575012273484584
test_loss: 0.004636549228187074
train_loss: 0.003556943114175283
test_loss: 0.004551382529695877
train_loss: 0.003656024299975332
test_loss: 0.004351864280260562
train_loss: 0.0039594002889710884
test_loss: 0.004653916831496739
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi1.2/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.2/300_300_300_1 --optimizer lbfgs --function f1 --psi 3 --phi 1.2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi1.2/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a413378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a3e87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a3e8488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a3e8ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a36aea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a36ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a32c9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a4adbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a2c8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a2f0268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a257158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a244d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a2448c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a243f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a1d39d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a1e1ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a1b5510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a1b5f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a14a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a14a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a11d8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a11d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a08d9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a0c2730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a0c28c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a081ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa74a081378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa749fde840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa749fde950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa749ff9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa749fbbb70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7272e3400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7273062f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7272ab8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa7272c58c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fa727291510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.5366538897719005e-05
Iter: 2 loss: 3.0654444195119809e-05
Iter: 3 loss: 2.04681113215541e-05
Iter: 4 loss: 1.9208654268668106e-05
Iter: 5 loss: 1.9784979874167455e-05
Iter: 6 loss: 1.8355264775171585e-05
Iter: 7 loss: 1.7141442927096141e-05
Iter: 8 loss: 2.2899346303997376e-05
Iter: 9 loss: 1.6921840255089143e-05
Iter: 10 loss: 1.5839854861994247e-05
Iter: 11 loss: 1.5380505143528703e-05
Iter: 12 loss: 1.4818868198335896e-05
Iter: 13 loss: 1.3831150409732815e-05
Iter: 14 loss: 2.066536842496473e-05
Iter: 15 loss: 1.3737986720442995e-05
Iter: 16 loss: 1.2855335052801184e-05
Iter: 17 loss: 1.8480113322549152e-05
Iter: 18 loss: 1.2756625672270266e-05
Iter: 19 loss: 1.2249089772235202e-05
Iter: 20 loss: 1.1483640191637559e-05
Iter: 21 loss: 1.1468529802534898e-05
Iter: 22 loss: 1.0679779196625331e-05
Iter: 23 loss: 1.9966670253693602e-05
Iter: 24 loss: 1.0668930937451408e-05
Iter: 25 loss: 1.0211463467184836e-05
Iter: 26 loss: 1.4962033262106103e-05
Iter: 27 loss: 1.0198998101578672e-05
Iter: 28 loss: 9.8626851933655451e-06
Iter: 29 loss: 9.5264637048030336e-06
Iter: 30 loss: 9.45754022342591e-06
Iter: 31 loss: 9.0448241194404217e-06
Iter: 32 loss: 8.9921302286022445e-06
Iter: 33 loss: 8.6984425697865914e-06
Iter: 34 loss: 8.1309918857566637e-06
Iter: 35 loss: 1.2581941196064644e-05
Iter: 36 loss: 8.0911320124023652e-06
Iter: 37 loss: 7.7195182588915567e-06
Iter: 38 loss: 8.756863515451061e-06
Iter: 39 loss: 7.6002088896596422e-06
Iter: 40 loss: 7.0939787798156448e-06
Iter: 41 loss: 1.0302015638355906e-05
Iter: 42 loss: 7.0366767735511536e-06
Iter: 43 loss: 6.85475783294654e-06
Iter: 44 loss: 7.4212741378097585e-06
Iter: 45 loss: 6.8016738885016228e-06
Iter: 46 loss: 6.5857106480203161e-06
Iter: 47 loss: 6.7762226817769766e-06
Iter: 48 loss: 6.459294662541551e-06
Iter: 49 loss: 6.2720401684468339e-06
Iter: 50 loss: 6.5541278462975272e-06
Iter: 51 loss: 6.1827774783912777e-06
Iter: 52 loss: 6.0280161752250122e-06
Iter: 53 loss: 6.0280077217717479e-06
Iter: 54 loss: 5.9058601205063536e-06
Iter: 55 loss: 5.7291852925030772e-06
Iter: 56 loss: 5.7238601001859263e-06
Iter: 57 loss: 5.5228748158341554e-06
Iter: 58 loss: 5.9437434698328625e-06
Iter: 59 loss: 5.4431865264716655e-06
Iter: 60 loss: 5.3130306040108872e-06
Iter: 61 loss: 5.3066369163278405e-06
Iter: 62 loss: 5.1964784186488725e-06
Iter: 63 loss: 5.06829077687046e-06
Iter: 64 loss: 5.0534249918539772e-06
Iter: 65 loss: 4.9047810219623943e-06
Iter: 66 loss: 5.7409954605735089e-06
Iter: 67 loss: 4.8840687324885415e-06
Iter: 68 loss: 4.7897557978469287e-06
Iter: 69 loss: 4.6855333116036936e-06
Iter: 70 loss: 4.670595814666731e-06
Iter: 71 loss: 4.5658064253044849e-06
Iter: 72 loss: 4.5655434456102048e-06
Iter: 73 loss: 4.4940461138964823e-06
Iter: 74 loss: 4.9883710642116655e-06
Iter: 75 loss: 4.4872802871548922e-06
Iter: 76 loss: 4.394394573218656e-06
Iter: 77 loss: 4.3472771842332035e-06
Iter: 78 loss: 4.3038147866029734e-06
Iter: 79 loss: 4.2218765124414487e-06
Iter: 80 loss: 4.74336436165657e-06
Iter: 81 loss: 4.2127066789665775e-06
Iter: 82 loss: 4.130667295287963e-06
Iter: 83 loss: 4.3032623637477868e-06
Iter: 84 loss: 4.0982302171623405e-06
Iter: 85 loss: 4.04550514200088e-06
Iter: 86 loss: 4.0870621184356183e-06
Iter: 87 loss: 4.0136523186974616e-06
Iter: 88 loss: 3.9434523989571522e-06
Iter: 89 loss: 4.5969053849027737e-06
Iter: 90 loss: 3.9404995445070515e-06
Iter: 91 loss: 3.90058613568988e-06
Iter: 92 loss: 3.8625991512789949e-06
Iter: 93 loss: 3.853554289123812e-06
Iter: 94 loss: 3.7899665882210541e-06
Iter: 95 loss: 3.9528777086501621e-06
Iter: 96 loss: 3.768088832139971e-06
Iter: 97 loss: 3.7161265870002455e-06
Iter: 98 loss: 3.7159647367067893e-06
Iter: 99 loss: 3.6833377448513758e-06
Iter: 100 loss: 3.6094368232866139e-06
Iter: 101 loss: 4.5983580672466868e-06
Iter: 102 loss: 3.6049159337663726e-06
Iter: 103 loss: 3.5458135155249707e-06
Iter: 104 loss: 4.1035470429208916e-06
Iter: 105 loss: 3.543429219195596e-06
Iter: 106 loss: 3.4909388163427452e-06
Iter: 107 loss: 3.5376144774616191e-06
Iter: 108 loss: 3.4603069770201885e-06
Iter: 109 loss: 3.4182007818942469e-06
Iter: 110 loss: 3.5580748643510568e-06
Iter: 111 loss: 3.406640788288789e-06
Iter: 112 loss: 3.3687045524483557e-06
Iter: 113 loss: 3.9197052739160576e-06
Iter: 114 loss: 3.3686490394956391e-06
Iter: 115 loss: 3.3335682139895491e-06
Iter: 116 loss: 3.3812650045174027e-06
Iter: 117 loss: 3.3160309347367978e-06
Iter: 118 loss: 3.2929138058110987e-06
Iter: 119 loss: 3.2894638145080409e-06
Iter: 120 loss: 3.2733428550231644e-06
Iter: 121 loss: 3.2264164743189659e-06
Iter: 122 loss: 3.37376535280545e-06
Iter: 123 loss: 3.212828559835312e-06
Iter: 124 loss: 3.1824204143696341e-06
Iter: 125 loss: 3.2064840484951796e-06
Iter: 126 loss: 3.1640728889383493e-06
Iter: 127 loss: 3.1393182386854488e-06
Iter: 128 loss: 3.1387551182115549e-06
Iter: 129 loss: 3.1239964466772766e-06
Iter: 130 loss: 3.0890507096893425e-06
Iter: 131 loss: 3.4925927329651815e-06
Iter: 132 loss: 3.0858640900044803e-06
Iter: 133 loss: 3.047065139600174e-06
Iter: 134 loss: 3.3765696084705749e-06
Iter: 135 loss: 3.0448781456680245e-06
Iter: 136 loss: 3.0243892121804253e-06
Iter: 137 loss: 3.0243559735553534e-06
Iter: 138 loss: 3.0089560053720591e-06
Iter: 139 loss: 2.9707035672543832e-06
Iter: 140 loss: 3.3397887479112263e-06
Iter: 141 loss: 2.9655751184877273e-06
Iter: 142 loss: 2.9303653042001631e-06
Iter: 143 loss: 3.0533352637101492e-06
Iter: 144 loss: 2.9211832128940303e-06
Iter: 145 loss: 2.88786694550212e-06
Iter: 146 loss: 3.1807840600066233e-06
Iter: 147 loss: 2.88617020802799e-06
Iter: 148 loss: 2.8686442756823441e-06
Iter: 149 loss: 2.8851336225968646e-06
Iter: 150 loss: 2.8585905958587588e-06
Iter: 151 loss: 2.845643587735387e-06
Iter: 152 loss: 2.8448373521980243e-06
Iter: 153 loss: 2.8321355222563573e-06
Iter: 154 loss: 2.8343124120530444e-06
Iter: 155 loss: 2.8225985511837962e-06
Iter: 156 loss: 2.8075067210110952e-06
Iter: 157 loss: 2.8008741259863802e-06
Iter: 158 loss: 2.7931879158521504e-06
Iter: 159 loss: 2.7809212872881747e-06
Iter: 160 loss: 2.7797815140272191e-06
Iter: 161 loss: 2.7712670152396726e-06
Iter: 162 loss: 2.7501964093306109e-06
Iter: 163 loss: 2.9566095886402631e-06
Iter: 164 loss: 2.7474520096568277e-06
Iter: 165 loss: 2.732749063364119e-06
Iter: 166 loss: 2.7307197067432181e-06
Iter: 167 loss: 2.7185477646637837e-06
Iter: 168 loss: 2.7015241394423912e-06
Iter: 169 loss: 2.7008431099439742e-06
Iter: 170 loss: 2.6886664721425863e-06
Iter: 171 loss: 2.7549697888153403e-06
Iter: 172 loss: 2.6868705741737835e-06
Iter: 173 loss: 2.6738485853472061e-06
Iter: 174 loss: 2.7433786624750239e-06
Iter: 175 loss: 2.6718679035791515e-06
Iter: 176 loss: 2.6587995829641664e-06
Iter: 177 loss: 2.6542612391356263e-06
Iter: 178 loss: 2.6468491031763765e-06
Iter: 179 loss: 2.6342247416549026e-06
Iter: 180 loss: 2.6200152749811662e-06
Iter: 181 loss: 2.618123267461695e-06
Iter: 182 loss: 2.5993481409063985e-06
Iter: 183 loss: 2.8488584823645341e-06
Iter: 184 loss: 2.5992491318254735e-06
Iter: 185 loss: 2.5857760215964527e-06
Iter: 186 loss: 2.6063786107426093e-06
Iter: 187 loss: 2.5793962355637523e-06
Iter: 188 loss: 2.5700369887521126e-06
Iter: 189 loss: 2.5700357628482767e-06
Iter: 190 loss: 2.5608972738065859e-06
Iter: 191 loss: 2.5852168489931558e-06
Iter: 192 loss: 2.5578464539686992e-06
Iter: 193 loss: 2.5500219365356036e-06
Iter: 194 loss: 2.5430372140484881e-06
Iter: 195 loss: 2.5410396846950967e-06
Iter: 196 loss: 2.5304312014081542e-06
Iter: 197 loss: 2.6410742382383241e-06
Iter: 198 loss: 2.530145987462049e-06
Iter: 199 loss: 2.5198796989968672e-06
Iter: 200 loss: 2.5184468918676208e-06
Iter: 201 loss: 2.5112206704259987e-06
Iter: 202 loss: 2.501150473558418e-06
Iter: 203 loss: 2.5529849382844372e-06
Iter: 204 loss: 2.4995296236920038e-06
Iter: 205 loss: 2.4874999523877515e-06
Iter: 206 loss: 2.512014825201259e-06
Iter: 207 loss: 2.4826452720738228e-06
Iter: 208 loss: 2.4752702766726064e-06
Iter: 209 loss: 2.4713550539716252e-06
Iter: 210 loss: 2.4680086935434691e-06
Iter: 211 loss: 2.4582098250668067e-06
Iter: 212 loss: 2.5701868922060881e-06
Iter: 213 loss: 2.4580437559363691e-06
Iter: 214 loss: 2.4495535178386038e-06
Iter: 215 loss: 2.4790633952282271e-06
Iter: 216 loss: 2.4473263288035027e-06
Iter: 217 loss: 2.4416180553246538e-06
Iter: 218 loss: 2.4292558378224618e-06
Iter: 219 loss: 2.6187463520427587e-06
Iter: 220 loss: 2.4287817761197259e-06
Iter: 221 loss: 2.4159072405974183e-06
Iter: 222 loss: 2.4828119233156692e-06
Iter: 223 loss: 2.4138645068433158e-06
Iter: 224 loss: 2.4036936603579581e-06
Iter: 225 loss: 2.4749023862105942e-06
Iter: 226 loss: 2.4027590935803572e-06
Iter: 227 loss: 2.3960825901292669e-06
Iter: 228 loss: 2.4571899777704674e-06
Iter: 229 loss: 2.3957848350605037e-06
Iter: 230 loss: 2.3896980996022474e-06
Iter: 231 loss: 2.4206333592830172e-06
Iter: 232 loss: 2.3886995225887542e-06
Iter: 233 loss: 2.384130404186545e-06
Iter: 234 loss: 2.37432810921293e-06
Iter: 235 loss: 2.5286416760171251e-06
Iter: 236 loss: 2.3739910025327103e-06
Iter: 237 loss: 2.3670716698950769e-06
Iter: 238 loss: 2.3667121585479676e-06
Iter: 239 loss: 2.3612597549661631e-06
Iter: 240 loss: 2.3649041161508702e-06
Iter: 241 loss: 2.3578279034918506e-06
Iter: 242 loss: 2.3499958943438463e-06
Iter: 243 loss: 2.3447029543020808e-06
Iter: 244 loss: 2.3418064402509443e-06
Iter: 245 loss: 2.3326488348671642e-06
Iter: 246 loss: 2.3324230873008546e-06
Iter: 247 loss: 2.329066303007086e-06
Iter: 248 loss: 2.3201588172920869e-06
Iter: 249 loss: 2.3840977371757876e-06
Iter: 250 loss: 2.3182226988713479e-06
Iter: 251 loss: 2.3116922157549584e-06
Iter: 252 loss: 2.3110056980945311e-06
Iter: 253 loss: 2.305936514999899e-06
Iter: 254 loss: 2.3292250765244269e-06
Iter: 255 loss: 2.3049774494983169e-06
Iter: 256 loss: 2.3014334504288626e-06
Iter: 257 loss: 2.2941700647173253e-06
Iter: 258 loss: 2.4237482223017758e-06
Iter: 259 loss: 2.2940342374583015e-06
Iter: 260 loss: 2.284765064095561e-06
Iter: 261 loss: 2.2844257631215726e-06
Iter: 262 loss: 2.277247248675703e-06
Iter: 263 loss: 2.267231239175706e-06
Iter: 264 loss: 2.4024330759070197e-06
Iter: 265 loss: 2.2671862889723844e-06
Iter: 266 loss: 2.2620405022778872e-06
Iter: 267 loss: 2.2620224874282239e-06
Iter: 268 loss: 2.2571664783335085e-06
Iter: 269 loss: 2.2631986171093182e-06
Iter: 270 loss: 2.2546448384577495e-06
Iter: 271 loss: 2.2497721972965546e-06
Iter: 272 loss: 2.2467669817630753e-06
Iter: 273 loss: 2.2448009919723777e-06
Iter: 274 loss: 2.2414170522873117e-06
Iter: 275 loss: 2.2412384541234713e-06
Iter: 276 loss: 2.2380972616923193e-06
Iter: 277 loss: 2.235330099802637e-06
Iter: 278 loss: 2.2345100050909935e-06
Iter: 279 loss: 2.228391529378826e-06
Iter: 280 loss: 2.2374110601076895e-06
Iter: 281 loss: 2.2254434148850567e-06
Iter: 282 loss: 2.2199655472728077e-06
Iter: 283 loss: 2.2991522312980709e-06
Iter: 284 loss: 2.2199566818430932e-06
Iter: 285 loss: 2.2165764535651968e-06
Iter: 286 loss: 2.2079667816765663e-06
Iter: 287 loss: 2.2827873309110796e-06
Iter: 288 loss: 2.2065727295691996e-06
Iter: 289 loss: 2.2042563637243627e-06
Iter: 290 loss: 2.2025058270200786e-06
Iter: 291 loss: 2.1988891319322223e-06
Iter: 292 loss: 2.2036653984663529e-06
Iter: 293 loss: 2.1970581597604684e-06
Iter: 294 loss: 2.1929028751403645e-06
Iter: 295 loss: 2.1901772704922237e-06
Iter: 296 loss: 2.1885940122324034e-06
Iter: 297 loss: 2.1831208684757579e-06
Iter: 298 loss: 2.1743079546557368e-06
Iter: 299 loss: 2.1742428565715032e-06
Iter: 300 loss: 2.1686881273595095e-06
Iter: 301 loss: 2.1679547614932522e-06
Iter: 302 loss: 2.16447031376205e-06
Iter: 303 loss: 2.1644585826999508e-06
Iter: 304 loss: 2.1612894320028648e-06
Iter: 305 loss: 2.1586139254381223e-06
Iter: 306 loss: 2.1577276721472932e-06
Iter: 307 loss: 2.1539784315685915e-06
Iter: 308 loss: 2.1591434474338104e-06
Iter: 309 loss: 2.1521152104352267e-06
Iter: 310 loss: 2.1488834091533364e-06
Iter: 311 loss: 2.1979238112815781e-06
Iter: 312 loss: 2.1488819735576611e-06
Iter: 313 loss: 2.1460495213872963e-06
Iter: 314 loss: 2.1422561297651708e-06
Iter: 315 loss: 2.1420487934209422e-06
Iter: 316 loss: 2.1366791072002457e-06
Iter: 317 loss: 2.1696934831160841e-06
Iter: 318 loss: 2.1360371780754494e-06
Iter: 319 loss: 2.1318634979893437e-06
Iter: 320 loss: 2.1554172136792554e-06
Iter: 321 loss: 2.1312833342171375e-06
Iter: 322 loss: 2.1279647899951937e-06
Iter: 323 loss: 2.1209898644738406e-06
Iter: 324 loss: 2.2372923298445869e-06
Iter: 325 loss: 2.1208040075624065e-06
Iter: 326 loss: 2.1168530521339794e-06
Iter: 327 loss: 2.1165665492591839e-06
Iter: 328 loss: 2.112176777585978e-06
Iter: 329 loss: 2.1151528221785857e-06
Iter: 330 loss: 2.1094230225164071e-06
Iter: 331 loss: 2.1060286820086024e-06
Iter: 332 loss: 2.1092337604940173e-06
Iter: 333 loss: 2.1040833984608656e-06
Iter: 334 loss: 2.0995745632794873e-06
Iter: 335 loss: 2.0972377588034954e-06
Iter: 336 loss: 2.0951578839565125e-06
Iter: 337 loss: 2.0907478453653384e-06
Iter: 338 loss: 2.1308035183119489e-06
Iter: 339 loss: 2.0905462216343497e-06
Iter: 340 loss: 2.0875071547557874e-06
Iter: 341 loss: 2.0874801867051831e-06
Iter: 342 loss: 2.0851646308857675e-06
Iter: 343 loss: 2.0814719243228607e-06
Iter: 344 loss: 2.0814392763764484e-06
Iter: 345 loss: 2.0780518260690014e-06
Iter: 346 loss: 2.0921748881088611e-06
Iter: 347 loss: 2.0773263226624458e-06
Iter: 348 loss: 2.074790409268926e-06
Iter: 349 loss: 2.1051407650036917e-06
Iter: 350 loss: 2.07475813309966e-06
Iter: 351 loss: 2.0723150997275296e-06
Iter: 352 loss: 2.0674008804391219e-06
Iter: 353 loss: 2.1594055580746033e-06
Iter: 354 loss: 2.0673328988184691e-06
Iter: 355 loss: 2.064000794386686e-06
Iter: 356 loss: 2.0638135914746242e-06
Iter: 357 loss: 2.0611285064297052e-06
Iter: 358 loss: 2.0597123233288833e-06
Iter: 359 loss: 2.0584882436072825e-06
Iter: 360 loss: 2.0539945046727311e-06
Iter: 361 loss: 2.057274772153613e-06
Iter: 362 loss: 2.0512255728199884e-06
Iter: 363 loss: 2.0481262988937428e-06
Iter: 364 loss: 2.0925166133111697e-06
Iter: 365 loss: 2.04812034910252e-06
Iter: 366 loss: 2.0448020175920319e-06
Iter: 367 loss: 2.0442775171894475e-06
Iter: 368 loss: 2.0419832768644632e-06
Iter: 369 loss: 2.0382874703689228e-06
Iter: 370 loss: 2.0409839780120081e-06
Iter: 371 loss: 2.0360098944492224e-06
Iter: 372 loss: 2.0322276048075489e-06
Iter: 373 loss: 2.0393581062132522e-06
Iter: 374 loss: 2.0306271845645267e-06
Iter: 375 loss: 2.0262766307515813e-06
Iter: 376 loss: 2.0385241195656791e-06
Iter: 377 loss: 2.0248894326705567e-06
Iter: 378 loss: 2.0227978633780578e-06
Iter: 379 loss: 2.0221479557391863e-06
Iter: 380 loss: 2.0208580396075271e-06
Iter: 381 loss: 2.017279307357684e-06
Iter: 382 loss: 2.03733004116042e-06
Iter: 383 loss: 2.016220927410615e-06
Iter: 384 loss: 2.0124821909351169e-06
Iter: 385 loss: 2.0569439833612169e-06
Iter: 386 loss: 2.01243255423092e-06
Iter: 387 loss: 2.0095180318185632e-06
Iter: 388 loss: 2.029538612825515e-06
Iter: 389 loss: 2.009238616889197e-06
Iter: 390 loss: 2.0068719316850304e-06
Iter: 391 loss: 2.0071231399759289e-06
Iter: 392 loss: 2.0050519063829308e-06
Iter: 393 loss: 2.0033123679403993e-06
Iter: 394 loss: 2.0294000205476543e-06
Iter: 395 loss: 2.0033112152606283e-06
Iter: 396 loss: 2.0015348281167033e-06
Iter: 397 loss: 1.9972277892358843e-06
Iter: 398 loss: 2.0428472058222079e-06
Iter: 399 loss: 1.9967500672234785e-06
Iter: 400 loss: 1.9935467915798174e-06
Iter: 401 loss: 1.9935460071347555e-06
Iter: 402 loss: 1.991089671027107e-06
Iter: 403 loss: 2.00018098619135e-06
Iter: 404 loss: 1.990486593418407e-06
Iter: 405 loss: 1.9874706133122211e-06
Iter: 406 loss: 1.9884265569658349e-06
Iter: 407 loss: 1.9853227139994548e-06
Iter: 408 loss: 1.9830464900394846e-06
Iter: 409 loss: 1.982772037030115e-06
Iter: 410 loss: 1.9811405503927458e-06
Iter: 411 loss: 1.9778345445689002e-06
Iter: 412 loss: 1.9879852987650652e-06
Iter: 413 loss: 1.9768576097381345e-06
Iter: 414 loss: 1.974464325453202e-06
Iter: 415 loss: 2.0124544170853224e-06
Iter: 416 loss: 1.9744643017747818e-06
Iter: 417 loss: 1.9720311577415292e-06
Iter: 418 loss: 1.9768365458490923e-06
Iter: 419 loss: 1.971029843357665e-06
Iter: 420 loss: 1.969006303672718e-06
Iter: 421 loss: 1.9643037334676256e-06
Iter: 422 loss: 2.0221764775545664e-06
Iter: 423 loss: 1.9639390537673492e-06
Iter: 424 loss: 1.9620850277702835e-06
Iter: 425 loss: 1.9614700116292e-06
Iter: 426 loss: 1.9588137348401511e-06
Iter: 427 loss: 1.9585535051558714e-06
Iter: 428 loss: 1.9566085282713335e-06
Iter: 429 loss: 1.9539671274515669e-06
Iter: 430 loss: 1.9687564540871122e-06
Iter: 431 loss: 1.9535950838813338e-06
Iter: 432 loss: 1.9514224514178683e-06
Iter: 433 loss: 1.9620208128029175e-06
Iter: 434 loss: 1.9510440962327832e-06
Iter: 435 loss: 1.949012556188833e-06
Iter: 436 loss: 1.9469830139951945e-06
Iter: 437 loss: 1.9465673147107427e-06
Iter: 438 loss: 1.9445320088593995e-06
Iter: 439 loss: 1.9608044091269077e-06
Iter: 440 loss: 1.9443961388732577e-06
Iter: 441 loss: 1.9418627196013485e-06
Iter: 442 loss: 1.9438969416535752e-06
Iter: 443 loss: 1.9403400252900848e-06
Iter: 444 loss: 1.937733576984666e-06
Iter: 445 loss: 1.9444419810162373e-06
Iter: 446 loss: 1.9368400828944936e-06
Iter: 447 loss: 1.9344854505159839e-06
Iter: 448 loss: 1.9308304177953147e-06
Iter: 449 loss: 1.9307809107826976e-06
Iter: 450 loss: 1.9274985003362295e-06
Iter: 451 loss: 1.9274976478178232e-06
Iter: 452 loss: 1.9256853155790171e-06
Iter: 453 loss: 1.9256788320180167e-06
Iter: 454 loss: 1.9241071648243563e-06
Iter: 455 loss: 1.9215791628621203e-06
Iter: 456 loss: 1.9215601046862386e-06
Iter: 457 loss: 1.9192114980640922e-06
Iter: 458 loss: 1.9188432155957749e-06
Iter: 459 loss: 1.9172174162193389e-06
Iter: 460 loss: 1.9155679671148266e-06
Iter: 461 loss: 1.9152411430499709e-06
Iter: 462 loss: 1.9134043343991486e-06
Iter: 463 loss: 1.9111655879124923e-06
Iter: 464 loss: 1.9109566800867281e-06
Iter: 465 loss: 1.9087687685368682e-06
Iter: 466 loss: 1.9297245577398375e-06
Iter: 467 loss: 1.9086857132184868e-06
Iter: 468 loss: 1.9064751503804494e-06
Iter: 469 loss: 1.9074650709885659e-06
Iter: 470 loss: 1.9049724997477017e-06
Iter: 471 loss: 1.9024359721979258e-06
Iter: 472 loss: 1.9067223304615288e-06
Iter: 473 loss: 1.9012948383736072e-06
Iter: 474 loss: 1.900069263432933e-06
Iter: 475 loss: 1.9000666888501434e-06
Iter: 476 loss: 1.8986296972714632e-06
Iter: 477 loss: 1.8953593805274804e-06
Iter: 478 loss: 1.9384696262240587e-06
Iter: 479 loss: 1.8951500715040286e-06
Iter: 480 loss: 1.8926483668675995e-06
Iter: 481 loss: 1.923020642738607e-06
Iter: 482 loss: 1.8926195657486048e-06
Iter: 483 loss: 1.8907645131242111e-06
Iter: 484 loss: 1.8882206277252931e-06
Iter: 485 loss: 1.8881025850262554e-06
Iter: 486 loss: 1.8865047848136694e-06
Iter: 487 loss: 1.8861760540097345e-06
Iter: 488 loss: 1.8843387516992892e-06
Iter: 489 loss: 1.8877904242246532e-06
Iter: 490 loss: 1.8835597501361114e-06
Iter: 491 loss: 1.882182635037647e-06
Iter: 492 loss: 1.8806147552844909e-06
Iter: 493 loss: 1.8804156265368268e-06
Iter: 494 loss: 1.8778181277266654e-06
Iter: 495 loss: 1.8832075071899084e-06
Iter: 496 loss: 1.8767817691161005e-06
Iter: 497 loss: 1.8754795118461336e-06
Iter: 498 loss: 1.875208994891505e-06
Iter: 499 loss: 1.8741080961088638e-06
Iter: 500 loss: 1.8714455997350795e-06
Iter: 501 loss: 1.8999202290320071e-06
Iter: 502 loss: 1.8711562882971038e-06
Iter: 503 loss: 1.8693462811253686e-06
Iter: 504 loss: 1.8692084048087164e-06
Iter: 505 loss: 1.8675700061366329e-06
Iter: 506 loss: 1.8657312041812349e-06
Iter: 507 loss: 1.8654835317750428e-06
Iter: 508 loss: 1.8636760771298137e-06
Iter: 509 loss: 1.8775150112183911e-06
Iter: 510 loss: 1.86354113677275e-06
Iter: 511 loss: 1.8622962011754654e-06
Iter: 512 loss: 1.8741585014278041e-06
Iter: 513 loss: 1.8622480257622888e-06
Iter: 514 loss: 1.8610603602198911e-06
Iter: 515 loss: 1.8582040262585219e-06
Iter: 516 loss: 1.8893846670309875e-06
Iter: 517 loss: 1.8579074186783009e-06
Iter: 518 loss: 1.8553830004888183e-06
Iter: 519 loss: 1.8709477041878588e-06
Iter: 520 loss: 1.8550827406085677e-06
Iter: 521 loss: 1.8528470358307398e-06
Iter: 522 loss: 1.8624839532115961e-06
Iter: 523 loss: 1.8523875654226609e-06
Iter: 524 loss: 1.8507249264911274e-06
Iter: 525 loss: 1.8507247019484197e-06
Iter: 526 loss: 1.8494063738523607e-06
Iter: 527 loss: 1.8475925503018272e-06
Iter: 528 loss: 1.847510369012187e-06
Iter: 529 loss: 1.845538218169387e-06
Iter: 530 loss: 1.8451941178610131e-06
Iter: 531 loss: 1.8438523735738023e-06
Iter: 532 loss: 1.8423094435884028e-06
Iter: 533 loss: 1.8422090861904552e-06
Iter: 534 loss: 1.8407674594670853e-06
Iter: 535 loss: 1.843624639299367e-06
Iter: 536 loss: 1.8401754913896577e-06
Iter: 537 loss: 1.8385358340049446e-06
Iter: 538 loss: 1.8362609030398237e-06
Iter: 539 loss: 1.8361640742646275e-06
Iter: 540 loss: 1.8352113523132613e-06
Iter: 541 loss: 1.8347284603986515e-06
Iter: 542 loss: 1.8337447975904202e-06
Iter: 543 loss: 1.8310536865136209e-06
Iter: 544 loss: 1.847446029188311e-06
Iter: 545 loss: 1.8303306611452721e-06
Iter: 546 loss: 1.8285077413973537e-06
Iter: 547 loss: 1.8282671878796778e-06
Iter: 548 loss: 1.8267942611152321e-06
Iter: 549 loss: 1.8325873629316669e-06
Iter: 550 loss: 1.8264563380661603e-06
Iter: 551 loss: 1.8253548351820427e-06
Iter: 552 loss: 1.8234423756349976e-06
Iter: 553 loss: 1.8234415243507462e-06
Iter: 554 loss: 1.8209541794562657e-06
Iter: 555 loss: 1.8227536042135552e-06
Iter: 556 loss: 1.8194179701038753e-06
Iter: 557 loss: 1.819496530891675e-06
Iter: 558 loss: 1.8181497506854481e-06
Iter: 559 loss: 1.8171061827625799e-06
Iter: 560 loss: 1.8159991680223704e-06
Iter: 561 loss: 1.8158143640658872e-06
Iter: 562 loss: 1.8141365293725448e-06
Iter: 563 loss: 1.8153793659317653e-06
Iter: 564 loss: 1.8131065165437464e-06
Iter: 565 loss: 1.8109969245554418e-06
Iter: 566 loss: 1.8107624306247465e-06
Iter: 567 loss: 1.8092368117335367e-06
Iter: 568 loss: 1.808737031079398e-06
Iter: 569 loss: 1.8078102036946921e-06
Iter: 570 loss: 1.8070286101319902e-06
Iter: 571 loss: 1.805858910139368e-06
Iter: 572 loss: 1.8058339162896259e-06
Iter: 573 loss: 1.804101369157102e-06
Iter: 574 loss: 1.8068691908845477e-06
Iter: 575 loss: 1.8032988314669547e-06
Iter: 576 loss: 1.8012034230774419e-06
Iter: 577 loss: 1.8171854827423259e-06
Iter: 578 loss: 1.8010454764790164e-06
Iter: 579 loss: 1.7997097026301692e-06
Iter: 580 loss: 1.7971896399191753e-06
Iter: 581 loss: 1.8523413028707174e-06
Iter: 582 loss: 1.7971820619857045e-06
Iter: 583 loss: 1.7968199036696356e-06
Iter: 584 loss: 1.795887842630706e-06
Iter: 585 loss: 1.7949944285838055e-06
Iter: 586 loss: 1.7928038247689826e-06
Iter: 587 loss: 1.815042260979406e-06
Iter: 588 loss: 1.7925382299273826e-06
Iter: 589 loss: 1.7902186576098536e-06
Iter: 590 loss: 1.8039370327843452e-06
Iter: 591 loss: 1.7899211616432432e-06
Iter: 592 loss: 1.7885260076786573e-06
Iter: 593 loss: 1.7979554951983696e-06
Iter: 594 loss: 1.7883875754726317e-06
Iter: 595 loss: 1.7871254340443582e-06
Iter: 596 loss: 1.79891386382032e-06
Iter: 597 loss: 1.7870729713864826e-06
Iter: 598 loss: 1.7861656120822398e-06
Iter: 599 loss: 1.7837281199929344e-06
Iter: 600 loss: 1.8001412075329076e-06
Iter: 601 loss: 1.7831506982322719e-06
Iter: 602 loss: 1.7810491229437955e-06
Iter: 603 loss: 1.808715889357888e-06
Iter: 604 loss: 1.7810368163936299e-06
Iter: 605 loss: 1.7791641299188872e-06
Iter: 606 loss: 1.7823744719600262e-06
Iter: 607 loss: 1.7783281347611633e-06
Iter: 608 loss: 1.775937724175334e-06
Iter: 609 loss: 1.7932569584614885e-06
Iter: 610 loss: 1.7757340739214501e-06
Iter: 611 loss: 1.7746837528895062e-06
Iter: 612 loss: 1.7737110532639973e-06
Iter: 613 loss: 1.7734601098552247e-06
Iter: 614 loss: 1.771524757686199e-06
Iter: 615 loss: 1.7846662311234271e-06
Iter: 616 loss: 1.7713345920367866e-06
Iter: 617 loss: 1.7696647674225233e-06
Iter: 618 loss: 1.7726273286985057e-06
Iter: 619 loss: 1.7689332693497687e-06
Iter: 620 loss: 1.7675594446153708e-06
Iter: 621 loss: 1.7677372899535675e-06
Iter: 622 loss: 1.7665120001576335e-06
Iter: 623 loss: 1.7645243518346516e-06
Iter: 624 loss: 1.7870903070356481e-06
Iter: 625 loss: 1.7644894038808287e-06
Iter: 626 loss: 1.7634978531185224e-06
Iter: 627 loss: 1.7611097622747279e-06
Iter: 628 loss: 1.7870420360765176e-06
Iter: 629 loss: 1.7608588620992498e-06
Iter: 630 loss: 1.7588285688577438e-06
Iter: 631 loss: 1.7828438211839329e-06
Iter: 632 loss: 1.7588006689784788e-06
Iter: 633 loss: 1.7577526195221923e-06
Iter: 634 loss: 1.7577289310127584e-06
Iter: 635 loss: 1.7567349846019574e-06
Iter: 636 loss: 1.7550342812055138e-06
Iter: 637 loss: 1.7550323261502313e-06
Iter: 638 loss: 1.7533408882933249e-06
Iter: 639 loss: 1.755054784196666e-06
Iter: 640 loss: 1.7523936531888719e-06
Iter: 641 loss: 1.7504230340544131e-06
Iter: 642 loss: 1.7556094938062413e-06
Iter: 643 loss: 1.7497592483354462e-06
Iter: 644 loss: 1.7475087817984067e-06
Iter: 645 loss: 1.7715689797439564e-06
Iter: 646 loss: 1.7474548188697228e-06
Iter: 647 loss: 1.746146094413383e-06
Iter: 648 loss: 1.7480038724679951e-06
Iter: 649 loss: 1.7455043546425825e-06
Iter: 650 loss: 1.7442962694176816e-06
Iter: 651 loss: 1.7439936663324103e-06
Iter: 652 loss: 1.7432327003613876e-06
Iter: 653 loss: 1.7412056202879021e-06
Iter: 654 loss: 1.7581117618735209e-06
Iter: 655 loss: 1.7410851739427344e-06
Iter: 656 loss: 1.7399498054573124e-06
Iter: 657 loss: 1.7398785717609697e-06
Iter: 658 loss: 1.7390197747102881e-06
Iter: 659 loss: 1.7375418610987854e-06
Iter: 660 loss: 1.7476409568725832e-06
Iter: 661 loss: 1.7373985731823983e-06
Iter: 662 loss: 1.7359569527660211e-06
Iter: 663 loss: 1.7384481291214079e-06
Iter: 664 loss: 1.7353161541921775e-06
Iter: 665 loss: 1.7338940146375076e-06
Iter: 666 loss: 1.730738741281616e-06
Iter: 667 loss: 1.7757655865650954e-06
Iter: 668 loss: 1.7305816160054843e-06
Iter: 669 loss: 1.7286374009493706e-06
Iter: 670 loss: 1.7285055113171827e-06
Iter: 671 loss: 1.7267909736583863e-06
Iter: 672 loss: 1.7425239598869659e-06
Iter: 673 loss: 1.7267152480177836e-06
Iter: 674 loss: 1.7260800765734519e-06
Iter: 675 loss: 1.7245129906710435e-06
Iter: 676 loss: 1.7400446417837938e-06
Iter: 677 loss: 1.7243135846114211e-06
Iter: 678 loss: 1.7225603669110424e-06
Iter: 679 loss: 1.735160703398421e-06
Iter: 680 loss: 1.72240823838612e-06
Iter: 681 loss: 1.7211184754863385e-06
Iter: 682 loss: 1.730860597584541e-06
Iter: 683 loss: 1.7210189139982944e-06
Iter: 684 loss: 1.7195268516729909e-06
Iter: 685 loss: 1.7206789378664847e-06
Iter: 686 loss: 1.7186205765390563e-06
Iter: 687 loss: 1.7173953554271901e-06
Iter: 688 loss: 1.7172965608430285e-06
Iter: 689 loss: 1.7163849049429263e-06
Iter: 690 loss: 1.7142969588229637e-06
Iter: 691 loss: 1.7262952702004257e-06
Iter: 692 loss: 1.7140154978335755e-06
Iter: 693 loss: 1.7123589486946228e-06
Iter: 694 loss: 1.718795573685023e-06
Iter: 695 loss: 1.7119735536048145e-06
Iter: 696 loss: 1.7108598666294767e-06
Iter: 697 loss: 1.71091156460049e-06
Iter: 698 loss: 1.7099842346932557e-06
Iter: 699 loss: 1.7084733957432746e-06
Iter: 700 loss: 1.7265275803156777e-06
Iter: 701 loss: 1.708453969422423e-06
Iter: 702 loss: 1.7077320524464825e-06
Iter: 703 loss: 1.7067768304799714e-06
Iter: 704 loss: 1.7067203974636375e-06
Iter: 705 loss: 1.7049360332271695e-06
Iter: 706 loss: 1.7039817462180536e-06
Iter: 707 loss: 1.7031762237522335e-06
Iter: 708 loss: 1.7040734300884464e-06
Iter: 709 loss: 1.7023516157670391e-06
Iter: 710 loss: 1.7016843214574964e-06
Iter: 711 loss: 1.6996039703715317e-06
Iter: 712 loss: 1.7033343090063931e-06
Iter: 713 loss: 1.698229531278565e-06
Iter: 714 loss: 1.69518327204468e-06
Iter: 715 loss: 1.7180683812486635e-06
Iter: 716 loss: 1.6949450368077727e-06
Iter: 717 loss: 1.6934759230586058e-06
Iter: 718 loss: 1.7055582610553235e-06
Iter: 719 loss: 1.6933851020021262e-06
Iter: 720 loss: 1.6922634958148952e-06
Iter: 721 loss: 1.7038469072380839e-06
Iter: 722 loss: 1.6922320183901577e-06
Iter: 723 loss: 1.6912437278631686e-06
Iter: 724 loss: 1.6891732806684673e-06
Iter: 725 loss: 1.7240073919065952e-06
Iter: 726 loss: 1.6891204323906114e-06
Iter: 727 loss: 1.6880381094579444e-06
Iter: 728 loss: 1.6880296935742492e-06
Iter: 729 loss: 1.6867772448547326e-06
Iter: 730 loss: 1.6859159963690042e-06
Iter: 731 loss: 1.6854610086699995e-06
Iter: 732 loss: 1.6835594841048079e-06
Iter: 733 loss: 1.6918456220029478e-06
Iter: 734 loss: 1.683174026932351e-06
Iter: 735 loss: 1.6819047237619919e-06
Iter: 736 loss: 1.690836456960009e-06
Iter: 737 loss: 1.6817892203574814e-06
Iter: 738 loss: 1.680399584168712e-06
Iter: 739 loss: 1.6791940501554498e-06
Iter: 740 loss: 1.6788218792700777e-06
Iter: 741 loss: 1.6772046462216371e-06
Iter: 742 loss: 1.6814428009251569e-06
Iter: 743 loss: 1.6766580815489339e-06
Iter: 744 loss: 1.6752312526578087e-06
Iter: 745 loss: 1.6861419640887566e-06
Iter: 746 loss: 1.67512441210919e-06
Iter: 747 loss: 1.6737030775869276e-06
Iter: 748 loss: 1.6806183174113924e-06
Iter: 749 loss: 1.6734545976850147e-06
Iter: 750 loss: 1.6724863028834382e-06
Iter: 751 loss: 1.6702783331434217e-06
Iter: 752 loss: 1.699200784571775e-06
Iter: 753 loss: 1.6701343792438089e-06
Iter: 754 loss: 1.6679612824312959e-06
Iter: 755 loss: 1.6776819865763451e-06
Iter: 756 loss: 1.66753533310172e-06
Iter: 757 loss: 1.6662871406369675e-06
Iter: 758 loss: 1.6662797994738312e-06
Iter: 759 loss: 1.6649372894378004e-06
Iter: 760 loss: 1.6656764161554538e-06
Iter: 761 loss: 1.6640570795230845e-06
Iter: 762 loss: 1.6626746836979093e-06
Iter: 763 loss: 1.6647112999833209e-06
Iter: 764 loss: 1.6620084752831808e-06
Iter: 765 loss: 1.6610738707898679e-06
Iter: 766 loss: 1.6680650081665803e-06
Iter: 767 loss: 1.661000001142152e-06
Iter: 768 loss: 1.6597558209956745e-06
Iter: 769 loss: 1.6581873966674006e-06
Iter: 770 loss: 1.6580644035839822e-06
Iter: 771 loss: 1.6564723072796246e-06
Iter: 772 loss: 1.6679191360400627e-06
Iter: 773 loss: 1.6563342313535493e-06
Iter: 774 loss: 1.6547570305595385e-06
Iter: 775 loss: 1.6604443692547364e-06
Iter: 776 loss: 1.6543588606518304e-06
Iter: 777 loss: 1.6531541852103401e-06
Iter: 778 loss: 1.6538148157290804e-06
Iter: 779 loss: 1.6523637498793152e-06
Iter: 780 loss: 1.6510703382005748e-06
Iter: 781 loss: 1.6538771612653036e-06
Iter: 782 loss: 1.6505691599921784e-06
Iter: 783 loss: 1.6496866748477216e-06
Iter: 784 loss: 1.649655850579281e-06
Iter: 785 loss: 1.648853880428172e-06
Iter: 786 loss: 1.6473222964882051e-06
Iter: 787 loss: 1.6798934131234577e-06
Iter: 788 loss: 1.6473155267287652e-06
Iter: 789 loss: 1.6458635403283177e-06
Iter: 790 loss: 1.6503005612663363e-06
Iter: 791 loss: 1.6454325214166306e-06
Iter: 792 loss: 1.6439158242009739e-06
Iter: 793 loss: 1.642040137779609e-06
Iter: 794 loss: 1.6418774492433932e-06
Iter: 795 loss: 1.6429473492103226e-06
Iter: 796 loss: 1.6408774692740248e-06
Iter: 797 loss: 1.6402009094892367e-06
Iter: 798 loss: 1.6386412581465515e-06
Iter: 799 loss: 1.6583603560489533e-06
Iter: 800 loss: 1.6385287873050182e-06
Iter: 801 loss: 1.6370538508979759e-06
Iter: 802 loss: 1.647884171693188e-06
Iter: 803 loss: 1.6369320358308993e-06
Iter: 804 loss: 1.6358445672227892e-06
Iter: 805 loss: 1.6451279333722529e-06
Iter: 806 loss: 1.6357841846945775e-06
Iter: 807 loss: 1.6347759942929721e-06
Iter: 808 loss: 1.6339664348347916e-06
Iter: 809 loss: 1.6336630386807461e-06
Iter: 810 loss: 1.6326678680866519e-06
Iter: 811 loss: 1.6434255766141432e-06
Iter: 812 loss: 1.6326452527784488e-06
Iter: 813 loss: 1.6314878056473522e-06
Iter: 814 loss: 1.6300441625929135e-06
Iter: 815 loss: 1.6299243364883734e-06
Iter: 816 loss: 1.6283360341816498e-06
Iter: 817 loss: 1.6333787155564188e-06
Iter: 818 loss: 1.627880871489784e-06
Iter: 819 loss: 1.6267857028150202e-06
Iter: 820 loss: 1.6440525967908474e-06
Iter: 821 loss: 1.6267856652002903e-06
Iter: 822 loss: 1.6257690507307556e-06
Iter: 823 loss: 1.6269712889448448e-06
Iter: 824 loss: 1.6252307230414676e-06
Iter: 825 loss: 1.6242104059944614e-06
Iter: 826 loss: 1.6227782476255998e-06
Iter: 827 loss: 1.6227225502765192e-06
Iter: 828 loss: 1.6212030002083783e-06
Iter: 829 loss: 1.6311927168783854e-06
Iter: 830 loss: 1.6210434230066864e-06
Iter: 831 loss: 1.619642487465671e-06
Iter: 832 loss: 1.6203422380254683e-06
Iter: 833 loss: 1.6187073800766636e-06
Iter: 834 loss: 1.6174386343967872e-06
Iter: 835 loss: 1.6173602201298811e-06
Iter: 836 loss: 1.6166244982700265e-06
Iter: 837 loss: 1.6145208281575961e-06
Iter: 838 loss: 1.624143660001995e-06
Iter: 839 loss: 1.6137569188130448e-06
Iter: 840 loss: 1.6136794542082838e-06
Iter: 841 loss: 1.6128439461414164e-06
Iter: 842 loss: 1.6119800754300823e-06
Iter: 843 loss: 1.6119762593751881e-06
Iter: 844 loss: 1.6112878333459824e-06
Iter: 845 loss: 1.6102005184682002e-06
Iter: 846 loss: 1.6121658984170356e-06
Iter: 847 loss: 1.6097291601704725e-06
Iter: 848 loss: 1.6087394798547464e-06
Iter: 849 loss: 1.6198788657226477e-06
Iter: 850 loss: 1.6087212234121706e-06
Iter: 851 loss: 1.6079288676263047e-06
Iter: 852 loss: 1.6066249594842483e-06
Iter: 853 loss: 1.6066190091082481e-06
Iter: 854 loss: 1.6051244547584054e-06
Iter: 855 loss: 1.6102900043438278e-06
Iter: 856 loss: 1.6047301913092536e-06
Iter: 857 loss: 1.6034722456806181e-06
Iter: 858 loss: 1.6034656045364507e-06
Iter: 859 loss: 1.6029007079107083e-06
Iter: 860 loss: 1.6016980258132827e-06
Iter: 861 loss: 1.621051476929935e-06
Iter: 862 loss: 1.6016603111759025e-06
Iter: 863 loss: 1.5999547895720032e-06
Iter: 864 loss: 1.6031431745508495e-06
Iter: 865 loss: 1.5992295383004583e-06
Iter: 866 loss: 1.5981220169982955e-06
Iter: 867 loss: 1.6029944046979689e-06
Iter: 868 loss: 1.5979001932353631e-06
Iter: 869 loss: 1.5967434179395884e-06
Iter: 870 loss: 1.6047481025827581e-06
Iter: 871 loss: 1.5966342440517954e-06
Iter: 872 loss: 1.5953993383891392e-06
Iter: 873 loss: 1.5960100366809722e-06
Iter: 874 loss: 1.5945736174464044e-06
Iter: 875 loss: 1.5934277280885853e-06
Iter: 876 loss: 1.5926605832979378e-06
Iter: 877 loss: 1.5922327021440312e-06
Iter: 878 loss: 1.5914664864840244e-06
Iter: 879 loss: 1.5913126693311058e-06
Iter: 880 loss: 1.5904190511604219e-06
Iter: 881 loss: 1.5890217552541889e-06
Iter: 882 loss: 1.5890047246087941e-06
Iter: 883 loss: 1.5876592630739366e-06
Iter: 884 loss: 1.5996622284410811e-06
Iter: 885 loss: 1.5875940279888145e-06
Iter: 886 loss: 1.5865131664125212e-06
Iter: 887 loss: 1.5921529305565891e-06
Iter: 888 loss: 1.5863427846703194e-06
Iter: 889 loss: 1.5856112932789327e-06
Iter: 890 loss: 1.5845773462076042e-06
Iter: 891 loss: 1.5845393427748683e-06
Iter: 892 loss: 1.5834244571255028e-06
Iter: 893 loss: 1.5834244554222233e-06
Iter: 894 loss: 1.5822239833414654e-06
Iter: 895 loss: 1.5817439690324477e-06
Iter: 896 loss: 1.5811029350766566e-06
Iter: 897 loss: 1.5799573343789178e-06
Iter: 898 loss: 1.5781977063644202e-06
Iter: 899 loss: 1.5781701841274834e-06
Iter: 900 loss: 1.5766202188159356e-06
Iter: 901 loss: 1.5766027525793262e-06
Iter: 902 loss: 1.5756557559618137e-06
Iter: 903 loss: 1.5768794763797918e-06
Iter: 904 loss: 1.5751719001251967e-06
Iter: 905 loss: 1.574019055057067e-06
Iter: 906 loss: 1.5871013802973598e-06
Iter: 907 loss: 1.5739987284309959e-06
Iter: 908 loss: 1.5734178632836467e-06
Iter: 909 loss: 1.5725224908279284e-06
Iter: 910 loss: 1.5725091295665161e-06
Iter: 911 loss: 1.5710812088438065e-06
Iter: 912 loss: 1.5722635340590007e-06
Iter: 913 loss: 1.5702301997440589e-06
Iter: 914 loss: 1.5693771826618362e-06
Iter: 915 loss: 1.5692339140421934e-06
Iter: 916 loss: 1.5685288346025425e-06
Iter: 917 loss: 1.5670442313170889e-06
Iter: 918 loss: 1.5916753105782081e-06
Iter: 919 loss: 1.5670037319813254e-06
Iter: 920 loss: 1.5659396208810556e-06
Iter: 921 loss: 1.5659180163280919e-06
Iter: 922 loss: 1.5649484087083757e-06
Iter: 923 loss: 1.5645853456032907e-06
Iter: 924 loss: 1.5640520772613146e-06
Iter: 925 loss: 1.5630194016188728e-06
Iter: 926 loss: 1.566783323418827e-06
Iter: 927 loss: 1.5627616483195016e-06
Iter: 928 loss: 1.5618182260913109e-06
Iter: 929 loss: 1.5730761309383402e-06
Iter: 930 loss: 1.5618059818567133e-06
Iter: 931 loss: 1.5611813522413698e-06
Iter: 932 loss: 1.5600645327289258e-06
Iter: 933 loss: 1.5876404601713167e-06
Iter: 934 loss: 1.560064502997573e-06
Iter: 935 loss: 1.558771623201357e-06
Iter: 936 loss: 1.5567985572042684e-06
Iter: 937 loss: 1.5567650473775806e-06
Iter: 938 loss: 1.5554644055138861e-06
Iter: 939 loss: 1.5553214658506007e-06
Iter: 940 loss: 1.5541801271939692e-06
Iter: 941 loss: 1.5625830044378134e-06
Iter: 942 loss: 1.5540864325231902e-06
Iter: 943 loss: 1.5531239489487591e-06
Iter: 944 loss: 1.5540690512489916e-06
Iter: 945 loss: 1.5525792924492091e-06
Iter: 946 loss: 1.5516414345098421e-06
Iter: 947 loss: 1.5502587705072333e-06
Iter: 948 loss: 1.5502241372350675e-06
Iter: 949 loss: 1.5493783876224805e-06
Iter: 950 loss: 1.5491971876005184e-06
Iter: 951 loss: 1.5484503648467242e-06
Iter: 952 loss: 1.5498674599535183e-06
Iter: 953 loss: 1.5481355499891847e-06
Iter: 954 loss: 1.54726403455654e-06
Iter: 955 loss: 1.5455270795599268e-06
Iter: 956 loss: 1.5788222139403129e-06
Iter: 957 loss: 1.5455067096368565e-06
Iter: 958 loss: 1.5444767916098514e-06
Iter: 959 loss: 1.5442141683550412e-06
Iter: 960 loss: 1.5436308063796699e-06
Iter: 961 loss: 1.5424999873753619e-06
Iter: 962 loss: 1.56570416340949e-06
Iter: 963 loss: 1.5424925875254564e-06
Iter: 964 loss: 1.5416104620263089e-06
Iter: 965 loss: 1.5415544357056875e-06
Iter: 966 loss: 1.5408211759964281e-06
Iter: 967 loss: 1.5400265334301738e-06
Iter: 968 loss: 1.5399038978777734e-06
Iter: 969 loss: 1.538945579320414e-06
Iter: 970 loss: 1.5397064768042239e-06
Iter: 971 loss: 1.5383678213768128e-06
Iter: 972 loss: 1.5371745807044273e-06
Iter: 973 loss: 1.5367242068854427e-06
Iter: 974 loss: 1.5360702236873311e-06
Iter: 975 loss: 1.5355987649505035e-06
Iter: 976 loss: 1.535197317551971e-06
Iter: 977 loss: 1.5343465667896374e-06
Iter: 978 loss: 1.5335804168634206e-06
Iter: 979 loss: 1.5333665570626272e-06
Iter: 980 loss: 1.5323036433903652e-06
Iter: 981 loss: 1.5352462519303762e-06
Iter: 982 loss: 1.5319599843849678e-06
Iter: 983 loss: 1.5308035535094724e-06
Iter: 984 loss: 1.53194642390183e-06
Iter: 985 loss: 1.5301505411585627e-06
Iter: 986 loss: 1.529024741878435e-06
Iter: 987 loss: 1.5290206594258951e-06
Iter: 988 loss: 1.528426599369331e-06
Iter: 989 loss: 1.5274693348699771e-06
Iter: 990 loss: 1.5274623655219607e-06
Iter: 991 loss: 1.5261401221828583e-06
Iter: 992 loss: 1.5342992255698545e-06
Iter: 993 loss: 1.5259831069120669e-06
Iter: 994 loss: 1.5247128724558381e-06
Iter: 995 loss: 1.5277850515168168e-06
Iter: 996 loss: 1.524256597113413e-06
Iter: 997 loss: 1.523355579350127e-06
Iter: 998 loss: 1.5259309835503952e-06
Iter: 999 loss: 1.5230719620756538e-06
Iter: 1000 loss: 1.5218236841537449e-06
Iter: 1001 loss: 1.525022908327649e-06
Iter: 1002 loss: 1.5213944306596321e-06
Iter: 1003 loss: 1.5206208454548e-06
Iter: 1004 loss: 1.5196282269933087e-06
Iter: 1005 loss: 1.5195576915455428e-06
Iter: 1006 loss: 1.5182964340949244e-06
Iter: 1007 loss: 1.5199930450627077e-06
Iter: 1008 loss: 1.5176629720187735e-06
Iter: 1009 loss: 1.5162992560197613e-06
Iter: 1010 loss: 1.5302355843111784e-06
Iter: 1011 loss: 1.5162591993938586e-06
Iter: 1012 loss: 1.5155976979371841e-06
Iter: 1013 loss: 1.5155928727728997e-06
Iter: 1014 loss: 1.5150169800942863e-06
Iter: 1015 loss: 1.5133858421403816e-06
Iter: 1016 loss: 1.5213703443985107e-06
Iter: 1017 loss: 1.5128305022194205e-06
Iter: 1018 loss: 1.5114866986492038e-06
Iter: 1019 loss: 1.5311846697817712e-06
Iter: 1020 loss: 1.5114850775754e-06
Iter: 1021 loss: 1.5103773579542998e-06
Iter: 1022 loss: 1.5179742288876988e-06
Iter: 1023 loss: 1.5102707914063804e-06
Iter: 1024 loss: 1.5092785379706431e-06
Iter: 1025 loss: 1.5100574336260457e-06
Iter: 1026 loss: 1.5086784742056559e-06
Iter: 1027 loss: 1.5077216890586651e-06
Iter: 1028 loss: 1.5100921063306445e-06
Iter: 1029 loss: 1.5073840685003946e-06
Iter: 1030 loss: 1.5065824760385178e-06
Iter: 1031 loss: 1.5160055392449016e-06
Iter: 1032 loss: 1.5065710195729537e-06
Iter: 1033 loss: 1.5059890271665756e-06
Iter: 1034 loss: 1.5051485523816867e-06
Iter: 1035 loss: 1.505122887017572e-06
Iter: 1036 loss: 1.5040351568407983e-06
Iter: 1037 loss: 1.5188929373712631e-06
Iter: 1038 loss: 1.5040309438146441e-06
Iter: 1039 loss: 1.5033453041378259e-06
Iter: 1040 loss: 1.5027047758941732e-06
Iter: 1041 loss: 1.5025436416867436e-06
Iter: 1042 loss: 1.5014797583844187e-06
Iter: 1043 loss: 1.5003448616401175e-06
Iter: 1044 loss: 1.50015919627501e-06
Iter: 1045 loss: 1.4985572564027884e-06
Iter: 1046 loss: 1.5050755795969946e-06
Iter: 1047 loss: 1.498204004763441e-06
Iter: 1048 loss: 1.4976304983525539e-06
Iter: 1049 loss: 1.4974138213689566e-06
Iter: 1050 loss: 1.4967658384326854e-06
Iter: 1051 loss: 1.4970144407952878e-06
Iter: 1052 loss: 1.496315235463192e-06
Iter: 1053 loss: 1.4954797895631774e-06
Iter: 1054 loss: 1.4941368408410843e-06
Iter: 1055 loss: 1.4941265875804181e-06
Iter: 1056 loss: 1.4931495132856812e-06
Iter: 1057 loss: 1.4931491695731293e-06
Iter: 1058 loss: 1.4919406733661155e-06
Iter: 1059 loss: 1.4917768616347069e-06
Iter: 1060 loss: 1.490922938127377e-06
Iter: 1061 loss: 1.4896768163168522e-06
Iter: 1062 loss: 1.4960614976272583e-06
Iter: 1063 loss: 1.4894748661022428e-06
Iter: 1064 loss: 1.4886995438245239e-06
Iter: 1065 loss: 1.494427835578303e-06
Iter: 1066 loss: 1.4886364429237562e-06
Iter: 1067 loss: 1.4878958826128163e-06
Iter: 1068 loss: 1.4883457578886825e-06
Iter: 1069 loss: 1.4874198629575089e-06
Iter: 1070 loss: 1.4866614294624737e-06
Iter: 1071 loss: 1.491982345764787e-06
Iter: 1072 loss: 1.4865919679379565e-06
Iter: 1073 loss: 1.4857888788685119e-06
Iter: 1074 loss: 1.485572935530978e-06
Iter: 1075 loss: 1.4850767582979406e-06
Iter: 1076 loss: 1.4842574733192367e-06
Iter: 1077 loss: 1.4846067819244734e-06
Iter: 1078 loss: 1.4836963368838737e-06
Iter: 1079 loss: 1.4823759912774224e-06
Iter: 1080 loss: 1.4818049186432715e-06
Iter: 1081 loss: 1.4811267804892537e-06
Iter: 1082 loss: 1.4792644905262369e-06
Iter: 1083 loss: 1.4861388541927696e-06
Iter: 1084 loss: 1.478805947747417e-06
Iter: 1085 loss: 1.4777270985554322e-06
Iter: 1086 loss: 1.4775528287793038e-06
Iter: 1087 loss: 1.4771428821063263e-06
Iter: 1088 loss: 1.4764394425178278e-06
Iter: 1089 loss: 1.4764387471332396e-06
Iter: 1090 loss: 1.4754198811128785e-06
Iter: 1091 loss: 1.4758147491630008e-06
Iter: 1092 loss: 1.4747123510371971e-06
Iter: 1093 loss: 1.474320000468982e-06
Iter: 1094 loss: 1.4740822617265344e-06
Iter: 1095 loss: 1.4735825035974963e-06
Iter: 1096 loss: 1.4724281157931702e-06
Iter: 1097 loss: 1.486925232217115e-06
Iter: 1098 loss: 1.4723433339407294e-06
Iter: 1099 loss: 1.4713070809809756e-06
Iter: 1100 loss: 1.4834251522691272e-06
Iter: 1101 loss: 1.4712917849656733e-06
Iter: 1102 loss: 1.4702360515887679e-06
Iter: 1103 loss: 1.4718978532998238e-06
Iter: 1104 loss: 1.4697433211180116e-06
Iter: 1105 loss: 1.4688527090173344e-06
Iter: 1106 loss: 1.4747685268183149e-06
Iter: 1107 loss: 1.46876111330827e-06
Iter: 1108 loss: 1.4680120207657349e-06
Iter: 1109 loss: 1.4699026480676851e-06
Iter: 1110 loss: 1.4677513687005106e-06
Iter: 1111 loss: 1.4670517229350802e-06
Iter: 1112 loss: 1.4663048657774623e-06
Iter: 1113 loss: 1.4661829956381575e-06
Iter: 1114 loss: 1.4650676706891396e-06
Iter: 1115 loss: 1.4661823638368327e-06
Iter: 1116 loss: 1.4644401784230923e-06
Iter: 1117 loss: 1.4631370061710657e-06
Iter: 1118 loss: 1.4703188053427587e-06
Iter: 1119 loss: 1.4629485617982143e-06
Iter: 1120 loss: 1.4619288411978045e-06
Iter: 1121 loss: 1.4687092964004628e-06
Iter: 1122 loss: 1.4618241819269694e-06
Iter: 1123 loss: 1.4605369859729696e-06
Iter: 1124 loss: 1.4610680398551823e-06
Iter: 1125 loss: 1.4596510120963425e-06
Iter: 1126 loss: 1.4588922625229078e-06
Iter: 1127 loss: 1.4586789697307971e-06
Iter: 1128 loss: 1.4582162323215251e-06
Iter: 1129 loss: 1.4570220145557559e-06
Iter: 1130 loss: 1.4651119740673925e-06
Iter: 1131 loss: 1.4569040922498694e-06
Iter: 1132 loss: 1.4560612575101882e-06
Iter: 1133 loss: 1.4668435684275659e-06
Iter: 1134 loss: 1.4560548060440725e-06
Iter: 1135 loss: 1.4556586286381721e-06
Iter: 1136 loss: 1.4547174146352528e-06
Iter: 1137 loss: 1.4654565564089962e-06
Iter: 1138 loss: 1.4546291186817615e-06
Iter: 1139 loss: 1.4538446431265829e-06
Iter: 1140 loss: 1.4538011008216767e-06
Iter: 1141 loss: 1.4531050995348421e-06
Iter: 1142 loss: 1.4524927583579267e-06
Iter: 1143 loss: 1.4523106393466095e-06
Iter: 1144 loss: 1.4514034994494838e-06
Iter: 1145 loss: 1.4637439800934378e-06
Iter: 1146 loss: 1.4513997988898484e-06
Iter: 1147 loss: 1.4507500458513612e-06
Iter: 1148 loss: 1.4493354736490094e-06
Iter: 1149 loss: 1.4706922265606102e-06
Iter: 1150 loss: 1.4492779360348089e-06
Iter: 1151 loss: 1.4479257645104771e-06
Iter: 1152 loss: 1.4592747738317173e-06
Iter: 1153 loss: 1.4478468497328833e-06
Iter: 1154 loss: 1.4469148935045718e-06
Iter: 1155 loss: 1.4469642986194891e-06
Iter: 1156 loss: 1.4461839462212366e-06
Iter: 1157 loss: 1.4453415344709072e-06
Iter: 1158 loss: 1.4587159995756794e-06
Iter: 1159 loss: 1.4453415264275774e-06
Iter: 1160 loss: 1.4445191213381223e-06
Iter: 1161 loss: 1.4466361860775446e-06
Iter: 1162 loss: 1.4442372728402103e-06
Iter: 1163 loss: 1.4433162590374022e-06
Iter: 1164 loss: 1.4423999880873807e-06
Iter: 1165 loss: 1.4422097718172275e-06
Iter: 1166 loss: 1.4411075373403543e-06
Iter: 1167 loss: 1.4416570246924645e-06
Iter: 1168 loss: 1.440371580177351e-06
Iter: 1169 loss: 1.4396465560643678e-06
Iter: 1170 loss: 1.4394397126424065e-06
Iter: 1171 loss: 1.4389116978275684e-06
Iter: 1172 loss: 1.437984776916189e-06
Iter: 1173 loss: 1.43798465856424e-06
Iter: 1174 loss: 1.4369851499677461e-06
Iter: 1175 loss: 1.4449085450061568e-06
Iter: 1176 loss: 1.4369168848743589e-06
Iter: 1177 loss: 1.4360727539097702e-06
Iter: 1178 loss: 1.4417360989525415e-06
Iter: 1179 loss: 1.4359876974519046e-06
Iter: 1180 loss: 1.4354899826806235e-06
Iter: 1181 loss: 1.4358435297860389e-06
Iter: 1182 loss: 1.4351812078721805e-06
Iter: 1183 loss: 1.4343615077639246e-06
Iter: 1184 loss: 1.435288045013759e-06
Iter: 1185 loss: 1.4339199161941304e-06
Iter: 1186 loss: 1.43304450933098e-06
Iter: 1187 loss: 1.4317052008176157e-06
Iter: 1188 loss: 1.4316831649021681e-06
Iter: 1189 loss: 1.4304954221154604e-06
Iter: 1190 loss: 1.4417417079568985e-06
Iter: 1191 loss: 1.4304484056851252e-06
Iter: 1192 loss: 1.4293898370694992e-06
Iter: 1193 loss: 1.4315496616470792e-06
Iter: 1194 loss: 1.4289629344871259e-06
Iter: 1195 loss: 1.4285058491617963e-06
Iter: 1196 loss: 1.4284071117721146e-06
Iter: 1197 loss: 1.4279683019878403e-06
Iter: 1198 loss: 1.4270702863317403e-06
Iter: 1199 loss: 1.4431529373448119e-06
Iter: 1200 loss: 1.4270538645585824e-06
Iter: 1201 loss: 1.4259074162298536e-06
Iter: 1202 loss: 1.4289029581847126e-06
Iter: 1203 loss: 1.4255190659955493e-06
Iter: 1204 loss: 1.4246523780043647e-06
Iter: 1205 loss: 1.4272608753670278e-06
Iter: 1206 loss: 1.4243915793169076e-06
Iter: 1207 loss: 1.4231052579618045e-06
Iter: 1208 loss: 1.4275595104391357e-06
Iter: 1209 loss: 1.4227665954779352e-06
Iter: 1210 loss: 1.4222218631805704e-06
Iter: 1211 loss: 1.4219480777519456e-06
Iter: 1212 loss: 1.4216916463565294e-06
Iter: 1213 loss: 1.4207281698560179e-06
Iter: 1214 loss: 1.4299107254582964e-06
Iter: 1215 loss: 1.4206909210392246e-06
Iter: 1216 loss: 1.4199892142889708e-06
Iter: 1217 loss: 1.4200684970710785e-06
Iter: 1218 loss: 1.4194509519924606e-06
Iter: 1219 loss: 1.4186899408748825e-06
Iter: 1220 loss: 1.4249121215671046e-06
Iter: 1221 loss: 1.418642119795206e-06
Iter: 1222 loss: 1.417973111871765e-06
Iter: 1223 loss: 1.4167595012043379e-06
Iter: 1224 loss: 1.445790273517189e-06
Iter: 1225 loss: 1.4167590937096769e-06
Iter: 1226 loss: 1.4154142744822552e-06
Iter: 1227 loss: 1.4167542283347594e-06
Iter: 1228 loss: 1.4146568647107483e-06
Iter: 1229 loss: 1.4133562591494523e-06
Iter: 1230 loss: 1.4252313714637424e-06
Iter: 1231 loss: 1.4132978309301701e-06
Iter: 1232 loss: 1.4124786773114837e-06
Iter: 1233 loss: 1.4221740722029225e-06
Iter: 1234 loss: 1.4124674658345429e-06
Iter: 1235 loss: 1.4117598550602036e-06
Iter: 1236 loss: 1.4132788390746425e-06
Iter: 1237 loss: 1.4114836822238461e-06
Iter: 1238 loss: 1.4108620065728903e-06
Iter: 1239 loss: 1.4097556405534806e-06
Iter: 1240 loss: 1.4373487628621566e-06
Iter: 1241 loss: 1.4097556395382102e-06
Iter: 1242 loss: 1.4086496399495613e-06
Iter: 1243 loss: 1.4219922870419618e-06
Iter: 1244 loss: 1.4086363056082021e-06
Iter: 1245 loss: 1.4079359495384633e-06
Iter: 1246 loss: 1.4136720254694915e-06
Iter: 1247 loss: 1.4078921459940727e-06
Iter: 1248 loss: 1.4071524432195988e-06
Iter: 1249 loss: 1.4063914325482072e-06
Iter: 1250 loss: 1.4062500266310906e-06
Iter: 1251 loss: 1.4054208377795348e-06
Iter: 1252 loss: 1.409791359302608e-06
Iter: 1253 loss: 1.4052921442766886e-06
Iter: 1254 loss: 1.4041383013038025e-06
Iter: 1255 loss: 1.4055448149270025e-06
Iter: 1256 loss: 1.403534491818992e-06
Iter: 1257 loss: 1.4030392006047729e-06
Iter: 1258 loss: 1.4068809066384109e-06
Iter: 1259 loss: 1.4030034142125899e-06
Iter: 1260 loss: 1.4024873959232278e-06
Iter: 1261 loss: 1.4020732247048106e-06
Iter: 1262 loss: 1.4019178387769365e-06
Iter: 1263 loss: 1.4010855348131951e-06
Iter: 1264 loss: 1.4008233308439526e-06
Iter: 1265 loss: 1.4003340222811293e-06
Iter: 1266 loss: 1.3991973297048843e-06
Iter: 1267 loss: 1.4041081846591022e-06
Iter: 1268 loss: 1.3989643673953756e-06
Iter: 1269 loss: 1.3979993633718321e-06
Iter: 1270 loss: 1.4008389682383679e-06
Iter: 1271 loss: 1.3977031262997384e-06
Iter: 1272 loss: 1.3964136916836264e-06
Iter: 1273 loss: 1.4043807348016674e-06
Iter: 1274 loss: 1.3962609360443458e-06
Iter: 1275 loss: 1.3955949111967515e-06
Iter: 1276 loss: 1.3956130891444591e-06
Iter: 1277 loss: 1.3950675091345514e-06
Iter: 1278 loss: 1.3941239124813174e-06
Iter: 1279 loss: 1.3943678274874141e-06
Iter: 1280 loss: 1.3934375661603041e-06
Iter: 1281 loss: 1.3926315608295783e-06
Iter: 1282 loss: 1.3926229367622117e-06
Iter: 1283 loss: 1.3918875690632156e-06
Iter: 1284 loss: 1.3927501741837908e-06
Iter: 1285 loss: 1.3914969176055505e-06
Iter: 1286 loss: 1.3908831151854527e-06
Iter: 1287 loss: 1.3920353135772543e-06
Iter: 1288 loss: 1.390622731231812e-06
Iter: 1289 loss: 1.3900116512897804e-06
Iter: 1290 loss: 1.3962242826247618e-06
Iter: 1291 loss: 1.3899933058158492e-06
Iter: 1292 loss: 1.3894056898134915e-06
Iter: 1293 loss: 1.3881877059603144e-06
Iter: 1294 loss: 1.4092792683471829e-06
Iter: 1295 loss: 1.38816084760551e-06
Iter: 1296 loss: 1.3874266462529763e-06
Iter: 1297 loss: 1.3874001797905604e-06
Iter: 1298 loss: 1.3867844791249464e-06
Iter: 1299 loss: 1.3867508250888898e-06
Iter: 1300 loss: 1.3862816778885186e-06
Iter: 1301 loss: 1.3855851993448139e-06
Iter: 1302 loss: 1.384401272508548e-06
Iter: 1303 loss: 1.3843994116655666e-06
Iter: 1304 loss: 1.3833732131157952e-06
Iter: 1305 loss: 1.3833694595624672e-06
Iter: 1306 loss: 1.3827369089865884e-06
Iter: 1307 loss: 1.3885353844529877e-06
Iter: 1308 loss: 1.3827088695767547e-06
Iter: 1309 loss: 1.3819806659701146e-06
Iter: 1310 loss: 1.3812674831103327e-06
Iter: 1311 loss: 1.3811118891846596e-06
Iter: 1312 loss: 1.3803911609417021e-06
Iter: 1313 loss: 1.3798691546389079e-06
Iter: 1314 loss: 1.3796218914462684e-06
Iter: 1315 loss: 1.3784490263064988e-06
Iter: 1316 loss: 1.3934455480619498e-06
Iter: 1317 loss: 1.3784400107927334e-06
Iter: 1318 loss: 1.3777416113146105e-06
Iter: 1319 loss: 1.3845257143827595e-06
Iter: 1320 loss: 1.3777164510703802e-06
Iter: 1321 loss: 1.3772273653697809e-06
Iter: 1322 loss: 1.3763756487398992e-06
Iter: 1323 loss: 1.3763753592653296e-06
Iter: 1324 loss: 1.3758405635871932e-06
Iter: 1325 loss: 1.3757691604118898e-06
Iter: 1326 loss: 1.3752560652150633e-06
Iter: 1327 loss: 1.3744283048048741e-06
Iter: 1328 loss: 1.3744224133857166e-06
Iter: 1329 loss: 1.3735083552826329e-06
Iter: 1330 loss: 1.377729714875748e-06
Iter: 1331 loss: 1.3733366247390258e-06
Iter: 1332 loss: 1.3725742890305804e-06
Iter: 1333 loss: 1.377962154404686e-06
Iter: 1334 loss: 1.3725055926201522e-06
Iter: 1335 loss: 1.3719525374010721e-06
Iter: 1336 loss: 1.3712958358580318e-06
Iter: 1337 loss: 1.3712264456261833e-06
Iter: 1338 loss: 1.3702485602219315e-06
Iter: 1339 loss: 1.3696114241423652e-06
Iter: 1340 loss: 1.3692363829337679e-06
Iter: 1341 loss: 1.3688083398974287e-06
Iter: 1342 loss: 1.3685539942965245e-06
Iter: 1343 loss: 1.3678888245657547e-06
Iter: 1344 loss: 1.3689896577088308e-06
Iter: 1345 loss: 1.3675862696099445e-06
Iter: 1346 loss: 1.3668723164760346e-06
Iter: 1347 loss: 1.3662466439092287e-06
Iter: 1348 loss: 1.3660585990136082e-06
Iter: 1349 loss: 1.3650867733454146e-06
Iter: 1350 loss: 1.3661772150053661e-06
Iter: 1351 loss: 1.3645618063910901e-06
Iter: 1352 loss: 1.3642118031695177e-06
Iter: 1353 loss: 1.3639475631608461e-06
Iter: 1354 loss: 1.3635589300349728e-06
Iter: 1355 loss: 1.3628795789959334e-06
Iter: 1356 loss: 1.3628794247130912e-06
Iter: 1357 loss: 1.3620699104172384e-06
Iter: 1358 loss: 1.3688964653436041e-06
Iter: 1359 loss: 1.3620233073018067e-06
Iter: 1360 loss: 1.3614067270496877e-06
Iter: 1361 loss: 1.3652125565113319e-06
Iter: 1362 loss: 1.3613335467152835e-06
Iter: 1363 loss: 1.3610211689658885e-06
Iter: 1364 loss: 1.360329235381486e-06
Iter: 1365 loss: 1.3702513090496316e-06
Iter: 1366 loss: 1.3602953536160978e-06
Iter: 1367 loss: 1.3592248998636185e-06
Iter: 1368 loss: 1.3651214505852421e-06
Iter: 1369 loss: 1.3590699973341494e-06
Iter: 1370 loss: 1.3581278884939739e-06
Iter: 1371 loss: 1.3602243371075975e-06
Iter: 1372 loss: 1.3577689616286051e-06
Iter: 1373 loss: 1.357033098304097e-06
Iter: 1374 loss: 1.3565606184619838e-06
Iter: 1375 loss: 1.3562744326743437e-06
Iter: 1376 loss: 1.3552536452243992e-06
Iter: 1377 loss: 1.3586539186869189e-06
Iter: 1378 loss: 1.3549742700524684e-06
Iter: 1379 loss: 1.354704681533083e-06
Iter: 1380 loss: 1.3544904143043308e-06
Iter: 1381 loss: 1.3541398016308082e-06
Iter: 1382 loss: 1.353474541600037e-06
Iter: 1383 loss: 1.36784033392742e-06
Iter: 1384 loss: 1.3534721245156571e-06
Iter: 1385 loss: 1.352697948471244e-06
Iter: 1386 loss: 1.3538647151961329e-06
Iter: 1387 loss: 1.3523288521132314e-06
Iter: 1388 loss: 1.351456739089935e-06
Iter: 1389 loss: 1.3547017571818739e-06
Iter: 1390 loss: 1.3512438489559852e-06
Iter: 1391 loss: 1.3503356718312205e-06
Iter: 1392 loss: 1.3596034154583235e-06
Iter: 1393 loss: 1.3503088321791287e-06
Iter: 1394 loss: 1.3499073124188627e-06
Iter: 1395 loss: 1.349393923973762e-06
Iter: 1396 loss: 1.349356701348699e-06
Iter: 1397 loss: 1.3484739177208945e-06
Iter: 1398 loss: 1.3562178774885402e-06
Iter: 1399 loss: 1.3484287750204122e-06
Iter: 1400 loss: 1.3479383144898469e-06
Iter: 1401 loss: 1.3476241135438023e-06
Iter: 1402 loss: 1.3474329576677237e-06
Iter: 1403 loss: 1.3467259456538649e-06
Iter: 1404 loss: 1.34867015783298e-06
Iter: 1405 loss: 1.3464960772226769e-06
Iter: 1406 loss: 1.3456786672119676e-06
Iter: 1407 loss: 1.3498095814385294e-06
Iter: 1408 loss: 1.3455434438352429e-06
Iter: 1409 loss: 1.3449162788892241e-06
Iter: 1410 loss: 1.3446551332525516e-06
Iter: 1411 loss: 1.3443267058619545e-06
Iter: 1412 loss: 1.3434109160465056e-06
Iter: 1413 loss: 1.3444696237619629e-06
Iter: 1414 loss: 1.342921714506007e-06
Iter: 1415 loss: 1.3418264076939023e-06
Iter: 1416 loss: 1.3473194595370491e-06
Iter: 1417 loss: 1.3416431479594869e-06
Iter: 1418 loss: 1.3410697260993115e-06
Iter: 1419 loss: 1.3410320146622292e-06
Iter: 1420 loss: 1.3407076915853211e-06
Iter: 1421 loss: 1.3397563501905709e-06
Iter: 1422 loss: 1.3433024730151635e-06
Iter: 1423 loss: 1.3393452922652564e-06
Iter: 1424 loss: 1.3385637195733867e-06
Iter: 1425 loss: 1.3505889600651008e-06
Iter: 1426 loss: 1.3385635297482455e-06
Iter: 1427 loss: 1.3378724468198283e-06
Iter: 1428 loss: 1.3435125263124157e-06
Iter: 1429 loss: 1.3378288001810059e-06
Iter: 1430 loss: 1.3372368580216565e-06
Iter: 1431 loss: 1.3372169753629129e-06
Iter: 1432 loss: 1.3367572861640391e-06
Iter: 1433 loss: 1.3362580589295966e-06
Iter: 1434 loss: 1.3428805181465313e-06
Iter: 1435 loss: 1.3362553561554543e-06
Iter: 1436 loss: 1.3357276669987441e-06
Iter: 1437 loss: 1.334809762442938e-06
Iter: 1438 loss: 1.3348094154740679e-06
Iter: 1439 loss: 1.3339776916349257e-06
Iter: 1440 loss: 1.3365471582314869e-06
Iter: 1441 loss: 1.3337332237282691e-06
Iter: 1442 loss: 1.3331777097499961e-06
Iter: 1443 loss: 1.3331740386393577e-06
Iter: 1444 loss: 1.3327882486023403e-06
Iter: 1445 loss: 1.3323725244357472e-06
Iter: 1446 loss: 1.3323069939555129e-06
Iter: 1447 loss: 1.331559345863417e-06
Iter: 1448 loss: 1.3323026335132726e-06
Iter: 1449 loss: 1.3311379841546697e-06
Iter: 1450 loss: 1.3303801459434997e-06
Iter: 1451 loss: 1.3326637856112255e-06
Iter: 1452 loss: 1.3301523406637173e-06
Iter: 1453 loss: 1.329350362093903e-06
Iter: 1454 loss: 1.3371161288664304e-06
Iter: 1455 loss: 1.3293211318027732e-06
Iter: 1456 loss: 1.3286279634296454e-06
Iter: 1457 loss: 1.3291178731882413e-06
Iter: 1458 loss: 1.3281974247838183e-06
Iter: 1459 loss: 1.3275345068705123e-06
Iter: 1460 loss: 1.3262940710111676e-06
Iter: 1461 loss: 1.3539648653662647e-06
Iter: 1462 loss: 1.3262913218437864e-06
Iter: 1463 loss: 1.3262501819003638e-06
Iter: 1464 loss: 1.3258163542987327e-06
Iter: 1465 loss: 1.3252936136063869e-06
Iter: 1466 loss: 1.3249571559445317e-06
Iter: 1467 loss: 1.324754319599017e-06
Iter: 1468 loss: 1.3241125777766322e-06
Iter: 1469 loss: 1.3279524330831667e-06
Iter: 1470 loss: 1.3240319598597869e-06
Iter: 1471 loss: 1.3234440567183005e-06
Iter: 1472 loss: 1.3261241081887726e-06
Iter: 1473 loss: 1.3233316581186229e-06
Iter: 1474 loss: 1.3228499878833076e-06
Iter: 1475 loss: 1.3216989354727389e-06
Iter: 1476 loss: 1.3345574572248526e-06
Iter: 1477 loss: 1.3215855191849165e-06
Iter: 1478 loss: 1.320764172607013e-06
Iter: 1479 loss: 1.3207554520257103e-06
Iter: 1480 loss: 1.3199416056987036e-06
Iter: 1481 loss: 1.3215234258457176e-06
Iter: 1482 loss: 1.319603410567221e-06
Iter: 1483 loss: 1.3190897386188106e-06
Iter: 1484 loss: 1.3196832454844892e-06
Iter: 1485 loss: 1.3188152816191047e-06
Iter: 1486 loss: 1.3182286775380041e-06
Iter: 1487 loss: 1.3194440063129937e-06
Iter: 1488 loss: 1.3179944117792892e-06
Iter: 1489 loss: 1.3174203419279871e-06
Iter: 1490 loss: 1.3220986539053921e-06
Iter: 1491 loss: 1.3173839402183419e-06
Iter: 1492 loss: 1.3167419154289222e-06
Iter: 1493 loss: 1.3172701349672836e-06
Iter: 1494 loss: 1.3163586084628167e-06
Iter: 1495 loss: 1.315761509450033e-06
Iter: 1496 loss: 1.315546064581044e-06
Iter: 1497 loss: 1.3152125227102139e-06
Iter: 1498 loss: 1.3141140064025154e-06
Iter: 1499 loss: 1.3148298724442404e-06
Iter: 1500 loss: 1.3134186031502838e-06
Iter: 1501 loss: 1.3134370367156711e-06
Iter: 1502 loss: 1.3129077503515544e-06
Iter: 1503 loss: 1.3125771545179597e-06
Iter: 1504 loss: 1.3118360860588892e-06
Iter: 1505 loss: 1.32208205573696e-06
Iter: 1506 loss: 1.3117951300994571e-06
Iter: 1507 loss: 1.311393074763922e-06
Iter: 1508 loss: 1.311303315167698e-06
Iter: 1509 loss: 1.3110165769589311e-06
Iter: 1510 loss: 1.310603259524907e-06
Iter: 1511 loss: 1.3105904237290496e-06
Iter: 1512 loss: 1.3100282834999988e-06
Iter: 1513 loss: 1.3096521775697143e-06
Iter: 1514 loss: 1.3094421248137245e-06
Iter: 1515 loss: 1.3086685964704267e-06
Iter: 1516 loss: 1.3210149897222785e-06
Iter: 1517 loss: 1.308668595720183e-06
Iter: 1518 loss: 1.3080119846299065e-06
Iter: 1519 loss: 1.3083703004344634e-06
Iter: 1520 loss: 1.3075807477198297e-06
Iter: 1521 loss: 1.3069904012611311e-06
Iter: 1522 loss: 1.3060214849341641e-06
Iter: 1523 loss: 1.3060167615142591e-06
Iter: 1524 loss: 1.3056236062261054e-06
Iter: 1525 loss: 1.3054162686396504e-06
Iter: 1526 loss: 1.3049644994276211e-06
Iter: 1527 loss: 1.3069333351364316e-06
Iter: 1528 loss: 1.3048729279670263e-06
Iter: 1529 loss: 1.3044607116789628e-06
Iter: 1530 loss: 1.3038660211666529e-06
Iter: 1531 loss: 1.3038476932219082e-06
Iter: 1532 loss: 1.3030314230912956e-06
Iter: 1533 loss: 1.30405845790039e-06
Iter: 1534 loss: 1.3026097268127475e-06
Iter: 1535 loss: 1.3018096553495478e-06
Iter: 1536 loss: 1.3125040951717659e-06
Iter: 1537 loss: 1.3018056603808884e-06
Iter: 1538 loss: 1.3010772414588378e-06
Iter: 1539 loss: 1.3029970048312428e-06
Iter: 1540 loss: 1.3008321605848948e-06
Iter: 1541 loss: 1.3002620183145043e-06
Iter: 1542 loss: 1.3006279734553503e-06
Iter: 1543 loss: 1.2998998678402805e-06
Iter: 1544 loss: 1.299104757988098e-06
Iter: 1545 loss: 1.3047610790193542e-06
Iter: 1546 loss: 1.2990341526429636e-06
Iter: 1547 loss: 1.2986786317563611e-06
Iter: 1548 loss: 1.2981110170394459e-06
Iter: 1549 loss: 1.2981060985822282e-06
Iter: 1550 loss: 1.2974966358187023e-06
Iter: 1551 loss: 1.3002786585895323e-06
Iter: 1552 loss: 1.2973803227088472e-06
Iter: 1553 loss: 1.2965808252146034e-06
Iter: 1554 loss: 1.2988061702535821e-06
Iter: 1555 loss: 1.2963234821100552e-06
Iter: 1556 loss: 1.2956322501559784e-06
Iter: 1557 loss: 1.2972366877170446e-06
Iter: 1558 loss: 1.2953764724630552e-06
Iter: 1559 loss: 1.2948736257220043e-06
Iter: 1560 loss: 1.2939632557512626e-06
Iter: 1561 loss: 1.315834391545973e-06
Iter: 1562 loss: 1.2939630076402527e-06
Iter: 1563 loss: 1.2944261196339858e-06
Iter: 1564 loss: 1.293550436364863e-06
Iter: 1565 loss: 1.2932831253801535e-06
Iter: 1566 loss: 1.2926526335979834e-06
Iter: 1567 loss: 1.3000307544428744e-06
Iter: 1568 loss: 1.2925969992105719e-06
Iter: 1569 loss: 1.2918744530688369e-06
Iter: 1570 loss: 1.2949712812216069e-06
Iter: 1571 loss: 1.2917248905444562e-06
Iter: 1572 loss: 1.2910136822301248e-06
Iter: 1573 loss: 1.2924399337735918e-06
Iter: 1574 loss: 1.2907237374261819e-06
Iter: 1575 loss: 1.2898133228510591e-06
Iter: 1576 loss: 1.2971508051065921e-06
Iter: 1577 loss: 1.2897538340747822e-06
Iter: 1578 loss: 1.2892683547877913e-06
Iter: 1579 loss: 1.2894150639390474e-06
Iter: 1580 loss: 1.2889207541443291e-06
Iter: 1581 loss: 1.2883288145768308e-06
Iter: 1582 loss: 1.2934489650982137e-06
Iter: 1583 loss: 1.2882972198487795e-06
Iter: 1584 loss: 1.2878505275392082e-06
Iter: 1585 loss: 1.2873726856544588e-06
Iter: 1586 loss: 1.287295304662704e-06
Iter: 1587 loss: 1.2866509431104378e-06
Iter: 1588 loss: 1.2868596122113122e-06
Iter: 1589 loss: 1.2861931746334636e-06
Iter: 1590 loss: 1.2856997341395732e-06
Iter: 1591 loss: 1.2856764596245687e-06
Iter: 1592 loss: 1.2851633346898261e-06
Iter: 1593 loss: 1.2846780277444117e-06
Iter: 1594 loss: 1.2845602702773553e-06
Iter: 1595 loss: 1.2839242705104037e-06
Iter: 1596 loss: 1.2845348540657786e-06
Iter: 1597 loss: 1.2835617068761155e-06
Iter: 1598 loss: 1.282734356616491e-06
Iter: 1599 loss: 1.2880962317004967e-06
Iter: 1600 loss: 1.2826449410119051e-06
Iter: 1601 loss: 1.2819743061739775e-06
Iter: 1602 loss: 1.2885344275084045e-06
Iter: 1603 loss: 1.2819507882776548e-06
Iter: 1604 loss: 1.2815757964895402e-06
Iter: 1605 loss: 1.2805327261200832e-06
Iter: 1606 loss: 1.2862827764638533e-06
Iter: 1607 loss: 1.2802187317450442e-06
Iter: 1608 loss: 1.2797921334821118e-06
Iter: 1609 loss: 1.2796407929679073e-06
Iter: 1610 loss: 1.2791166543715351e-06
Iter: 1611 loss: 1.281271115121843e-06
Iter: 1612 loss: 1.2790024694361675e-06
Iter: 1613 loss: 1.2784324085164213e-06
Iter: 1614 loss: 1.2780413434024353e-06
Iter: 1615 loss: 1.2778337396816853e-06
Iter: 1616 loss: 1.2774028706471872e-06
Iter: 1617 loss: 1.2773865589033276e-06
Iter: 1618 loss: 1.2770182534249533e-06
Iter: 1619 loss: 1.2762391358480891e-06
Iter: 1620 loss: 1.2890032622224965e-06
Iter: 1621 loss: 1.2762165813018953e-06
Iter: 1622 loss: 1.2751561377266976e-06
Iter: 1623 loss: 1.277399930719478e-06
Iter: 1624 loss: 1.2747383574157263e-06
Iter: 1625 loss: 1.2740386554490227e-06
Iter: 1626 loss: 1.2774196716905971e-06
Iter: 1627 loss: 1.2739151422604331e-06
Iter: 1628 loss: 1.2732865118825389e-06
Iter: 1629 loss: 1.280262216291038e-06
Iter: 1630 loss: 1.273273996106856e-06
Iter: 1631 loss: 1.2729437145442611e-06
Iter: 1632 loss: 1.2722288330730682e-06
Iter: 1633 loss: 1.283204881736217e-06
Iter: 1634 loss: 1.2722015932565468e-06
Iter: 1635 loss: 1.2713962371582592e-06
Iter: 1636 loss: 1.2768104186296823e-06
Iter: 1637 loss: 1.271315427116615e-06
Iter: 1638 loss: 1.2706421460120134e-06
Iter: 1639 loss: 1.2788910544670776e-06
Iter: 1640 loss: 1.2706348698517125e-06
Iter: 1641 loss: 1.2702331654511921e-06
Iter: 1642 loss: 1.2697879939609992e-06
Iter: 1643 loss: 1.2697249391590658e-06
Iter: 1644 loss: 1.2690033029308972e-06
Iter: 1645 loss: 1.2687683549065912e-06
Iter: 1646 loss: 1.2683489877888995e-06
Iter: 1647 loss: 1.2683764065780133e-06
Iter: 1648 loss: 1.2679470346589344e-06
Iter: 1649 loss: 1.2676052498218145e-06
Iter: 1650 loss: 1.2669749040268483e-06
Iter: 1651 loss: 1.2815066452051973e-06
Iter: 1652 loss: 1.2669741942539647e-06
Iter: 1653 loss: 1.2664709889317357e-06
Iter: 1654 loss: 1.2737578892105165e-06
Iter: 1655 loss: 1.2664702043474724e-06
Iter: 1656 loss: 1.2660133693205227e-06
Iter: 1657 loss: 1.2658985835305294e-06
Iter: 1658 loss: 1.2656110556846728e-06
Iter: 1659 loss: 1.2648776184614219e-06
Iter: 1660 loss: 1.2644525084901289e-06
Iter: 1661 loss: 1.2641408306531181e-06
Iter: 1662 loss: 1.2632783320819593e-06
Iter: 1663 loss: 1.2676180764688796e-06
Iter: 1664 loss: 1.2631347071562075e-06
Iter: 1665 loss: 1.2623664069960869e-06
Iter: 1666 loss: 1.2674008191664937e-06
Iter: 1667 loss: 1.2622851784425955e-06
Iter: 1668 loss: 1.2615011233617782e-06
Iter: 1669 loss: 1.262447249095407e-06
Iter: 1670 loss: 1.2610891986335659e-06
Iter: 1671 loss: 1.2604340381824948e-06
Iter: 1672 loss: 1.2617775217950271e-06
Iter: 1673 loss: 1.2601706657265521e-06
Iter: 1674 loss: 1.2598038925531137e-06
Iter: 1675 loss: 1.2633408972802365e-06
Iter: 1676 loss: 1.2597903148455282e-06
Iter: 1677 loss: 1.2593117992849e-06
Iter: 1678 loss: 1.2589303847351721e-06
Iter: 1679 loss: 1.2587849033575209e-06
Iter: 1680 loss: 1.258276832541082e-06
Iter: 1681 loss: 1.2585384086513038e-06
Iter: 1682 loss: 1.2579395228374145e-06
Iter: 1683 loss: 1.2572290839150721e-06
Iter: 1684 loss: 1.26076302551551e-06
Iter: 1685 loss: 1.2571087873034036e-06
Iter: 1686 loss: 1.2563980940822469e-06
Iter: 1687 loss: 1.2625777842446852e-06
Iter: 1688 loss: 1.2563607552066237e-06
Iter: 1689 loss: 1.2560021056512983e-06
Iter: 1690 loss: 1.2554497984771763e-06
Iter: 1691 loss: 1.2554414519595039e-06
Iter: 1692 loss: 1.2548890512553339e-06
Iter: 1693 loss: 1.2548822685261988e-06
Iter: 1694 loss: 1.2545764105821319e-06
Iter: 1695 loss: 1.253920210764627e-06
Iter: 1696 loss: 1.2642513603437057e-06
Iter: 1697 loss: 1.2538976356500825e-06
Iter: 1698 loss: 1.2531859619728852e-06
Iter: 1699 loss: 1.2570455919437748e-06
Iter: 1700 loss: 1.25308035148318e-06
Iter: 1701 loss: 1.2524346154044815e-06
Iter: 1702 loss: 1.2536481246738336e-06
Iter: 1703 loss: 1.2521608602287065e-06
Iter: 1704 loss: 1.2513674387160357e-06
Iter: 1705 loss: 1.2569809306608124e-06
Iter: 1706 loss: 1.2512961106850587e-06
Iter: 1707 loss: 1.2506836736802948e-06
Iter: 1708 loss: 1.2499949490665864e-06
Iter: 1709 loss: 1.2499029280752255e-06
Iter: 1710 loss: 1.2490714627241768e-06
Iter: 1711 loss: 1.2563483443714658e-06
Iter: 1712 loss: 1.2490286415794544e-06
Iter: 1713 loss: 1.248563701587473e-06
Iter: 1714 loss: 1.255176985223502e-06
Iter: 1715 loss: 1.2485626956339877e-06
Iter: 1716 loss: 1.2481501501146671e-06
Iter: 1717 loss: 1.2472422301283746e-06
Iter: 1718 loss: 1.2605177815172382e-06
Iter: 1719 loss: 1.2472006933475175e-06
Iter: 1720 loss: 1.2465810876684396e-06
Iter: 1721 loss: 1.2496600158793256e-06
Iter: 1722 loss: 1.2464760191239736e-06
Iter: 1723 loss: 1.2458780495468746e-06
Iter: 1724 loss: 1.2530659960615486e-06
Iter: 1725 loss: 1.2458706605680577e-06
Iter: 1726 loss: 1.2453747485263199e-06
Iter: 1727 loss: 1.2450842094809029e-06
Iter: 1728 loss: 1.2448752790865858e-06
Iter: 1729 loss: 1.2443898752459226e-06
Iter: 1730 loss: 1.2470117941264035e-06
Iter: 1731 loss: 1.244317373649271e-06
Iter: 1732 loss: 1.2436835337437249e-06
Iter: 1733 loss: 1.2438254011241504e-06
Iter: 1734 loss: 1.2432166887962016e-06
Iter: 1735 loss: 1.2427646088235487e-06
Iter: 1736 loss: 1.2424200407822676e-06
Iter: 1737 loss: 1.2422742237756549e-06
Iter: 1738 loss: 1.2417059278734503e-06
Iter: 1739 loss: 1.2480061654200762e-06
Iter: 1740 loss: 1.2416945549664651e-06
Iter: 1741 loss: 1.2412004774403593e-06
Iter: 1742 loss: 1.2436595346025581e-06
Iter: 1743 loss: 1.2411168842686493e-06
Iter: 1744 loss: 1.2406285639241896e-06
Iter: 1745 loss: 1.2405572482458068e-06
Iter: 1746 loss: 1.2402156599736359e-06
Iter: 1747 loss: 1.2394154094836571e-06
Iter: 1748 loss: 1.2391979009846224e-06
Iter: 1749 loss: 1.2387050104587321e-06
Iter: 1750 loss: 1.2387580921649528e-06
Iter: 1751 loss: 1.2383206150670092e-06
Iter: 1752 loss: 1.2380797098726515e-06
Iter: 1753 loss: 1.2377357935676584e-06
Iter: 1754 loss: 1.2377241665773925e-06
Iter: 1755 loss: 1.2371735185400537e-06
Iter: 1756 loss: 1.2366803276092622e-06
Iter: 1757 loss: 1.2365405705940898e-06
Iter: 1758 loss: 1.2363502902053335e-06
Iter: 1759 loss: 1.2361799088903679e-06
Iter: 1760 loss: 1.235780207291409e-06
Iter: 1761 loss: 1.235002305376741e-06
Iter: 1762 loss: 1.2508134138937229e-06
Iter: 1763 loss: 1.2349967146483228e-06
Iter: 1764 loss: 1.2343642330896453e-06
Iter: 1765 loss: 1.2393581457713413e-06
Iter: 1766 loss: 1.2343205750393277e-06
Iter: 1767 loss: 1.2337400575723106e-06
Iter: 1768 loss: 1.2362740024406853e-06
Iter: 1769 loss: 1.233622631177768e-06
Iter: 1770 loss: 1.2331223649814197e-06
Iter: 1771 loss: 1.2325066189429439e-06
Iter: 1772 loss: 1.2324518949324391e-06
Iter: 1773 loss: 1.2317669189572047e-06
Iter: 1774 loss: 1.2332537585048747e-06
Iter: 1775 loss: 1.231501539349007e-06
Iter: 1776 loss: 1.2310977608073197e-06
Iter: 1777 loss: 1.2310892134635061e-06
Iter: 1778 loss: 1.2306578811538668e-06
Iter: 1779 loss: 1.2305421333585585e-06
Iter: 1780 loss: 1.2302754855127974e-06
Iter: 1781 loss: 1.2297099774468704e-06
Iter: 1782 loss: 1.2301918949407742e-06
Iter: 1783 loss: 1.2293756978367115e-06
Iter: 1784 loss: 1.228734672652857e-06
Iter: 1785 loss: 1.2344566887912139e-06
Iter: 1786 loss: 1.2287036535399515e-06
Iter: 1787 loss: 1.2281473154446787e-06
Iter: 1788 loss: 1.2302315206645165e-06
Iter: 1789 loss: 1.2280125089310071e-06
Iter: 1790 loss: 1.2275752788936229e-06
Iter: 1791 loss: 1.2267147234759942e-06
Iter: 1792 loss: 1.2437281377860037e-06
Iter: 1793 loss: 1.2267068191927986e-06
Iter: 1794 loss: 1.2259302537015482e-06
Iter: 1795 loss: 1.2259296089009426e-06
Iter: 1796 loss: 1.2255040536116871e-06
Iter: 1797 loss: 1.2319293831538017e-06
Iter: 1798 loss: 1.2255038273632433e-06
Iter: 1799 loss: 1.2252694089906952e-06
Iter: 1800 loss: 1.2246195632062708e-06
Iter: 1801 loss: 1.2282787605363131e-06
Iter: 1802 loss: 1.2244284331985545e-06
Iter: 1803 loss: 1.2240256610521023e-06
Iter: 1804 loss: 1.2239296083089863e-06
Iter: 1805 loss: 1.2234991917060396e-06
Iter: 1806 loss: 1.2229615347709751e-06
Iter: 1807 loss: 1.2229172641820288e-06
Iter: 1808 loss: 1.2222955900173007e-06
Iter: 1809 loss: 1.2223691658656016e-06
Iter: 1810 loss: 1.221819667161833e-06
Iter: 1811 loss: 1.221001380487806e-06
Iter: 1812 loss: 1.22653939698451e-06
Iter: 1813 loss: 1.2209204130076985e-06
Iter: 1814 loss: 1.2204424500842838e-06
Iter: 1815 loss: 1.2273310228161728e-06
Iter: 1816 loss: 1.2204416318889233e-06
Iter: 1817 loss: 1.2200061286964042e-06
Iter: 1818 loss: 1.2194434691173477e-06
Iter: 1819 loss: 1.2194050422481277e-06
Iter: 1820 loss: 1.218841696132489e-06
Iter: 1821 loss: 1.2208104426622497e-06
Iter: 1822 loss: 1.2186947658930002e-06
Iter: 1823 loss: 1.2179168860669748e-06
Iter: 1824 loss: 1.2211392477437911e-06
Iter: 1825 loss: 1.2177489777520366e-06
Iter: 1826 loss: 1.2172583942479912e-06
Iter: 1827 loss: 1.2175228598150198e-06
Iter: 1828 loss: 1.2169354537897417e-06
Iter: 1829 loss: 1.2163131052941022e-06
Iter: 1830 loss: 1.2169059579571257e-06
Iter: 1831 loss: 1.2159574320162512e-06
Iter: 1832 loss: 1.215529576747237e-06
Iter: 1833 loss: 1.2154719747927994e-06
Iter: 1834 loss: 1.2152075937282776e-06
Iter: 1835 loss: 1.2146473493388639e-06
Iter: 1836 loss: 1.2237809885248355e-06
Iter: 1837 loss: 1.2146307716653153e-06
Iter: 1838 loss: 1.2139581097366981e-06
Iter: 1839 loss: 1.2174747499093788e-06
Iter: 1840 loss: 1.21385238723373e-06
Iter: 1841 loss: 1.2133518162485028e-06
Iter: 1842 loss: 1.2190149685352524e-06
Iter: 1843 loss: 1.2133428399590505e-06
Iter: 1844 loss: 1.2130328754975611e-06
Iter: 1845 loss: 1.212192623070437e-06
Iter: 1846 loss: 1.2175818802147761e-06
Iter: 1847 loss: 1.211980702012064e-06
Iter: 1848 loss: 1.210967341708475e-06
Iter: 1849 loss: 1.2172584868437066e-06
Iter: 1850 loss: 1.2108483645306505e-06
Iter: 1851 loss: 1.210287948429123e-06
Iter: 1852 loss: 1.2186829829288942e-06
Iter: 1853 loss: 1.2102875641116488e-06
Iter: 1854 loss: 1.2097098716262156e-06
Iter: 1855 loss: 1.2100264197420666e-06
Iter: 1856 loss: 1.2093307682796537e-06
Iter: 1857 loss: 1.2086523271566455e-06
Iter: 1858 loss: 1.2095867368324288e-06
Iter: 1859 loss: 1.208315099896297e-06
Iter: 1860 loss: 1.2080587818570987e-06
Iter: 1861 loss: 1.208017374858693e-06
Iter: 1862 loss: 1.2077208732519649e-06
Iter: 1863 loss: 1.2069324573554726e-06
Iter: 1864 loss: 1.2125322304133663e-06
Iter: 1865 loss: 1.2067585751449261e-06
Iter: 1866 loss: 1.2060563813416442e-06
Iter: 1867 loss: 1.2134888933588654e-06
Iter: 1868 loss: 1.2060387315103195e-06
Iter: 1869 loss: 1.2055474671074089e-06
Iter: 1870 loss: 1.211978706774978e-06
Iter: 1871 loss: 1.2055444267896911e-06
Iter: 1872 loss: 1.2051641410132944e-06
Iter: 1873 loss: 1.2049351437978875e-06
Iter: 1874 loss: 1.2047785298173955e-06
Iter: 1875 loss: 1.2042858299118829e-06
Iter: 1876 loss: 1.2042267951408247e-06
Iter: 1877 loss: 1.2038733929536245e-06
Iter: 1878 loss: 1.2032355484533307e-06
Iter: 1879 loss: 1.203235408951295e-06
Iter: 1880 loss: 1.2029110733439841e-06
Iter: 1881 loss: 1.202924740992357e-06
Iter: 1882 loss: 1.2026556578973449e-06
Iter: 1883 loss: 1.2021978567008342e-06
Iter: 1884 loss: 1.201477076735792e-06
Iter: 1885 loss: 1.2014691752372596e-06
Iter: 1886 loss: 1.2005573922214998e-06
Iter: 1887 loss: 1.2052493772763066e-06
Iter: 1888 loss: 1.2004105660153198e-06
Iter: 1889 loss: 1.1999188644529813e-06
Iter: 1890 loss: 1.1998849508574372e-06
Iter: 1891 loss: 1.1995016025695452e-06
Iter: 1892 loss: 1.1989402632815348e-06
Iter: 1893 loss: 1.1989252199350471e-06
Iter: 1894 loss: 1.1983192708756214e-06
Iter: 1895 loss: 1.2033976139748745e-06
Iter: 1896 loss: 1.1982837556731967e-06
Iter: 1897 loss: 1.1978104675971309e-06
Iter: 1898 loss: 1.2013835106278047e-06
Iter: 1899 loss: 1.1977738912987966e-06
Iter: 1900 loss: 1.1974457242181989e-06
Iter: 1901 loss: 1.1966255808991667e-06
Iter: 1902 loss: 1.2043464195268964e-06
Iter: 1903 loss: 1.1965103779227678e-06
Iter: 1904 loss: 1.1966325365989188e-06
Iter: 1905 loss: 1.1962025495727445e-06
Iter: 1906 loss: 1.1958704834761878e-06
Iter: 1907 loss: 1.1951061645480832e-06
Iter: 1908 loss: 1.20481938262502e-06
Iter: 1909 loss: 1.1950518209064429e-06
Iter: 1910 loss: 1.1943683697041079e-06
Iter: 1911 loss: 1.1987995792646962e-06
Iter: 1912 loss: 1.1942945670099002e-06
Iter: 1913 loss: 1.1937665295827768e-06
Iter: 1914 loss: 1.1976667618561055e-06
Iter: 1915 loss: 1.1937235108490851e-06
Iter: 1916 loss: 1.193209799344282e-06
Iter: 1917 loss: 1.19329771295398e-06
Iter: 1918 loss: 1.1928240762662995e-06
Iter: 1919 loss: 1.1923031846365174e-06
Iter: 1920 loss: 1.1927490259791261e-06
Iter: 1921 loss: 1.1919956648381877e-06
Iter: 1922 loss: 1.1914197386076707e-06
Iter: 1923 loss: 1.1921499841533003e-06
Iter: 1924 loss: 1.1911231402484574e-06
Iter: 1925 loss: 1.1904148664303347e-06
Iter: 1926 loss: 1.195240983087862e-06
Iter: 1927 loss: 1.1903457814131422e-06
Iter: 1928 loss: 1.1898861286302246e-06
Iter: 1929 loss: 1.1960137289898773e-06
Iter: 1930 loss: 1.1898837666318311e-06
Iter: 1931 loss: 1.1896218471001963e-06
Iter: 1932 loss: 1.1889825658110592e-06
Iter: 1933 loss: 1.1955883115903685e-06
Iter: 1934 loss: 1.1889078140756412e-06
Iter: 1935 loss: 1.1884366855786983e-06
Iter: 1936 loss: 1.1883680281333429e-06
Iter: 1937 loss: 1.1880183432484153e-06
Iter: 1938 loss: 1.1874994701451044e-06
Iter: 1939 loss: 1.1874873114992449e-06
Iter: 1940 loss: 1.1869350546207417e-06
Iter: 1941 loss: 1.1899313314440582e-06
Iter: 1942 loss: 1.1868531687157189e-06
Iter: 1943 loss: 1.1864567618869963e-06
Iter: 1944 loss: 1.1864567340883849e-06
Iter: 1945 loss: 1.1862097792100019e-06
Iter: 1946 loss: 1.185527879479011e-06
Iter: 1947 loss: 1.1894605252346532e-06
Iter: 1948 loss: 1.1853326577886064e-06
Iter: 1949 loss: 1.1848833661727328e-06
Iter: 1950 loss: 1.1848637393474116e-06
Iter: 1951 loss: 1.1843400692677073e-06
Iter: 1952 loss: 1.184517368260329e-06
Iter: 1953 loss: 1.1839700063632228e-06
Iter: 1954 loss: 1.1833320697736417e-06
Iter: 1955 loss: 1.1832711281269343e-06
Iter: 1956 loss: 1.182802950742218e-06
Iter: 1957 loss: 1.1820902607723936e-06
Iter: 1958 loss: 1.1866331945255805e-06
Iter: 1959 loss: 1.1820107005615703e-06
Iter: 1960 loss: 1.1815259898109893e-06
Iter: 1961 loss: 1.1818484702007861e-06
Iter: 1962 loss: 1.1812205974181889e-06
Iter: 1963 loss: 1.180634224358581e-06
Iter: 1964 loss: 1.1884415917392996e-06
Iter: 1965 loss: 1.180631171036072e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.2/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.6
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi1.6
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.6 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi1.6
+ date
Sun Nov  8 08:00:53 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.6/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.2/300_300_300_1 --function f1 --psi 3 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.6/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1b4ce18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1c651e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1b4cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1b4c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1b9b048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1b9bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1a6ee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1aa79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1b0a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1b26510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1ab2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1988e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1999950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1a1f9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be196b9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1965d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be1946400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb6c37ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be18f2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be18f27b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be19e12f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7be18f21e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb6b90950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb6bebae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb6beba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b90261400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb6bc26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb6bce620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb6bce400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb6bce7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b901b19d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb6b4e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb6b4e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7bb6b569d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b9017e9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7f7b90132158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
train_loss: 0.005240280193660522
test_loss: 0.006002177198017314
train_loss: 0.004730565065727006
test_loss: 0.00547929224403258
train_loss: 0.0047725590053913835
test_loss: 0.005925994612842894
train_loss: 0.004387903953073539
test_loss: 0.0052655623817589176
train_loss: 0.004880631545444665
test_loss: 0.005416067781283566
train_loss: 0.004856946138484514
test_loss: 0.0055163764954653935
train_loss: 0.004367490601836979
test_loss: 0.005209746084901955
train_loss: 0.004441930174154823
test_loss: 0.00526551276420582
train_loss: 0.004188564927533903
test_loss: 0.004958129786136033
train_loss: 0.004176788545311928
test_loss: 0.004969899561250648
train_loss: 0.0045191876465149935
test_loss: 0.005621540059369382
train_loss: 0.0044996401614171575
test_loss: 0.005086171379454197
train_loss: 0.004315579483098779
test_loss: 0.005203372171300552
train_loss: 0.004338254295582995
test_loss: 0.005044596476178077
train_loss: 0.004541388202272076
test_loss: 0.005165498961061155
train_loss: 0.004152683974212122
test_loss: 0.005061661821560572
train_loss: 0.004181158070992937
test_loss: 0.00516688225908656
train_loss: 0.004240953799487519
test_loss: 0.005156329443235321
train_loss: 0.004213732090830228
test_loss: 0.005005127678354014
train_loss: 0.004126804252413269
test_loss: 0.005135287532077762
train_loss: 0.004701835751758277
test_loss: 0.0050360768244720805
train_loss: 0.003964214876521435
test_loss: 0.004852559902564315
train_loss: 0.0040099975427372175
test_loss: 0.004856160754464732
train_loss: 0.004215804174602657
test_loss: 0.0049117704123209415
train_loss: 0.00400350900361309
test_loss: 0.004894499074080684
train_loss: 0.004003444924747749
test_loss: 0.004849258014973321
train_loss: 0.004160869169780521
test_loss: 0.004913322735146656
train_loss: 0.004124060223154257
test_loss: 0.005096573066142248
train_loss: 0.0039255780454889875
test_loss: 0.004892107477380515
train_loss: 0.0040283288045882566
test_loss: 0.004834192333307492
train_loss: 0.004351617520933709
test_loss: 0.00511849162310629
train_loss: 0.004161739986522218
test_loss: 0.004987226087987685
train_loss: 0.003927673592263933
test_loss: 0.004905670855188007
train_loss: 0.004054649124624304
test_loss: 0.004790071576182516
train_loss: 0.0039292163663870805
test_loss: 0.004727531936907206
train_loss: 0.004160982014169076
test_loss: 0.004847807424614993
train_loss: 0.0042080657618659605
test_loss: 0.0049863339783632965
train_loss: 0.003869860007762694
test_loss: 0.0046176148958716285
train_loss: 0.004023444875338708
test_loss: 0.005054826491585865
train_loss: 0.0038585073597834623
test_loss: 0.004889756867702475
train_loss: 0.004130326459006262
test_loss: 0.0048798950580751605
train_loss: 0.003947047231175779
test_loss: 0.004762840707721293
train_loss: 0.003937965928233037
test_loss: 0.004844030391469959
train_loss: 0.004085385027504829
test_loss: 0.005175095750060964
train_loss: 0.0038585153183426325
test_loss: 0.004889809210087745
train_loss: 0.004147700356769182
test_loss: 0.004776526094976658
train_loss: 0.003538448722712399
test_loss: 0.004636959766703315
train_loss: 0.004048661635195055
test_loss: 0.004824031986225129
train_loss: 0.0036151762862514716
test_loss: 0.004748515355170324
train_loss: 0.003783048495998394
test_loss: 0.00474803621960839
train_loss: 0.003909909410644428
test_loss: 0.00466456227000316
train_loss: 0.003872667822736593
test_loss: 0.0047068702626700395
train_loss: 0.0037227084869514092
test_loss: 0.004877235116529023
train_loss: 0.0039085461008602305
test_loss: 0.004915405101881891
train_loss: 0.003962392916287209
test_loss: 0.0049050190840753335
train_loss: 0.0039269126784213446
test_loss: 0.00507585701152765
train_loss: 0.003724157486657326
test_loss: 0.004702146343700635
train_loss: 0.003974862228740738
test_loss: 0.004920131450456201
train_loss: 0.0036490676008780107
test_loss: 0.004739862198020546
train_loss: 0.0037361563471618817
test_loss: 0.004822300753394581
train_loss: 0.003980871229791579
test_loss: 0.004908034781886963
train_loss: 0.0040844003484298095
test_loss: 0.0047201194543959974
train_loss: 0.003777292972138043
test_loss: 0.004750690345040666
train_loss: 0.0035767931285417796
test_loss: 0.004956373641600064
train_loss: 0.003936532602950351
test_loss: 0.00457102808789122
train_loss: 0.003923856965008579
test_loss: 0.00460872810867939
train_loss: 0.0036206185366189337
test_loss: 0.004495638204660259
train_loss: 0.003785806610741358
test_loss: 0.004714763075548863
train_loss: 0.00381949182476676
test_loss: 0.004673240239861133
train_loss: 0.0038123305396814983
test_loss: 0.004740731053642354
train_loss: 0.0036654809409358046
test_loss: 0.0049118032927031085
train_loss: 0.0037638181116898713
test_loss: 0.004769848434380295
train_loss: 0.0038339111870169234
test_loss: 0.0046597995344993415
train_loss: 0.0036238106189625426
test_loss: 0.0046224495609539056
train_loss: 0.003698764992726737
test_loss: 0.004636785986712547
train_loss: 0.0036324743360871144
test_loss: 0.0047304139493836335
train_loss: 0.003968576320467916
test_loss: 0.004919447636598314
train_loss: 0.003698372590076584
test_loss: 0.00463643353843379
train_loss: 0.004030058415474829
test_loss: 0.0046239535290483105
train_loss: 0.0038109674029903435
test_loss: 0.0046211385037165115
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi1.6/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --n_pairs 8000 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.6/300_300_300_1 --optimizer lbfgs --function f1 --psi 3 --phi 1.6 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi1.6/ --save_name 300_300_300_1 --max_epochs 2000 --loss_func weighted_MSE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda8057d378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda80537ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda80537d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda805e0048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda804c8b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda805e0158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda804949d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda8042e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda8042e598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda803f7378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda803d1158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda803a97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda803a9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda8039dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda8039d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda8039d400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda803136a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda80313d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda802c2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda802c2268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda80296488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda80296268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda801ef730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda801fa730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda801fa1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda801d9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda80163730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda80137268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda80137158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda801376a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda801631e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda800d97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda800c0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda800d3268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda80072598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fda8002fbf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Iter: 1 loss: 2.20563189927145e-05
Iter: 2 loss: 2.0586149648143635e-05
Iter: 3 loss: 1.8855384864457813e-05
Iter: 4 loss: 1.7424134793066033e-05
Iter: 5 loss: 2.3651668340318267e-05
Iter: 6 loss: 1.7135650427834772e-05
Iter: 7 loss: 1.6182420528389643e-05
Iter: 8 loss: 1.753775215978467e-05
Iter: 9 loss: 1.5714969587986643e-05
Iter: 10 loss: 1.461399266652462e-05
Iter: 11 loss: 1.6820106117226136e-05
Iter: 12 loss: 1.4164200800690856e-05
Iter: 13 loss: 1.3530427517008327e-05
Iter: 14 loss: 1.409472837108027e-05
Iter: 15 loss: 1.3160711668660383e-05
Iter: 16 loss: 1.266237650800775e-05
Iter: 17 loss: 1.2650787541368134e-05
Iter: 18 loss: 1.2327787361421046e-05
Iter: 19 loss: 1.1596768807454872e-05
Iter: 20 loss: 2.1429655319518811e-05
Iter: 21 loss: 1.1552479506991893e-05
Iter: 22 loss: 1.0685716278634866e-05
Iter: 23 loss: 1.6850493724138561e-05
Iter: 24 loss: 1.0608867720344854e-05
Iter: 25 loss: 1.0055457331060325e-05
Iter: 26 loss: 1.4560880450209274e-05
Iter: 27 loss: 1.0020360707084338e-05
Iter: 28 loss: 9.5620446946563157e-06
Iter: 29 loss: 9.3632594391415448e-06
Iter: 30 loss: 9.1281000358976814e-06
Iter: 31 loss: 8.66208234067634e-06
Iter: 32 loss: 8.9340078135040547e-06
Iter: 33 loss: 8.360177090081397e-06
Iter: 34 loss: 7.8627975297840926e-06
Iter: 35 loss: 1.2581924191775899e-05
Iter: 36 loss: 7.8432027138302267e-06
Iter: 37 loss: 7.5265131215841287e-06
Iter: 38 loss: 8.414321065715275e-06
Iter: 39 loss: 7.4253314142732144e-06
Iter: 40 loss: 7.1261622671442464e-06
Iter: 41 loss: 7.5592114271937618e-06
Iter: 42 loss: 6.9807153524667487e-06
Iter: 43 loss: 6.7787942308501736e-06
Iter: 44 loss: 6.7365761124656214e-06
Iter: 45 loss: 6.6464359299448329e-06
Iter: 46 loss: 6.43723336553694e-06
Iter: 47 loss: 9.0202581324486183e-06
Iter: 48 loss: 6.4211874289861955e-06
Iter: 49 loss: 6.1546388367441982e-06
Iter: 50 loss: 6.825752056210229e-06
Iter: 51 loss: 6.0616273076768867e-06
Iter: 52 loss: 5.9004958587839541e-06
Iter: 53 loss: 5.8949126723153913e-06
Iter: 54 loss: 5.7623889075257142e-06
Iter: 55 loss: 5.5742131092970561e-06
Iter: 56 loss: 5.5675618186518012e-06
Iter: 57 loss: 5.4253031761377047e-06
Iter: 58 loss: 5.9981622422089186e-06
Iter: 59 loss: 5.3934978440782916e-06
Iter: 60 loss: 5.2226035172406909e-06
Iter: 61 loss: 5.9788620983800671e-06
Iter: 62 loss: 5.1886274973748975e-06
Iter: 63 loss: 5.0615776912015206e-06
Iter: 64 loss: 5.5064223464560865e-06
Iter: 65 loss: 5.0285453112666739e-06
Iter: 66 loss: 4.9262576203598625e-06
Iter: 67 loss: 4.9650631710320623e-06
Iter: 68 loss: 4.8550155859309563e-06
Iter: 69 loss: 4.6943220309738674e-06
Iter: 70 loss: 5.6605956830578678e-06
Iter: 71 loss: 4.6742743479613888e-06
Iter: 72 loss: 4.6179076202873487e-06
Iter: 73 loss: 4.5528053579084933e-06
Iter: 74 loss: 4.545026992971973e-06
Iter: 75 loss: 4.4327034676150509e-06
Iter: 76 loss: 4.9620140913246284e-06
Iter: 77 loss: 4.4121691389720291e-06
Iter: 78 loss: 4.3421802293986136e-06
Iter: 79 loss: 4.825841343255971e-06
Iter: 80 loss: 4.3355504864564794e-06
Iter: 81 loss: 4.2521528248569285e-06
Iter: 82 loss: 4.5985827428762359e-06
Iter: 83 loss: 4.2342097086620843e-06
Iter: 84 loss: 4.1894590132865239e-06
Iter: 85 loss: 4.0767704597090625e-06
Iter: 86 loss: 5.1045019235054782e-06
Iter: 87 loss: 4.0600072130828168e-06
Iter: 88 loss: 3.9854423072092487e-06
Iter: 89 loss: 3.9813122728376461e-06
Iter: 90 loss: 3.9366422561216387e-06
Iter: 91 loss: 4.37892757571464e-06
Iter: 92 loss: 3.9351469277298437e-06
Iter: 93 loss: 3.8970820166934738e-06
Iter: 94 loss: 3.8414543387031819e-06
Iter: 95 loss: 3.8399336345566752e-06
Iter: 96 loss: 3.7871721705720437e-06
Iter: 97 loss: 3.8410043908076232e-06
Iter: 98 loss: 3.7577007842808506e-06
Iter: 99 loss: 3.7036609176753286e-06
Iter: 100 loss: 3.7030510208645652e-06
Iter: 101 loss: 3.6774056824444634e-06
Iter: 102 loss: 3.65567843201465e-06
Iter: 103 loss: 3.6485429547449134e-06
Iter: 104 loss: 3.5942872163195413e-06
Iter: 105 loss: 3.6428101683427372e-06
Iter: 106 loss: 3.5626724506026233e-06
Iter: 107 loss: 3.514263103142294e-06
Iter: 108 loss: 3.5142339217439747e-06
Iter: 109 loss: 3.4881622333146393e-06
Iter: 110 loss: 3.4398486624328103e-06
Iter: 111 loss: 4.5422162122951311e-06
Iter: 112 loss: 3.4397790578115421e-06
Iter: 113 loss: 3.379360429625902e-06
Iter: 114 loss: 3.5757050771685689e-06
Iter: 115 loss: 3.3624354868907064e-06
Iter: 116 loss: 3.3455236238256697e-06
Iter: 117 loss: 3.3295644931388356e-06
Iter: 118 loss: 3.3175693023483021e-06
Iter: 119 loss: 3.2925987190574862e-06
Iter: 120 loss: 3.7199369231830302e-06
Iter: 121 loss: 3.2920137620595309e-06
Iter: 122 loss: 3.2593938941857274e-06
Iter: 123 loss: 3.2648554063684569e-06
Iter: 124 loss: 3.2348715540883956e-06
Iter: 125 loss: 3.2060307132553256e-06
Iter: 126 loss: 3.4694007718363821e-06
Iter: 127 loss: 3.2047356487162262e-06
Iter: 128 loss: 3.1720238223433817e-06
Iter: 129 loss: 3.2716224676239639e-06
Iter: 130 loss: 3.1622807148167425e-06
Iter: 131 loss: 3.1416923635376765e-06
Iter: 132 loss: 3.1326394366241911e-06
Iter: 133 loss: 3.1221570106320404e-06
Iter: 134 loss: 3.0853445242084206e-06
Iter: 135 loss: 3.129806032840155e-06
Iter: 136 loss: 3.0660120663877618e-06
Iter: 137 loss: 3.0430520147087721e-06
Iter: 138 loss: 3.0410586852471802e-06
Iter: 139 loss: 3.0237417760658884e-06
Iter: 140 loss: 2.99786143315927e-06
Iter: 141 loss: 2.9972998595132508e-06
Iter: 142 loss: 2.9752012771184874e-06
Iter: 143 loss: 3.1577758448524393e-06
Iter: 144 loss: 2.9738519215968605e-06
Iter: 145 loss: 2.948903227512132e-06
Iter: 146 loss: 2.9739642777621041e-06
Iter: 147 loss: 2.93488904391165e-06
Iter: 148 loss: 2.9167809835377777e-06
Iter: 149 loss: 3.0140269722277967e-06
Iter: 150 loss: 2.9140533543870133e-06
Iter: 151 loss: 2.900983887796014e-06
Iter: 152 loss: 2.998486919733865e-06
Iter: 153 loss: 2.8999453786452181e-06
Iter: 154 loss: 2.8822849172782678e-06
Iter: 155 loss: 2.8541045048755527e-06
Iter: 156 loss: 2.8538578872936546e-06
Iter: 157 loss: 2.8356704165044624e-06
Iter: 158 loss: 2.8290295457600717e-06
Iter: 159 loss: 2.8189194575924021e-06
Iter: 160 loss: 2.7960862886363412e-06
Iter: 161 loss: 2.9598747750803952e-06
Iter: 162 loss: 2.7940943244114253e-06
Iter: 163 loss: 2.773940789472596e-06
Iter: 164 loss: 2.9667157564223576e-06
Iter: 165 loss: 2.7731722532442178e-06
Iter: 166 loss: 2.7584872660548175e-06
Iter: 167 loss: 2.7906350678470171e-06
Iter: 168 loss: 2.7528322623395557e-06
Iter: 169 loss: 2.741597693113234e-06
Iter: 170 loss: 2.7186225155923328e-06
Iter: 171 loss: 3.1310385657021452e-06
Iter: 172 loss: 2.7182071288588593e-06
Iter: 173 loss: 2.703693096965164e-06
Iter: 174 loss: 2.7022308972139452e-06
Iter: 175 loss: 2.6871367128630015e-06
Iter: 176 loss: 2.7009708502267507e-06
Iter: 177 loss: 2.6784055304174975e-06
Iter: 178 loss: 2.6609742779530063e-06
Iter: 179 loss: 2.6938918301235939e-06
Iter: 180 loss: 2.6536055851479264e-06
Iter: 181 loss: 2.6423116938081419e-06
Iter: 182 loss: 2.6959809646881169e-06
Iter: 183 loss: 2.640271358791568e-06
Iter: 184 loss: 2.6267551304995094e-06
Iter: 185 loss: 2.6555597929525935e-06
Iter: 186 loss: 2.6214548011558778e-06
Iter: 187 loss: 2.6125215742366522e-06
Iter: 188 loss: 2.6770761004377596e-06
Iter: 189 loss: 2.6117555751681276e-06
Iter: 190 loss: 2.6008624856604427e-06
Iter: 191 loss: 2.6080997092587323e-06
Iter: 192 loss: 2.5939963368632726e-06
Iter: 193 loss: 2.5863205112908441e-06
Iter: 194 loss: 2.5783297765395549e-06
Iter: 195 loss: 2.5769043405241456e-06
Iter: 196 loss: 2.5614779386231166e-06
Iter: 197 loss: 2.5826264035381579e-06
Iter: 198 loss: 2.5537941710653238e-06
Iter: 199 loss: 2.537702790397754e-06
Iter: 200 loss: 2.5826820328790583e-06
Iter: 201 loss: 2.532541665035634e-06
Iter: 202 loss: 2.519568992114461e-06
Iter: 203 loss: 2.519335196981241e-06
Iter: 204 loss: 2.511030904459216e-06
Iter: 205 loss: 2.4986520931479971e-06
Iter: 206 loss: 2.4983757286674041e-06
Iter: 207 loss: 2.4847142113132126e-06
Iter: 208 loss: 2.530394484956779e-06
Iter: 209 loss: 2.4809895372204608e-06
Iter: 210 loss: 2.4676408815947445e-06
Iter: 211 loss: 2.5045384998449953e-06
Iter: 212 loss: 2.46331922227788e-06
Iter: 213 loss: 2.4537913105634304e-06
Iter: 214 loss: 2.4536391910358253e-06
Iter: 215 loss: 2.4488994108431859e-06
Iter: 216 loss: 2.4348693845361512e-06
Iter: 217 loss: 2.483014115659908e-06
Iter: 218 loss: 2.4284188687638409e-06
Iter: 219 loss: 2.4166076269476539e-06
Iter: 220 loss: 2.4147347330832054e-06
Iter: 221 loss: 2.4076942599045353e-06
Iter: 222 loss: 2.4721501837014272e-06
Iter: 223 loss: 2.4073808323847706e-06
Iter: 224 loss: 2.40190015303973e-06
Iter: 225 loss: 2.4082163808254194e-06
Iter: 226 loss: 2.3989689488637371e-06
Iter: 227 loss: 2.3913516070872461e-06
Iter: 228 loss: 2.3887850482889304e-06
Iter: 229 loss: 2.3844141960594852e-06
Iter: 230 loss: 2.3755794517968143e-06
Iter: 231 loss: 2.3663559178755017e-06
Iter: 232 loss: 2.3647265020645644e-06
Iter: 233 loss: 2.3540809261247193e-06
Iter: 234 loss: 2.453700237841796e-06
Iter: 235 loss: 2.3536418623229676e-06
Iter: 236 loss: 2.3449851225166067e-06
Iter: 237 loss: 2.3894140642126321e-06
Iter: 238 loss: 2.3435851430876746e-06
Iter: 239 loss: 2.3375006219807605e-06
Iter: 240 loss: 2.4205779677859555e-06
Iter: 241 loss: 2.3374768802522969e-06
Iter: 242 loss: 2.3328956031483624e-06
Iter: 243 loss: 2.3207809781913355e-06
Iter: 244 loss: 2.40920386549929e-06
Iter: 245 loss: 2.3182105732812169e-06
Iter: 246 loss: 2.3069453564594557e-06
Iter: 247 loss: 2.3977056401358896e-06
Iter: 248 loss: 2.306207901229248e-06
Iter: 249 loss: 2.2966055683792692e-06
Iter: 250 loss: 2.3524979374871379e-06
Iter: 251 loss: 2.2953391163637014e-06
Iter: 252 loss: 2.2881074618021906e-06
Iter: 253 loss: 2.3569327631232524e-06
Iter: 254 loss: 2.2878265104230995e-06
Iter: 255 loss: 2.2830492967938063e-06
Iter: 256 loss: 2.2738080787287685e-06
Iter: 257 loss: 2.4644382255972689e-06
Iter: 258 loss: 2.2737506171754892e-06
Iter: 259 loss: 2.2766936783658868e-06
Iter: 260 loss: 2.2708056339553802e-06
Iter: 261 loss: 2.2678974491536703e-06
Iter: 262 loss: 2.2606502287578765e-06
Iter: 263 loss: 2.3296675574006041e-06
Iter: 264 loss: 2.2596541551456841e-06
Iter: 265 loss: 2.2516810604958576e-06
Iter: 266 loss: 2.3379450650563988e-06
Iter: 267 loss: 2.2515006048304303e-06
Iter: 268 loss: 2.2460840301902998e-06
Iter: 269 loss: 2.2387979539859939e-06
Iter: 270 loss: 2.2384112313609366e-06
Iter: 271 loss: 2.2282705114232223e-06
Iter: 272 loss: 2.2797458642757515e-06
Iter: 273 loss: 2.2266041012650853e-06
Iter: 274 loss: 2.2207447165410116e-06
Iter: 275 loss: 2.2178981677203471e-06
Iter: 276 loss: 2.2150801830045875e-06
Iter: 277 loss: 2.2105976886400178e-06
Iter: 278 loss: 2.20937054540452e-06
Iter: 279 loss: 2.2054458314730027e-06
Iter: 280 loss: 2.2030812352224037e-06
Iter: 281 loss: 2.2014656945569194e-06
Iter: 282 loss: 2.1965495948045324e-06
Iter: 283 loss: 2.2005615992958473e-06
Iter: 284 loss: 2.1936076304623237e-06
Iter: 285 loss: 2.18682391340717e-06
Iter: 286 loss: 2.1934774157951134e-06
Iter: 287 loss: 2.1829837600375067e-06
Iter: 288 loss: 2.1787123783595331e-06
Iter: 289 loss: 2.1785684196233651e-06
Iter: 290 loss: 2.1737847592171392e-06
Iter: 291 loss: 2.16880330965284e-06
Iter: 292 loss: 2.1679154896929396e-06
Iter: 293 loss: 2.1626486709739494e-06
Iter: 294 loss: 2.2271327481138531e-06
Iter: 295 loss: 2.1625915483541128e-06
Iter: 296 loss: 2.1578688002506621e-06
Iter: 297 loss: 2.1796385045657572e-06
Iter: 298 loss: 2.1569793347370712e-06
Iter: 299 loss: 2.1536815724564066e-06
Iter: 300 loss: 2.1486814359212533e-06
Iter: 301 loss: 2.1485894199859149e-06
Iter: 302 loss: 2.1446817075015839e-06
Iter: 303 loss: 2.1903714890680604e-06
Iter: 304 loss: 2.1446239276516956e-06
Iter: 305 loss: 2.1405075685207207e-06
Iter: 306 loss: 2.1377594298087769e-06
Iter: 307 loss: 2.1362180180217703e-06
Iter: 308 loss: 2.1306437794039234e-06
Iter: 309 loss: 2.1285023560438218e-06
Iter: 310 loss: 2.12547087261559e-06
Iter: 311 loss: 2.1207428868923672e-06
Iter: 312 loss: 2.1205830730340952e-06
Iter: 313 loss: 2.11633434642263e-06
Iter: 314 loss: 2.1297988357431575e-06
Iter: 315 loss: 2.1151147262678756e-06
Iter: 316 loss: 2.1097232937754265e-06
Iter: 317 loss: 2.1029464916375706e-06
Iter: 318 loss: 2.1024066736910759e-06
Iter: 319 loss: 2.0977262586833578e-06
Iter: 320 loss: 2.1024548863030809e-06
Iter: 321 loss: 2.0951027687852376e-06
Iter: 322 loss: 2.0888815741212624e-06
Iter: 323 loss: 2.1418641328169553e-06
Iter: 324 loss: 2.08853354199628e-06
Iter: 325 loss: 2.0857546136787648e-06
Iter: 326 loss: 2.1278396224902226e-06
Iter: 327 loss: 2.0857532832299886e-06
Iter: 328 loss: 2.0830201790517332e-06
Iter: 329 loss: 2.0785540476926571e-06
Iter: 330 loss: 2.0785299217196334e-06
Iter: 331 loss: 2.0757783739098081e-06
Iter: 332 loss: 2.0751303850365929e-06
Iter: 333 loss: 2.07363168179053e-06
Iter: 334 loss: 2.0693824964605116e-06
Iter: 335 loss: 2.0900337950541595e-06
Iter: 336 loss: 2.0679257318650226e-06
Iter: 337 loss: 2.0625730932296824e-06
Iter: 338 loss: 2.1014458957218957e-06
Iter: 339 loss: 2.062119575394285e-06
Iter: 340 loss: 2.0576926850386762e-06
Iter: 341 loss: 2.07714435676125e-06
Iter: 342 loss: 2.0568046625012442e-06
Iter: 343 loss: 2.0521783077792938e-06
Iter: 344 loss: 2.0599492994950448e-06
Iter: 345 loss: 2.0500902125614343e-06
Iter: 346 loss: 2.0460543446951889e-06
Iter: 347 loss: 2.0424054494424234e-06
Iter: 348 loss: 2.0413979802477706e-06
Iter: 349 loss: 2.0396660682756312e-06
Iter: 350 loss: 2.0384255081977361e-06
Iter: 351 loss: 2.0356132884436513e-06
Iter: 352 loss: 2.0323670376771596e-06
Iter: 353 loss: 2.031978092392003e-06
Iter: 354 loss: 2.0269297377002131e-06
Iter: 355 loss: 2.0344841037676268e-06
Iter: 356 loss: 2.0245147424947818e-06
Iter: 357 loss: 2.0199622477450253e-06
Iter: 358 loss: 2.0213828734159718e-06
Iter: 359 loss: 2.0167142392585591e-06
Iter: 360 loss: 2.0128570581393523e-06
Iter: 361 loss: 2.0128464602274872e-06
Iter: 362 loss: 2.00959191933231e-06
Iter: 363 loss: 2.0326054766515477e-06
Iter: 364 loss: 2.0092989056631261e-06
Iter: 365 loss: 2.0071992039381133e-06
Iter: 366 loss: 2.0145001617795066e-06
Iter: 367 loss: 2.0066487619714567e-06
Iter: 368 loss: 2.0040928344600609e-06
Iter: 369 loss: 2.0018771099759642e-06
Iter: 370 loss: 2.0011917171997121e-06
Iter: 371 loss: 1.9972754362393204e-06
Iter: 372 loss: 2.0010080461380288e-06
Iter: 373 loss: 1.9950377331343e-06
Iter: 374 loss: 1.9914846596327165e-06
Iter: 375 loss: 1.9923801910935006e-06
Iter: 376 loss: 1.9888942445498548e-06
Iter: 377 loss: 1.985673411044374e-06
Iter: 378 loss: 1.9855922073853109e-06
Iter: 379 loss: 1.9824384991673143e-06
Iter: 380 loss: 1.97889563575637e-06
Iter: 381 loss: 1.9784202654891015e-06
Iter: 382 loss: 1.9752021733791259e-06
Iter: 383 loss: 2.0059524104181686e-06
Iter: 384 loss: 1.9750788652586003e-06
Iter: 385 loss: 1.9723111203809271e-06
Iter: 386 loss: 1.9817701011014323e-06
Iter: 387 loss: 1.9715727046046231e-06
Iter: 388 loss: 1.9679070674731595e-06
Iter: 389 loss: 1.9682003740499516e-06
Iter: 390 loss: 1.9650607649421209e-06
Iter: 391 loss: 1.9626366053305408e-06
Iter: 392 loss: 1.9617333712261445e-06
Iter: 393 loss: 1.9603971755949821e-06
Iter: 394 loss: 1.9553041053204238e-06
Iter: 395 loss: 1.9635540615640848e-06
Iter: 396 loss: 1.9529612443344749e-06
Iter: 397 loss: 1.9509315190752512e-06
Iter: 398 loss: 1.950529555162626e-06
Iter: 399 loss: 1.947992699883475e-06
Iter: 400 loss: 1.9492106623901246e-06
Iter: 401 loss: 1.9462876134437078e-06
Iter: 402 loss: 1.9444612593565614e-06
Iter: 403 loss: 1.9571958464804393e-06
Iter: 404 loss: 1.9442917312850828e-06
Iter: 405 loss: 1.9423985578484716e-06
Iter: 406 loss: 1.9385047409433791e-06
Iter: 407 loss: 2.0073246955388469e-06
Iter: 408 loss: 1.9384279574439533e-06
Iter: 409 loss: 1.9351387235123396e-06
Iter: 410 loss: 1.9504169979896438e-06
Iter: 411 loss: 1.9345254547225878e-06
Iter: 412 loss: 1.9312881153576391e-06
Iter: 413 loss: 1.9392177841837389e-06
Iter: 414 loss: 1.930135994919222e-06
Iter: 415 loss: 1.9275251372044625e-06
Iter: 416 loss: 1.940941892648877e-06
Iter: 417 loss: 1.9271038252156706e-06
Iter: 418 loss: 1.9233108115019579e-06
Iter: 419 loss: 1.9212314393177794e-06
Iter: 420 loss: 1.9195495227681846e-06
Iter: 421 loss: 1.9173603923304775e-06
Iter: 422 loss: 1.9469925083743162e-06
Iter: 423 loss: 1.9173509167392403e-06
Iter: 424 loss: 1.9148711640371065e-06
Iter: 425 loss: 1.9143158849495591e-06
Iter: 426 loss: 1.9127104341783215e-06
Iter: 427 loss: 1.9091846064311632e-06
Iter: 428 loss: 1.9145938817372892e-06
Iter: 429 loss: 1.9075180287742124e-06
Iter: 430 loss: 1.9043463206479421e-06
Iter: 431 loss: 1.9047114440242514e-06
Iter: 432 loss: 1.9019155082794593e-06
Iter: 433 loss: 1.90004897284615e-06
Iter: 434 loss: 1.8998397678224632e-06
Iter: 435 loss: 1.8976008902373944e-06
Iter: 436 loss: 1.9022509329019726e-06
Iter: 437 loss: 1.896708135484711e-06
Iter: 438 loss: 1.8949780113879847e-06
Iter: 439 loss: 1.8947223949210454e-06
Iter: 440 loss: 1.8935141701515093e-06
Iter: 441 loss: 1.8909033159122543e-06
Iter: 442 loss: 1.9039716166677735e-06
Iter: 443 loss: 1.8904652653193562e-06
Iter: 444 loss: 1.8879955798761617e-06
Iter: 445 loss: 1.8870936979796624e-06
Iter: 446 loss: 1.8857208876569366e-06
Iter: 447 loss: 1.8827645317426541e-06
Iter: 448 loss: 1.8833274258027898e-06
Iter: 449 loss: 1.880560481257826e-06
Iter: 450 loss: 1.8773139801076034e-06
Iter: 451 loss: 1.8993872328135568e-06
Iter: 452 loss: 1.8769957870958287e-06
Iter: 453 loss: 1.8744893763418907e-06
Iter: 454 loss: 1.8990805631829499e-06
Iter: 455 loss: 1.8744024480008053e-06
Iter: 456 loss: 1.8725800049407266e-06
Iter: 457 loss: 1.8780234124386117e-06
Iter: 458 loss: 1.8720278755686069e-06
Iter: 459 loss: 1.8701731453760149e-06
Iter: 460 loss: 1.8699471816854782e-06
Iter: 461 loss: 1.8686193590082181e-06
Iter: 462 loss: 1.8657241310041712e-06
Iter: 463 loss: 1.8890625226834129e-06
Iter: 464 loss: 1.8655350080006489e-06
Iter: 465 loss: 1.8638691891223374e-06
Iter: 466 loss: 1.8599618432104296e-06
Iter: 467 loss: 1.9065670391972971e-06
Iter: 468 loss: 1.8596331349013088e-06
Iter: 469 loss: 1.8562354698146339e-06
Iter: 470 loss: 1.9095776401910624e-06
Iter: 471 loss: 1.8562352740801354e-06
Iter: 472 loss: 1.8545209971929281e-06
Iter: 473 loss: 1.8545028203200415e-06
Iter: 474 loss: 1.852772845851267e-06
Iter: 475 loss: 1.8491135585264103e-06
Iter: 476 loss: 1.9090683343094463e-06
Iter: 477 loss: 1.849007726568312e-06
Iter: 478 loss: 1.8465551782160252e-06
Iter: 479 loss: 1.8566314438406408e-06
Iter: 480 loss: 1.8460205433233812e-06
Iter: 481 loss: 1.8433301414664995e-06
Iter: 482 loss: 1.8621506364984709e-06
Iter: 483 loss: 1.8430821747302772e-06
Iter: 484 loss: 1.84155330434253e-06
Iter: 485 loss: 1.8387704962500956e-06
Iter: 486 loss: 1.9048310651264666e-06
Iter: 487 loss: 1.8387692184490545e-06
Iter: 488 loss: 1.8357206747705034e-06
Iter: 489 loss: 1.8608387811845155e-06
Iter: 490 loss: 1.835533085694795e-06
Iter: 491 loss: 1.8330351310473821e-06
Iter: 492 loss: 1.8289192696634041e-06
Iter: 493 loss: 1.8289010736683765e-06
Iter: 494 loss: 1.8297623409356157e-06
Iter: 495 loss: 1.8267528110459594e-06
Iter: 496 loss: 1.8256550803567604e-06
Iter: 497 loss: 1.8242959597373895e-06
Iter: 498 loss: 1.8241787489492362e-06
Iter: 499 loss: 1.8216033993129922e-06
Iter: 500 loss: 1.8285312704191903e-06
Iter: 501 loss: 1.8207509514136173e-06
Iter: 502 loss: 1.8182833475823282e-06
Iter: 503 loss: 1.8291072193332761e-06
Iter: 504 loss: 1.817787297550857e-06
Iter: 505 loss: 1.8162871565258561e-06
Iter: 506 loss: 1.8133762964135921e-06
Iter: 507 loss: 1.8729865258807709e-06
Iter: 508 loss: 1.8133568035845656e-06
Iter: 509 loss: 1.8136526238282698e-06
Iter: 510 loss: 1.8119275992879053e-06
Iter: 511 loss: 1.8105588624837432e-06
Iter: 512 loss: 1.8080466332832118e-06
Iter: 513 loss: 1.8665945129588416e-06
Iter: 514 loss: 1.8080445055954657e-06
Iter: 515 loss: 1.8053703113258162e-06
Iter: 516 loss: 1.8098818830344296e-06
Iter: 517 loss: 1.8041661620839154e-06
Iter: 518 loss: 1.8018040228573566e-06
Iter: 519 loss: 1.8110801743599549e-06
Iter: 520 loss: 1.801261075906902e-06
Iter: 521 loss: 1.7986757057127754e-06
Iter: 522 loss: 1.8151868259834393e-06
Iter: 523 loss: 1.7983881402461704e-06
Iter: 524 loss: 1.7968339972441014e-06
Iter: 525 loss: 1.7937325496157753e-06
Iter: 526 loss: 1.8529869543146217e-06
Iter: 527 loss: 1.7936953000446185e-06
Iter: 528 loss: 1.7908785198828355e-06
Iter: 529 loss: 1.8072241895811747e-06
Iter: 530 loss: 1.7905050254408514e-06
Iter: 531 loss: 1.7880887100201016e-06
Iter: 532 loss: 1.8022617398533894e-06
Iter: 533 loss: 1.7877741961860087e-06
Iter: 534 loss: 1.7855495330024633e-06
Iter: 535 loss: 1.8001230801896145e-06
Iter: 536 loss: 1.7853142114278678e-06
Iter: 537 loss: 1.7832024662687479e-06
Iter: 538 loss: 1.7838752201448163e-06
Iter: 539 loss: 1.7816994385366056e-06
Iter: 540 loss: 1.779947991846788e-06
Iter: 541 loss: 1.7938405029321111e-06
Iter: 542 loss: 1.7798285564404325e-06
Iter: 543 loss: 1.7779688092492373e-06
Iter: 544 loss: 1.7762318820161892e-06
Iter: 545 loss: 1.7757945840881143e-06
Iter: 546 loss: 1.7740164540173386e-06
Iter: 547 loss: 1.7919859621263704e-06
Iter: 548 loss: 1.773961724923506e-06
Iter: 549 loss: 1.7720334677231721e-06
Iter: 550 loss: 1.7789703310617075e-06
Iter: 551 loss: 1.7715454972734333e-06
Iter: 552 loss: 1.7701504442635014e-06
Iter: 553 loss: 1.7686740823985679e-06
Iter: 554 loss: 1.7684254947040292e-06
Iter: 555 loss: 1.7661984324993608e-06
Iter: 556 loss: 1.7662253650581542e-06
Iter: 557 loss: 1.7644248752307931e-06
Iter: 558 loss: 1.7630966467944411e-06
Iter: 559 loss: 1.7629020936171852e-06
Iter: 560 loss: 1.7612682011765112e-06
Iter: 561 loss: 1.7587503751392044e-06
Iter: 562 loss: 1.7587126486030831e-06
Iter: 563 loss: 1.7566265793815936e-06
Iter: 564 loss: 1.7778780157140665e-06
Iter: 565 loss: 1.7565644875174127e-06
Iter: 566 loss: 1.7551002365342717e-06
Iter: 567 loss: 1.7517309190415973e-06
Iter: 568 loss: 1.7945851857526439e-06
Iter: 569 loss: 1.75149196458492e-06
Iter: 570 loss: 1.750868719776557e-06
Iter: 571 loss: 1.7498459023597764e-06
Iter: 572 loss: 1.7481825676283716e-06
Iter: 573 loss: 1.7501239838020075e-06
Iter: 574 loss: 1.7472972551487386e-06
Iter: 575 loss: 1.7455525556309809e-06
Iter: 576 loss: 1.7476115158907144e-06
Iter: 577 loss: 1.744627900334549e-06
Iter: 578 loss: 1.7428208251006098e-06
Iter: 579 loss: 1.752057795992444e-06
Iter: 580 loss: 1.7425269031936636e-06
Iter: 581 loss: 1.7407340495022726e-06
Iter: 582 loss: 1.7452526639185947e-06
Iter: 583 loss: 1.7401095845248035e-06
Iter: 584 loss: 1.7391019288066128e-06
Iter: 585 loss: 1.7390919722538516e-06
Iter: 586 loss: 1.7381961345845469e-06
Iter: 587 loss: 1.7357616957495518e-06
Iter: 588 loss: 1.7511636920442239e-06
Iter: 589 loss: 1.7351371526906764e-06
Iter: 590 loss: 1.7328637909254332e-06
Iter: 591 loss: 1.7410891161678578e-06
Iter: 592 loss: 1.7322919461850133e-06
Iter: 593 loss: 1.7298531229409521e-06
Iter: 594 loss: 1.740064587396169e-06
Iter: 595 loss: 1.7293334538448646e-06
Iter: 596 loss: 1.7282043660956541e-06
Iter: 597 loss: 1.728170436294519e-06
Iter: 598 loss: 1.7271047373669225e-06
Iter: 599 loss: 1.7245112041197475e-06
Iter: 600 loss: 1.7516057752702496e-06
Iter: 601 loss: 1.7242148763520761e-06
Iter: 602 loss: 1.72201534065707e-06
Iter: 603 loss: 1.730633419975506e-06
Iter: 604 loss: 1.7215084863379204e-06
Iter: 605 loss: 1.719228514356342e-06
Iter: 606 loss: 1.7271409656076155e-06
Iter: 607 loss: 1.7186295483522725e-06
Iter: 608 loss: 1.7164520887681912e-06
Iter: 609 loss: 1.7430608603618112e-06
Iter: 610 loss: 1.7164281157073492e-06
Iter: 611 loss: 1.7149439553610308e-06
Iter: 612 loss: 1.7173776016696259e-06
Iter: 613 loss: 1.7142656086224964e-06
Iter: 614 loss: 1.7129683407123942e-06
Iter: 615 loss: 1.7115012332417112e-06
Iter: 616 loss: 1.7113096979902998e-06
Iter: 617 loss: 1.7093365961366193e-06
Iter: 618 loss: 1.7093187220783207e-06
Iter: 619 loss: 1.7081110294803036e-06
Iter: 620 loss: 1.7119723756338686e-06
Iter: 621 loss: 1.7077672336895687e-06
Iter: 622 loss: 1.7063532048770604e-06
Iter: 623 loss: 1.7056554763840599e-06
Iter: 624 loss: 1.7049819588021923e-06
Iter: 625 loss: 1.7035326477828018e-06
Iter: 626 loss: 1.7051417751748369e-06
Iter: 627 loss: 1.7027466842621272e-06
Iter: 628 loss: 1.700986154854377e-06
Iter: 629 loss: 1.702502471720008e-06
Iter: 630 loss: 1.6999486534638549e-06
Iter: 631 loss: 1.6980800483236489e-06
Iter: 632 loss: 1.7075447692920767e-06
Iter: 633 loss: 1.6977719736522856e-06
Iter: 634 loss: 1.6958428789962965e-06
Iter: 635 loss: 1.7113282171244802e-06
Iter: 636 loss: 1.6957154465408196e-06
Iter: 637 loss: 1.6948105930378476e-06
Iter: 638 loss: 1.6935627414958578e-06
Iter: 639 loss: 1.693507161890432e-06
Iter: 640 loss: 1.6914075244644724e-06
Iter: 641 loss: 1.695811282865289e-06
Iter: 642 loss: 1.6905755683187339e-06
Iter: 643 loss: 1.6889395806588108e-06
Iter: 644 loss: 1.6922021429261336e-06
Iter: 645 loss: 1.6882703769892452e-06
Iter: 646 loss: 1.6860751331501461e-06
Iter: 647 loss: 1.7074301691913143e-06
Iter: 648 loss: 1.6859965220738019e-06
Iter: 649 loss: 1.6850939622655784e-06
Iter: 650 loss: 1.6832243599747439e-06
Iter: 651 loss: 1.715654838538101e-06
Iter: 652 loss: 1.6831835002337662e-06
Iter: 653 loss: 1.681037944839833e-06
Iter: 654 loss: 1.7132466112396775e-06
Iter: 655 loss: 1.6810365709261e-06
Iter: 656 loss: 1.6792507477559692e-06
Iter: 657 loss: 1.6849760450234647e-06
Iter: 658 loss: 1.6787436018862351e-06
Iter: 659 loss: 1.6776909062907122e-06
Iter: 660 loss: 1.6803115121325662e-06
Iter: 661 loss: 1.6773207751608805e-06
Iter: 662 loss: 1.6761723697523895e-06
Iter: 663 loss: 1.6761379831370857e-06
Iter: 664 loss: 1.6752432509443291e-06
Iter: 665 loss: 1.6738972748283227e-06
Iter: 666 loss: 1.6719866268473402e-06
Iter: 667 loss: 1.6719188426708395e-06
Iter: 668 loss: 1.6699099804867755e-06
Iter: 669 loss: 1.6699024505235819e-06
Iter: 670 loss: 1.668701955085022e-06
Iter: 671 loss: 1.67346129670563e-06
Iter: 672 loss: 1.6684290370397527e-06
Iter: 673 loss: 1.6666812945618397e-06
Iter: 674 loss: 1.6653143560314045e-06
Iter: 675 loss: 1.6647692048564873e-06
Iter: 676 loss: 1.6626515188670579e-06
Iter: 677 loss: 1.6635770078163189e-06
Iter: 678 loss: 1.6612064684400595e-06
Iter: 679 loss: 1.6591814941980102e-06
Iter: 680 loss: 1.6800188346359432e-06
Iter: 681 loss: 1.6591237323400166e-06
Iter: 682 loss: 1.6575781105511642e-06
Iter: 683 loss: 1.6666506259553702e-06
Iter: 684 loss: 1.657377235833133e-06
Iter: 685 loss: 1.6557293107697436e-06
Iter: 686 loss: 1.6590592464126165e-06
Iter: 687 loss: 1.6550607193423755e-06
Iter: 688 loss: 1.653939747416923e-06
Iter: 689 loss: 1.6541948176860136e-06
Iter: 690 loss: 1.6531151790629271e-06
Iter: 691 loss: 1.6515100708841227e-06
Iter: 692 loss: 1.6730232355202195e-06
Iter: 693 loss: 1.6515022997474475e-06
Iter: 694 loss: 1.6508401678251063e-06
Iter: 695 loss: 1.6496788667367627e-06
Iter: 696 loss: 1.6496786963919322e-06
Iter: 697 loss: 1.6480446797680057e-06
Iter: 698 loss: 1.6536253981221696e-06
Iter: 699 loss: 1.6476084613737535e-06
Iter: 700 loss: 1.6457932520026322e-06
Iter: 701 loss: 1.648455334997039e-06
Iter: 702 loss: 1.6449165432824439e-06
Iter: 703 loss: 1.6433148310780597e-06
Iter: 704 loss: 1.6452692000755791e-06
Iter: 705 loss: 1.642476991565082e-06
Iter: 706 loss: 1.6408459278242328e-06
Iter: 707 loss: 1.6424933551649189e-06
Iter: 708 loss: 1.63993151051213e-06
Iter: 709 loss: 1.6387800436273266e-06
Iter: 710 loss: 1.6386043707475636e-06
Iter: 711 loss: 1.6379357576150256e-06
Iter: 712 loss: 1.636430536092241e-06
Iter: 713 loss: 1.6569661680937649e-06
Iter: 714 loss: 1.636343750562133e-06
Iter: 715 loss: 1.6343663649057844e-06
Iter: 716 loss: 1.6440072626650966e-06
Iter: 717 loss: 1.6340217788370172e-06
Iter: 718 loss: 1.6325916297853447e-06
Iter: 719 loss: 1.6329574844761081e-06
Iter: 720 loss: 1.6315503669360039e-06
Iter: 721 loss: 1.6294351872527655e-06
Iter: 722 loss: 1.6580402886679211e-06
Iter: 723 loss: 1.6294259272306942e-06
Iter: 724 loss: 1.6286026669528558e-06
Iter: 725 loss: 1.6278803923059209e-06
Iter: 726 loss: 1.6276639707440736e-06
Iter: 727 loss: 1.6262089945847708e-06
Iter: 728 loss: 1.6442498975125311e-06
Iter: 729 loss: 1.6261946123360242e-06
Iter: 730 loss: 1.6253859120467696e-06
Iter: 731 loss: 1.6238548657615282e-06
Iter: 732 loss: 1.6570894299844779e-06
Iter: 733 loss: 1.6238496862030182e-06
Iter: 734 loss: 1.6219620484015557e-06
Iter: 735 loss: 1.6271215163079949e-06
Iter: 736 loss: 1.6213452891462423e-06
Iter: 737 loss: 1.6202853013749702e-06
Iter: 738 loss: 1.6202736712629248e-06
Iter: 739 loss: 1.6194454129578385e-06
Iter: 740 loss: 1.6173713965650171e-06
Iter: 741 loss: 1.6367389432408122e-06
Iter: 742 loss: 1.6170756926298641e-06
Iter: 743 loss: 1.615361528104349e-06
Iter: 744 loss: 1.6414522903918004e-06
Iter: 745 loss: 1.6153608504918639e-06
Iter: 746 loss: 1.6140331218970794e-06
Iter: 747 loss: 1.6172462406785939e-06
Iter: 748 loss: 1.613556387400846e-06
Iter: 749 loss: 1.6118677880453233e-06
Iter: 750 loss: 1.6196353168417114e-06
Iter: 751 loss: 1.6115488358576083e-06
Iter: 752 loss: 1.6104227096992759e-06
Iter: 753 loss: 1.6085383934773109e-06
Iter: 754 loss: 1.6085330396273762e-06
Iter: 755 loss: 1.6070138151472121e-06
Iter: 756 loss: 1.6194478026504208e-06
Iter: 757 loss: 1.6069186184763339e-06
Iter: 758 loss: 1.6055222403243669e-06
Iter: 759 loss: 1.6173328827799816e-06
Iter: 760 loss: 1.6054425536041052e-06
Iter: 761 loss: 1.6041876137533033e-06
Iter: 762 loss: 1.6053695035241786e-06
Iter: 763 loss: 1.6034678155102169e-06
Iter: 764 loss: 1.6022679850218582e-06
Iter: 765 loss: 1.6120421612149028e-06
Iter: 766 loss: 1.602191821179684e-06
Iter: 767 loss: 1.6008508455089691e-06
Iter: 768 loss: 1.5991939428141959e-06
Iter: 769 loss: 1.5990495322018092e-06
Iter: 770 loss: 1.5978031139115505e-06
Iter: 771 loss: 1.5964751876332983e-06
Iter: 772 loss: 1.5962569505892332e-06
Iter: 773 loss: 1.5944166147218612e-06
Iter: 774 loss: 1.594416511192549e-06
Iter: 775 loss: 1.593140842317664e-06
Iter: 776 loss: 1.5995406341696661e-06
Iter: 777 loss: 1.5929275325668319e-06
Iter: 778 loss: 1.5920895744306642e-06
Iter: 779 loss: 1.5919410227117494e-06
Iter: 780 loss: 1.5913724916960082e-06
Iter: 781 loss: 1.5898754437854799e-06
Iter: 782 loss: 1.5890481673364227e-06
Iter: 783 loss: 1.5883882035884088e-06
Iter: 784 loss: 1.5878696386251178e-06
Iter: 785 loss: 1.5873297614931512e-06
Iter: 786 loss: 1.5864962149762862e-06
Iter: 787 loss: 1.5842680633039788e-06
Iter: 788 loss: 1.599671662456093e-06
Iter: 789 loss: 1.5837582058649035e-06
Iter: 790 loss: 1.5817454764270967e-06
Iter: 791 loss: 1.5817453882272449e-06
Iter: 792 loss: 1.5806166270466618e-06
Iter: 793 loss: 1.5824462259252979e-06
Iter: 794 loss: 1.580097618306462e-06
Iter: 795 loss: 1.5788566566062856e-06
Iter: 796 loss: 1.596378015167036e-06
Iter: 797 loss: 1.578853628034211e-06
Iter: 798 loss: 1.5781788363161477e-06
Iter: 799 loss: 1.5784501973845628e-06
Iter: 800 loss: 1.5777126633747092e-06
Iter: 801 loss: 1.5764329361935124e-06
Iter: 802 loss: 1.5774705055722895e-06
Iter: 803 loss: 1.5756658094577304e-06
Iter: 804 loss: 1.5747435385947595e-06
Iter: 805 loss: 1.5748080476370658e-06
Iter: 806 loss: 1.5740247155504186e-06
Iter: 807 loss: 1.5726446113778952e-06
Iter: 808 loss: 1.5734305398153713e-06
Iter: 809 loss: 1.5717456577815112e-06
Iter: 810 loss: 1.5700731064190995e-06
Iter: 811 loss: 1.5740792032095521e-06
Iter: 812 loss: 1.5694680470103734e-06
Iter: 813 loss: 1.5674693529713683e-06
Iter: 814 loss: 1.5864000762029908e-06
Iter: 815 loss: 1.5673903208382069e-06
Iter: 816 loss: 1.5664966515536444e-06
Iter: 817 loss: 1.5657031884771411e-06
Iter: 818 loss: 1.5654729344506931e-06
Iter: 819 loss: 1.5638617655764126e-06
Iter: 820 loss: 1.5709002354185153e-06
Iter: 821 loss: 1.563536191771146e-06
Iter: 822 loss: 1.5626285396342023e-06
Iter: 823 loss: 1.5626221720090529e-06
Iter: 824 loss: 1.5619159526854047e-06
Iter: 825 loss: 1.5602225185966556e-06
Iter: 826 loss: 1.5789091659686787e-06
Iter: 827 loss: 1.560050890101604e-06
Iter: 828 loss: 1.5583066984968576e-06
Iter: 829 loss: 1.5625217301565763e-06
Iter: 830 loss: 1.5576798412520526e-06
Iter: 831 loss: 1.5567078479700446e-06
Iter: 832 loss: 1.5564550742348849e-06
Iter: 833 loss: 1.5556289575460215e-06
Iter: 834 loss: 1.5557686962164579e-06
Iter: 835 loss: 1.5550082202607452e-06
Iter: 836 loss: 1.5538997275471596e-06
Iter: 837 loss: 1.5585045906642046e-06
Iter: 838 loss: 1.5536612834303091e-06
Iter: 839 loss: 1.5525700954186124e-06
Iter: 840 loss: 1.5520297390406619e-06
Iter: 841 loss: 1.5515111624466688e-06
Iter: 842 loss: 1.5504563153429262e-06
Iter: 843 loss: 1.5492828642610412e-06
Iter: 844 loss: 1.5491191193800066e-06
Iter: 845 loss: 1.5474453207093074e-06
Iter: 846 loss: 1.5654427461497412e-06
Iter: 847 loss: 1.5474062516667637e-06
Iter: 848 loss: 1.5463021105979366e-06
Iter: 849 loss: 1.5519719694174375e-06
Iter: 850 loss: 1.5461237549411263e-06
Iter: 851 loss: 1.5450469663250531e-06
Iter: 852 loss: 1.548987985695117e-06
Iter: 853 loss: 1.5447793961765037e-06
Iter: 854 loss: 1.5436365067250195e-06
Iter: 855 loss: 1.5420939338733299e-06
Iter: 856 loss: 1.5420139219753345e-06
Iter: 857 loss: 1.5407101323418797e-06
Iter: 858 loss: 1.5614481963115655e-06
Iter: 859 loss: 1.540710125081414e-06
Iter: 860 loss: 1.5394675179524433e-06
Iter: 861 loss: 1.5422484419478698e-06
Iter: 862 loss: 1.538995941441675e-06
Iter: 863 loss: 1.5379253775220766e-06
Iter: 864 loss: 1.5386239224905845e-06
Iter: 865 loss: 1.5372478942147218e-06
Iter: 866 loss: 1.53599860210871e-06
Iter: 867 loss: 1.53897676043082e-06
Iter: 868 loss: 1.5355451048065586e-06
Iter: 869 loss: 1.5346281476714759e-06
Iter: 870 loss: 1.5345725229526477e-06
Iter: 871 loss: 1.5341013366746762e-06
Iter: 872 loss: 1.5331602385782217e-06
Iter: 873 loss: 1.5511040978523252e-06
Iter: 874 loss: 1.5331487613021662e-06
Iter: 875 loss: 1.5318835951364701e-06
Iter: 876 loss: 1.5419324379557634e-06
Iter: 877 loss: 1.5317976277093916e-06
Iter: 878 loss: 1.531063250740988e-06
Iter: 879 loss: 1.5290786289648223e-06
Iter: 880 loss: 1.5420239213440682e-06
Iter: 881 loss: 1.5285886403860998e-06
Iter: 882 loss: 1.5271189754523536e-06
Iter: 883 loss: 1.5270275811510204e-06
Iter: 884 loss: 1.5260936339616926e-06
Iter: 885 loss: 1.5243878257410005e-06
Iter: 886 loss: 1.564579822881196e-06
Iter: 887 loss: 1.5243867953409659e-06
Iter: 888 loss: 1.5235962301293296e-06
Iter: 889 loss: 1.5234086969548373e-06
Iter: 890 loss: 1.5223756620887297e-06
Iter: 891 loss: 1.5226729824038262e-06
Iter: 892 loss: 1.5216321857808178e-06
Iter: 893 loss: 1.5202879658694579e-06
Iter: 894 loss: 1.5225952261047338e-06
Iter: 895 loss: 1.5196882695884015e-06
Iter: 896 loss: 1.5184118657774323e-06
Iter: 897 loss: 1.5256379674138824e-06
Iter: 898 loss: 1.5182353619244479e-06
Iter: 899 loss: 1.517138429425921e-06
Iter: 900 loss: 1.5240095239572248e-06
Iter: 901 loss: 1.5170117849453261e-06
Iter: 902 loss: 1.5162657214257749e-06
Iter: 903 loss: 1.5147870351856488e-06
Iter: 904 loss: 1.5435313288871546e-06
Iter: 905 loss: 1.5147714295798507e-06
Iter: 906 loss: 1.5149671890301184e-06
Iter: 907 loss: 1.5140301984736302e-06
Iter: 908 loss: 1.5135702570972409e-06
Iter: 909 loss: 1.5127556611924155e-06
Iter: 910 loss: 1.5127556518369047e-06
Iter: 911 loss: 1.5116229705145873e-06
Iter: 912 loss: 1.5124777004962333e-06
Iter: 913 loss: 1.5109308444868494e-06
Iter: 914 loss: 1.5094688007522487e-06
Iter: 915 loss: 1.523056228337219e-06
Iter: 916 loss: 1.5094069706150766e-06
Iter: 917 loss: 1.5088190696388012e-06
Iter: 918 loss: 1.5071853647108114e-06
Iter: 919 loss: 1.5162452587294574e-06
Iter: 920 loss: 1.5066968067837879e-06
Iter: 921 loss: 1.5052804691457388e-06
Iter: 922 loss: 1.5052589338059042e-06
Iter: 923 loss: 1.5040444433375628e-06
Iter: 924 loss: 1.502767253181283e-06
Iter: 925 loss: 1.5025473407849721e-06
Iter: 926 loss: 1.5008868960384725e-06
Iter: 927 loss: 1.5204715567182247e-06
Iter: 928 loss: 1.5008636599975836e-06
Iter: 929 loss: 1.4996195475335851e-06
Iter: 930 loss: 1.5107836033473808e-06
Iter: 931 loss: 1.4995603682097214e-06
Iter: 932 loss: 1.4987496307329287e-06
Iter: 933 loss: 1.4978750993235713e-06
Iter: 934 loss: 1.4977377642332628e-06
Iter: 935 loss: 1.4967603545183534e-06
Iter: 936 loss: 1.496758536398353e-06
Iter: 937 loss: 1.4959674919826278e-06
Iter: 938 loss: 1.4962624186247048e-06
Iter: 939 loss: 1.4954152806462604e-06
Iter: 940 loss: 1.4944148020505659e-06
Iter: 941 loss: 1.4990021972285496e-06
Iter: 942 loss: 1.494224979084279e-06
Iter: 943 loss: 1.4931065002004902e-06
Iter: 944 loss: 1.4974574627212646e-06
Iter: 945 loss: 1.4928466559736367e-06
Iter: 946 loss: 1.492308524167993e-06
Iter: 947 loss: 1.4910273209106212e-06
Iter: 948 loss: 1.5055337088323709e-06
Iter: 949 loss: 1.4909049442722393e-06
Iter: 950 loss: 1.4900277646099258e-06
Iter: 951 loss: 1.4898674612379319e-06
Iter: 952 loss: 1.489285775396612e-06
Iter: 953 loss: 1.4882930977224705e-06
Iter: 954 loss: 1.4882917949137678e-06
Iter: 955 loss: 1.4872579756788881e-06
Iter: 956 loss: 1.4917681098987165e-06
Iter: 957 loss: 1.4870486997505661e-06
Iter: 958 loss: 1.4859133491495903e-06
Iter: 959 loss: 1.4841932004892556e-06
Iter: 960 loss: 1.4841612770440255e-06
Iter: 961 loss: 1.4827102888845384e-06
Iter: 962 loss: 1.5010660939340792e-06
Iter: 963 loss: 1.4826980666088703e-06
Iter: 964 loss: 1.4814690310282779e-06
Iter: 965 loss: 1.4853389353043663e-06
Iter: 966 loss: 1.4811140867752407e-06
Iter: 967 loss: 1.4798798400210985e-06
Iter: 968 loss: 1.4879567425834295e-06
Iter: 969 loss: 1.4797490069194961e-06
Iter: 970 loss: 1.4784149067569279e-06
Iter: 971 loss: 1.4775205329716363e-06
Iter: 972 loss: 1.477023046020635e-06
Iter: 973 loss: 1.4762136797792277e-06
Iter: 974 loss: 1.4761774988161792e-06
Iter: 975 loss: 1.4753681395409962e-06
Iter: 976 loss: 1.475457385356017e-06
Iter: 977 loss: 1.4747466887041855e-06
Iter: 978 loss: 1.473719481181859e-06
Iter: 979 loss: 1.4832904984649634e-06
Iter: 980 loss: 1.4736764145772078e-06
Iter: 981 loss: 1.4730709797983038e-06
Iter: 982 loss: 1.4721601561515697e-06
Iter: 983 loss: 1.4721418020839167e-06
Iter: 984 loss: 1.4708355417747239e-06
Iter: 985 loss: 1.4725385326072643e-06
Iter: 986 loss: 1.4701706187214822e-06
Iter: 987 loss: 1.4695555293057092e-06
Iter: 988 loss: 1.46939898825164e-06
Iter: 989 loss: 1.4689502728873077e-06
Iter: 990 loss: 1.4676074279440672e-06
Iter: 991 loss: 1.4717408492988181e-06
Iter: 992 loss: 1.4669417553842631e-06
Iter: 993 loss: 1.4651925398118253e-06
Iter: 994 loss: 1.4752540565745658e-06
Iter: 995 loss: 1.4649570754159213e-06
Iter: 996 loss: 1.463335379422716e-06
Iter: 997 loss: 1.4730388092598572e-06
Iter: 998 loss: 1.4631316522590892e-06
Iter: 999 loss: 1.4621032130663003e-06
Iter: 1000 loss: 1.4630091948704927e-06
Iter: 1001 loss: 1.4615011317747496e-06
Iter: 1002 loss: 1.4602849407744156e-06
Iter: 1003 loss: 1.4645950522251284e-06
Iter: 1004 loss: 1.4599722923721455e-06
Iter: 1005 loss: 1.4585326827627956e-06
Iter: 1006 loss: 1.4685552524592584e-06
Iter: 1007 loss: 1.4583985828246688e-06
Iter: 1008 loss: 1.4574309920844562e-06
Iter: 1009 loss: 1.4601531946216596e-06
Iter: 1010 loss: 1.4571223175971434e-06
Iter: 1011 loss: 1.4564419669625421e-06
Iter: 1012 loss: 1.4637792273046943e-06
Iter: 1013 loss: 1.4564263209335111e-06
Iter: 1014 loss: 1.4557207406440677e-06
Iter: 1015 loss: 1.4548143707659357e-06
Iter: 1016 loss: 1.4547503772067164e-06
Iter: 1017 loss: 1.4538227852234504e-06
Iter: 1018 loss: 1.4637542543777163e-06
Iter: 1019 loss: 1.4538006961122958e-06
Iter: 1020 loss: 1.4531412755656855e-06
Iter: 1021 loss: 1.4523345123632148e-06
Iter: 1022 loss: 1.4522606259208208e-06
Iter: 1023 loss: 1.4513199912100186e-06
Iter: 1024 loss: 1.4574345725601114e-06
Iter: 1025 loss: 1.4512189501691397e-06
Iter: 1026 loss: 1.4501336002071737e-06
Iter: 1027 loss: 1.4525006726973702e-06
Iter: 1028 loss: 1.4497144315194192e-06
Iter: 1029 loss: 1.4488315724012698e-06
Iter: 1030 loss: 1.44944631957618e-06
Iter: 1031 loss: 1.4482812214345081e-06
Iter: 1032 loss: 1.4471842093288344e-06
Iter: 1033 loss: 1.4469176442278323e-06
Iter: 1034 loss: 1.4462212153302774e-06
Iter: 1035 loss: 1.4448180261631041e-06
Iter: 1036 loss: 1.4459084428958969e-06
Iter: 1037 loss: 1.4439671959665825e-06
Iter: 1038 loss: 1.4426632328630531e-06
Iter: 1039 loss: 1.4426583722671247e-06
Iter: 1040 loss: 1.4417638101718254e-06
Iter: 1041 loss: 1.4416017521976137e-06
Iter: 1042 loss: 1.4409971171870383e-06
Iter: 1043 loss: 1.4404002037966995e-06
Iter: 1044 loss: 1.4402833496434097e-06
Iter: 1045 loss: 1.4396159694795798e-06
Iter: 1046 loss: 1.4392069834873158e-06
Iter: 1047 loss: 1.4389362597721269e-06
Iter: 1048 loss: 1.438017918498152e-06
Iter: 1049 loss: 1.4499694319816598e-06
Iter: 1050 loss: 1.438011901681321e-06
Iter: 1051 loss: 1.4375914217820438e-06
Iter: 1052 loss: 1.4366969781741779e-06
Iter: 1053 loss: 1.4511237529521539e-06
Iter: 1054 loss: 1.4366692222785954e-06
Iter: 1055 loss: 1.4352179376901552e-06
Iter: 1056 loss: 1.4378706742978411e-06
Iter: 1057 loss: 1.43459275586317e-06
Iter: 1058 loss: 1.4331177152355115e-06
Iter: 1059 loss: 1.4451763602675794e-06
Iter: 1060 loss: 1.433025000035667e-06
Iter: 1061 loss: 1.4324097049355268e-06
Iter: 1062 loss: 1.4331480758321104e-06
Iter: 1063 loss: 1.4320857314167023e-06
Iter: 1064 loss: 1.4310420967354416e-06
Iter: 1065 loss: 1.4325701086309799e-06
Iter: 1066 loss: 1.4305376594192568e-06
Iter: 1067 loss: 1.4297650715453047e-06
Iter: 1068 loss: 1.4295213610014466e-06
Iter: 1069 loss: 1.4290673770322948e-06
Iter: 1070 loss: 1.4280621385909453e-06
Iter: 1071 loss: 1.4335608314988599e-06
Iter: 1072 loss: 1.4279150062382525e-06
Iter: 1073 loss: 1.4269186327488073e-06
Iter: 1074 loss: 1.4257503423170915e-06
Iter: 1075 loss: 1.4256196433206248e-06
Iter: 1076 loss: 1.4242014752415844e-06
Iter: 1077 loss: 1.4333660430871392e-06
Iter: 1078 loss: 1.4240473093013678e-06
Iter: 1079 loss: 1.4227020506050763e-06
Iter: 1080 loss: 1.43013195094941e-06
Iter: 1081 loss: 1.422508248177836e-06
Iter: 1082 loss: 1.4222330437056884e-06
Iter: 1083 loss: 1.4220218765841074e-06
Iter: 1084 loss: 1.4216524539067478e-06
Iter: 1085 loss: 1.420765287670128e-06
Iter: 1086 loss: 1.4305007123282712e-06
Iter: 1087 loss: 1.4206742514858658e-06
Iter: 1088 loss: 1.4193995858822826e-06
Iter: 1089 loss: 1.427792354437632e-06
Iter: 1090 loss: 1.419266109600218e-06
Iter: 1091 loss: 1.4186200049575572e-06
Iter: 1092 loss: 1.4177653805110884e-06
Iter: 1093 loss: 1.4177147848259669e-06
Iter: 1094 loss: 1.416654172614596e-06
Iter: 1095 loss: 1.4286092669038271e-06
Iter: 1096 loss: 1.4166347657046585e-06
Iter: 1097 loss: 1.4157004236361791e-06
Iter: 1098 loss: 1.4169455412959172e-06
Iter: 1099 loss: 1.4152292496980042e-06
Iter: 1100 loss: 1.4143900345233563e-06
Iter: 1101 loss: 1.4184587808169174e-06
Iter: 1102 loss: 1.4142425960054861e-06
Iter: 1103 loss: 1.413522520182689e-06
Iter: 1104 loss: 1.4160067047042891e-06
Iter: 1105 loss: 1.4133322265086243e-06
Iter: 1106 loss: 1.4126285723431626e-06
Iter: 1107 loss: 1.4117195872921738e-06
Iter: 1108 loss: 1.4116574612478913e-06
Iter: 1109 loss: 1.41058442495802e-06
Iter: 1110 loss: 1.4110462735248178e-06
Iter: 1111 loss: 1.4098505238852952e-06
Iter: 1112 loss: 1.4086727520352381e-06
Iter: 1113 loss: 1.4234449403922414e-06
Iter: 1114 loss: 1.4086621088737017e-06
Iter: 1115 loss: 1.4077201864194571e-06
Iter: 1116 loss: 1.4077898759428723e-06
Iter: 1117 loss: 1.406987143452825e-06
Iter: 1118 loss: 1.406349974845769e-06
Iter: 1119 loss: 1.4062957007583218e-06
Iter: 1120 loss: 1.4054708502765309e-06
Iter: 1121 loss: 1.4052045949055763e-06
Iter: 1122 loss: 1.4047237573203309e-06
Iter: 1123 loss: 1.4040276900463838e-06
Iter: 1124 loss: 1.406884630196839e-06
Iter: 1125 loss: 1.4038757730055785e-06
Iter: 1126 loss: 1.4031783576364539e-06
Iter: 1127 loss: 1.4046992802684847e-06
Iter: 1128 loss: 1.4029090005075951e-06
Iter: 1129 loss: 1.4020445494635629e-06
Iter: 1130 loss: 1.4015255178135461e-06
Iter: 1131 loss: 1.4011686288529693e-06
Iter: 1132 loss: 1.4006379366946892e-06
Iter: 1133 loss: 1.4006375198914772e-06
Iter: 1134 loss: 1.4000064980523615e-06
Iter: 1135 loss: 1.3992176788445861e-06
Iter: 1136 loss: 1.3991529818094917e-06
Iter: 1137 loss: 1.3981855730650222e-06
Iter: 1138 loss: 1.4025354083870477e-06
Iter: 1139 loss: 1.3979972229092868e-06
Iter: 1140 loss: 1.3968213269389623e-06
Iter: 1141 loss: 1.3985788445066438e-06
Iter: 1142 loss: 1.3962584729829985e-06
Iter: 1143 loss: 1.3955510614869606e-06
Iter: 1144 loss: 1.3984186351448545e-06
Iter: 1145 loss: 1.3953943615154695e-06
Iter: 1146 loss: 1.3946748178398797e-06
Iter: 1147 loss: 1.3934649441882766e-06
Iter: 1148 loss: 1.3934620295902332e-06
Iter: 1149 loss: 1.3920414651497028e-06
Iter: 1150 loss: 1.396221212483027e-06
Iter: 1151 loss: 1.3916053343635902e-06
Iter: 1152 loss: 1.3906833295010173e-06
Iter: 1153 loss: 1.3906414831995577e-06
Iter: 1154 loss: 1.3899407873834737e-06
Iter: 1155 loss: 1.397068583261647e-06
Iter: 1156 loss: 1.3899198000029772e-06
Iter: 1157 loss: 1.3894378834348386e-06
Iter: 1158 loss: 1.3883355989804727e-06
Iter: 1159 loss: 1.4026353118653394e-06
Iter: 1160 loss: 1.3882616461721236e-06
Iter: 1161 loss: 1.3873873719011351e-06
Iter: 1162 loss: 1.3960438199794796e-06
Iter: 1163 loss: 1.3873581307308539e-06
Iter: 1164 loss: 1.3865003949708085e-06
Iter: 1165 loss: 1.3893036579640912e-06
Iter: 1166 loss: 1.386261263788494e-06
Iter: 1167 loss: 1.3856721372499778e-06
Iter: 1168 loss: 1.3852913688602807e-06
Iter: 1169 loss: 1.3850636696709809e-06
Iter: 1170 loss: 1.3840046266092378e-06
Iter: 1171 loss: 1.3902662237120638e-06
Iter: 1172 loss: 1.3838687326883605e-06
Iter: 1173 loss: 1.382849583319491e-06
Iter: 1174 loss: 1.3859997517888198e-06
Iter: 1175 loss: 1.382550200076343e-06
Iter: 1176 loss: 1.3817949883092e-06
Iter: 1177 loss: 1.3807679571629826e-06
Iter: 1178 loss: 1.3807173851642615e-06
Iter: 1179 loss: 1.3796727771766076e-06
Iter: 1180 loss: 1.3796532586199792e-06
Iter: 1181 loss: 1.3789487220796518e-06
Iter: 1182 loss: 1.3780305920984595e-06
Iter: 1183 loss: 1.3779710121844995e-06
Iter: 1184 loss: 1.3768143701050041e-06
Iter: 1185 loss: 1.379188124541437e-06
Iter: 1186 loss: 1.3763496600832887e-06
Iter: 1187 loss: 1.3753777258429358e-06
Iter: 1188 loss: 1.3850792533773516e-06
Iter: 1189 loss: 1.3753462551566356e-06
Iter: 1190 loss: 1.3749586991766114e-06
Iter: 1191 loss: 1.3749562354258578e-06
Iter: 1192 loss: 1.3744588809638725e-06
Iter: 1193 loss: 1.3731904880750167e-06
Iter: 1194 loss: 1.3841529389800372e-06
Iter: 1195 loss: 1.372983197607261e-06
Iter: 1196 loss: 1.3720426809869619e-06
Iter: 1197 loss: 1.3773134646381468e-06
Iter: 1198 loss: 1.3719103958982162e-06
Iter: 1199 loss: 1.3709926140422487e-06
Iter: 1200 loss: 1.3741325637376884e-06
Iter: 1201 loss: 1.3707480156317609e-06
Iter: 1202 loss: 1.3698175245793229e-06
Iter: 1203 loss: 1.3760472734271696e-06
Iter: 1204 loss: 1.3697233606068125e-06
Iter: 1205 loss: 1.3692453090543262e-06
Iter: 1206 loss: 1.36834236255601e-06
Iter: 1207 loss: 1.3880518775985928e-06
Iter: 1208 loss: 1.3683395372173434e-06
Iter: 1209 loss: 1.3676096848677624e-06
Iter: 1210 loss: 1.3675443347254368e-06
Iter: 1211 loss: 1.3670683069360519e-06
Iter: 1212 loss: 1.36607066603554e-06
Iter: 1213 loss: 1.3828385719963885e-06
Iter: 1214 loss: 1.3660450746973666e-06
Iter: 1215 loss: 1.365004777283486e-06
Iter: 1216 loss: 1.3740142642050328e-06
Iter: 1217 loss: 1.3649494502451253e-06
Iter: 1218 loss: 1.364285985999333e-06
Iter: 1219 loss: 1.3682016223933117e-06
Iter: 1220 loss: 1.3642005806311573e-06
Iter: 1221 loss: 1.3635146189553969e-06
Iter: 1222 loss: 1.3631248294038137e-06
Iter: 1223 loss: 1.3628287404835296e-06
Iter: 1224 loss: 1.3617447033614587e-06
Iter: 1225 loss: 1.3604455941439102e-06
Iter: 1226 loss: 1.3603140726926385e-06
Iter: 1227 loss: 1.3607822692715875e-06
Iter: 1228 loss: 1.3596885607125941e-06
Iter: 1229 loss: 1.3592036902179933e-06
Iter: 1230 loss: 1.3609465983923832e-06
Iter: 1231 loss: 1.3590808791774975e-06
Iter: 1232 loss: 1.3586694738130717e-06
Iter: 1233 loss: 1.3575036343376256e-06
Iter: 1234 loss: 1.36318810124843e-06
Iter: 1235 loss: 1.3571052842147049e-06
Iter: 1236 loss: 1.3561630859910299e-06
Iter: 1237 loss: 1.3680812790558079e-06
Iter: 1238 loss: 1.3561551420136418e-06
Iter: 1239 loss: 1.3554530326743673e-06
Iter: 1240 loss: 1.3629077667297478e-06
Iter: 1241 loss: 1.3554356396325631e-06
Iter: 1242 loss: 1.3548330673987182e-06
Iter: 1243 loss: 1.3549913620844085e-06
Iter: 1244 loss: 1.3543954524700985e-06
Iter: 1245 loss: 1.3535847003831956e-06
Iter: 1246 loss: 1.3536473499378152e-06
Iter: 1247 loss: 1.3529545256283675e-06
Iter: 1248 loss: 1.3520982206432635e-06
Iter: 1249 loss: 1.3520928689612228e-06
Iter: 1250 loss: 1.3515141562842575e-06
Iter: 1251 loss: 1.3503413960393229e-06
Iter: 1252 loss: 1.3718859126847483e-06
Iter: 1253 loss: 1.3503230265793783e-06
Iter: 1254 loss: 1.349311325261128e-06
Iter: 1255 loss: 1.3539104411885677e-06
Iter: 1256 loss: 1.3491171499629576e-06
Iter: 1257 loss: 1.3484147498960369e-06
Iter: 1258 loss: 1.3484122568081413e-06
Iter: 1259 loss: 1.3479098519398386e-06
Iter: 1260 loss: 1.3471699727740817e-06
Iter: 1261 loss: 1.3471512376432517e-06
Iter: 1262 loss: 1.3461295790951463e-06
Iter: 1263 loss: 1.3511175071659443e-06
Iter: 1264 loss: 1.3459518560999287e-06
Iter: 1265 loss: 1.3454312887991365e-06
Iter: 1266 loss: 1.3453919307528332e-06
Iter: 1267 loss: 1.3449373087576907e-06
Iter: 1268 loss: 1.3437967130390495e-06
Iter: 1269 loss: 1.3543644582292776e-06
Iter: 1270 loss: 1.3436317172930542e-06
Iter: 1271 loss: 1.3427129921907353e-06
Iter: 1272 loss: 1.3435225065122047e-06
Iter: 1273 loss: 1.3421751960840607e-06
Iter: 1274 loss: 1.3406660960183829e-06
Iter: 1275 loss: 1.3460687425537684e-06
Iter: 1276 loss: 1.340282206065533e-06
Iter: 1277 loss: 1.3398593570732208e-06
Iter: 1278 loss: 1.3397388919533519e-06
Iter: 1279 loss: 1.3392786403147378e-06
Iter: 1280 loss: 1.3383051850806132e-06
Iter: 1281 loss: 1.3542610843264098e-06
Iter: 1282 loss: 1.3382770663609423e-06
Iter: 1283 loss: 1.3375667131874414e-06
Iter: 1284 loss: 1.3486286235132552e-06
Iter: 1285 loss: 1.337566630260272e-06
Iter: 1286 loss: 1.33685124110983e-06
Iter: 1287 loss: 1.3372574534270626e-06
Iter: 1288 loss: 1.3363850222787538e-06
Iter: 1289 loss: 1.3355513733730228e-06
Iter: 1290 loss: 1.3359827852614388e-06
Iter: 1291 loss: 1.334998425380988e-06
Iter: 1292 loss: 1.3339023451729603e-06
Iter: 1293 loss: 1.3337929823646305e-06
Iter: 1294 loss: 1.332991793784322e-06
Iter: 1295 loss: 1.3325371734140157e-06
Iter: 1296 loss: 1.3323375504288221e-06
Iter: 1297 loss: 1.3317294473102043e-06
Iter: 1298 loss: 1.3309062210708749e-06
Iter: 1299 loss: 1.3308643906538396e-06
Iter: 1300 loss: 1.3306992693669042e-06
Iter: 1301 loss: 1.3304151146876002e-06
Iter: 1302 loss: 1.3300724624312482e-06
Iter: 1303 loss: 1.3295226330694982e-06
Iter: 1304 loss: 1.3295182910140126e-06
Iter: 1305 loss: 1.3287993270662597e-06
Iter: 1306 loss: 1.3282607609080888e-06
Iter: 1307 loss: 1.3280238005093046e-06
Iter: 1308 loss: 1.3270428883832732e-06
Iter: 1309 loss: 1.3289591763128796e-06
Iter: 1310 loss: 1.3266365215755533e-06
Iter: 1311 loss: 1.325766906674766e-06
Iter: 1312 loss: 1.3358039452849527e-06
Iter: 1313 loss: 1.325753008929461e-06
Iter: 1314 loss: 1.3249927678141223e-06
Iter: 1315 loss: 1.3286676359548423e-06
Iter: 1316 loss: 1.3248586387936662e-06
Iter: 1317 loss: 1.3241294585566953e-06
Iter: 1318 loss: 1.3244419139081263e-06
Iter: 1319 loss: 1.3236304085536854e-06
Iter: 1320 loss: 1.3230497843522952e-06
Iter: 1321 loss: 1.3258808743863613e-06
Iter: 1322 loss: 1.3229485943758856e-06
Iter: 1323 loss: 1.3222041011225879e-06
Iter: 1324 loss: 1.3229797874229389e-06
Iter: 1325 loss: 1.3217911157481023e-06
Iter: 1326 loss: 1.3210714256488814e-06
Iter: 1327 loss: 1.3205040839478133e-06
Iter: 1328 loss: 1.3202819486591979e-06
Iter: 1329 loss: 1.3194187016824043e-06
Iter: 1330 loss: 1.3267160516132441e-06
Iter: 1331 loss: 1.3193693494981626e-06
Iter: 1332 loss: 1.3186905793353048e-06
Iter: 1333 loss: 1.3215617396702739e-06
Iter: 1334 loss: 1.3185477480918105e-06
Iter: 1335 loss: 1.3178843979169656e-06
Iter: 1336 loss: 1.323684445577078e-06
Iter: 1337 loss: 1.3178501353600394e-06
Iter: 1338 loss: 1.3173574920823861e-06
Iter: 1339 loss: 1.3178630886654821e-06
Iter: 1340 loss: 1.3170827909604289e-06
Iter: 1341 loss: 1.3162828574364576e-06
Iter: 1342 loss: 1.3147175763296319e-06
Iter: 1343 loss: 1.3461196858760106e-06
Iter: 1344 loss: 1.3147048785994541e-06
Iter: 1345 loss: 1.3136902301431669e-06
Iter: 1346 loss: 1.3196811724155354e-06
Iter: 1347 loss: 1.3135597281164429e-06
Iter: 1348 loss: 1.3125647096464078e-06
Iter: 1349 loss: 1.3139670970533279e-06
Iter: 1350 loss: 1.3120752034784132e-06
Iter: 1351 loss: 1.3113180819076961e-06
Iter: 1352 loss: 1.316566199739238e-06
Iter: 1353 loss: 1.3112468854881821e-06
Iter: 1354 loss: 1.3106305855174853e-06
Iter: 1355 loss: 1.3180834665192374e-06
Iter: 1356 loss: 1.3106232780074956e-06
Iter: 1357 loss: 1.3102195696380698e-06
Iter: 1358 loss: 1.3091804387243484e-06
Iter: 1359 loss: 1.317802964708753e-06
Iter: 1360 loss: 1.3089987492873649e-06
Iter: 1361 loss: 1.308353998115954e-06
Iter: 1362 loss: 1.3082086038168491e-06
Iter: 1363 loss: 1.3077588914978805e-06
Iter: 1364 loss: 1.3084579955432959e-06
Iter: 1365 loss: 1.3075477028170994e-06
Iter: 1366 loss: 1.3069712677722217e-06
Iter: 1367 loss: 1.3054613374462922e-06
Iter: 1368 loss: 1.3170117459534129e-06
Iter: 1369 loss: 1.305161877018778e-06
Iter: 1370 loss: 1.304354837786928e-06
Iter: 1371 loss: 1.3043118972720992e-06
Iter: 1372 loss: 1.3037129785224781e-06
Iter: 1373 loss: 1.3037116876820754e-06
Iter: 1374 loss: 1.3033452282011809e-06
Iter: 1375 loss: 1.3030726444592381e-06
Iter: 1376 loss: 1.3029508287214971e-06
Iter: 1377 loss: 1.3023472947797739e-06
Iter: 1378 loss: 1.3035608164384281e-06
Iter: 1379 loss: 1.3021016633688206e-06
Iter: 1380 loss: 1.3014025214898181e-06
Iter: 1381 loss: 1.3022748711315811e-06
Iter: 1382 loss: 1.301040096144743e-06
Iter: 1383 loss: 1.3003587508416621e-06
Iter: 1384 loss: 1.29983879299013e-06
Iter: 1385 loss: 1.2996193712400255e-06
Iter: 1386 loss: 1.2985484557468224e-06
Iter: 1387 loss: 1.3031481219034483e-06
Iter: 1388 loss: 1.2983273515707771e-06
Iter: 1389 loss: 1.2972525688663965e-06
Iter: 1390 loss: 1.3046787393965804e-06
Iter: 1391 loss: 1.2971507968764795e-06
Iter: 1392 loss: 1.2963224786780317e-06
Iter: 1393 loss: 1.3036786391612647e-06
Iter: 1394 loss: 1.2962817381947683e-06
Iter: 1395 loss: 1.2959124240375368e-06
Iter: 1396 loss: 1.2951706100580978e-06
Iter: 1397 loss: 1.309111785205196e-06
Iter: 1398 loss: 1.2951606029490313e-06
Iter: 1399 loss: 1.2945268624117981e-06
Iter: 1400 loss: 1.2945073273653948e-06
Iter: 1401 loss: 1.293994960202411e-06
Iter: 1402 loss: 1.293478774204348e-06
Iter: 1403 loss: 1.2933759038450248e-06
Iter: 1404 loss: 1.29269765400062e-06
Iter: 1405 loss: 1.2937893372187153e-06
Iter: 1406 loss: 1.2923846662279533e-06
Iter: 1407 loss: 1.2916786624001508e-06
Iter: 1408 loss: 1.2991798910511231e-06
Iter: 1409 loss: 1.291661231022844e-06
Iter: 1410 loss: 1.2909718279103669e-06
Iter: 1411 loss: 1.29387185621078e-06
Iter: 1412 loss: 1.2908257781292425e-06
Iter: 1413 loss: 1.2904426376176758e-06
Iter: 1414 loss: 1.2893808919645e-06
Iter: 1415 loss: 1.2953710761824113e-06
Iter: 1416 loss: 1.2890693504569307e-06
Iter: 1417 loss: 1.2885746853423093e-06
Iter: 1418 loss: 1.2883090632616912e-06
Iter: 1419 loss: 1.2878189654929497e-06
Iter: 1420 loss: 1.2872630607863343e-06
Iter: 1421 loss: 1.2871913571316184e-06
Iter: 1422 loss: 1.2863410949228761e-06
Iter: 1423 loss: 1.2888779717918474e-06
Iter: 1424 loss: 1.2860832532926156e-06
Iter: 1425 loss: 1.2853021269828247e-06
Iter: 1426 loss: 1.2857774743579456e-06
Iter: 1427 loss: 1.2848002238258425e-06
Iter: 1428 loss: 1.2843961557338042e-06
Iter: 1429 loss: 1.2843272408304094e-06
Iter: 1430 loss: 1.2838261192226823e-06
Iter: 1431 loss: 1.2832229839595787e-06
Iter: 1432 loss: 1.2831631637222457e-06
Iter: 1433 loss: 1.2822853771886065e-06
Iter: 1434 loss: 1.2851479656450495e-06
Iter: 1435 loss: 1.2820401547782172e-06
Iter: 1436 loss: 1.2815151206367817e-06
Iter: 1437 loss: 1.2857443212112784e-06
Iter: 1438 loss: 1.2814807549572535e-06
Iter: 1439 loss: 1.2809126157569675e-06
Iter: 1440 loss: 1.281257383356559e-06
Iter: 1441 loss: 1.2805473402064432e-06
Iter: 1442 loss: 1.2799942077636928e-06
Iter: 1443 loss: 1.2792499282619863e-06
Iter: 1444 loss: 1.2792105074330599e-06
Iter: 1445 loss: 1.2786579172154628e-06
Iter: 1446 loss: 1.2784810027509627e-06
Iter: 1447 loss: 1.2781222604351483e-06
Iter: 1448 loss: 1.2774381297909619e-06
Iter: 1449 loss: 1.2920370815469963e-06
Iter: 1450 loss: 1.2774352304558506e-06
Iter: 1451 loss: 1.2765888072489671e-06
Iter: 1452 loss: 1.2766924373058072e-06
Iter: 1453 loss: 1.275941778648217e-06
Iter: 1454 loss: 1.2751039857006473e-06
Iter: 1455 loss: 1.2799720747933765e-06
Iter: 1456 loss: 1.2749931534712638e-06
Iter: 1457 loss: 1.2740607747721749e-06
Iter: 1458 loss: 1.2787432522230275e-06
Iter: 1459 loss: 1.2739051063435128e-06
Iter: 1460 loss: 1.2733526661996378e-06
Iter: 1461 loss: 1.2733245546610686e-06
Iter: 1462 loss: 1.2729021695762838e-06
Iter: 1463 loss: 1.2720030155554985e-06
Iter: 1464 loss: 1.2733018124105496e-06
Iter: 1465 loss: 1.2715656406424918e-06
Iter: 1466 loss: 1.271132969765418e-06
Iter: 1467 loss: 1.271111223514337e-06
Iter: 1468 loss: 1.2706171400628472e-06
Iter: 1469 loss: 1.2700861959237846e-06
Iter: 1470 loss: 1.2700016417653201e-06
Iter: 1471 loss: 1.2692245395325345e-06
Iter: 1472 loss: 1.2697022347095093e-06
Iter: 1473 loss: 1.2687262824107278e-06
Iter: 1474 loss: 1.2682102722332563e-06
Iter: 1475 loss: 1.2681811210562557e-06
Iter: 1476 loss: 1.2677205232538324e-06
Iter: 1477 loss: 1.2675271570630378e-06
Iter: 1478 loss: 1.2672869510463623e-06
Iter: 1479 loss: 1.2665456314094569e-06
Iter: 1480 loss: 1.2692644029738665e-06
Iter: 1481 loss: 1.2663618055219202e-06
Iter: 1482 loss: 1.2656049139999916e-06
Iter: 1483 loss: 1.2715128145313888e-06
Iter: 1484 loss: 1.2655510979820766e-06
Iter: 1485 loss: 1.265243011482539e-06
Iter: 1486 loss: 1.2643743238306459e-06
Iter: 1487 loss: 1.2687591635208543e-06
Iter: 1488 loss: 1.2640874864458273e-06
Iter: 1489 loss: 1.2633220975823488e-06
Iter: 1490 loss: 1.2633209037182335e-06
Iter: 1491 loss: 1.2626695106921694e-06
Iter: 1492 loss: 1.2616536237961226e-06
Iter: 1493 loss: 1.2616407531576149e-06
Iter: 1494 loss: 1.2614355925834323e-06
Iter: 1495 loss: 1.2610582753742819e-06
Iter: 1496 loss: 1.2606142870571308e-06
Iter: 1497 loss: 1.2600489978726936e-06
Iter: 1498 loss: 1.2600070267445763e-06
Iter: 1499 loss: 1.2593005053871813e-06
Iter: 1500 loss: 1.2620995751422422e-06
Iter: 1501 loss: 1.2591397618808097e-06
Iter: 1502 loss: 1.2584864811159902e-06
Iter: 1503 loss: 1.2621548746973887e-06
Iter: 1504 loss: 1.2583948897109513e-06
Iter: 1505 loss: 1.2576987424697633e-06
Iter: 1506 loss: 1.2595825981249265e-06
Iter: 1507 loss: 1.2574694095057169e-06
Iter: 1508 loss: 1.2570057114295677e-06
Iter: 1509 loss: 1.256090079438852e-06
Iter: 1510 loss: 1.2740512524530885e-06
Iter: 1511 loss: 1.2560810963205429e-06
Iter: 1512 loss: 1.2559947270489405e-06
Iter: 1513 loss: 1.2556509415518694e-06
Iter: 1514 loss: 1.255230863171055e-06
Iter: 1515 loss: 1.2546752489126613e-06
Iter: 1516 loss: 1.2546423431252982e-06
Iter: 1517 loss: 1.2538681730827793e-06
Iter: 1518 loss: 1.2629464623447752e-06
Iter: 1519 loss: 1.2538569302632606e-06
Iter: 1520 loss: 1.2534498719125284e-06
Iter: 1521 loss: 1.2530270439616385e-06
Iter: 1522 loss: 1.2529510301433564e-06
Iter: 1523 loss: 1.2522110348366141e-06
Iter: 1524 loss: 1.2515203378405012e-06
Iter: 1525 loss: 1.2513461225944511e-06
Iter: 1526 loss: 1.2503016845999027e-06
Iter: 1527 loss: 1.2557783328989024e-06
Iter: 1528 loss: 1.2501382796974994e-06
Iter: 1529 loss: 1.2495263337767726e-06
Iter: 1530 loss: 1.254845763333459e-06
Iter: 1531 loss: 1.2494941530962966e-06
Iter: 1532 loss: 1.2489066060843823e-06
Iter: 1533 loss: 1.24914040778018e-06
Iter: 1534 loss: 1.2485001027335728e-06
Iter: 1535 loss: 1.2475367010565811e-06
Iter: 1536 loss: 1.2528036960087269e-06
Iter: 1537 loss: 1.2473955544425924e-06
Iter: 1538 loss: 1.2468453167908066e-06
Iter: 1539 loss: 1.247083161783013e-06
Iter: 1540 loss: 1.2464692321528825e-06
Iter: 1541 loss: 1.2459696427515484e-06
Iter: 1542 loss: 1.2459677687679645e-06
Iter: 1543 loss: 1.2455359178796138e-06
Iter: 1544 loss: 1.2448146333908068e-06
Iter: 1545 loss: 1.2448124618786685e-06
Iter: 1546 loss: 1.2441492603559995e-06
Iter: 1547 loss: 1.2485001153266787e-06
Iter: 1548 loss: 1.244079303874368e-06
Iter: 1549 loss: 1.2436642956017358e-06
Iter: 1550 loss: 1.2487364485464426e-06
Iter: 1551 loss: 1.2436597300243415e-06
Iter: 1552 loss: 1.2431899382256419e-06
Iter: 1553 loss: 1.2427461374478995e-06
Iter: 1554 loss: 1.2426380853205587e-06
Iter: 1555 loss: 1.2422166492080019e-06
Iter: 1556 loss: 1.2422161063241611e-06
Iter: 1557 loss: 1.2419081887468247e-06
Iter: 1558 loss: 1.2410240412812367e-06
Iter: 1559 loss: 1.2449441619682141e-06
Iter: 1560 loss: 1.2406936827775002e-06
Iter: 1561 loss: 1.2396510228353443e-06
Iter: 1562 loss: 1.2479455111967534e-06
Iter: 1563 loss: 1.2395804726221191e-06
Iter: 1564 loss: 1.2385924910702494e-06
Iter: 1565 loss: 1.2394265710181308e-06
Iter: 1566 loss: 1.2380068845085376e-06
Iter: 1567 loss: 1.2370396910402875e-06
Iter: 1568 loss: 1.2399683694122623e-06
Iter: 1569 loss: 1.2367502367396275e-06
Iter: 1570 loss: 1.236052519188022e-06
Iter: 1571 loss: 1.2462640039168219e-06
Iter: 1572 loss: 1.2360516477961415e-06
Iter: 1573 loss: 1.2354815292548113e-06
Iter: 1574 loss: 1.2371219953816559e-06
Iter: 1575 loss: 1.2353030873835407e-06
Iter: 1576 loss: 1.234726900751494e-06
Iter: 1577 loss: 1.2361593003054078e-06
Iter: 1578 loss: 1.2345241052602713e-06
Iter: 1579 loss: 1.2341622792797706e-06
Iter: 1580 loss: 1.2376648891814478e-06
Iter: 1581 loss: 1.2341490758157444e-06
Iter: 1582 loss: 1.2337439599794818e-06
Iter: 1583 loss: 1.2326307551805251e-06
Iter: 1584 loss: 1.2392399573885831e-06
Iter: 1585 loss: 1.2323224919506075e-06
Iter: 1586 loss: 1.2319767087409772e-06
Iter: 1587 loss: 1.2317453443895085e-06
Iter: 1588 loss: 1.2311527552884759e-06
Iter: 1589 loss: 1.2321280382215981e-06
Iter: 1590 loss: 1.2308824194465458e-06
Iter: 1591 loss: 1.230348950214667e-06
Iter: 1592 loss: 1.2303440765734444e-06
Iter: 1593 loss: 1.2299207111986522e-06
Iter: 1594 loss: 1.2294208122266014e-06
Iter: 1595 loss: 1.2360390812333448e-06
Iter: 1596 loss: 1.229418049193137e-06
Iter: 1597 loss: 1.2290359168216243e-06
Iter: 1598 loss: 1.2285284459995115e-06
Iter: 1599 loss: 1.2284991511254377e-06
Iter: 1600 loss: 1.2278971569965965e-06
Iter: 1601 loss: 1.2276154674829466e-06
Iter: 1602 loss: 1.2273193937730502e-06
Iter: 1603 loss: 1.2265116986541965e-06
Iter: 1604 loss: 1.2351570894639205e-06
Iter: 1605 loss: 1.2264924406916318e-06
Iter: 1606 loss: 1.2258152159106951e-06
Iter: 1607 loss: 1.2255223488121974e-06
Iter: 1608 loss: 1.2251744776272167e-06
Iter: 1609 loss: 1.2242426409350623e-06
Iter: 1610 loss: 1.2297004803470259e-06
Iter: 1611 loss: 1.2241210658650412e-06
Iter: 1612 loss: 1.2235664282424924e-06
Iter: 1613 loss: 1.2235501902319451e-06
Iter: 1614 loss: 1.2231713121849669e-06
Iter: 1615 loss: 1.2226720818649697e-06
Iter: 1616 loss: 1.2226418029195404e-06
Iter: 1617 loss: 1.221904683133402e-06
Iter: 1618 loss: 1.2275544284217872e-06
Iter: 1619 loss: 1.2218498096761289e-06
Iter: 1620 loss: 1.2213191129758556e-06
Iter: 1621 loss: 1.2220824160656508e-06
Iter: 1622 loss: 1.2210604620992729e-06
Iter: 1623 loss: 1.2205698383482393e-06
Iter: 1624 loss: 1.2258083531580575e-06
Iter: 1625 loss: 1.2205580010166686e-06
Iter: 1626 loss: 1.2200877414680084e-06
Iter: 1627 loss: 1.2197435137024744e-06
Iter: 1628 loss: 1.2195841692638103e-06
Iter: 1629 loss: 1.2190781189161209e-06
Iter: 1630 loss: 1.2187388726457388e-06
Iter: 1631 loss: 1.2185501639019965e-06
Iter: 1632 loss: 1.2180569235393571e-06
Iter: 1633 loss: 1.2180175805475715e-06
Iter: 1634 loss: 1.2176683588102967e-06
Iter: 1635 loss: 1.2172193097867119e-06
Iter: 1636 loss: 1.2171877845142702e-06
Iter: 1637 loss: 1.2165559304551167e-06
Iter: 1638 loss: 1.218053507690848e-06
Iter: 1639 loss: 1.2163256028455229e-06
Iter: 1640 loss: 1.2157116217680765e-06
Iter: 1641 loss: 1.2148634852600912e-06
Iter: 1642 loss: 1.2148261785363202e-06
Iter: 1643 loss: 1.2140827360129526e-06
Iter: 1644 loss: 1.2140719503037979e-06
Iter: 1645 loss: 1.2133076482130308e-06
Iter: 1646 loss: 1.2125204436972894e-06
Iter: 1647 loss: 1.2123747407533489e-06
Iter: 1648 loss: 1.2127670637489136e-06
Iter: 1649 loss: 1.2119601230223501e-06
Iter: 1650 loss: 1.2116981045970753e-06
Iter: 1651 loss: 1.211015435731826e-06
Iter: 1652 loss: 1.2163725815889529e-06
Iter: 1653 loss: 1.2108851359828359e-06
Iter: 1654 loss: 1.2105562853038987e-06
Iter: 1655 loss: 1.2104324369088971e-06
Iter: 1656 loss: 1.2101134158912906e-06
Iter: 1657 loss: 1.2104452588570838e-06
Iter: 1658 loss: 1.2099363512333232e-06
Iter: 1659 loss: 1.2094749029065343e-06
Iter: 1660 loss: 1.21012062297493e-06
Iter: 1661 loss: 1.2092471668396333e-06
Iter: 1662 loss: 1.2088251450276304e-06
Iter: 1663 loss: 1.2087467771834539e-06
Iter: 1664 loss: 1.208462827804458e-06
Iter: 1665 loss: 1.2078388568723089e-06
Iter: 1666 loss: 1.2084608133733738e-06
Iter: 1667 loss: 1.2074874970185796e-06
Iter: 1668 loss: 1.207005722003479e-06
Iter: 1669 loss: 1.2069987594091558e-06
Iter: 1670 loss: 1.2065455500270537e-06
Iter: 1671 loss: 1.2055978901805939e-06
Iter: 1672 loss: 1.2216247418387068e-06
Iter: 1673 loss: 1.2055743093335699e-06
Iter: 1674 loss: 1.20484494400012e-06
Iter: 1675 loss: 1.2116203361389143e-06
Iter: 1676 loss: 1.2048140474720372e-06
Iter: 1677 loss: 1.2042347839087583e-06
Iter: 1678 loss: 1.2041878977238065e-06
Iter: 1679 loss: 1.2037569981447093e-06
Iter: 1680 loss: 1.2029960311133379e-06
Iter: 1681 loss: 1.2059305620327881e-06
Iter: 1682 loss: 1.2028174734341208e-06
Iter: 1683 loss: 1.2022145326985878e-06
Iter: 1684 loss: 1.2047182700358749e-06
Iter: 1685 loss: 1.2020847686823834e-06
Iter: 1686 loss: 1.2015080209078579e-06
Iter: 1687 loss: 1.2075835926771436e-06
Iter: 1688 loss: 1.2014932020413674e-06
Iter: 1689 loss: 1.2010109451254632e-06
Iter: 1690 loss: 1.2016709734018912e-06
Iter: 1691 loss: 1.2007705696171017e-06
Iter: 1692 loss: 1.2003504369166935e-06
Iter: 1693 loss: 1.2025056644788855e-06
Iter: 1694 loss: 1.2002824711790009e-06
Iter: 1695 loss: 1.1997012094579612e-06
Iter: 1696 loss: 1.1992611998462404e-06
Iter: 1697 loss: 1.1990720891827058e-06
Iter: 1698 loss: 1.1986013979698376e-06
Iter: 1699 loss: 1.1995761437232627e-06
Iter: 1700 loss: 1.1984133581815062e-06
Iter: 1701 loss: 1.1976475552531506e-06
Iter: 1702 loss: 1.1978307265534459e-06
Iter: 1703 loss: 1.1970866265620715e-06
Iter: 1704 loss: 1.1964595309216416e-06
Iter: 1705 loss: 1.2005080876514382e-06
Iter: 1706 loss: 1.1963912445121058e-06
Iter: 1707 loss: 1.1959533215594403e-06
Iter: 1708 loss: 1.1974282988625689e-06
Iter: 1709 loss: 1.1958347777756877e-06
Iter: 1710 loss: 1.1952882773106705e-06
Iter: 1711 loss: 1.1964098521558874e-06
Iter: 1712 loss: 1.1950687015816561e-06
Iter: 1713 loss: 1.1945837587157097e-06
Iter: 1714 loss: 1.1939239980361966e-06
Iter: 1715 loss: 1.1938916047119244e-06
Iter: 1716 loss: 1.1933237111627284e-06
Iter: 1717 loss: 1.2004690992161319e-06
Iter: 1718 loss: 1.193318708031078e-06
Iter: 1719 loss: 1.1927621542312531e-06
Iter: 1720 loss: 1.1919273826802089e-06
Iter: 1721 loss: 1.191909977231142e-06
Iter: 1722 loss: 1.1910215147217227e-06
Iter: 1723 loss: 1.1978075228388226e-06
Iter: 1724 loss: 1.1909547875539669e-06
Iter: 1725 loss: 1.1904916974411349e-06
Iter: 1726 loss: 1.1904510327356857e-06
Iter: 1727 loss: 1.1900943006006e-06
Iter: 1728 loss: 1.1901773799714564e-06
Iter: 1729 loss: 1.1898324062344152e-06
Iter: 1730 loss: 1.1893465357093237e-06
Iter: 1731 loss: 1.1920413413678386e-06
Iter: 1732 loss: 1.189277013644618e-06
Iter: 1733 loss: 1.188810498179832e-06
Iter: 1734 loss: 1.1883436008988759e-06
Iter: 1735 loss: 1.1882485275946754e-06
Iter: 1736 loss: 1.1877394661196488e-06
Iter: 1737 loss: 1.1878196728075037e-06
Iter: 1738 loss: 1.1873553277001827e-06
Iter: 1739 loss: 1.1868893497531275e-06
Iter: 1740 loss: 1.1868826156932e-06
Iter: 1741 loss: 1.1864382111386402e-06
Iter: 1742 loss: 1.1859223418104648e-06
Iter: 1743 loss: 1.1858620140844043e-06
Iter: 1744 loss: 1.1852429339000084e-06
Iter: 1745 loss: 1.1906829554950662e-06
Iter: 1746 loss: 1.1852114500725195e-06
Iter: 1747 loss: 1.1847829610891705e-06
Iter: 1748 loss: 1.1870330128315442e-06
Iter: 1749 loss: 1.1847160681789172e-06
Iter: 1750 loss: 1.1842628307032812e-06
Iter: 1751 loss: 1.1835091874493363e-06
Iter: 1752 loss: 1.1835065920375602e-06
Iter: 1753 loss: 1.18275727688032e-06
Iter: 1754 loss: 1.1855073593328891e-06
Iter: 1755 loss: 1.1825716166076137e-06
Iter: 1756 loss: 1.1819146160249991e-06
Iter: 1757 loss: 1.1820196146464416e-06
Iter: 1758 loss: 1.1814192640267046e-06
Iter: 1759 loss: 1.1804727749636502e-06
Iter: 1760 loss: 1.1902237376590235e-06
Iter: 1761 loss: 1.1804459144085245e-06
Iter: 1762 loss: 1.1801173326700159e-06
Iter: 1763 loss: 1.1800787324414459e-06
Iter: 1764 loss: 1.1797694166671178e-06
Iter: 1765 loss: 1.1791190343217777e-06
Iter: 1766 loss: 1.1899537833585452e-06
Iter: 1767 loss: 1.1791016161200002e-06
Iter: 1768 loss: 1.1785475203031055e-06
Iter: 1769 loss: 1.17853918675588e-06
Iter: 1770 loss: 1.1783095870681514e-06
Iter: 1771 loss: 1.1777527288526213e-06
Iter: 1772 loss: 1.18364521669493e-06
Iter: 1773 loss: 1.1776908193454179e-06
Iter: 1774 loss: 1.1769861211418508e-06
Iter: 1775 loss: 1.1804487918006708e-06
Iter: 1776 loss: 1.1768646681391644e-06
Iter: 1777 loss: 1.176301864394598e-06
Iter: 1778 loss: 1.1776184353589141e-06
Iter: 1779 loss: 1.1760947736489431e-06
Iter: 1780 loss: 1.1753815484936976e-06
Iter: 1781 loss: 1.1798683220214575e-06
Iter: 1782 loss: 1.1752998720718671e-06
Iter: 1783 loss: 1.1747945011842778e-06
Iter: 1784 loss: 1.1743010445412129e-06
Iter: 1785 loss: 1.1741923690360407e-06
Iter: 1786 loss: 1.1735846433387331e-06
Iter: 1787 loss: 1.1735798534172532e-06
Iter: 1788 loss: 1.1732098444182269e-06
Iter: 1789 loss: 1.1733535059951615e-06
Iter: 1790 loss: 1.1729529627990653e-06
Iter: 1791 loss: 1.1723921233399318e-06
Iter: 1792 loss: 1.1716962172441004e-06
Iter: 1793 loss: 1.1716368880199755e-06
Iter: 1794 loss: 1.1709324474003395e-06
Iter: 1795 loss: 1.1734196176041325e-06
Iter: 1796 loss: 1.170750648115516e-06
Iter: 1797 loss: 1.1705286090398675e-06
Iter: 1798 loss: 1.1704196146686377e-06
Iter: 1799 loss: 1.1700899339624222e-06
Iter: 1800 loss: 1.1704995305154575e-06
Iter: 1801 loss: 1.1699187344114249e-06
Iter: 1802 loss: 1.169545500453081e-06
Iter: 1803 loss: 1.1691557686568823e-06
Iter: 1804 loss: 1.1690869682844266e-06
Iter: 1805 loss: 1.1685508728798127e-06
Iter: 1806 loss: 1.1764835658667092e-06
Iter: 1807 loss: 1.1685503568228104e-06
Iter: 1808 loss: 1.1681859545334922e-06
Iter: 1809 loss: 1.167380950147104e-06
Iter: 1810 loss: 1.1790195013690565e-06
Iter: 1811 loss: 1.1673426239837963e-06
Iter: 1812 loss: 1.1665375486349927e-06
Iter: 1813 loss: 1.1685144976643833e-06
Iter: 1814 loss: 1.1662515528708835e-06
Iter: 1815 loss: 1.1654508438312898e-06
Iter: 1816 loss: 1.1725067016256885e-06
Iter: 1817 loss: 1.1654104882869187e-06
Iter: 1818 loss: 1.165023792612502e-06
Iter: 1819 loss: 1.1650236998760819e-06
Iter: 1820 loss: 1.1647329720747066e-06
Iter: 1821 loss: 1.1643998494936995e-06
Iter: 1822 loss: 1.1643586611058724e-06
Iter: 1823 loss: 1.1638174365859011e-06
Iter: 1824 loss: 1.16428717568327e-06
Iter: 1825 loss: 1.1634992160246165e-06
Iter: 1826 loss: 1.1626981788153758e-06
Iter: 1827 loss: 1.1680839092801852e-06
Iter: 1828 loss: 1.1626178224223208e-06
Iter: 1829 loss: 1.1622112740587685e-06
Iter: 1830 loss: 1.1622806461432936e-06
Iter: 1831 loss: 1.1619059586139613e-06
Iter: 1832 loss: 1.1612854746005962e-06
Iter: 1833 loss: 1.1619568758255835e-06
Iter: 1834 loss: 1.1609458373889009e-06
Iter: 1835 loss: 1.160837730299715e-06
Iter: 1836 loss: 1.1606648653195124e-06
Iter: 1837 loss: 1.1603855016312455e-06
Iter: 1838 loss: 1.1598100294329683e-06
Iter: 1839 loss: 1.1699398289597011e-06
Iter: 1840 loss: 1.1597984211669798e-06
Iter: 1841 loss: 1.1593331501395532e-06
Iter: 1842 loss: 1.1605489965210098e-06
Iter: 1843 loss: 1.1591755488803096e-06
Iter: 1844 loss: 1.1585335121481999e-06
Iter: 1845 loss: 1.1615607308039908e-06
Iter: 1846 loss: 1.1584162394720933e-06
Iter: 1847 loss: 1.1580508308439787e-06
Iter: 1848 loss: 1.1582672255978568e-06
Iter: 1849 loss: 1.1578147031562007e-06
Iter: 1850 loss: 1.1573026364228331e-06
Iter: 1851 loss: 1.1565421985179279e-06
Iter: 1852 loss: 1.1565245292869887e-06
Iter: 1853 loss: 1.155846395477225e-06
Iter: 1854 loss: 1.162309537548362e-06
Iter: 1855 loss: 1.1558201802090197e-06
Iter: 1856 loss: 1.1552279745039321e-06
Iter: 1857 loss: 1.1605483451380804e-06
Iter: 1858 loss: 1.1551999064539167e-06
Iter: 1859 loss: 1.1547564544185975e-06
Iter: 1860 loss: 1.1557251707046e-06
Iter: 1861 loss: 1.1545853814180086e-06
Iter: 1862 loss: 1.1541480080957801e-06
Iter: 1863 loss: 1.1532141449104853e-06
Iter: 1864 loss: 1.1681224452788385e-06
Iter: 1865 loss: 1.1531838195735087e-06
Iter: 1866 loss: 1.1533167192453133e-06
Iter: 1867 loss: 1.1527589308686502e-06
Iter: 1868 loss: 1.1525319836812435e-06
Iter: 1869 loss: 1.1520431360810085e-06
Iter: 1870 loss: 1.159653103320665e-06
Iter: 1871 loss: 1.1520255148750788e-06
Iter: 1872 loss: 1.1514074301072489e-06
Iter: 1873 loss: 1.1565645598397056e-06
Iter: 1874 loss: 1.1513707478841599e-06
Iter: 1875 loss: 1.1507551236499386e-06
Iter: 1876 loss: 1.1536430946422319e-06
Iter: 1877 loss: 1.1506418761066769e-06
Iter: 1878 loss: 1.1502440011384512e-06
Iter: 1879 loss: 1.1495238050902446e-06
Iter: 1880 loss: 1.1668346427332738e-06
Iter: 1881 loss: 1.1495236128876967e-06
Iter: 1882 loss: 1.1489331845737924e-06
Iter: 1883 loss: 1.1541527498248904e-06
Iter: 1884 loss: 1.148903714853679e-06
Iter: 1885 loss: 1.1483577766601614e-06
Iter: 1886 loss: 1.1518770927377298e-06
Iter: 1887 loss: 1.1482981482931722e-06
Iter: 1888 loss: 1.1480164909189716e-06
Iter: 1889 loss: 1.1475929520108203e-06
Iter: 1890 loss: 1.1475843735903288e-06
Iter: 1891 loss: 1.1469218506367293e-06
Iter: 1892 loss: 1.1480713325669529e-06
Iter: 1893 loss: 1.146627988700311e-06
Iter: 1894 loss: 1.1460008330122583e-06
Iter: 1895 loss: 1.147341133018614e-06
Iter: 1896 loss: 1.1457553625214754e-06
Iter: 1897 loss: 1.145349057425733e-06
Iter: 1898 loss: 1.1453381264722726e-06
Iter: 1899 loss: 1.1448996145338721e-06
Iter: 1900 loss: 1.1443734036825529e-06
Iter: 1901 loss: 1.1443204692935442e-06
Iter: 1902 loss: 1.1437955371590603e-06
Iter: 1903 loss: 1.146785163824813e-06
Iter: 1904 loss: 1.143723668232312e-06
Iter: 1905 loss: 1.1432375933525294e-06
Iter: 1906 loss: 1.14342065123214e-06
Iter: 1907 loss: 1.1428987317766706e-06
Iter: 1908 loss: 1.1423471156479226e-06
Iter: 1909 loss: 1.1423466962632304e-06
Iter: 1910 loss: 1.1420469871301914e-06
Iter: 1911 loss: 1.1433925198276575e-06
Iter: 1912 loss: 1.1419885200013198e-06
Iter: 1913 loss: 1.1416239383471556e-06
Iter: 1914 loss: 1.1410231370574241e-06
Iter: 1915 loss: 1.1410204891555274e-06
Iter: 1916 loss: 1.1405051929537622e-06
Iter: 1917 loss: 1.140844291640038e-06
Iter: 1918 loss: 1.1401797119571251e-06
Iter: 1919 loss: 1.1395457392776132e-06
Iter: 1920 loss: 1.1452423748934337e-06
Iter: 1921 loss: 1.1395157157326803e-06
Iter: 1922 loss: 1.1390487569608306e-06
Iter: 1923 loss: 1.1426444168840284e-06
Iter: 1924 loss: 1.1390143911410232e-06
Iter: 1925 loss: 1.1386920410814573e-06
Iter: 1926 loss: 1.1380604731080651e-06
Iter: 1927 loss: 1.1506931178296192e-06
Iter: 1928 loss: 1.1380552075375076e-06
Iter: 1929 loss: 1.1375112339474393e-06
Iter: 1930 loss: 1.1402124405504396e-06
Iter: 1931 loss: 1.1374188953109131e-06
Iter: 1932 loss: 1.1368035893906394e-06
Iter: 1933 loss: 1.1376652411117069e-06
Iter: 1934 loss: 1.1365000107459598e-06
Iter: 1935 loss: 1.136053869151177e-06
Iter: 1936 loss: 1.1387063510947161e-06
Iter: 1937 loss: 1.135997177599397e-06
Iter: 1938 loss: 1.1353374010021616e-06
Iter: 1939 loss: 1.1354198761350474e-06
Iter: 1940 loss: 1.1348335514460522e-06
Iter: 1941 loss: 1.1342866783249511e-06
Iter: 1942 loss: 1.1350751217806338e-06
Iter: 1943 loss: 1.1340204247410003e-06
Iter: 1944 loss: 1.1334098722303788e-06
Iter: 1945 loss: 1.1373700113096164e-06
Iter: 1946 loss: 1.1333440012074677e-06
Iter: 1947 loss: 1.1327566187913621e-06
Iter: 1948 loss: 1.1373685768185437e-06
Iter: 1949 loss: 1.1327154795681549e-06
Iter: 1950 loss: 1.132372846336549e-06
Iter: 1951 loss: 1.1319955684347368e-06
Iter: 1952 loss: 1.1319407767804428e-06
Iter: 1953 loss: 1.1314365099018951e-06
Iter: 1954 loss: 1.1361987091888052e-06
Iter: 1955 loss: 1.1314163558638737e-06
Iter: 1956 loss: 1.1310849345780786e-06
Iter: 1957 loss: 1.130607236792353e-06
Iter: 1958 loss: 1.1305923929516468e-06
Iter: 1959 loss: 1.1301938266613185e-06
Iter: 1960 loss: 1.1301938029590621e-06
Iter: 1961 loss: 1.1297601557163474e-06
Iter: 1962 loss: 1.129829662027014e-06
Iter: 1963 loss: 1.1294332590627535e-06
Iter: 1964 loss: 1.1288912527860836e-06
Iter: 1965 loss: 1.1294993161941971e-06
Iter: 1966 loss: 1.1285984485464379e-06
/home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /home/mrdouglas/Manifold/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
++ basename experiments.final/script130
+ '[' -r STOP.script130 ']'
+ MODEL='--load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.6/300_300_300_1'
+ for phi in 0 0.4 0.8 1.2 1.6 2 2.4 2.8 3
+ OUTDIR=/home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi2
+ OUTDIR2=/home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi2
+ mkdir -p /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi2 /home/mrdouglas/Manifold/experiments.final/output131/f1_psi3_phi2
+ date
Sun Nov  8 09:07:36 EST 2020
+ '[' -r /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi2/300_300_300_1 ']'
+ python biholoNN_train64.py --seed 1234 --load_model /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi1.6/300_300_300_1 --function f1 --psi 3 --phi 2 --layers 300_300_300_1 --save_dir /home/mrdouglas/Manifold/experiments.final/output130/f1_psi3_phi2/ --save_name 300_300_300_1 --optimizer adam --n_pairs 20000 --batch_size 5000 --max_epochs 400 --loss_func weighted_MAPE
Processing model: 300_300_300_1
WARNING:tensorflow:5 out of the last 5 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb81969a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:6 out of the last 6 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8196d9620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:7 out of the last 7 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb81969a268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:8 out of the last 8 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb81969ac80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:9 out of the last 9 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8195f26a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:10 out of the last 10 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8195f2ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8194bf7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8195189d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb81950c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8194fa158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb81959f7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8193f27b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8193f22f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8193d57b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb819493510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb819495158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8194959d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb81935a620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8149458c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb814945510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb819381950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb81937cf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb814903950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb81945a2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb81945a7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb81486d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb81480d598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb814812510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb814812a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8148b9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb81479e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8147e1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8147f62f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb8147d3d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb814754a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to <function Hypersurface.num_FS_volume_form_tf at 0x7fb814743840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
